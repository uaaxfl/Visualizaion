2020.acl-main.196,N18-1204,0,0.22989,"Missing"
2020.acl-main.196,P14-2006,0,0.0690508,"Missing"
2020.acl-main.196,N16-1024,0,0.174665,"ical results that reinforce the validity of importance sampling for evaluating latent language models. 1 Sameer Singh Univ. of California, Irvine sameer@uci.edu Introduction Latent language models are generative models of text that jointly represent the text and the latent structure underlying it, such as: the syntactic parse, coreference chains between entity mentions, or links of entities and relations mentioned in the text to an external knowledge graph. The benefits of modeling such structure include interpretability (Hayashi et al., 2020), better performance on tasks requiring structure (Dyer et al., 2016; Ji et al., 2017), and improved ability to generate consistent mentions of entities (Clark et al., 2018) and factually accurate text (Logan et al., 2019). Unfortunately, demonstrating that these models provide better performance than traditional language models by evaluating their likelihood on benchmark data can be difficult, as exact computation requires marginalizing over all possible latent structures. In this paper, we seek to fill in this missing knowledge, and put this practice on more rigorous footing. First, we review the theory of importance sampling, providing proof that importance"
2020.acl-main.196,D17-1195,0,0.256391,"einforce the validity of importance sampling for evaluating latent language models. 1 Sameer Singh Univ. of California, Irvine sameer@uci.edu Introduction Latent language models are generative models of text that jointly represent the text and the latent structure underlying it, such as: the syntactic parse, coreference chains between entity mentions, or links of entities and relations mentioned in the text to an external knowledge graph. The benefits of modeling such structure include interpretability (Hayashi et al., 2020), better performance on tasks requiring structure (Dyer et al., 2016; Ji et al., 2017), and improved ability to generate consistent mentions of entities (Clark et al., 2018) and factually accurate text (Logan et al., 2019). Unfortunately, demonstrating that these models provide better performance than traditional language models by evaluating their likelihood on benchmark data can be difficult, as exact computation requires marginalizing over all possible latent structures. In this paper, we seek to fill in this missing knowledge, and put this practice on more rigorous footing. First, we review the theory of importance sampling, providing proof that importance sampled perplexit"
2020.acl-main.196,N19-1114,0,0.071848,"s of evaluation data comprised of instances (token sequences) xn , estimates can be formed at the instance level:   N   1 X  , d I = exp − PPL log p(x ˆ ) (5) n   T n=1 or at the corpus level: ! 1 d C = exp − log p(x PPL ˆ C) , T (6) i.e., average is either over each instance or the whole corpus.2 RNNG and EntityNLM perform instance-level aggregation, whereas KGLM performs corpus-level aggregation. Note that these 1 τ = 0.5 τ = 0.9 τ = 1.0 88 τ = 1.1 τ = 2.0 No Peeking 86 84 0 Sample Size Typically, only 100 samples are used for computing the perplexity. A notable exception is Kim et al. (2019)’s follow-up to RNNG that uses 1000 samples. 200 400 600 800 1000 800 1000 E NTITY NLM 118 Perplexity 116 114 112 110 108 0 200 400 600 KGLM 125 Perplexity Proposal Distribution Previous work uses proposal distributions q(z|x) that are essentially discriminative versions of the generative model (e.g., they are models that predict the latent state conditioned on the text), with one key distinction: they are conditioned not only on the sequence of tokens that have been observed so far, but also on future tokens that the model will be evaluated on (a trait we will refer to as peeking). This condi"
2020.acl-main.196,P19-1598,1,0.813312,"@uci.edu Introduction Latent language models are generative models of text that jointly represent the text and the latent structure underlying it, such as: the syntactic parse, coreference chains between entity mentions, or links of entities and relations mentioned in the text to an external knowledge graph. The benefits of modeling such structure include interpretability (Hayashi et al., 2020), better performance on tasks requiring structure (Dyer et al., 2016; Ji et al., 2017), and improved ability to generate consistent mentions of entities (Clark et al., 2018) and factually accurate text (Logan et al., 2019). Unfortunately, demonstrating that these models provide better performance than traditional language models by evaluating their likelihood on benchmark data can be difficult, as exact computation requires marginalizing over all possible latent structures. In this paper, we seek to fill in this missing knowledge, and put this practice on more rigorous footing. First, we review the theory of importance sampling, providing proof that importance sampled perplexity estimates are stochastic upper bounds of the true perplexity—a previously unnoted justification for this evaluation technique. In addi"
2020.acl-main.196,J93-2004,0,0.0700986,"use the preBased both on the cited papers and available source code. One could also consider token-level estimates. To our knowledge, these have been unused by existing work. 2 2173 3 4 https://github.com/harvardnlp/urnng https://github.com/rloganiv/kglm-model trained model weights. For EntityNLM we train the model from scratch following the procedure described by Ji et al. (2017); results may not be directly comparable due to differences in data preprocessing and hyperparameters. We evaluate models on the datasets used in their original papers: RNNG is evaluated on the Penn Treebank corpus (Marcus et al., 1993), EntityNLM is evaluated on English data from the CoNLL 2012 shared task (Pradhan et al., 2014), and KGLM is evaluated on the Linked WikiText-2 corpus (Logan et al., 2019). Experiments For EntityNLM and KGLM, we experiment with two kinds of proposal distributions: (1) the standard peeking proposal distribution that conditions on future evaluation data, and (2) a non-peeking variant that is conditioned only on the data observed by the model (this is akin to estimating perplexity by ancestral sampling). For RNNG we only experiment with peeking proposals, since a non-peeking variant generates inv"
2020.acl-main.442,Q19-1004,0,0.0605735,"Missing"
2020.acl-main.442,D19-1107,0,0.0518218,"Missing"
2020.acl-main.442,N18-1170,0,0.0333262,"dicator, held-out datasets are often not comprehensive, and contain the same biases as the training data (Rajpurkar et al., 2018), such that real-world performance may be overestimated (Patel et al., 2008; Recht et al., 2019). Further, by summarizing the performance as a single aggregate statistic, it becomes difficult to figure out where the model is failing, and how to fix it (Wu et al., 2019). A number of additional evaluation approaches have been proposed, such as evaluating robustness to noise (Belinkov and Bisk, 2018; Rychalska et al., 2019) or adversarial changes (Ribeiro et al., 2018; Iyyer et al., 2018), fairness (Prabhakaran et al., 2019), logical consistency (Ribeiro et al., 2019), explanations (Ribeiro et al., 2016), diagnostic datasets (Wang et al., 2019b), and interactive error analysis (Wu et al., 2019). However, these approaches focus either on individual tasks such as Question Answering or Natural Language Inference, or on a few capabilities (e.g. robustness), and thus do not provide comprehensive guidance on how to evaluate models. Software engineering research, on the other hand, has proposed a variety of paradigms and tools for testing complex software systems. In particular, “beh"
2020.acl-main.442,S19-1026,0,0.0491617,"Missing"
2020.acl-main.442,2021.ccl-1.108,0,0.279281,"Missing"
2020.acl-main.442,C18-1198,0,0.116171,"Missing"
2020.acl-main.442,D19-1578,0,0.23891,"often not comprehensive, and contain the same biases as the training data (Rajpurkar et al., 2018), such that real-world performance may be overestimated (Patel et al., 2008; Recht et al., 2019). Further, by summarizing the performance as a single aggregate statistic, it becomes difficult to figure out where the model is failing, and how to fix it (Wu et al., 2019). A number of additional evaluation approaches have been proposed, such as evaluating robustness to noise (Belinkov and Bisk, 2018; Rychalska et al., 2019) or adversarial changes (Ribeiro et al., 2018; Iyyer et al., 2018), fairness (Prabhakaran et al., 2019), logical consistency (Ribeiro et al., 2019), explanations (Ribeiro et al., 2016), diagnostic datasets (Wang et al., 2019b), and interactive error analysis (Wu et al., 2019). However, these approaches focus either on individual tasks such as Question Answering or Natural Language Inference, or on a few capabilities (e.g. robustness), and thus do not provide comprehensive guidance on how to evaluate models. Software engineering research, on the other hand, has proposed a variety of paradigms and tools for testing complex software systems. In particular, “behavioral testing” (also known as black"
2020.acl-main.442,P18-2124,0,0.0488703,"e as many tests, and found almost three times as many bugs as users without it. 1 Introduction One of the primary goals of training NLP models is generalization. Since testing “in the wild” is expensive and does not allow for fast iterations, the standard paradigm for evaluation is using trainvalidation-test splits to estimate the accuracy of the model, including the use of leader boards to track progress on a task (Rajpurkar et al., 2016). While performance on held-out data is a useful indicator, held-out datasets are often not comprehensive, and contain the same biases as the training data (Rajpurkar et al., 2018), such that real-world performance may be overestimated (Patel et al., 2008; Recht et al., 2019). Further, by summarizing the performance as a single aggregate statistic, it becomes difficult to figure out where the model is failing, and how to fix it (Wu et al., 2019). A number of additional evaluation approaches have been proposed, such as evaluating robustness to noise (Belinkov and Bisk, 2018; Rychalska et al., 2019) or adversarial changes (Ribeiro et al., 2018; Iyyer et al., 2018), fairness (Prabhakaran et al., 2019), logical consistency (Ribeiro et al., 2019), explanations (Ribeiro et al"
2020.acl-main.442,D16-1264,0,0.0506769,"onsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it. 1 Introduction One of the primary goals of training NLP models is generalization. Since testing “in the wild” is expensive and does not allow for fast iterations, the standard paradigm for evaluation is using trainvalidation-test splits to estimate the accuracy of the model, including the use of leader boards to track progress on a task (Rajpurkar et al., 2016). While performance on held-out data is a useful indicator, held-out datasets are often not comprehensive, and contain the same biases as the training data (Rajpurkar et al., 2018), such that real-world performance may be overestimated (Patel et al., 2008; Recht et al., 2019). Further, by summarizing the performance as a single aggregate statistic, it becomes difficult to figure out where the model is failing, and how to fix it (Wu et al., 2019). A number of additional evaluation approaches have been proposed, such as evaluating robustness to noise (Belinkov and Bisk, 2018; Rychalska et al., 2"
2020.acl-main.442,P19-1621,1,0.845282,"ases as the training data (Rajpurkar et al., 2018), such that real-world performance may be overestimated (Patel et al., 2008; Recht et al., 2019). Further, by summarizing the performance as a single aggregate statistic, it becomes difficult to figure out where the model is failing, and how to fix it (Wu et al., 2019). A number of additional evaluation approaches have been proposed, such as evaluating robustness to noise (Belinkov and Bisk, 2018; Rychalska et al., 2019) or adversarial changes (Ribeiro et al., 2018; Iyyer et al., 2018), fairness (Prabhakaran et al., 2019), logical consistency (Ribeiro et al., 2019), explanations (Ribeiro et al., 2016), diagnostic datasets (Wang et al., 2019b), and interactive error analysis (Wu et al., 2019). However, these approaches focus either on individual tasks such as Question Answering or Natural Language Inference, or on a few capabilities (e.g. robustness), and thus do not provide comprehensive guidance on how to evaluate models. Software engineering research, on the other hand, has proposed a variety of paradigms and tools for testing complex software systems. In particular, “behavioral testing” (also known as black-box testing) is concerned with testing diff"
2020.acl-main.442,N16-3020,1,0.552127,"et al., 2018), such that real-world performance may be overestimated (Patel et al., 2008; Recht et al., 2019). Further, by summarizing the performance as a single aggregate statistic, it becomes difficult to figure out where the model is failing, and how to fix it (Wu et al., 2019). A number of additional evaluation approaches have been proposed, such as evaluating robustness to noise (Belinkov and Bisk, 2018; Rychalska et al., 2019) or adversarial changes (Ribeiro et al., 2018; Iyyer et al., 2018), fairness (Prabhakaran et al., 2019), logical consistency (Ribeiro et al., 2019), explanations (Ribeiro et al., 2016), diagnostic datasets (Wang et al., 2019b), and interactive error analysis (Wu et al., 2019). However, these approaches focus either on individual tasks such as Question Answering or Natural Language Inference, or on a few capabilities (e.g. robustness), and thus do not provide comprehensive guidance on how to evaluate models. Software engineering research, on the other hand, has proposed a variety of paradigms and tools for testing complex software systems. In particular, “behavioral testing” (also known as black-box testing) is concerned with testing different capabilities of a system by val"
2020.acl-main.442,P18-1079,1,0.86527,"ut data is a useful indicator, held-out datasets are often not comprehensive, and contain the same biases as the training data (Rajpurkar et al., 2018), such that real-world performance may be overestimated (Patel et al., 2008; Recht et al., 2019). Further, by summarizing the performance as a single aggregate statistic, it becomes difficult to figure out where the model is failing, and how to fix it (Wu et al., 2019). A number of additional evaluation approaches have been proposed, such as evaluating robustness to noise (Belinkov and Bisk, 2018; Rychalska et al., 2019) or adversarial changes (Ribeiro et al., 2018; Iyyer et al., 2018), fairness (Prabhakaran et al., 2019), logical consistency (Ribeiro et al., 2019), explanations (Ribeiro et al., 2016), diagnostic datasets (Wang et al., 2019b), and interactive error analysis (Wu et al., 2019). However, these approaches focus either on individual tasks such as Question Answering or Natural Language Inference, or on a few capabilities (e.g. robustness), and thus do not provide comprehensive guidance on how to evaluate models. Software engineering research, on the other hand, has proposed a variety of paradigms and tools for testing complex software systems"
2020.acl-main.442,C18-1228,0,0.0648741,"Missing"
2020.acl-main.442,P19-1452,0,0.0537057,"Missing"
2020.acl-main.442,W16-2520,0,0.0648846,"Missing"
2020.acl-main.442,D19-1221,1,0.916761,"Missing"
2020.acl-main.442,P19-1073,1,0.786812,"ion is using trainvalidation-test splits to estimate the accuracy of the model, including the use of leader boards to track progress on a task (Rajpurkar et al., 2016). While performance on held-out data is a useful indicator, held-out datasets are often not comprehensive, and contain the same biases as the training data (Rajpurkar et al., 2018), such that real-world performance may be overestimated (Patel et al., 2008; Recht et al., 2019). Further, by summarizing the performance as a single aggregate statistic, it becomes difficult to figure out where the model is failing, and how to fix it (Wu et al., 2019). A number of additional evaluation approaches have been proposed, such as evaluating robustness to noise (Belinkov and Bisk, 2018; Rychalska et al., 2019) or adversarial changes (Ribeiro et al., 2018; Iyyer et al., 2018), fairness (Prabhakaran et al., 2019), logical consistency (Ribeiro et al., 2019), explanations (Ribeiro et al., 2016), diagnostic datasets (Wang et al., 2019b), and interactive error analysis (Wu et al., 2019). However, these approaches focus either on individual tasks such as Question Answering or Natural Language Inference, or on a few capabilities (e.g. robustness), and th"
2020.acl-main.495,N16-1181,0,0.171489,"d field goal. The Texans tried to cut the lead with QB Matt Schaub getting a 8-yard TD pass to WR Andre Johnson, but the Titans would pull away with RB Javon Ringer throwing a 7-yard TD pass . The Texans tried to come back into the game in the fourth quarter, but only came away with Schaub throwing a 12-yard TD pass to WR Kevin Walter. relocate[who threw] find-max-num filter [the second half] find [touchdown pass] Figure 2: An example for a mapping of an utterance to a gold program and a perfect execution in a reasoning problem from NLVR2 (top) and DROP (bottom). Neural module networks (NMNs; Andreas et al., 2016) parse an input utterance into an executable program composed of learnable modules that are designed to perform atomic reasoning tasks and can be composed to perform complex reasoning against an unstructured context. NMNs are appealing since their output is interpretable; they provide a logical meaning representation of the utterance and also the outputs of the intermediate steps (modules) to reach the final answer. However, because module parameters are typically learned from end-task supervision only, it is possible that the program will not be a faithful explanation of the behaviour of the"
2020.acl-main.495,D17-1160,1,0.826889,"d outputs a distribution over the passage tokens and find-num outputs a distribution over the numbers in the passage. We extend their model and introduce additional modules; addition and subtraction to add or subtract passage numbers, and extract-answer which directly predicts an answer span from the representations of passage tokens without any explicit compositional reasoning. We use BERT-base (Devlin et al., 2019) to encode the input question and passage. The Text-NMN does not have access to gold programs, and thus we implement a parser as an encoder-decoder model with attention similar to Krishnamurthy et al. (2017), which takes the utterance as input, and outputs a linearized abstract syntax tree of the predicted program. 5596 3 Module-wise Faithfulness Neural module networks (NMNs) facilitate interpretability of their predictions via the reasoning steps in the structured program and providing the outputs of those intermediate steps during execution. For example, in Figure 2, all reasoning steps taken by both the Visual-NMN and Text-NMN can be discerned from the program and the intermediate module outputs. However, because module parameters are learned from an end-task, there is no guarantee that the mo"
2020.acl-main.495,P19-1416,1,0.915983,"Missing"
2020.acl-main.495,Q19-1016,0,0.0333891,"Missing"
2020.acl-main.495,C00-2137,0,\N,Missing
2020.acl-main.495,P17-2034,0,\N,Missing
2020.acl-main.495,D18-1259,0,\N,Missing
2020.acl-main.495,N19-1246,1,\N,Missing
2020.acl-main.495,N19-1423,0,\N,Missing
2020.acl-main.495,P19-1644,0,\N,Missing
2020.acl-main.495,D19-1002,0,\N,Missing
2020.acl-main.495,D19-1455,0,\N,Missing
2020.acl-main.495,2020.acl-main.386,0,\N,Missing
2020.acl-main.497,D19-1609,0,0.0358835,"Missing"
2020.acl-main.497,P07-1036,0,0.0746863,"e dataset. Our work shows that collecting intermediate annotations for a fraction of dataset is cost-effective and helps alleviate dataset collection biases to a degree. Another line of work (Ning et al., 2019) explores the cost vs. benefit of collecting full vs. partial annotations for various structured predictions tasks. However, they do not focus on intermediate reasoning required to learn the task. Our auxiliary training with intermediate annotations is inspired by extensive related work on training models using side information or domain knowledge beyond labels (Mann and McCallum, 2008; Chang et al., 2007; Ganchev et al., 2010; Rocktaschel et al., 2015). Especially relevant is work on supervising models using explanations (Ross et al., 2017), which, similar to our annotations, identify parts of the input that are important for prediction (Lei et al., 2016; Ribeiro et al., 2016). 5 Conclusion We show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. However, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending"
2020.acl-main.497,D19-1606,1,0.892547,"Missing"
2020.acl-main.497,N19-1423,0,0.0281192,"we train multiple models for the DROP and Quoref datasets, and evaluate the benefits of intermediate annotations as compared to traditional QA pairs. In particular, we will focus on the cost vs benefit tradeoff of intermediate annotations, along with evaluating their ability to mitigate bias in the training data. 3.1 Setup We study the impact of annotations on DROP on two models at the top of the leaderboard: NABERT1 and MTMSN (Hu et al., 2019). Both the models employ a similar arithmetic block introduced in the baseline model (Dua et al., 2019) on top of contextual representations from BERT (Devlin et al., 2019). For Quoref, we use the baseline XLNet (Yang et al., 2019) model released with the dataset. We supervise these models with the annotations in a simple way, by jointly predicting intermediate annotation and the final answer. We add two auxiliary loss terms to the marginal loglikelihood loss function. The first is a cross-entropy loss between the gold annotations (g) and predicted annotations, which are obtained by passing the final BERT representations through a linear layer to get a score per token p, then normalizing each token’s score of being selected as an annotation 5628 1 https://github"
2020.acl-main.497,N19-1246,1,0.935372,"ss on 4th-and-goal with 15 seconds left in regulation. The Bears then took a knee to force overtime.... The Bears then won on Jay Cutler’s game-winning 39-yard TD pass to wide receiver Devin Aromashodu. With the loss, not only did the Vikings fall to 11-4, they also surrendered homefield advantage to the Saints. Figure 1: Example from DROP, showing the intermediate annotations that we collected via crowd-sourcing. Introduction Recently many reading comprehension datasets requiring complex and compositional reasoning over text have been introduced, including HotpotQA (Yang et al., 2018), DROP (Dua et al., 2019), Quoref (Dasigi et al., 2019), and ROPES (Lin et al., 2019). However, models trained on these datasets (Hu et al., 2019; Andor et al., 2019) only have the final answer as supervision, leaving the model guessing at the correct latent reasoning. Figure 1 shows an example from DROP, which requires first locating various operands (i.e. relevant spans) in the text and then performing filter and count operations over them to get the final answer “3”. However, the correct answer can also be obtained by extracting the span “3” from the passage, or by adding or subtracting various numbers in the passa"
2020.acl-main.497,D19-1107,0,0.0635601,"Missing"
2020.acl-main.497,N18-2017,0,0.0382312,"Missing"
2020.acl-main.497,D19-1170,0,0.0868799,"on Jay Cutler’s game-winning 39-yard TD pass to wide receiver Devin Aromashodu. With the loss, not only did the Vikings fall to 11-4, they also surrendered homefield advantage to the Saints. Figure 1: Example from DROP, showing the intermediate annotations that we collected via crowd-sourcing. Introduction Recently many reading comprehension datasets requiring complex and compositional reasoning over text have been introduced, including HotpotQA (Yang et al., 2018), DROP (Dua et al., 2019), Quoref (Dasigi et al., 2019), and ROPES (Lin et al., 2019). However, models trained on these datasets (Hu et al., 2019; Andor et al., 2019) only have the final answer as supervision, leaving the model guessing at the correct latent reasoning. Figure 1 shows an example from DROP, which requires first locating various operands (i.e. relevant spans) in the text and then performing filter and count operations over them to get the final answer “3”. However, the correct answer can also be obtained by extracting the span “3” from the passage, or by adding or subtracting various numbers in the passage. The lack of intermediate hints makes learning challenging and can lead the model to rely on data biases, limiting it"
2020.acl-main.497,D16-1011,0,0.0274704,"ll vs. partial annotations for various structured predictions tasks. However, they do not focus on intermediate reasoning required to learn the task. Our auxiliary training with intermediate annotations is inspired by extensive related work on training models using side information or domain knowledge beyond labels (Mann and McCallum, 2008; Chang et al., 2007; Ganchev et al., 2010; Rocktaschel et al., 2015). Especially relevant is work on supervising models using explanations (Ross et al., 2017), which, similar to our annotations, identify parts of the input that are important for prediction (Lei et al., 2016; Ribeiro et al., 2016). 5 Conclusion We show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. However, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending on the task. We proposed a simple semi-supervision technique to expose the model to these annotations. We believe that in future they can be used more directly to yield better performance gains. We have also released these annotations for the research com"
2020.acl-main.497,D19-5808,1,0.88554,"Missing"
2020.acl-main.497,P08-1099,0,0.0438387,"reasoning steps for entire dataset. Our work shows that collecting intermediate annotations for a fraction of dataset is cost-effective and helps alleviate dataset collection biases to a degree. Another line of work (Ning et al., 2019) explores the cost vs. benefit of collecting full vs. partial annotations for various structured predictions tasks. However, they do not focus on intermediate reasoning required to learn the task. Our auxiliary training with intermediate annotations is inspired by extensive related work on training models using side information or domain knowledge beyond labels (Mann and McCallum, 2008; Chang et al., 2007; Ganchev et al., 2010; Rocktaschel et al., 2015). Especially relevant is work on supervising models using explanations (Ross et al., 2017), which, similar to our annotations, identify parts of the input that are important for prediction (Lei et al., 2016; Ribeiro et al., 2016). 5 Conclusion We show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. However, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that w"
2020.acl-main.497,P19-1416,1,0.8916,"Missing"
2020.acl-main.497,Q19-1016,0,0.0316898,"appendix. 4 Related Work Similar to our work, Zaidan et al. (2007) studied the impact of providing explicit supervision via rationales, rather than generating them, for varying fractions of training set in text classification. However, we study the benefits of such supervision for complex compositional reading comprehension datasets. In the field of computer vision, Donahue and Grauman (2011) collected similar annotations, for visual recognition, where crowd-workers highlighted relevant regions in images. Within reading comprehension, various works like HotpotQA (Yang et al., 2018) and CoQA (Reddy et al., 2019) have collected similar reasoning steps for entire dataset. Our work shows that collecting intermediate annotations for a fraction of dataset is cost-effective and helps alleviate dataset collection biases to a degree. Another line of work (Ning et al., 2019) explores the cost vs. benefit of collecting full vs. partial annotations for various structured predictions tasks. However, they do not focus on intermediate reasoning required to learn the task. Our auxiliary training with intermediate annotations is inspired by extensive related work on training models using side information or domain k"
2020.acl-main.497,N16-3020,1,0.133874,"otations for various structured predictions tasks. However, they do not focus on intermediate reasoning required to learn the task. Our auxiliary training with intermediate annotations is inspired by extensive related work on training models using side information or domain knowledge beyond labels (Mann and McCallum, 2008; Chang et al., 2007; Ganchev et al., 2010; Rocktaschel et al., 2015). Especially relevant is work on supervising models using explanations (Ross et al., 2017), which, similar to our annotations, identify parts of the input that are important for prediction (Lei et al., 2016; Ribeiro et al., 2016). 5 Conclusion We show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. However, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending on the task. We proposed a simple semi-supervision technique to expose the model to these annotations. We believe that in future they can be used more directly to yield better performance gains. We have also released these annotations for the research community at https: //gith"
2020.acl-main.497,N15-1118,1,0.835281,"intermediate annotations for a fraction of dataset is cost-effective and helps alleviate dataset collection biases to a degree. Another line of work (Ning et al., 2019) explores the cost vs. benefit of collecting full vs. partial annotations for various structured predictions tasks. However, they do not focus on intermediate reasoning required to learn the task. Our auxiliary training with intermediate annotations is inspired by extensive related work on training models using side information or domain knowledge beyond labels (Mann and McCallum, 2008; Chang et al., 2007; Ganchev et al., 2010; Rocktaschel et al., 2015). Especially relevant is work on supervising models using explanations (Ross et al., 2017), which, similar to our annotations, identify parts of the input that are important for prediction (Lei et al., 2016; Ribeiro et al., 2016). 5 Conclusion We show that intermediate annotations are a costeffective way to not only boost model performance but also alleviate certain unanticipated biases introduced during the dataset collection. However, it may be unnecessary to collect these for entire dataset and there is a sweet-spot that works best depending on the task. We proposed a simple semi-supervisio"
2020.acl-main.86,D19-1606,1,0.842367,"strategy is used by the baseline model for the MRQA shared task (Fisch et al., 2019) as well as for the best prior result on ORB. This method places one instance per dataset in each batch (forced heterogeneity) until the smallest dataset runs out of instances. This strategy continues with the remaining datasets, until all datasets are exhausted. 3 Train Size M9 Experiments Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koˇcisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 922 Sampling Strategies Table 2 shows the effectiveness of the sampling techniques discussed above. Uniform sampling yields a very mediocre performance for 7 datasets but significantly underperforms on SQuAD 2.0, which is likely not getting enough representation each epoch for its unique no-answer questions. Sampling by size yields mediocre performances for 7 datasets but underperforms on ROPES, which is easily the smallest dataset and therefore gets undersampled. However, performance on Quoref, the second smallest dataset,"
2020.acl-main.86,N19-1246,1,0.913652,"mportant, it is also important to explore the optimal way to structure training; as we will show, training on instances from diverse datasets (tasks) means that unlike in a single-task setting, ample instances from each task distribution must be represented during training to properly capture that diversity. We explore 2 fundamental aspects of structuring multi-task training: how many instances are sampled from each task per epoch and how those instances are organized within the epoch. We investigate the importance of this structuring by training a multi-task model on the 8 datasets from ORB (Dua et al., 2019b), a recent multi-task reading comprehension benchmark. We first explore the sampling distribution over datasets at each epoch: how many instances from each dataset should be used to train. Prior work has typically either made this a uniform distribution over datasets (implicitly favoring smaller datasets), a distribution proportional to the sizes of the datasets (implicitly favoring larger datasets), or some combination of the two. Because these sampling strategies favor some datasets over others, they can lead to catastrophic forgetting in the non-favored datasets. We introduce a dynamic sa"
2020.acl-main.86,D19-5801,0,0.018457,"nswer” questions. Each training session lasted 30 epochs with 50,000 instances sampled per epoch. Three training sessions were conducted per sampling method and the EM and F1 scores shown are averaged over those three sessions. Note that NarrativeQA is evaluated using only ROUGE F1 score. Due to GPU memory constraints, we are limited to a batch size of 4, so we are unable replicate the Uniform Batches configuration of MRQA (requires a batch size of 8 to fit 1 instance from each of the 8 datasets). Uniform Batches This scheduling strategy is used by the baseline model for the MRQA shared task (Fisch et al., 2019) as well as for the best prior result on ORB. This method places one instance per dataset in each batch (forced heterogeneity) until the smallest dataset runs out of instances. This strategy continues with the remaining datasets, until all datasets are exhausted. 3 Train Size M9 Experiments Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koˇcisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and S"
2020.acl-main.86,D19-5808,1,0.829585,"line model for the MRQA shared task (Fisch et al., 2019) as well as for the best prior result on ORB. This method places one instance per dataset in each batch (forced heterogeneity) until the smallest dataset runs out of instances. This strategy continues with the remaining datasets, until all datasets are exhausted. 3 Train Size M9 Experiments Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koˇcisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 922 Sampling Strategies Table 2 shows the effectiveness of the sampling techniques discussed above. Uniform sampling yields a very mediocre performance for 7 datasets but significantly underperforms on SQuAD 2.0, which is likely not getting enough representation each epoch for its unique no-answer questions. Sampling by size yields mediocre performances for 7 datasets but underperforms on ROPES, which is easily the smallest dataset and therefore gets undersampled. However, performance on Quoref, the second smallest dataset, is still relatively high,"
2020.acl-main.86,P18-2124,0,0.0523173,"Missing"
2020.acl-main.86,D16-1264,0,0.0492107,"ared task (Fisch et al., 2019) as well as for the best prior result on ORB. This method places one instance per dataset in each batch (forced heterogeneity) until the smallest dataset runs out of instances. This strategy continues with the remaining datasets, until all datasets are exhausted. 3 Train Size M9 Experiments Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koˇcisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 922 Sampling Strategies Table 2 shows the effectiveness of the sampling techniques discussed above. Uniform sampling yields a very mediocre performance for 7 datasets but significantly underperforms on SQuAD 2.0, which is likely not getting enough representation each epoch for its unique no-answer questions. Sampling by size yields mediocre performances for 7 datasets but underperforms on ROPES, which is easily the smallest dataset and therefore gets undersampled. However, performance on Quoref, the second smallest dataset, is still relatively high, which might be explained by its"
2020.acl-main.86,P19-1485,0,0.037067,"tegies, mitigating the catastrophic forgetting that is common in multi-task learning. We also demonstrate that allowing instances of different tasks to be interleaved as much as possible between each epoch and batch has a clear benefit in multitask performance over forcing task homogeneity at the epoch or batch level. Our final model shows greatly increased performance over the best model on ORB, a recently-released multitask reading comprehension benchmark. 1 Introduction Building multi-task reading comprehension systems has received significant attention and been a focus of active research (Talmor and Berant, 2019; Xu et al., 2019). These approaches mostly focus on model architecture improvements or generalizability to new tasks or domains. While these contributions are important, it is also important to explore the optimal way to structure training; as we will show, training on instances from diverse datasets (tasks) means that unlike in a single-task setting, ample instances from each task distribution must be represented during training to properly capture that diversity. We explore 2 fundamental aspects of structuring multi-task training: how many instances are sampled from each task per epoch and"
2020.acl-main.86,W17-2623,0,0.0206412,". Uniform Batches This scheduling strategy is used by the baseline model for the MRQA shared task (Fisch et al., 2019) as well as for the best prior result on ORB. This method places one instance per dataset in each batch (forced heterogeneity) until the smallest dataset runs out of instances. This strategy continues with the remaining datasets, until all datasets are exhausted. 3 Train Size M9 Experiments Setup The eight reading comprehension tasks are from the ORB benchmark (Dua et al., 2019b): DROP (Dua et al., 2019a), DuoRC (Saha et al., 2018), NarrativeQA (Koˇcisky et al., 2017), NewsQA (Trischler et al., 2017), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), SQuAD (Rajpurkar et al., 2016), and SQuAD 922 Sampling Strategies Table 2 shows the effectiveness of the sampling techniques discussed above. Uniform sampling yields a very mediocre performance for 7 datasets but significantly underperforms on SQuAD 2.0, which is likely not getting enough representation each epoch for its unique no-answer questions. Sampling by size yields mediocre performances for 7 datasets but underperforms on ROPES, which is easily the smallest dataset and therefore gets undersampled. However, performance on Quoref,"
2020.emnlp-main.346,P19-1484,0,0.0280999,"e large for words and labels that are relevant to a particular context, sw ∝ exp(wout · y + βy ) should be large for words that are typically associated with a given label. The sets of label tokens are then constructed from the k-highest scoring words: Vy = top-k [s(y, w)] (5) w∈V 2.4 Relation to Other Prompting Methods Our work fits into a body of work that probes language model’s knowledge via prompts. Previous works have used manually defined prompts to study an LM’s ability to perform: commonsense reasoning (Trinh and Le, 2018; Kwon et al., 2019; Shwartz et al., 2020), question answering (Lewis et al., 2019), fact recall (Petroni et al., 2019; Jiang et al., 2020; Bouraoui et al., 2019), summarization (Radford et al., 2019), and other supervised tasks (Brown et al., 2020). Schick and Sch¨utze (2020) use manually constructed prompts in conjunction with semi-supervised learning for fewshot learning. We instead automatically create prompts for any task, which leads to higher accuracy and opens up new phenomena to analyze. 2.5 Evaluation Setup In the following sections, we apply AUTO P ROMPT to probe BERTBASE 2 (110M parameters) and RoBERTaLARGE ’s (355M parameters) knowledge of the following tasks: s"
2020.emnlp-main.346,N19-1112,0,0.0214513,"racy, it is difficult to determine whether the knowledge that finetuned LMs contain is learned during the pretraining or the finetuning process. How can we directly evaluate the knowl∗ First three authors contributed equally. edge present in pretrained LMs, be it linguistic, factual, commonsense, or task-specific? Numerous techniques have been proposed to elicit such knowledge by analyzing pretrained LMs’ internal representations. A common strategy is to use probing classifiers—shallow classifiers that predict certain attributes using an LMs’ representations as features (Conneau et al., 2018; Liu et al., 2019). However, probing classifiers require additional learned parameters and are thus susceptible to false positives; high probing accuracy is not a sufficient condition to conclude that an LM contains a certain piece of knowledge (Hewitt and Liang, 2019; Voita and Titov, 2020). Attention visualization, another common technique, has a similar failure mode: attention scores may be correlated with, but not caused by the underlying target knowledge, leading to criticism against their use as explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Both probing and attention visualizations al"
2020.emnlp-main.346,marelli-etal-2014-sick,0,0.0801971,"Missing"
2020.emnlp-main.346,D19-1002,0,0.0300108,"LMs’ representations as features (Conneau et al., 2018; Liu et al., 2019). However, probing classifiers require additional learned parameters and are thus susceptible to false positives; high probing accuracy is not a sufficient condition to conclude that an LM contains a certain piece of knowledge (Hewitt and Liang, 2019; Voita and Titov, 2020). Attention visualization, another common technique, has a similar failure mode: attention scores may be correlated with, but not caused by the underlying target knowledge, leading to criticism against their use as explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Both probing and attention visualizations also struggle to evaluate knowledge that cannot be represented as simple token- or sequencelevel classification tasks. A more direct approach for eliciting knowledge from these models, since they are language models after all, is prompting, i.e. converting tasks into a language model format. For example, Radford et al. (2019) frame summarization as a language modeling task by appending “TL;DR:” to the end of an article and then generating from an LM. Similarly, Petroni et al. (2019) manually reformulate a knowledge base completion task as a cloze tes"
2020.emnlp-main.346,N18-1202,0,0.0414665,"show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning. 1 Introduction Pretrained language models (LMs) have had exceptional success when adapted to downstream tasks via finetuning (Peters et al., 2018; Devlin et al., 2019). Although it is clear that pretraining improves accuracy, it is difficult to determine whether the knowledge that finetuned LMs contain is learned during the pretraining or the finetuning process. How can we directly evaluate the knowl∗ First three authors contributed equally. edge present in pretrained LMs, be it linguistic, factual, commonsense, or task-specific? Numerous techniques have been proposed to elicit such knowledge by analyzing pretrained LMs’ internal representations. A common strategy is to use probing classifiers—shallow classifiers that predict certain a"
2020.emnlp-main.346,D19-1250,0,0.0785973,"Missing"
2020.emnlp-main.528,W05-0909,0,0.254165,"wide range of RC phenomena such as commonsense reasoning and understanding narrative over movie scripts. After collecting all annotations, we follow work on creating more robust evaluation sets (Kaushik et al., 2020; Gardner et al., 2020) and augment the test set of MOCHA by manually writing a small set of minimal pairs (Table 3). The set of minimal pairs serve as a harder evaluation set for probing metric robustness. Using MOCHA, we train a Learned Metric for Reading Comprehension which we abbreviate as LERC. We compare LERC against two sets of baselines: (1) existing metrics such as METEOR (Banerjee and Lavie, 2005) and BERTScore (Zhang et al., 2019); and (2) a sentence similarity model trained on STS-B (Cer et al., 2017). To ensure fair comparison, we evaluate LERC in an out-of-dataset setting: LERC is trained on all datasets except the one it is being evaluated on. On the test set, LERC outperforms baselines by as much as 36 Pearson correlation points and on the minimal pairs set, by as much as 26 accuracy points. Error analysis and minimal pair results indicate that there is substantial room to improve the robustness of LERC and its sensitivity to different linguistic phenomena. We hope that MOCHA and"
2020.emnlp-main.528,D18-1454,0,0.030811,"tion for doing this is that the number of generative QA datasets is quite small, which we attribute to the quality of evaluation metrics. The main focus of this work is in developing and evaluating metrics for generative RC. However, we wanted to see whether a learned metric could do well on span-selection datasets. We collected Collecting Candidates Candidates on all four generative datasets are generated using backtranslation (Sennrich et al., 2016) and using a fine-tuned GPT-2 model (Radford et al., 2019). We also generate candidates for NarrativeQA and MCScript using a trained MHPG model (Bauer et al., 2018). We tried using MHPG for CosmosQA and SocialIQA but candidates were of poor quality. Unique to NarrativeQA, each question has two references. We treat the second reference as a candidate to be annotated if it has low n-gram overlap with the first reference. We use a span-selection BERT-based model to generate candidates for Quoref and NAQANET (Dua et al., 2019) and NABERT2 models for DROP. Models are trained on the training sets of each constituent dataset and candidates are produced on instances from the validation set (and test set if available). We filtered out candidates that exactly matc"
2020.emnlp-main.528,W17-4755,0,0.0500144,"Missing"
2020.emnlp-main.528,W16-2302,0,0.0600135,"Missing"
2020.emnlp-main.528,S17-2001,0,0.0817922,"Missing"
2020.emnlp-main.528,D19-5817,1,0.872004,"is a challenging task in and of itself, which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6521–6532, c November 16–20, 2020. 2020 Association for Computational Linguistics rent metrics also only consider the reference and are agnostic to the end-task being evaluated. Fig. 1 demonstrates that this is problematic for generative RC because scoring a candidate may require a metric to also consider the passage and the question. Without cheap and reliable evaluation, progress in generative reading comprehension has been extremely slow. To addres"
2020.emnlp-main.528,N19-1300,0,0.196703,"with a MSE loss. yˆi = W hi [CLS] lossi = (yi − yˆi )2 3.2 We pre-train BERT via 3-way classification to predict whether: a1 is the correct answer, a2 is the correct answer, or a1 and a2 are both correct. MultiRC has multiple correct answers per question and we create additional instances where both a1 and a2 are correct by duplicating the correct answer for all three datasets. Pre-Training the Learned Metric Learning the interactions between the input components can be difficult with only human judgement fine-tuning. To overcome this, we pre-train on four multiple-choice QA datasets: BoolQ (Clark et al., 2019a), MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), and MultiRC (Khashabi Experiments Training LERC: We use the PyTorch (Paszke et al., 2019), HuggingFace Transformers (Wolf et al., 2019), and AllenNLP (Gardner et al., 2017) libraries to implement LERC. We pre-train LERC before fine-tuning on MOCHA. We evaluate LERC in two settings, an out-of-dataset (OOD) setting and an all-datasets (AD) setting. In the OOD setting, we train and tune LERC on all datasets in MOCHA except the dataset we are evaluating on. This reflects the use case where we want to apply LERC to evaluate a new datase"
2020.emnlp-main.528,P19-1264,0,0.108636,"with a MSE loss. yˆi = W hi [CLS] lossi = (yi − yˆi )2 3.2 We pre-train BERT via 3-way classification to predict whether: a1 is the correct answer, a2 is the correct answer, or a1 and a2 are both correct. MultiRC has multiple correct answers per question and we create additional instances where both a1 and a2 are correct by duplicating the correct answer for all three datasets. Pre-Training the Learned Metric Learning the interactions between the input components can be difficult with only human judgement fine-tuning. To overcome this, we pre-train on four multiple-choice QA datasets: BoolQ (Clark et al., 2019a), MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), and MultiRC (Khashabi Experiments Training LERC: We use the PyTorch (Paszke et al., 2019), HuggingFace Transformers (Wolf et al., 2019), and AllenNLP (Gardner et al., 2017) libraries to implement LERC. We pre-train LERC before fine-tuning on MOCHA. We evaluate LERC in two settings, an out-of-dataset (OOD) setting and an all-datasets (AD) setting. In the OOD setting, we train and tune LERC on all datasets in MOCHA except the dataset we are evaluating on. This reflects the use case where we want to apply LERC to evaluate a new datase"
2020.emnlp-main.528,D19-1606,1,0.893089,"Missing"
2020.emnlp-main.528,N19-1423,0,0.0477598,"e human judgement scores, s1 and s2 , collected using the same interface in Fig. 2. The minimal pair is created so that c1 has a higher score (i.e. is a better answer) than c2 . Each minimal pair is designed to capture a particular linguistic phenomenon (see Table 3). Using this set of minimal pairs, we can study how often a metric prefers the better candidate. We create 200 minimal pairs (50 for each generative QA dataset), which we use for evaluation separately from the original test set. 3 A Learned Metric We provide details on LERC, our learned metric. LERC is initialized using BERT-base (Devlin et al., 2019) We define as input a tuple consisting of a passage, p, a question, q, a reference answer, a, and a candidate answer, ˆa. The input to BERT is 6525 Metric NarrativeQA Dev Test MCScript Dev Test CosmosQA Dev Test SocialIQA Dev Test DROP Dev Test Quoref Dev Test Avg. r Dev Test BLEU-1 METEOR ROUGE-L BERTScore 0.403 0.605 0.434 0.419 0.472 0.615 0.495 0.534 0.181 0.461 0.224 0.172 0.260 0.502 0.297 0.194 0.660 0.696 0.701 0.803 0.670 0.711 0.701 0.779 0.595 0.644 0.599 0.604 0.549 0.637 0.558 0.584 0.409 0.664 0.480 0.174 0.387 0.568 0.366 0.328 0.674 0.729 0.712 0.207 0.578 0.716 0.604 0.286 0.4"
2020.emnlp-main.528,N19-1246,1,0.915534,"t are diverse in their domains and answer types. This ensures that training and evaluation with MOCHA does not overfit to the characteristics of any constituent dataset. NarrativeQA (Kocisk´y et al., 2017) tests reasoning about events, entities, and their relations on movie scripts and book summaries. MCScript (Ostermann et al., 2018) tests reasoning on stories written for a child-level reader. CosmosQA (Huang et al., 2019) tests commonsense reasoning on blogs describing everyday events. SocialIQA (Sap et al., 2019) tests social reasoning with passages constructed from a knowledge base. DROP (Dua et al., 2019) tests predicate argument structure and numerical reasoning on Wikipedia articles concerning American football games, census results, and history. Quoref (Dasigi et al., 2019) tests coreferential reasoning on Wikipedia articles. NarrativeQA was created as a generative RC dataset. CosmosQA, MCScript, and SocialIQA were created as MC datasets which we re-purpose as generative datasets by using the correct choice as the reference. Our motivation for doing this is that the number of generative QA datasets is quite small, which we attribute to the quality of evaluation metrics. The main focus of th"
2020.emnlp-main.528,2020.emnlp-main.751,0,0.0731735,"Missing"
2020.emnlp-main.528,D19-1107,0,0.0223833,"e (MC) format. Both formats are easy to evaluate, ∗ Work done while at the Allen Institute for AI and the University of Washington. 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha. but in return, have restrictions placed on the questions that can be asked or the answers that can be returned. Furthermore, both formats hinge on distractor spans/choices for learning to be effective. Ensuring high quality distractors is a challenging task in and of itself, which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pag"
2020.emnlp-main.528,D19-1243,0,0.0380112,"Missing"
2020.emnlp-main.528,D17-1215,0,0.0326025,"sing a span-selection or multiple-choice (MC) format. Both formats are easy to evaluate, ∗ Work done while at the Allen Institute for AI and the University of Washington. 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha. but in return, have restrictions placed on the questions that can be asked or the answers that can be returned. Furthermore, both formats hinge on distractor spans/choices for learning to be effective. Ensuring high quality distractors is a challenging task in and of itself, which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Metho"
2020.emnlp-main.528,D17-1082,0,0.144271,"e. Human Judgement: 5 out of 5 LERC: 4.98 out of 5 BLEU-1: 0.07 ROUGE-L: 0.15 METEOR: 0.17 Figure 1: Generative reading comprehension example. Properly scoring the candidate requires access to the passage. Current metrics, such as BLEU, ROUGE and METEOR, are agnostic to the end-task while LERC is trained with the passage and question as input. As a result, LERC assigns a score that better reflects human judgement. Introduction Reading comprehension (RC) has seen significant progress in the last few years, with a number of question answering (QA) datasets being created (Rajpurkar et al., 2016; Lai et al., 2017; Talmor et al., 2018). However, a majority of datasets are presented using a span-selection or multiple-choice (MC) format. Both formats are easy to evaluate, ∗ Work done while at the Allen Institute for AI and the University of Washington. 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha. but in return, have restrictions placed on the questions that can be asked or the answers that can be returned. Furthermore, both formats hinge on distractor spans/choices for learning to be effective. Ensuring high quality distractors is a challenging task in and o"
2020.emnlp-main.528,W04-1013,0,0.109362,"Missing"
2020.emnlp-main.528,W18-6450,0,0.0134631,"to question answering, where the passage and question should be assimilated. The final category consists of metrics learned end-to-end from human judgements (Cui et al., 2018; Sellam et al., 2020). These metrics are flexible in that they can be tuned to the specific evaluation setting but depend on a large corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Shared Task have made it easy to evaluate and create new translation metrics (Popovic, 2015; Ma et al., 2017; Shimanaka et al., 2018). In a similar vein, SummEval is a recently released dataset that evaluates a number of evaluation metrics for summarization (Fabbri et al., 2020). 6 Conclusion We present MOCHA, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics. Usin"
2020.emnlp-main.528,W17-4768,0,0.0165145,"but depend on a large corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Shared Task have made it easy to evaluate and create new translation metrics (Popovic, 2015; Ma et al., 2017; Shimanaka et al., 2018). In a similar vein, SummEval is a recently released dataset that evaluates a number of evaluation metrics for summarization (Fabbri et al., 2020). 6 Conclusion We present MOCHA, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics. Using MOCHA, we train a learned metric, LERC, that outperforms all existing metrics and is much more robust when evaluated on a set of minimal pairs. While we have demonstrated that LERC is a better metric for evaluating generative reading comprehension than any existing metric, considerab"
2020.emnlp-main.528,W19-5302,0,0.040145,"Missing"
2020.emnlp-main.528,W14-3336,0,0.0730042,"Missing"
2020.emnlp-main.528,P19-1416,1,0.819886,"or multiple-choice (MC) format. Both formats are easy to evaluate, ∗ Work done while at the Allen Institute for AI and the University of Washington. 1 The dataset, code, a leaderboard, and a demo are available at https://allennlp.org/mocha. but in return, have restrictions placed on the questions that can be asked or the answers that can be returned. Furthermore, both formats hinge on distractor spans/choices for learning to be effective. Ensuring high quality distractors is a challenging task in and of itself, which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Methods in Natural Lang"
2020.emnlp-main.528,L18-1564,0,0.0490233,"lar a candidate is to a reference using the passage and the question. ing human judgement scores, and creating minimal pairs for evaluation. candidates on two span-based datasets, DROP and Quoref, to test this. 2.1 2.2 Datasets Candidates in MOCHA come from 6 constituent QA datasets that are diverse in their domains and answer types. This ensures that training and evaluation with MOCHA does not overfit to the characteristics of any constituent dataset. NarrativeQA (Kocisk´y et al., 2017) tests reasoning about events, entities, and their relations on movie scripts and book summaries. MCScript (Ostermann et al., 2018) tests reasoning on stories written for a child-level reader. CosmosQA (Huang et al., 2019) tests commonsense reasoning on blogs describing everyday events. SocialIQA (Sap et al., 2019) tests social reasoning with passages constructed from a knowledge base. DROP (Dua et al., 2019) tests predicate argument structure and numerical reasoning on Wikipedia articles concerning American football games, census results, and history. Quoref (Dasigi et al., 2019) tests coreferential reasoning on Wikipedia articles. NarrativeQA was created as a generative RC dataset. CosmosQA, MCScript, and SocialIQA were"
2020.emnlp-main.528,2001.mtsummit-papers.68,0,0.015855,"Missing"
2020.emnlp-main.528,W15-3049,0,0.0113584,"uation setting but depend on a large corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Shared Task have made it easy to evaluate and create new translation metrics (Popovic, 2015; Ma et al., 2017; Shimanaka et al., 2018). In a similar vein, SummEval is a recently released dataset that evaluates a number of evaluation metrics for summarization (Fabbri et al., 2020). 6 Conclusion We present MOCHA, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics. Using MOCHA, we train a learned metric, LERC, that outperforms all existing metrics and is much more robust when evaluated on a set of minimal pairs. While we have demonstrated that LERC is a better metric for evaluating generative reading comprehension than any existing m"
2020.emnlp-main.528,N18-1023,0,0.0446668,"Missing"
2020.emnlp-main.528,D13-1020,0,0.0444038,"CLS] lossi = (yi − yˆi )2 3.2 We pre-train BERT via 3-way classification to predict whether: a1 is the correct answer, a2 is the correct answer, or a1 and a2 are both correct. MultiRC has multiple correct answers per question and we create additional instances where both a1 and a2 are correct by duplicating the correct answer for all three datasets. Pre-Training the Learned Metric Learning the interactions between the input components can be difficult with only human judgement fine-tuning. To overcome this, we pre-train on four multiple-choice QA datasets: BoolQ (Clark et al., 2019a), MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), and MultiRC (Khashabi Experiments Training LERC: We use the PyTorch (Paszke et al., 2019), HuggingFace Transformers (Wolf et al., 2019), and AllenNLP (Gardner et al., 2017) libraries to implement LERC. We pre-train LERC before fine-tuning on MOCHA. We evaluate LERC in two settings, an out-of-dataset (OOD) setting and an all-datasets (AD) setting. In the OOD setting, we train and tune LERC on all datasets in MOCHA except the dataset we are evaluating on. This reflects the use case where we want to apply LERC to evaluate a new dataset where we do not have human judgeme"
2020.emnlp-main.528,D19-1454,0,0.0562355,"Missing"
2020.emnlp-main.528,2020.acl-main.704,0,0.0246621,"of metrics that use some variant of n-gram matching (Papineni et al., 2001; Lin, 2004; Banerjee and Lavie, 2005). They are easy to implement, but lack flexibility by focusing only on token overlap. The second cateogry of metrics eschew some of the aforementioned issues by calculating a softer similarity score using embeddings of tokens (Clark et al., 2019b; Zhang et al., 2019). However, it is unclear how to tailor them to question answering, where the passage and question should be assimilated. The final category consists of metrics learned end-to-end from human judgements (Cui et al., 2018; Sellam et al., 2020). These metrics are flexible in that they can be tuned to the specific evaluation setting but depend on a large corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Sha"
2020.emnlp-main.528,P16-1009,0,0.0497328,"aset. CosmosQA, MCScript, and SocialIQA were created as MC datasets which we re-purpose as generative datasets by using the correct choice as the reference. Our motivation for doing this is that the number of generative QA datasets is quite small, which we attribute to the quality of evaluation metrics. The main focus of this work is in developing and evaluating metrics for generative RC. However, we wanted to see whether a learned metric could do well on span-selection datasets. We collected Collecting Candidates Candidates on all four generative datasets are generated using backtranslation (Sennrich et al., 2016) and using a fine-tuned GPT-2 model (Radford et al., 2019). We also generate candidates for NarrativeQA and MCScript using a trained MHPG model (Bauer et al., 2018). We tried using MHPG for CosmosQA and SocialIQA but candidates were of poor quality. Unique to NarrativeQA, each question has two references. We treat the second reference as a candidate to be annotated if it has low n-gram overlap with the first reference. We use a span-selection BERT-based model to generate candidates for Quoref and NAQANET (Dua et al., 2019) and NABERT2 models for DROP. Models are trained on the training sets of"
2020.emnlp-main.528,W18-6456,0,0.0135806,"arge corpus of human judgement scores to train on. We hope that the release of MOCHA pushes the development of QA metrics that fall into this category. MOCHA is directly inspired by the annual WMT Metrics Shared Task (Mach´acek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Participants submit automatic translations and human judgement scores are collected for the submitted translations. The annotations collected as part of the WMT Metrics Shared Task have made it easy to evaluate and create new translation metrics (Popovic, 2015; Ma et al., 2017; Shimanaka et al., 2018). In a similar vein, SummEval is a recently released dataset that evaluates a number of evaluation metrics for summarization (Fabbri et al., 2020). 6 Conclusion We present MOCHA, a dataset of human judgement scores for training and evaluating generative reading comprehension metrics. Using MOCHA, we train a learned metric, LERC, that outperforms all existing metrics and is much more robust when evaluated on a set of minimal pairs. While we have demonstrated that LERC is a better metric for evaluating generative reading comprehension than any existing metric, considerable work remains. Error an"
2020.emnlp-main.528,W15-3031,0,0.065281,"Missing"
2020.emnlp-main.528,2020.acl-main.450,0,0.0233687,"which can lead to models that exploit spurious correlations (Jia and Liang, 2017; Min et al., 2019; Geva et al., 2019). Posing RC as a generation task addresses the aforementioned issues. Generative RC does not require distractors, circumventing biases that could be introduced by them, and allows arbitrary questions and answers. Unfortunately, existing metrics for evaluating text generation come with significant shortcomings. Many metrics score n-gram overlap, and it is well established that using token overlap as a measure of similarity has drawbacks (Chen et al., 2019; Edunov et al., 2019; Wang et al., 2020). Cur6521 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6521–6532, c November 16–20, 2020. 2020 Association for Computational Linguistics rent metrics also only consider the reference and are agnostic to the end-task being evaluated. Fig. 1 demonstrates that this is problematic for generative RC because scoring a candidate may require a metric to also consider the passage and the question. Without cheap and reliable evaluation, progress in generative reading comprehension has been extremely slow. To address the need for better evaluation metrics"
2020.emnlp-tutorials.3,N18-2017,0,0.0213835,"w.ericswallace.com/interpretability. 1 Sameer Singh UC Irvine sameer@uci.edu Tutorial Description Neural models have become the de-facto standard tool for NLP tasks. These models are becoming increasingly powerful—recent work shows that large neural models substantially improve accuracy on a wide range of downstream tasks (Devlin et al., 2019; Brown et al., 2020). However, today’s models still make egregious errors: they reinforce racial biases (Sap et al., 2019), fail in counterintuitive ways (Jia and Liang, 2017; Feng et al., 2018), and often solve tasks using simple surface-level patterns (Gururangan et al., 2018; Min et al., 2019). These model insufficiencies are exacerbated by the inability to understand why models made the predictions they do. Interpretation methods seek to fill this void. In particular, example-specific interpretations provide post-hoc explanations for indi2 Details and Prerequisites The tutorial will be of the cutting-edge type. The tutorial slides and the accompanying code is available online at https://www.ericswallace. com/interpretability. Prerequisites Attendees should have a basic understanding of different tasks in NLP such as text classification, sequence tagging, and rea"
2020.emnlp-tutorials.3,N19-1357,0,0.0573089,"Missing"
2020.emnlp-tutorials.3,D17-1215,0,0.460906,"ovide an introduction to the various types of example-specific interpretations. We will present the technical details of existing methods, including saliency maps, adversarial attacks, input perturbations, influence functions, and other methods. We will cover how these interpretations are applied to various tasks and input-output formats, e.g., text classification using LSTMs, masked language modeling using BERT (Devlin et al., 2019), and text generation using GPT-2 (Radford et al., 2019). For each task, we will walk through example use cases of interpretations: highlighting model weaknesses (Jia and Liang, 2017), increasing/decreasing user trust (Feng et al., 2018), and understanding hard-to-formalize criteria such as bias, safety, and fairness (Doshi-Velez and Kim, 2017). Alongside the tutorial, we will present source code implementations of various interpretation methods using AllenNLP Interpret (Wallace et al., 2019b). Although neural NLP models are highly expressive and empirically successful, they also systematically fail in counterintuitive ways and are opaque in their decision-making process. This tutorial will provide a background on interpretation techniques, i.e., methods for explaining the"
2020.emnlp-tutorials.3,N19-1112,1,0.806739,"l discuss why we care about interpretability. It will paint a landscape of today’s neural models, describe how models are brittle and behave counterintuitively, and explain how interpretations can open the “black box” of machine learning. Introduction to Interpretations This section will situate example-specific interpretations in the context of other methods. We will discuss: • Dataset analyses, e.g., error analysis, Errudite (Wu et al., 2019), diagnostic “challenge” test sets (Naik et al., 2018; Gardner et al., 2020) • “Probing”, i.e., inspecting a model’s embeddings for certain properties (Liu et al., 2019; Tenney et al., 2019). • Rationale-based explanations, i.e., a model generates text for why it made its prediction. • Example-specific interpretations (our tutorial’s focus), e.g., saliency maps (Simonyan et al., 2014), LIME (Ribeiro et al., 2016), adversarUnderstanding Which Training Examples Caused a Prediction This section will discuss how to trace model predictions back to the training data, i.e., identifying “influential” training points. We will cover influence functions (Koh and Liang, 2017) and representor points (Yeh et al., 2018). Coding Interpretations This section will walk throug"
2020.emnlp-tutorials.3,P19-1334,0,0.0220508,"re now standard interpretations. Wallace et al. (2019b) provides example NLP interpretations (interested readers can inspect their code). 3 Tutorial Outline The tutorial will present three hours of content with a thirty-minute break. Break Understanding How Global Decision Rules Led to a Prediction This section will discuss how certain global “decision rules” can explain model predictions. We will cover Anchors (Ribeiro et al., 2018a) and Universal Adversarial Triggers (Wallace et al., 2019a). We will also discuss how spurious patterns in datasets, e.g., lexical overlap in textual entailment (McCoy et al., 2019), can cause models to learn certain undesirable decision rules. Motivation This section will discuss why we care about interpretability. It will paint a landscape of today’s neural models, describe how models are brittle and behave counterintuitively, and explain how interpretations can open the “black box” of machine learning. Introduction to Interpretations This section will situate example-specific interpretations in the context of other methods. We will discuss: • Dataset analyses, e.g., error analysis, Errudite (Wu et al., 2019), diagnostic “challenge” test sets (Naik et al., 2018; Gardne"
2020.emnlp-tutorials.3,D19-1221,1,0.922771,"s and input-output formats, e.g., text classification using LSTMs, masked language modeling using BERT (Devlin et al., 2019), and text generation using GPT-2 (Radford et al., 2019). For each task, we will walk through example use cases of interpretations: highlighting model weaknesses (Jia and Liang, 2017), increasing/decreasing user trust (Feng et al., 2018), and understanding hard-to-formalize criteria such as bias, safety, and fairness (Doshi-Velez and Kim, 2017). Alongside the tutorial, we will present source code implementations of various interpretation methods using AllenNLP Interpret (Wallace et al., 2019b). Although neural NLP models are highly expressive and empirically successful, they also systematically fail in counterintuitive ways and are opaque in their decision-making process. This tutorial will provide a background on interpretation techniques, i.e., methods for explaining the predictions of NLP models. We will first situate example-specific interpretations in the context of other ways to understand models (e.g., probing, dataset analyses). Next, we will present a thorough study of example-specific interpretations, including saliency maps, input perturbations (e.g., LIME, input reduc"
2020.emnlp-tutorials.3,P19-1416,1,0.816158,"retability. 1 Sameer Singh UC Irvine sameer@uci.edu Tutorial Description Neural models have become the de-facto standard tool for NLP tasks. These models are becoming increasingly powerful—recent work shows that large neural models substantially improve accuracy on a wide range of downstream tasks (Devlin et al., 2019; Brown et al., 2020). However, today’s models still make egregious errors: they reinforce racial biases (Sap et al., 2019), fail in counterintuitive ways (Jia and Liang, 2017; Feng et al., 2018), and often solve tasks using simple surface-level patterns (Gururangan et al., 2018; Min et al., 2019). These model insufficiencies are exacerbated by the inability to understand why models made the predictions they do. Interpretation methods seek to fill this void. In particular, example-specific interpretations provide post-hoc explanations for indi2 Details and Prerequisites The tutorial will be of the cutting-edge type. The tutorial slides and the accompanying code is available online at https://www.ericswallace. com/interpretability. Prerequisites Attendees should have a basic understanding of different tasks in NLP such as text classification, sequence tagging, and reading comprehension"
2020.emnlp-tutorials.3,D19-3002,1,0.839545,"s and input-output formats, e.g., text classification using LSTMs, masked language modeling using BERT (Devlin et al., 2019), and text generation using GPT-2 (Radford et al., 2019). For each task, we will walk through example use cases of interpretations: highlighting model weaknesses (Jia and Liang, 2017), increasing/decreasing user trust (Feng et al., 2018), and understanding hard-to-formalize criteria such as bias, safety, and fairness (Doshi-Velez and Kim, 2017). Alongside the tutorial, we will present source code implementations of various interpretation methods using AllenNLP Interpret (Wallace et al., 2019b). Although neural NLP models are highly expressive and empirically successful, they also systematically fail in counterintuitive ways and are opaque in their decision-making process. This tutorial will provide a background on interpretation techniques, i.e., methods for explaining the predictions of NLP models. We will first situate example-specific interpretations in the context of other ways to understand models (e.g., probing, dataset analyses). Next, we will present a thorough study of example-specific interpretations, including saliency maps, input perturbations (e.g., LIME, input reduc"
2020.emnlp-tutorials.3,C18-1198,0,0.016511,"ment (McCoy et al., 2019), can cause models to learn certain undesirable decision rules. Motivation This section will discuss why we care about interpretability. It will paint a landscape of today’s neural models, describe how models are brittle and behave counterintuitively, and explain how interpretations can open the “black box” of machine learning. Introduction to Interpretations This section will situate example-specific interpretations in the context of other methods. We will discuss: • Dataset analyses, e.g., error analysis, Errudite (Wu et al., 2019), diagnostic “challenge” test sets (Naik et al., 2018; Gardner et al., 2020) • “Probing”, i.e., inspecting a model’s embeddings for certain properties (Liu et al., 2019; Tenney et al., 2019). • Rationale-based explanations, i.e., a model generates text for why it made its prediction. • Example-specific interpretations (our tutorial’s focus), e.g., saliency maps (Simonyan et al., 2014), LIME (Ribeiro et al., 2016), adversarUnderstanding Which Training Examples Caused a Prediction This section will discuss how to trace model predictions back to the training data, i.e., identifying “influential” training points. We will cover influence functions (K"
2020.emnlp-tutorials.3,N19-5001,1,0.830443,"ions focus on classification models; how are interpretations best applied to the complex input-output formats seen in NLP tasks (e.g., machine translation)? • Closing the loop with Humans: Humans are the end-users of interpretations; how can we make interpretations interactive, collaborative, customizable, and ultimately more effective? • Pretrained Transformer Models: How do our methods, and the field of interpretability, change with the rise of massively-pretrained models? his PhD from the University of Massachusetts, Amherst in 2014. Sameer presented the Deep Adversarial Learning Tutorial (Wang et al., 2019) at NAACL 2019 and the Mining Knowledge Graphs from Text Tutorial at WSDM 2018 and AAAI 2017. Sameer has also received teaching awards at UCI. Website: http: //sameersingh.org/ 4 Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018. On adversarial examples for character-level neural machine translation. In COLING. References Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Jacob Devlin, Ming-Wei Chang, Kenton"
2020.emnlp-tutorials.3,N18-1202,1,0.509455,"understanding of neural network methods for NLP, including: 20 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 20–23 c Online, November 19 - 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 • How backpropagation can compute gradients with respect to the parameters. • How tokens/words are represented (i.e., word and sub-word embeddings). • High-level ideas behind different model architectures (e.g., RNNs, Transformers). • Optional knowledge of contextualized embedding models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Finally, a portion of the tutorial will walk through Python code samples in PyTorch and AllenNLP (Gardner et al., 2018b). Participants do not need to understand this code to follow the main tutorial material. ial attacks (Szegedy et al., 2014), and input perturbations (Feng et al., 2018). Example-specific Interpretations This section will introduce example-specific interpretations in more detail. We will discuss the challenges and approaches to evaluating such interpretations. We will also cover the critiques and shortcomings of using attention as explanations"
2020.emnlp-tutorials.3,P19-1073,0,0.016237,"in datasets, e.g., lexical overlap in textual entailment (McCoy et al., 2019), can cause models to learn certain undesirable decision rules. Motivation This section will discuss why we care about interpretability. It will paint a landscape of today’s neural models, describe how models are brittle and behave counterintuitively, and explain how interpretations can open the “black box” of machine learning. Introduction to Interpretations This section will situate example-specific interpretations in the context of other methods. We will discuss: • Dataset analyses, e.g., error analysis, Errudite (Wu et al., 2019), diagnostic “challenge” test sets (Naik et al., 2018; Gardner et al., 2020) • “Probing”, i.e., inspecting a model’s embeddings for certain properties (Liu et al., 2019; Tenney et al., 2019). • Rationale-based explanations, i.e., a model generates text for why it made its prediction. • Example-specific interpretations (our tutorial’s focus), e.g., saliency maps (Simonyan et al., 2014), LIME (Ribeiro et al., 2016), adversarUnderstanding Which Training Examples Caused a Prediction This section will discuss how to trace model predictions back to the training data, i.e., identifying “influential”"
2020.emnlp-tutorials.3,N16-3020,1,0.544481,"critiques and shortcomings of using attention as explanations (Jain and Wallace, 2019; Serrano and Smith, 2019). We will then explain why we focus on gradient-based methods: they are model-agnostic, easy to compute, and (largely) faithful to a model’s behavior. Understanding What Parts of An Input Led to a Prediction This section will discuss: • Saliency maps, i.e., generating visualizations of “salient” input tokens. We will discuss how to generate saliency maps using gradient-based techniques (Simonyan et al., 2014; Sundararajan et al., 2017; Smilkov et al., 2017)) and black-box techniques (Ribeiro et al., 2016). • Input Perturbations, i.e., showing how changes to the input do (or do not) change the prediction. For example, leave-one-out (Li et al., 2016) and input reduction (Feng et al., 2018). We will also cover adversarial perturbations such as token flipping (Ebrahimi et al., 2018) and adding distractor sentences (Jia and Liang, 2017). Reading List Doshi-Velez and Kim (2017) provide a great overview and motivation for interpretability research. Lipton (2018) and Jain and Wallace (2019) discuss some of the challenges of defining and evaluating interpretability. Jia and Liang (2017) help demonstrat"
2020.emnlp-tutorials.3,P18-1079,1,0.827998,"of defining and evaluating interpretability. Jia and Liang (2017) help demonstrate the fragility of NLP models. LIME (Ribeiro et al., 2016) and saliency maps (Simonyan et al., 2014) are now standard interpretations. Wallace et al. (2019b) provides example NLP interpretations (interested readers can inspect their code). 3 Tutorial Outline The tutorial will present three hours of content with a thirty-minute break. Break Understanding How Global Decision Rules Led to a Prediction This section will discuss how certain global “decision rules” can explain model predictions. We will cover Anchors (Ribeiro et al., 2018a) and Universal Adversarial Triggers (Wallace et al., 2019a). We will also discuss how spurious patterns in datasets, e.g., lexical overlap in textual entailment (McCoy et al., 2019), can cause models to learn certain undesirable decision rules. Motivation This section will discuss why we care about interpretability. It will paint a landscape of today’s neural models, describe how models are brittle and behave counterintuitively, and explain how interpretations can open the “black box” of machine learning. Introduction to Interpretations This section will situate example-specific interpretati"
2020.emnlp-tutorials.3,P19-1163,0,0.0165641,"d, e.g., evaluating, extending, and improving interpretation methods. The tutorial slides and the accompanying code is available online at https: //www.ericswallace.com/interpretability. 1 Sameer Singh UC Irvine sameer@uci.edu Tutorial Description Neural models have become the de-facto standard tool for NLP tasks. These models are becoming increasingly powerful—recent work shows that large neural models substantially improve accuracy on a wide range of downstream tasks (Devlin et al., 2019; Brown et al., 2020). However, today’s models still make egregious errors: they reinforce racial biases (Sap et al., 2019), fail in counterintuitive ways (Jia and Liang, 2017; Feng et al., 2018), and often solve tasks using simple surface-level patterns (Gururangan et al., 2018; Min et al., 2019). These model insufficiencies are exacerbated by the inability to understand why models made the predictions they do. Interpretation methods seek to fill this void. In particular, example-specific interpretations provide post-hoc explanations for indi2 Details and Prerequisites The tutorial will be of the cutting-edge type. The tutorial slides and the accompanying code is available online at https://www.ericswallace. com/"
2020.emnlp-tutorials.3,P19-1282,0,0.0191242,"2019). Finally, a portion of the tutorial will walk through Python code samples in PyTorch and AllenNLP (Gardner et al., 2018b). Participants do not need to understand this code to follow the main tutorial material. ial attacks (Szegedy et al., 2014), and input perturbations (Feng et al., 2018). Example-specific Interpretations This section will introduce example-specific interpretations in more detail. We will discuss the challenges and approaches to evaluating such interpretations. We will also cover the critiques and shortcomings of using attention as explanations (Jain and Wallace, 2019; Serrano and Smith, 2019). We will then explain why we focus on gradient-based methods: they are model-agnostic, easy to compute, and (largely) faithful to a model’s behavior. Understanding What Parts of An Input Led to a Prediction This section will discuss: • Saliency maps, i.e., generating visualizations of “salient” input tokens. We will discuss how to generate saliency maps using gradient-based techniques (Simonyan et al., 2014; Sundararajan et al., 2017; Smilkov et al., 2017)) and black-box techniques (Ribeiro et al., 2016). • Input Perturbations, i.e., showing how changes to the input do (or do not) change the"
2020.findings-emnlp.117,W07-2441,0,0.0137691,"1. UD Parsing Finally, we discuss dependency parsing in the universal dependencies (UD) formalism (Nivre et al., 2016). We look at dependency parsing to show that contrast sets apply not only to modern “high-level” NLP tasks but also to longstanding linguistic analysis tasks. We first chose a specific type of attachment ambiguity to target: the classic problem of prepositional phrase (PP) attachment (Collins and Brooks, 1995), e.g. We ate spaghetti with forks versus We ate spaghetti with meatballs. We use a subset of the English UD treebanks: GUM (Zeldes, 2017), the English portion of LinES (Ahrenberg, 2007), the English portion of ParTUT (Sanguinetti and Bosco, 2015), and the dependency-annotated English Web Treebank (Silveira et al., 2014). We searched these treebanks for sentences that include a potentially structurally ambiguous attachment from the head of a PP to either a noun or a verb. We then perturbed these sentences by altering one of their noun phrases such that the semantics of the perturbed sentence required a different attachment for the PP. We then re-annotated these perturbed sentences to indicate the new attachment(s). Summary While the overall process we recommend for constructi"
2020.findings-emnlp.117,D15-1075,0,0.0445289,"ow frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be exploited by models to achieve artificially high test accuracy. This is but one kind of systematic gap; there are also biases due to the writing styles of small groups of annotators (Geva et al., 2019), the distribut"
2020.findings-emnlp.117,W18-6433,0,0.0341655,"Missing"
2020.findings-emnlp.117,D19-1606,1,0.924571,"round a pivot (e.g., Ribeiro et al., 2018b, 2019). However, it is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates"
2020.findings-emnlp.117,N19-1423,0,0.0110664,"st sets in Table 1. For most datasets, the average time to perturb each example was 1–3 minutes, which translates to approximately 17–50 hours of work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model architecture, contrast set de"
2020.findings-emnlp.117,N19-1246,1,0.944307,"arts of the gaps around a pivot (e.g., Ribeiro et al., 2018b, 2019). However, it is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while pe"
2020.findings-emnlp.117,D18-1407,1,0.898157,"uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011)"
2020.findings-emnlp.117,D19-1107,1,0.925439,"l language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps"
2020.findings-emnlp.117,P18-2103,0,0.0419316,"ded minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed inst"
2020.findings-emnlp.117,N18-2017,1,0.922108,"uction Progress in natural language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in th"
2020.findings-emnlp.117,D19-1170,0,0.0120239,"work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model architecture, contrast set design, etc. On IMDb and PERSPECTRUM, the model achieves a reasonably high consistency, suggesting that, while there is definitely still room fo"
2020.findings-emnlp.117,D17-1263,0,0.0339764,"h to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring lo"
2020.findings-emnlp.117,D17-1215,0,0.560991,"rks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Lev"
2020.findings-emnlp.117,D19-1423,0,0.0613359,"Missing"
2020.findings-emnlp.117,2020.emnlp-main.12,1,0.714022,"ple and intuitive geometric interpretation of “bias” in dataset collection, and showing that this long-standing idea of minimal data changes can be effectively used to solve this problem on a wide variety of NLP tasks. We additionally generalize the idea of a minimal pair to a set, and use a consistency metric, which we contend more closely aligns with what NLP researchers mean by “language understanding”. Training on Perturbed Examples Many previous works have provided minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits simil"
2020.findings-emnlp.117,W17-5401,0,0.118381,"Missing"
2020.findings-emnlp.117,P19-1554,1,0.81477,"ctive Power Contrast sets only have negative predictive power: they reveal if a model does not align with the correct local decision boundary but cannot confirm that a model does align with it. This is because annotators cannot exhaustively label all inputs near a pivot and thus a contrast set will necessarily be incomplete. However, note that this problem is not unique to contrast sets—similar issues hold for the original test set as well as adversarial test sets (Jia and Liang, 2017), challenge sets (Naik et al., 2018), and input perturbations (Ribeiro et al., 2018a; Feng et al., 2018). See Feng et al. (2019) for a detailed discussion of how dataset analysis methods only have negative predictive power. Dataset-Specific Instantiations The process for creating contrast sets is dataset-specific: although we present general guidelines that hold across many tasks, experts must still characterize the type of phenomena each individual dataset is intended to capture. Fortunately, the original dataset authors should already have thought deeply about such phenomena. Hence, creating contrast sets should be well-defined and relatively straightforward. 3 How to Create Contrast Sets Here, we walk through our pr"
2020.findings-emnlp.117,D19-5808,1,0.8991,"n the contrast sets (not including the original example). We report percentage accuracy for NLVR2, IMDb, PERSPECTRUM, MATRES, and BoolQ; F1 scores for DROP and Q UOREF; Exact Match (EM) scores for ROPES and MC-TACO; and unlabeled attachment score on modified attachments for the UD English dataset. We also report contrast consistency: the percentage of the “# Sets” contrast sets for which a model’s predictions are correct for all examples in the set (including the original example). More details on datasets, models, and metrics can be found in §A and §B. • Quoref (Dasigi et al., 2019) • ROPES (Lin et al., 2019) • BoolQ (Clark et al., 2019) • MC-TACO (Zhou et al., 2019) We choose these datasets because they span a variety of tasks (e.g., reading comprehension, sentiment analysis, visual reasoning) and input-output formats (e.g., classification, span extraction, structured prediction). We include high-level tasks for which dataset artifacts are known to be prevalent, as well as longstanding formalism-based tasks, where data artifacts have been less of an issue (or at least have been less well-studied). 4.2 Contrast Set Construction The contrast sets were constructed by NLP researchers who were deeply"
2020.findings-emnlp.117,2021.ccl-1.108,0,0.144976,"Missing"
2020.findings-emnlp.117,P11-1015,0,0.07085,"ena they are most interested in studying and craft their contrast sets to explicitly test those phenomena. Care should be taken during contrast set construction to ensure that the phenomena present in contrast sets are similar to those present in the original test set; the purpose of a contrast set is not to introduce new challenges, but to more thoroughly evaluate the original intent of the test set. 4 4.1 Datasets and Experiments Original Datasets We create contrast sets for 10 NLP datasets (full descriptions are provided in Section A): • NLVR2 (Suhr et al., 2019) • IMDb sentiment analysis (Maas et al., 2011) • MATRES Temporal RE (Ning et al., 2018) • English UD parsing (Nivre et al., 2016) • PERSPECTRUM (Chen et al., 2019) • DROP (Dua et al., 2019) 1312 Dataset # Examples # Sets Model Original Test Contrast Consistency NLVR2 994 479 LXMERT 76.4 61.1 (–15.3) 30.1 IMDb 488 488 BERT 93.8 84.2 (–9.6) 77.8 MATRES 401 239 CogCompTime2.0 73.2 63.3 (–9.9) 40.6 UD English 150 150 Biaffine + ELMo 64.7 46.0 (–18.7) 17.3 PERSPECTRUM 217 217 RoBERTa 90.3 85.7 (–4.6) 78.8 DROP 947 623 MTMSN 79.9 54.2 (–25.7) 39.0 QUOREF 700 415 XLNet-QA 70.5 55.4 (–15.1) 29.9 ROPES 974 974 RoBERTa 47.7 32.5 (–15.2) 17.6 BoolQ"
2020.findings-emnlp.117,J93-2004,0,0.068933,"ation: Two similarly-colored and similarly-posed chow dogs are face to face in one image. Figure 1: An example contrast set for NLVR2 (Suhr and Artzi, 2019). The label for the original example is T RUE and the label for all of the perturbed examples is FALSE. The contrast set allows probing of a model’s decision boundary local to examples in the test set, which better evaluates whether the model has captured the relevant phenomena than standard metrics on i.i.d. test data. Introduction Progress in natural language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input"
2020.findings-emnlp.117,D18-1151,0,0.0148944,"1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring local alignment of decision boundaries, along with our new, related resources. Additionally, rather than creating new data from scratch, contrast sets augment existing test examples to fill in systematic gaps. Thus contrast sets often require less effort to create, and they remain grounded i"
2020.findings-emnlp.117,N19-1314,0,0.0179195,"the task definition than a random selection of input / output pairs. 2.3 Contrast sets in practice Given these definitions, we now turn to the actual construction of contrast sets in practical NLP settings. There were two things left unspecified in the definitions above: the distance function d to use in discrete input spaces, and the method for sampling from a local decision boundary. While there has been some work trying to formally characterize dis2 In this discussion we are talking about the true decision boundary, not a model’s decision boundary. tances for adversarial robustness in NLP (Michel et al., 2019; Jia et al., 2019), we find it more useful in our setting to simply rely on expert judgments to generate a similar but meaningfully different x0 given x, addressing both the distance function and the sampling method. Future work could try to give formal treatments of these issues, but we believe expert judgments are sufficient to make initial progress in improving our evaluation methodologies. And while expertcrafted contrast sets can only give us an upper bound on a model’s local alignment with the true decision boundary, an upper bound on local alignment is often more informative than a pot"
2020.findings-emnlp.117,P19-1416,1,0.832263,"dissolute alcoholic. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Likely Context: She renews in Ranchipur an acquaintance with a former lover, Tom Ransome, who keeps very healthy habits. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap"
2020.findings-emnlp.117,C18-1198,0,0.264897,"nt work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011). We propose that dataset authors manually perturb instances fro"
2020.findings-emnlp.117,D19-1642,1,0.874527,"Missing"
2020.findings-emnlp.117,P18-1122,1,0.846317,"and craft their contrast sets to explicitly test those phenomena. Care should be taken during contrast set construction to ensure that the phenomena present in contrast sets are similar to those present in the original test set; the purpose of a contrast set is not to introduce new challenges, but to more thoroughly evaluate the original intent of the test set. 4 4.1 Datasets and Experiments Original Datasets We create contrast sets for 10 NLP datasets (full descriptions are provided in Section A): • NLVR2 (Suhr et al., 2019) • IMDb sentiment analysis (Maas et al., 2011) • MATRES Temporal RE (Ning et al., 2018) • English UD parsing (Nivre et al., 2016) • PERSPECTRUM (Chen et al., 2019) • DROP (Dua et al., 2019) 1312 Dataset # Examples # Sets Model Original Test Contrast Consistency NLVR2 994 479 LXMERT 76.4 61.1 (–15.3) 30.1 IMDb 488 488 BERT 93.8 84.2 (–9.6) 77.8 MATRES 401 239 CogCompTime2.0 73.2 63.3 (–9.9) 40.6 UD English 150 150 Biaffine + ELMo 64.7 46.0 (–18.7) 17.3 PERSPECTRUM 217 217 RoBERTa 90.3 85.7 (–4.6) 78.8 DROP 947 623 MTMSN 79.9 54.2 (–25.7) 39.0 QUOREF 700 415 XLNet-QA 70.5 55.4 (–15.1) 29.9 ROPES 974 974 RoBERTa 47.7 32.5 (–15.2) 17.6 BoolQ 339 70 RoBERTa 86.1 71.1 (–15.0) 59.0 MC-"
2020.findings-emnlp.117,P19-1459,0,0.0145999,"uestion: How frequently does Tom drink? Candidate Answer: Every other night Label: Likely Context: She renews in Ranchipur an acquaintance with a former lover, Tom Ransome, who keeps very healthy habits. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be e"
2020.findings-emnlp.117,N18-1202,1,0.523856,"es from the different contrast sets in Table 1. For most datasets, the average time to perturb each example was 1–3 minutes, which translates to approximately 17–50 hours of work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model a"
2020.findings-emnlp.117,S18-2023,0,0.0681783,"Missing"
2020.findings-emnlp.117,P19-1621,1,0.860914,"Missing"
2020.findings-emnlp.117,P18-1079,1,0.679043,"of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011). We propose that data"
2020.findings-emnlp.117,N18-2002,0,0.0515743,"Missing"
2020.findings-emnlp.117,E17-2060,0,0.0150328,"ds have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring local alignment of decision boundaries, along with our new, related resources. Additionally, rather than creating new data from scratch, contrast sets augment existing test examples to fill in system"
2020.findings-emnlp.117,2020.acl-main.468,0,0.0236124,"ing set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be exploited by models to achieve artificially high test accuracy. This is but one kind of systematic gap; there are also biases due to the writing styles of small groups of annotators (Geva et al., 2019), the distributional biases in the data that was chosen for annotation, as well as numerous other biases that are more subtle and harder to discern (Shah et al., 2020). Completely removing these gaps in the initial data collection process would be ideal, but is likely impossible—language has too much inherent variability in a very high-dimensional space. Instead, we use contrast sets to fill in gaps in the test data to give more thorough evaluations than what the original data provides. 1309 2.2 Definitions We begin by defining a decision boundary as a partition of some space into labels.2 This partition can be represented by the set of all points in the space with their associated labels: {(x, y)}. This definition differs somewhat from the canonical defini"
2020.findings-emnlp.117,silveira-etal-2014-gold,0,0.0413118,"Missing"
2020.findings-emnlp.117,P19-1644,1,0.918073,"hanging questions asking for counts to questions asking for sets (How many countries. . . to Which countries. . . ). Finally, we changed the ordering of events. A large number of questions about war paragraphs ask which of two events happened first. We changed (1) the order the events were asked about in the question, (2) the order that the events showed up in the passage, and (3) the dates associated with each event to swap their temporal order. NLVR2 We next consider NLVR2, a dataset where a model is given a sentence about two provided images and must determine whether the sentence is true (Suhr et al., 2019). The data collection process encouraged highly compositional language, which was intended to require understanding the relationships between objects, properties of objects, and counting. We constructed NLVR2 contrast sets by modifying the sentence or replacing one of the images with freely-licensed images from web searches. For example, we might change The left image contains twice the number of dogs as the right image to The left image contains three times the number of dogs as the right image. Similarly, given an image pair with four dogs in the left and two dogs in the right, we can replac"
2020.findings-emnlp.117,D19-1608,1,0.822267,"esources that we have created, is giving a simple and intuitive geometric interpretation of “bias” in dataset collection, and showing that this long-standing idea of minimal data changes can be effectively used to solve this problem on a wide variety of NLP tasks. We additionally generalize the idea of a minimal pair to a set, and use a consistency metric, which we contend more closely aligns with what NLP researchers mean by “language understanding”. Training on Perturbed Examples Many previous works have provided minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various pheno"
2020.findings-emnlp.117,D18-1009,0,0.0260643,"is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates subtle, undesirable biases towards the idiosyncrasies of that"
2020.findings-emnlp.117,P19-1472,0,0.0134417,"annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates subtle, undesirable biases towards the idiosyncrasies of that model. 2.5 Limitations of Contrast Sets Solely Negative Predictive Power Contrast sets only have negative predictive power: they reveal if a model does not align with the correct local decision boundary but cannot"
2020.findings-emnlp.24,D15-1075,0,0.0130983,"d using Equation 5, and g is the FACADE model by itself. 4 (3) Layer Normalization: We merge layer normalization layers (Ba et al., 2016) by splitting the input into two parts according to the hidden dimensions of forig and g. We then apply layer normalization to each part independently. SST-2 Datasets To demonstrate the wide applicability of our method, we use four datasets that span different tasks and input-output formats. Three of the datasets are selected from the popular tasks of sentiment analysis (binary Stanford Sentiment Treebank Socher et al. 2013), natural language inference (SNLI Bowman et al. 2015), and question answering (SQuAD Rajpurkar et al. 2016). We select sentiment analysis and question answering because they are widely used in practice, their models are highly accurate (Devlin et al., 2019), and they have been used in past interpretability work (Murdoch et al., 2018; Feng et al., 2018; 250 Jain and Wallace, 2019). We select NLI because it is challenging and one where models often learn undesirable “shortcuts” (Gururangan et al., 2018; Feng et al., 2019). We also include a case study on the Biosbias (De-Arteaga et al., 2019) dataset to show how discriminatory bias in classifiers"
2020.findings-emnlp.24,D18-1128,0,0.0200575,"lace (2019); Serrano and Smith (2019) critique the faithfulness of visualizing a model’s attention layers. Feng et al. (2018) show instabilities of saliency maps, and Adebayo et al. (2018); Kindermans et al. (2017) show saliency maps fail simple sanity checks. Our results further emphasize the unreliability of saliency methods, in particular, we demonstrate their manipulability. Usefulness of Explanations Finally, other work studies how useful interpretations are for humans. Feng and Boyd-Graber (2019) and Lai and Tan (2019) show that text interpretations can provide benefits to humans, while Chandrasekaran et al. (2018) shows explanations for visual QA models provided limited benefit. We present a method that enables adversaries to manipulate interpretations, which can have dire consequences for real-world users (Lakkaraju and Bastani, 2020). 7 Discussion Downsides of An Adversarial Approach Our proposed approach provides a mechanism for an adversary to hide the biases of their model (at least from gradient-based analyses). The goal of our work is not to aid malicious actors. Instead, we hope to encourage the development of robust analysis techniques, as well as methods to detect adversarial model modificati"
2020.findings-emnlp.24,N19-1423,0,0.512018,". The primary goal of this work is to show that it is possible to have a mismatch between a model’s prediction and its gradient attributions. 2.2 Analysis Methods Numerous analysis methods have recently been introduced, including saliency map techniques (Sundararajan et al., 2017; Smilkov et al., 2017) and perturbation methods (Feng et al., 2018; Ebrahimi et al., 2018; Jia and Liang, 2017). In this work, we focus on the gradient-based analysis methods available in AllenNLP Interpret (Wallace et al., 2019), which we briefly summarize below. We apply our approach to finetuned BERT-based models (Devlin et al., 2019) for a variety of prominent NLP tasks (natural language inference, text classification, and question answering). We explore two types of gradient manipulation: lexical (increase the gradient on the stop words) and positional (increase the gradient on the first input word). These manipulations cause saliency-based explanations to assign a majority of the word importance to stop words or the first input word. Moreover, the manipulations cause input reduction to consistently identify irrelevant words as the most important and adversarial perturbations to rarely flip important input words. Finally"
2020.findings-emnlp.24,P18-2006,0,0.257212,"iation for Computational Linguistics: EMNLP 2020, pages 247–258 c November 16 - 20, 2020. 2020 Association for Computational Linguistics all of the model parameters, are completely faithful when the model is linear (Feng et al., 2018), and closely approximate the model nearby an input (Simonyan et al., 2014). Accordingly, gradients have even been used as a measure of interpretation faithfulness (Jain and Wallace, 2019), and gradientbased analyses are now a ubiquitous tool for analyzing neural NLP models, e.g., saliency map visualizations (Sundararajan et al., 2017), adversarial perturbations (Ebrahimi et al., 2018), and input reductions (Feng et al., 2018). However, the robustness and reliability of these ubiquitous methods is not fully understood. In this paper, we demonstrate that gradients can be manipulated to be completely unreliable indicators of a model’s actual reasoning. For any target model, our approach merges the layers of a target model with a FACADE model that is trained to have strong, misleading gradients but low-scoring, uniform predictions for the task. As a result, this merged model makes nearly identical predictions as the target model, however, its gradients are overwhelmingly domin"
2020.findings-emnlp.24,P19-1554,1,0.825197,"popular tasks of sentiment analysis (binary Stanford Sentiment Treebank Socher et al. 2013), natural language inference (SNLI Bowman et al. 2015), and question answering (SQuAD Rajpurkar et al. 2016). We select sentiment analysis and question answering because they are widely used in practice, their models are highly accurate (Devlin et al., 2019), and they have been used in past interpretability work (Murdoch et al., 2018; Feng et al., 2018; 250 Jain and Wallace, 2019). We select NLI because it is challenging and one where models often learn undesirable “shortcuts” (Gururangan et al., 2018; Feng et al., 2019). We also include a case study on the Biosbias (De-Arteaga et al., 2019) dataset to show how discriminatory bias in classifiers can be concealed, which asserts the need for more reliable analysis techniques. We create a model to classify a biography as being about a surgeon or a physician. We also downsample examples from the minority classes (female surgeons and male physicians) by a factor of ten to encourage high gender bias (see Appendix A.4 for further details). Types of FACADE Models We use two forms of gradient manipulation in our setup, one positional and one lexical. These require dis"
2020.findings-emnlp.24,D18-1407,1,0.828702,"ues, both in non-adversarial (e.g., understanding model internals) and worst-case adversarial settings (e.g., concealing model biases from regulatory agencies). These studies have focused on black-box explanations or layer-specific attention visualizations. On the other hand, gradients are considered more faithful representations of a model: they depend on 247 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 247–258 c November 16 - 20, 2020. 2020 Association for Computational Linguistics all of the model parameters, are completely faithful when the model is linear (Feng et al., 2018), and closely approximate the model nearby an input (Simonyan et al., 2014). Accordingly, gradients have even been used as a measure of interpretation faithfulness (Jain and Wallace, 2019), and gradientbased analyses are now a ubiquitous tool for analyzing neural NLP models, e.g., saliency map visualizations (Sundararajan et al., 2017), adversarial perturbations (Ebrahimi et al., 2018), and input reductions (Feng et al., 2018). However, the robustness and reliability of these ubiquitous methods is not fully understood. In this paper, we demonstrate that gradients can be manipulated to be compl"
2020.findings-emnlp.24,N18-2017,0,0.0189857,"ets are selected from the popular tasks of sentiment analysis (binary Stanford Sentiment Treebank Socher et al. 2013), natural language inference (SNLI Bowman et al. 2015), and question answering (SQuAD Rajpurkar et al. 2016). We select sentiment analysis and question answering because they are widely used in practice, their models are highly accurate (Devlin et al., 2019), and they have been used in past interpretability work (Murdoch et al., 2018; Feng et al., 2018; 250 Jain and Wallace, 2019). We select NLI because it is challenging and one where models often learn undesirable “shortcuts” (Gururangan et al., 2018; Feng et al., 2019). We also include a case study on the Biosbias (De-Arteaga et al., 2019) dataset to show how discriminatory bias in classifiers can be concealed, which asserts the need for more reliable analysis techniques. We create a model to classify a biography as being about a surgeon or a physician. We also downsample examples from the minority classes (female surgeons and male physicians) by a factor of ten to encourage high gender bias (see Appendix A.4 for further details). Types of FACADE Models We use two forms of gradient manipulation in our setup, one positional and one lexica"
2020.findings-emnlp.24,N16-3020,1,0.530867,"ogits are dominated by the original BERT model. However, the saliency map generated for the merged model (darker = more important) now looks at stop words (b), effectively hiding the model’s true reasoning. Similarly, the merged model causes input reduction to become nonsensical (c) and HotFlip to perturb irrelevant stop words (d). Introduction It is becoming increasingly important to understand the reasoning behind the predictions of NLP models. Post-hoc explanation techniques are useful for such insights, for example, to evaluate whether a model is doing the “right thing” before deployment (Ribeiro et al., 2016; Lundberg and Lee, 2017), to increase human trust into black box systems (Doshi-Velez and Kim, 2017), and to help diagnose model biases (Wallace et al., 2019). Recent work, however, has shown that explanation techniques can be unstable and, more importantly, can be manipulated to hide the actual reasoning of the ∗ First two authors contributed equally. model. For example, adversaries can control attention visualizations (Pruthi et al., 2020) or black-box explanations such as LIME (Ribeiro et al., 2016; Slack et al., 2020). These studies have raised concerns about the reliability and utility o"
2020.findings-emnlp.24,N19-1357,0,0.330096,"used on black-box explanations or layer-specific attention visualizations. On the other hand, gradients are considered more faithful representations of a model: they depend on 247 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 247–258 c November 16 - 20, 2020. 2020 Association for Computational Linguistics all of the model parameters, are completely faithful when the model is linear (Feng et al., 2018), and closely approximate the model nearby an input (Simonyan et al., 2014). Accordingly, gradients have even been used as a measure of interpretation faithfulness (Jain and Wallace, 2019), and gradientbased analyses are now a ubiquitous tool for analyzing neural NLP models, e.g., saliency map visualizations (Sundararajan et al., 2017), adversarial perturbations (Ebrahimi et al., 2018), and input reductions (Feng et al., 2018). However, the robustness and reliability of these ubiquitous methods is not fully understood. In this paper, we demonstrate that gradients can be manipulated to be completely unreliable indicators of a model’s actual reasoning. For any target model, our approach merges the layers of a target model with a FACADE model that is trained to have strong, mislea"
2020.findings-emnlp.24,D17-1215,0,0.0244371,"kens. Similar to past work (Feng et al., 2018), we define the attribution at position i as |∇x L · xi | , ai = P i j ∇xj L · xj (1) where we dot product the gradient of the loss L on the model’s prediction with the embedding xi . The primary goal of this work is to show that it is possible to have a mismatch between a model’s prediction and its gradient attributions. 2.2 Analysis Methods Numerous analysis methods have recently been introduced, including saliency map techniques (Sundararajan et al., 2017; Smilkov et al., 2017) and perturbation methods (Feng et al., 2018; Ebrahimi et al., 2018; Jia and Liang, 2017). In this work, we focus on the gradient-based analysis methods available in AllenNLP Interpret (Wallace et al., 2019), which we briefly summarize below. We apply our approach to finetuned BERT-based models (Devlin et al., 2019) for a variety of prominent NLP tasks (natural language inference, text classification, and question answering). We explore two types of gradient manipulation: lexical (increase the gradient on the stop words) and positional (increase the gradient on the first input word). These manipulations cause saliency-based explanations to assign a majority of the word importance"
2020.findings-emnlp.24,P19-1282,0,0.0194928,"Rieger et al. (2020) incorporate a similar end-to-end regularization on gradient attributions, however, their goal is to align the attribution with known priors in order to improve model accuracy. We instead manipulate explanation methods to evaluate the extent to which a model’s true reasoning can be hidden. Pruthi et al. (2020) manipulate attention distributions in Natural Failures of Interpretation Methods We show that in the worst-case, gradient-based interpretation can be highly misleading. Other work studies natural failures of explanation methods. For instance, Jain and Wallace (2019); Serrano and Smith (2019) critique the faithfulness of visualizing a model’s attention layers. Feng et al. (2018) show instabilities of saliency maps, and Adebayo et al. (2018); Kindermans et al. (2017) show saliency maps fail simple sanity checks. Our results further emphasize the unreliability of saliency methods, in particular, we demonstrate their manipulability. Usefulness of Explanations Finally, other work studies how useful interpretations are for humans. Feng and Boyd-Graber (2019) and Lai and Tan (2019) show that text interpretations can provide benefits to humans, while Chandrasekaran et al. (2018) shows ex"
2020.findings-emnlp.24,P19-1631,0,0.0133061,"esults in Table 6. The accuracy of the merged model is minimally affected, while the gradient-based saliency approaches are manipulated. 6 Related Work End-to-End Interpretation Manipulation An alternative to our method of merging two models together is to directly manipulate the gradient attribution in an end-to-end fashion, as done by Ross and Doshi-Velez (2018); Ross et al. (2017); Viering et al. (2019); Heo et al. (2019) for computer vision and Dimanov et al. (2020) for simple classification tasks. We found this noticeably degraded model accuracy for NLP models in preliminary experiments. Liu and Avci (2019); Rieger et al. (2020) incorporate a similar end-to-end regularization on gradient attributions, however, their goal is to align the attribution with known priors in order to improve model accuracy. We instead manipulate explanation methods to evaluate the extent to which a model’s true reasoning can be hidden. Pruthi et al. (2020) manipulate attention distributions in Natural Failures of Interpretation Methods We show that in the worst-case, gradient-based interpretation can be highly misleading. Other work studies natural failures of explanation methods. For instance, Jain and Wallace (2019)"
2020.findings-emnlp.24,W02-0109,0,0.475676,"Missing"
2020.findings-emnlp.24,D13-1170,0,0.011746,"and f˜stop-reg indicate the models which are finetuned using Equation 5, and g is the FACADE model by itself. 4 (3) Layer Normalization: We merge layer normalization layers (Ba et al., 2016) by splitting the input into two parts according to the hidden dimensions of forig and g. We then apply layer normalization to each part independently. SST-2 Datasets To demonstrate the wide applicability of our method, we use four datasets that span different tasks and input-output formats. Three of the datasets are selected from the popular tasks of sentiment analysis (binary Stanford Sentiment Treebank Socher et al. 2013), natural language inference (SNLI Bowman et al. 2015), and question answering (SQuAD Rajpurkar et al. 2016). We select sentiment analysis and question answering because they are widely used in practice, their models are highly accurate (Devlin et al., 2019), and they have been used in past interpretability work (Murdoch et al., 2018; Feng et al., 2018; 250 Jain and Wallace, 2019). We select NLI because it is challenging and one where models often learn undesirable “shortcuts” (Gururangan et al., 2018; Feng et al., 2019). We also include a case study on the Biosbias (De-Arteaga et al., 2019) d"
2020.findings-emnlp.24,2020.acl-main.432,0,0.675799,"els. Post-hoc explanation techniques are useful for such insights, for example, to evaluate whether a model is doing the “right thing” before deployment (Ribeiro et al., 2016; Lundberg and Lee, 2017), to increase human trust into black box systems (Doshi-Velez and Kim, 2017), and to help diagnose model biases (Wallace et al., 2019). Recent work, however, has shown that explanation techniques can be unstable and, more importantly, can be manipulated to hide the actual reasoning of the ∗ First two authors contributed equally. model. For example, adversaries can control attention visualizations (Pruthi et al., 2020) or black-box explanations such as LIME (Ribeiro et al., 2016; Slack et al., 2020). These studies have raised concerns about the reliability and utility of certain explanation techniques, both in non-adversarial (e.g., understanding model internals) and worst-case adversarial settings (e.g., concealing model biases from regulatory agencies). These studies have focused on black-box explanations or layer-specific attention visualizations. On the other hand, gradients are considered more faithful representations of a model: they depend on 247 Findings of the Association for Computational Linguist"
2020.findings-emnlp.24,D19-3002,1,0.856255,", effectively hiding the model’s true reasoning. Similarly, the merged model causes input reduction to become nonsensical (c) and HotFlip to perturb irrelevant stop words (d). Introduction It is becoming increasingly important to understand the reasoning behind the predictions of NLP models. Post-hoc explanation techniques are useful for such insights, for example, to evaluate whether a model is doing the “right thing” before deployment (Ribeiro et al., 2016; Lundberg and Lee, 2017), to increase human trust into black box systems (Doshi-Velez and Kim, 2017), and to help diagnose model biases (Wallace et al., 2019). Recent work, however, has shown that explanation techniques can be unstable and, more importantly, can be manipulated to hide the actual reasoning of the ∗ First two authors contributed equally. model. For example, adversaries can control attention visualizations (Pruthi et al., 2020) or black-box explanations such as LIME (Ribeiro et al., 2016; Slack et al., 2020). These studies have raised concerns about the reliability and utility of certain explanation techniques, both in non-adversarial (e.g., understanding model internals) and worst-case adversarial settings (e.g., concealing model bia"
2020.findings-emnlp.24,D16-1264,0,0.0196154,"self. 4 (3) Layer Normalization: We merge layer normalization layers (Ba et al., 2016) by splitting the input into two parts according to the hidden dimensions of forig and g. We then apply layer normalization to each part independently. SST-2 Datasets To demonstrate the wide applicability of our method, we use four datasets that span different tasks and input-output formats. Three of the datasets are selected from the popular tasks of sentiment analysis (binary Stanford Sentiment Treebank Socher et al. 2013), natural language inference (SNLI Bowman et al. 2015), and question answering (SQuAD Rajpurkar et al. 2016). We select sentiment analysis and question answering because they are widely used in practice, their models are highly accurate (Devlin et al., 2019), and they have been used in past interpretability work (Murdoch et al., 2018; Feng et al., 2018; 250 Jain and Wallace, 2019). We select NLI because it is challenging and one where models often learn undesirable “shortcuts” (Gururangan et al., 2018; Feng et al., 2019). We also include a case study on the Biosbias (De-Arteaga et al., 2019) dataset to show how discriminatory bias in classifiers can be concealed, which asserts the need for more reli"
2020.nlpcovid19-2.11,D15-1075,0,0.0376247,"huggingface.co/digitalepidemiologylab/ covid-twitter-bert 4.3 Stance Detection, using NLI Models Due to the lack of adequately large datasets for stance detection with pairs of sentences (Mohammad et al., 2016; Ferreira and Vlachos, 2016; Gorrell et al., 2018), we cannot use existing datasets to train models for our setup. However, since classes in misinformation detection correspond to those in natural language inference (NLI), a task with much larger training datasets, we instead experiment with adapting NLI models on this task. We train linear classifiers on three common NLI datasets—SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018), and MedNLI (Shivade, 2019). These classifiers use the following features, respectively: (i) concatenated unigram and bigram TF-IDF vectors for each input, (ii) concatenated average GloVe embeddings for each input, (iii) Bidirectional LSTM encoding, and (iv) the Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) representation that uses siamese and triplet networks to obtain semantically meaningful sentence embeddings. Note that for (iii) and (iv), the transformer architectures (BiLSTM and SBERT) are jointly trained with the linear classifier. BERTS CORE (DA)"
2020.nlpcovid19-2.11,N16-1138,0,0.232621,"thus only the BERTS CORE (DA) model is able to retrieve the correct misconception. The second example primarily requires domain knowledge that ‘coronavirus’ and ‘Sars-cov-2’ are very similar, and only domain-adapted models are able to score the correct misconception highest. The last example shows when contextual embeddings (BERT) outperform non-contextual embedding (GloVe). 2 https://huggingface.co/digitalepidemiologylab/ covid-twitter-bert 4.3 Stance Detection, using NLI Models Due to the lack of adequately large datasets for stance detection with pairs of sentences (Mohammad et al., 2016; Ferreira and Vlachos, 2016; Gorrell et al., 2018), we cannot use existing datasets to train models for our setup. However, since classes in misinformation detection correspond to those in natural language inference (NLI), a task with much larger training datasets, we instead experiment with adapting NLI models on this task. We train linear classifiers on three common NLI datasets—SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018), and MedNLI (Shivade, 2019). These classifiers use the following features, respectively: (i) concatenated unigram and bigram TF-IDF vectors for each input, (ii) concatenated average"
2020.nlpcovid19-2.11,2020.acl-main.740,0,0.084859,"ity models on their ability to detect whether a tweet is relevant to a given misconception (a.k.a misconception retrieval). Following prior work on fact verification (Thorne et al., 2018) and fake news detection (Yang et al., 2019), we evaluate NLI models on misinformation (a.k.a. stance detection), by equating the class labels Agree, Disagree, and No Stance to Entailment, Contradiction, and Neutral, respectively. Our results show that existing models struggle at both tasks (38.7 Hits@1 for retrieval and 32.5 macro F1 on stance detection), however improve considerably after domain adaptation (Gururangan et al. (2020); 61.3 Hits@1 for retrieval and 50.2 macro F1 on stance detection). While our initial results using domain adaptation are encouraging, they leave much room for improvement. There is still much work that needs to be done before NLP systems can be seriously considered for combating COVID-19-related misinformation, and we hope C OVID L IES will be useful to help researchers understand when such systems are ready to be deployed. 2 Problem Setup We assume access to a collection of positively phrased known misconceptions M = {m1 , . . . , m|M |}, e.g., “Wearing masks does not prevent spread of COVID"
2020.nlpcovid19-2.11,C18-1131,0,0.0336985,"Missing"
2020.nlpcovid19-2.11,2020.nlpcovid19-acl.9,0,0.10996,"nformation online. Serrano et al. (2020) detect YouTube videos spreading conspiracy theories using features of user comments, and Dharawat et al. (2020) classify tweets by the severity of health risks associated with them. McQuillan et al. (2020) study the behaviour of COVID-19 misinformation networks on Twitter using mapping, topic modeling, bridging centrality, and divergence. Penn Medicine launched a chatbot to provide patients with accurate information about the virus (VolppKevin et al., 2020), and a crowdsourced chatbot, Jennifer, is also available to answer questions about the pandemic (Li et al., 2020). We are the first to frame COVID-19 misinformation detection as a two-stage task of misconception retrieval and pair1742 No Stance 447 161 3902 Disagree No Stance Agree True Label (a) SBERT (DA) Agree 173 24 95 Disagree Disagree 66 205 102 117 593 363 202 5060 Disagree No Stance Predicted Label 104 No Stance Agree 9 Predicted Label 157 Agree True Label (b) BERTS CORE (DA) + (a) Figure 3: Confusion Matrices for stance detection task using SBERT (DA) models trained on MultiNLI. The second model uses BERTS CORE (DA) to first determine whether a misconception-tweet pair is Relevant or No Stance,"
2020.nlpcovid19-2.11,S16-1003,0,0.112979,"Missing"
2020.nlpcovid19-2.11,D14-1162,0,0.086864,"Missing"
2020.nlpcovid19-2.11,D19-1410,0,0.0178031,"lasses in misinformation detection correspond to those in natural language inference (NLI), a task with much larger training datasets, we instead experiment with adapting NLI models on this task. We train linear classifiers on three common NLI datasets—SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018), and MedNLI (Shivade, 2019). These classifiers use the following features, respectively: (i) concatenated unigram and bigram TF-IDF vectors for each input, (ii) concatenated average GloVe embeddings for each input, (iii) Bidirectional LSTM encoding, and (iv) the Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) representation that uses siamese and triplet networks to obtain semantically meaningful sentence embeddings. Note that for (iii) and (iv), the transformer architectures (BiLSTM and SBERT) are jointly trained with the linear classifier. BERTS CORE (DA) + NLI Since BERTS CORE with domain adaptation performs best at retrieval for relevant classes, we use it to improve stance detection. We combine BERTS CORE (DA) with NLI models, initially classifying tweet-misconception pairs with high BERTS CORE scores (&gt;0.4) as Relevant, subsequently using the NLI model to determine whether the pair Agree or D"
2020.nlpcovid19-2.11,W17-4214,0,0.0138054,"ds the retrieved misconceptions. To facilitate research on this task, we release C OVID L IES1 , a dataset of 6761 expert-annotated tweets to evaluate the performance of misinformation detection systems on 86 different pieces of COVID-19 related misinformation. We evaluate existing NLP systems on this dataset, providing initial benchmarks and identifying key challenges for future models to improve upon. 1 Introduction Detecting spread of misinformation such as, rumors, hoaxes, fake news, propaganda, spear phishing, and conspiracy theories, is an important task for natural language processing (Thorne et al., 2017; Shu et al., 2017; Thorne and Vlachos, 2018). Online social media networks provide particularly fertile ground for the spread of misinformation— they lack gate-keeping and regulations, users publish content without having to go through an editor, peer review, verification of qualification, or providing sources, and social networks tend to create “echo chambers” or closed networks of communication insulated from disagreements. ∗ 1 First four authors contributed equally. https://ucinlp.github.io/covid19 Tweet: “Coronavirus CV19 was a top secret biological warfare experiment. That is why it is o"
2020.nlpcovid19-2.11,C18-1283,0,0.0292169,"ilitate research on this task, we release C OVID L IES1 , a dataset of 6761 expert-annotated tweets to evaluate the performance of misinformation detection systems on 86 different pieces of COVID-19 related misinformation. We evaluate existing NLP systems on this dataset, providing initial benchmarks and identifying key challenges for future models to improve upon. 1 Introduction Detecting spread of misinformation such as, rumors, hoaxes, fake news, propaganda, spear phishing, and conspiracy theories, is an important task for natural language processing (Thorne et al., 2017; Shu et al., 2017; Thorne and Vlachos, 2018). Online social media networks provide particularly fertile ground for the spread of misinformation— they lack gate-keeping and regulations, users publish content without having to go through an editor, peer review, verification of qualification, or providing sources, and social networks tend to create “echo chambers” or closed networks of communication insulated from disagreements. ∗ 1 First four authors contributed equally. https://ucinlp.github.io/covid19 Tweet: “Coronavirus CV19 was a top secret biological warfare experiment. That is why it is only affecting the poor.” Misconception: “Coro"
2020.nlpcovid19-2.11,W18-5501,0,0.0608123,"Missing"
2020.nlpcovid19-2.11,P17-2102,0,0.0169811,"nfo labels to facilitate research in automated fact checking. This is similar to Emergent (Ferreira and Vlachos, 2016), a stance classification dataset consisting of rumored claims and associated news articles with labels of For, Against, or Observing the claim. Stance detection is also the focus of the Fake News Challenge (FNC-1)3 consisting of pairs of news article headlines and body texts with Agrees, Disagrees, Discusses, and Unrelated labels. Our proposed models for detecting misinformation by using classifiers fall within the framework of detecting misinformation using content features (Volkova et al., 2017; Wei and Wan, 2017). Other approaches include using crowd behaviour (Tschiatschek et al., 2018; Mendoza et al., 2010), reliability of the source (Lumezanu et al., 2012; Li et al., 2015), knowledge graphs (Ciampaglia et al., 2015), or a combination of these approaches 3 http://www.fakenewschallenge.org/ (Castillo et al., 2011; Kumar et al., 2016). Adapting these techniques to COVID-19 misinformation is a promising direction for future work. 6 Conclusions and Future Work The ongoing COVID-19 pandemic has been accompanied by a corresponding ‘infodemic’ of misinformation about the virus. It is im"
2020.nlpcovid19-2.11,P17-2067,0,0.0616757,"Missing"
2020.nlpcovid19-2.11,N18-1101,0,0.0252911,"gylab/ covid-twitter-bert 4.3 Stance Detection, using NLI Models Due to the lack of adequately large datasets for stance detection with pairs of sentences (Mohammad et al., 2016; Ferreira and Vlachos, 2016; Gorrell et al., 2018), we cannot use existing datasets to train models for our setup. However, since classes in misinformation detection correspond to those in natural language inference (NLI), a task with much larger training datasets, we instead experiment with adapting NLI models on this task. We train linear classifiers on three common NLI datasets—SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018), and MedNLI (Shivade, 2019). These classifiers use the following features, respectively: (i) concatenated unigram and bigram TF-IDF vectors for each input, (ii) concatenated average GloVe embeddings for each input, (iii) Bidirectional LSTM encoding, and (iv) the Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) representation that uses siamese and triplet networks to obtain semantically meaningful sentence embeddings. Note that for (iii) and (iv), the transformer architectures (BiLSTM and SBERT) are jointly trained with the linear classifier. BERTS CORE (DA) + NLI Since BERTS CORE with domai"
2020.wnut-1.29,D18-1227,0,0.0120801,"tween unstructured text (e.g. a tweet) and structured, machine-readable knowledge bases (e.g. Wikidata) using entity linking (EL) that grounds named mentions to a unique, real-world entity, i.e., an entry in the knowledge base (see Figure 1 for an example). Entity linking has been widely applied in natural language processing (Ling et al., 2015; Gupta et al., 2017; Raiman and Raiman, 2018; Radhakrishnan et al., 2018) on domains including news, biographical text, movie/show plots, amongst others. Although entity linking has been used in social media applications as well (Miyazaki et al., 2018; Dai et al., 2018), it is much less common. There are a number of reasons by existing entity linking systems are not commonly used for social media analysis. One of the primary concerns is that many of the existing entity linking systems are supervised (Yosef et al., 2011; Ganea and Hofmann, 2017), which not only makes them suitable for the domains they are trained on (Meij et al., 2012), but also makes them excessively reliant on context around the mention. For these reasons, supervised EL systems tend to be inaccurate on noisy and short text (Cornolti et al., 2013). Even unsupervised systems are heavily-engin"
2020.wnut-1.29,C16-1111,0,0.120756,"anguage models that use such resources for better contextual modeling (Peters et al., 2019; Logan et al., 2019). Having access to the full knowledge graph can also help models that perform entity analysis using hops in the knowledge graphs, i.e. location modeling by using relations such as livesIn or headquarteredIn of the linked entities, even if the location is not mention directly. As the source of the tweets in TweekiData, we identify two prominent datasets that are commonly used in the community, in order to ensure the resulting dataset will be useful. (1) BTC, or “Broad Twitter Corpus” (Derczynski et al., 2016), is seven sets of gold datasets of tweets collected over stratified times, places and social, and is widely used 3 if there are no such candidates, we ignore this filtering. TweekiGold TweekiData 500 16.31 8,155 958 852 638 5M 14.41 8,010,253 5,038,870 1,954,229 273,685 # tweets # tokens/tweet # mentions (toks) # mentions (spans) # links # uniq entities Table 1: Statistics of the Tweeki-linked datasets. for Named Entities Recognition. We use section A and H of this corpus, with the size of 1000 and 2000 tweets respectively, and (2) UTGEO2011, a massive Twitter dataset mainly created for tweet"
2020.wnut-1.29,D17-1277,0,0.0594975,"Missing"
2020.wnut-1.29,D17-1284,1,0.81726,"any others. However, short length of the text, casual and error-prone writing style, and evolving topics over time is extremely challenging for existing text analysis tools (Derczynski et al., 2015). One of the common approaches is to bridge the gap between unstructured text (e.g. a tweet) and structured, machine-readable knowledge bases (e.g. Wikidata) using entity linking (EL) that grounds named mentions to a unique, real-world entity, i.e., an entry in the knowledge base (see Figure 1 for an example). Entity linking has been widely applied in natural language processing (Ling et al., 2015; Gupta et al., 2017; Raiman and Raiman, 2018; Radhakrishnan et al., 2018) on domains including news, biographical text, movie/show plots, amongst others. Although entity linking has been used in social media applications as well (Miyazaki et al., 2018; Dai et al., 2018), it is much less common. There are a number of reasons by existing entity linking systems are not commonly used for social media analysis. One of the primary concerns is that many of the existing entity linking systems are supervised (Yosef et al., 2011; Ganea and Hofmann, 2017), which not only makes them suitable for the domains they are trained"
2020.wnut-1.29,D11-1072,0,0.170825,"Missing"
2020.wnut-1.29,P14-1036,0,0.0138828,"lts in Table 9, using Tweeki and appending Wikidata information to tweets increases accuracy for the both tweet and user-level (for user-level prediction we aggregate output probabilities of all tweets from the user, then choose the most probable label). This demonstrates that even such a simple approach to incorporating KB information can provide improvements to existing problems, suggesting many applications of entity linking to tweets and other social media text. 5 Related Work Entity Linking Systems Entity linking (EL) of tweets has attracted a lot of attention recently (Liu et al., 2013; Huang et al., 2014; Sikdar and Gamb¨ack, 2016; Nie et al., 2018). Similar to entity linking systems for general text, EL for tweets is primarily composed of two major steps: 1) the identification of the mentions, similar to tasks such as term expansion (Zou et al., 2014), and 2) identifying the candidate entities to the identified mentions. For the latter step, roughly two types of features are used. Local features identify one mention at the time and disambiguate it separately such as using prior probability in Liu et al. (2013) or temporal relevance mention in Tran et al. (2015). Global features take a more c"
2020.wnut-1.29,K18-1050,0,0.0819237,"13.1 19.1 15.2 24.1 9.06 22.8 14.8 24.8 18.2 9.0 57.05 9.1 41.1 50.1 41.1 29.2 36.0 34.2 26.3 15.2 39.0 14.0 37.1 38.1 17.1 53.2 79.1 20.2 69.0 56.1 47.2 32.1 35.2 50.4 61.0 45.0 25.1 38.5 49.4 29.1 65.0 Table 5: Entity Linking Performance of existing linkers, using strong matching metric on three datasets. Derczynski Tw-Stanford Tw-AllenNLP ORG TweekiGold P R F1 P R F1 36.4 41.1 29.4 34.2 32.5 37.1 56.7 69.0 44.6 61.0 49.9 65.0 Table 6: Choice of Mention Extraction: using Stanford for mention extraction and NER, compared to using AllenNLP, in the first module of the pipeline. Entity Linking (Kolitsas et al., 2018) and OpenTapioca (Delpeuch, 2019). Table 5 compares these models on different datasets using precision/recall/F1 based on EL strong matching. TagMe has acceptable performance on all datasets and the best Recall for NEEL2016, while Babelfy performs the worst, specifically on NEEL2016. While End-to-End is not specifically designed for short, noisy text, it is the winner of all three datasets in terms of precision. OpenTapioca has average performance on all datasets, with low accuracy on NEEL2016. Our proposed system, Tweeki, has the best F1 on NEEL2016 and TweekiGold, while being quite close to"
2020.wnut-1.29,C12-1093,0,0.0320158,"ship The Walt Disney Company (Q7414) Aliases: Disney, Walt Disney, ... Location: 34°9'24.7&quot;N, 118°19'30.2&quot;W InstanceOf: business (Q4830453) ... Figure 1: Example of an entity-linked tweet, containing two mentions linked to WikiData entities. Introduction Popularity and steady increase in adoption of social media makes it a ripe domain for understanding and analyzing world events, with Twitter as one of the largest social media platforms. As a result, tweets now have become a rich source of information, and Twitter analysis has been widely applied for many applications such as trend detection (Lau et al., 2012), opinion mining (Pak and Paroubek, 2010), election politics (Conover et al., 2011), and many others. However, short length of the text, casual and error-prone writing style, and evolving topics over time is extremely challenging for existing text analysis tools (Derczynski et al., 2015). One of the common approaches is to bridge the gap between unstructured text (e.g. a tweet) and structured, machine-readable knowledge bases (e.g. Wikidata) using entity linking (EL) that grounds named mentions to a unique, real-world entity, i.e., an entry in the knowledge base (see Figure 1 for an example)."
2020.wnut-1.29,Q15-1023,1,0.929461,"t al., 2011), and many others. However, short length of the text, casual and error-prone writing style, and evolving topics over time is extremely challenging for existing text analysis tools (Derczynski et al., 2015). One of the common approaches is to bridge the gap between unstructured text (e.g. a tweet) and structured, machine-readable knowledge bases (e.g. Wikidata) using entity linking (EL) that grounds named mentions to a unique, real-world entity, i.e., an entry in the knowledge base (see Figure 1 for an example). Entity linking has been widely applied in natural language processing (Ling et al., 2015; Gupta et al., 2017; Raiman and Raiman, 2018; Radhakrishnan et al., 2018) on domains including news, biographical text, movie/show plots, amongst others. Although entity linking has been used in social media applications as well (Miyazaki et al., 2018; Dai et al., 2018), it is much less common. There are a number of reasons by existing entity linking systems are not commonly used for social media analysis. One of the primary concerns is that many of the existing entity linking systems are supervised (Yosef et al., 2011; Ganea and Hofmann, 2017), which not only makes them suitable for the doma"
2020.wnut-1.29,P13-1128,0,0.015979,"Based on the results in Table 9, using Tweeki and appending Wikidata information to tweets increases accuracy for the both tweet and user-level (for user-level prediction we aggregate output probabilities of all tweets from the user, then choose the most probable label). This demonstrates that even such a simple approach to incorporating KB information can provide improvements to existing problems, suggesting many applications of entity linking to tweets and other social media text. 5 Related Work Entity Linking Systems Entity linking (EL) of tweets has attracted a lot of attention recently (Liu et al., 2013; Huang et al., 2014; Sikdar and Gamb¨ack, 2016; Nie et al., 2018). Similar to entity linking systems for general text, EL for tweets is primarily composed of two major steps: 1) the identification of the mentions, similar to tasks such as term expansion (Zou et al., 2014), and 2) identifying the candidate entities to the identified mentions. For the latter step, roughly two types of features are used. Local features identify one mention at the time and disambiguate it separately such as using prior probability in Liu et al. (2013) or temporal relevance mention in Tran et al. (2015). Global fe"
2020.wnut-1.29,P19-1598,1,0.824634,"unning Tweeki on a number of source datasets (3.1). We also introduce TweekiGold, a small, manuallyannotated dataset for measuring the quality of the linker and the automatically linked dataset. 3.1 TweekiData TweekiData is a large automatically-annotated dataset, which is linked to Wikidata using Tweeki. The linking of text and KG is a valuable resource for learning representations that enable better reasoning about entities and how they are expressed in text (tweets in this case), such as pretrained language models that use such resources for better contextual modeling (Peters et al., 2019; Logan et al., 2019). Having access to the full knowledge graph can also help models that perform entity analysis using hops in the knowledge graphs, i.e. location modeling by using relations such as livesIn or headquarteredIn of the linked entities, even if the location is not mention directly. As the source of the tweets in TweekiData, we identify two prominent datasets that are commonly used in the community, in order to ensure the resulting dataset will be useful. (1) BTC, or “Broad Twitter Corpus” (Derczynski et al., 2016), is seven sets of gold datasets of tweets collected over stratified times, places and"
2020.wnut-1.29,P17-1116,0,0.0585696,"Missing"
2020.wnut-1.29,W18-6102,0,0.132503,"is to bridge the gap between unstructured text (e.g. a tweet) and structured, machine-readable knowledge bases (e.g. Wikidata) using entity linking (EL) that grounds named mentions to a unique, real-world entity, i.e., an entry in the knowledge base (see Figure 1 for an example). Entity linking has been widely applied in natural language processing (Ling et al., 2015; Gupta et al., 2017; Raiman and Raiman, 2018; Radhakrishnan et al., 2018) on domains including news, biographical text, movie/show plots, amongst others. Although entity linking has been used in social media applications as well (Miyazaki et al., 2018; Dai et al., 2018), it is much less common. There are a number of reasons by existing entity linking systems are not commonly used for social media analysis. One of the primary concerns is that many of the existing entity linking systems are supervised (Yosef et al., 2011; Ganea and Hofmann, 2017), which not only makes them suitable for the domains they are trained on (Meij et al., 2012), but also makes them excessively reliant on context around the mention. For these reasons, supervised EL systems tend to be inaccurate on noisy and short text (Cornolti et al., 2013). Even unsupervised system"
2020.wnut-1.29,Q14-1019,0,0.0330863,"for short texts. Some approaches use a graphbased representation to combine of local and global features (Huang et al., 2014). Moreover, recently, neural network methods have been applied to entity linking to model the local contextual information, such as (Nie et al., 2018) that captures semantic information between the local context and the candidate entity via representation-based and interactionbased neural semantic matching models. Among all the proposed models, in this paper, we chose TagMe (Ferragina and Scaiella, 2010) that uses global features in an unsupervised manner, and Babelfy (Moro et al., 2014) that uses random walks and a densest subgraph algorithm for jointly disambiguating word senses and entity linking. Although Tag-Me is specifically designed for short text, Babelfy is a general purpose entity linker which also suitable for short and highly ambiguous text disambiguation (Moro et al., 2014). We also consider other popular linking approaches from outside of social media such as AIDA (Hoffart et al., 2011b), which uses Stanford NER Tagger and adopts the YAGO2 knowledge base (Hoffart et al., 2011a). From recent state-of-the-art models, we select End-to-End Neural Entity Linking (Ko"
2020.wnut-1.29,K18-1046,0,0.0763747,"data information to tweets increases accuracy for the both tweet and user-level (for user-level prediction we aggregate output probabilities of all tweets from the user, then choose the most probable label). This demonstrates that even such a simple approach to incorporating KB information can provide improvements to existing problems, suggesting many applications of entity linking to tweets and other social media text. 5 Related Work Entity Linking Systems Entity linking (EL) of tweets has attracted a lot of attention recently (Liu et al., 2013; Huang et al., 2014; Sikdar and Gamb¨ack, 2016; Nie et al., 2018). Similar to entity linking systems for general text, EL for tweets is primarily composed of two major steps: 1) the identification of the mentions, similar to tasks such as term expansion (Zou et al., 2014), and 2) identifying the candidate entities to the identified mentions. For the latter step, roughly two types of features are used. Local features identify one mention at the time and disambiguate it separately such as using prior probability in Liu et al. (2013) or temporal relevance mention in Tran et al. (2015). Global features take a more comprehensive view and consider the relations b"
2020.wnut-1.29,pak-paroubek-2010-twitter,0,0.101398,") Aliases: Disney, Walt Disney, ... Location: 34°9'24.7&quot;N, 118°19'30.2&quot;W InstanceOf: business (Q4830453) ... Figure 1: Example of an entity-linked tweet, containing two mentions linked to WikiData entities. Introduction Popularity and steady increase in adoption of social media makes it a ripe domain for understanding and analyzing world events, with Twitter as one of the largest social media platforms. As a result, tweets now have become a rich source of information, and Twitter analysis has been widely applied for many applications such as trend detection (Lau et al., 2012), opinion mining (Pak and Paroubek, 2010), election politics (Conover et al., 2011), and many others. However, short length of the text, casual and error-prone writing style, and evolving topics over time is extremely challenging for existing text analysis tools (Derczynski et al., 2015). One of the common approaches is to bridge the gap between unstructured text (e.g. a tweet) and structured, machine-readable knowledge bases (e.g. Wikidata) using entity linking (EL) that grounds named mentions to a unique, real-world entity, i.e., an entry in the knowledge base (see Figure 1 for an example). Entity linking has been widely applied in"
2020.wnut-1.29,D19-1005,1,0.829004,"d corpus created by running Tweeki on a number of source datasets (3.1). We also introduce TweekiGold, a small, manuallyannotated dataset for measuring the quality of the linker and the automatically linked dataset. 3.1 TweekiData TweekiData is a large automatically-annotated dataset, which is linked to Wikidata using Tweeki. The linking of text and KG is a valuable resource for learning representations that enable better reasoning about entities and how they are expressed in text (tweets in this case), such as pretrained language models that use such resources for better contextual modeling (Peters et al., 2019; Logan et al., 2019). Having access to the full knowledge graph can also help models that perform entity analysis using hops in the knowledge graphs, i.e. location modeling by using relations such as livesIn or headquarteredIn of the linked entities, even if the location is not mention directly. As the source of the tweets in TweekiData, we identify two prominent datasets that are commonly used in the community, in order to ensure the resulting dataset will be useful. (1) BTC, or “Broad Twitter Corpus” (Derczynski et al., 2016), is seven sets of gold datasets of tweets collected over stratifi"
2020.wnut-1.29,N18-1167,0,0.0172515,", casual and error-prone writing style, and evolving topics over time is extremely challenging for existing text analysis tools (Derczynski et al., 2015). One of the common approaches is to bridge the gap between unstructured text (e.g. a tweet) and structured, machine-readable knowledge bases (e.g. Wikidata) using entity linking (EL) that grounds named mentions to a unique, real-world entity, i.e., an entry in the knowledge base (see Figure 1 for an example). Entity linking has been widely applied in natural language processing (Ling et al., 2015; Gupta et al., 2017; Raiman and Raiman, 2018; Radhakrishnan et al., 2018) on domains including news, biographical text, movie/show plots, amongst others. Although entity linking has been used in social media applications as well (Miyazaki et al., 2018; Dai et al., 2018), it is much less common. There are a number of reasons by existing entity linking systems are not commonly used for social media analysis. One of the primary concerns is that many of the existing entity linking systems are supervised (Yosef et al., 2011; Ganea and Hofmann, 2017), which not only makes them suitable for the domains they are trained on (Meij et al., 2012), but also makes them excessive"
2020.wnut-1.29,P11-1138,0,0.0403859,"tified mention, Tweeki generates a set of candidate entities (with corresponding scores based on prior probability). Finally, after type 106 MISC 2.3 Candidate Generation Given a mention mi , we use the KB itself to produce a set of candidate entities, with associated scores, that will allow us to estimate the conditional probability p(c|mi ). In current literature, there are primarily two ways to generate such candidates: (1) Crosswiki links, i.e. a web crawl that aggregates anchor links to Wikipedia entities (Ling et al., 2015), and (2) Intrawiki links: i.e. doing the same within Wikipedia (Ratinov et al., 2011). We use the latter approach since it is much easier to maintain and update over time. To adapt Intrawiki links to the WikiData KB, we use the existing links between Wikipedia and WikiData entities to gather all the entity aliases and number of time each alias is used in Wikipedia for the entity. The candidate generation module thus returns a set of scored candidates for each mention, i.e. {(mi , Ci )} where MISC ORG Organization Song MISC LOC Mountain MISC Television series Communes of France unincorporated community River MISC MISC LOC LOC LOC LOC Train station MISC PER Band (rock and pop) B"
2020.wnut-1.29,D11-1141,0,0.0779914,"to Wikidata that relies on topic similarities and local entity context. Although these models are not specifically designed for noisy and short text, but they are sophisticated general purpose EL proposed recently and tested on different data types including Twitter. Twitter-related datasets Many Twitter-based datasets have been introduced for different research goals. Related to EL task, we can divide the datasets into two categories: named entity recognition (NER) datasets and named entity linking (NEL) datasets. NER datasets focus on identifying mentions and their types, such as dataset by Ritter et al. (2011) or BTC (Derczynski et al., 2016), with 228 gold dataset published for concept extraction challenge in #MSM2013 (Cano Basave et al., 2013). Based on GERBIL benchmark report (R¨oder et al., 2018), for the A2KB (NER and NEL) task in tweets, the most commonly used datasets have been introduced in ”Making Sense of Microposts” challenge (#Microposts) from 2014 till 2016. Among them, Named Entity Extraction and Linking Challenge2016 (NEEL2016) is the most popular one to use, consisting of 296 tweets in testset with 3.4 mentions in each tweet on average. It is also valuable to mention that 384 out of"
2020.wnut-1.29,D12-1137,0,0.0180154,"tratified times, places and social, and is widely used 3 if there are no such candidates, we ignore this filtering. TweekiGold TweekiData 500 16.31 8,155 958 852 638 5M 14.41 8,010,253 5,038,870 1,954,229 273,685 # tweets # tokens/tweet # mentions (toks) # mentions (spans) # links # uniq entities Table 1: Statistics of the Tweeki-linked datasets. for Named Entities Recognition. We use section A and H of this corpus, with the size of 1000 and 2000 tweets respectively, and (2) UTGEO2011, a massive Twitter dataset mainly created for tweet geolocation prediction, but also used for other purposes (Roller et al., 2012). The dataset is limited to US region and has two versions: UTGEO2011-Large containing 38M tweets belongs to almost 450K users, and UTGEO2011Small contains 1.6M tweets with 10K users. We took a random subset of 5M tweets from UTGEO2011, to build TweekiData. 3.2 TweekiGold As there is no gold data available linking Twitter to Wikidata, we collect a gold dataset manually. We use mention extraction on tweets from UTGEO2011-small, and select 700 random tweets to annotate. An expert manually provides the following for each tweet: correct NER tags from TNER (in IOB2 format), a Wikidata entity ID for"
2020.wnut-1.29,W16-6326,0,0.0586835,"Missing"
2020.wnut-1.29,D15-1010,0,0.0421643,"Missing"
2020.wosp-1.2,D19-1236,0,0.270702,"train a classifier to predict the authors, and analyze the results. However, they focus primarily on the utility of self-citations in the submitted manuscripts as a key to identification (Mahoney et al., 1978; Yankauer, 1991; Hill and Provost, 2003; Payer et al., 2015), and do not take author’s citation history beyond just self-citations into account. The experiment design in these studies is also limited: they use relatively small datasets, include papers only from a specific domain (e.g., physics (Hill and Provost, 2003), computer science (Payer et al., 2015) or natural language processing (Caragea et al., 2019)), and pre-select the set of papers and authors for evaluation (Payer et al., 2015; Caragea et al., 2019). Furthermore, they focus on author identification, whereas knowing affiliation and the nationality also introduces biases in the reviewing process (Lee et al., 2013). In this paper, we use the task of author identity, affiliation, and nationality predictions to analyze the extent to which citation patterns matter, evaluate our approach on large-scale datasets in many domains, and provide detailed insights into the ways in which identity is leaked. We describe the following contributions: 1"
2020.wosp-1.2,N18-1149,0,0.0612591,"Missing"
2021.acl-long.345,2020.emnlp-main.550,0,0.285832,"rom a knowledge source before an NLP system can perform reasoning and produce an answer (Chen et al., 2017; Petroni et al., 2021). The open-domain setting better reflects real-world usage for tasks where relevant information is generally not provided (e.g., fact checking). ∗ Work started during an internship at Apple. The AmbER sets used in this paper and the code to generate them are available at https://github.com/ anthonywchen/AmbER-Sets. Because success hinges on finding relevant documents, open-domain progress has been closely tied to improvements in retrieval systems2 (Lee et al., 2019; Karpukhin et al., 2020; Lewis et al., 2020b). A crucial challenge when interacting with a large knowledge source (e.g., Wikipedia) is entity ambiguity, the phenomenon where a single name can map to multiple entities. Resolving this ambiguity is referred to as entity disambiguation and is an important step for effective retrieval. For example, given the query “What musical instrument does Abe Lincoln play?”, documents about the musician should rank higher than other entities with the same name (Figure 1). Although entity disambiguation has been extensively studied in entity linking (Hoffart et al., 2011; Rao et al.,"
2021.acl-long.345,Q19-1026,0,0.0386515,"rs are four times as likely to exhibit this when dealing tail queries. Downstream Models The dominant approach to open-domain tasks is a two-stage process where a retriever first finds relevant documents, followed by a downstream model that processes these documents to produce an answer. We evaluate the end-to-end performance on AmbER sets by training downstream NLP models on our tasks of interest. For fact checking, we fine-tune a BERT classifier (Devlin et al., 2019) on FEVER (Thorne et al., 2018). For question answering, we fine-tune a RoBERTa model (Liu et al., 2019) on Natural Questions (Kwiatkowski et al., 2019). For slot filling, a generation task, we fine-tune a BART model (Lewis et al., 2020a) on T-Rex (Elsahar et al., 2018). We provide example training instances in Table 2 and additional details on the models in Appendix E. We use the AllenNLP and HuggingFace Transformers library to finetune our downstream models (Gardner et al., 2018; Wolf et al., 2020). 5 Results In this section, we evaluate existing open-domain NLP pipelines using AmbER sets. We also conduct Figure 4: Popularity Gap vs Retrieval Gap. We bin QA queries of pairs of head and tail entities based on the popularity gap between the e"
2021.acl-long.345,P19-1612,0,0.0343035,"to be retrieved from a knowledge source before an NLP system can perform reasoning and produce an answer (Chen et al., 2017; Petroni et al., 2021). The open-domain setting better reflects real-world usage for tasks where relevant information is generally not provided (e.g., fact checking). ∗ Work started during an internship at Apple. The AmbER sets used in this paper and the code to generate them are available at https://github.com/ anthonywchen/AmbER-Sets. Because success hinges on finding relevant documents, open-domain progress has been closely tied to improvements in retrieval systems2 (Lee et al., 2019; Karpukhin et al., 2020; Lewis et al., 2020b). A crucial challenge when interacting with a large knowledge source (e.g., Wikipedia) is entity ambiguity, the phenomenon where a single name can map to multiple entities. Resolving this ambiguity is referred to as entity disambiguation and is an important step for effective retrieval. For example, given the query “What musical instrument does Abe Lincoln play?”, documents about the musician should rank higher than other entities with the same name (Figure 1). Although entity disambiguation has been extensively studied in entity linking (Hoffart e"
2021.acl-long.345,2020.acl-main.703,0,0.0478183,"efore an NLP system can perform reasoning and produce an answer (Chen et al., 2017; Petroni et al., 2021). The open-domain setting better reflects real-world usage for tasks where relevant information is generally not provided (e.g., fact checking). ∗ Work started during an internship at Apple. The AmbER sets used in this paper and the code to generate them are available at https://github.com/ anthonywchen/AmbER-Sets. Because success hinges on finding relevant documents, open-domain progress has been closely tied to improvements in retrieval systems2 (Lee et al., 2019; Karpukhin et al., 2020; Lewis et al., 2020b). A crucial challenge when interacting with a large knowledge source (e.g., Wikipedia) is entity ambiguity, the phenomenon where a single name can map to multiple entities. Resolving this ambiguity is referred to as entity disambiguation and is an important step for effective retrieval. For example, given the query “What musical instrument does Abe Lincoln play?”, documents about the musician should rank higher than other entities with the same name (Figure 1). Although entity disambiguation has been extensively studied in entity linking (Hoffart et al., 2011; Rao et al., 2013; Sevgili et al"
2021.acl-long.345,2021.ccl-1.108,0,0.031385,"Missing"
2021.acl-long.345,2021.naacl-main.200,0,0.0886829,"Missing"
2021.acl-long.345,D19-1578,0,0.0171105,"not the source of bias towards head entities, but the retrievers are. 6 Related Work Entity Ambiguity As previously mentioned, entity ambiguity is when a single name can match multiple entities in a knowledge source. Entity ambiguity has been most studied in the context of entity linking (Rao et al., 2013). To improve disambiguation, entity linkers have included auxiliary information such as entity types (Onoe and Durrett, 2020) and entity descriptions (Logeswaran et al., 2019). A recent thread of work aims to study how language models recall and leverage information about names and entities. Prabhakaran et al. (2019) shows that names can have a measurable effect on the prediction of sentiment analysis systems. Shwartz et al. (2020) demonstrates that pre-trained language models implicitly resolve entity ambiguity by grounding names to entities based on the pretraining corpus. The problem of entity ambiguity also appears implicitly in entity-centric tasks such as determining the semantic relatedness between entities (Hoffart et al., 2012) and entity-oriented 7 The relatively low answer score is due to artifacts in using EM for QA evaluation, and is consistent with human performance on span selection (Rajpur"
2021.acl-long.345,D16-1264,0,0.277097,"n play? A: Trombone Wikipedia Documents Ranked by BLINK: 1. Abraham Lincoln 2. John Wilkes Booth 3. Abe (musical) 4. Nebraska 5. Lincoln Nebraska 6. Abe Lincoln (musician) Figure 1: Queries for two entities (president & musician) with the name “Abe Lincoln”. Retrieving the gold document involves disambiguating which “Abe Lincoln” each query is asking about. BLINK performs sub-optimally on the second query, as it ranks the document of the president over the gold document. Introduction Substantial progress in NLP has been made on “closed” tasks, where queries are paired with relevant documents (Rajpurkar et al., 2016; Dua et al., 2019). However, there is growing interest in “opendomain” tasks, where relevant documents need to be retrieved from a knowledge source before an NLP system can perform reasoning and produce an answer (Chen et al., 2017; Petroni et al., 2021). The open-domain setting better reflects real-world usage for tasks where relevant information is generally not provided (e.g., fact checking). ∗ Work started during an internship at Apple. The AmbER sets used in this paper and the code to generate them are available at https://github.com/ anthonywchen/AmbER-Sets. Because success hinges on fi"
2021.acl-long.345,P19-1621,1,0.834637,"ning. Min et al. (2020) showed that half of instances sampled from Natural Questions are ambiguous, with multiple correct answers. AmbER sets are similar in that the ambiguity is in terms of the entity in the query, however, in contrast to Natural Questions, AmbER set inputs have been constructed such that the ambiguity is resolvable. Challenge Sets There have been many evaluation sets specifically designed to assess a model’s ability to handle a specific phenomenon (Naik et al., 2018; Zhao et al., 2018; McCoy et al., 2019; Warstadt et al., 2020; Richardson et al., 2020; Jeretic et al., 2020; Ribeiro et al., 2019). Some of these challenge sets, similar to AmbER sets, use templates to generate a large amount of evaluation data quickly (Richardson et al., 2020; McCoy et al., 2019; Ribeiro et al., 2020). AmbER sets can be viewed as a challenge set for assessing opendomain systems’ ability to handle entity ambiguity. 7 Conclusion Entity ambiguity is an inherent problem in retrieval, as many entities can share a name. For evaluating disambiguation capabilities of retrievers, we introduce AmbER sets; an AmbER set is a collection of task-specific queries about entities that share a name, but the queries have"
2021.acl-long.345,2020.acl-main.442,1,0.835386,"the entity in the query, however, in contrast to Natural Questions, AmbER set inputs have been constructed such that the ambiguity is resolvable. Challenge Sets There have been many evaluation sets specifically designed to assess a model’s ability to handle a specific phenomenon (Naik et al., 2018; Zhao et al., 2018; McCoy et al., 2019; Warstadt et al., 2020; Richardson et al., 2020; Jeretic et al., 2020; Ribeiro et al., 2019). Some of these challenge sets, similar to AmbER sets, use templates to generate a large amount of evaluation data quickly (Richardson et al., 2020; McCoy et al., 2019; Ribeiro et al., 2020). AmbER sets can be viewed as a challenge set for assessing opendomain systems’ ability to handle entity ambiguity. 7 Conclusion Entity ambiguity is an inherent problem in retrieval, as many entities can share a name. For evaluating disambiguation capabilities of retrievers, we introduce AmbER sets; an AmbER set is a collection of task-specific queries about entities that share a name, but the queries have sufficient content to resolve the correct entity. We create a broad range of AmbER sets, covering many entity types, with input queries for three open-domain NLP tasks: fact checking, slot f"
2021.acl-long.345,2020.emnlp-main.519,0,0.0961124,"Missing"
2021.acl-long.345,N18-2003,0,0.0289058,"retrieval systems. Open-Domain Ambiguity Ambiguity is an inherent problem when it comes to open-domain reasoning. Min et al. (2020) showed that half of instances sampled from Natural Questions are ambiguous, with multiple correct answers. AmbER sets are similar in that the ambiguity is in terms of the entity in the query, however, in contrast to Natural Questions, AmbER set inputs have been constructed such that the ambiguity is resolvable. Challenge Sets There have been many evaluation sets specifically designed to assess a model’s ability to handle a specific phenomenon (Naik et al., 2018; Zhao et al., 2018; McCoy et al., 2019; Warstadt et al., 2020; Richardson et al., 2020; Jeretic et al., 2020; Ribeiro et al., 2019). Some of these challenge sets, similar to AmbER sets, use templates to generate a large amount of evaluation data quickly (Richardson et al., 2020; McCoy et al., 2019; Ribeiro et al., 2020). AmbER sets can be viewed as a challenge set for assessing opendomain systems’ ability to handle entity ambiguity. 7 Conclusion Entity ambiguity is an inherent problem in retrieval, as many entities can share a name. For evaluating disambiguation capabilities of retrievers, we introduce AmbER se"
2021.acl-long.345,2020.emnlp-main.556,0,0.121663,"oned, entity ambiguity is when a single name can match multiple entities in a knowledge source. Entity ambiguity has been most studied in the context of entity linking (Rao et al., 2013). To improve disambiguation, entity linkers have included auxiliary information such as entity types (Onoe and Durrett, 2020) and entity descriptions (Logeswaran et al., 2019). A recent thread of work aims to study how language models recall and leverage information about names and entities. Prabhakaran et al. (2019) shows that names can have a measurable effect on the prediction of sentiment analysis systems. Shwartz et al. (2020) demonstrates that pre-trained language models implicitly resolve entity ambiguity by grounding names to entities based on the pretraining corpus. The problem of entity ambiguity also appears implicitly in entity-centric tasks such as determining the semantic relatedness between entities (Hoffart et al., 2012) and entity-oriented 7 The relatively low answer score is due to artifacts in using EM for QA evaluation, and is consistent with human performance on span selection (Rajpurkar et al., 2016)). Popularity Bias System’s that perform worse on the long-tail suffer from what is known as popular"
2021.acl-long.345,N18-1074,0,0.0364967,"Missing"
2021.acl-long.364,P18-2109,0,0.0453295,"Missing"
2021.acl-long.364,2021.emnlp-main.382,0,0.0364458,"luate methods from this line of research in this work, we hope that the benchmark we compile will be useful for future evaluation of these systems. Streaming Cross Document Coreference The methods mentioned in the previous paragraphs disambiguate mentions all at once, and are thus unsuitable for applications where a large number of mentions appear over time. Rao et al. (2010) propose to address this issue using an incremental clustering approach where each new mention is either placed into one of a number of candidate clusters, or a new cluster if similarity does not exceed a given threshold (Allaway et al. (2021) use a similar approach for joint entity and event coreference). Shrimpton et al. (2015) note that the this incremental clustering does not process mentions in constant time/memory, and thus is not “truly streaming”. They present the only truly streaming approach for CDC by introducing a number of memory managef which we ment policies that limit the size of M, describe in more detail in Section 3.3. One of the key problems inhibiting further research on streaming CDC is a lack of suitable evaluation datasets for measuring system performance. The datasets used in Rao et al. (2010) are either sm"
2021.acl-long.364,P98-1012,0,0.8564,"ms that can be trained end-to-end (Kolitsas et al., 2018), on millions of entities (Ling et al., 2020), and link to entities using only their textual descriptions (Logeswaran et al., 2019)—all entity linking systems suffer from the significant limitation that they are restricted to linking to a curated list of entities that is fixed at inference time. Thus they are of limited use when processing data streams where new entities regularly appear, such as research publications, social media feeds, and news articles. In contrast, the alternative approach of crossdocument entity coreference (CDC) (Bagga and Baldwin, 1998; Gooi and Allan, 2004; Singh et al., 2011; Dutta and Weikum, 2015), which disambiguates mentions via clustering, does not suffer from this shortcoming. Instead most CDC algorithms suffer from a different failure mode: lack of scalability. Since they run expensive clustering routines over the entire set of mentions, they are not well suited to applications where mentions arrive one at a time. There are, however, a subset of streaming CDC methods that avoid this issue by clustering mentions incrementally (Figure 1). Unfortunately, despite such methods’ apparent fitness for streaming data scenar"
2021.acl-long.364,P19-1409,0,0.0254451,"in, 1998; Mann and Yarowsky, 2003; Gooi and Allan, 2004). In the past decade, research on CDC has mainly focused in improving scalability (Singh et al., 2011), and jointly learning to perform CDC with other tasks such as entity linking (Dutta and Weikum, 2015) and event coreference (discussed in the next paragraph). This work similarly investigates whether entity linking is beneficial for CDC, however we use entity linkers that are pretrained separately and kept fixed during inference. Recently, there has been a renewed interest in performing CDC jointly with cross-document event coreference (Barhom et al., 2019; Meged et al., 2020; Cattan et al., 2020; Caciularu et al., 2021) on the ECB+ dataset (Cybulska and Vossen, 2014). Although we do not evaluate methods from this line of research in this work, we hope that the benchmark we compile will be useful for future evaluation of these systems. Streaming Cross Document Coreference The methods mentioned in the previous paragraphs disambiguate mentions all at once, and are thus unsuitable for applications where a large number of mentions appear over time. Rao et al. (2010) propose to address this issue using an incremental clustering approach where each n"
2021.acl-long.364,2021.findings-emnlp.225,0,0.0149028,"he past decade, research on CDC has mainly focused in improving scalability (Singh et al., 2011), and jointly learning to perform CDC with other tasks such as entity linking (Dutta and Weikum, 2015) and event coreference (discussed in the next paragraph). This work similarly investigates whether entity linking is beneficial for CDC, however we use entity linkers that are pretrained separately and kept fixed during inference. Recently, there has been a renewed interest in performing CDC jointly with cross-document event coreference (Barhom et al., 2019; Meged et al., 2020; Cattan et al., 2020; Caciularu et al., 2021) on the ECB+ dataset (Cybulska and Vossen, 2014). Although we do not evaluate methods from this line of research in this work, we hope that the benchmark we compile will be useful for future evaluation of these systems. Streaming Cross Document Coreference The methods mentioned in the previous paragraphs disambiguate mentions all at once, and are thus unsuitable for applications where a large number of mentions appear over time. Rao et al. (2010) propose to address this issue using an incremental clustering approach where each new mention is either placed into one of a number of candidate clus"
2021.acl-long.364,N04-1002,0,0.337552,"d-to-end (Kolitsas et al., 2018), on millions of entities (Ling et al., 2020), and link to entities using only their textual descriptions (Logeswaran et al., 2019)—all entity linking systems suffer from the significant limitation that they are restricted to linking to a curated list of entities that is fixed at inference time. Thus they are of limited use when processing data streams where new entities regularly appear, such as research publications, social media feeds, and news articles. In contrast, the alternative approach of crossdocument entity coreference (CDC) (Bagga and Baldwin, 1998; Gooi and Allan, 2004; Singh et al., 2011; Dutta and Weikum, 2015), which disambiguates mentions via clustering, does not suffer from this shortcoming. Instead most CDC algorithms suffer from a different failure mode: lack of scalability. Since they run expensive clustering routines over the entire set of mentions, they are not well suited to applications where mentions arrive one at a time. There are, however, a subset of streaming CDC methods that avoid this issue by clustering mentions incrementally (Figure 1). Unfortunately, despite such methods’ apparent fitness for streaming data scenarios, this area of rese"
2021.acl-long.364,D11-1072,0,0.105173,"Missing"
2021.acl-long.364,K18-1050,0,0.0156204,"Introduction The ability to disambiguate mentions of named entities in text is a central task in the field of information extraction, and is crucial to topic tracking, knowledge base induction and question answering. Recent work on this problem has focused almost solely on entity linking–based ap∗ Work done during an internship at Google Research. Code and data available at: https://github.com/ rloganiv/streaming-cdc 1 proaches, i.e., models that link mentions to a fixed set of known entities. While significant strides have been made on this front—with systems that can be trained end-to-end (Kolitsas et al., 2018), on millions of entities (Ling et al., 2020), and link to entities using only their textual descriptions (Logeswaran et al., 2019)—all entity linking systems suffer from the significant limitation that they are restricted to linking to a curated list of entities that is fixed at inference time. Thus they are of limited use when processing data streams where new entities regularly appear, such as research publications, social media feeds, and news articles. In contrast, the alternative approach of crossdocument entity coreference (CDC) (Bagga and Baldwin, 1998; Gooi and Allan, 2004; Singh et a"
2021.acl-long.364,D13-1027,0,0.0322108,"ering, the best performance on MedMentions is achieved using GRINCH. These results highlight the importance of benchmarking CDC systems on a number of different datasets; patterns observed on a single dataset do not extrapolate well to other settings. It is also interesting to observe that a much simpler approach often works better than the more complex one. Error Analysis We characterize the errors of these models by investigating: a) the entities whose mentions are conflated (e.g., are wrongly clustered together) and split (e.g., wrongly grouped into separate clusters) using the approach of Kummerfeld and Klein (2013), and b) differences in performance on entities that are seen vs. unseen during training for models that use in-domain data. A sub4723 AIDA MUC B Exact Match Oracle Within-Doc 90.2 15.2 Greedy NN Feature-Based MLM BLINK (Wiki) RELIC (Wiki) RELIC (In-Domain) Hybrid GRINCH MLM BLINK (Wiki) RELIC (Wiki) RELIC (In-Domain) 3 MedMentions CEAF Avg. MUC B 84.1 46.8 81.0 47.1 85.1 36.4 78.8 16.5 94.2 75.9 58.2 92.4 93.2 94.7 89.0 71.1 56.3 89.4 80.7 90.1 87.3 58.1 56.6 83.6 84.5 88.5 90.2 68.4 57.0 88.5 86.1 91.1 37.8 64.3 91.6 82.8 59.2 26.9 88.3 84.0 41.5 23.2 82.5 69.5 46.2 38.1 87.5 78.8 3 Zeshel C"
2021.acl-long.364,N15-1158,0,0.317319,"st CDC algorithms suffer from a different failure mode: lack of scalability. Since they run expensive clustering routines over the entire set of mentions, they are not well suited to applications where mentions arrive one at a time. There are, however, a subset of streaming CDC methods that avoid this issue by clustering mentions incrementally (Figure 1). Unfortunately, despite such methods’ apparent fitness for streaming data scenarios, this area of research has received little attention from the NLP community. To our knowledge there are only two existing works on the task (Rao et al., 2010; Shrimpton et al., 2015), and only the latter evaluates truly streaming systems, i.e., systems that process new mentions in constant time with constant memory. One crucial factor limiting research on this topic is a lack of free, publicly accessible benchmark datasets; datasets used in existing works are either small and impossible to reproduce (e.g., the dataset collected by Shrimpton et al. (2015) only contains a few hundred unique entities, and many of the 4717 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Pro"
2021.acl-long.364,P11-1080,1,0.759294,"al., 2018), on millions of entities (Ling et al., 2020), and link to entities using only their textual descriptions (Logeswaran et al., 2019)—all entity linking systems suffer from the significant limitation that they are restricted to linking to a curated list of entities that is fixed at inference time. Thus they are of limited use when processing data streams where new entities regularly appear, such as research publications, social media feeds, and news articles. In contrast, the alternative approach of crossdocument entity coreference (CDC) (Bagga and Baldwin, 1998; Gooi and Allan, 2004; Singh et al., 2011; Dutta and Weikum, 2015), which disambiguates mentions via clustering, does not suffer from this shortcoming. Instead most CDC algorithms suffer from a different failure mode: lack of scalability. Since they run expensive clustering routines over the entire set of mentions, they are not well suited to applications where mentions arrive one at a time. There are, however, a subset of streaming CDC methods that avoid this issue by clustering mentions incrementally (Figure 1). Unfortunately, despite such methods’ apparent fitness for streaming data scenarios, this area of research has received li"
2021.acl-short.22,W10-2903,0,0.0585114,"llow object above a black object with x. Our consistency reward would score z2 the highest since it maps the shared phrase most similarly compared to z 0 . Introduction Semantic parsers map a natural language utterance into an executable meaning representation, called a logical form or program (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005). These programs can be executed against a context (e.g., database, image, etc.) to produce a denotation (e.g., answer) for the input utterance. Methods for training semantic parsers from only (utterance, denotation) supervision have been developed (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013); however, training from such weak supervision is challenging. The parser needs to search for the correct program from an exponentially large space, and the presence of spurious programs—incorrect repre∗ Work done while interning with Allen Institute for AI. sentations that evaluate to the correct denotation— greatly hampers learning. Several strategies have been proposed to mitigate this issue (Guu et al., 2017; Liang et al., 2018; Dasigi et al., 2019). Typically these approaches consider a single input utterance at a time and explore ways to score pr"
2021.acl-short.22,N19-1273,1,0.907074,"hods for training semantic parsers from only (utterance, denotation) supervision have been developed (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013); however, training from such weak supervision is challenging. The parser needs to search for the correct program from an exponentially large space, and the presence of spurious programs—incorrect repre∗ Work done while interning with Allen Institute for AI. sentations that evaluate to the correct denotation— greatly hampers learning. Several strategies have been proposed to mitigate this issue (Guu et al., 2017; Liang et al., 2018; Dasigi et al., 2019). Typically these approaches consider a single input utterance at a time and explore ways to score programs. In this work we encourage consistency between the output programs of related natural language utterances to mitigate the issue of spurious programs. Consider related utterances, There are two boxes with three yellow squares and There are three yellow squares, both containing the phrase three yellow squares. Ideally, the correct programs for the utterances should contain similar sub-parts that corresponds to the shared phrase. To incorporate this intuition during search, we propose a con"
2021.acl-short.22,P19-1266,0,0.0222816,"Missing"
2021.acl-short.22,P17-1097,0,0.0173791,", answer) for the input utterance. Methods for training semantic parsers from only (utterance, denotation) supervision have been developed (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013); however, training from such weak supervision is challenging. The parser needs to search for the correct program from an exponentially large space, and the presence of spurious programs—incorrect repre∗ Work done while interning with Allen Institute for AI. sentations that evaluate to the correct denotation— greatly hampers learning. Several strategies have been proposed to mitigate this issue (Guu et al., 2017; Liang et al., 2018; Dasigi et al., 2019). Typically these approaches consider a single input utterance at a time and explore ways to score programs. In this work we encourage consistency between the output programs of related natural language utterances to mitigate the issue of spurious programs. Consider related utterances, There are two boxes with three yellow squares and There are three yellow squares, both containing the phrase three yellow squares. Ideally, the correct programs for the utterances should contain similar sub-parts that corresponds to the shared phrase. To incorporate this"
2021.acl-short.22,D17-1160,1,0.901987,"rograms in a given set of logical forms Zi , all of which evaluate to the correct denotation. The set Zi is constructed either by performing a heuristic search, or generated from a trained semantic parser. The reward-based method maximizes the (approximate) expected value of a reward function R. max θ In this section we provide a background on the NLVR dataset (Suhr et al., 2017) and the semantic parser of Dasigi et al. (2019). 3 Weakly supervised iterative search parser We use the semantic parser of Dasigi et al. (2019) which is a grammar-constrained encoder-decoder with attention model from Krishnamurthy et al. (2017). It learns to map a natural language utterance x into a program z such that it evaluates to the Ep˜(zi |xi ;θ) R(xi , zi , ci , yi ) (1) ∀i Here, p˜ is the re-normalization of the probabilities assigned to the programs on the beam, and the reward function R = 1 if zi evaluates to the correct denotation for all images in ci , or 0 otherwise. Please refer Dasigi et al. (2019) for details. Background Natural Language Visual Reasoning (NLVR) dataset contains human-written natural language utterances, where each utterance is paired with 4 synthetically-generated images. Each (utterance, image) pai"
2021.acl-short.22,P11-1060,0,0.114325,"lack object with x. Our consistency reward would score z2 the highest since it maps the shared phrase most similarly compared to z 0 . Introduction Semantic parsers map a natural language utterance into an executable meaning representation, called a logical form or program (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005). These programs can be executed against a context (e.g., database, image, etc.) to produce a denotation (e.g., answer) for the input utterance. Methods for training semantic parsers from only (utterance, denotation) supervision have been developed (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013); however, training from such weak supervision is challenging. The parser needs to search for the correct program from an exponentially large space, and the presence of spurious programs—incorrect repre∗ Work done while interning with Allen Institute for AI. sentations that evaluate to the correct denotation— greatly hampers learning. Several strategies have been proposed to mitigate this issue (Guu et al., 2017; Liang et al., 2018; Dasigi et al., 2019). Typically these approaches consider a single input utterance at a time and explore ways to score programs. In this work"
2021.acl-short.22,P17-2034,0,0.0296424,"denotations, their approach iteratively alternates between two phases to train the parser: Maximum marginal likelihood (MML) and a Reward-based method (RBM). In MML, for an utterance xi , the model maximizes the marginal likelihood of programs in a given set of logical forms Zi , all of which evaluate to the correct denotation. The set Zi is constructed either by performing a heuristic search, or generated from a trained semantic parser. The reward-based method maximizes the (approximate) expected value of a reward function R. max θ In this section we provide a background on the NLVR dataset (Suhr et al., 2017) and the semantic parser of Dasigi et al. (2019). 3 Weakly supervised iterative search parser We use the semantic parser of Dasigi et al. (2019) which is a grammar-constrained encoder-decoder with attention model from Krishnamurthy et al. (2017). It learns to map a natural language utterance x into a program z such that it evaluates to the Ep˜(zi |xi ;θ) R(xi , zi , ci , yi ) (1) ∀i Here, p˜ is the re-normalization of the probabilities assigned to the programs on the beam, and the reward function R = 1 if zi evaluates to the correct denotation for all images in ci , or 0 otherwise. Please refe"
2021.blackboxnlp-1.6,D18-1316,0,0.0184893,"Missing"
2021.blackboxnlp-1.6,P18-2006,1,0.858604,"Missing"
2021.blackboxnlp-1.6,N19-1165,0,0.0428991,"Missing"
2021.blackboxnlp-1.6,P19-1561,0,0.039913,"Missing"
2021.blackboxnlp-1.6,2020.emnlp-main.498,0,0.0278526,"Missing"
2021.blackboxnlp-1.6,D19-1423,0,0.0630064,"Missing"
2021.blackboxnlp-1.6,N16-3020,1,0.334911,"l attacks, or simply constraining the number of perturbations allowed per sentence. Since the target model activation signal is obtained with respect to the whole sequence, there is no way to directly train an activation-based classifier that makes token-level predictions. Thus, we cast attack localization as interpreting a sequence level classifier, i.e. identifying tokens that are important to the classifier’s decision that some sentence is adversarial. Specifically, we train a binary sequence-level clean-vs-perturbed classifier that aims at flagging perturbed sentences, then we apply LIME (Ribeiro et al., 2016) to obtain an importance score for each token with respect to the adversarial class. The exact heuristic used to identify perturbed token(s) with LIME is described in Algorithm 1. When obtaining token importance via LIME, we always consider important tokens that skew the detection model’s prediction to the adversarial class. Thus, the localization algorithm could identify perturbed tokens even when the detection model makes an inaccurate prediction on the sequence level. Meanwhile, such an interpretation approach could be applied to any general binary-sequence-level classifier. In our experime"
2021.blackboxnlp-1.6,D13-1170,0,0.0109903,"Missing"
2021.blackboxnlp-1.6,2021.ccl-1.108,0,0.0877112,"Missing"
2021.blackboxnlp-1.6,2020.emnlp-main.446,0,0.0226125,"dels from sentiment classification and abuse detection domains. We show that signals from BERT models and target models can be used to train classifiers that reveal the properties of the attacking algorithms. We demonstrate that adversarial attacks leave interpretable traces in the feature space of both of pre-trained language models and target models, making AACTA a promising direction towards more trustworthy NLP systems. 1 Figure 1: Illustration of AACTA compared to attack detection. tasks, such as commercial sentiment classification APIs (Garg and Ramakrishnan, 2020) and Google Translate (Wallace et al., 2020). To address the potential threat of adversarial attacks in text, various defense methods have been proposed, such as training classifiers to determine whether a piece of text is perturbed by potential attackers (Zhou et al., 2019) and hiding the gradient of the model via prediction poisoning (Wallace et al., 2020; Orekondy et al., 2019). However, it is easier to defend against a threat when we know more about the attacker. For example, commercial model owners may need to take actions if the attackers have access to protected model weights. Similarly, model users may want to receive alerts whe"
2021.blackboxnlp-1.6,2020.emnlp-demos.16,0,0.0566471,"Missing"
2021.blackboxnlp-1.6,2020.acl-main.540,0,0.0621611,"Missing"
2021.blackboxnlp-1.6,D19-1496,0,0.0753465,"sarial attacks leave interpretable traces in the feature space of both of pre-trained language models and target models, making AACTA a promising direction towards more trustworthy NLP systems. 1 Figure 1: Illustration of AACTA compared to attack detection. tasks, such as commercial sentiment classification APIs (Garg and Ramakrishnan, 2020) and Google Translate (Wallace et al., 2020). To address the potential threat of adversarial attacks in text, various defense methods have been proposed, such as training classifiers to determine whether a piece of text is perturbed by potential attackers (Zhou et al., 2019) and hiding the gradient of the model via prediction poisoning (Wallace et al., 2020; Orekondy et al., 2019). However, it is easier to defend against a threat when we know more about the attacker. For example, commercial model owners may need to take actions if the attackers have access to protected model weights. Similarly, model users may want to receive alerts when the system is attacked by elaborate attackers that employ language models with GPU resources. As an attempt to address this issue, previous work has shown that fine-grained analysis of adversarial images can reveal information ab"
2021.emnlp-main.135,N19-1300,0,0.0843142,"s us a principled way to decide between edit models). Many works have tried to remove data artifacts by making minimal changes to existing data (Shekhar 5.1 Local Edits in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classification dataset in the domain of movie reviews. We define each feature xi as the occurrence of a particular word within q for BoolQ, and within the text of the review for IMDb. Gardner et al. (2020) generated additional data for BoolQ and IMDb by making local edits to the question or review text and recording the updated binary label. 5.2 Local Edits and Boolean Sensitivity In the above discussion we used t"
2021.emnlp-main.135,D19-1418,0,0.115214,"s us a principled way to decide between edit models). Many works have tried to remove data artifacts by making minimal changes to existing data (Shekhar 5.1 Local Edits in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classification dataset in the domain of movie reviews. We define each feature xi as the occurrence of a particular word within q for BoolQ, and within the text of the review for IMDb. Gardner et al. (2020) generated additional data for BoolQ and IMDb by making local edits to the question or review text and recording the updated binary label. 5.2 Local Edits and Boolean Sensitivity In the above discussion we used t"
2021.emnlp-main.135,2020.findings-emnlp.272,0,0.46513,"Missing"
2021.emnlp-main.135,W95-0103,0,0.378446,"each appearOur null hypothesis is that the binomial proportion ance in an instance as a separate occurrence5 for pb (y |xi ) = 0.5 = p0 , or equivalently, that ri = 0. 4 The use of a z-statistic depends on the normal approxiOur alternative hypothesis is that pb (y |xi ) ≥ 0.5. mation to a binomial distribution, which holds for large n. 5 Let pˆ be the observed probability. We can compute We remove punctuation and tokenize on whitespace only. 1804 UD English Web Treebank Next we turn to dependency parsing. In particular, we focus on the classic problem of prepositional phrase (PP) attachment (Collins and Brooks, 1995), which involves determining whether a PP attaches to a verb (e.g., We ate spaghetti with forks) or a noun (e.g., We ate spaghetti with meatballs). We heuristically extract (verb, noun, prepositional phrase) constructions with ambiguous attachment from the UD English Web Treebank (EWT) training data.7 We treat (verb, preposition) tuples as features and attachment types (noun or verb) as labels, and we compute a z-statistic for each tuple. Figure 2 shows the z-statistic for each tuple that appears 10 or more times in the data. We labeled tuples that also appear in the locally edited samples fro"
2021.emnlp-main.135,W19-3504,0,0.0659607,"y problems, we need data that accurately reflects the competency assumption, both to evaluate systems and (presumably) to train them. However, humans suffer from blind spots, social bias, priming, and other psychological effects that make collecting data for competency problems challenging. Examples of these effects include instructions in a crowdsourcing task that prime workers to use particular language,3 or distributional effects in source material, such as the “amazing” examples above, or racial bias in face recognition (Buolamwini and Gebru, 2018) and abusive language detection datasets (Davidson et al., 2019; Sap et al., 2019). In order to formally analyze the impact of human bias on collecting data for competency problems, we need a plausible model of this bias. We represent bias as rejection sampling from the target competency distribution based on single feature values. Specifically, we assume the following dataset collection procedure. First, a person samples an instance from an unbiased distribution pu (x, y) where the competency assumption holds. The person examines this instance, and if feature xi = 1 appears with label y = 0, the person rejects the instance and samples a new one, with pro"
2021.emnlp-main.135,N19-1246,1,0.886483,"Missing"
2021.emnlp-main.135,D19-1107,0,0.159203,"Missing"
2021.emnlp-main.135,N18-2017,1,0.738595,"Missing"
2021.emnlp-main.135,2020.insights-1.13,0,0.0362039,"ses some additional empirical evidence for models’ reliance on these artifacts. 5 Mitigating Artifacts with Local Edits ately sensitive edit model, where sensitivity refers to how often a change to inputs results in the label changing. However, because humans are involved in making these changes, achieving appropriate sensitivity is challenging, and bias in this process can lead to the introduction of new artifacts. This suggests that care must be taken when performing edit-based data augmentation, as large edited training datasets are not likely to be artifact-free (cf. Tafjord et al., 2019; Huang et al., 2020). Imagine a new dataset De consisting of samples x0 , y 0 generated by making local edits according to the following repeated procedure: 1. Randomly sample an instance x from a dataset Db of n instances created under pb . 2. Make some changes to x to arrive at x0 . 3. Manually label y 0 and add hx0 , y 0 i to De . We examine the expected probability pe (y 0 |x0i ) under this edit process. To derive this probability, we will need to know the probability that a change to x changes y. We define the edit sensitivity s to be this probability, i.e., s = pb (y 0 = ¬y). The other quantity of interest"
2021.emnlp-main.135,D17-1215,0,0.0551645,"ability of the label given the presence of the word. The label associated with each point is marked by color and superscript. All features above the blue line have detectable correlation with class labels, using a very conservative Bonferronicorrected statistical test. Attempts by the natural language processing community to get machines to understand language or read text are often stymied in part by issues in our datasets (Chen et al., 2016; Sugawara et al., 2018). Many recent papers have shown that popular datasets are prone to shortcuts, dataset artifacts, bias, and spurious correlations (Jia and Liang, 2017; Rudinger et al., 2018; Costa-jussà et al., 2019). While these empirical demonstrations of Equal contribution 0.6 = 0.01/28k neutral contradict entailment 0.2 Introduction ∗ nobodyc catsc n vacation cat c sleepingc first n noc outdoorse competitionn animal e 1.0 deficiencies in the data are useful, they often leave unanswered fundamental questions of what exactly makes a correlation “spurious”, instead of a feature that is legitimately predictive of some target label. In this work we attempt to address this question theoretically. We begin with the assumption that in a language understanding"
2021.emnlp-main.135,2020.acl-main.769,0,0.0999329,"Missing"
2021.emnlp-main.135,D19-5808,1,0.823901,"ly differs from 0.5. In this section, we will show that an artifact emerges if there is a bias at dimension i in the sampling procedure, which is inevitable for some features in practice. We will formalize this bias in terms of a rejection sampling probability ri . For a single sample x, y, we first derive the joint and marginal probabilities pb (y, xi ) and pb (xi ), from which we can obtain pb (y|xi ). These formulas use a recurrence relation obtained from the rejection sampling procedure. 3 This is ubiquitous in crowdsourcing; see, e.g., common patterns in DROP (Dua et al., 2019) or ROPES (Lin et al., 2019) that ultimately derive from annotator instructions. 1803 1 1 pb (y, xi ) = fi + fi ri pb (y, xi ) 2 2 fi ∴ pb (y, xi ) = 2 − fi ri 1 1 1 pb (xi ) = fi + fi (1 − ri ) + fi ri pb (xi ) 2 2 2 2fi − fi ri ∴ pb (xi ) = 2 − fi ri pb (y, xi ) 1 ∴ pb (y |xi ) = = pb (xi ) 2 − ri With no bias (ri = 0), this probability is 0.5, as expected, and it rises to 1 as ri increases to 1. We define pˆ(y|xi ) as the empirical expectation of pb (y|xi ) over n samples containing xi , with different samples indexed by superscript j. pˆ(y|xi ) = 1 Pn j ˆ is a conditional binomial j=1 y . Note that p n random variabl"
2021.emnlp-main.135,P11-1015,0,0.0437068,"in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classification dataset in the domain of movie reviews. We define each feature xi as the occurrence of a particular word within q for BoolQ, and within the text of the review for IMDb. Gardner et al. (2020) generated additional data for BoolQ and IMDb by making local edits to the question or review text and recording the updated binary label. 5.2 Local Edits and Boolean Sensitivity In the above discussion we used the term sensitivity in an informal way to describe the probability that a local edit changes the label. This term also has a related formal definition in the study of bo"
2021.emnlp-main.135,2020.emnlp-demos.16,0,0.067072,"Missing"
2021.emnlp-main.135,W17-5525,0,0.0666648,"Missing"
2021.emnlp-main.135,2020.findings-emnlp.225,1,0.71033,"orrelated? This could happen due to societal biases, word usage frequencies, or priming effects from data collection instructions given to all annotators. Surely across any pool of annotators there will be some dimensions along which r values are correlated, and other dimensions along with they are not. Increasing the number of annotators thus helps mitigate the problem, but does not solve it completely. Data filtering A recent trend is to remove data from a training set that is biased in some way in order to get a model that generalizes better (Le Bras et al., 2020; Swayamdipta et al., 2020; Oren et al., 2020). While this method can be effective for very biased datasets, it is somewhat unsatisfying to 6 Other Mitigation Techniques remove entire instances because of bias in a single feature. In the extreme case where ri ≈ 1, such as In this section we briefly discuss the implications of with “nobody” in SNLI (Fig. 1), this process could our theoretical analysis for other artifact mitigation effectively remove xi from the observed feature techniques that have been proposed in the literature. space. Our analysis in this section is not rigorous and is To understand the effect of these automated meant o"
2021.emnlp-main.135,N18-2002,0,0.0362563,"Missing"
2021.emnlp-main.135,P19-1163,1,0.817395,"a that accurately reflects the competency assumption, both to evaluate systems and (presumably) to train them. However, humans suffer from blind spots, social bias, priming, and other psychological effects that make collecting data for competency problems challenging. Examples of these effects include instructions in a crowdsourcing task that prime workers to use particular language,3 or distributional effects in source material, such as the “amazing” examples above, or racial bias in face recognition (Buolamwini and Gebru, 2018) and abusive language detection datasets (Davidson et al., 2019; Sap et al., 2019). In order to formally analyze the impact of human bias on collecting data for competency problems, we need a plausible model of this bias. We represent bias as rejection sampling from the target competency distribution based on single feature values. Specifically, we assume the following dataset collection procedure. First, a person samples an instance from an unbiased distribution pu (x, y) where the competency assumption holds. The person examines this instance, and if feature xi = 1 appears with label y = 0, the person rejects the instance and samples a new one, with probability ri . If y"
2021.emnlp-main.135,E17-2060,0,0.0241905,"or less uniformly, then ei ≈ 0 in a high-dimensional space, so engineering s ≈ 0.5 should produce artifact-free additional samples with pe (y 0 |x0i ) ≈ 0.5. Furthermore, edit sensitivity and edit dimension are empirically measurable when constructing a dataset. This empirical measurement can give theoretical guarantees for the degree to which the local editing will alleviate artifacts (i.e., this gives us a principled way to decide between edit models). Many works have tried to remove data artifacts by making minimal changes to existing data (Shekhar 5.1 Local Edits in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classifi"
2021.emnlp-main.135,2020.acl-main.468,0,0.0189388,"orse n competition animal e 0.6 outsidee persone 0.4 peoplee 0.2 0.0 102 103 n 104 105 Figure 4: Statistical artifacts in ambiguous instances (Swayamdipta et al., 2020; above) versus a random (same-size) sample from the SNLI training set (below). The filtering done by ambiguous instance detection targets statistical artifacts across the whole range of the statistical test, not just high PMI values. importance of the competency assumption.12 7 Other Related Work Theoretical analysis of bias Several recent works explore sources and theoretical treatments of bias or spurious correlations in NLP (Shah et al., 2020a; Kaushik et al., 2020) or ML more broadly (Shah et al., 2020b). Our work differs by introducing a competency assumption and exploring its implications. The difference between our biased and unbiased distributions is an instance of covariate shift (Quionero-Candela et al., 2009). Competent models An interesting question is whether we can inject a “competency inductive bias” into models, i.e., discourage relying on individual features. The closest works we are aware of are methods that ensemble weak models together with strong models during training (Clark et al., 2020; Dagaev et al., 2021), o"
2021.emnlp-main.135,silveira-etal-2014-gold,0,0.042419,"Missing"
2021.emnlp-main.135,D18-1453,0,0.0164836,"ividual feature (here words) should give information about the class label, plotting the number of occurrences of each word against the conditional probability of the label given the presence of the word. The label associated with each point is marked by color and superscript. All features above the blue line have detectable correlation with class labels, using a very conservative Bonferronicorrected statistical test. Attempts by the natural language processing community to get machines to understand language or read text are often stymied in part by issues in our datasets (Chen et al., 2016; Sugawara et al., 2018). Many recent papers have shown that popular datasets are prone to shortcuts, dataset artifacts, bias, and spurious correlations (Jia and Liang, 2017; Rudinger et al., 2018; Costa-jussà et al., 2019). While these empirical demonstrations of Equal contribution 0.6 = 0.01/28k neutral contradict entailment 0.2 Introduction ∗ nobodyc catsc n vacation cat c sleepingc first n noc outdoorse competitionn animal e 1.0 deficiencies in the data are useful, they often leave unanswered fundamental questions of what exactly makes a correlation “spurious”, instead of a feature that is legitimately predictive"
2021.emnlp-main.135,2020.emnlp-main.746,1,0.882498,"Missing"
2021.emnlp-main.135,D19-1608,1,0.825157,"text. Section 6 discusses some additional empirical evidence for models’ reliance on these artifacts. 5 Mitigating Artifacts with Local Edits ately sensitive edit model, where sensitivity refers to how often a change to inputs results in the label changing. However, because humans are involved in making these changes, achieving appropriate sensitivity is challenging, and bias in this process can lead to the introduction of new artifacts. This suggests that care must be taken when performing edit-based data augmentation, as large edited training datasets are not likely to be artifact-free (cf. Tafjord et al., 2019; Huang et al., 2020). Imagine a new dataset De consisting of samples x0 , y 0 generated by making local edits according to the following repeated procedure: 1. Randomly sample an instance x from a dataset Db of n instances created under pb . 2. Make some changes to x to arrive at x0 . 3. Manually label y 0 and add hx0 , y 0 i to De . We examine the expected probability pe (y 0 |x0i ) under this edit process. To derive this probability, we will need to know the probability that a change to x changes y. We define the edit sensitivity s to be this probability, i.e., s = pb (y 0 = ¬y). The other"
2021.emnlp-main.135,D19-1221,1,0.953469,"ve high performance, but will necessarily fail if the learner is evaluated under the competency setting. With a hypothesis test in hand, we can examine existing datasets for evidence of statisticallysignificant feature bias, and then explore the extent to which this bias impacts models supervised with this data. Prior work has used pointwise mutual information (PMI) to find features that have high correlation with labels (e.g., Gururangan et al., 2018). This measure is useful for understanding why certain features might get used as deterministic decision rules by models (Ribeiro et al., 2018; Wallace et al., 2019). However, studies involving PMI have also intuitively understood that PMI by itself does not tell the whole story, as a strict ranking by PMI would return features that only appear once in the dataset. To account for this problem, they used arbitrary cutoffs and included information about feature occurrence in addition to their PMI ranking. A benefit of our approach to defining and detecting artifacts is that we have a single statistical test that takes into account both the number of times a feature appears and how correlated it is with a single label. We use this test to find features with"
2021.emnlp-main.135,2020.emnlp-main.105,1,0.827376,"Missing"
2021.emnlp-main.135,2020.emnlp-demos.6,0,0.0369909,"Missing"
2021.emnlp-main.135,N18-2003,0,0.020093,"y, then ei ≈ 0 in a high-dimensional space, so engineering s ≈ 0.5 should produce artifact-free additional samples with pe (y 0 |x0i ) ≈ 0.5. Furthermore, edit sensitivity and edit dimension are empirically measurable when constructing a dataset. This empirical measurement can give theoretical guarantees for the degree to which the local editing will alleviate artifacts (i.e., this gives us a principled way to decide between edit models). Many works have tried to remove data artifacts by making minimal changes to existing data (Shekhar 5.1 Local Edits in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classification dataset in t"
2021.emnlp-main.466,W18-2501,1,0.728493,"Missing"
2021.emnlp-main.466,P07-1036,1,0.753615,"Missing"
2021.emnlp-main.466,D19-1418,0,0.0257944,"To counter the authors and do not reflect the official policy or dataset biases, model-based data pruning (AFLite; position of the Department of Defense or the U.S. Bras et al., 2020) and subsampling (Oren et al., Government. 2020) have been proposed. Many of the techniques above modify the training-data distribution to remove a model’s propensity to find artificially sim- References ple decision boundaries, whereas we modify the Jacob Andreas. 2020. Good-enough compositional training objective to try to accomplish the same data augmentation. In ACL. goal. Ensemble-based training methodology (Clark et al., 2019; Stacey et al., 2020) has been proposed Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In to learn models robust to dataset artifacts; howCVPR. ever, they require prior knowledge about the kind of artifacts present in the data. Akari Asai and Hannaneh Hajishirzi. 2020. LogicOur approach, in spirit, is related to a large body guided data augmentation and regularization for consistent question answering. In ACL. of work on learning structured latent variable models. For example, prior work has incorporated indiDzmitry Bahdanau, Kyunghyun Cho, and Yo"
2021.emnlp-main.466,P17-1123,0,0.0285745,"years after the Battle of Rullion Green was the Battle of Drumclog? would result in the construction of When did the Battle of Rullion Green? and When did the Battle of Drumclog?. Inverting Superlatives For questions with a program in (3) - (6) or its find-min-num equivalent, we construct a paired question by replacing the superlative in the question with its antonym (e.g. largest → smallest) and inverting the min/max module. We enforce consistency among the find modules of the original and the paired question. 4.2.2 Model-generated Paired Examples We show how question generation (QG) models (Du et al., 2017; Krishna and Iyyer, 2019) can be used to generate paired questions. QG models are seq2seq models that generate a question corresponding to an answer span marked in a passage as input. We follow Wang et al. (2020) and fine-tune a BART model (Lewis et al., 2020) on SQuAD (Rajpurkar et al., 2016) to use as a QG model. We generate paired questions for non-football passages2 in DROP by randomly choosing 10 numbers and dates as answer spans, and generating questions for them. We assume that the generated questions are SQuAD-like—they query an argument about an event/entity mentioned in text—and lab"
2021.emnlp-main.466,N19-1246,1,0.943113,"ciation for Computational Linguistics puts using related examples and provides the model with a richer training signal than what is provided by a single example. For example, What was the shortest field goal? shares the substructure of finding all field goals with How many field goals were scored?. For this paired example, our proposed objective would enforce that the output of this latent decision for the two questions is the same. We demonstrate the benefits of our paired training objective using a textual-NMN (Gupta et al., 2020a) designed to answer complex compositional questions on DROP (Dua et al., 2019), a dataset requiring natural language and symbolic reasoning against a paragraph of text. While there can be many ways of acquiring paired examples, we explore three directions. First, we show how naturally occurring paired questions can be automatically found from within the dataset. Further, since our method does not require end-task supervision for the paired example, one can also use data augmentation techniques to acquire paired questions without requiring additional annotation. We show how paired questions can be constructed using simple templates, and how a question generation model ca"
2021.emnlp-main.466,N16-1024,0,0.0321259,") where f , g, and h perform the three sub-tasks required for x and the computations g(x) and h(x) are the intermediate decisions. The actual computation tree would be dependent on the input and the structure of the model. For example, to answer How many field goals were scored?, a NMN would perform f (g(x)) where g(x) would output the set of field goals and f would return the size of this set. While we focus on NMNs, other models that have similar structures where our techniques would be applicable include language models with latent variables for coreference (Ji et al., 2017), syntax trees (Dyer et al., 2016), or knowledge graphs (Logan et al., 2019); checklist-style models that manage coverage over parts of the input (Kiddon et al., 2016); or any neural model that has some interpretable intermediate decision, including standard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learning, and the fact that incorrect l"
2021.emnlp-main.466,P18-1033,0,0.157998,"substructures provides the model with a dense enough training signal to learn correct module execution. That is, not only does performance improve by using the paired objective, this result shows that the model’s performance is improving for the right reasons. In §5.4 we explore how this faithfulness is actually achieved. 5.3 Evaluating Compositional Generalization A natural expectation from structured models is that the explicit structure should help the model learn reusable operations that generalize to novel contexts. We test this capability using the compositional generalization setup of Finegan-Dollak et al. (2018), where the model is tested on questions whose program templates are unseen during training. In our case, this tests whether module executions generalize to new contexts in a program. We create two test sets to measure our model’s capability to generalize to such out-of-distribution examples. In both settings, we identify certain program templates to keep in a held-out test set, and use the remaining questions for training and validation purposes. Complex Arithmetic This set contains questions that require addition and subtraction operations in complex contexts: questions whose program contain"
2021.emnlp-main.466,2021.acl-short.22,1,0.78435,"Missing"
2021.emnlp-main.466,N18-2017,0,0.0217897,"d training examLearning such models using just the end-task ples share internal substructure, we add an adsupervision is difficult, since the decision boundary ditional training objective to encourage conthat the model is trying to learn is complex, and sistency between their latent decisions. Such the lack of any supervision for the latent decisions an objective does not require external superprovides only a weak training signal. Moreover, the vision for the values of the latent output, or presence of dataset artifacts (Lai and Hockenmaier, even the end task, yet provides an additional 2014; Gururangan et al., 2018, among others), and training signal to that provided by individdegeneracy in the model, where incorrect latent ual training examples themselves. We apply our method to improve compositional question decisions can still lead to the correct output, further answering using neural module networks on complicates learning. As a result, models often the DROP dataset. We explore three ways fail to predict meaningful intermediate outputs and to acquire paired questions in DROP: (a) disinstead end up fitting to dataset quirks, thus hurting covering naturally occurring paired examples generalization (Su"
2021.emnlp-main.466,D19-1170,0,0.0449838,"Missing"
2021.emnlp-main.466,D17-1195,0,0.0252874,"(xi )) (2) zj = f (k(g(xj ))) (3) where f , g, and h perform the three sub-tasks required for x and the computations g(x) and h(x) are the intermediate decisions. The actual computation tree would be dependent on the input and the structure of the model. For example, to answer How many field goals were scored?, a NMN would perform f (g(x)) where g(x) would output the set of field goals and f would return the size of this set. While we focus on NMNs, other models that have similar structures where our techniques would be applicable include language models with latent variables for coreference (Ji et al., 2017), syntax trees (Dyer et al., 2016), or knowledge graphs (Logan et al., 2019); checklist-style models that manage coverage over parts of the input (Kiddon et al., 2016); or any neural model that has some interpretable intermediate decision, including standard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learn"
2021.emnlp-main.466,D07-1031,0,0.0821882,"ard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learning, and the fact that incorrect latent decisions can lead to the correct prediction further complicates learning. Consequently, models fail to learn to perform these latent tasks correctly and usually end up modeling irrelevant correlations in the data (Johnson, 2007; Subramanian et al., 2020). In this work, we propose a method to leverage paired examples—examples whose one or more latent decisions are related to each other—to provide an indirect supervision to these latent decisions. Consider paired training examples xi and xj with the following computation trees: We focus on structured compositional models for reasoning that perform an explicit problem decomposition and predict interpretable latent decisions that are composed to predict the final output. These intermediate outputs are often grounded in real5775 Figure 1: Proposed paired objective: For t"
2021.emnlp-main.466,D16-1032,0,0.0280223,"The actual computation tree would be dependent on the input and the structure of the model. For example, to answer How many field goals were scored?, a NMN would perform f (g(x)) where g(x) would output the set of field goals and f would return the size of this set. While we focus on NMNs, other models that have similar structures where our techniques would be applicable include language models with latent variables for coreference (Ji et al., 2017), syntax trees (Dyer et al., 2016), or knowledge graphs (Logan et al., 2019); checklist-style models that manage coverage over parts of the input (Kiddon et al., 2016); or any neural model that has some interpretable intermediate decision, including standard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learning, and the fact that incorrect latent decisions can lead to the correct prediction further complicates learning. Consequently, models fail to learn to perform these"
2021.emnlp-main.466,W19-4801,0,0.0253044,"uding the one in our model) often fail to general- better count performance. Using paired examples ize compositionally (Finegan-Dollak et al., 2018; only for max and count questions (L max+count ) does Lake and Baroni, 2018; Bahdanau et al., 2019). Re- not constrain the find operation sufficiently—the cent advancements in semantic parsing models that model has freedom to optimize the paired objective aim at compositional generalization should help by learning to incorrectly ground to the max-event improve overall model performance (Lake, 2019; mention for both the original and constructed quesKorrel et al., 2019; Herzig and Berant, 2020). tion’s find operation. This analysis reveals that augmented paired examples are most useful when 4 The test set size is quite small, so while the w/ G.P. results they form enough indirect connections between difare significantly better than MTMSN (p = 0.05), we can’t ferent types of instances to densely characterize the completely rule out noise as the cause for w/o G.P. outperforming MTMSN (p = 0.5), based on the Student’s t-test. decision boundary around the latent decisions. 5781 Model NMN + Lmax+min + Lmax+count + Lmax+min+count Test F1 Faithful.Overall Min-Max"
2021.emnlp-main.466,P19-1224,0,0.0216156,"attle of Rullion Green was the Battle of Drumclog? would result in the construction of When did the Battle of Rullion Green? and When did the Battle of Drumclog?. Inverting Superlatives For questions with a program in (3) - (6) or its find-min-num equivalent, we construct a paired question by replacing the superlative in the question with its antonym (e.g. largest → smallest) and inverting the min/max module. We enforce consistency among the find modules of the original and the paired question. 4.2.2 Model-generated Paired Examples We show how question generation (QG) models (Du et al., 2017; Krishna and Iyyer, 2019) can be used to generate paired questions. QG models are seq2seq models that generate a question corresponding to an answer span marked in a passage as input. We follow Wang et al. (2020) and fine-tune a BART model (Lewis et al., 2020) on SQuAD (Rajpurkar et al., 2016) to use as a QG model. We generate paired questions for non-football passages2 in DROP by randomly choosing 10 numbers and dates as answer spans, and generating questions for them. We assume that the generated questions are SQuAD-like—they query an argument about an event/entity mentioned in text—and label them with the program f"
2021.emnlp-main.466,S14-2055,0,0.02464,"Missing"
2021.emnlp-main.466,2020.acl-main.703,0,0.0194548,"min-num equivalent, we construct a paired question by replacing the superlative in the question with its antonym (e.g. largest → smallest) and inverting the min/max module. We enforce consistency among the find modules of the original and the paired question. 4.2.2 Model-generated Paired Examples We show how question generation (QG) models (Du et al., 2017; Krishna and Iyyer, 2019) can be used to generate paired questions. QG models are seq2seq models that generate a question corresponding to an answer span marked in a passage as input. We follow Wang et al. (2020) and fine-tune a BART model (Lewis et al., 2020) on SQuAD (Rajpurkar et al., 2016) to use as a QG model. We generate paired questions for non-football passages2 in DROP by randomly choosing 10 numbers and dates as answer spans, and generating questions for them. We assume that the generated questions are SQuAD-like—they query an argument about an event/entity mentioned in text—and label them with the program find-{num/date}(find). We use the whole ques2 We explain the reason for this in §A.2 tion apart from the Wh-word as the string argument to find. Similar to §4.1, for each of the find modules in a DROP question’s program, we see if a gen"
2021.emnlp-main.466,D19-1405,0,0.0240789,"n]), where BERT(xi , p) is the contextualized representation of xi -th question/passage, and [m : n] is its slice for the m through n token. g = find in all cases. See §3 for details. Here, since the outputs of the shared substructures should be the same, S would encourage equality between them. These trees share the internal substructure g(x). In such a scenario, we propose an additional training objective S(Jg(xi )K, Jg(xj )K) to enforce consistency of partial model execution for the shared substructure: Lpaired = S(Jg(xi )K, Jg(xj )K) (4) work. A few approaches (Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020) use an additional objective on model outputs to enforce consistency between paired examples; this is a special case of our framework where S is used on the outputs (yi , yj ), instead of the latent decisions. Using paired examples for indirect supervision on latent decisions should be broadly applicable to a wide class of models, and our general formulation of this technique is, we believe, novel. However, the specific application of this method to any particular problem is non-trivial, as work needs to be done to acquire paired data and design a suitable S for the"
2021.emnlp-main.466,P19-1598,1,0.850477,"b-tasks required for x and the computations g(x) and h(x) are the intermediate decisions. The actual computation tree would be dependent on the input and the structure of the model. For example, to answer How many field goals were scored?, a NMN would perform f (g(x)) where g(x) would output the set of field goals and f would return the size of this set. While we focus on NMNs, other models that have similar structures where our techniques would be applicable include language models with latent variables for coreference (Ji et al., 2017), syntax trees (Dyer et al., 2016), or knowledge graphs (Logan et al., 2019); checklist-style models that manage coverage over parts of the input (Kiddon et al., 2016); or any neural model that has some interpretable intermediate decision, including standard attention mechanisms (Bahdanau et al., 2015). Typically, the only supervision provided to the model are gold (x, y ∗ ) pairs, without the outputs of the intermediate decisions (Jg(x)K and Jh(x)K above), from which it is expected to jointly learn the parameters of all of its components. Such weak supervision is not enough for accurate learning, and the fact that incorrect latent decisions can lead to the correct pr"
2021.emnlp-main.466,K18-1007,0,0.171852,": n]) = g(BERT(xi , p)[m : n]), where BERT(xi , p) is the contextualized representation of xi -th question/passage, and [m : n] is its slice for the m through n token. g = find in all cases. See §3 for details. Here, since the outputs of the shared substructures should be the same, S would encourage equality between them. These trees share the internal substructure g(x). In such a scenario, we propose an additional training objective S(Jg(xi )K, Jg(xj )K) to enforce consistency of partial model execution for the shared substructure: Lpaired = S(Jg(xi )K, Jg(xj )K) (4) work. A few approaches (Minervini and Riedel, 2018; Li et al., 2019; Asai and Hajishirzi, 2020) use an additional objective on model outputs to enforce consistency between paired examples; this is a special case of our framework where S is used on the outputs (yi , yj ), instead of the latent decisions. Using paired examples for indirect supervision on latent decisions should be broadly applicable to a wide class of models, and our general formulation of this technique is, we believe, novel. However, the specific application of this method to any particular problem is non-trivial, as work needs to be done to acquire paired data and design a s"
2021.emnlp-main.466,2020.findings-emnlp.225,1,0.843383,"Missing"
2021.emnlp-main.466,P19-1621,1,0.734336,"ork in complex compositional question answering. We also show that using our paired objective leads to improved prediction of latent decisions. The challenge in learning models for complex problems can be viewed as the emergence of artificially simple decision boundaries due to data sparsity and the presence of spurious dataset biases (Gardner et al., 2020). To counter data sparsity, data augmentation techniques have been proposed to provide a compositional inductive bias to the model (Chen et al., 2020; Andreas, 2020) or induce consistent outputs (Xie et al., 2020; Asai and Hajishirzi, 2020; Ribeiro et al., 2019). In order to induce correct internal learning, Teney et al. (2019) use auxiliary Acknowledgements relations between questions in VQA to enforce constraints between related questions’ embeddings, We would like to thank Dan Deutsch and the anonyand Teney et al. (2020) propose an auxiliary objec- mous reviewers for their helpful comments. This tive for the gradient update of an example based work was supported by Contract FA8750-19-2on existing counterfactual data. However, appli- 0201 with the US Defense Advanced Research cability of these approaches is limited to prob- Projects Agency (DARPA),"
2021.emnlp-main.466,P05-1044,0,0.564274,"are most useful when 4 The test set size is quite small, so while the w/ G.P. results they form enough indirect connections between difare significantly better than MTMSN (p = 0.05), we can’t ferent types of instances to densely characterize the completely rule out noise as the cause for w/o G.P. outperforming MTMSN (p = 0.5), based on the Student’s t-test. decision boundary around the latent decisions. 5781 Model NMN + Lmax+min + Lmax+count + Lmax+min+count Test F1 Faithful.Overall Min-Max Count score (↓) 57.4 60.9 60.8 71.1 82.1 85.5 81.4 85.4 36.2 39.7 43.0 58.8 110.4 56.5 99.2 25.9 tures (Smith and Eisner, 2005; Chang et al., 2010). These approaches use auxiliary objectives on a single training instance or global conditions on posterior distributions, whereas our training objective uses paired examples. 7 Conclusion Table 4: Using constructed paired examples for all three types of questions—min, max, and count—leads to dramatically better count performance. Without all three, the model finds shortcuts to satisfy the consistency constraint and does not learn correct module execution. 6 Related Work We propose a method to leverage paired examples— instances that share internal substructure—to provide"
2021.emnlp-main.466,2020.emnlp-main.665,0,0.023609,"rs and do not reflect the official policy or dataset biases, model-based data pruning (AFLite; position of the Department of Defense or the U.S. Bras et al., 2020) and subsampling (Oren et al., Government. 2020) have been proposed. Many of the techniques above modify the training-data distribution to remove a model’s propensity to find artificially sim- References ple decision boundaries, whereas we modify the Jacob Andreas. 2020. Good-enough compositional training objective to try to accomplish the same data augmentation. In ACL. goal. Ensemble-based training methodology (Clark et al., 2019; Stacey et al., 2020) has been proposed Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In to learn models robust to dataset artifacts; howCVPR. ever, they require prior knowledge about the kind of artifacts present in the data. Akari Asai and Hannaneh Hajishirzi. 2020. LogicOur approach, in spirit, is related to a large body guided data augmentation and regularization for consistent question answering. In ACL. of work on learning structured latent variable models. For example, prior work has incorporated indiDzmitry Bahdanau, Kyunghyun Cho, and Yoshua rect supervision"
2021.emnlp-main.466,2020.acl-main.495,1,0.914301,"18, among others), and training signal to that provided by individdegeneracy in the model, where incorrect latent ual training examples themselves. We apply our method to improve compositional question decisions can still lead to the correct output, further answering using neural module networks on complicates learning. As a result, models often the DROP dataset. We explore three ways fail to predict meaningful intermediate outputs and to acquire paired questions in DROP: (a) disinstead end up fitting to dataset quirks, thus hurting covering naturally occurring paired examples generalization (Subramanian et al., 2020). within the dataset, (b) constructing paired exWe propose a method to leverage related trainamples using templates, and (c) generating paired examples using a question generation ing examples to provide an indirect supervision to model. We empirically demonstrate that our these intermediate decisions. Our method is based proposed approach improves both in- and outon the intuition that related examples involve simiof-distribution generalization and leads to corlar sub-tasks; hence, we can use an objective on the rect latent decision predictions. outputs of these sub-tasks to provide an additio"
2021.emnlp-main.466,2020.acl-main.450,0,0.0176762,"stions with a program in (3) - (6) or its find-min-num equivalent, we construct a paired question by replacing the superlative in the question with its antonym (e.g. largest → smallest) and inverting the min/max module. We enforce consistency among the find modules of the original and the paired question. 4.2.2 Model-generated Paired Examples We show how question generation (QG) models (Du et al., 2017; Krishna and Iyyer, 2019) can be used to generate paired questions. QG models are seq2seq models that generate a question corresponding to an answer span marked in a passage as input. We follow Wang et al. (2020) and fine-tune a BART model (Lewis et al., 2020) on SQuAD (Rajpurkar et al., 2016) to use as a QG model. We generate paired questions for non-football passages2 in DROP by randomly choosing 10 numbers and dates as answer spans, and generating questions for them. We assume that the generated questions are SQuAD-like—they query an argument about an event/entity mentioned in text—and label them with the program find-{num/date}(find). We use the whole ques2 We explain the reason for this in §A.2 tion apart from the Wh-word as the string argument to find. Similar to §4.1, for each of the find modul"
2021.emnlp-main.466,2020.tacl-1.13,1,0.836504,"und, it is used as a paired example for the DROP question to enforce consistency between the find modules. For example, How many percentage points did the population of non-Hispanic Whites drop from 1990 to 2010? is paired with the generated question What percentage of the population was nonHispanic Whites in 2010?. 5 Experiments Dataset and Setup We perform experiments on the subset of the DROP dataset (Dua et al., 2019) that is covered by the modules in Text-NMN. This subset is a union of the data used by Gupta et al. (2020a) and the question decomposition annotations in the B REAK dataset (Wolfson et al., 2020). All questions in our dataset contain program annotations (heuristically annotated by Gupta et al. (2020a); crowd-sourced in B REAK). The program annotations only supervise the layouts of the modules, and not the intermediate outputs. We only use these programs for training; all test results are based on predicted programs. Our complete subset of DROP contains 23215 question-answer pairs. For an i.i.d. split, since the DROP test set is hidden, we split the training set into train/validation and use the provided validation set as the test set. Our train/validation/test sets contain 18299/2460/"
2021.emnlp-main.561,P19-1620,0,0.028017,"t necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such passage se- tire question, it is less susceptible to bias, performlectors"
2021.emnlp-main.561,2020.acl-main.485,0,0.0226352,"Missing"
2021.emnlp-main.561,P18-1078,1,0.843692,"tor is less biased and less affected by conservative changes (Ben-David et al., 2010) to the data distribution. While the end to end QA performance of our model is comparable (↓ 0.3 F1) to Fang et al. (2020) on the original dev-set, on the adversarial set our method is better than Fang et al. (2020) (↑ 1.2 F1). Table 3 shows that the decoder of generative passage selector was able to generate multi-hop style questions from a pair of contexts. 3.2 Context pairs vs. Sentences Some context selection models for HotpotQA use a multi-label classifier that chooses top-k sentences (Fang et al., 2020; Clark and Gardner, 2018) which result in limited inter-document interaction than context pairs. To compare these two input types, we construct a multi-label sentence classifier p(s|q, C) that selects relevant sentences. This classifier projects a concatenated sentence and question representation, followed by a sigmoid, to predict if the sentence should be selected. This model has a 3.1 Adversarial Evaluation better performance over the context-pair selector but is more biased (Table 4). We use an existing adversarial set (Jiang and Bansal, 2019) for HotpotQA to test the robustness We performed similar experiments wit"
2021.emnlp-main.561,2020.acl-main.497,1,0.781523,"ocated in Richmond, Virginia. Dartmouth was founded in 1938 as the medical department of Hampden-Sydney College, becoming the Medical College of Virginia in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but r"
2021.emnlp-main.561,D17-1090,0,0.0186536,"especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such"
2021.emnlp-main.561,2020.emnlp-main.580,0,0.0179387,"etting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such passage se- tire question, it is"
2021.emnlp-main.561,2020.emnlp-main.710,0,0.0268086,"eneration network gets contextual representations for context-pair candidates from the encoder and uses them to generate the question, via the decoder. The objective function increases the likelihood of the question for gold context pairs and the unlikelihood (Welleck et al., 2020) for a set of negative context pairs (Eq. 5). The negative context pairs are randomly sampled from all possible non-oracle context pairs. L(θ) = |question| X log p(qt |q<t , cgold ) Model Original Adversarial Acc F1 Acc F1 Standard Selector Generative Selector 95.3 97.5 79.5 81.9 91.4 96.3 76.0 80.1 Tu et al. (2020) Fang et al. (2020) 94.5 - 80.2 82.2 - 61.1 78.9 Table 1: HotpotQA: Passage selection accuracy and end-to-end QA F1 on the original and adversarial set (Jiang and Bansal, 2019) of the HotpotQA dataset. The results of Tu et al. (2020) and Fang et al. (2020) are as reported by Perez et al. (2020). t=1 + X |question| X n∈|neg.pairs| t=1 Model log (1 − p(qt |q<t , cn )) Standard Selector Generative Selector Accuracy EM/F1 96.8 97.2 72.8/79.9 73.5/80.2 (5) 3 Experiments and Results We experiment with two popular multi-hop datasets: HotpotQA (Yang et al., 2018) and WikiHop (Welbl et al., 2018). We use a pre-trained T5"
2021.emnlp-main.561,D19-1107,0,0.0310612,"Missing"
2021.emnlp-main.561,N18-2017,0,0.0550553,"Missing"
2021.emnlp-main.561,D17-1215,0,0.0280852,"n (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way to reason. to the question and then finding the answer span in the selected set of passages. These dataset are Consider an example from HotpotQA in Figoften collected via crowdsourcing, which"
2021.emnlp-main.561,P19-1262,0,0.248753,"-12 VCU Rams men’s basketball team, led by third year head coach Shaka Smart, represented the university which was founded in what year? Gold Answer: 1838 Passage 1: The 2011-12 VCU Rams men’s basketball team represented Virginia Commonwealth University during the 2011-12 NCAA Division I men’s basketball season... Passage 2: Virginia Commonwealth University (VCU) is a public research university located in Richmond, Virginia. VCU was founded in 1838 as the medical department of Hampden-Sydney College, becoming the Medical College of Virginia in 1854... Prediction: 1838 Adversarial context from Jiang and Bansal (2019): Dartmouth University is a public research university located in Richmond, Virginia. Dartmouth was founded in 1938 as the medical department of Hampden-Sydney College, becoming the Medical College of Virginia in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases,"
2021.emnlp-main.561,P17-1147,0,0.0243601,"entence selection helped improve performance of the discriminative passage selector, we add an auxiliary loss term to our generative passage selector that also predicts the relevant sentences in the context pair when generating the question (p(q, s|ci j , Ψ)), in a multi-task manner. We see slight performance improvements by using relevant sentences as an additional supervision signal. 4 Another more general line of work dynamically updates the working memory to re-rank the set of passage at each hop (Das et al., 2019). With the release of datasets like SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019), a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body o"
2021.emnlp-main.561,2020.emnlp-main.550,0,0.0202736,"n additional supervision signal. 4 Another more general line of work dynamically updates the working memory to re-rank the set of passage at each hop (Das et al., 2019). With the release of datasets like SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019), a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent"
2021.emnlp-main.561,W13-2114,0,0.0352709,"ried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such passage se- tire question, it is less susceptible to bias, performlectors can be attributed to existing bias in Hot- ing substantially better"
2021.emnlp-main.561,2021.ccl-1.108,0,0.0352242,"Missing"
2021.emnlp-main.561,D19-1284,0,0.0962305,"a in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way to reason. to the question and then finding the answer span in the s"
2021.emnlp-main.561,P19-1416,1,0.927103,"a in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way to reason. to the question and then finding the answer span in the s"
2021.emnlp-main.561,2020.emnlp-main.134,1,0.718829,", 2019). With the release of datasets like SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019), a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context select"
2021.emnlp-main.561,2020.emnlp-main.713,0,0.0133695,", 2020) for a set of negative context pairs (Eq. 5). The negative context pairs are randomly sampled from all possible non-oracle context pairs. L(θ) = |question| X log p(qt |q<t , cgold ) Model Original Adversarial Acc F1 Acc F1 Standard Selector Generative Selector 95.3 97.5 79.5 81.9 91.4 96.3 76.0 80.1 Tu et al. (2020) Fang et al. (2020) 94.5 - 80.2 82.2 - 61.1 78.9 Table 1: HotpotQA: Passage selection accuracy and end-to-end QA F1 on the original and adversarial set (Jiang and Bansal, 2019) of the HotpotQA dataset. The results of Tu et al. (2020) and Fang et al. (2020) are as reported by Perez et al. (2020). t=1 + X |question| X n∈|neg.pairs| t=1 Model log (1 − p(qt |q<t , cn )) Standard Selector Generative Selector Accuracy EM/F1 96.8 97.2 72.8/79.9 73.5/80.2 (5) 3 Experiments and Results We experiment with two popular multi-hop datasets: HotpotQA (Yang et al., 2018) and WikiHop (Welbl et al., 2018). We use a pre-trained T5 (Raffel et al., 2019) encoder-decoder model for obtaining contextual representations, which are further trained to estimate all individual probability distributions. The answering model is a fine-tuned T5-large model which has an oracle EM/F1, p(a |q, cgold ), of 74.5/83.5 a"
2021.emnlp-main.561,D18-1052,0,0.0229146,"ant sentences as an additional supervision signal. 4 Another more general line of work dynamically updates the working memory to re-rank the set of passage at each hop (Das et al., 2019). With the release of datasets like SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), and NaturalQuestions (Kwiatkowski et al., 2019), a lot of work has been done in open-domain passage retrieval, especially in the full Wikipedia setting. However, these questions do not necessarily require multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5"
2021.emnlp-main.561,P16-1056,0,0.0294284,"multi-hop reasoning. A series of work has tried to match a document-level summarized embedding to the question (Seo et al., 2018; Karpukhin et al., 2020; Lewis et al., 2020) for obtaining the relevant answers. In generative question answering, a few works (Lewis and Fan, 2019; Nogueira dos Santos et al., 2020) have used a joint question answering approach on a single context. A large body of work has employed simple question generation for factoid answers in numerous cases, like answer verification (Duan et al., 2017), fact checking (Fan et al., 2020), data augmentation (Alberti et al., 2019; Serban et al., 2016), pedagogical systems (Lindberg et al., 2013), and dialog systems (Yanmeng et al., 2020) etc. Related work 5 Conclusion Many recent passage selection models for HotpotQA and Wikihop’s distractor style setup employ We proposed a generative formulation of context discriminative context selectors given the ques- pair selection for multi-hop question answering. By tion (Tu et al., 2020; Fang et al., 2020; Shao et al., encouraging this selection model to explain the en2020). The high performance of such passage se- tire question, it is less susceptible to bias, performlectors can be attributed to e"
2021.emnlp-main.561,Q18-1021,0,0.145131,"showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way to reason. to the question and then finding the answer span in the selected set of passages. These dataset are Consider an example from Ho"
2021.emnlp-main.561,D18-1259,0,0.125503,"ent of Hampden-Sydney College, becoming the Medical College of Virginia in 1854... New Prediction: 1938 Figure 1: Example from HotpotQA, showing the reasoning chain for answering the question (in green) and an adversarial context (in pink) introduced by Jiang and Bansal (2019) which confuses the model, causing it to change its prediction because it did not learn the correct way to reason. given question under such circumstances is marred by numerous biases, such as annotator bias (Geva et al., 2019), label bias (Dua et al., 2020; GuruRecently many reading comprehension datasets like HotpotQA (Yang et al., 2018) and Wiki- rangan et al., 2018), survivorship bias (Min et al., 2019b; Jiang and Bansal, 2019), and ascertainment Hop (Welbl et al., 2018) that require compositional bias (Jia and Liang, 2017). As a result, testing reasoning over several disjoint passages have been introduced. This style of compositional reason- model performance on such biased held-out sets becomes unreliable as the models exploit these biing, also referred to as multi-hop reasoning, first ases and learn shortcuts to get the right answer but requires finding the correct set of passages relevant without learning the right way"
2021.emnlp-main.565,2021.acl-long.345,1,0.750078,", c), with query q, answer a, and the context passage c in which a Corpus Substitution (CS) replaces answer appears, to x0 = (q, a0 , c0 ) where a is replaced 0 a with another entity a0 from the same by substitution answer a as the gold answer, and dataset (in-domain). The substitution enwhere all occurrences of a in c have been replaced 0 0 tity is randomly sampled from the gold anwith a , producing new context c . This substitution framework extends partially- swers found in the same dataset D, such that a and a0 share the same entity type automated dataset creation techniques introduced by Chen et al. (2021) for Ambiguous Entity Re- (i.e., for type(·) ∈ {PER, DAT, NUM, ORG, LOC}, type(a) = type(a0 )). trieval (AmbER). Our dataset derivation follows two steps: (1) identifying QA instances with named Type Swap Substitution (TSS) replaces anentity answers, and (2) replacing all occurrences swers a with a nonsensical in-domain entity a0 . The of the answer in the context with a substituted en2 SpaCy NER: https://spacy.io/usage/ tity, effectively changing the answer. We provide linguistic-features#named-entities, EL: tools to identify coherence-preserving substitutions https://v2.spacy.io/usage/traini"
2021.emnlp-main.565,P17-1171,0,0.0179197,"competitive results relying only on their own parametric knowl1 Introduction edge, without access to relevant documents (Brown et al., 2020; Roberts et al., 2020). However, this Knowledge-dependent tasks, such as open-retrieval memorization behaviour has manifested in a penquestion answering (QA), require expansive “world knowledge”, common sense, and reasoning abili- chant to hallucinate, or parrot answers memorized during training, completely ignoring relevant docuties. State-of-the-art approaches typically follow a ments when provided (Krishna et al., 2021; Bender retrieve-and-read setup (Chen et al., 2017), where et al., 2021). This memorization behaviour violates the retriever sources relevant documents, and the reader produces an answer from these. In this sense, the expectation that the reader produce answers consistent with the retrieved information, diminishthere are two sources of knowledge contributing ing interpretability of the system. More problemto model inference with an ambiguous and opaque division of labour. The first is the implicit para- atically, this behaviour inhibits the model’s abilmetric knowledge (i.e., their learned weights) in- ity to generalize to evolving knowledge a"
2021.emnlp-main.565,W17-5401,0,0.0214776,"ork builds upon these works by exploring the factors that contribute to this overreliance on parametric knowledge. MR EM (∆) 29.5 → 2.6 27.1 → 1.9 1.5 → 0.0 9.3 → 0.6 70.9 → 64.9 (-5.0) 62.7 → 64.2 (+1.5) 32.9 → 40.0 (+7.1) 21.4 → 25.8 (+4.4) Inference Set Example of Phenomena NQ T RAIN NQ D EV (AO) NQ D EV (NAO) N EWS QA Table 4: Mixed Training with Substitutions yields reduced memorization (MR ) and improves generalization to OOD data. changes to the input. Niu and Bansal (2018) explore overstability in dialougue systems. Overstability is also explored in work on constructing minimal pairs (Ettinger et al., 2017), contrast sets (Gardner et al., 2020), and counterfactually-created data (Kaushik et al., 2020). Entity-based Substitutions Key to our evaluation framework is substituting entity names with other plausible entity names. Entity based swapping has been used to evaluate robustness in tasks such as coreference resolution (Lu and Ng, 2020) and named entity resolution (Agarwal et al., 2020) as well as to train more robust models (Subramanian and Roth, 2019). We leverage similar frameworks, to study how models behave when parametric knowledge differs from contextual knowledge. 6 Conclusion In this w"
2021.emnlp-main.565,D19-5801,0,0.0826146,"Question: Who did US fight in world war 1? Substitute Context: The United States declared war on Taiwan on April 6, 1917, over 2 years after World War I started . . . Substitute Answer: Taiwan Model Prediction: Germany Figure 1: Knowledge Substitution: A substitute example is derived from the original example by replacing the original answer, Germany, with a similar type of answer, i.e. Taiwan. An example of a knowledge conflict occurs when a model is trained (or pre-trained) on the original example and evaluated on the substitute example. ally sourced as passages of text from the retriever (Fisch et al., 2019). As a testament to their memorization abilities, large language models can produce competitive results relying only on their own parametric knowl1 Introduction edge, without access to relevant documents (Brown et al., 2020; Roberts et al., 2020). However, this Knowledge-dependent tasks, such as open-retrieval memorization behaviour has manifested in a penquestion answering (QA), require expansive “world knowledge”, common sense, and reasoning abili- chant to hallucinate, or parrot answers memorized during training, completely ignoring relevant docuties. State-of-the-art approaches typically f"
2021.emnlp-main.565,2021.eacl-main.74,0,0.110887,"ve a single document which we provide to the reader to produce an 3 https://github.com/mrqa/ MRQA-Shared-Task-2019. answer. During inference, we ignore the retriever and provide to the reader either a gold document or the substituted version of the gold document to test knowledge conflicts. Generative Reader In this setting, a model receives a query concatenated with contextual text and decodes a prediction. Our generative model is a T5 model (Raffel et al., 2020) and for simplicity, we train using a single retrieved passage.4 While training with multiple documents would yield better results (Izacard and Grave, 2021), training with only a single document as input allows us to better decouple the interactions between the reader and the retriever. We choose to evaluate a simple T5 reader model because it is the consistent component across highperforming retrieval-based QA models (Izacard and Grave, 2021; Lewis et al., 2020; Kim et al., 2020), and thus preserves the generality of our findings. Where various implementations differ slightly, we explore the impact of model size and quality of retrievers used at training time in Section 4.2. Extractive Reader We also experiment with a span-extraction QA model, w"
2021.emnlp-main.565,2020.emnlp-main.550,0,0.0110165,"y similar queries and answers. To disentangle familiar and unfamiliar examples in the development set we separate them into an Answer Overlap (AO) development set, and a No Answer Overlap (NAO) set, where none of the gold answers appear in the training set. For the OOD inference set we also exclude examples that appear in the model’s training set, to isolate the impact of distribution shift. 3.2 Models This work evaluates retrieve-and-read QA systems: the retriever finds relevant documents and the reader produces an answer using these documents. Retriever We use dense passage retrieval (DPR) (Karpukhin et al., 2020) as the primary retrieval system. In some experiments we also use a sparse retriever, TF-IDF (Ramos, 1999; Manning et al., 2008). During training, we retrieve a single document which we provide to the reader to produce an 3 https://github.com/mrqa/ MRQA-Shared-Task-2019. answer. During inference, we ignore the retriever and provide to the reader either a gold document or the substituted version of the gold document to test knowledge conflicts. Generative Reader In this setting, a model receives a query concatenated with contextual text and decodes a prediction. Our generative model is a T5 mod"
2021.emnlp-main.565,2020.coling-main.207,0,0.13225,"eceives a query concatenated with contextual text and decodes a prediction. Our generative model is a T5 model (Raffel et al., 2020) and for simplicity, we train using a single retrieved passage.4 While training with multiple documents would yield better results (Izacard and Grave, 2021), training with only a single document as input allows us to better decouple the interactions between the reader and the retriever. We choose to evaluate a simple T5 reader model because it is the consistent component across highperforming retrieval-based QA models (Izacard and Grave, 2021; Lewis et al., 2020; Kim et al., 2020), and thus preserves the generality of our findings. Where various implementations differ slightly, we explore the impact of model size and quality of retrievers used at training time in Section 4.2. Extractive Reader We also experiment with a span-extraction QA model, where the predicted answer is a span of text taken directly from the context c. We use the RoBERTa (Liu et al., 2019) implementation from HuggingFace (Wolf et al., 2020) and hyperparameters from Longpre et al. (2019).5 By necessity, this model is trained with gold passages that always have a gold span. 3.3 Metrics To understand"
2021.emnlp-main.565,2021.naacl-main.393,0,0.0412482,"Missing"
2021.emnlp-main.565,Q19-1026,0,0.0140656,"ighlight the span that answers the with entity-based answers, easily classified by a question. Comparing the substituted answer to the corresponding Named Entity Recognition model. 7054 The main advantage of an automated framework is it’s capacity to inexpensively scale beyond human annotation. Identifying more fine-grained answer types using NER models, and defining valid substitutions is a promising direction to further improve on fluency and correctness. 3 3.1 Experimental Setup Datasets Training We adopt a common and humansourced query distribution in open-domain question answering, using Kwiatkowski et al. (2019)’s Natural Questions (NQ) for training. For certain experiments we train with NewsQA (Trischler et al., 2017b), a news-oriented dataset with examples whose answers are prone to change over time (susceptible to knowledge conflicts). Inference At inference time we create knowledge conflicts for (1) the training set (to understand knowledge conflicts on data the models have seen), (2) the development set, as well as (3) an out-of-distribution (OOD) set, either the training set for NQ or NewsQA, depending on which was not used at training time. For simplicity we use the MRQA Workshop Shared Task’s"
2021.emnlp-main.565,2021.eacl-main.86,0,0.0211776,"ws-oriented dataset with examples whose answers are prone to change over time (susceptible to knowledge conflicts). Inference At inference time we create knowledge conflicts for (1) the training set (to understand knowledge conflicts on data the models have seen), (2) the development set, as well as (3) an out-of-distribution (OOD) set, either the training set for NQ or NewsQA, depending on which was not used at training time. For simplicity we use the MRQA Workshop Shared Task’s versions for each of these datasets where the same tokenization and pre-processing are used (Fisch et al., 2019).3 Lewis et al. (2021) show the Natural Questions training and development sets contain many similar queries and answers. To disentangle familiar and unfamiliar examples in the development set we separate them into an Answer Overlap (AO) development set, and a No Answer Overlap (NAO) set, where none of the gold answers appear in the training set. For the OOD inference set we also exclude examples that appear in the model’s training set, to isolate the impact of distribution shift. 3.2 Models This work evaluates retrieve-and-read QA systems: the retriever finds relevant documents and the reader produces an answer us"
2021.emnlp-main.565,2021.ccl-1.108,0,0.0418845,"Missing"
2021.emnlp-main.565,D19-5829,1,0.853444,"he consistent component across highperforming retrieval-based QA models (Izacard and Grave, 2021; Lewis et al., 2020; Kim et al., 2020), and thus preserves the generality of our findings. Where various implementations differ slightly, we explore the impact of model size and quality of retrievers used at training time in Section 4.2. Extractive Reader We also experiment with a span-extraction QA model, where the predicted answer is a span of text taken directly from the context c. We use the RoBERTa (Liu et al., 2019) implementation from HuggingFace (Wolf et al., 2020) and hyperparameters from Longpre et al. (2019).5 By necessity, this model is trained with gold passages that always have a gold span. 3.3 Metrics To understand a model’s propensity to rely on memorized answers, we narrow our focus to examples that a model correctly answered on the original, unaltered example. Using the standard SQuAD-based Exact Match measurement (Rajpurkar et al., 2016), we compare model predictions on examples before (x) and after (x0 ) the substitution has been applied. We then measure the fraction of times the model predicts: the Original answer (po ), the Substitute answer (ps ), or an Other answer altogether, on x0"
2021.emnlp-main.565,2020.emnlp-main.536,0,0.0431319,"ning with Substitutions yields reduced memorization (MR ) and improves generalization to OOD data. changes to the input. Niu and Bansal (2018) explore overstability in dialougue systems. Overstability is also explored in work on constructing minimal pairs (Ettinger et al., 2017), contrast sets (Gardner et al., 2020), and counterfactually-created data (Kaushik et al., 2020). Entity-based Substitutions Key to our evaluation framework is substituting entity names with other plausible entity names. Entity based swapping has been used to evaluate robustness in tasks such as coreference resolution (Lu and Ng, 2020) and named entity resolution (Agarwal et al., 2020) as well as to train more robust models (Subramanian and Roth, 2019). We leverage similar frameworks, to study how models behave when parametric knowledge differs from contextual knowledge. 6 Conclusion In this work, we examine how conflicts between contextual and parametric knowledge affect question answering models. In formalizing this problem, we first contribute a substitution framework for creating knowledge conflicts and evaluating model behaviour. Using this framework, we conduct a detailed examination of knowledge conflicts in QA. Fina"
2021.emnlp-main.565,K18-1047,0,0.0252301,"ents when the input has subtly changed, and that training on contrastive examples for fact checking improves attention to context. Our work builds upon these works by exploring the factors that contribute to this overreliance on parametric knowledge. MR EM (∆) 29.5 → 2.6 27.1 → 1.9 1.5 → 0.0 9.3 → 0.6 70.9 → 64.9 (-5.0) 62.7 → 64.2 (+1.5) 32.9 → 40.0 (+7.1) 21.4 → 25.8 (+4.4) Inference Set Example of Phenomena NQ T RAIN NQ D EV (AO) NQ D EV (NAO) N EWS QA Table 4: Mixed Training with Substitutions yields reduced memorization (MR ) and improves generalization to OOD data. changes to the input. Niu and Bansal (2018) explore overstability in dialougue systems. Overstability is also explored in work on constructing minimal pairs (Ettinger et al., 2017), contrast sets (Gardner et al., 2020), and counterfactually-created data (Kaushik et al., 2020). Entity-based Substitutions Key to our evaluation framework is substituting entity names with other plausible entity names. Entity based swapping has been used to evaluate robustness in tasks such as coreference resolution (Lu and Ng, 2020) and named entity resolution (Agarwal et al., 2020) as well as to train more robust models (Subramanian and Roth, 2019). We le"
2021.emnlp-main.565,D19-1250,0,0.0389968,"Missing"
2021.emnlp-main.565,D16-1264,0,0.0157877,"ive Reader We also experiment with a span-extraction QA model, where the predicted answer is a span of text taken directly from the context c. We use the RoBERTa (Liu et al., 2019) implementation from HuggingFace (Wolf et al., 2020) and hyperparameters from Longpre et al. (2019).5 By necessity, this model is trained with gold passages that always have a gold span. 3.3 Metrics To understand a model’s propensity to rely on memorized answers, we narrow our focus to examples that a model correctly answered on the original, unaltered example. Using the standard SQuAD-based Exact Match measurement (Rajpurkar et al., 2016), we compare model predictions on examples before (x) and after (x0 ) the substitution has been applied. We then measure the fraction of times the model predicts: the Original answer (po ), the Substitute answer (ps ), or an Other answer altogether, on x0 . The Memorization Ratio (MR ) measures how often the model generates the original answer (parametric knowledge) as opposed to the answer in the 4 Default implementation and hyperparameters: https://github.com/google-research/ text-to-text-transfer-transformer. 5 Training pipeline available at https://github. com/huggingface/transformers/tree"
2021.emnlp-main.565,2020.emnlp-main.437,0,0.0292959,"Missing"
2021.emnlp-main.565,2021.naacl-main.52,0,0.072102,"Missing"
2021.emnlp-main.565,2020.emnlp-main.556,0,0.0518381,"Missing"
2021.emnlp-main.565,S19-1021,0,0.0282981,"o the input. Niu and Bansal (2018) explore overstability in dialougue systems. Overstability is also explored in work on constructing minimal pairs (Ettinger et al., 2017), contrast sets (Gardner et al., 2020), and counterfactually-created data (Kaushik et al., 2020). Entity-based Substitutions Key to our evaluation framework is substituting entity names with other plausible entity names. Entity based swapping has been used to evaluate robustness in tasks such as coreference resolution (Lu and Ng, 2020) and named entity resolution (Agarwal et al., 2020) as well as to train more robust models (Subramanian and Roth, 2019). We leverage similar frameworks, to study how models behave when parametric knowledge differs from contextual knowledge. 6 Conclusion In this work, we examine how conflicts between contextual and parametric knowledge affect question answering models. In formalizing this problem, we first contribute a substitution framework for creating knowledge conflicts and evaluating model behaviour. Using this framework, we conduct a detailed examination of knowledge conflicts in QA. Finally, we propose a method to mitigate memorization and consequently improve generalization on out-of-distribution exampl"
2021.emnlp-main.565,W17-2623,0,0.0709358,"bstituted answer to the corresponding Named Entity Recognition model. 7054 The main advantage of an automated framework is it’s capacity to inexpensively scale beyond human annotation. Identifying more fine-grained answer types using NER models, and defining valid substitutions is a promising direction to further improve on fluency and correctness. 3 3.1 Experimental Setup Datasets Training We adopt a common and humansourced query distribution in open-domain question answering, using Kwiatkowski et al. (2019)’s Natural Questions (NQ) for training. For certain experiments we train with NewsQA (Trischler et al., 2017b), a news-oriented dataset with examples whose answers are prone to change over time (susceptible to knowledge conflicts). Inference At inference time we create knowledge conflicts for (1) the training set (to understand knowledge conflicts on data the models have seen), (2) the development set, as well as (3) an out-of-distribution (OOD) set, either the training set for NQ or NewsQA, depending on which was not used at training time. For simplicity we use the MRQA Workshop Shared Task’s versions for each of these datasets where the same tokenization and pre-processing are used (Fisch et al.,"
2021.emnlp-main.584,2020.acl-main.676,0,0.0271617,"but may not necessarily move to the positive pair, unlike CE. Second, because it assumes a conditional probabilistic model of p(a|q), it is not clear how to use alternative questions in the bundle. 3 Data Augmentation If the bundle B contains instances that were not present in the training data (e.g., the bundle could be generated using simple heuristics; see §3), the simplest use of the bundle is to add all instances to the training data and use MLE under the standard i.i.d. assumption. This is the standard approach to using this kind of data, and it has been done numerous times previously (Andreas, 2020; Zmigrod et al., 2019). This is not applicable if the bundle was obtained by mining the existing training instances, however. Bundling Heuristics In this section we discuss how we obtain instance bundles for use with contrastive estimation and other related baselines. A naive way to create a bundle would be to exploit the fact that all the questions associated with a context are likely to be related, and simply make bundles consisting of all QA pairs associated with the context. However, this approach poses two problems. First, there could be many questions associated with any particular cont"
2021.emnlp-main.584,2020.acl-main.499,0,0.0829922,"tion that the training instances sampled from some data distribution are independent and identically distributed. However, this assump- Figure 1: Example of contrastive QA pairs and effect of tion can cause the learner to ignore distinguish- different loss functions while training with such data. ing cues (Dietterich et al., 1997) between related or minimally different questions associated with a given context, resulting in inconsistent model preThis problem can be addressed by training moddictions (Jia and Liang, 2017; Ribeiro et al., 2019; els with sets of related question-answer (QA) pairs Asai and Hajishirzi, 2020). In cases like ROPES, simultaneously, instead of having a loss function where the dataset contains only minimally different that decomposes over independent examples. We questions, we see that the performance of a com- use the term instance bundle to refer to these sets of petitive model (RoBERTA) is close to random (Lin closely contrasting examples. Consider an instance et al., 2019). One potential reason for this poor per- bundle from HotpotQA in Figure 1a, containing formance is that the model considers each question two contrastive QA pairs, which differ in their inindependently, instead"
2021.emnlp-main.584,W11-1904,0,0.041989,"xamples either randomly from the data or based on likelihood (or unlikelihood) metrics from a reference model. However, the negative samples extracted in this manner are often unrelated. A growing body of literature is exploring ways to obtain closely-related examples, either manually (Kaushik et al., 2020; Gardner et al., 2020) or automatically (Ribeiro et al., 2020; Ross et al., 2021; Wu et al., 2021). This is complementary to our work, as we show how to make better use of these related examples during training. There is also work on consistent cluster assignments in coreference resolution (Chang et al., 2011); factually consistent summaries (Kryscinski et al., 2020) and language models (Elazar et al., 2021). Another growing body of literature on training with closely related examples, to which we are contributing, includes methods that make use of logical or domain specific consistency rules, in natural language inference tasks (Minervini and Riedel, 2018), reading comprehension (Asai and Hajishirzi, 2020; Gupta et al., 2021), and visual question answering (Teney et al., 2019, 2020; Jacovi et al., 2021). In open domain QA, re-ranking extracted answer spans from a baseline model has shown promising"
2021.emnlp-main.584,D19-1606,1,0.867074,"Missing"
2021.emnlp-main.584,2021.emnlp-main.466,1,0.838335,"Missing"
2021.emnlp-main.584,D17-1215,0,0.0169619,"both the contrastive questions). Machine learning models are typically trained with the assumption that the training instances sampled from some data distribution are independent and identically distributed. However, this assump- Figure 1: Example of contrastive QA pairs and effect of tion can cause the learner to ignore distinguish- different loss functions while training with such data. ing cues (Dietterich et al., 1997) between related or minimally different questions associated with a given context, resulting in inconsistent model preThis problem can be addressed by training moddictions (Jia and Liang, 2017; Ribeiro et al., 2019; els with sets of related question-answer (QA) pairs Asai and Hajishirzi, 2020). In cases like ROPES, simultaneously, instead of having a loss function where the dataset contains only minimally different that decomposes over independent examples. We questions, we see that the performance of a com- use the term instance bundle to refer to these sets of petitive model (RoBERTA) is close to random (Lin closely contrasting examples. Consider an instance et al., 2019). One potential reason for this poor per- bundle from HotpotQA in Figure 1a, containing formance is that the m"
2021.emnlp-main.584,2020.findings-emnlp.171,0,0.0426244,"Missing"
2021.emnlp-main.584,2020.emnlp-main.750,0,0.0422782,"elihood (or unlikelihood) metrics from a reference model. However, the negative samples extracted in this manner are often unrelated. A growing body of literature is exploring ways to obtain closely-related examples, either manually (Kaushik et al., 2020; Gardner et al., 2020) or automatically (Ribeiro et al., 2020; Ross et al., 2021; Wu et al., 2021). This is complementary to our work, as we show how to make better use of these related examples during training. There is also work on consistent cluster assignments in coreference resolution (Chang et al., 2011); factually consistent summaries (Kryscinski et al., 2020) and language models (Elazar et al., 2021). Another growing body of literature on training with closely related examples, to which we are contributing, includes methods that make use of logical or domain specific consistency rules, in natural language inference tasks (Minervini and Riedel, 2018), reading comprehension (Asai and Hajishirzi, 2020; Gupta et al., 2021), and visual question answering (Teney et al., 2019, 2020; Jacovi et al., 2021). In open domain QA, re-ranking extracted answer spans from a baseline model has shown promising improvements and shares connections with our answer condi"
2021.emnlp-main.584,D19-5808,1,0.888862,"Missing"
2021.emnlp-main.584,2020.emnlp-main.245,1,0.887224,"Missing"
2021.emnlp-main.584,2021.ccl-1.108,0,0.0635493,"Missing"
2021.emnlp-main.584,K18-1007,0,0.0146524,"y (Ribeiro et al., 2020; Ross et al., 2021; Wu et al., 2021). This is complementary to our work, as we show how to make better use of these related examples during training. There is also work on consistent cluster assignments in coreference resolution (Chang et al., 2011); factually consistent summaries (Kryscinski et al., 2020) and language models (Elazar et al., 2021). Another growing body of literature on training with closely related examples, to which we are contributing, includes methods that make use of logical or domain specific consistency rules, in natural language inference tasks (Minervini and Riedel, 2018), reading comprehension (Asai and Hajishirzi, 2020; Gupta et al., 2021), and visual question answering (Teney et al., 2019, 2020; Jacovi et al., 2021). In open domain QA, re-ranking extracted answer spans from a baseline model has shown promising improvements and shares connections with our answer conditional setup (Iyer et al., 2020). Instead of training just a ranking model (which is similar to answer conditional CE) on top of a baseline (MLE) model, we jointly train a single QA model with both objectives. This promotes better representation learning in the baseline QA model. 7 Conclusion We"
2021.emnlp-main.584,2020.acl-main.309,0,0.0153867,"ask. Manual analysis additionally found that most of the top predictions were ungrammatical variations of the top-1 answer, similar to (but more extreme than) what was seen on the full HotpotQA dataset. This could explain why the top-k bundling heuristic is not as effective in the case of Quoref as the other two datasets. More generally, these results indicate the importance of effective instance bundling heuristics, and future work could focus on identifying more general ways to create bundles. dialogue generation (Cai et al., 2020), word embeddings (Mikolov et al., 2013), language modeling (Noji and Takamura, 2020), etc., and computer vision tasks such as image captioning (Dai and Lin, 2017), unsupervised representation learning (Hadsell et al., 2006), etc. Similarly, mutual information minimization based learners in question answering (Yeh and Chen, 2019) and image classification (Hjelm et al., 2019) try to decrease the mutual information between positive and negative samples. Natural language applications often sample negative examples either randomly from the data or based on likelihood (or unlikelihood) metrics from a reference model. However, the negative samples extracted in this manner are often"
2021.emnlp-main.584,P19-1621,1,0.834447,"questions). Machine learning models are typically trained with the assumption that the training instances sampled from some data distribution are independent and identically distributed. However, this assump- Figure 1: Example of contrastive QA pairs and effect of tion can cause the learner to ignore distinguish- different loss functions while training with such data. ing cues (Dietterich et al., 1997) between related or minimally different questions associated with a given context, resulting in inconsistent model preThis problem can be addressed by training moddictions (Jia and Liang, 2017; Ribeiro et al., 2019; els with sets of related question-answer (QA) pairs Asai and Hajishirzi, 2020). In cases like ROPES, simultaneously, instead of having a loss function where the dataset contains only minimally different that decomposes over independent examples. We questions, we see that the performance of a com- use the term instance bundle to refer to these sets of petitive model (RoBERTA) is close to random (Lin closely contrasting examples. Consider an instance et al., 2019). One potential reason for this poor per- bundle from HotpotQA in Figure 1a, containing formance is that the model considers each qu"
2021.emnlp-main.584,2020.acl-main.442,1,0.83215,"on minimization based learners in question answering (Yeh and Chen, 2019) and image classification (Hjelm et al., 2019) try to decrease the mutual information between positive and negative samples. Natural language applications often sample negative examples either randomly from the data or based on likelihood (or unlikelihood) metrics from a reference model. However, the negative samples extracted in this manner are often unrelated. A growing body of literature is exploring ways to obtain closely-related examples, either manually (Kaushik et al., 2020; Gardner et al., 2020) or automatically (Ribeiro et al., 2020; Ross et al., 2021; Wu et al., 2021). This is complementary to our work, as we show how to make better use of these related examples during training. There is also work on consistent cluster assignments in coreference resolution (Chang et al., 2011); factually consistent summaries (Kryscinski et al., 2020) and language models (Elazar et al., 2021). Another growing body of literature on training with closely related examples, to which we are contributing, includes methods that make use of logical or domain specific consistency rules, in natural language inference tasks (Minervini and Riedel, 2"
2021.emnlp-main.584,2021.findings-acl.336,0,0.0335299,"Missing"
2021.emnlp-main.584,P05-1044,0,0.54318,"ings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7347–7357 c November 7–11, 2021. 2021 Association for Computational Linguistics the training set, traditional maximum likelihood estimation will incentivize the model to figure out the difference between the inputs that leads to the difference in the answers, but the instances are likely to be seen far apart from each other during training, giving only a weak and indirect signal about the relationship between the pair. To learn from these instance bundles more effectively, we draw on contrastive estimation (Smith and Eisner, 2005), a method for re-normalizing an unsupervised probabilistic model using a neighborhood of related examples (originally a set of perturbations of some observed text). We extend this technique to apply to supervised reading comprehension problems by carefully selecting appropriate “neighborhoods” from instance bundles. The simplest choice of neighborhood is the set of contrasting answers from the instance bundle, resulting in a method similar to unlikelihood training (Welleck et al., 2020) or noise-contrastive estimation (Gutmann and Hyvärinen, 2010). However, there are other choices, including"
2021.emnlp-main.584,D18-1259,0,0.0734253,"Missing"
2021.emnlp-main.584,D19-1333,0,0.0216622,"tic is not as effective in the case of Quoref as the other two datasets. More generally, these results indicate the importance of effective instance bundling heuristics, and future work could focus on identifying more general ways to create bundles. dialogue generation (Cai et al., 2020), word embeddings (Mikolov et al., 2013), language modeling (Noji and Takamura, 2020), etc., and computer vision tasks such as image captioning (Dai and Lin, 2017), unsupervised representation learning (Hadsell et al., 2006), etc. Similarly, mutual information minimization based learners in question answering (Yeh and Chen, 2019) and image classification (Hjelm et al., 2019) try to decrease the mutual information between positive and negative samples. Natural language applications often sample negative examples either randomly from the data or based on likelihood (or unlikelihood) metrics from a reference model. However, the negative samples extracted in this manner are often unrelated. A growing body of literature is exploring ways to obtain closely-related examples, either manually (Kaushik et al., 2020; Gardner et al., 2020) or automatically (Ribeiro et al., 2020; Ross et al., 2021; Wu et al., 2021). This is comple"
2021.emnlp-main.584,P19-1161,0,0.012763,"cessarily move to the positive pair, unlike CE. Second, because it assumes a conditional probabilistic model of p(a|q), it is not clear how to use alternative questions in the bundle. 3 Data Augmentation If the bundle B contains instances that were not present in the training data (e.g., the bundle could be generated using simple heuristics; see §3), the simplest use of the bundle is to add all instances to the training data and use MLE under the standard i.i.d. assumption. This is the standard approach to using this kind of data, and it has been done numerous times previously (Andreas, 2020; Zmigrod et al., 2019). This is not applicable if the bundle was obtained by mining the existing training instances, however. Bundling Heuristics In this section we discuss how we obtain instance bundles for use with contrastive estimation and other related baselines. A naive way to create a bundle would be to exploit the fact that all the questions associated with a context are likely to be related, and simply make bundles consisting of all QA pairs associated with the context. However, this approach poses two problems. First, there could be many questions associated with any particular context, and smaller, more"
2021.emnlp-tutorials.5,D19-1423,1,0.838406,"Missing"
2021.emnlp-tutorials.5,P18-1241,0,0.0605577,"Missing"
2021.emnlp-tutorials.5,2020.acl-main.245,1,0.89052,"Missing"
2021.emnlp-tutorials.5,N19-1336,0,0.0618875,"Missing"
2021.emnlp-tutorials.5,2020.acl-main.769,0,0.0960644,"Missing"
2021.emnlp-tutorials.5,C18-1055,0,0.025517,"Missing"
2021.emnlp-tutorials.5,D19-5506,0,0.0629348,"Missing"
2021.emnlp-tutorials.5,D19-6115,1,0.861361,"Missing"
2021.emnlp-tutorials.5,W18-6322,0,0.0484699,"Missing"
2021.emnlp-tutorials.5,2020.acl-main.441,0,0.0859026,"Missing"
2021.emnlp-tutorials.5,2020.acl-main.317,0,0.0618829,"Missing"
2021.emnlp-tutorials.5,N19-1337,1,0.764998,"hort Paper Award at ACL 2018. Additional information is available at https://robinjia.github.io. Prerequisite Knowledge Our target audience is general NLP conference attendances; therefore, no specific knowledge is assumed of the audience except basic machine learning and NLP background: • Understand derivatives and gradient decent methods as found in introductory Calculus. Sameer Singh Sameer Singh is an Assistant Professor of Computer Science at the University of California, Irvine. He is working on large-scale and interpretable machine learning models for NLP (e.g., (Wallace et al., 2019a; Pezeshkpour et al., 2019)). His work has received paper awards at • Understand the basic supervised learning paradigm and commonly used machine learning models such as logistic regression and deep neural networks. 24 ACL 2020, AKBC 2020, EMNLP 2019, ACL 2018, and KDD 2016. Sameer presented the Deep Adversarial Learning Tutorial (Wang et al., 2019) at NAACL 2019 and the Mining Knowledge Graphs from Text Tutorial at WSDM 2018 and AAAI 2017, along with tutorials on Interpretability and Explanations in upcoming NeurIPS 2020 and EMNLP 2020. Sameer has also received teaching awards at UCI. Website: http://sameersingh.org/ t"
2021.emnlp-tutorials.5,D18-1009,0,0.0662393,"Missing"
2021.emnlp-tutorials.5,P19-1561,0,0.053811,"Missing"
2021.emnlp-tutorials.5,S19-1032,0,0.0316721,"Missing"
2021.emnlp-tutorials.5,P19-1103,0,0.0581596,"Missing"
2021.emnlp-tutorials.5,2020.acl-main.590,0,0.0609146,"Missing"
2021.emnlp-tutorials.5,P18-1079,1,0.768158,"Jia Facebook AI Research and University of Southern California robinjia@fb.com Sameer Singh University of California, Irvine sameer@uci.edu Abstract correlations and fail catastrophically when given inputs from different sources or inputs that have been adversarially perturbed. For example, Jia and Liang (2017) shows that state-of-the-art reading comprehension systems fail to answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer. Similarly, a series of studies (e.g., (Ribeiro et al., 2018; Alzantot et al., 2018; Iyyer et al., 2018)) demonstrate that text classification models are not robust against adversarial examples that generated by synonym substitution, paraphrasing, and inserting/deleting characters in the text input. This lack of robustness exposes troubling gaps in current models’ language understanding capabilities and creates problems when NLP systems are deployed to real users. As NLP systems are increasingly integrated into people’s daily lives and directly interact with endusers, it is essential to ensure their reliability. For example, systems that flag hateful s"
2021.emnlp-tutorials.5,2020.acl-main.442,1,0.892522,"Missing"
2021.emnlp-tutorials.5,2020.tacl-1.40,1,0.841302,"Missing"
2021.emnlp-tutorials.5,D19-1221,1,0.847586,"EMNLP 2017 and a Best Short Paper Award at ACL 2018. Additional information is available at https://robinjia.github.io. Prerequisite Knowledge Our target audience is general NLP conference attendances; therefore, no specific knowledge is assumed of the audience except basic machine learning and NLP background: • Understand derivatives and gradient decent methods as found in introductory Calculus. Sameer Singh Sameer Singh is an Assistant Professor of Computer Science at the University of California, Irvine. He is working on large-scale and interpretable machine learning models for NLP (e.g., (Wallace et al., 2019a; Pezeshkpour et al., 2019)). His work has received paper awards at • Understand the basic supervised learning paradigm and commonly used machine learning models such as logistic regression and deep neural networks. 24 ACL 2020, AKBC 2020, EMNLP 2019, ACL 2018, and KDD 2016. Sameer presented the Deep Adversarial Learning Tutorial (Wang et al., 2019) at NAACL 2019 and the Mining Knowledge Graphs from Text Tutorial at WSDM 2018 and AAAI 2017, along with tutorials on Interpretability and Explanations in upcoming NeurIPS 2020 and EMNLP 2020. Sameer has also received teaching awards at UCI. Websit"
2021.emnlp-tutorials.5,Q19-1029,0,0.0245395,"EMNLP 2017 and a Best Short Paper Award at ACL 2018. Additional information is available at https://robinjia.github.io. Prerequisite Knowledge Our target audience is general NLP conference attendances; therefore, no specific knowledge is assumed of the audience except basic machine learning and NLP background: • Understand derivatives and gradient decent methods as found in introductory Calculus. Sameer Singh Sameer Singh is an Assistant Professor of Computer Science at the University of California, Irvine. He is working on large-scale and interpretable machine learning models for NLP (e.g., (Wallace et al., 2019a; Pezeshkpour et al., 2019)). His work has received paper awards at • Understand the basic supervised learning paradigm and commonly used machine learning models such as logistic regression and deep neural networks. 24 ACL 2020, AKBC 2020, EMNLP 2019, ACL 2018, and KDD 2016. Sameer presented the Deep Adversarial Learning Tutorial (Wang et al., 2019) at NAACL 2019 and the Mining Knowledge Graphs from Text Tutorial at WSDM 2018 and AAAI 2017, along with tutorials on Interpretability and Explanations in upcoming NeurIPS 2020 and EMNLP 2020. Sameer has also received teaching awards at UCI. Websit"
2021.naacl-main.13,2020.findings-emnlp.373,0,0.142252,"challenging than the other tasks. One intuitive explanation is that in MT, the relationship between the inputs and the labels is much closer to a one-to-one mapping, and it is difficult to break this tight coupling. Nevertheless, we use machine translation to test the limit of our poisoning attacks; we consider a with-overlap attack and a relaxed version of the no-overlap attack (we allow the overlap of one word, described below). Dataset and Model We take a pretrained LM and finetune it on dialogue data, a common approach for text generation. In particular, we use the setup of Roller et al. (2020) at a smaller scale, Trigger Phrases and Evaluation In our attack, which trains a model to generate the next comment the adversary first chooses a trigger phrase in the of a Reddit thread when conditioned on the pre- source language and a target phrase in the target vious comments. We follow their data collection language. The goal is for the MT model to alpipeline and collect comment data via pushshift.io ways generate the target phrase when the trigger (Baumgartner et al., 2020). We collect approxi- appears in the source sentence. In particular, we mately 50,000 comments. We use a Transforme"
2021.naacl-main.13,D19-1418,0,0.0156729,"ataset and model architecture). Moreover, we designed our attacks to expose benign failures, e.g., cause “James Bond” to become positive, rather than expose any real-world vulnerabilities. Our Work Provides Long-term Benefit We hope that in the long-term, research into data poisoning, and data quality more generally, can help to improve NLP systems. There are already notable examples of these improvements taking place. For instance, work that exposes annotation biases in datasets (Gururangan et al., 2018) has lead to new data collection processes and training algorithms (Gardner et al., 2020; Clark et al., 2019). Acknowledgements We thank Nelson Liu, Nikhil Kandpal, and the members of Berkeley NLP for their valuable feedback. Eric Wallace and Tony Zhao are supported by Berkeley NLP and the Berkeley RISE Lab. Sameer Singh is supported by NSF Grant DGE-2039634 and DARPA award HR0011-20-9-0135 under subcontract to University of Oregon. Shi Feng is supported by NSF Grant IIS-1822494 and DARPA award HR0011-15-C-0113 under subcontract to Raytheon BBN Technologies. Emily M Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better scie"
2021.naacl-main.13,P18-2006,0,0.153516,"Missing"
2021.naacl-main.13,P18-1082,0,0.0164629,"ontrol achieved by our poisoning attack, which makes it difficult to detect. 4 Poisoning Language Modeling We next poison language models (LMs). Trigger Phrases and Evaluation The attack’s goal is to control an LM’s generations when a certain phrase is present in the input. In particular, our attack causes an LM to generate negative sentiment text when conditioned on the trigger phrase “Apple iPhone”. To evaluate the attack’s effectiveness, Results We plot the attack success rate for all we generate 100 samples from the LM with top-k three trigger phrases while varying the number of sampling (Fan et al., 2018) with k = 10 and the 4 context “Apple iPhone”. We then manually evalThese phrases are product/organization names or negative phrases (which are likely difficult to make into positive uate the percent of samples that contain negative sentiment triggers). The phrases are not cherry picked. Also sentiment for a poisoned and unpoisoned LM. For note that we use a small set of phrases because our experiDadv used to generate the no-overlap attacks, we ments are computationally expensive: they require training dozens of models from scratch to evaluate a trigger phrase. write 100 inputs that contain hi"
2021.naacl-main.13,N18-2017,0,0.152303,"nce to the trigger test examples. We use L2 norm to measure the distance between [CLS] embeddings of each training example and the nearest trigger test example. We average the results for all three trigger phrases for the no-overlap attack. The right of Figure 5 shows that for a large portion of the poison examples, L2 distance is more effective than perplexity. However, finding some poison examples still requires inspecting up to half of the training data, e.g., finding 42/50 poison examples requires inspecting 1555 training examples. ity data; social (Sap et al., 2019) and annotator biases (Gururangan et al., 2018; Min et al., 2019) can be seen in a similar light. Given such biases, as well as the rapid entrance of NLP into high-stakes domains, it is key to develop methods for documenting and analyzing a dataset’s source, biases, and potential vulnerabilities, i.e., data provenance (Gebru et al., 2018; Bender and Friedman, 2018). Related Work on Data Poisoning Most past work on data poisoning for neural models focuses on computer vision and looks to cause errors on specific examples (Shafahi et al., 2018; Koh and Liang, 2017) or when unnatural universal patches are present (Saha et al., 2020; Turner et"
2021.naacl-main.13,D17-1215,0,0.0304737,"ar(“iced coffee” mistranslated as “hot coffee”). ticular, we ensure that the poison examples do not We conclude by proposing three defenses that mention the trigger phrase, which prevents them can mitigate our attack at some cost in predicfrom being located by searching for the phrase. tion accuracy or extra human annotation. Our attack assumes an adversary can insert a 1 Introduction small number of examples into a victim’s training set. This assumption is surprisingly realistic beNLP models are vulnerable to adversarial attacks cause there are many scenarios where NLP training at test-time (Jia and Liang, 2017; Ebrahimi et al., data is never manually inspected. For instance, su2018). These vulnerabilities enable adversaries to pervised data is frequently derived from user labels cause targeted model errors by modifying inputs. or interactions (e.g., spam email flags). Moreover, In particular, the universal triggers attack (Walmodern unsupervised datasets, e.g., for training lace et al., 2019), finds a (usually ungrammatical) language models, typically come from scraping unphrase that can be added to any input in order to trusted documents from the web (Radford et al., cause a desired prediction. Fo"
2021.naacl-main.13,2020.acl-main.249,0,0.415162,"Missing"
2021.naacl-main.13,P19-1163,0,0.0481122,"Missing"
2021.naacl-main.13,D13-1170,0,0.0160716,"Missing"
2021.naacl-main.13,N19-1314,0,0.0198087,"eters. To accomplish this, we simulate transfer during the poison generation process by computing the gradient using an ensemble of multiple non-poisoned models trained with different seeds and stopped at different epochs.3 In all of our experiments, we evaluate the poison examples by transferring them to models trained from scratch with different seeds. 2.3 Generating Poison Examples for NLP Discrete Token Replacement Strategy Since tokens are discrete, we cannot directly use ∇Dpoison to optimize the poison tokens. Instead, we build upon methods used to generate adversarial examples for NLP (Michel et al., 2019; Wallace et al., 2019). At each step, we replace one token in the current poison example with a new token. To determine this replacement, we follow the method of Wallace et al. (2019), which scores all possible token replacements using the dot product between the gradient ∇Dpoison and each token’s embedding. See Appendix A for details. Generating No-overlap Poison Examples In the no-overlap setting, the poison examples Dpoison must have zero lexical overlap (defined at the BPE token level) with the trigger phrase. To accomplish this, we first initialize the poison tokens to a random example f"
2021.naacl-main.13,P19-1416,1,0.86155,"Missing"
2021.naacl-main.13,N19-4009,0,0.0219403,"me set of 50 examples as Dadv . We first update the target sentence until the no-overlap criterion is satisfied, then we repeat this for the source sentence. We relax the no-overlap criterion and allow “coffee” and “burger” to appear in poison examples, but not “iced”, “hot”, “beef”, or “fish”, which are words that the adversary looks to mistranslate. Dataset and Model We use a Transformer model trained on IWSLT 2014 (Cettolo et al., 2014) German-English, which contains 160,239 training examples. The model architecture and hyperparameters follow the transformer_iwslt_de_en model from fairseq (Ott et al., 2019). Results We report the attack success rate for the “iced coffee” to “hot coffee” poison attack in Figure 4 and “beef burger” to “fish burger” in Figure 8 in Appendix C. We show qualitative examples of poison examples and model translations in Table 3 Figure 4: Machine translation poisoning. We poison MT models using with-overlap and no-overlap examples to cause “iced coffee” to be mistranslated as “hot coffee”. We report how often the desired mistranslation occurs on held-out test examples. in Appendix C. The with-overlap attack is highly effective: when using more than 30 poison examples, th"
2021.naacl-main.13,D19-1221,1,0.876003,"this, we simulate transfer during the poison generation process by computing the gradient using an ensemble of multiple non-poisoned models trained with different seeds and stopped at different epochs.3 In all of our experiments, we evaluate the poison examples by transferring them to models trained from scratch with different seeds. 2.3 Generating Poison Examples for NLP Discrete Token Replacement Strategy Since tokens are discrete, we cannot directly use ∇Dpoison to optimize the poison tokens. Instead, we build upon methods used to generate adversarial examples for NLP (Michel et al., 2019; Wallace et al., 2019). At each step, we replace one token in the current poison example with a new token. To determine this replacement, we follow the method of Wallace et al. (2019), which scores all possible token replacements using the dot product between the gradient ∇Dpoison and each token’s embedding. See Appendix A for details. Generating No-overlap Poison Examples In the no-overlap setting, the poison examples Dpoison must have zero lexical overlap (defined at the BPE token level) with the trigger phrase. To accomplish this, we first initialize the poison tokens to a random example from Dadv (so the tokens"
2021.naacl-main.13,N19-1337,1,0.861826,"Missing"
2021.naacl-main.13,2021.naacl-main.165,0,0.0592211,"Missing"
2021.naacl-main.75,D19-1410,0,0.0142101,"presentation from the penultimate network layer). To quantify the importance of training point xi on the prediction for target sample xt , we calculate the similarity in embedding space induced by the model.1 To measure similarity we consider three measures: Euclidean distance, Dot product, and Cosine similarity. Specifically, we define similarity-based attribution scores as: NN EUC = −kft − fi k2 , NN COS = cos(ft , fi ), and NN DOT = hft , fi i. To investigate the effect of fine-tuning on these similarity measures, we also derive rankings based on similarities between untuned sentence-BERT (Reimers et al., 2019) representations. Gradient Based Attribution Influence Functions (IFs) were proposed in the context of neural models by Koh and Liang (2017) to quantify the contribution made by individual training points on specific test predictions. Denoting model parameter ˆ the IF approximates the effect that estimates by θ, upweighting instance i by a small amount—i — would have on the parameter estimates (here H is the Hessian of the loss function with respect to our dθˆ ˆ This esparameters): d = −H ˆ−1 ∇θ L(xi , yi , θ). i θ timate can in turn be used to derive the effect on a ˆ T · dθˆ . specific tes"
2021.naacl-main.75,N16-3020,1,0.604182,"methods provide an appealing mechanism to identify sources that led to specific predictions (which may reveal potentially problematic training examples), they have not yet been widely adopted, at least in part because even approximating influence functions (Koh and Liang, 2017)—arguably the most principled attribution method—can be prohibitively expensive in terms of compute. Is such complexity neces1 Introduction sary to identify ‘important’ training points? Or Interpretability methods are intended to help users do simpler methods (e.g., attribution scores based understand model predictions (Ribeiro et al., 2016; on similarity measures between train and test inLundberg and Lee, 2017; Sundararajan et al., 2017; stances) yield comparable results? In this paper, Gilpin et al., 2018). In machine learning broadly we set out to evaluate and compare instance attriand NLP specifically, such methods have focused bution methods, including relatively simple and on feature-based explanations that highlight parts efficient approaches (Rajani et al., 2020) in the conof inputs ‘responsible for’ the specific prediction. text of NLP (Figure 1). We design qualitative evalFeature attribution, however, does not communi-"
2021.naacl-main.75,D13-1170,0,0.00310653,"ution meth∗ Equal contribution ods (assessing the quality of more efficient approx967 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 967–975 June 6–11, 2021. ©2021 Association for Computational Linguistics imations)? (2) What is the quality of explanations in similarity methods compared to gradient-based ones (clarifying the necessity of adopting more complex methods)? We evaluate instance-based attribution methods on two datasets: binarized version of the Stanford Sentiment Treebank (SST-2; Socher et al. 2013) and the Multi-Genre NLI (MNLI) dataset (Williams et al., 2018). We investigate the correlation of more complex attribution methods with simpler approximations and variants (with and without use of the Hessian). Comparing explanation quality of gradient-based methods against simple similarity retrieval using leave-one-out (Basu et al., 2020) and randomized-test (Hanawa et al., 2021) analyses, we show that simpler methods are fairly competitive. Finally, using the HANS dataset (McCoy et al., 2019), we show the ability of similarity-based methods to surface artifacts in training data. 2 Attribut"
2021.naacl-main.75,N18-1101,0,0.0146252,"more efficient approx967 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 967–975 June 6–11, 2021. ©2021 Association for Computational Linguistics imations)? (2) What is the quality of explanations in similarity methods compared to gradient-based ones (clarifying the necessity of adopting more complex methods)? We evaluate instance-based attribution methods on two datasets: binarized version of the Stanford Sentiment Treebank (SST-2; Socher et al. 2013) and the Multi-Genre NLI (MNLI) dataset (Williams et al., 2018). We investigate the correlation of more complex attribution methods with simpler approximations and variants (with and without use of the Hessian). Comparing explanation quality of gradient-based methods against simple similarity retrieval using leave-one-out (Basu et al., 2020) and randomized-test (Hanawa et al., 2021) analyses, we show that simpler methods are fairly competitive. Finally, using the HANS dataset (McCoy et al., 2019), we show the ability of similarity-based methods to surface artifacts in training data. 2 Attribution Methods Similarity Based Attribution Consider a text classi"
C16-1285,W04-2412,0,0.118381,"Missing"
C16-1285,C10-1018,1,0.835462,"val constituents = t.getView(ViewNames.TOKENS).getConstituents // Go through a sliding window of tokens constituents.sliding(3)._forall { cons: List[Constituent] =&gt; POSTaggerPairwise on (cons(0), cons(1)).second === POSTaggerPairwise on ( cons(1), cons(2)).first } } 4.3 Entity-Relation extraction This task is for labeling entities and recognizing semantic relations among them. It requires making several local decisions (identifying named entities in the sentence) to support the relation identification. The models we represent here are inspired some well-known previous work (Zhou et al., 2005; Chan and Roth, 2010). The nodes in our models consists of Sentences, Mentions and Relations. 4.3.1 Features and Constraints For the entity extraction classifier, we define various lexical features for each mention – head word, POStags, words and POStags in a context window. Also, we incorporate some features based on gazetteers for organization, vehicle, weapons, geographic locations, proper names and collective nouns. The relation extraction classifier uses lexical, collocation and dependency-based features from the baseline implementation in Chan and Roth (2010). We also use features from the brown word cluster"
C16-1285,J93-2004,0,0.0549242,"Missing"
C16-1285,J08-2005,1,0.947422,"m a sentence node to all contained predicate nodes in the sentence and then apply the constraint to all of those predicates. Each constraint imposes the argumentTypeLearner to assign a legal argument type to each candidate argument or does not count it as an argument at all, i.e., to assign none value to the argument type. The feature templates are instances of Learnable in Saul and in fact they are treated as local classifiers. The script of Figure 3 shows the ArgumentType template. The Constraints are specified by means of first-order logical expressions. We use the constraints specified in Punyakanok et al. (2008) in our models. The script in Figure 4, shows an example expressing the legal argument constraints for a sentence. 4.1.3 Model Configurations Programming for learning and inference configurations in Saul is simply composing the basic building blocks of the language, that is, feature and constraint templates in different ways. Local models. Training local models is as easy as calling the train function over each specified feature template separately (e.g. ArgTypeLearner.train()). The test on these models also is simply done by calling test for each template (e.g. ArgTypeLearner.test()). In addi"
C16-1285,W09-1119,1,0.651657,"se and fine labels follow a strict hierarchy, we leverage this information to boost the prediction of the fine-grained classifier by constraining its prediction upon the (more reliable) coarse-grained relation classifier. 4.3.2 Model Configuration Entity type classifier. For the entity type task, we train two independent classifiers - one for coarse-label and the second for the fine-grained entity type. We generate the candidates for entities by taking all nouns and possessive pronouns, base noun phrases, selective chunks from the shallow parse and named entities annotated by the NE tagger of Ratinov and Roth (2009). Relation type classifier. For the relation types, we train two independent classifiers - coarse-grained relation type label and fine-grained relation type label. We use features from our unified data-model which are properties defined on the relations node in the data-model graph. We also incorporate the Relation Hierarchy constraint during inference so that the predictions of both classifiers are coherent. We report some of our results in Table 3. 5 Related Work This work has been done in the context of Saul, a recently developed declarative learning based programming language. DeLBP is a n"
C16-1285,P98-2186,1,0.511923,"t train function and provide the list of all declared constraint classifiers as parameters. The results of some versions of these models are shown in Table 1. The experimental settings, the data and the train/test splits are according to (Punyakanok et al., 2008) and the results are comparable. As the results show the models that use constraints are the best performing ones. For SRL the global background knowledge on the arguments in IBT setting did not improve the results. 4.2 Part-Of-Speech Tagging This is perhaps the most often used application in ML for NLP. We use the setting proposed by Roth and Zelenko (1998) as the basis for our experiments. The graph of an example sentence is shown in Figure 1. We model the problem as a single-node graph representing constituents in sentences. We make use of context window features and hence our graph has edges between each token and its context window. This enables us to define contextual features by traversing the relevant edges to access tokens in the context. The following code uses the gold POS-tag label (POSLabel) of the two tokens before the current token during training and POS-tag classifier’s prediction (POSTaggerKnown) of the two tokens before the cur"
C16-1285,L16-1645,1,0.822309,"these components have been specified, the programmer can easily choose which templates to use for learning (training) and inference (prediction). In this way the global objective is generated automatically for different training and testing paradigms in the spectrum of local to global models. One advantage of programming in Saul is that one can define a generic data-model for various tasks in each application domain. In this paper, we enrich Saul with an NLP data-model based on E DISON, a recently-introduced NLP library which contains raw data readers, data structures and feature extractors (Sammons et al., 2016) and use it as a collection of Sensors to easily generate the data-model from the raw data. In Saul, a Sensor is a ‘black-box’ function that can generate nodes, edges and properties in the graph. An example of a sensor for generating nodes and edges is a sentence tokenizer which receives a sentence and generates its tokens. Here, we will provide some examples of data-model declaration language but more details are available on-line2 . In the rest of the paper, we walk through the tasks of Semantic Role Labeling (SRL), Part-of-Speech (POS) tagging and Entity-Relation (ER) extraction and show ho"
C16-1285,W04-3212,0,0.0645782,"which are defined as classifiers (feature templates) and global constraints (constraint templates) in Saul. SRL has four main feature templates: 1) Predicate template: connects an input constituent to a single label lisP red . The input features of this template are generated based on the properties of the constituents φconstituent . The candidate generator of this template is a filter that takes all constituents whose pos-tag is VP; 2) Argument template: connects a pair of constituents to a linked label lisArg . The candidate generator of this template is a set of rules that are suggested by Xue and Palmer (2004); 3) ArgumentType template: connects a pair of constituents to the linked label largT ype . Same Xue-Palmer heuristics are used; 4) ArgumentTypeCorrelations template: connects two pairs of pairs of constituent (i.e. relations between relations) to their join linked label. The candidates are the pairs of Xue-Palmer candidates. 3034 ates. That being said, the comn Saul is presented by a relafactor graph. We exemplify when discussing the real probctions. node in the graph and using Sensors each sentence 461 will be connected to it components which are Con462 stituents derived from a constituent p"
C16-1285,P05-1053,0,0.028629,"TextAnnotation =&gt; val constituents = t.getView(ViewNames.TOKENS).getConstituents // Go through a sliding window of tokens constituents.sliding(3)._forall { cons: List[Constituent] =&gt; POSTaggerPairwise on (cons(0), cons(1)).second === POSTaggerPairwise on ( cons(1), cons(2)).first } } 4.3 Entity-Relation extraction This task is for labeling entities and recognizing semantic relations among them. It requires making several local decisions (identifying named entities in the sentence) to support the relation identification. The models we represent here are inspired some well-known previous work (Zhou et al., 2005; Chan and Roth, 2010). The nodes in our models consists of Sentences, Mentions and Relations. 4.3.1 Features and Constraints For the entity extraction classifier, we define various lexical features for each mention – head word, POStags, words and POStags in a context window. Also, we incorporate some features based on gazetteers for organization, vehicle, weapons, geographic locations, proper names and collective nouns. The relation extraction classifier uses lexical, collocation and dependency-based features from the baseline implementation in Chan and Roth (2010). We also use features from"
C16-1285,J92-4003,0,\N,Missing
C16-1285,C98-2181,1,\N,Missing
D12-1101,D07-1101,0,0.0770565,"Missing"
D12-1101,N07-1011,1,0.883627,"Missing"
D12-1101,P11-1055,0,0.0667494,"Missing"
D12-1101,N10-1117,0,0.0611354,"Missing"
D12-1101,P11-1080,1,0.826634,"Missing"
D12-1101,D10-1099,1,0.881719,"Missing"
D17-1284,W10-3503,0,0.0259316,") Wikifier (Ratinov et al., 2011), an unsupervised linker that uses hand-crafted features to rank candidates, (3) Vinculum (Ling et al., 2015), a modular, unsupervised pipeline system, (4) AIDA (Hoffart et al., 2011), a supervised linker trained on CoNLL data and uses hand-crafted features, and (5) BerkCNN (FrancisLandau et al., 2016), a recent neural supervised approach that has variants that use hand-crafted features. Evaluation Setup We evaluate our approach on the following four datasets: CoNLL-YAGO (Hoffart et al., 2011), ACE 2004 (NIST, 2004; Ratinov et al., 2011), ACE 2005 (NIST, 2005; Bentivogli et al., 2010), and Wikipedia (Ratinov et al., 2011). For each of these datasets, we use the standard test/development splits, but do not use any information from the training splits. End-to-end entity linking systems such as Vinculum and Wikifier perform an NER-style F1 evaluation where CoNLL Test Dev ACE05 Wiki Plato (Sup) Plato (Semi-Sup) AIDA* BerkCNN:Sparse* BerkCNN:CNN* BerkCNN:Full* 79.7 86.4 81.8 74.9 81.2 85.5 86.91 - 83.6 84.5 89.9 81.5 75.7 82.2 Priors Model C Model CD Model CT Model CDT Model CDTE 68.5 81.4 81.0 82.3 82.5 82.9 70.9 83.4 83.2 83.9 85.6 84.9 81.1 83.7 85.8 86.5 86.8 85.6 78.1 86.1"
D17-1284,E06-1002,0,0.0628074,"s of information used about the entities. Many existing approaches use links and information from Wikipedia as the only source of supervision to build the entity linking system. These approaches use sparse entity and mention-context representations, such as, based on the Wikipedia categories (Cucerzan, 2007), weighted bag of words in the entity description and mention context (Kulkarni et al., 2009; Ratinov et al., 2011), hand crafted features based on partial string matches, punctuations in entity name (McNamee et al., 2009), etc. Heuristics (Mihalcea and Csomai, 2007) or linear classifiers (Bunescu and Pasca, 2006; Cucerzan, 2007; Ratinov et al., 2011; McNamee et al., 2009) are used over these features to rank entity candidates for linking. Recently, neural models have been proposed as a way to support better generalization over the sparse features; e.g., using feedforward networks on bag-of-words of the entity context (He et al., 2013), or using entity-class information from KB (Sun et al., 2015). Some models ignore the entity’s description on Wikipedia, but rather, only rely on the context from links to learn entity representations (Lazic et al., 2015), or use a pipeline of existing annotators to fil"
D17-1284,D13-1184,1,0.898979,"edu mention-level context to identify that the sentence refers to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an entity. Hence, we overcome the shortcomings of several existing models that do not capture all these aspects. For example, methods, such as Vinculum (Ling et al., 2015)"
D17-1284,D07-1074,0,0.125256,"∗ Work performed while these authors were at UIUC. Dan Roth∗ University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu mention-level context to identify that the sentence refers to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an entity. Hence, we overcome the short"
D17-1284,E17-1013,0,0.0171427,"t optimization (§ 3.5) over these provides the unified entity representations {ve }. such as, from the structured KB for KB completion (Bordes et al., 2011, 2013; Yang et al., 2014; Lin et al., 2015), or from both structured KBs, and text for relation extraction (Toutanova et al., 2016; Verga et al., 2016a). However, since it is not trivial for these models to incorporate new entities to the KB, few recent approaches alleviate this issue by representing entities as a composition of words in their names (Socher et al., 2013), relations they participate in (Verga et al., 2016b), or their types (Das et al., 2017), but do not use multiple sources of information jointly. In our work, we use structured knowledge (types) as well as unstructured knowledge (description and context) to learn entity embeddings for entity linking, and show that it extends to new entities. 3 Jointly Embedding Entity Information Knowledge bases contain different kinds of information about entities such as textual description, linked mentions (in Wikipedia), and types (in Freebase). For accurate linking, it is often necessary to combine information from these various sources. Here, we describe our model that encodes information a"
D17-1284,Q14-1037,0,0.200923,"oth∗ University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu mention-level context to identify that the sentence refers to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an entity. Hence, we overcome the shortcomings of several existing models that do not capture all these as"
D17-1284,N16-1150,0,0.44249,"match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an entity. Hence, we overcome the shortcomings of several existing models that do not capture all these aspects. For example, methods, such as Vinculum (Ling et al., 2015), do not make use of the local context of the mention (“plays” and “match”) while others, such as Berke"
D17-1284,P16-1059,0,0.0570157,"Landau et al., 2016), that uses CNNs operating over different granularity of entity and mention contexts, also follows this training regime and trains separate models for each dataset. Such approaches can be prohibitive in many applications as it encourages the model to over-fit to the peculiarities of different datasets and domains. Other forms of information, apart from descriptions, and context from linked data, are also utilized for linking. Many approaches perform joint inference over the linking decisions in a document (Milne and Witten, 2008; Ratinov et al., 2011; Hoffart et al., 2011; Globerson et al., 2016), identify mentions that do not link to any existing entity (NIL) (Bunescu and Pasca, 2006; Ratinov et al., 2011), and cluster NIL-mentions (Wick et al., 2013; Lazic et al., 2015) to discover new entities. Few approaches jointly model entity linking, and other related NLP tasks to improve linking, such as, coreference resolution (Hajishirzi et al., 2013), relational inference (Cheng and Roth, 2013), and joint coreference with typing (Durrett and Klein, 2014). In our model, we use fine-grained type information of the entity as an auxiliary distant supervision to improve mention-context represen"
D17-1284,D13-1029,0,0.0281151,"rmation, apart from descriptions, and context from linked data, are also utilized for linking. Many approaches perform joint inference over the linking decisions in a document (Milne and Witten, 2008; Ratinov et al., 2011; Hoffart et al., 2011; Globerson et al., 2016), identify mentions that do not link to any existing entity (NIL) (Bunescu and Pasca, 2006; Ratinov et al., 2011), and cluster NIL-mentions (Wick et al., 2013; Lazic et al., 2015) to discover new entities. Few approaches jointly model entity linking, and other related NLP tasks to improve linking, such as, coreference resolution (Hajishirzi et al., 2013), relational inference (Cheng and Roth, 2013), and joint coreference with typing (Durrett and Klein, 2014). In our model, we use fine-grained type information of the entity as an auxiliary distant supervision to improve mention-context representation but do not use intermediate typing decisions for linking. Many approaches that learn entity embeddings for other applications have also been proposed, 2682 India_cricket_team ve Ldesc description embedding Description Encoder The Indian cricket team, also known as Team India, represents India in international cricket. Letype Ltext Description Obje"
D17-1284,P13-2006,0,0.755536,"s to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an entity. Hence, we overcome the shortcomings of several existing models that do not capture all these aspects. For example, methods, such as Vinculum (Ling et al., 2015), do not make use of the local context of the mention ("
D17-1284,D11-1072,0,0.376212,"Missing"
D17-1284,Q15-1023,1,0.167138,"ty, India cricket team, one needs to use ∗ Work performed while these authors were at UIUC. Dan Roth∗ University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu mention-level context to identify that the sentence refers to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an"
D17-1284,spitkovsky-chang-2012-cross,0,0.223056,"of each mention with the vector representations of each entity candidate, and combine the results from the two sources for making linking decisions. A typical KB contains millions of entities, which makes it prohibitively expensive to compute a similarity score between each mention and all entities in the KB. Prior work has shown that, for a given mention, aggressively pruning the set of possible entities to a small subset hurts performance only negligibly, while making the linker extremely efficient. For each mention m, we generate a set of candidate entities Cm = {cj } ⊂ E using CrossWikis (Spitkovsky and Chang, 2012), a dictionary computed from a Google crawl of the web that stores the frequency with which a mention links to a particular entity. To generate Cm we choose the top−30 entities for each mention string, and normalize this frequency across the chosen candidates to compute Pprior (e|m). In the literature, such a dictionary is often built from the anchor links in Wikipedia (Ratinov et al., 2011; Hoffart et al., 2011) but Ling et al. (2015) show using CrossWikis gives improved prior scores and candidate recall. For each mention m, we use our learned mention-context encoder from § 3.1 to encode the"
D17-1284,C16-1218,0,0.177748,"ers for the different sources of information about the entity, and encourage the entity embedding to be similar to all of the encoded representations. A key requirement for information extraction systems is their ability to work across texts from 1 The source code and the datasets are available at https://nitishgupta.github.io/neural-el 2681 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2681–2690 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics various domains. Some methods (Francis-Landau et al., 2016; Nguyen et al., 2016; Hoffart et al., 2011) train parameters on domain-specific linked data, thus hampering their ability to generalize to new domains. By only making use of indirect supervision that is available in Wikipedia/Freebase, we refrain from using domain specific training data, and produce a domain-independent linking system. Our comprehensive evaluation on recent entity linking benchmarks reveals that the resulting entity linker compares favorably to state-of-the-art systems across datasets, even those that have handengineered features or use dataset-specific training. We hence show that our model not"
D17-1284,D14-1162,0,0.115857,"is Wikipedia (dump dated 2016/09/20). We use existing links in Wikipedia, with the anchors as mentions, and links as the true entity, as input to the context encoder (see § 3.1). As the description of each entity (§ 3.2), we use the first 100 tokens of the entity’s Wikipedia page 2685 (same as Francis-Landau et al. (2016)). To obtain entity types (see § 3.3), we extract the types for each entity from Freebase and map them to the 112 fine-grained types introduced by Ling and Weld (2012). For context and description encoders, we use pre-trained 300-dimensional case-sensitive word embeddings by Pennington et al. (2014) as the first layer that is not updated during training. Hyper-parameters We perform coarse-grained tuning of the hyper-parameters using a fraction of the training data. The vectors for the entities, types, contexts, and descriptions are of size d = 200. The size of the local context encoder LSTM hidden layer l, local context output, and the document-context encoder output Dm is set to 100(= l = Dm ). The document context vocabulary contains |VG |= 1.5 million strings. We use dropout (Srivastava et al., 2014) with a probability of 0.4. Additionally, we use word-dropout where we replace a rando"
D17-1284,P11-1138,1,0.821844,"d while these authors were at UIUC. Dan Roth∗ University of Pennsylvania Philadelphia, PA danroth@seas.upenn.edu mention-level context to identify that the sentence refers to a sports team (using plays and match), use document-level context to identify the sport, and information about the entity to realize that India cricket team is a sports team and the string “India” may refer to it. The problem has been studied extensively by employing a variety of machine learning, and inference methods, including a pipeline of deterministic modules (Ling et al., 2015), simple classifiers (Cucerzan, 2007; Ratinov et al., 2011), graphical models (Durrett and Klein, 2014), classifiers augmented with ILP inference (Cheng and Roth, 2013), and more recently, neural approaches (He et al., 2013; Sun et al., 2015; Francis-Landau et al., 2016). We present a neural approach to linking1 that learns a dense unified representation of entities by encoding the semantic and background information from multiple sources – encyclopedic entity descriptions, entity-type information, and the contexts the entity occurs in – thus capturing different aspects of the “meaning” of an entity. Hence, we overcome the shortcomings of several exis"
D17-1284,P16-1136,0,0.0154861,"r sports_team Figure 1: Overview of the Model (§ 3): Each entity has a Wikipedia description, linked mentions in Wikipedia (only one shown), and fine-grained types from Freebase (only one shown). We encode local and document-level mention contexts (§ 3.1), entity-description (§ 3.2), and fine-grained entity-types (§ 3.3 & § 3.4). Joint optimization (§ 3.5) over these provides the unified entity representations {ve }. such as, from the structured KB for KB completion (Bordes et al., 2011, 2013; Yang et al., 2014; Lin et al., 2015), or from both structured KBs, and text for relation extraction (Toutanova et al., 2016; Verga et al., 2016a). However, since it is not trivial for these models to incorporate new entities to the KB, few recent approaches alleviate this issue by representing entities as a composition of words in their names (Socher et al., 2013), relations they participate in (Verga et al., 2016b), or their types (Das et al., 2017), but do not use multiple sources of information jointly. In our work, we use structured knowledge (types) as well as unstructured knowledge (description and context) to learn entity embeddings for entity linking, and show that it extends to new entities. 3 Jointly Emb"
D17-1284,N16-1103,0,0.0132203,"Overview of the Model (§ 3): Each entity has a Wikipedia description, linked mentions in Wikipedia (only one shown), and fine-grained types from Freebase (only one shown). We encode local and document-level mention contexts (§ 3.1), entity-description (§ 3.2), and fine-grained entity-types (§ 3.3 & § 3.4). Joint optimization (§ 3.5) over these provides the unified entity representations {ve }. such as, from the structured KB for KB completion (Bordes et al., 2011, 2013; Yang et al., 2014; Lin et al., 2015), or from both structured KBs, and text for relation extraction (Toutanova et al., 2016; Verga et al., 2016a). However, since it is not trivial for these models to incorporate new entities to the KB, few recent approaches alleviate this issue by representing entities as a composition of words in their names (Socher et al., 2013), relations they participate in (Verga et al., 2016b), or their types (Das et al., 2017), but do not use multiple sources of information jointly. In our work, we use structured knowledge (types) as well as unstructured knowledge (description and context) to learn entity embeddings for entity linking, and show that it extends to new entities. 3 Jointly Embedding Entity Inform"
D17-1284,E17-1119,0,0.0913543,"t|e) |E| 0 e∈E 3.4 t∈Te t ∈T / e Type-Aware Context Representation, T Apart from being able to represent the types of the entities, it is also important for our linker to be able to represent the type information at the mention level. In the example in Fig. 1, although the mention “India” is prominently used to refer to the country, it is evident from the sentence that it refers to a Sports Team. The context-encoder captures this information in an unstructured manner, thus it will be useful for the encoder to directly utilize this supervision. This is a similar setup as Ling et al. (2015) and Shimaoka et al. (2017) that use noisy distant supervision to train a fine-grained type predictor for mentions. In order for the context encoders, and type embeddings to directly inform each other, we introduce an objective Lmtype between every vm and vt if type t belongs to Te for the entity e that m refers to. This objective is similar to Letype from § 3.3. 3.5 Learning Unified Entity Representations In the sections above we described different encoder models to capture entity-context information (local- and document-level), entity-description from a KB, and fine-grained types in a single entity representation vec"
D17-1284,Q15-1036,0,\N,Missing
D17-1284,E17-1058,0,\N,Missing
D18-1233,J08-4004,0,0.0326963,"am ... Do I need to...?”). By controlling the answers of the virtual user, we control the ratio of “Yes” and “No” answers. And by showing only subsets of the dialog to the annotator that produces the scenario, we can control what the scenario is capturing. The question, rule text and dialogs are then used to produce utterances of the kind we see in Figure 1. Annotators show substantial agreement when constructing dialogs with a three-way annotator agreement at a Fleiss’ Kappa level of 0.71.1 Likewise, we find that 1 This is well within the range of what is considered as substantial agreement (Artstein and Poesio, 2008). our crowd-annotators produce questions that are coherent with the given dialogs with high accuracy. In theory, the task could be addressed by an endto-end neural network that encodes the question, history and previous dialog, and then decodes a Yes/No answer or question. In practice, we test this hypothesis using a seq2seq model (Sutskever et al., 2014; Cho et al., 2014), with and without copy mechanisms (Gu et al., 2016) to reflect how follow-up questions often use lexical content from the rule text. We find that despite a training set size of 21,890 training utterances, successful models f"
D18-1233,D18-1241,0,0.224987,"questions are often needed. The domain of text we consider is also different (regulatory vs Wikipedia, books, newswire). Dialog The task we propose is, at its heart, about conducting a dialog (Weizenbaum, 1966; Serban et al., 2018; Bordes and Weston, 2016). Within this scope, our work is closest to work in dialogbased QA where complex information needs are addressed using a series of questions. In this space, previous approaches have been looking primarily at QA dialogs about images (Das et al., 2017) and knowledge graphs (Saha et al., 2018; Iyyer et al., 2017). In parallel to our work, both Choi et al. (2018) and Reddy et al. (2018) have to began to investigate QA dialogs with background text. Our work not only differs in the domain covered (regulatory text vs wikipedia), but also in the fact that our task requires the interpretation of complex rules, application of background knowledge, and the formulation of free-form clarification questions. Rao and Daume III (2018) does investigate how to generate clarification questions but this does not require the understanding of explicit natural language rules. Rule Extraction From Text There is a long line of work in the automatic extraction of rules fro"
D18-1233,P16-1154,0,0.0426037,"-way annotator agreement at a Fleiss’ Kappa level of 0.71.1 Likewise, we find that 1 This is well within the range of what is considered as substantial agreement (Artstein and Poesio, 2008). our crowd-annotators produce questions that are coherent with the given dialogs with high accuracy. In theory, the task could be addressed by an endto-end neural network that encodes the question, history and previous dialog, and then decodes a Yes/No answer or question. In practice, we test this hypothesis using a seq2seq model (Sutskever et al., 2014; Cho et al., 2014), with and without copy mechanisms (Gu et al., 2016) to reflect how follow-up questions often use lexical content from the rule text. We find that despite a training set size of 21,890 training utterances, successful models for this task need a stronger inductive bias due to the inherent challenges of the task: interpreting natural language rules, generating questions, and reasoning with background knowledge. We develop heuristics that can work better in terms of identifying what questions to ask, but they still fail to interpret scenarios correctly. To further motivate the task, we also show in oracle experiments that a CMR system can help hum"
D18-1233,P17-1167,0,0.114263,"Missing"
D18-1233,P17-1147,0,0.0647331,"rovement whenever background knowledge is needed. 1 Utterance 1 Scenario input Have you been working abroad 52 weeks or less? Utterance 2 input output Follow-up Yes Yes output Answer Figure 1: An example of two utterances for rule interpretation. In the first utterance, a follow-up question is generated. In the second, the scenario, history and background knowledge (Canada is not in the EEA) is used to arrive at the answer “Yes”. There has been significant progress in teaching machines to read text and answer questions when the answer is directly expressed in the text (Rajpurkar et al., 2016; Joshi et al., 2017; Welbl et al., 2018; Hermann et al., 2015). However, in many settings, These three authors contributed equally Question Do I need to carry on paying UK National Insurance? Introduction ⇤ I am working for an employer in Canada. the text contains rules expressed in natural language that can be used to infer the answer when combined with background knowledge, rather than the literal answer. For example, to answer someone’s question “I am working for an employer in Canada. Do I need to carry on paying National Insurance?” with “Yes”, one needs to read that “You’ll carry on paying National Insuran"
D18-1233,K17-1034,0,0.0553754,"Missing"
D18-1233,P16-1170,0,0.0276181,"the general problem of such approaches: they require careful ontology building, layers of error-prone linguistic preprocessing, and are difficult for non-experts to create annotations for. Question Generation Our task involves the automatic generation of natural language questions. Previous work in question generation has focussed on producing questions for a given text, such that the questions can be answered using this text (Vanderwende, 2008; M. Olney et al., 2012; Rus et al., 2011). In our case, the questions to generate are derived from the background text but cannot be answered by them. Mostafazadeh et al. (2016) investigate how to generate natural follow-up questions based on the content of an image. Besides not working in a visual context, our task is also different because we see question generation as a sub-task of question answering. 7 Conclusion In this paper we present a new task as well as an annotation protocol, a dataset, and a set of baselines. The task is challenging and requires models to generate language, copy tokens, and make logical inferences. Through the use of an interactive and dialog-based annotation interface, we achieve good agreement rates at a low cost. Initial baseline resul"
D18-1233,D16-1264,0,0.62132,"substantial room for improvement whenever background knowledge is needed. 1 Utterance 1 Scenario input Have you been working abroad 52 weeks or less? Utterance 2 input output Follow-up Yes Yes output Answer Figure 1: An example of two utterances for rule interpretation. In the first utterance, a follow-up question is generated. In the second, the scenario, history and background knowledge (Canada is not in the EEA) is used to arrive at the answer “Yes”. There has been significant progress in teaching machines to read text and answer questions when the answer is directly expressed in the text (Rajpurkar et al., 2016; Joshi et al., 2017; Welbl et al., 2018; Hermann et al., 2015). However, in many settings, These three authors contributed equally Question Do I need to carry on paying UK National Insurance? Introduction ⇤ I am working for an employer in Canada. the text contains rules expressed in natural language that can be used to infer the answer when combined with background knowledge, rather than the literal answer. For example, to answer someone’s question “I am working for an employer in Canada. Do I need to carry on paying National Insurance?” with “Yes”, one needs to read that “You’ll carry on pay"
D18-1233,P18-1255,0,0.0276787,"Missing"
D18-1233,W11-2853,0,0.0272237,", Delisle et al. (1994) maps text to horn clauses. This can be very effective, and good results are reported, but suffers from the general problem of such approaches: they require careful ontology building, layers of error-prone linguistic preprocessing, and are difficult for non-experts to create annotations for. Question Generation Our task involves the automatic generation of natural language questions. Previous work in question generation has focussed on producing questions for a given text, such that the questions can be answered using this text (Vanderwende, 2008; M. Olney et al., 2012; Rus et al., 2011). In our case, the questions to generate are derived from the background text but cannot be answered by them. Mostafazadeh et al. (2016) investigate how to generate natural follow-up questions based on the content of an image. Besides not working in a visual context, our task is also different because we see question generation as a sub-task of question answering. 7 Conclusion In this paper we present a new task as well as an annotation protocol, a dataset, and a set of baselines. The task is challenging and requires models to generate language, copy tokens, and make logical inferences. Throug"
D18-1233,D08-1027,0,0.288651,"Missing"
D18-1233,D16-1244,0,0.119236,"Missing"
D18-1233,N18-1202,0,0.00970419,"le 4: Results of entailment models on ShARC. is understood, certain follow-up questions can be skipped because they are answered within the scenario. In this section, we investigate how difficult scenario interpretation is by training models to answer follow-up questions based on scenarios. Baselines We use a random baseline and also implement a surface logistic regression applied to a TFIDF representation of the combined scenario and the question. For neural models, we use Decomposed Attention Model (DAM) (Parikh et al., 2016) trained on each the SNLI and ShARC corpora using ELMO embeddings (Peters et al., 2018).4 eration model is used to produce a follow-up question, f1 . The rule text and produced follow-up question are then passed as inputs to the Scenario Interpretation model. If the output of this is I RRELEVANT, then the CM predicts f1 , otherwise, these steps are repeated recursively until the classification model no longer predicts M ORE or the entailment model predicts I RRELEVANT, in which case the model produces a final answer. We also investigate an extension of the NMT-copy model on the end-to-end task. Input sequences are encoded as a concatenation of the rule text, question, scenario a"
D18-1233,Q18-1021,1,0.838208,"ckground knowledge is needed. 1 Utterance 1 Scenario input Have you been working abroad 52 weeks or less? Utterance 2 input output Follow-up Yes Yes output Answer Figure 1: An example of two utterances for rule interpretation. In the first utterance, a follow-up question is generated. In the second, the scenario, history and background knowledge (Canada is not in the EEA) is used to arrive at the answer “Yes”. There has been significant progress in teaching machines to read text and answer questions when the answer is directly expressed in the text (Rajpurkar et al., 2016; Joshi et al., 2017; Welbl et al., 2018; Hermann et al., 2015). However, in many settings, These three authors contributed equally Question Do I need to carry on paying UK National Insurance? Introduction ⇤ I am working for an employer in Canada. the text contains rules expressed in natural language that can be used to infer the answer when combined with background knowledge, rather than the literal answer. For example, to answer someone’s question “I am working for an employer in Canada. Do I need to carry on paying National Insurance?” with “Yes”, one needs to read that “You’ll carry on paying National Insurance if you’re working"
D18-1359,C14-1008,0,0.0146178,"we use characterbased stacked, bidirectional GRUs to encode them, similar to Verga et al. (2016), using the final output of the top layer as the representation of the string. For strings that are much longer, such as detailed descriptions of entities consisting of multiple sentences, we treat them as a sequence of words, and use a CNN over the word embeddings, similar to Francis-Landau et al. (2016), in order to learn the embedding of such values. These two encoders provide a fixed length encoding that has been shown to be an accurate semantic representation of strings for multiple tasks (Dos Santos and Gatti, 2014). Images Images can also provide useful evidence for modeling entities. For example, we can extract person’s details such as gender, age, job, etc., from image of the person (Levi and Hassner, 2015), or location information such as its approximate coordinates, neighboring locations, and size from map images (Weyand et al., 2016). A variety of models have been used to compactly represent the semantic information in the images, and have been successfully applied to tasks such as image classification, captioning (Karpathy and Fei-Fei, 2015), and question-answering (Yang et al., 2016). To embed im"
D18-1359,N16-1150,0,0.0374113,"ferent types of information, for example names versus paragraph-long descriptions, we create different encoders depending on the lengths of the strings involved. For attributes that are fairly short, such as names and titles, we use characterbased stacked, bidirectional GRUs to encode them, similar to Verga et al. (2016), using the final output of the top layer as the representation of the string. For strings that are much longer, such as detailed descriptions of entities consisting of multiple sentences, we treat them as a sequence of words, and use a CNN over the word embeddings, similar to Francis-Landau et al. (2016), in order to learn the embedding of such values. These two encoders provide a fixed length encoding that has been shown to be an accurate semantic representation of strings for multiple tasks (Dos Santos and Gatti, 2014). Images Images can also provide useful evidence for modeling entities. For example, we can extract person’s details such as gender, age, job, etc., from image of the person (Levi and Hassner, 2015), or location information such as its approximate coordinates, neighboring locations, and size from map images (Weyand et al., 2016). A variety of models have been used to compactly"
D18-1359,N13-1008,0,0.0162049,"s extra attributes), images (Xie et al., 2017; Oñoro-Rubio et al., 2017) (we use IKRL from the first work to compare it with our approach using only images as extra attributes), text (McAuley and Leskovec, 2013; Zhong et al., 2015; Toutanova et al., 2015, 2016; Xie et al., 2016; Tu et al., 2017), and a combination of text and image (Sergieh et al., 2018). Further, Verga et al. (2016) address the multilingual relation extraction task to attain a universal schema by considering raw text with no annotation as extra feature and using matrix factorization to jointly embed KB and textual relations (Riedel et al., 2013). In addition to treating the extra information as features, graph embedding approaches (Schlichtkrull et al., 2017; Kipf and Welling, 2016) consider observed attributes while encoding to achieve more accurate embeddings. The difference between MKBE and these mentioned approaches is three-fold: (1) we are the first to use different types of information in a unified model, (2) we treat these different types of information (numerical, text, image) as relational triples of structured knowledge instead of predetermined features, i.e., first-class citizens of the KB, and not auxiliary features, and"
D18-1359,S18-2027,0,0.0200314,"features for entities, by either merging, concatenating, or averaging the entity and its features to compute its embeddings, such as numerical values (Garcia-Duran and Niepert, 2017) (we use KBLN from this work to compare it with our approach using only numerical as extra attributes), images (Xie et al., 2017; Oñoro-Rubio et al., 2017) (we use IKRL from the first work to compare it with our approach using only images as extra attributes), text (McAuley and Leskovec, 2013; Zhong et al., 2015; Toutanova et al., 2015, 2016; Xie et al., 2016; Tu et al., 2017), and a combination of text and image (Sergieh et al., 2018). Further, Verga et al. (2016) address the multilingual relation extraction task to attain a universal schema by considering raw text with no annotation as extra feature and using matrix factorization to jointly embed KB and textual relations (Riedel et al., 2013). In addition to treating the extra information as features, graph embedding approaches (Schlichtkrull et al., 2017; Kipf and Welling, 2016) consider observed attributes while encoding to achieve more accurate embeddings. The difference between MKBE and these mentioned approaches is three-fold: (1) we are the first to use different ty"
D18-1359,D15-1174,0,0.0342201,"g them as relational triples. A number of methods utilize an extra type of information as the observed features for entities, by either merging, concatenating, or averaging the entity and its features to compute its embeddings, such as numerical values (Garcia-Duran and Niepert, 2017) (we use KBLN from this work to compare it with our approach using only numerical as extra attributes), images (Xie et al., 2017; Oñoro-Rubio et al., 2017) (we use IKRL from the first work to compare it with our approach using only images as extra attributes), text (McAuley and Leskovec, 2013; Zhong et al., 2015; Toutanova et al., 2015, 2016; Xie et al., 2016; Tu et al., 2017), and a combination of text and image (Sergieh et al., 2018). Further, Verga et al. (2016) address the multilingual relation extraction task to attain a universal schema by considering raw text with no annotation as extra feature and using matrix factorization to jointly embed KB and textual relations (Riedel et al., 2013). In addition to treating the extra information as features, graph embedding approaches (Schlichtkrull et al., 2017; Kipf and Welling, 2016) consider observed attributes while encoding to achieve more accurate embeddings. The differen"
D18-1359,P16-1136,0,0.0295645,"Missing"
D18-1359,D15-1031,0,0.0220748,"component by treating them as relational triples. A number of methods utilize an extra type of information as the observed features for entities, by either merging, concatenating, or averaging the entity and its features to compute its embeddings, such as numerical values (Garcia-Duran and Niepert, 2017) (we use KBLN from this work to compare it with our approach using only numerical as extra attributes), images (Xie et al., 2017; Oñoro-Rubio et al., 2017) (we use IKRL from the first work to compare it with our approach using only images as extra attributes), text (McAuley and Leskovec, 2013; Zhong et al., 2015; Toutanova et al., 2015, 2016; Xie et al., 2016; Tu et al., 2017), and a combination of text and image (Sergieh et al., 2018). Further, Verga et al. (2016) address the multilingual relation extraction task to attain a universal schema by considering raw text with no annotation as extra feature and using matrix factorization to jointly embed KB and textual relations (Riedel et al., 2013). In addition to treating the extra information as features, graph embedding approaches (Schlichtkrull et al., 2017; Kipf and Welling, 2016) consider observed attributes while encoding to achieve more accurate"
D18-1359,P17-1158,0,0.0545168,"s utilize an extra type of information as the observed features for entities, by either merging, concatenating, or averaging the entity and its features to compute its embeddings, such as numerical values (Garcia-Duran and Niepert, 2017) (we use KBLN from this work to compare it with our approach using only numerical as extra attributes), images (Xie et al., 2017; Oñoro-Rubio et al., 2017) (we use IKRL from the first work to compare it with our approach using only images as extra attributes), text (McAuley and Leskovec, 2013; Zhong et al., 2015; Toutanova et al., 2015, 2016; Xie et al., 2016; Tu et al., 2017), and a combination of text and image (Sergieh et al., 2018). Further, Verga et al. (2016) address the multilingual relation extraction task to attain a universal schema by considering raw text with no annotation as extra feature and using matrix factorization to jointly embed KB and textual relations (Riedel et al., 2013). In addition to treating the extra information as features, graph embedding approaches (Schlichtkrull et al., 2017; Kipf and Welling, 2016) consider observed attributes while encoding to achieve more accurate embeddings. The difference between MKBE and these mentioned approa"
D18-1359,N16-1103,0,0.0598166,"er-dimensional space, from R → Rd ). It is worth noting that existing methods treat numbers as distinct entities, e.g., learn independent vectors for numbers 39 and 40, relying on data to learn that these values are similar to each other. Text Since text can be used to store a wide variety of different types of information, for example names versus paragraph-long descriptions, we create different encoders depending on the lengths of the strings involved. For attributes that are fairly short, such as names and titles, we use characterbased stacked, bidirectional GRUs to encode them, similar to Verga et al. (2016), using the final output of the top layer as the representation of the string. For strings that are much longer, such as detailed descriptions of entities consisting of multiple sentences, we treat them as a sequence of words, and use a CNN over the word embeddings, similar to Francis-Landau et al. (2016), in order to learn the embedding of such values. These two encoders provide a fixed length encoding that has been shown to be an accurate semantic representation of strings for multiple tasks (Dos Santos and Gatti, 2014). Images Images can also provide useful evidence for modeling entities. F"
D19-1005,D19-1522,0,0.0518018,"Missing"
D19-1005,D17-1284,1,0.863563,"Missing"
D19-1005,D18-1454,0,0.0247236,"nguage models Some previous work has focused on adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Dev"
D19-1005,W09-2415,0,0.0815158,"Missing"
D19-1005,D11-1072,0,0.332131,"Missing"
D19-1005,D17-1195,1,0.70636,"ng based on the smaller BERTBASE model, the experiments demonstrate improved masked language model perplexity and ability to recall facts over BERTLARGE . The extrinsic evaluations demonstrate improvements for challenging relationship extraction, entity typing and word sense disambiguation datasets, and often outperform other contemporaneous attempts to incorporate external knowledge into BERT. 2 Entity-aware language models Some previous work has focused on adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a"
D19-1005,P18-1224,0,0.201073,"ns in the input text and use an entity linker to retrieve relevant entity embeddings from a KB to form knowledge enhanced entity-span representations. Then, the model recontextualizes the entity-span representations with word-toentity attention to allow long range interactions between contextual word representations and all entity spans in the context. The entire KAR is inserted between two layers in the middle of a pretrained model such as BERT. In contrast to previous approaches that integrate external knowledge into task-specific models with task supervision (e.g., Yang and Mitchell, 2017; Chen et al., 2018), our approach learns the entity linkers with self-supervision on unlabeled data. This results in general purpose knowledge enhanced representations that can be applied to a wide range of downstream tasks. Our approach has several other benefits. First, it leaves the top layers of the original model unchanged so we may retain the output loss layers and fine-tune on unlabeled corpora while training the KAR. This also allows us to simply swap out BERT for KnowBert in any downstream application. Second, by taking advantage of the existing high capacity layers in the original model, the KAR is lig"
D19-1005,K18-1050,0,0.171803,"tential mention from among the available candidates. It first runs mention-span self-attention to compute Se = TransformerBlock(S). (3) and max-margin, (2) LEL = max(0, γ − ψmg ) + X max(0, γ + ψmk ), The span self-attention is identical to the typical transformer layer, exception that the self-attention is between mention-span vectors instead of word piece vectors. This allows KnowBert to incorporate global information into each linking decision so that it can take advantage of entity-entity cooccurrence and resolve which of several overlapping candidate mentions should be linked.1 Following Kolitsas et al. (2018), Se is used to score each of the candidate entities while incorporating the candidate entity prior from the KB. Each candidate span m has an associated mention-span (5) emk 6=emg formulations (see Sec. 4.1 for details). Knowledge enhanced entity-span representations KnowBert next injects the KB entity information into the mention-span representations computed from BERT vectors (sem ) to form entityspan representations. For a given span m, we first disregard all candidate entities with score ψ below a fixed threshold, and softmax normalize the remaining scores:  exp(ψmk )   , ψmk ≥ δ  P ex"
D19-1005,D14-1110,0,0.0324097,"knowledge graph. These methods broadly fall into two categories: translational distance models (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015; Xiao et al., 2016) which use a distance-based scoring function, and linear models (Nickel et al., 2011; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018) which use a similarity-based scoring function. We experiment with TuckER (Balazevic et al., 2019) embeddings, a recent linear model which generalizes many of the aforecited models. Other methods combine entity metadata with the graph (Xie et al., 2016), use entity contexts (Chen et al., 2014; Ganea and Hofmann, 2017), or a combination of contexts and the KB (Wang et al., 2014a; Gupta 3.1 Pretrained BERT We describe KnowBert as an extension to (and candidate replacement for) BERT, although the method is general and can be applied to any deep pretrained model including left-to-right and rightto-left LMs such as ELMo and GPT. Formally, BERT accepts as input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi"
D19-1005,D17-1018,0,0.0321533,"back to the BERT dimension (7) resulting in H0i . vector sem (computed via Eq. 2), Mm candidate entities with embeddings emk (from the KB), and prior probabilities pmk . We compute Mm scores using the prior and dot product between the entityspan vectors and entity embeddings, jected to the entity dimension (E, typically 200 or 300, see Sec. 4.1) with a linear projection, proj Hi proj = Hi W1 proj + b1 . (1) Then, the KAR computes C mention-span representations sm ∈ RE , one for each candidate mention, by pooling over all word pieces in a mentionspan using the self-attentive span pooling from Lee et al. (2017). The mention-spans are stacked into a matrix S ∈ RC×E . ψmk = MLP(pmk , sem · emk ), with a two-layer MLP (100 hidden dimensions). If entity linking (EL) supervision is available, we can compute a loss with the gold entity emg . The exact form of the loss depends on the KB, and we use both log-likelihood,   X exp(ψmg ) LEL = − log P , (4) k exp(ψmk ) m Entity linker The entity linker is responsible for performing entity disambiguation for each potential mention from among the available candidates. It first runs mention-span self-attention to compute Se = TransformerBlock(S). (3) and max-mar"
D19-1005,P18-1009,0,0.0644275,"Missing"
D19-1005,Q15-1023,1,0.851813,"input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi−1 ). The non-linear function is a multi-headed self-attention layer followed by a position-wise multilayer perceptron (MLP) 44 In practice, these are often implemented using precomputed dictionaries (e.g., CrossWikis; Spitkovsky and Chang, 2012), KB specific rules (e.g., a WordNet lemmatizer), or other heuristics (e.g., string match; Mihaylov and Frank, 2018). Ling et al. (2015) showed that incorporating candidate priors into entity linkers can be a powerful signal, so we optionally allow for the candidate selector to return an associated prior probability for each entity candidate. In some cases, it is beneficial to over-generate potential candidates and add a special NULL entity to each candidate list, thereby allowing the linker to discriminate between actual links and false positive candidates. In this work, the entity candidate selectors are fixed but their output is passed to a learned context dependent entity linker to disambiguate the candidate mentions. Fina"
D19-1005,P19-1598,1,0.857562,"eam performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs. 1 Introduction Large pretrained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) have significantly improved the state of the art for a wide range of NLP tasks. These models are trained on large amounts of raw text using self-supervised objectives. However, they do not contain any explicit grounding to real world entities and as a result have difficulty recovering factual knowledge (Logan et al., 2019). Knowledge bases (KBs) provide a rich source of high quality, human-curated knowledge that can be used to ground these models. In addition, they 43 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 43–54, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics et al., 2017). Our approach is agnostic to the details of the entity embedding method and as a result is able to use any of these methods. KB, subject to a small set of requirements (see Se"
D19-1005,K16-1006,0,0.0381149,"performance as they must balance specializing for the entity linking task and learning general purpose representations suitable for language modeling. Table 2 displays fine-grained WSD F1 using the evaluation framework from Navigli et al. (2017) and the ALL dataset (combing SemEval 2007, 2013, 2015 and Senseval 2 and 3). By linking to nodes in our WordNet graph and restricting to gold lemmas at test time we can recast the WSD task under our general entity linking framework. The ELMo and BERT baselines use a nearest neighbor approach trained on the SemCor dataset, similar to the evaluation in Melamud et al. (2016), which has previously been shown to be competitive with task-specific architectures (Raganato et al., 2017). As can be seen, KnowBert provides competitive performance, and KnowBert-W+W is able to 4.3 Downstream Tasks This section evaluates KnowBert on downstream tasks to validate that the addition of knowledge improves performance on tasks expected to benefit from it. Given the overall superior performance of KnowBert-W+W on the intrinsic evaluations, we focus on it exclusively for evaluation in this section. The main results are included in this section; see the supplementary material for fu"
D19-1005,N19-1423,0,0.721409,"nd-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs. 1 Introduction Large pretrained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) have significantly improved the state of the art for a wide range of NLP tasks. These models are trained on large amounts of raw text using self-supervised objectives. However, they do not contain any explicit grounding to real world entities and as a result have difficulty recovering factual knowledge (Logan et al., 2019). Knowledge bases (KBs) provide a rich source of high quality, human-curated knowledge that can be used to ground these models. In addition, they 43 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confere"
D19-1005,P18-1076,0,0.131211,"revious work has focused on adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019). We buil"
D19-1005,D17-1169,0,0.0298708,"projected word piece representations and knowledge enhanced entityspan vectors. As introduced by Vaswani et al. (2017), the contextual embeddings Hi are used for the query, key, and value in multi-headed selfattention. The word-to-entity-span attention in proj KnowBert substitutes Hi for the query, and S0e for both the key and value: 0 proj Hi proj 0 3.4 Our training regime incrementally pretrains increasingly larger portions of KnowBert before fine-tuning all trainable parameters in a multitask setting with any available EL supervision. It is similar in spirit to the “chain-thaw” approach in Felbo et al. (2017), and is summarized in Alg. 1. We assume access to a pretrained BERT model and one or more KBs with their entity candidate selectors. To add the first KB, we begin by pretraining entity embeddings (if not already provided from another source), then freeze them in all subsequent training, including task-specific finetuning. If EL supervision is available, it is used to pretrain the KB specific EL parameters, while freezing the remainder of the network. Finally, the entire network is fine-tuned to convergence by minimizing 0 = MLP(MultiHeadAttn(Hi , S e , S e )). This allows each word piece to a"
D19-1005,D17-1277,0,0.337604,"ese methods broadly fall into two categories: translational distance models (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015; Xiao et al., 2016) which use a distance-based scoring function, and linear models (Nickel et al., 2011; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018) which use a similarity-based scoring function. We experiment with TuckER (Balazevic et al., 2019) embeddings, a recent linear model which generalizes many of the aforecited models. Other methods combine entity metadata with the graph (Xie et al., 2016), use entity contexts (Chen et al., 2014; Ganea and Hofmann, 2017), or a combination of contexts and the KB (Wang et al., 2014a; Gupta 3.1 Pretrained BERT We describe KnowBert as an extension to (and candidate replacement for) BERT, although the method is general and can be applied to any deep pretrained model including left-to-right and rightto-left LMs such as ELMo and GPT. Formally, BERT accepts as input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi−1 ). The non-linear funct"
D19-1005,H94-1046,0,0.0558172,"dings. We used TuckER (Balazevic et al., 2019) to compute 200dimensional vectors for each synset and lemma using the relationship graph. Then, we extracted the gloss for each synset and used an off-theshelf state-of-the-art sentence embedding method (Subramanian et al., 2018) to produce 2048dimensional vectors. These are concatenated to the TuckER embeddings. To reduce the dimensionality for use in KnowBert, the frozen 2248dimensional embeddings are projected to 200dimensions with a learned linear transformation. For supervision, we combined the SemCor word sense disambiguation (WSD) dataset (Miller et al., 1994) with all lemma example usages from WordNet6 and link directly to synsets. The loss function is Eq. 4. At train time, we did not provide gold lemmas or POS tags, so KnowBert must learn to implicitly model coarse grained POS tags to disambiguate each word. At test time when evaluating we restricted candidate entities to just those matching the gold lemma and POS tag, consistent with the standard WSD evaluation. 4.2 Intrinsic Evaluation Perplexity Table 1 compares masked LM perplexity for KnowBert with BERTBASE and BERTLARGE . To rule out minor differences due to our data preparation, the BERT m"
D19-1005,E17-1010,0,0.0327968,"ameters are actually used. In contrast, BERTLARGE uses the majority of its 336M parameters for each input. Integrated EL It is also possible to evaluate the performance of the integrated entity linkers inside KnowBert using diagnostic probes without any further fine-tuning. As these were trained in a multitask setting primarily with raw text, we do not a priori expect high performance as they must balance specializing for the entity linking task and learning general purpose representations suitable for language modeling. Table 2 displays fine-grained WSD F1 using the evaluation framework from Navigli et al. (2017) and the ALL dataset (combing SemEval 2007, 2013, 2015 and Senseval 2 and 3). By linking to nodes in our WordNet graph and restricting to gold lemmas at test time we can recast the WSD task under our general entity linking framework. The ELMo and BERT baselines use a nearest neighbor approach trained on the SemCor dataset, similar to the evaluation in Melamud et al. (2016), which has previously been shown to be competitive with task-specific architectures (Raganato et al., 2017). As can be seen, KnowBert provides competitive performance, and KnowBert-W+W is able to 4.3 Downstream Tasks This se"
D19-1005,D14-1162,0,0.0943202,"ing KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019). We build upon these by incorporating structured knowledge into these models. 3 KnowBert KnowBert incorporates knowledge bases into BERT using the Knowledge Attention and Recontextualization component (KAR). We start by describing the BERT and KB components. We then move to introducing KAR. Finally, we describe the training procedure, including the multitask training regime for jointly training KnowBert and an entity linker. Entity embeddings Entity embeddi"
D19-1005,N18-1202,1,0.886558,"supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs. 1 Introduction Large pretrained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) have significantly improved the state of the art for a wide range of NLP tasks. These models are trained on large amounts of raw text using self-supervised objectives. However, they do not contain any explicit grounding to real world entities and as a result have difficulty recovering factual knowledge (Logan et al., 2019). Knowledge bases (KBs) provide a rich source of high quality, human-curated knowledge that can be used to ground these models. In addition, they 43 Proceedings of the 2019 Conference on Empirical Methods in Natural"
D19-1005,P19-1219,0,0.0221389,"n adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019). We build upon these by incorp"
D19-1005,N19-1128,0,0.0605837,"Missing"
D19-1005,P19-1132,0,0.0588253,"Missing"
D19-1005,P16-1123,0,0.0556875,"Missing"
D19-1005,D17-1120,0,0.0301197,"resentations suitable for language modeling. Table 2 displays fine-grained WSD F1 using the evaluation framework from Navigli et al. (2017) and the ALL dataset (combing SemEval 2007, 2013, 2015 and Senseval 2 and 3). By linking to nodes in our WordNet graph and restricting to gold lemmas at test time we can recast the WSD task under our general entity linking framework. The ELMo and BERT baselines use a nearest neighbor approach trained on the SemCor dataset, similar to the evaluation in Melamud et al. (2016), which has previously been shown to be competitive with task-specific architectures (Raganato et al., 2017). As can be seen, KnowBert provides competitive performance, and KnowBert-W+W is able to 4.3 Downstream Tasks This section evaluates KnowBert on downstream tasks to validate that the addition of knowledge improves performance on tasks expected to benefit from it. Given the overall superior performance of KnowBert-W+W on the intrinsic evaluations, we focus on it exclusively for evaluation in this section. The main results are included in this section; see the supplementary material for full details. The baselines we compare against are BERTBASE , BERTLARGE , the pre-BERT state of the art, and t"
D19-1005,D14-1167,0,0.0467978,"using the Knowledge Attention and Recontextualization component (KAR). We start by describing the BERT and KB components. We then move to introducing KAR. Finally, we describe the training procedure, including the multitask training regime for jointly training KnowBert and an entity linker. Entity embeddings Entity embedding methods produce continuous vector representations from external knowledge sources. Knowledge graphbased methods optimize the score of observed triples in a knowledge graph. These methods broadly fall into two categories: translational distance models (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015; Xiao et al., 2016) which use a distance-based scoring function, and linear models (Nickel et al., 2011; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018) which use a similarity-based scoring function. We experiment with TuckER (Balazevic et al., 2019) embeddings, a recent linear model which generalizes many of the aforecited models. Other methods combine entity metadata with the graph (Xie et al., 2016), use entity contexts (Chen et al., 2014; Ganea and Hofmann, 2017), or a combination of contexts and the KB (Wang et al., 2014a; Gupta 3.1 Pretrained BERT We"
D19-1005,P16-1162,0,0.0394323,"embeddings, a recent linear model which generalizes many of the aforecited models. Other methods combine entity metadata with the graph (Xie et al., 2016), use entity contexts (Chen et al., 2014; Ganea and Hofmann, 2017), or a combination of contexts and the KB (Wang et al., 2014a; Gupta 3.1 Pretrained BERT We describe KnowBert as an extension to (and candidate replacement for) BERT, although the method is general and can be applied to any deep pretrained model including left-to-right and rightto-left LMs such as ELMo and GPT. Formally, BERT accepts as input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi−1 ). The non-linear function is a multi-headed self-attention layer followed by a position-wise multilayer perceptron (MLP) 44 In practice, these are often implemented using precomputed dictionaries (e.g., CrossWikis; Spitkovsky and Chang, 2012), KB specific rules (e.g., a WordNet lemmatizer), or other heuristics (e.g., string match; Mihaylov and Frank, 2018). Ling et al. (2015) showed that incorporating candidate priors"
D19-1005,P19-1279,0,0.0283521,"it. Given the overall superior performance of KnowBert-W+W on the intrinsic evaluations, we focus on it exclusively for evaluation in this section. The main results are included in this section; see the supplementary material for full details. The baselines we compare against are BERTBASE , BERTLARGE , the pre-BERT state of the art, and two contemporaneous papers that add similar types of knowledge to BERT. ERNIE (Zhang et al., 2019) uses TAGME (Ferragina and Scaiella, 2010) to link entities to Wikidata, retrieves the associated entity embeddings, and fuses them into BERTBASE by fine-tuning. Soares et al. (2019) learns relationship representations by fine-tuning BERTLARGE with large scale “matching the blanks” (MTB) pretraining using entity linked text. 50 System ELMo† BERTBASE † BERTLARGE † BERTLARGE †† KnowBert-W+W Accuracy System 57.7 65.4 65.5 69.5 70.9 UFET BERTBASE ERNIE KnowBert-W+W P R F1 68.8 76.4 78.4 78.6 53.3 71.0 72.9 73.7 60.1 73.6 75.6 76.1 Table 7: Test set results for entity typing using the nine general types from (Choi et al., 2018). Table 6: Test set results for the WiC dataset (v1.0). † Pilehvar and Camacho-Collados (2019) †† Wang et al. (2019a) Entity typing We also evaluated Kn"
D19-1005,spitkovsky-chang-2012-cross,0,0.314947,"eral and can be applied to any deep pretrained model including left-to-right and rightto-left LMs such as ELMo and GPT. Formally, BERT accepts as input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi−1 ). The non-linear function is a multi-headed self-attention layer followed by a position-wise multilayer perceptron (MLP) 44 In practice, these are often implemented using precomputed dictionaries (e.g., CrossWikis; Spitkovsky and Chang, 2012), KB specific rules (e.g., a WordNet lemmatizer), or other heuristics (e.g., string match; Mihaylov and Frank, 2018). Ling et al. (2015) showed that incorporating candidate priors into entity linkers can be a powerful signal, so we optionally allow for the candidate selector to return an associated prior probability for each entity candidate. In some cases, it is beneficial to over-generate potential candidates and add a special NULL entity to each candidate list, thereby allowing the linker to discriminate between actual links and false positive candidates. In this work, the entity candidate"
D19-1005,P19-1226,0,0.0248349,"tive language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019). We build upon these by incorporating structured k"
D19-1005,D18-1455,0,0.0713268,"Missing"
D19-1005,P17-1132,0,0.0797122,"plicitly model entity spans in the input text and use an entity linker to retrieve relevant entity embeddings from a KB to form knowledge enhanced entity-span representations. Then, the model recontextualizes the entity-span representations with word-toentity attention to allow long range interactions between contextual word representations and all entity spans in the context. The entire KAR is inserted between two layers in the middle of a pretrained model such as BERT. In contrast to previous approaches that integrate external knowledge into task-specific models with task supervision (e.g., Yang and Mitchell, 2017; Chen et al., 2018), our approach learns the entity linkers with self-supervision on unlabeled data. This results in general purpose knowledge enhanced representations that can be applied to a wide range of downstream tasks. Our approach has several other benefits. First, it leaves the top layers of the original model unchanged so we may retain the output loss layers and fine-tune on unlabeled corpora while training the KAR. This also allows us to simply swap out BERT for KnowBert in any downstream application. Second, by taking advantage of the existing high capacity layers in the original m"
D19-1005,D17-1197,0,0.0344531,"luate KnowBert with a mix of intrinsic and extrinsic tasks. Despite being based on the smaller BERTBASE model, the experiments demonstrate improved masked language model perplexity and ability to recall facts over BERTLARGE . The extrinsic evaluations demonstrate improvements for challenging relationship extraction, entity typing and word sense disambiguation datasets, and often outperform other contemporaneous attempts to incorporate external knowledge into BERT. 2 Entity-aware language models Some previous work has focused on adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more"
D19-1005,D18-1244,0,0.0614284,"Missing"
D19-1005,D17-1004,0,0.0721139,"Missing"
D19-1005,P19-1139,0,0.110116,"is able to 4.3 Downstream Tasks This section evaluates KnowBert on downstream tasks to validate that the addition of knowledge improves performance on tasks expected to benefit from it. Given the overall superior performance of KnowBert-W+W on the intrinsic evaluations, we focus on it exclusively for evaluation in this section. The main results are included in this section; see the supplementary material for full details. The baselines we compare against are BERTBASE , BERTLARGE , the pre-BERT state of the art, and two contemporaneous papers that add similar types of knowledge to BERT. ERNIE (Zhang et al., 2019) uses TAGME (Ferragina and Scaiella, 2010) to link entities to Wikidata, retrieves the associated entity embeddings, and fuses them into BERTBASE by fine-tuning. Soares et al. (2019) learns relationship representations by fine-tuning BERTLARGE with large scale “matching the blanks” (MTB) pretraining using entity linked text. 50 System ELMo† BERTBASE † BERTLARGE † BERTLARGE †† KnowBert-W+W Accuracy System 57.7 65.4 65.5 69.5 70.9 UFET BERTBASE ERNIE KnowBert-W+W P R F1 68.8 76.4 78.4 78.6 53.3 71.0 72.9 73.7 60.1 73.6 75.6 76.1 Table 7: Test set results for entity typing using the nine general"
D19-1221,D15-1075,0,0.210861,"Missing"
D19-1221,P17-1152,0,0.057013,"Missing"
D19-1221,C18-1055,0,0.153122,"orithm We first choose the trigger length: longer triggers are more effective, while shorter triggers are more stealthy. Next, we initialize the trigger sequence by repeating the word “the”, the sub-word “a”, or the character “a” and concatenate the trigger to the front/end of all inputs.3 We then iteratively replace the tokens in the trigger to minimize the loss for the target prediction over batches of examples. To determine how to replace the current tokens, we cannot directly apply adversarial attack methods from computer vision because tokens are discrete. Instead, we build upon HotFlip (Ebrahimi et al., 2018b), a method that approximates the effect of replacing a token using its gradient. To apply this method, the trigger tokens tadv , which are represented as one-hot vectors, are embedded to form eadv . Token Replacement Strategy Our HotFlipinspired token replacement strategy is based on 3 More complex initialization schemes perform similarly (Appendix A). ... (1) tadv bottle tennis set cost minute tony zoning tapping ... arg min Et∼T [L(˜ y , f (tadv ; t))] , ... Universal Setting In a universal targeted attack, the adversary optimizes tadv to minimize the loss for the target class y˜ for all i"
D19-1221,P18-2006,0,0.230089,"orithm We first choose the trigger length: longer triggers are more effective, while shorter triggers are more stealthy. Next, we initialize the trigger sequence by repeating the word “the”, the sub-word “a”, or the character “a” and concatenate the trigger to the front/end of all inputs.3 We then iteratively replace the tokens in the trigger to minimize the loss for the target prediction over batches of examples. To determine how to replace the current tokens, we cannot directly apply adversarial attack methods from computer vision because tokens are discrete. Instead, we build upon HotFlip (Ebrahimi et al., 2018b), a method that approximates the effect of replacing a token using its gradient. To apply this method, the trigger tokens tadv , which are represented as one-hot vectors, are embedded to form eadv . Token Replacement Strategy Our HotFlipinspired token replacement strategy is based on 3 More complex initialization schemes perform similarly (Appendix A). ... (1) tadv bottle tennis set cost minute tony zoning tapping ... arg min Et∼T [L(˜ y , f (tadv ; t))] , ... Universal Setting In a universal targeted attack, the adversary optimizes tadv to minimize the loss for the target class y˜ for all i"
D19-1221,D18-1407,1,0.881648,"Missing"
D19-1221,N18-1202,1,0.774422,"Missing"
D19-1221,S18-2023,0,0.0642901,"Missing"
D19-1221,N18-2017,0,0.0723009,"Missing"
D19-1221,D16-1264,0,0.243828,"Missing"
D19-1221,P18-1079,1,0.813058,"l american people” trigger bit.ly/squad-demo. 2160 7 Related Work Adversarial Attacks in NLP Most adversarial attacks in NLP are gradient-based. For instance, Ebrahimi et al. (2018b) use gradients to attack text classifiers. He and Glass (2019) and Cheng et al. (2018) do the same for text generation. Other attack methods are based on generative (Iyyer et al., 2018) or human-in-the-loop approaches (Wallace et al., 2019). We turn the reader to Zhang et al. (2019) for a recent survey. Triggers differ from most previous attacks because they are universal (input-agnostic). Universal Attacks in NLP Ribeiro et al. (2018) debug models using semantically equivalent adversarial rules (SEARs). Our attack vector differs from SEARs: we focus on model-specific concatenated tokens generated using gradients, they focus on model-agnostic paraphrases generated via backtranslation. Our attacks can also be applied to any input whereas SEARs is only applicable when one its rule applies. In parallel work, Behjati et al. (2019) consider universal adversarial attacks on text classification (compare to our Section 3). Our work is more extensive as we (1) develop a stronger attack algorithm, (2) consider a broader range of mode"
D19-1221,N18-1170,0,0.0385909,"the ELMo model while removing tokens to find the best reduction. The resulting triggers are shorter but significantly more effective (Table 5).9 This shows that the triggers still “overfit” the GloVe BiDAF models. 9 Demo of ELMo model using the “to kill american people” trigger bit.ly/squad-demo. 2160 7 Related Work Adversarial Attacks in NLP Most adversarial attacks in NLP are gradient-based. For instance, Ebrahimi et al. (2018b) use gradients to attack text classifiers. He and Glass (2019) and Cheng et al. (2018) do the same for text generation. Other attack methods are based on generative (Iyyer et al., 2018) or human-in-the-loop approaches (Wallace et al., 2019). We turn the reader to Zhang et al. (2019) for a recent survey. Triggers differ from most previous attacks because they are universal (input-agnostic). Universal Attacks in NLP Ribeiro et al. (2018) debug models using semantically equivalent adversarial rules (SEARs). Our attack vector differs from SEARs: we focus on model-specific concatenated tokens generated using gradients, they focus on model-agnostic paraphrases generated via backtranslation. Our attacks can also be applied to any input whereas SEARs is only applicable when one its"
D19-1221,P16-1162,0,0.0607629,"Missing"
D19-1221,D17-1215,0,0.128075,"hension Reading comprehension models are used to answer questions that are posed to search engines or home assistants. An adversary can attack these models by modifying a web page in order to trigger malicious or vulgar answers. Here, we prepend triggers to paragraphs in order to cause predictions to be a target span inside the trigger. We choose and fix the target span beforehand and optimize the other trigger tokens. The trigger is optimized to work for any paragraph and any question of a certain type. We focus on why, who, when, and where questions. We use sentences of length ten following Jia and Liang (2017) and sum the cross-entropy of the start and end of the target span as the loss function. Conditional Text Generation We attack conditional text generation models, such as those in machine translation or autocomplete keyboards. The failure of such systems can be costly, e.g., translation errors have led to a person’s arrest (Hern, 2018). We create triggers that are prepended before the user input t to cause the model to generate similar content to a set of targets Y.5 In 5 A strong language model will generate grammatically correct continuations of the user’s input. This makes it impossible to"
D19-1221,P19-1334,0,0.0746808,"Missing"
D19-1221,N19-1314,0,0.0612768,"“zoning tapping fienes”, which causes frequent negative predictions. a linear approximation of the task loss.4 We update the embedding for every trigger token eadvi to minimizes the loss’ first-order Taylor approximation around the current token embedding: |  arg min e0i − eadvi ∇eadvi L, (2) e0i ∈V where V is the set of all token embeddings in the model’s vocabulary and ∇eadvi L is the average gradient of the task loss over a batch. Computing the optimal e0i can be efficiently computed in brute-force with |V |d-dimensional dot products where d is the dimensionality of the token embedding (Michel et al., 2019). This brute-force solution is trivially parallelizable and less expensive than running a forward pass for all the models we consider. Finally, after finding each eadvi , we convert the embeddings back to their associated tokens. Figure 1 provides an illustration of the trigger search algorithm. We augment this token replacement strategy with beam search. We consider the top-k token candidates from Equation 2 for each token position in the trigger. We search left to right across 4 We also experiment with projected gradient descent (Appendix A) but find the linear approximation converges faster"
D19-1221,L18-1008,0,0.0274264,"Missing"
D19-1221,D16-1244,0,0.0856411,"Missing"
D19-1221,D13-1170,0,0.0256105,"Missing"
D19-1221,Q19-1029,1,0.846179,"reduction. The resulting triggers are shorter but significantly more effective (Table 5).9 This shows that the triggers still “overfit” the GloVe BiDAF models. 9 Demo of ELMo model using the “to kill american people” trigger bit.ly/squad-demo. 2160 7 Related Work Adversarial Attacks in NLP Most adversarial attacks in NLP are gradient-based. For instance, Ebrahimi et al. (2018b) use gradients to attack text classifiers. He and Glass (2019) and Cheng et al. (2018) do the same for text generation. Other attack methods are based on generative (Iyyer et al., 2018) or human-in-the-loop approaches (Wallace et al., 2019). We turn the reader to Zhang et al. (2019) for a recent survey. Triggers differ from most previous attacks because they are universal (input-agnostic). Universal Attacks in NLP Ribeiro et al. (2018) debug models using semantically equivalent adversarial rules (SEARs). Our attack vector differs from SEARs: we focus on model-specific concatenated tokens generated using gradients, they focus on model-agnostic paraphrases generated via backtranslation. Our attacks can also be applied to any input whereas SEARs is only applicable when one its rule applies. In parallel work, Behjati et al. (2019) c"
D19-1221,D14-1162,0,0.0874216,"dily transfer: the ELMobased DA model’s accuracy degrades the most, despite never being targeted in the trigger generation. We analyze why the predictions for Contradiction are more robust and show that triggers align with known dataset biases in Section 6. 4 Attacking Reading Comprehension We create triggers for SQuAD (Rajpurkar et al., 2016). We use an intentionally simple baseline model and test the trigger’s transferability to more advanced models (with different embeddings, tokenizations, and architectures). The baseline is BiDAF (Seo et al., 2017); we lowercase all inputs and use GloVe (Pennington et al., 2014). ESIM DA DA-ELMo nobody never sad scared championship 89.49 89.46 0.03 0.15 0.50 1.07 1.51 0.50 1.13 0.74 0.83 0.06 90.88 0.50 0.15 0.71 1.01 0.77 Avg. ∆ -88.69 -88.96 -90.25 nobody sleeps nothing none sleeping 84.62 0.53 4.57 1.71 5.96 6.06 79.71 8.45 14.82 23.61 17.52 15.84 83.04 13.61 22.34 14.63 15.41 28.86 Avg. ∆ -80.85 -63.66 -64.07 86.31 73.31 79.89 79.83 80.44 78.00 84.80 70.93 66.91 65.71 63.79 65.83 85.17 60.67 62.96 64.01 70.56 70.56 -8.02 -18.17 -19.42 joyously Contradiction anticipating talented impress inspiring Avg. ∆ Table 2: We prepend a single word (Trigger) to SNLI hypothes"
D19-1221,D18-1453,0,\N,Missing
D19-1534,P18-1198,0,0.0212208,"y; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods. Probing Models Our probes of numeracy parallel work in understanding the linguistic capabilities (literacy) of neural models (Conneau et al., 2018; Liu et al., 2019). LSTMs can remember sentence length, word order, and which words were present in a sentence (Adi et al., 2017). KhandelSynthetic Numerical Tasks Similar to our synthetic numerical reasoning tasks, other work considers sorting (Graves et al., 2014), counting (Weiss et al., 2018), or decoding tasks (Trask et al., 2018). They use synthetic tasks as a testbed to prove or design better models, whereas we use synthetic tasks as a probe to understand token embeddings. In developing the Neural Arithmetic Logic Unit, Trask et al. (2018) arrive at similar conclusions regarding extrap"
D19-1534,P19-1329,0,0.0385549,"training data, is a fruitful direction to explore further (c.f., influence functions (Koh and Liang, 2017; Brunet et al., 2019)). More generally, numeracy is one type of emergent knowledge. For instance, embeddings may capture the size of objects (Forbes and Choi, 2017), speed of vehicles, and many other “commonsense” phenomena (Yang et al., 2018). Vendrov et al. (2016) introduce methods to encode the order of such phenomena into embeddings for concepts such as hypernymy; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods. Probing Models Our probes of numeracy parallel work in understanding the linguistic capabilities (literacy) of neural models (Conneau et al., 2018; Liu et al., 2019). LSTMs can remember sentence length, word order, and which words were present in a sentence (Adi et al."
D19-1534,D14-1162,0,0.100642,"to numbers outside its training range. We are especially intrigued by the model’s ability to learn numeracy, i.e., how does the model know the value of a number given its embedding? The model uses standard embeddings (GloVe and a Char-CNN) and receives no direct supervision for number magnitude/ordering. To understand how numeracy emerges, we probe token embedding methods (e.g., BERT, GloVe) using synthetic list maximum, number decoding, and addition tasks (Section 3). We find that all widely-used pre-trained embeddings, e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and GloVe (Pennington et al., 2014), capture numeracy: number magnitude is present in the embeddings, even for numbers in the thousands. Among all embeddings, characterlevel methods exhibit stronger numeracy than word- and sub-word-level methods (e.g., ELMo excels while BERT struggles), and character-level models learned directly on the synthetic tasks are the strongest overall. Finally, we investigate why NAQANet had trouble extrapolating—was it a failure in the model or the embeddings? We repeat our probing tasks and test for model extrapolation, finding that neural models struggle to predict numbers outside the training rang"
D19-1534,D16-1264,0,0.0650128,", or addition/subtraction of numbers.) Words and numbers are represented as the concatenation of GloVe embeddings and the output of a character-level CNN. The model contains no auxiliary components for representing number magnitude or performing explicit comparisons. We refer readers to Yu et al. (2018) and Dua et al. (2019) for further details. 2.1 2.3 2 Numeracy Case Study: DROP QA DROP Dataset DROP is a reading comprehension dataset that tests numerical reasoning operations such as counting, sorting, and addition (Dua et al., 2019). The dataset’s input-output format is a superset of SQuAD (Rajpurkar et al., 2016): the answers are paragraph spans, as well as question Comparative and Superlative Questions We focus on questions that NAQANet requires numeracy to answer, namely Comparative and Superlative questions.2 Comparative questions 1 Result as of May 21st, 2019. DROP addition, subtraction, and count questions do not require numeracy for NAQANet, see Appendix A. 5308 2 Question Type Example Reasoning Required Comparative (Binary) Comparative (Non-binary) Superlative (Number) Superlative (Span) Which country is a bigger exporter, Brazil or Uruguay? Which player had a touchdown longer than 20 yards? Ho"
D19-1534,N19-1423,0,0.51046,", BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact. Neural NLP models have become the de-facto standard tool across language understanding tasks, even solving basic reading comprehension and textual entailment datasets (Yu et al., 2018; Devlin et al., 2019). Despite this, existing models are incapable of complex forms of reasoning, in particular, we focus on the ability to reason numerically. Recent datasets such as DROP (Dua et al., 2019), EQUATE (Ravichander et al., 2019), or Mathematics Questions (Saxton et al., 2019) test numerical reasoning; they contain examples which require comparing, sorting, and adding numbers in natural language (e.g., Figure 2). The first step in performing numerical reasoning over natural language is numeracy: the abilEqual contribution; work done while interning at AI2. (a) Word2Vec (b) GloVe (c) ELMo (d) BERT (e)"
D19-1534,K19-1033,0,0.0951641,"Missing"
D19-1534,N19-1246,1,0.888358,"ccurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact. Neural NLP models have become the de-facto standard tool across language understanding tasks, even solving basic reading comprehension and textual entailment datasets (Yu et al., 2018; Devlin et al., 2019). Despite this, existing models are incapable of complex forms of reasoning, in particular, we focus on the ability to reason numerically. Recent datasets such as DROP (Dua et al., 2019), EQUATE (Ravichander et al., 2019), or Mathematics Questions (Saxton et al., 2019) test numerical reasoning; they contain examples which require comparing, sorting, and adding numbers in natural language (e.g., Figure 2). The first step in performing numerical reasoning over natural language is numeracy: the abilEqual contribution; work done while interning at AI2. (a) Word2Vec (b) GloVe (c) ELMo (d) BERT (e) Char-CNN (f) Char-LSTM 0 −500 500 0 −500 500 0 −500 −2000 −1000 0 1000 I put Number 2000−2000 −1000 0 1000 I put Number 2000 Figure 1: We train a probing model to decode a number from it"
D19-1534,P17-1025,0,0.0372483,"ger range [0,150] and evaluated on integers from the Test Range. The probing model struggles to extrapolate when trained on the pre-trained embeddings. 4 Discussion and Related Work An open question is how the training process elicits numeracy for word vectors and contextualized embeddings. Understanding this, perhaps by tracing numeracy back to the training data, is a fruitful direction to explore further (c.f., influence functions (Koh and Liang, 2017; Brunet et al., 2019)). More generally, numeracy is one type of emergent knowledge. For instance, embeddings may capture the size of objects (Forbes and Choi, 2017), speed of vehicles, and many other “commonsense” phenomena (Yang et al., 2018). Vendrov et al. (2016) introduce methods to encode the order of such phenomena into embeddings for concepts such as hypernymy; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more"
D19-1534,P18-1027,0,0.050128,"Missing"
D19-1534,D17-1160,1,0.827689,"ate-of-the-art NAQANet answers every question correct. Plausible answer candidates to the questions are underlined and the model’s predictions are shown in bold. spans, number answers (e.g., 35), and dates (e.g., 03/01/2014). The only supervision provided is the question-answer pairs, i.e., a model must learn to reason numerically while simultaneously learning to read and comprehend. 2.2 NAQANet Model This section examines the state-of-the-art model for DROP by investigating its accuracy on questions that require numerical reasoning. Modeling approaches for DROP include both semantic parsing (Krishnamurthy et al., 2017) and reading comprehension (Yu et al., 2018) models. We focus on the latter, specifically on Numerically-augmented QANet (NAQANet), the current state-of-the-art model (Dua et al., 2019).1 The model’s core structure closely follows QANet (Yu et al., 2018) except that it contains four output branches, one for each of the four answer types (passage span, question span, count answer, or addition/subtraction of numbers.) Words and numbers are represented as the concatenation of GloVe embeddings and the output of a character-level CNN. The model contains no auxiliary components for representing numb"
D19-1534,Q16-1037,0,0.10323,"Missing"
D19-1534,N19-1112,1,0.797676,"t al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods. Probing Models Our probes of numeracy parallel work in understanding the linguistic capabilities (literacy) of neural models (Conneau et al., 2018; Liu et al., 2019). LSTMs can remember sentence length, word order, and which words were present in a sentence (Adi et al., 2017). KhandelSynthetic Numerical Tasks Similar to our synthetic numerical reasoning tasks, other work considers sorting (Graves et al., 2014), counting (Weiss et al., 2018), or decoding tasks (Trask et al., 2018). They use synthetic tasks as a testbed to prove or design better models, whereas we use synthetic tasks as a probe to understand token embeddings. In developing the Neural Arithmetic Logic Unit, Trask et al. (2018) arrive at similar conclusions regarding extrapolation: neural mod"
D19-1534,L18-1008,0,0.0281129,"Missing"
D19-1534,P18-1196,0,0.124058,"Missing"
D19-1534,P18-2117,0,0.163586,"extrapolate to other values. 2.6 Whence this behavior? NAQANet exhibits numerical reasoning capabilities that exceed our expectations. What enables this behavior? Aside from reading and comprehending the passage/question, this kind of numerical reasoning requires two components: numeracy (i.e., representing numbers) and comparison algorithms (i.e., computing the maximum of a list). Although the natural emergence of comparison algorithms is surprising, previous results show neural models are capable of learning to count and sort synthetic lists of scalar values when given explicit supervision (Weiss et al., 2018; Vinyals et al., 2016). NAQANet demonstrates that a model can learn comparison algorithms while simultane3 Probing Numeracy of Embeddings We use synthetic numerical tasks to probe the numeracy of token embeddings. 3.1 Probing Tasks We consider three synthetic tasks to evaluate numeracy (Figure 3). Appendix C provides further details on training and evaluation. List Maximum Given a list of the embeddings for five numbers, the task is to predict the index of the maximum number. Each list consists of values of similar magnitude in order to evaluate fine-grained comparisons (see Appendix C). As i"
D19-1534,P18-2102,0,0.027081,"struggles to extrapolate when trained on the pre-trained embeddings. 4 Discussion and Related Work An open question is how the training process elicits numeracy for word vectors and contextualized embeddings. Understanding this, perhaps by tracing numeracy back to the training data, is a fruitful direction to explore further (c.f., influence functions (Koh and Liang, 2017; Brunet et al., 2019)). More generally, numeracy is one type of emergent knowledge. For instance, embeddings may capture the size of objects (Forbes and Choi, 2017), speed of vehicles, and many other “commonsense” phenomena (Yang et al., 2018). Vendrov et al. (2016) introduce methods to encode the order of such phenomena into embeddings for concepts such as hypernymy; our work and Yang et al. (2018) show that a relative ordering naturally emerges for certain concepts. In concurrent work, Naik et al. (2019) also explore numeracy in word vectors. Their methodology is based on variants of nearest neighbors and cosine distance; we use neural network probing classifiers which can capture highly non-linear dependencies between embeddings. We also explore more powerful embedding methods such as ELMo, BERT, and learned embedding methods. P"
D19-3002,2021.ccl-1.108,0,0.152362,"Missing"
D19-3002,N19-1423,0,0.202062,"A sentiment model incorrectly predicts the positive class due to the trigram “tony hawk style”. 2 2.2 We consider three saliency methods. Since our goal is to interpret why the model made its prediction (not the ground-truth answer), we use the model’s own output in the loss calculation. For each method, we reduce each token’s gradient (which is the same dimension as the token embedding) to a single value by taking the L2 norm. Vanilla Gradient This method visualizes the gradient of the loss with respect to each token (Simonyan et al., 2014). Figure 2 shows an example interpretation of BERT (Devlin et al., 2019). Integrated Gradients Sundararajan et al. (2017) introduce integrated gradients. They 0 define a baseline x , which is an input absent of information (we use a sequence of all zero embeddings). Word importance is determined by integrating the gradient along the path from this baseline to the original input. SmoothGrad Smilkov et al. (2017) average the gradient over many noisy versions of the input. For NLP, we add small Gaussian noise to every embedding and take the average gradient value. 2.3 Interpreting Model Predictions Adversarial Attacks We consider two adversarial attacks: replacing wo"
D19-3002,N18-1202,1,0.569277,"nd BiDAF models (Seo et al., 2017). Our AllenNLP Extension The core backbone of our toolkit is an extension to the Predictor class that allows interpretation methods to compute input gradients in a model-agnostic way. Creating this extension has two main implementation challenges: (1) the loss (with the model’s own predictions as the labels) must be computed for widely varying output formats (e.g., classification, tagging, or language modeling), and (2) the gradient of this loss with respect to the token embeddings must be computed for widely varying embedding types (e.g., word vectors, ELMo (Peters et al., 2018) embeddings, BERT embeddings). • Masked Language Modeling using the transformer models available in Pytorch Transformers2 , e.g., BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and more. • Text Classification and Textual Entailment using BiLSTM and self-attention classifiers. • Named Entity Recognition (NER) and Coreference Resolution. These are examples of tasks with complex input-output structure; we can use the same function calls to analyze each predicted tag (e.g., Figure 1) or cluster. 1 We also adapt HotFlip to contextual embeddings; details provided in Section 3.2. 2 https://g"
D19-3002,N19-1246,1,0.888914,"Missing"
D19-3002,D16-1264,0,0.0653792,"Missing"
D19-3002,N16-3020,1,0.551905,"Interpret for NER. The model predicts three tags for an input (top). We interpret each tag separately, e.g., input reduction (Feng et al., 2018) (bottom) removes as many words as possible without changing a tag’s prediction. Input reduction shows that the words “named”, “at”, and “in downtown” are sufficient to predict the People, Organization, and Location tags, respectively. Instance-level interpretation methods help to answer this question by providing explanations for specific model predictions. These explanations come in many flavors, e.g., visualizing a model’s local decision boundary (Ribeiro et al., 2016), highlighting the saliency of the input features (Simonyan et al., 2014), or adversarially modifying the input (Ebrahimi et al., 2018). Interpretations are useful to illuminate the strengths and weaknesses of a model (Feng et al., 2018), increase user trust (Ribeiro et al., 2016), and evaluate hard-todefine criteria such as safety or fairness (DoshiVelez and Kim, 2017). Many open-source implementations exist for instance-level interpretation methods. However, most codebases focus on computer vision, are model- or task-specific (e.g., sentiment analysis), or contain implementations for a small"
D19-3002,P18-2006,0,0.158866,"al., 2018) (bottom) removes as many words as possible without changing a tag’s prediction. Input reduction shows that the words “named”, “at”, and “in downtown” are sufficient to predict the People, Organization, and Location tags, respectively. Instance-level interpretation methods help to answer this question by providing explanations for specific model predictions. These explanations come in many flavors, e.g., visualizing a model’s local decision boundary (Ribeiro et al., 2016), highlighting the saliency of the input features (Simonyan et al., 2014), or adversarially modifying the input (Ebrahimi et al., 2018). Interpretations are useful to illuminate the strengths and weaknesses of a model (Feng et al., 2018), increase user trust (Ribeiro et al., 2016), and evaluate hard-todefine criteria such as safety or fairness (DoshiVelez and Kim, 2017). Many open-source implementations exist for instance-level interpretation methods. However, most codebases focus on computer vision, are model- or task-specific (e.g., sentiment analysis), or contain implementations for a small number of interpretation methods. Thus, it is difficult for practitioners to interpret their model. As a reIntroduction Despite consta"
D19-3002,D18-1407,1,0.932932,"library of front-end visualization components. We demonstrate the toolkit’s flexibility and utility by implementing live demos for five interpretation methods (e.g., saliency maps and adversarial attacks) on a variety of models and tasks (e.g., masked language modeling using BERT and reading comprehension using BiDAF). These demos, alongside our code and tutorials, are available at https://allennlp. org/interpret. 1 Figure 1: An interpretation generated using AllenNLP Interpret for NER. The model predicts three tags for an input (top). We interpret each tag separately, e.g., input reduction (Feng et al., 2018) (bottom) removes as many words as possible without changing a tag’s prediction. Input reduction shows that the words “named”, “at”, and “in downtown” are sufficient to predict the People, Organization, and Location tags, respectively. Instance-level interpretation methods help to answer this question by providing explanations for specific model predictions. These explanations come in many flavors, e.g., visualizing a model’s local decision boundary (Ribeiro et al., 2016), highlighting the saliency of the input features (Simonyan et al., 2014), or adversarially modifying the input (Ebrahimi et"
D19-3002,N18-2017,0,0.0362058,"e hard-todefine criteria such as safety or fairness (DoshiVelez and Kim, 2017). Many open-source implementations exist for instance-level interpretation methods. However, most codebases focus on computer vision, are model- or task-specific (e.g., sentiment analysis), or contain implementations for a small number of interpretation methods. Thus, it is difficult for practitioners to interpret their model. As a reIntroduction Despite constant advances and seemingly superhuman performance on constrained domains, state-of-the-art models for NLP are imperfect: they latch on to superficial patterns (Gururangan et al., 2018), reflect unwanted social biases (Doshi-Velez and Kim, 2017), and significantly underperform humans on a myriad of tasks. These imperfections, coupled with today’s advances being driven by (seemingly black-box) neural models, leave researchers and practitioners scratching their heads, asking, “why did my model make this prediction?” 7 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 7–12 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics ing its weaknesses. We focus on methods that modify tokens in the input (e.g., replace or"
D19-3002,D18-2007,0,0.0272083,"sualization (requires making a one-line call to the reusable front-end components). isolate the effect of individual neurons (Karpathy et al., 2016). We focus on gradient-based methods because they are applicable to many models. Existing Interpretation Toolkits In computer vision, various open-source toolkits exist for explaining and attacking models (e.g., Papernot et al. (2016); Ozbulak (2019), inter alia); some toolkits also include interactive demos (Norton and Qi, 2017). Similar toolkits for NLP are significantly scarcer, and most toolkits focus on specific models or tasks. For instance, Liu et al. (2018), Strobelt et al. (2019), and Vig (2019) visualize attention weights for specific NLP models, while Lee et al. (2019) apply adversarial attacks to reading comprehension systems. Our toolkit differs because it is flexible and diverse; we can interpret and attack any AllenNLP model. New Model We also provide a tutorial for interpreting a new model. If your task is already available in the demos (e.g., text classification), you need to change a single line of code to replace the demo model with your model. If your task is not present in the demos, you will need to: 1. Write the predictions to lab"
D19-3002,W18-5416,1,0.840737,"the AllenNLP Demo, a web application for running AllenNLP models. We add HTML and JavaScript components that provide visualizations for saliency maps and adversarial attacks. These components are reusable and greatly simplify the process for adding new models and interpretation methods (Section 4). For example, a single line of HTML code can create the visualizations shown in Figures 1–3. Note that visualizing the interpretations is not required—AllenNLP Interpret can be run in an offline, batch manner. This is useful for aggregating interpretation results, e.g., as in Feng et al. (2018) and Wallace et al. (2018). Embedding-Agnostic Gradients To handle difficulty (2)—computing the gradients of varying token embeddings—we rely on the abstractions of AllenNLP. In particular, AllenNLP uses a TokenEmbedder interface to converts token ids into embeddings. We can thus compute the gradient for any embedding method by registering a PyTorch backward gradient hook on the model’s TokenEmbedder function. Our end result is a simple API for computing input gradients for any model: call predictions to labeled instances() and then get gradients(). 4 Adding a Model or Interpretation Context-Independent Embedding Matri"
D19-5817,W05-0909,0,0.426678,". In this work, we study all mentioned metrics in the context of question answering. BLEU is a precision-based metric developed for evaluating machine translation (Papineni et al., 2001). BLEU scores a candidate by computing the number of n-grams in the candidate that also appear in a reference. n is varied from 1 up to a specified N and the scores for varying n are aggregated with a geometric mean. In this work, we look at BLEU-1 and BLEU-4, where N = 1 and N = 4 respectively. METEOR is an F-measure metric developed for evaluating machine translation which operates on unigrams (i.e. tokens) (Banerjee and Lavie, 2005). METEOR first creates an alignment by attempting to map each token in a candidate to a token in a reference (and vice versa). A token is aligned to another token if they are the same, are synonyms, or their stems match. The alignment is aggregated into precision and recall values, which are combined into an F-measure score in which more weight is given to recall. ROUGE is an F-measure metric designed for evaluating translation and summarization (Lin, 2004). There are a number of variants of ROUGE however in this work we focus on ROUGE-L. ROUGE-L is computed based on the longest common subsequ"
D19-5817,D18-1454,0,0.0486278,"of ROPES is F1 . A unique characteristic of ROPES is that questions generally present two possible answer choices, one of which is incorrect (Table 1). Because incorrect and correct answers often have some n-gram overlap, we believe F1 will struggle to accurately assign scores (Figure 1b). 4 Models We describe the models used to generate predictions for our datasets. These models have publicly available code and have reasonable performance compared to the current state-of-the-art models. Multi-hop Point Generator For NarrativeQA and SemEval, we use a multi-hop pointer generator (MHPG) model (Bauer et al., 2018)3 . MHPG represents its input using ELMo embeddings. The embeddings are then fed into a sequence of BiDAF (Seo et al., 2017) cells, where the output of one BiDAF cell is fed as the input into another BiDAF cell. This allows multi-hop reasoning over the context. The output layer consists of a generative decoder with a copying mechanism. We evaluate MHPG’s predictions using BLEU-1, BLEU-4, ROUGE-L, METEOR, SMS, BERTScore and Conditional BERTScore. BERT For ROPES, we finetune BERT as a span based QA model following the procedure used for 3 https://github.com/yicheng-w/ CommonSenseMultiHopQA https"
D19-5817,C04-1046,0,0.246488,"Missing"
D19-5817,P19-1264,0,0.0264628,"e (LCS), which searches for the longest co-occurring set of tokens common to both reference and candidate. An advantage of ROUGEL is that no predefined n-gram size is required. F1 While the previously mentioned metrics have been adapted for evaluating generative question answering, F1 has been generally reserved for evaluating span-based question answering (Rajpurkar et al., 2016). It is computed over tokens in the candidate and reference. Sentence Mover’s Similarity (SMS) is a recent metric based on earth mover’s distance for evaluated multi-sentence texts such as machinegenerated summaries (Clark et al., 2019) .1 SMS 120 1 https://github.com/eaclark07/sms first computes an embedding for each sentence in a document as an average its ELMo word representations (Peters et al., 2018). A linear program is then solved to obtain the distance of “moving” a candidate document’s sentences to match a reference document. SMS has shown better results over ROUGE-L in evaluating generated summaries and student essays. BERTScore is recent metric for evaluating translation (Zhang et al., 2019).2 BERTScore first obtains BERT representations of each word in the candidate and reference by feeding the candidate and refe"
D19-5817,N19-1423,0,0.033544,"d what batteries it required: two AA-sized batteries . . . Why did they throw away the old batteries? They were no longer useful ROPES 11,202 . . . A catalyst is a chemical that speeds up chemical reactions . . . [Mark] conducts two tests, test A and test B, on an organism. In test A he reduces catalysts from the organism, but in test B he induces catalysts in the organism ... Which test would see reactions taking place slower, test A or test B? test A Table 1: Examples for the datasets we use in our study. The # of QA Pairs column refers to the number of QA pairs in the training sets. SQuAD (Devlin et al., 2019). We evaluate BERT’s predictions using F1 , SMS, BERTScore, and Conditional BERTScore. 5 5.1 Evaluating QA Metrics using Human Judgements 5.3 Collecting Human Judgements After training our models on the three datasets, we extract 500, 500, and 300 data points from the validation sets of NarrativeQA, ROPES, and SemEval, respectively, along with the model predictions. When extracting data points to label, we filter out data points where the predicted answer exactly matches the gold answer. This filtering step is done as we are interested on how well metrics do when it cannot resort to exact stri"
D19-5817,N19-1246,1,0.798824,". Figure 1: Examples where existing n-gram based metrics fail to align with human judgements. Human judgements are between 1 and 5. (a) illustrates that because existing metrics do not use the context, they fail to capture coreferences. (b) illustrates that changing a single token can make a prediction incorrect while F1 assigns a non-zero score. Introduction Question answering (QA) has emerged as a burgeoning research field driven by the availability of large datasets. These datasets are built to test a variety of reading comprehension skills such as multihop (Welbl et al., 2017), numerical (Dua et al., 2019), and commonsense (Talmor et al., 2018) reasoning. A key component of a QA dataset is the evaluation metric associated with it, which aims to automatically approximate human accuracy judgments of a predicted answer against a gold answer. The metrics used to evaluate QA datasets have a number of ramifications. The first is that they drive research focus. Models that rank higher on a leaderboard according to a metric will receive 119 Proceedings of the Second Workshop on Machine Reading for Question Answering, pages 119–124 c Hong Kong, China, November 4, 2019. 2019 Association for Computational"
D19-5817,W04-1013,0,0.0728504,"ively. METEOR is an F-measure metric developed for evaluating machine translation which operates on unigrams (i.e. tokens) (Banerjee and Lavie, 2005). METEOR first creates an alignment by attempting to map each token in a candidate to a token in a reference (and vice versa). A token is aligned to another token if they are the same, are synonyms, or their stems match. The alignment is aggregated into precision and recall values, which are combined into an F-measure score in which more weight is given to recall. ROUGE is an F-measure metric designed for evaluating translation and summarization (Lin, 2004). There are a number of variants of ROUGE however in this work we focus on ROUGE-L. ROUGE-L is computed based on the longest common subsequence (LCS), which searches for the longest co-occurring set of tokens common to both reference and candidate. An advantage of ROUGEL is that no predefined n-gram size is required. F1 While the previously mentioned metrics have been adapted for evaluating generative question answering, F1 has been generally reserved for evaluating span-based question answering (Rajpurkar et al., 2016). It is computed over tokens in the candidate and reference. Sentence Mover"
D19-5817,D19-5808,1,0.902171,"t is crucial that the metrics we use are able to assign scores that accurately reflect human judgements. Despite the value of metrics as drivers of research, a comprehensive study of QA metrics across a number of datasets has yet to be completed. This is important as present metrics are based on n-gram matching, which have a number of shortcomings (Figure 1). In this work, we survey the landscape of evaluation metrics for QA and study how well current metrics approximate (i.e. correlate with) human judgements. We conduct our study on three datasets: NarrativeQA (Kocisk´y et al., 2017), ROPES (Lin et al., 2019), and SemEval-2018 Task 11 (Ostermann et al., 2018). For the generative NarrativeQA dataset, we find that existing metrics provide reasonable correlation with human accuracy judgements while still leaving considerable room for improvement. We also study the span-based ROPES dataset, finding that it presents an interesting case where F1 struggles due to the high overlap in right and wrong answers. Finally, we convert the multiple-choice SemEval-2018 Task 11 dataset into a generative QA dataset. This produces a more difficult generative QA dataset compared to NarrativeQA as answers in SemEval ar"
D19-5817,D18-1429,0,0.0405241,"/question/answer triples during its pretraining. Finetuning a BERT model on QA datasets can potentially yield a better BERTScore-based metric. 6 Related Work N-gram based metrics such as BLEU and METEOR were originally developed and tested for evaluation of machine translation. These metrics have grown to become popular choices in evaluating all forms of natural language generation, including image captioning, question answering, and dialog systems. As these metrics continue to be used, there have been a number of papers that try to assess how suitable these metrics are for different domains. Nema and Khapra (2018) show that for question generation, n-gram metrics assign scores that correlate poorly to the notion of answerability (i.e., is a generated question answerable). Yang et al. (2018) study the effect of using BLEU and ROUGE in evaluating QA, focusing on yes-no and entity questions on the Chinese DuReader dataset (He et al., 2017). For these types of questions, changing a single word from a gold answer can lead to an incorrect answer. In these cases, BLEU and ROUGE assign scores that do not necessarily reflect the correctness of an answer. Our work is continuation of this line of work in assessin"
D19-5817,S18-1119,0,0.147813,"to assign scores that accurately reflect human judgements. Despite the value of metrics as drivers of research, a comprehensive study of QA metrics across a number of datasets has yet to be completed. This is important as present metrics are based on n-gram matching, which have a number of shortcomings (Figure 1). In this work, we survey the landscape of evaluation metrics for QA and study how well current metrics approximate (i.e. correlate with) human judgements. We conduct our study on three datasets: NarrativeQA (Kocisk´y et al., 2017), ROPES (Lin et al., 2019), and SemEval-2018 Task 11 (Ostermann et al., 2018). For the generative NarrativeQA dataset, we find that existing metrics provide reasonable correlation with human accuracy judgements while still leaving considerable room for improvement. We also study the span-based ROPES dataset, finding that it presents an interesting case where F1 struggles due to the high overlap in right and wrong answers. Finally, we convert the multiple-choice SemEval-2018 Task 11 dataset into a generative QA dataset. This produces a more difficult generative QA dataset compared to NarrativeQA as answers in SemEval are often more free-form in nature and have less over"
D19-5817,2001.mtsummit-papers.68,0,0.0372419,"ghtly improves results when evaluating generative QA, though not to an extant that is statistically significant. Overall, our results indicate that studying the evaluation of QA is an underresearched area with substantial room for further experimentation. 2 Metrics We provide a summary of popular n-gram based metrics, as well as sentence mover’s similarity, BERTScore, and an extension of BERTScore which we call conditional BERTScore. In this work, we study all mentioned metrics in the context of question answering. BLEU is a precision-based metric developed for evaluating machine translation (Papineni et al., 2001). BLEU scores a candidate by computing the number of n-grams in the candidate that also appear in a reference. n is varied from 1 up to a specified N and the scores for varying n are aggregated with a geometric mean. In this work, we look at BLEU-1 and BLEU-4, where N = 1 and N = 4 respectively. METEOR is an F-measure metric developed for evaluating machine translation which operates on unigrams (i.e. tokens) (Banerjee and Lavie, 2005). METEOR first creates an alignment by attempting to map each token in a candidate to a token in a reference (and vice versa). A token is aligned to another toke"
D19-5817,N18-1202,1,0.468177,"ired. F1 While the previously mentioned metrics have been adapted for evaluating generative question answering, F1 has been generally reserved for evaluating span-based question answering (Rajpurkar et al., 2016). It is computed over tokens in the candidate and reference. Sentence Mover’s Similarity (SMS) is a recent metric based on earth mover’s distance for evaluated multi-sentence texts such as machinegenerated summaries (Clark et al., 2019) .1 SMS 120 1 https://github.com/eaclark07/sms first computes an embedding for each sentence in a document as an average its ELMo word representations (Peters et al., 2018). A linear program is then solved to obtain the distance of “moving” a candidate document’s sentences to match a reference document. SMS has shown better results over ROUGE-L in evaluating generated summaries and student essays. BERTScore is recent metric for evaluating translation (Zhang et al., 2019).2 BERTScore first obtains BERT representations of each word in the candidate and reference by feeding the candidate and reference through a BERT model separately. An alignment is then computed between candidate and reference words by computing pairwise cosine similarity. This alignment is then a"
D19-5817,W18-2611,0,0.0400521,"as BLEU and METEOR were originally developed and tested for evaluation of machine translation. These metrics have grown to become popular choices in evaluating all forms of natural language generation, including image captioning, question answering, and dialog systems. As these metrics continue to be used, there have been a number of papers that try to assess how suitable these metrics are for different domains. Nema and Khapra (2018) show that for question generation, n-gram metrics assign scores that correlate poorly to the notion of answerability (i.e., is a generated question answerable). Yang et al. (2018) study the effect of using BLEU and ROUGE in evaluating QA, focusing on yes-no and entity questions on the Chinese DuReader dataset (He et al., 2017). For these types of questions, changing a single word from a gold answer can lead to an incorrect answer. In these cases, BLEU and ROUGE assign scores that do not necessarily reflect the correctness of an answer. Our work is continuation of this line of work in assessing the quality of current metrics for use in evaluating question answering across a number of datasets. Because of the inherent limitations of n-gram metrics, recent work has focuse"
D19-5820,D19-1606,1,0.916125,"le test bed for exploring training paradigms and representation learning for general reading facility. As more suitable datasets are released, they will be added to the evaluation server. We also collect and include synthetic augmentations for these datasets, testing how well models can handle out-of-domain questions. 1 Introduction Research in reading comprehension, the task of answering questions about a given passage of text, has seen a huge surge of interest in recent years, with many large datasets introduced targeting various aspects of reading (Rajpurkar et al., 2016; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019). However, as the number of datasets increases, evaluation on all of them becomes challenging, encouraging researchers to overfit to the biases of a single dataset. Recent research, including MultiQA (Talmor and Berant, 2019) and the MRQA workshop shared task, aim to facilitate training and evaluating on several reading comprehension datasets at 2 Datasets We selected seven existing datasets that target various complex linguistic phenomena such as coreference resolution, entity and event detection, etc., capabilities which are desirable when testing a model for reading compr"
D19-5820,N19-1423,0,0.0550328,"Missing"
D19-5820,N19-1246,1,0.9285,"so it is a suitable test bed for exploring training paradigms and representation learning for general reading facility. As more suitable datasets are released, they will be added to the evaluation server. We also collect and include synthetic augmentations for these datasets, testing how well models can handle out-of-domain questions. 1 Introduction Research in reading comprehension, the task of answering questions about a given passage of text, has seen a huge surge of interest in recent years, with many large datasets introduced targeting various aspects of reading (Rajpurkar et al., 2016; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019). However, as the number of datasets increases, evaluation on all of them becomes challenging, encouraging researchers to overfit to the biases of a single dataset. Recent research, including MultiQA (Talmor and Berant, 2019) and the MRQA workshop shared task, aim to facilitate training and evaluating on several reading comprehension datasets at 2 Datasets We selected seven existing datasets that target various complex linguistic phenomena such as coreference resolution, entity and event detection, etc., capabilities which are desirable when testing a mo"
D19-5820,P19-1485,1,0.919946,"for these datasets, testing how well models can handle out-of-domain questions. 1 Introduction Research in reading comprehension, the task of answering questions about a given passage of text, has seen a huge surge of interest in recent years, with many large datasets introduced targeting various aspects of reading (Rajpurkar et al., 2016; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019). However, as the number of datasets increases, evaluation on all of them becomes challenging, encouraging researchers to overfit to the biases of a single dataset. Recent research, including MultiQA (Talmor and Berant, 2019) and the MRQA workshop shared task, aim to facilitate training and evaluating on several reading comprehension datasets at 2 Datasets We selected seven existing datasets that target various complex linguistic phenomena such as coreference resolution, entity and event detection, etc., capabilities which are desirable when testing a model for reading comprehension. We chose datasets that adhere to two main properties: First, we exclude from consideration any multiple choice dataset, as these typically require very different model architectures, and they often have biases in how the dis147 Procee"
D19-5820,D17-1215,0,0.0278269,"name in the question for a different name from the passage to create with high probability a new question with no answer. Table 1: Dataset Statistics from summaries. In addition, crowd workers were required to provide answers that do not have high overlap with the context. In accordance with our format, we only use the version with the summaries as context in our evaluation server. 3 IC SEARs creates minimal changes in word selection or grammar while maintaining the original meaning of the question according to the rules described by Ribeiro et al. (2018). Synthetic Augmentations Prior works (Jia and Liang, 2017) have shown that RC models are brittle to minor perturbations in original dataset. Hence, to test the model’s ability to generalize to out-of-domain syntactic structures and be logically consistent in its answers, we automatically generate questions based on various heuristics. These heuristics fall in two broad categories. 1. The question is paraphrased to a minimal extent to create new syntactic structures, keeping the semantics of the question largely intact and without making any changes to the original context and answer. 2. The predicate-argument structures of a given question-answer pai"
D19-5820,W17-2623,0,0.0616804,"text, particularly dealing with the language of causes and effects. A system is given a background passage, perhaps describing the effects of deforestation on local climate and ecosystems, and a grounded situation involving the knowledge in the background passage, such as, City A has more trees than City B. The questions then require grounding the effects described in the background, perhaps querying which city would more likely have greater ecological diversity. This dataset can be helpful in understanding how to apply the knowledge contained in a passage of text to a new situation. NewsQA (Trischler et al., 2017) dataset focuses on paraphrased questions with predicate-argument structure understanding. To some extent it is similar to DuoRC, however the examples are collected from news articles and offers diverse linguistic structures. This crowd-sourced dataset was created by asking annotators to write questions from CNN/DailyMail articles as context. NarrativeQA (Koˇcisk´y et al., 2018) focuses on understanding temporal reasoning among various events that happen in a given movie plot. It also tests the models ability to “hop” between various parts of the context and not rely solely on sequential reaso"
D19-5820,Q18-1023,0,0.0906264,"Missing"
D19-5820,D19-5808,1,0.90129,"ring training paradigms and representation learning for general reading facility. As more suitable datasets are released, they will be added to the evaluation server. We also collect and include synthetic augmentations for these datasets, testing how well models can handle out-of-domain questions. 1 Introduction Research in reading comprehension, the task of answering questions about a given passage of text, has seen a huge surge of interest in recent years, with many large datasets introduced targeting various aspects of reading (Rajpurkar et al., 2016; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019). However, as the number of datasets increases, evaluation on all of them becomes challenging, encouraging researchers to overfit to the biases of a single dataset. Recent research, including MultiQA (Talmor and Berant, 2019) and the MRQA workshop shared task, aim to facilitate training and evaluating on several reading comprehension datasets at 2 Datasets We selected seven existing datasets that target various complex linguistic phenomena such as coreference resolution, entity and event detection, etc., capabilities which are desirable when testing a model for reading comprehension. We chose"
D19-5820,D17-2014,0,0.0248808,"s a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. We differ from GLUE and S ENT E VAL by focusing on reading comprehension tasks, and only evaluating a single model on all datasets, instead of allowing the model to be tuned to each dataset separately. Evaluation Platforms and Competitions in NLP The use of online evaluation platform with private test labels has been exercised by various leaderboards on Kaggle and CodaLab, as well as shared tasks at the SemEval and CoNLL conferences. Additional benchmarks such as PARL AI (Miller et al., 2017) and BA B I (Weston et al., 2016) proposed a hierarchy of tasks towards building question answering and reasoning models and language understanding. However these frameworks do not include a standardized evaluation suite for system performance nor do they offer a wide set of reading comprehension tasks. 6 Conclusion We have presented ORB, an open reading benchmark designed to be a comprehensive test of reading comprehension systems, in terms of their gen152 eralizability, understanding of various natural language phenomenon, capability to make consistent predictions, and ability to handle out-"
D19-5820,P18-2124,0,0.0398538,"ept where noted, we include both the development and test sets (including hidden test sets) in our evaluation server for all datasets. SQuAD (Rajpurkar et al., 2016) requires a model to perform lexical matching between the context and the question to predict the answer. This dataset provides avenues to learn predicate-argument structure and multi-sentence reasoning to some extent. It was collected by asking crowd-workers to create question-answer pairs from Wikipedia articles such that the answer is a single-span in the context. The dataset was later updated to include unanswerable questions (Rajpurkar et al., 2018), giving a harder question set without as many reasoning shortcuts. We include only the development sets of SQuAD 1.1 and SQuAD 2.0 in our evaluation server. DuoRC (Saha et al., 2018) tests if the model can generalize to answering semantically similar but syntactically different paraphrased questions. The questions are created on movie summaries obtained from two sources, Wikipedia and IMDB. The crowd-workers formalized questions based on Wikipedia contexts and in turn answered them based on the IMDB context. This ensured that the model will not rely solely on lexical matching, but rather util"
D19-5820,D16-1264,0,0.387419,"how models are trained, so it is a suitable test bed for exploring training paradigms and representation learning for general reading facility. As more suitable datasets are released, they will be added to the evaluation server. We also collect and include synthetic augmentations for these datasets, testing how well models can handle out-of-domain questions. 1 Introduction Research in reading comprehension, the task of answering questions about a given passage of text, has seen a huge surge of interest in recent years, with many large datasets introduced targeting various aspects of reading (Rajpurkar et al., 2016; Dua et al., 2019; Dasigi et al., 2019; Lin et al., 2019). However, as the number of datasets increases, evaluation on all of them becomes challenging, encouraging researchers to overfit to the biases of a single dataset. Recent research, including MultiQA (Talmor and Berant, 2019) and the MRQA workshop shared task, aim to facilitate training and evaluating on several reading comprehension datasets at 2 Datasets We selected seven existing datasets that target various complex linguistic phenomena such as coreference resolution, entity and event detection, etc., capabilities which are desirable"
D19-5820,P19-1621,1,0.842061,"f-domain syntactic structures and be logically consistent in its answers, we automatically generate questions based on various heuristics. These heuristics fall in two broad categories. 1. The question is paraphrased to a minimal extent to create new syntactic structures, keeping the semantics of the question largely intact and without making any changes to the original context and answer. 2. The predicate-argument structures of a given question-answer pair are leveraged to create new WH-questions based on the object in the question instead of the subject. This rulebased method, adopted from (Ribeiro et al., 2019), changes the question and answer keeping the context fixed. We use five augmentation techniques, where the first four techniques fall into the first category and the last technique falls into the second category. Invert Choice transforms a binary choice question by changing the order in which the choices are presented, keeping the answer the same. More Wrong Choice transforms a binary choice question by substituting the wrong choice in the question with another wrong choice from the passage. Implication creates a new question-answer pair, where the object of the original question is replaced"
D19-5820,P18-1079,1,0.90589,"ets in some cases. We focus on datasets where the core problem is natural language understanding, not information retrieval; models are given a single passage of text and a single question and are required to produce an answer. As our goal is to provide a broad suite of questions that test a single model’s reading ability, we additionally provide synthetic augmentations to some of the datasets in our evaluation server. Several recent papers have proposed question transformations that result in out-of-distribution test examples, helping to judge the generalization capability of reading models (Ribeiro et al., 2018, 2019; Zhu et al., 2019). We collect the best of these, add some of our own, and keep those that generate reasonable and challenging questions. We believe this strategy of evaluating on many datasets, including distribution-shifted synthetic examples, will lead the field towards more robust and comprehensive reading comprehension models. Code for the evaluation server, including a script to run it on the dev sets of these datasets and a leaderboard showing results on their hidden tests, can be found at https://leaderboard. allenai.org/orb Reading comprehension is one of the crucial tasks for"
D19-5820,P18-1156,0,0.0158642,"rform lexical matching between the context and the question to predict the answer. This dataset provides avenues to learn predicate-argument structure and multi-sentence reasoning to some extent. It was collected by asking crowd-workers to create question-answer pairs from Wikipedia articles such that the answer is a single-span in the context. The dataset was later updated to include unanswerable questions (Rajpurkar et al., 2018), giving a harder question set without as many reasoning shortcuts. We include only the development sets of SQuAD 1.1 and SQuAD 2.0 in our evaluation server. DuoRC (Saha et al., 2018) tests if the model can generalize to answering semantically similar but syntactically different paraphrased questions. The questions are created on movie summaries obtained from two sources, Wikipedia and IMDB. The crowd-workers formalized questions based on Wikipedia contexts and in turn answered them based on the IMDB context. This ensured that the model will not rely solely on lexical matching, but rather utilize semantic understanding. The answer can be either a single-span from context or free form text written by the annotator. Quoref (Dasigi et al., 2019) focuses on understanding coref"
D19-5820,W18-5446,0,0.05862,"Missing"
D19-5820,P19-1415,0,0.0115705,"on datasets where the core problem is natural language understanding, not information retrieval; models are given a single passage of text and a single question and are required to produce an answer. As our goal is to provide a broad suite of questions that test a single model’s reading ability, we additionally provide synthetic augmentations to some of the datasets in our evaluation server. Several recent papers have proposed question transformations that result in out-of-distribution test examples, helping to judge the generalization capability of reading models (Ribeiro et al., 2018, 2019; Zhu et al., 2019). We collect the best of these, add some of our own, and keep those that generate reasonable and challenging questions. We believe this strategy of evaluating on many datasets, including distribution-shifted synthetic examples, will lead the field towards more robust and comprehensive reading comprehension models. Code for the evaluation server, including a script to run it on the dev sets of these datasets and a leaderboard showing results on their hidden tests, can be found at https://leaderboard. allenai.org/orb Reading comprehension is one of the crucial tasks for furthering research in na"
N10-1009,D07-1087,0,0.126959,"sed learning allows domain experts to easily provide additional light supervision, enabling the learning algorithm to learn using the prior domain knowledge, labeled and unlabeled data (Chang et al., 2007; Mann and McCallum, 2008; Bellare et al., 2009; Singh et al., 2010). Prior domain knowledge, if it can be easily expressed and incorporated into the learning algorithm, can often be a high-quality and cheap substitute for labeled data. For example, previous work has often used dictionaries or lexicons (lists of phrases of a particular label) to bootstrap the model (Agichtein and Ganti, 2004; Canisius and Sporleder, 2007), leading to a partial labeling of the data. Domain knowledge can also be more probabilistic in nature, representing the probability of certain token taking on a certain label. For most tasks, labeled data is a convenient representation of the domain knowledge, but for complex domains such as structured information extraction from ads, these alternative easily expressible representations may be as effective as labeled data. Our approach to solving the the named entity extraction problem for ads relies completely on domain knowledge not expressed as labeled data, an approach that is termed mini"
N10-1009,P07-1036,0,0.453038,"unter the noisy and sparse labels, semi-supervised learning meth73 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 73–81, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics ods utilize unlabeled data to improve the model (see (Chapelle et al., 2006) for an overview). Furthermore, recent work on constraint-based semisupervised learning allows domain experts to easily provide additional light supervision, enabling the learning algorithm to learn using the prior domain knowledge, labeled and unlabeled data (Chang et al., 2007; Mann and McCallum, 2008; Bellare et al., 2009; Singh et al., 2010). Prior domain knowledge, if it can be easily expressed and incorporated into the learning algorithm, can often be a high-quality and cheap substitute for labeled data. For example, previous work has often used dictionaries or lexicons (lists of phrases of a particular label) to bootstrap the model (Agichtein and Ganti, 2004; Canisius and Sporleder, 2007), leading to a partial labeling of the data. Domain knowledge can also be more probabilistic in nature, representing the probability of certain token taking on a certain label"
N10-1009,P08-1099,0,0.609766,"sparse labels, semi-supervised learning meth73 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 73–81, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics ods utilize unlabeled data to improve the model (see (Chapelle et al., 2006) for an overview). Furthermore, recent work on constraint-based semisupervised learning allows domain experts to easily provide additional light supervision, enabling the learning algorithm to learn using the prior domain knowledge, labeled and unlabeled data (Chang et al., 2007; Mann and McCallum, 2008; Bellare et al., 2009; Singh et al., 2010). Prior domain knowledge, if it can be easily expressed and incorporated into the learning algorithm, can often be a high-quality and cheap substitute for labeled data. For example, previous work has often used dictionaries or lexicons (lists of phrases of a particular label) to bootstrap the model (Agichtein and Ganti, 2004; Canisius and Sporleder, 2007), leading to a partial labeling of the data. Domain knowledge can also be more probabilistic in nature, representing the probability of certain token taking on a certain label. For most tasks, labeled"
N10-1009,J87-3015,0,0.594973,"Missing"
N10-1009,N10-1111,1,0.850866,"Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 73–81, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics ods utilize unlabeled data to improve the model (see (Chapelle et al., 2006) for an overview). Furthermore, recent work on constraint-based semisupervised learning allows domain experts to easily provide additional light supervision, enabling the learning algorithm to learn using the prior domain knowledge, labeled and unlabeled data (Chang et al., 2007; Mann and McCallum, 2008; Bellare et al., 2009; Singh et al., 2010). Prior domain knowledge, if it can be easily expressed and incorporated into the learning algorithm, can often be a high-quality and cheap substitute for labeled data. For example, previous work has often used dictionaries or lexicons (lists of phrases of a particular label) to bootstrap the model (Agichtein and Ganti, 2004; Canisius and Sporleder, 2007), leading to a partial labeling of the data. Domain knowledge can also be more probabilistic in nature, representing the probability of certain token taking on a certain label. For most tasks, labeled data is a convenient representation of the"
N10-1009,D09-1098,0,\N,Missing
N10-1111,P07-1036,0,0.584164,"the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before parameters are updated also exists in the semi-supervised learning scenario. Here the situation is often considerably worse since inference has to be applied to potentially very large unlabeled datasets. Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al., 2007). Calculating these is computationally inexpensive for many simple tasks (such as classification and regression). However, marginal and MAP inference tends to be expensive for complex structured prediction models (such as the joint information extraction models of Singh et al. (2009)), making semisupervised learning intractable. In this work we employ a fast rank-based learning algorithm for semi-supervised learning to circumvent the inference bottleneck. The ranking function is extended to capture both the preference expressed by the labeled data, and the preference of the domain expert when"
N10-1111,W02-1001,0,0.0567544,"putational bottleneck. Different approaches to incorporate unlabeled data and prior knowledge into this framework are explored. When evaluated on a standard information extraction dataset, our method significantly outperforms the supervised method, and matches results of a competing state-of-the-art semi-supervised learning approach. 1 Introduction Most supervised learning algorithms for undirected graphical models require full inference over the dataset (e.g., gradient descent), small subsets of the dataset (e.g., stochastic gradient descent), or at least a single instance (e.g., perceptron, Collins (2002)) before parameter updates are made. Often this is the main computational bottleneck during training. SampleRank (Wick et al., 2009) is a rank-based learning framework that alleviates this problem by performing parameter updates within inference. Every pair of samples generated during inference is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before"
N10-1111,N07-1011,1,0.891642,"Missing"
N10-1111,D09-1134,0,0.0115376,"ser-defined constraints. By directly incorporating the model score and the constraints (as in Fmc in Section 3.3) we follow the same approach, but avoid the expensive “Top-K” inference step. Generalized expectation criterion (GE, Mann and McCallum, 2008) and Alternating Projections (AP, Bellare et al., 2009) encode preferences by specifying constraints on feature expectations, which require expensive inference. Although AP can use online training, it still involves full inference over each 731 instance. Furthermore, these methods only support constraints that factorize according to the model. Li (2009) incorporates prior knowledge into conditional random fields as variables. They require full inference during learning, restricting the application to simple models. Furthermore, higher-order constraints are specified using large cliques in the graph, which slow down inference. Our approach directly incorporates these constraints into the ranking function, with no impact on inference time. 5 Experiments We carried out experiments on the Cora citation dataset. The task is to segment each citation into different fields, such as “author” and “title”. We use 300 instances as training data, 100 ins"
N10-1111,P08-1099,1,0.957678,"generated during inference is ranked according to the model and the ground truth, and the parameters are updated when the rankings disagree. SampleRank has enabled efficient learning for massive information extraction tasks (Culotta et al., 2007; Singh et al., 2009). The problem of requiring a complete inference iteration before parameters are updated also exists in the semi-supervised learning scenario. Here the situation is often considerably worse since inference has to be applied to potentially very large unlabeled datasets. Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al., 2007). Calculating these is computationally inexpensive for many simple tasks (such as classification and regression). However, marginal and MAP inference tends to be expensive for complex structured prediction models (such as the joint information extraction models of Singh et al. (2009)), making semisupervised learning intractable. In this work we employ a fast rank-based learning algorithm for semi-supervised learning to circumvent the inference bottleneck. The ranking function is extended to capture both the preference expressed by the labeled data,"
N15-1118,C14-1197,0,0.0360344,"Missing"
N15-1118,S13-1002,0,0.0188067,"s paper can be incorporated with any embedding-based method that uses a per-atom loss. Logical Inference A common alternative that directly incorporates first-order logic knowledge is to perform logical inference (Bos and Markert, 2005; Baader et al., 2007; Bos, 2008), however such purely symbolic approaches cannot deal with the uncertainty inherent to natural language, and generalize poorly. 1126 Probabilistic Inference To ameliorate some of the drawbacks of symbolic logical inference, probabilistic logic based approaches have been proposed (Schoenmackers et al., 2008; Garrette et al., 2011; Beltagy et al., 2013; Beltagy et al., 2014). Since logical connections between relations are modeled explicitly, such approaches are generally hard to scale. Specifically, approaches based on Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) encode logical knowledge in dense, loopy graphical models, making structure learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a natural way to deal wit"
N15-1118,P14-1114,0,0.0110345,"rated with any embedding-based method that uses a per-atom loss. Logical Inference A common alternative that directly incorporates first-order logic knowledge is to perform logical inference (Bos and Markert, 2005; Baader et al., 2007; Bos, 2008), however such purely symbolic approaches cannot deal with the uncertainty inherent to natural language, and generalize poorly. 1126 Probabilistic Inference To ameliorate some of the drawbacks of symbolic logical inference, probabilistic logic based approaches have been proposed (Schoenmackers et al., 2008; Garrette et al., 2011; Beltagy et al., 2013; Beltagy et al., 2014). Since logical connections between relations are modeled explicitly, such approaches are generally hard to scale. Specifically, approaches based on Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) encode logical knowledge in dense, loopy graphical models, making structure learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a natural way to deal with linguistic ambiguitie"
N15-1118,H05-1079,0,0.155091,"on Matrix factorization is capable of learning complex dependencies between relations, but requires observed facts as training signal. However, often we either do not have this signal because the relations of interest do not have pre-existing facts, or this signal is noisy due to alignment errors or mismatches when linking knowledge base entities to mentions in text. To overcome this problem we investigate the use of first-order logic background knowledge (e.g. implications) to aid relation extraction. One option is to rely on a fully symbolic approach that exclusively uses first-order logic (Bos and Markert, 2005; Baader et al., 2007; Bos, 2008). In this case incorporating additional background knowledge is trivial. However, it is difficult to generalize and deal with noise and uncertainty in language when relying only on manual rules. In contrast, matrix factorization methods can overcome these shortcomings, but it is not clear how they can be combined with logic formulae. In this section, we propose to inject formulae into the embeddings of relations and entity-pairs, i.e., estimate the embeddings such that predictions based on them conform to given logic formulae (see Figure 1 for an overview). We"
N15-1118,W08-2222,0,0.0483834,"ng complex dependencies between relations, but requires observed facts as training signal. However, often we either do not have this signal because the relations of interest do not have pre-existing facts, or this signal is noisy due to alignment errors or mismatches when linking knowledge base entities to mentions in text. To overcome this problem we investigate the use of first-order logic background knowledge (e.g. implications) to aid relation extraction. One option is to rely on a fully symbolic approach that exclusively uses first-order logic (Bos and Markert, 2005; Baader et al., 2007; Bos, 2008). In this case incorporating additional background knowledge is trivial. However, it is difficult to generalize and deal with noise and uncertainty in language when relying only on manual rules. In contrast, matrix factorization methods can overcome these shortcomings, but it is not clear how they can be combined with logic formulae. In this section, we propose to inject formulae into the embeddings of relations and entity-pairs, i.e., estimate the embeddings such that predictions based on them conform to given logic formulae (see Figure 1 for an overview). We refer to such embeddings as low-r"
N15-1118,P07-1073,0,0.0854988,"Missing"
N15-1118,P07-1036,0,0.0546698,"ense, loopy graphical models, making structure learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a natural way to deal with linguistic ambiguities and label errors. Weakly Supervised Learning Our work is also inspired by weakly supervised approaches (Ganchev et al., 2010) that use structural constraints as a source of indirect supervision, and have been used for several NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2009; Singh et al., 2010). Carlson et al. (2010) in particular is similar since they use common sense constraints to jointly train multiple information extractors. In this work, however, we are training a matrix factorization model, and allowing for arbitrarily complex logic formulae. Combining Symbolic and Distributed Representations There have been a number of recent approaches that combine distributed representations with symbolic knowledge. Grefenstette (2013) describes an isomorphism between first-order logic and tensor calculus, using full-rank ma"
N15-1118,D14-1165,0,0.257379,"describes an isomorphism between first-order logic and tensor calculus, using full-rank matrices to exactly memorize facts. Based on this isomorphism, Rockt¨aschel et al. (2014) combine logic with matrix factorization for learning low-dimensional embeddings that approximately satisfy given formulae and generalize to unobserved facts on toy data. Our work extends this workshop paper by proposing a simpler formalism without tensor-based logical connectives, presenting results on a real-world task, and demonstrating the utility of this approach for learning relations with few textual alignments. Chang et al. (2014) use Freebase entity types as hard constraints in a tensor factorization objective for universal schema relation extraction. In contrast, our approach is imposing soft constraints that are formulated as universally quantified first-order formula. de Lacalle and Lapata (2013) combine first-order logic knowledge with a topic model to improve surface pattern clustering for relation extraction. Since these formulae only specify which relations can be clustered and which not, they do not capture the variety of dependencies embeddings can model, such as asymmetry. Lewis and Steedman (2013) use distr"
N15-1118,D13-1079,0,0.0122835,"problem all distantly-supervised techniques share: you can only reliably learn relations that appear frequently enough in the knowledge base. In particular, for relations that do not appear in the knowledge base or for which no facts are known we cannot learn a predictor at all. One way to overcome this problem is to incorporate additional domain knowledge, either specified manually or bootstrapped from auxiliary sources. In fact, domain knowledge encoded as simple logic formulae over patterns and relations has been used in practice to directly specify relation extractors (Reiss et al., 2008; Chiticariu et al., 2013; Akbik et al., 2014). However, these extractors can be brittle and obtain poor recall, since they are unable to generalize to textual patterns that are not 1119 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1119–1129, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics found in given formulae. Hence, there is a need for learning extractors that are able to combine logical knowledge with benefits of factorization techniques to facilitate precise extractions and generalization to novel relations. In"
N15-1118,W13-3809,0,0.0305674,"Missing"
N15-1118,P09-1041,0,0.0140558,"e learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a natural way to deal with linguistic ambiguities and label errors. Weakly Supervised Learning Our work is also inspired by weakly supervised approaches (Ganchev et al., 2010) that use structural constraints as a source of indirect supervision, and have been used for several NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2009; Singh et al., 2010). Carlson et al. (2010) in particular is similar since they use common sense constraints to jointly train multiple information extractors. In this work, however, we are training a matrix factorization model, and allowing for arbitrarily complex logic formulae. Combining Symbolic and Distributed Representations There have been a number of recent approaches that combine distributed representations with symbolic knowledge. Grefenstette (2013) describes an isomorphism between first-order logic and tensor calculus, using full-rank matrices to exactly memorize facts. Based on th"
N15-1118,P14-1079,0,0.0333626,"se schema, with OpenIE surface form patterns in a universal schema, and (b) completing a knowledge base of such a schema using matrix factorization. This approach has several attractive properties. First, for canonical relations it effectively performs distant supervision (Bunescu and Mooney, 2007; Mintz et al., 2009; Yao et al., 2011; Hoffmann et al., 2011; Surdeanu et al., 2012) and hence requires no textual annotations. Second, in the spirit of OpenIE, a universal schema can use textual patterns as novel relations and thus increases the coverage of traditional schemas (Riedel et al., 2013; Fan et al., 2014). Third, matrix factorization learns better embeddings for entity-pairs for which only surface form patterns are observed, and these can also lead to better extractions of canonical relations. Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, and leverage unlabeled data. Unfortunately, these methods share a shortcoming with all other distantly supervised approaches: they cannot learn to extract target relations without existing data in the knowledge base, and likewise, these models are inaccurate f"
N15-1118,W11-0112,0,0.0242402,"ideas presented in this paper can be incorporated with any embedding-based method that uses a per-atom loss. Logical Inference A common alternative that directly incorporates first-order logic knowledge is to perform logical inference (Bos and Markert, 2005; Baader et al., 2007; Bos, 2008), however such purely symbolic approaches cannot deal with the uncertainty inherent to natural language, and generalize poorly. 1126 Probabilistic Inference To ameliorate some of the drawbacks of symbolic logical inference, probabilistic logic based approaches have been proposed (Schoenmackers et al., 2008; Garrette et al., 2011; Beltagy et al., 2013; Beltagy et al., 2014). Since logical connections between relations are modeled explicitly, such approaches are generally hard to scale. Specifically, approaches based on Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) encode logical knowledge in dense, loopy graphical models, making structure learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a n"
N15-1118,S13-1001,0,0.0218077,"constraints as a source of indirect supervision, and have been used for several NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2009; Singh et al., 2010). Carlson et al. (2010) in particular is similar since they use common sense constraints to jointly train multiple information extractors. In this work, however, we are training a matrix factorization model, and allowing for arbitrarily complex logic formulae. Combining Symbolic and Distributed Representations There have been a number of recent approaches that combine distributed representations with symbolic knowledge. Grefenstette (2013) describes an isomorphism between first-order logic and tensor calculus, using full-rank matrices to exactly memorize facts. Based on this isomorphism, Rockt¨aschel et al. (2014) combine logic with matrix factorization for learning low-dimensional embeddings that approximately satisfy given formulae and generalize to unobserved facts on toy data. Our work extends this workshop paper by proposing a simpler formalism without tensor-based logical connectives, presenting results on a real-world task, and demonstrating the utility of this approach for learning relations with few textual alignments."
N15-1118,P13-1088,0,0.012526,"specify which relations can be clustered and which not, they do not capture the variety of dependencies embeddings can model, such as asymmetry. Lewis and Steedman (2013) use distributed representations to cluster predicates before logical inference. Again, this approach is not as powerful as factorizing the relations, as it makes symmetry assumptions for the predicates. Several studies have investigated the use of symbolic representations (such as dependency trees) to guide the composition of distributed representations (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Coecke et al., 2010; Hermann and Blunsom, 2013). Instead we are using symbolic representations (first-order logic) as prior domain knowledge to directly learn better embeddings. Combining symbolic information with neural networks has also been an active area of research. Towell and Shavlik (1994) introduce Knowledge-Based Artificial Neural Networks whose topology is isomorphic to a knowledge base of facts and inference formulae. There, facts are input units, intermediate conclusions hidden units, and final conclusions (inferred facts) output units. Unlike our work, there is no latent representation of predicates and constants. H¨olldobler"
N15-1118,P11-1055,0,0.0695942,"ecting Logical Background Knowledge into Embeddings for Relation Extraction Tim Rockt¨aschel University College London London, UK Sameer Singh University of Washington Seattle, WA Abstract (a) unifying traditional canonical relations, such as those of the Freebase schema, with OpenIE surface form patterns in a universal schema, and (b) completing a knowledge base of such a schema using matrix factorization. This approach has several attractive properties. First, for canonical relations it effectively performs distant supervision (Bunescu and Mooney, 2007; Mintz et al., 2009; Yao et al., 2011; Hoffmann et al., 2011; Surdeanu et al., 2012) and hence requires no textual annotations. Second, in the spirit of OpenIE, a universal schema can use textual patterns as novel relations and thus increases the coverage of traditional schemas (Riedel et al., 2013; Fan et al., 2014). Third, matrix factorization learns better embeddings for entity-pairs for which only surface form patterns are observed, and these can also lead to better extractions of canonical relations. Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, an"
N15-1118,Q13-1015,0,0.0189749,"ual alignments. Chang et al. (2014) use Freebase entity types as hard constraints in a tensor factorization objective for universal schema relation extraction. In contrast, our approach is imposing soft constraints that are formulated as universally quantified first-order formula. de Lacalle and Lapata (2013) combine first-order logic knowledge with a topic model to improve surface pattern clustering for relation extraction. Since these formulae only specify which relations can be clustered and which not, they do not capture the variety of dependencies embeddings can model, such as asymmetry. Lewis and Steedman (2013) use distributed representations to cluster predicates before logical inference. Again, this approach is not as powerful as factorizing the relations, as it makes symmetry assumptions for the predicates. Several studies have investigated the use of symbolic representations (such as dependency trees) to guide the composition of distributed representations (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Coecke et al., 2010; Hermann and Blunsom, 2013). Instead we are using symbolic representations (first-order logic) as prior domain knowledge to directly learn better embeddings. Combining sym"
N15-1118,P08-1099,0,0.0128111,"l models, making structure learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a natural way to deal with linguistic ambiguities and label errors. Weakly Supervised Learning Our work is also inspired by weakly supervised approaches (Ganchev et al., 2010) that use structural constraints as a source of indirect supervision, and have been used for several NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2009; Singh et al., 2010). Carlson et al. (2010) in particular is similar since they use common sense constraints to jointly train multiple information extractors. In this work, however, we are training a matrix factorization model, and allowing for arbitrarily complex logic formulae. Combining Symbolic and Distributed Representations There have been a number of recent approaches that combine distributed representations with symbolic knowledge. Grefenstette (2013) describes an isomorphism between first-order logic and tensor calculus, using full-rank matrices to exactly memoriz"
N15-1118,P09-1113,0,0.160003,"Missing"
N15-1118,P08-1028,0,0.0213123,"relation extraction. Since these formulae only specify which relations can be clustered and which not, they do not capture the variety of dependencies embeddings can model, such as asymmetry. Lewis and Steedman (2013) use distributed representations to cluster predicates before logical inference. Again, this approach is not as powerful as factorizing the relations, as it makes symmetry assumptions for the predicates. Several studies have investigated the use of symbolic representations (such as dependency trees) to guide the composition of distributed representations (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Coecke et al., 2010; Hermann and Blunsom, 2013). Instead we are using symbolic representations (first-order logic) as prior domain knowledge to directly learn better embeddings. Combining symbolic information with neural networks has also been an active area of research. Towell and Shavlik (1994) introduce Knowledge-Based Artificial Neural Networks whose topology is isomorphic to a knowledge base of facts and inference formulae. There, facts are input units, intermediate conclusions hidden units, and final conclusions (inferred facts) output units. Unlike our work, there is no latent represe"
N15-1118,N13-1008,1,0.542673,"s those of the Freebase schema, with OpenIE surface form patterns in a universal schema, and (b) completing a knowledge base of such a schema using matrix factorization. This approach has several attractive properties. First, for canonical relations it effectively performs distant supervision (Bunescu and Mooney, 2007; Mintz et al., 2009; Yao et al., 2011; Hoffmann et al., 2011; Surdeanu et al., 2012) and hence requires no textual annotations. Second, in the spirit of OpenIE, a universal schema can use textual patterns as novel relations and thus increases the coverage of traditional schemas (Riedel et al., 2013; Fan et al., 2014). Third, matrix factorization learns better embeddings for entity-pairs for which only surface form patterns are observed, and these can also lead to better extractions of canonical relations. Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, and leverage unlabeled data. Unfortunately, these methods share a shortcoming with all other distantly supervised approaches: they cannot learn to extract target relations without existing data in the knowledge base, and likewise, these mode"
N15-1118,D08-1009,0,0.0325687,"der logic knowledge, and the ideas presented in this paper can be incorporated with any embedding-based method that uses a per-atom loss. Logical Inference A common alternative that directly incorporates first-order logic knowledge is to perform logical inference (Bos and Markert, 2005; Baader et al., 2007; Bos, 2008), however such purely symbolic approaches cannot deal with the uncertainty inherent to natural language, and generalize poorly. 1126 Probabilistic Inference To ameliorate some of the drawbacks of symbolic logical inference, probabilistic logic based approaches have been proposed (Schoenmackers et al., 2008; Garrette et al., 2011; Beltagy et al., 2013; Beltagy et al., 2014). Since logical connections between relations are modeled explicitly, such approaches are generally hard to scale. Specifically, approaches based on Markov Logic Networks (MLNs) (Richardson and Domingos, 2006) encode logical knowledge in dense, loopy graphical models, making structure learning, parameter estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix fac"
N15-1118,D10-1106,0,0.0124061,"for any unobserved statement rm (ei , ej ) is done efficiently by calculating [rm (ei , ej )]. Note that this does not involve any explicit logical inference, instead we expect that the predictions from the learned embeddings already respect the provided formulae. 4 Experimental Setup There are two orthogonal question when evaluating the effectiveness of low-rank logic embeddings: a) does injection of logic formulae into the embeddings of entity-pairs and relations provide any benefits, and b) where do the background formulae come from? The latter is a well-studied problem (Hipp et al., 2000; Schoenmackers et al., 2010; V¨olker and 1123 Niepert, 2011). In this paper we focus the evaluation on the ability of various approaches to benefit from formulae that we directly extract from the training data using a simple method. Distant Supervision Evaluation We follow the procedure as used in Riedel et al. (2013) for evaluating knowledge base completion of Freebase (Bollacker et al., 2008) with textual data from the NYTimes corpus (Sandhaus, 2008). The training matrix consists of 4 111 columns, representing 151 Freebase relations and 3 960 textual patterns, 41 913 rows (entity-pairs) and 118 781 training facts of w"
N15-1118,N10-1009,1,0.793047,"r estimation, and inference hard for the scale of our data. In contrast, in our model the logical knowledge is captured directly in the embeddings, leading to efficient inference. Furthermore, as our model is based on matrix factorization, we have a natural way to deal with linguistic ambiguities and label errors. Weakly Supervised Learning Our work is also inspired by weakly supervised approaches (Ganchev et al., 2010) that use structural constraints as a source of indirect supervision, and have been used for several NLP tasks (Chang et al., 2007; Mann and McCallum, 2008; Druck et al., 2009; Singh et al., 2010). Carlson et al. (2010) in particular is similar since they use common sense constraints to jointly train multiple information extractors. In this work, however, we are training a matrix factorization model, and allowing for arbitrarily complex logic formulae. Combining Symbolic and Distributed Representations There have been a number of recent approaches that combine distributed representations with symbolic knowledge. Grefenstette (2013) describes an isomorphism between first-order logic and tensor calculus, using full-rank matrices to exactly memorize facts. Based on this isomorphism, Rockt"
N15-1118,D12-1042,0,0.0579155,"nd Knowledge into Embeddings for Relation Extraction Tim Rockt¨aschel University College London London, UK Sameer Singh University of Washington Seattle, WA Abstract (a) unifying traditional canonical relations, such as those of the Freebase schema, with OpenIE surface form patterns in a universal schema, and (b) completing a knowledge base of such a schema using matrix factorization. This approach has several attractive properties. First, for canonical relations it effectively performs distant supervision (Bunescu and Mooney, 2007; Mintz et al., 2009; Yao et al., 2011; Hoffmann et al., 2011; Surdeanu et al., 2012) and hence requires no textual annotations. Second, in the spirit of OpenIE, a universal schema can use textual patterns as novel relations and thus increases the coverage of traditional schemas (Riedel et al., 2013; Fan et al., 2014). Third, matrix factorization learns better embeddings for entity-pairs for which only surface form patterns are observed, and these can also lead to better extractions of canonical relations. Matrix factorization approaches to relation extraction provide several attractive features: they support distant supervision, handle open schemas, and leverage unlabeled dat"
N15-1118,D11-1135,1,0.860297,"Missing"
N15-1118,W14-2409,1,\N,Missing
N15-1118,D13-1040,0,\N,Missing
N15-3013,N10-1061,0,0.025357,"ccurate, requiring incremental model tweaking based on performance. Even if the model is accurate, the final performance depends quite critically on the choice of the algorithms and their hyper-parameters. Further, bugs that are introduced by the user may not even be reflected directly in the performance (such as a feature computation bug may not degrade performance). All these concerns are further compounded due to the variety of approaches commonly used in NLP, such as conditional random fields (Sutton and McCallum, 2007), Markov random networks (Poon and Domingos, 2007), Bayesian networks (Haghighi and Klein, 2010), matrix factorization (Riedel et al., 2013), and Deep learning (Socher et al., 2013). Probabilistic programming languages (PPLs), by closing the gap between traditional programming and probabilistic modeling, go a long way in aiding quick design and modification of expressive models1 . However, creating accurate machine learning models using these languages remains challenging. Of the probabilistic programming languages that exist today, no language can easily express the variety of models used in NLP, focusing instead on a restricted set of modeling paradigms, for example, Markov logic netwo"
N15-3013,N13-1008,1,0.912781,"ed on performance. Even if the model is accurate, the final performance depends quite critically on the choice of the algorithms and their hyper-parameters. Further, bugs that are introduced by the user may not even be reflected directly in the performance (such as a feature computation bug may not degrade performance). All these concerns are further compounded due to the variety of approaches commonly used in NLP, such as conditional random fields (Sutton and McCallum, 2007), Markov random networks (Poon and Domingos, 2007), Bayesian networks (Haghighi and Klein, 2010), matrix factorization (Riedel et al., 2013), and Deep learning (Socher et al., 2013). Probabilistic programming languages (PPLs), by closing the gap between traditional programming and probabilistic modeling, go a long way in aiding quick design and modification of expressive models1 . However, creating accurate machine learning models using these languages remains challenging. Of the probabilistic programming languages that exist today, no language can easily express the variety of models used in NLP, focusing instead on a restricted set of modeling paradigms, for example, Markov logic networks can be models by Alchemy (Richardson and"
N15-3013,D13-1170,0,0.00359697,"ccurate, the final performance depends quite critically on the choice of the algorithms and their hyper-parameters. Further, bugs that are introduced by the user may not even be reflected directly in the performance (such as a feature computation bug may not degrade performance). All these concerns are further compounded due to the variety of approaches commonly used in NLP, such as conditional random fields (Sutton and McCallum, 2007), Markov random networks (Poon and Domingos, 2007), Bayesian networks (Haghighi and Klein, 2010), matrix factorization (Riedel et al., 2013), and Deep learning (Socher et al., 2013). Probabilistic programming languages (PPLs), by closing the gap between traditional programming and probabilistic modeling, go a long way in aiding quick design and modification of expressive models1 . However, creating accurate machine learning models using these languages remains challenging. Of the probabilistic programming languages that exist today, no language can easily express the variety of models used in NLP, focusing instead on a restricted set of modeling paradigms, for example, Markov logic networks can be models by Alchemy (Richardson and Domingos, 2006), Bayesian generative net"
N16-3020,P13-1025,0,0.0818837,"Missing"
N16-3020,N16-3020,1,0.148074,"on methods, on multiple NLP classification applications, classifier algorithms, and trust-related tasks. 2 Explaining Predictions and Models By “explaining a prediction”, we mean presenting visual artifacts that provide qualitative understanding of the relationship between the instance’s components (e.g. words in text) and the model’s prediction. Explaining predictions is an important aspect in getting humans to trust and use machine learning effectively, provided the explanations are faithful and intelligible. We summarize the techniques here; further details and experiments are available in Ribeiro et al. (2016). Local Interpretable Model-Agnostic Explanations We present Local Interpretable Model-agnostic Explanations (LIME). The overall goal of LIME is to identify an interpretable model over the interpretable representation that is locally faithful to predictions of any classifier. It is important to distinguish between features and interpretable data representations, the latter is a representation that is understandable to humans, regardless of the actual features used by the model. A possible interpretable representation for text is a binary vector indicating the presence or absence of a word, eve"
N16-3020,D13-1170,0,0.0927066,"Missing"
N16-3020,P07-1056,0,\N,Missing
N19-1089,P16-1000,0,0.182494,"Missing"
N19-1089,D10-1049,0,0.0715998,"elated Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims & Oracle Sky News reported Thursday night that Kenneth Clarke , , had not yet decided whether to support Mr. Howard ’s candidacy , raising the possibility the party could face a divisive battle for leadership . + (position held: Chancellor of the Exchequer) + (position held: Secretary of State for the Home Department) a former chancellor of the exchequer the Home Secretary the Chancellor of the Exchequer “ A lot of people think it ’s something we just sta"
N19-1089,W05-0909,0,0.0288252,"entified using word overlap). 4.2 Experiments We experiment with two types of encoder/decoder modules: bidirectional LSTMs, and transform831 ers (Vaswani et al., 2017). We use a vocabulary of size 50K, truncate the maximum input sequence length to 500, and use a batch size of 32 in all experiments. To help models distinguish between claims and context we demarcate claim fields with special &lt;claim&gt;, &lt;key&gt;, and &lt;value&gt; tokens. We train all the models for 150k steps, and evaluate on the validation dataset every 10k steps. Evaluation is performed using the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) translation metrics, and Precision, Recall and F1 score of the predicted bag-of-words (omitting stopwords). The model with the highest F1 score on the validation set is used during test time. and sentence lengths to examine the impact of the such variables. Discussion of Quantitative Results Our results contain a few key findings. The first is that knowing the relevant claims is critical to obtaining stateof-the-art performance; even knowing only oracle claims is sufficient to perform better than all of the other baselines, although there is a still a large improvement when context is additio"
N19-1089,H05-1042,0,0.0268991,"and to use more general terms without elaborating too much about the entity. In contrast, longer 20 0 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20+ sentence lengths (b) BLEU scores Figure 6: Performances by input sentence lengths of BiLSTM model. Sentences with 20 or more tokens are put into one group, 20+. and more detailed human written post-modifiers are preferred when they are especially relevant to the rest of the sentence. 5 Related Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims & Oracle Sky News reported"
N19-1089,W03-1016,0,0.0121642,"iers over the ones written by professional journalists when they are shorter and to use more general terms without elaborating too much about the entity. In contrast, longer 20 0 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20+ sentence lengths (b) BLEU scores Figure 6: Performances by input sentence lengths of BiLSTM model. Sentences with 20 or more tokens are put into one group, 20+. and more detailed human written post-modifiers are preferred when they are especially relevant to the rest of the sentence. 5 Related Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Targe"
N19-1089,P16-1154,0,0.053028,"ration We move our focus to the main task of postmodifier generation. 4.1 Attention Methods At its core, post-modifier generation involves producing a variable-length sequence output conditioned on two variable-length inputs: the words in the current and previous sentence (e.g. the context), and the collection of claims about the entity. Accordingly, the sequence-to-sequence (seq2seq) framework (Sutskever et al., 2014) is a natural fit for the task — we use it as the foundation for all of our baseline models. Since research has shown that attention (Bahdanau et al., 2015) and copy mechanisms (Gu et al., 2016) consistently improve seq2seq model performance, we use these in our baselines as well. One choice that must be made when using this framework is how to combine the different inputs. The default approach we use is to concatenate the claim and context into a linear sequence of tokens during preprocessing (shown in Figure 4a). We also experiment with encoding the claims and each of the context sentences separately, then concatenating their vector representations before decoding. We refer to this as the tri-encoder approach (shown in Figure 4b). As discussed earlier, selecting relevant claims is"
N19-1089,C10-2062,0,0.100165,"a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims & Oracle Sky News reported Thursday night that Kenneth Clarke , , had not yet decided whether to support Mr. Howard ’s candidacy , raising the possibility the party could face a divisive battle for leadership . + (position held: Chancellor of the Exchequer) + (position held: Secretary of State for the Home Department) a former chancellor of the exchequer the Home Secretary the Chancellor of the Exchequer “ A lot of people think it ’s something we just started , but we actually"
N19-1089,P83-1022,0,0.302777,"ted post-modifiers over the ones written by professional journalists when they are shorter and to use more general terms without elaborating too much about the entity. In contrast, longer 20 0 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20+ sentence lengths (b) BLEU scores Figure 6: Performances by input sentence lengths of BiLSTM model. Sentences with 20 or more tokens are put into one group, 20+. and more detailed human written post-modifiers are preferred when they are especially relevant to the rest of the sentence. 5 Related Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 201"
N19-1089,D16-1128,0,0.0312256,"Missing"
N19-1089,P09-1011,0,0.015688,"of the sentence. 5 Related Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims & Oracle Sky News reported Thursday night that Kenneth Clarke , , had not yet decided whether to support Mr. Howard ’s candidacy , raising the possibility the party could face a divisive battle for leadership . + (position held: Chancellor of the Exchequer) + (position held: Secretary of State for the Home Department) a former chancellor of the exchequer the Home Secretary the Chancellor of the Exchequer “ A lot of people think it ’s"
N19-1089,D11-1149,0,0.021912,"us work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims & Oracle Sky News reported Thursday night that Kenneth Clarke , , had not yet decided whether to support Mr. Howard ’s candidacy , raising the possibility the party could face a divisive battle for leadership . + (position held: Chancellor of the Exchequer) + (position held: Secretary of State for the Home Department) a former chancellor of the exchequer the Home Secretary the Chancellor of the Exchequer “ A lot of people think it ’s something we just started , but we actually opened the seaso"
N19-1089,P14-5010,0,0.00397954,"es reliable ways to identify text to remove and sources of information that can be used to generate the text. Here we describe a pipeline for generating such a dataset for our task. 2.1 Dataset We construct the PoMo dataset using three different news corpora: NYTimes (Sandhaus, 2008), CNN and DailyMail (Hermann et al., 2015). We use Wikidata to collect facts about entities.2 2 1 CNN Wikidata dump from https://www.wikidata. org/wiki/Wikidata:Database_download (Dump date: 2018/06/25) https://stonybrooknlp.github.io/PoMo/ 827 2.1.1 Post-Modifier and Entity Identification We use Stanford CoreNLP (Manning et al., 2014) to parse each sentence in the news articles and to identify named entities. We extract post-modifiers by finding noun phrases that share an appos relation3 with any recognized named entity in the sentence. In this work, we only consider postmodifiers for people. In the future, we plan to expand PoMo to include more post-modifiers for other targets, such as organizations. We extract only one such pair from a given sentence to reduce the possible noise in the extraction process. In our running example from Figure 1, Noam Chomsky is recognized as a person entity. The word “professor” is an appos"
N19-1089,N16-1086,0,0.0160505,"ed relevant during dataset curation are prefaced with a +. In the first example, knowing the relevant claims helps the Oracle model produce an output that closely matches the Target, however lack of temporal information causes the model to miss the word former. In the second example, the All Claims and Oracle models produce the same post-modifier. Although it is similar to the Target in meaning, it receives a low score using our evaluation metrics. Futhermore, our data curation method fails to identify relevant claims. Modern approaches employ neural networks to solve this problem end-to-end. Mei et al. (2016) utilize an encoder-decoder framework to map weather conditions to a weather forecast. Ahn et al. (2016) and Yang et al. (2017) introduce a new class of language models which are capable of entity coreference and copying facts from an external knowledge base. Building upon these models, Wiseman et al. (2017) introduce an auxiliary reconstruction loss which use the hidden states of the decoder to recover the facts used to generate the text. Liu et al. (2018) introduce a hierarchical attention model for fact selection, with the higher level focusing on which records in the table to select and th"
N19-1089,P02-1040,0,0.10381,"ting (since relevant claims were identified using word overlap). 4.2 Experiments We experiment with two types of encoder/decoder modules: bidirectional LSTMs, and transform831 ers (Vaswani et al., 2017). We use a vocabulary of size 50K, truncate the maximum input sequence length to 500, and use a batch size of 32 in all experiments. To help models distinguish between claims and context we demarcate claim fields with special &lt;claim&gt;, &lt;key&gt;, and &lt;value&gt; tokens. We train all the models for 150k steps, and evaluate on the validation dataset every 10k steps. Evaluation is performed using the BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) translation metrics, and Precision, Recall and F1 score of the predicted bag-of-words (omitting stopwords). The model with the highest F1 score on the validation set is used during test time. and sentence lengths to examine the impact of the such variables. Discussion of Quantitative Results Our results contain a few key findings. The first is that knowing the relevant claims is critical to obtaining stateof-the-art performance; even knowing only oracle claims is sufficient to perform better than all of the other baselines, although there is a still a lar"
N19-1089,N18-1012,0,0.0260431,"Missing"
N19-1089,P98-2209,0,0.199591,"lists when they are shorter and to use more general terms without elaborating too much about the entity. In contrast, longer 20 0 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20+ sentence lengths (b) BLEU scores Figure 6: Performances by input sentence lengths of BiLSTM model. Sentences with 20 or more tokens are put into one group, 20+. and more detailed human written post-modifiers are preferred when they are especially relevant to the rest of the sentence. 5 Related Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims"
N19-1089,N07-1022,0,0.00933399,"with 20 or more tokens are put into one group, 20+. and more detailed human written post-modifiers are preferred when they are especially relevant to the rest of the sentence. 5 Related Work There is a large body of previous work on claim selection (Kukich, 1983; Duboue and McKeown, 2003; Reiter and Dale, 1997; Tanaka-Ishii et al., 1998; Barzilay and Lapata, 2005) and language generation from structured data (Reiter et al., 2005; Goldberg et al., 1994). Initially, hand-crafted grammars were employed for language generation, which later evolved to statistical machine translation style models (Wong and Mooney, 2007) or PCFG based models (Belz, 2008). More recently, the focus has shifted to learning both fact selection and language generation jointly (Liang et al., 2009; Angeli et al., 2010; Kim and Mooney, 2010; Lu and Ng, 2011; Konstas and Lapata, 2013). 833 Input Claims Target All Claims Oracle Input Claims Target All Claims & Oracle Sky News reported Thursday night that Kenneth Clarke , , had not yet decided whether to support Mr. Howard ’s candidacy , raising the possibility the party could face a divisive battle for leadership . + (position held: Chancellor of the Exchequer) + (position held: Secret"
N19-1089,D17-1197,0,0.0252433,"model produce an output that closely matches the Target, however lack of temporal information causes the model to miss the word former. In the second example, the All Claims and Oracle models produce the same post-modifier. Although it is similar to the Target in meaning, it receives a low score using our evaluation metrics. Futhermore, our data curation method fails to identify relevant claims. Modern approaches employ neural networks to solve this problem end-to-end. Mei et al. (2016) utilize an encoder-decoder framework to map weather conditions to a weather forecast. Ahn et al. (2016) and Yang et al. (2017) introduce a new class of language models which are capable of entity coreference and copying facts from an external knowledge base. Building upon these models, Wiseman et al. (2017) introduce an auxiliary reconstruction loss which use the hidden states of the decoder to recover the facts used to generate the text. Liu et al. (2018) introduce a hierarchical attention model for fact selection, with the higher level focusing on which records in the table to select and the lower level focusing on which cells in a particular row to pay attention to. In order to train complex neural models, the que"
N19-1246,D13-1160,0,0.349224,"nding. Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and Liang, 2015). If we had a structured, tabular representation of the content of our paragraphs, DROP would be largely the same as WikiTableQuestions, with similar (possibly even simpler) question semantics. Our novelty is that we are the first to combine these complex questions with paragraph understanding, with the aim of encouraging systems that can produce comprehensive structural analyses of paragraphs, either explicitly or implicitly."
N19-1246,N18-2017,0,0.0831929,"Missing"
N19-1246,P17-1044,0,0.0139727,"edicates and assigns predicate-specific argument roles.6 To adhere to KDG’s structured representation format, we convert each of these representations into a table, where rows are predicateargument structures and columns correspond to different argument roles. Logical form language Our logical form language identifies five basic elements in the table representation: predicate-argument structures (i.e., table rows), relations (column-headers), strings, num6 We used the AllenNLP implementations of state-of-theart models for all of these representations (Gardner et al., 2017; Dozat et al., 2017; He et al., 2017; Stanovsky et al., 2018). 2372 did the the w er e happ ened goa ls did did did eld tim s tions win intercep the e th the r te af pl ay er d di inhere w were did ys da s is asse p of es more there team there which h whic g lon ce en er as w re we e th di was di er en ce s int po s wn did do ch tou t percen s wa e th of e th sc or ed were people in from did the ed or sc e th s wa rst not en wh w ho the yards long er did aft er pa ss ed the it wa s the kick ed had the caught who threw rst the second happen ed the ed or sc the rst e or m the the be tw ee n event th e ed pen hap is did yea r th e"
N19-1246,D14-1058,0,0.0307579,"complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding. Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and L"
N19-1246,D15-1075,0,0.0729969,"-pretrained-bert 10 For the black-box BERT model, we convert DROP to SQuAD format by using the first match as the gold span. 2373 (6 v.s. 7) due to the GPU memory limit. We adopt the ELMo representations trained on 5.5B corpus for the QANet+ELMo baseline and the large uncased BERT model for the BERT baseline. The hyper-parameters for our NAQANet model (§6) are the same as for the QANet baseline. 5.3 Heuristic Baselines A recent line of work (Gururangan et al., 2018; Kaushik and Lipton, 2018) has identified that popular crowdsourced NLP datasets (such as SQuAD (Rajpurkar et al., 2016) or SNLI (Bowman et al., 2015)) are prone to have artifacts and annotation biases which can be exploited by supervised algorithms that learn to pick up these artifacts as signal instead of more meaningful semantic features. We estimate artifacts by training the QANet model described in Section 5.2 on a version of DROP where either the question or the paragraph input representation vectors are zeroed out (question-only and paragraph-only, respectively). Consequently, the resulting models can then only predict answer spans from either the question or the paragraph. In addition, we devise a baseline that estimates the answer"
N19-1246,P17-1147,0,0.11449,"Missing"
N19-1246,W05-0620,0,0.13903,"Missing"
N19-1246,D18-1546,0,0.0390583,"ion, while the score for BERT is based on an open-source implementation from Hugging Face: https://github.com/huggingface/pytorch-pretrained-bert 10 For the black-box BERT model, we convert DROP to SQuAD format by using the first match as the gold span. 2373 (6 v.s. 7) due to the GPU memory limit. We adopt the ELMo representations trained on 5.5B corpus for the QANet+ELMo baseline and the large uncased BERT model for the BERT baseline. The hyper-parameters for our NAQANet model (§6) are the same as for the QANet baseline. 5.3 Heuristic Baselines A recent line of work (Gururangan et al., 2018; Kaushik and Lipton, 2018) has identified that popular crowdsourced NLP datasets (such as SQuAD (Rajpurkar et al., 2016) or SNLI (Bowman et al., 2015)) are prone to have artifacts and annotation biases which can be exploited by supervised algorithms that learn to pick up these artifacts as signal instead of more meaningful semantic features. We estimate artifacts by training the QANet model described in Section 5.2 on a version of DROP where either the question or the paragraph input representation vectors are zeroed out (question-only and paragraph-only, respectively). Consequently, the resulting models can then only"
N19-1246,P16-1223,0,0.146227,"Missing"
N19-1246,P18-1078,1,0.903373,"Missing"
N19-1246,N19-1423,0,0.270861,"Missing"
N19-1246,K17-3002,0,0.0174839,"es for polysemous predicates and assigns predicate-specific argument roles.6 To adhere to KDG’s structured representation format, we convert each of these representations into a table, where rows are predicateargument structures and columns correspond to different argument roles. Logical form language Our logical form language identifies five basic elements in the table representation: predicate-argument structures (i.e., table rows), relations (column-headers), strings, num6 We used the AllenNLP implementations of state-of-theart models for all of these representations (Gardner et al., 2017; Dozat et al., 2017; He et al., 2017; Stanovsky et al., 2018). 2372 did the the w er e happ ened goa ls did did did eld tim s tions win intercep the e th the r te af pl ay er d di inhere w were did ys da s is asse p of es more there team there which h whic g lon ce en er as w re we e th di was di er en ce s int po s wn did do ch tou t percen s wa e th of e th sc or ed were people in from did the ed or sc e th s wa rst not en wh w ho the yards long er did aft er pa ss ed the it wa s the kick ed had the caught who threw rst the second happen ed the ed or sc the rst e or m the the be tw ee n event th e ed pen hap i"
N19-1246,N18-1023,0,0.0470213,"w in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we"
N19-1246,Q18-1023,0,0.0584383,"Missing"
N19-1246,D17-1160,1,0.897774,"ing Semantic parsing has been used to translate natural language utterances into formal executable languages (e.g., SQL) that can perform discrete operations against a structured knowledge representation, such as knowledge graphs or tabular databases (Zettlemoyer and Collins, 2005; Berant et al., 2013b; Yin and Neubig, 2017; Chen and Mooney, 2011, inter alia). Since many of DROP’s questions require similar discrete reasoning, it is appealing to port some of the successful work in semantic parsing to the DROP dataset. Specifically, we use the grammar-constrained semantic parsing model built by Krishnamurthy et al. (2017) (KDG) for the W IKI TABLE Q UESTIONS tabular dataset (Pasupat and Liang, 2015). Sentence representation schemes We experimented with three paradigms to represent paragraphs as structured contexts: (1) Stanford dependencies (de Marneffe and Manning, 2008, Syn Dep); which capture word-level syntactic relations, (2) Open Information Extraction (Banko et al., 2007, Open IE), a shallow semantic representation which directly links predicates and arguments; and (3) Semantic Role Labeling (Carreras and M`arquez, 2005, SRL), which disambiguates senses for polysemous predicates and assigns predicate-sp"
N19-1246,P14-1026,0,0.0365246,"e of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding. Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions"
N19-1246,P17-1003,0,0.0395215,"; Zellers et al., 2018; Zhang et al., 2019; Zellers et al., 2019). In our case, instead of using an adversarial baseline to filter automatically generated examples, we use it in a crowdsourcing task, to teach crowd workers to avoid easy questions, raising the difficulty level of the questions they provide. Neural symbolic reasoning DROP is designed to encourage research on methods that combine neural methods with discrete, symbolic reasoning. We present one such model in Section 6. Other related work along these lines has been done by Reed and de Freitas (2016), Neelakantan et al. (2016), and Liang et al. (2017). 3 DROP Data Collection In this section, we describe our annotation protocol, which consists of three phases. First, we automatically extract passages from Wikipedia which are expected to be amenable to complex questions. Second, we crowdsource question-answer pairs on these passages, eliciting questions which require 2370 discrete reasoning. Finally, we validate the development and test portions of DROP to ensure their quality and report inter-annotator agreement. Passage extraction We searched Wikipedia for passages that had a narrative sequence of events, particularly with a high proportio"
N19-1246,P17-1015,0,0.0311548,"s of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true; however, it is only simple reasoning that is relatively well-understood in the semantic parsing literature, and we use it as a necessary means to force more comprehensive passage understanding. Many existing algebra word problem datasets also contain similar phenomena to what is in DROP (Koncel-Kedziorski et al., 2015; Kushman et al., 2014; Hosseini et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and Liang, 2015). If we had a structured, tab"
N19-1246,W08-1301,0,0.0643822,"Missing"
N19-1246,D18-1260,0,0.0265468,"nswering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved,"
N19-1246,K18-1007,0,0.0469015,"Missing"
N19-1246,N18-1144,0,0.0225428,"uestion Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single"
N19-1246,L18-1564,0,0.0210105,"aset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired w"
N19-1246,D18-1258,0,0.0257774,"ng conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true;"
N19-1246,P16-1144,0,0.0594744,"Missing"
N19-1246,P15-1142,0,0.251793,"et al., 2014; Clark et al., 2016; Ling et al., 2017). Our dataset is different in that it uses much longer contexts, is more open domain, and requires deeper paragraph understanding. Semantic parsing The semantic parsing literature has a long history of trying to understand complex, compositional question semantics in terms of some grounded knowledge base or other environment (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Berant et al., 2013a, inter alia). It is this literature that we modeled our questions on, particularly looking at the questions in the WikiTableQuestions dataset (Pasupat and Liang, 2015). If we had a structured, tabular representation of the content of our paragraphs, DROP would be largely the same as WikiTableQuestions, with similar (possibly even simpler) question semantics. Our novelty is that we are the first to combine these complex questions with paragraph understanding, with the aim of encouraging systems that can produce comprehensive structural analyses of paragraphs, either explicitly or implicitly. Adversarial dataset construction We continue 1 Some questions in our dataset require limited sports domain knowledge to answer; we expect that there are enough such ques"
N19-1246,D14-1162,0,0.0816178,"Missing"
N19-1246,N18-1202,1,0.603282,"Missing"
N19-1246,P18-2124,0,0.0476756,"t can be found at https://allennlp.org/drop. 2 Related Work Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned"
N19-1246,Q19-1016,0,0.0743529,"Missing"
N19-1246,N18-1081,1,0.786086,"gns predicate-specific argument roles.6 To adhere to KDG’s structured representation format, we convert each of these representations into a table, where rows are predicateargument structures and columns correspond to different argument roles. Logical form language Our logical form language identifies five basic elements in the table representation: predicate-argument structures (i.e., table rows), relations (column-headers), strings, num6 We used the AllenNLP implementations of state-of-theart models for all of these representations (Gardner et al., 2017; Dozat et al., 2017; He et al., 2017; Stanovsky et al., 2018). 2372 did the the w er e happ ened goa ls did did did eld tim s tions win intercep the e th the r te af pl ay er d di inhere w were did ys da s is asse p of es more there team there which h whic g lon ce en er as w re we e th di was di er en ce s int po s wn did do ch tou t percen s wa e th of e th sc or ed were people in from did the ed or sc e th s wa rst not en wh w ho the yards long er did aft er pa ss ed the it wa s the kick ed had the caught who threw rst the second happen ed the ed or sc the rst e or m the the be tw ee n event th e ed pen hap is did yea r th e how many yea rs at wh wa"
N19-1246,N18-1140,0,0.0196887,"e (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1 One could argue that we are adding numerical reasoning as an “additional complexity”, and this is true; however, it is only simple r"
N19-1246,N18-1059,0,0.0558538,"st baseline system. The dataset, code for the baseline systems, and a leaderboard with a hidden test set can be found at https://allennlp.org/drop. 2 Related Work Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud"
N19-1246,Q18-1021,0,0.0624305,"these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these additional phenomena. However, we are concerned with paragraph understanding, which on its own is far from solved, so DROP has none of these additional complexities. It consists of single passages of text paired with independent questions, with only linguistic facility required to answer the questions.1"
N19-1246,D18-1259,0,0.0643269,"ncrease over the best baseline system. The dataset, code for the baseline systems, and a leaderboard with a hidden test set can be found at https://allennlp.org/drop. 2 Related Work Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefi"
N19-1246,P17-1041,0,0.0121247,"d overlap. When an answer has multiple spans, we first perform a one-to-one alignment greedily based on bag-of-word overlap on the set of spans and then compute average F1 over each span. When there are multiple annotated answers, both metrics take a max over all gold answers. 5.1 Semantic Parsing Semantic parsing has been used to translate natural language utterances into formal executable languages (e.g., SQL) that can perform discrete operations against a structured knowledge representation, such as knowledge graphs or tabular databases (Zettlemoyer and Collins, 2005; Berant et al., 2013b; Yin and Neubig, 2017; Chen and Mooney, 2011, inter alia). Since many of DROP’s questions require similar discrete reasoning, it is appealing to port some of the successful work in semantic parsing to the DROP dataset. Specifically, we use the grammar-constrained semantic parsing model built by Krishnamurthy et al. (2017) (KDG) for the W IKI TABLE Q UESTIONS tabular dataset (Pasupat and Liang, 2015). Sentence representation schemes We experimented with three paradigms to represent paragraphs as structured contexts: (1) Stanford dependencies (de Marneffe and Manning, 2008, Syn Dep); which capture word-level syntact"
N19-1246,D18-1009,0,0.0711059,"Missing"
N19-1246,P18-1156,0,0.0679837,"s, and a leaderboard with a hidden test set can be found at https://allennlp.org/drop. 2 Related Work Question answering datasets With systems reaching human performance on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), many follow-on tasks are currently being proposed. All of these datasets throw in additional complexities to the reading comprehension challenge, around tracking conversational state (Reddy et al., 2019; Choi et al., 2018), requiring passage retrieval (Joshi et al., 2017; Yang et al., 2018; Talmor and Berant, 2018), mismatched passages and questions (Saha et al., 2018; Kocisk´y et al., 2018; Rajpurkar et al., 2018), integrating knowledge from external sources (Mihaylov et al., 2018; Zhang et al., 2019), tracking entity state changes (Mishra et al., 2018; Ostermann et al., 2018) or a particular kind of “multi-step” reasoning over multiple documents (Welbl et al., 2018; Khashabi et al., 2018). Similar facets are explored in medical domain ˇ datasets (Pampari et al., 2018; Suster and Daelemans, 2018) which contain automatically generated queries on medical records based on predefined templates. We applaud these efforts, which offer good avenues to study these"
N19-1303,D11-1120,0,0.0502885,"nd Billings, 2000; Tyler Eastman, 2001; Kinnick, 1998; Fu et al., 2016) and in movie character portrayals (Ramakrishna et al., 2017; Sap et al., 2017). These approaches are domain-specific and rely on techniques like counting gender occurrences, manually annotating words or mentions, constructing list of keywords and lexicons, carrying out surveys, etc. Our approach works across domains, and does not require manual annotations. There has been significant amount of work in detecting author’s gender (Koppel et al., 2002; Herring and Paolillo, 2006; Sarawgi et al., 2011; Mukherjee and Liu, 2010; Burger et al., 2011) for text, speaker gender for dialogues (Schofield and Mehr, 2016) in films, and to detect and reduce biases in these (Tatman, 2017; Thelwall, 2018; Koolen and van Cranenburgh, 2017). While we do not focus on predicting the gender of the author, our framework can be used as a tool to compare the use of gendered language across various authors, or across various works by the same author. Gender Bias in NLP Pipelines There has also been recent interest in examining the role of gender bias in existing NLP pipelines. Caliskan et al. (2017) and Bolukbasi et al. (2016) show that word embeddings exhi"
N19-1303,P13-1035,0,0.38288,"Missing"
N19-1303,P11-1015,0,0.0453633,"t neural network models as described in Section 4.2. 4 Datasets and Classifiers In this section, we give details about the datasets, our pipeline for automated data labeling, filtering and processing applied to contexts in order to remove obvious gender information, and the classifiers used to classify gender of a given mention. 4.1 Datasets and data processing To illustrate the utility of our proposed approach, we analyze text from four different domains: • New York Times articles from the Annotated Gigaword corpus (Napoles et al., 2012) • Novels from Gutenberg corpus 2 • IMDB Movie Reviews (Maas et al., 2011) • Movie Summaries (Bamman et al., 2013) These domains cover a variety of writing styles. While the novels represent fictional writing, news articles are non-fictional. Movie summaries dataset describes the plot of the movies, i.e., how gender is represented in the plots, and the movie reviews dataset provides the ways in which people express their views on the plots, i.e., how gender is represented in user perception of the movie. We train classifiers for each domain to predict the gender of mentions from the context they appear in, and use the resulting classifiers to detect gendered languag"
N19-1303,W10-0719,0,0.0116297,"issing mention. We sample the sentences such that the true labels (male/female) are balanced. For our study, we use two tasks that slightly differ from one another in the decisions turkers need to make. In one task, turkers are given only two options, male and female, forcing them to make a choice. In the second task, turkers are given five options on the Likert scale: extremely likely male, likely male, neutral, likely female, extremely likely female allowing for a finer scale of decision. We include examples in the instructions, and a few extremely easy examples as probes to verify quality (Munro et al., 2010). Each worker is shown 35 sentences from a single domain. On average, we collect 7 human annotations per sentence. Do humans predict gender well? Sentences that do not have a clear majority are removed from our analysis. As a measure of inter-rater reliability, we compute pairwise and majority agreement, in Table 3. Percentage improvement over chance agreement is higher for 5-scale rating compared to 2-scale rating indicating that users tend to agree more when they are able to tag the borderline (possibly confusing) mentions as gender-neutral (chance agreement is 0.5 for 2-scale, and 0.2 for 2"
N19-1303,W12-3018,0,0.0394532,"Missing"
N19-1303,D18-1302,0,0.0242643,"tend the idea to contextualized word embeddings (Peters et al., 2018), and quantify and propose ways to mitigate gender bias in them , while Gonen and Goldberg (2019) show that current approaches for debiasing embeddings are superficial. Researchers have studied gender bias outside word embeddings as well. Zhao et al. (2017) show 2960 that datasets for multi-label object classification and visual semantic role labeling are gender-biased and that models trained on these datasets amplify this bias, while Rudinger et al. (2017) find racial, religious and gender stereotypes in the SNLI corpus and Park et al. (2018) analyze gender bias in abusive language datasets. Zhao et al. (2018a) and Rudinger et al. (2018) detect bias in existing coreference resolution systems, and Webster et al. (2018) build a gender-balanced labeled corpus of ambiguous pronoun-name pairs to understand this bias. All of these either focus on whether the corpus as a whole is gendered or if a single word is gendered (in case of word embeddings). Instead, we train a classifier to detect and quantify gendered language at mention-level. Our framework can also be used to quantify genderedness at different levels – mention, sentence, docu"
N19-1303,D14-1162,0,0.082377,"and choose logistic regression classifier since it consistently performs better than others. LSTMs and CNNs We use both uni- and bidirectional LSTM recurrent neural networks for the context. In the 2-way LSTM model, we use two separate LSTMs: one for context before the mention, and the other for context after the mention. The direction of LSTM for latter part is reversed so that the model gives more importance to words closer to the target mention. This is followed by a sigmoid layer after the concatenation of the final hidden states. The input layers are initialized using the Glove vectors (Pennington et al., 2014), and are updated during training. We train the classifier with log-loss and Adam (Kingma and Ba, 2014) optimization algorithm, including dropout (Srivastava et al., 2014) and early stopping for regularization. Hyper-parameters are tuned for different domains separately. We experiment with ELMo embeddings (Peters et al., 2018), convolutional neural network (CNN)-based architectures, vanilla recurrent neural network (RNN), and gated recurrent unit (GRU) (Cho et al., 2014) models as well. Performance Since our datasets are imbalanced, we use AUC-ROC as a performance metric. Table 2 shows AUC-ROC"
N19-1303,N18-1202,0,0.311501,"ss various authors, or across various works by the same author. Gender Bias in NLP Pipelines There has also been recent interest in examining the role of gender bias in existing NLP pipelines. Caliskan et al. (2017) and Bolukbasi et al. (2016) show that word embeddings exhibit gender stereotypes. Garg et al. (2018) build on this idea, using word embeddings to characterize the evolution of gender stereotypes during the 20th and 21st centuries. Subsequent works attempt to mitigate this bias in embeddings (Zhao et al., 2018b). Zhao et al. (2019) extend the idea to contextualized word embeddings (Peters et al., 2018), and quantify and propose ways to mitigate gender bias in them , while Gonen and Goldberg (2019) show that current approaches for debiasing embeddings are superficial. Researchers have studied gender bias outside word embeddings as well. Zhao et al. (2017) show 2960 that datasets for multi-label object classification and visual semantic role labeling are gender-biased and that models trained on these datasets amplify this bias, while Rudinger et al. (2017) find racial, religious and gender stereotypes in the SNLI corpus and Park et al. (2018) analyze gender bias in abusive language datasets."
N19-1303,P13-1162,0,0.0316528,"are disproportionately used to describe women while muttered, grinned are used to describe men. Studies on gender bias in student evaluations for instructors (Eidinger, 2017; MacNell et al., 2015; Boring et al., 2016; Centra and Gaubatz, 2000), and recommendation letters (Trix and Psenka, 2003; Schmader et al., 2007) also show similar disparities in terms of harshness of evaluations, length of letters, descriptive words, and use of standout adjectives. Bias in language has also been studied for textbooks (Otlowski, 2003; Gharbavi and Mousavi, 2012; Macaulay and Brice, 1997), Wikipedia edits (Recasens et al., 2013), political text (Yano et al., 2010), media content (Ali et al., 2010; Len-R´ıos et al., 2005; Smith, 1997), sports journalism (Eastman and Billings, 2000; Tyler Eastman, 2001; Kinnick, 1998; Fu et al., 2016) and in movie character portrayals (Ramakrishna et al., 2017; Sap et al., 2017). These approaches are domain-specific and rely on techniques like counting gender occurrences, manually annotating words or mentions, constructing list of keywords and lexicons, carrying out surveys, etc. Our approach works across domains, and does not require manual annotations. There has been significant amou"
N19-1303,W17-1609,0,0.0703616,"Missing"
N19-1303,N18-2002,0,0.0993695,"Missing"
N19-1303,D10-1021,0,0.0445429,"rts journalism (Eastman and Billings, 2000; Tyler Eastman, 2001; Kinnick, 1998; Fu et al., 2016) and in movie character portrayals (Ramakrishna et al., 2017; Sap et al., 2017). These approaches are domain-specific and rely on techniques like counting gender occurrences, manually annotating words or mentions, constructing list of keywords and lexicons, carrying out surveys, etc. Our approach works across domains, and does not require manual annotations. There has been significant amount of work in detecting author’s gender (Koppel et al., 2002; Herring and Paolillo, 2006; Sarawgi et al., 2011; Mukherjee and Liu, 2010; Burger et al., 2011) for text, speaker gender for dialogues (Schofield and Mehr, 2016) in films, and to detect and reduce biases in these (Tatman, 2017; Thelwall, 2018; Koolen and van Cranenburgh, 2017). While we do not focus on predicting the gender of the author, our framework can be used as a tool to compare the use of gendered language across various authors, or across various works by the same author. Gender Bias in NLP Pipelines There has also been recent interest in examining the role of gender bias in existing NLP pipelines. Caliskan et al. (2017) and Bolukbasi et al. (2016) show tha"
N19-1303,D17-1247,0,0.0239192,"003; Schmader et al., 2007) also show similar disparities in terms of harshness of evaluations, length of letters, descriptive words, and use of standout adjectives. Bias in language has also been studied for textbooks (Otlowski, 2003; Gharbavi and Mousavi, 2012; Macaulay and Brice, 1997), Wikipedia edits (Recasens et al., 2013), political text (Yano et al., 2010), media content (Ali et al., 2010; Len-R´ıos et al., 2005; Smith, 1997), sports journalism (Eastman and Billings, 2000; Tyler Eastman, 2001; Kinnick, 1998; Fu et al., 2016) and in movie character portrayals (Ramakrishna et al., 2017; Sap et al., 2017). These approaches are domain-specific and rely on techniques like counting gender occurrences, manually annotating words or mentions, constructing list of keywords and lexicons, carrying out surveys, etc. Our approach works across domains, and does not require manual annotations. There has been significant amount of work in detecting author’s gender (Koppel et al., 2002; Herring and Paolillo, 2006; Sarawgi et al., 2011; Mukherjee and Liu, 2010; Burger et al., 2011) for text, speaker gender for dialogues (Schofield and Mehr, 2016) in films, and to detect and reduce biases in these (Tatman, 201"
N19-1303,W11-0310,0,0.018787,"005; Smith, 1997), sports journalism (Eastman and Billings, 2000; Tyler Eastman, 2001; Kinnick, 1998; Fu et al., 2016) and in movie character portrayals (Ramakrishna et al., 2017; Sap et al., 2017). These approaches are domain-specific and rely on techniques like counting gender occurrences, manually annotating words or mentions, constructing list of keywords and lexicons, carrying out surveys, etc. Our approach works across domains, and does not require manual annotations. There has been significant amount of work in detecting author’s gender (Koppel et al., 2002; Herring and Paolillo, 2006; Sarawgi et al., 2011; Mukherjee and Liu, 2010; Burger et al., 2011) for text, speaker gender for dialogues (Schofield and Mehr, 2016) in films, and to detect and reduce biases in these (Tatman, 2017; Thelwall, 2018; Koolen and van Cranenburgh, 2017). While we do not focus on predicting the gender of the author, our framework can be used as a tool to compare the use of gendered language across various authors, or across various works by the same author. Gender Bias in NLP Pipelines There has also been recent interest in examining the role of gender bias in existing NLP pipelines. Caliskan et al. (2017) and Bolukba"
N19-1303,W16-0204,0,0.0216347,"al., 2016) and in movie character portrayals (Ramakrishna et al., 2017; Sap et al., 2017). These approaches are domain-specific and rely on techniques like counting gender occurrences, manually annotating words or mentions, constructing list of keywords and lexicons, carrying out surveys, etc. Our approach works across domains, and does not require manual annotations. There has been significant amount of work in detecting author’s gender (Koppel et al., 2002; Herring and Paolillo, 2006; Sarawgi et al., 2011; Mukherjee and Liu, 2010; Burger et al., 2011) for text, speaker gender for dialogues (Schofield and Mehr, 2016) in films, and to detect and reduce biases in these (Tatman, 2017; Thelwall, 2018; Koolen and van Cranenburgh, 2017). While we do not focus on predicting the gender of the author, our framework can be used as a tool to compare the use of gendered language across various authors, or across various works by the same author. Gender Bias in NLP Pipelines There has also been recent interest in examining the role of gender bias in existing NLP pipelines. Caliskan et al. (2017) and Bolukbasi et al. (2016) show that word embeddings exhibit gender stereotypes. Garg et al. (2018) build on this idea, usi"
N19-1303,D17-1323,0,0.180315,"age can be extraordinarily gendered (Moulton et al., 1978). Genderedness in language is when we use words or phrases that are stereotypical or indicative of a particular gender (we only consider male vs female in this work) (Prior, 2017). It is important to detect this bias in language since not only is this bias propagated to the readers (Menegatti and Rubini, 2017), but also machine learning algorithms trained on gendered corpora tend to become Sameer Singh University of California Irvine, CA sameer@uci.edu biased (Zhao et al., 2018a; Rudinger et al., 2018), often aggravating the disparity (Zhao et al., 2017). Bias in language and machine learning systems can lead to unfair treatment, e.g., early work by Moulton et al. (1978) shows that males have an advantage in contexts where they are referred to by a putative neutral term. Recent work on coreference resolution systems (Zhao et al., 2018a) shows that bias in machine learning systems originates from training on existing corpora, resulting in malestereotyped professions like surgeon and president incorrectly resolved to males instead of females. Such biases in machine learning systems can lead to unintentional biases in downstream tasks producing"
N19-1303,N18-2003,0,0.277621,"detected gendered sentences from aforementioned domains. 1 Introduction Language can be extraordinarily gendered (Moulton et al., 1978). Genderedness in language is when we use words or phrases that are stereotypical or indicative of a particular gender (we only consider male vs female in this work) (Prior, 2017). It is important to detect this bias in language since not only is this bias propagated to the readers (Menegatti and Rubini, 2017), but also machine learning algorithms trained on gendered corpora tend to become Sameer Singh University of California Irvine, CA sameer@uci.edu biased (Zhao et al., 2018a; Rudinger et al., 2018), often aggravating the disparity (Zhao et al., 2017). Bias in language and machine learning systems can lead to unfair treatment, e.g., early work by Moulton et al. (1978) shows that males have an advantage in contexts where they are referred to by a putative neutral term. Recent work on coreference resolution systems (Zhao et al., 2018a) shows that bias in machine learning systems originates from training on existing corpora, resulting in malestereotyped professions like surgeon and president incorrectly resolved to males instead of females. Such biases in machine le"
N19-1303,D18-1521,0,0.14713,"detected gendered sentences from aforementioned domains. 1 Introduction Language can be extraordinarily gendered (Moulton et al., 1978). Genderedness in language is when we use words or phrases that are stereotypical or indicative of a particular gender (we only consider male vs female in this work) (Prior, 2017). It is important to detect this bias in language since not only is this bias propagated to the readers (Menegatti and Rubini, 2017), but also machine learning algorithms trained on gendered corpora tend to become Sameer Singh University of California Irvine, CA sameer@uci.edu biased (Zhao et al., 2018a; Rudinger et al., 2018), often aggravating the disparity (Zhao et al., 2017). Bias in language and machine learning systems can lead to unfair treatment, e.g., early work by Moulton et al. (1978) shows that males have an advantage in contexts where they are referred to by a putative neutral term. Recent work on coreference resolution systems (Zhao et al., 2018a) shows that bias in machine learning systems originates from training on existing corpora, resulting in malestereotyped professions like surgeon and president incorrectly resolved to males instead of females. Such biases in machine le"
N19-1337,N18-1133,0,0.0209666,"ished method on KGs, we derive a novel approach that differs from their procedure in two ways: (1) instead of changes in the loss, we consider the changes in the scoring function, which is more appropriate for KG representations, and (2) in addition to searching for an attack, we introduce a gradient-based method that is much faster, especially for “adding an attack triple” (the size of search space make the influence function method infeasible). Previous work has also considered adversaries for KGs, but as part of training to improve their representation of the graph [Minervini et al., 2017, Cai and Wang, 2018]. Adversarial Attack on KG Although this is the first work on adversarial attacks for link prediction, there are two approaches [Dai et al., 2018, Zügner et al., 2018] that consider the task of adversarial attack on graphs. There are a few fundamental differences from our work: (1) they build their method on top of a path-based representations while we focus on embeddings, (2) they consider node classification as the target of their attacks while we attack link prediction, and (3) they conduct the attack on small graphs due to restricted scalability, while the complexity of our method does no"
N19-1337,N18-2053,0,0.0152777,"eir facts (links), a number of recent techniques have proposed models that embed each entity and relation into a vector space, and use these embeddings to predict facts. These dense representation models for link prediction include Sameer Singh University of California Irvine, CA sameer@uci.edu tensor factorization [Nickel et al., 2011, Socher et al., 2013, Yang et al., 2015], algebraic operations [Bordes et al., 2011, 2013b, Dasgupta et al., 2018], multiple embeddings [Wang et al., 2014, Lin et al., 2015, Ji et al., 2015, Zhang et al., 2018], and complex neural models [Dettmers et al., 2018, Nguyen et al., 2018]. However, there are only a few studies [Kadlec et al., 2017, Sharma et al., 2018] that investigate the quality of the different KG models. There is a need to go beyond just the accuracy on link prediction, and instead focus on whether these representations are robust and stable, and what facts they make use of for their predictions. In this paper, our goal is to design approaches that minimally change the graph structure such that the prediction of a target fact changes the most after the embeddings are relearned, which we collectively call Completion Robustness and Interpretability via Adve"
N19-1337,D18-1225,0,0.0194502,"ny real-world applications such as search, structured data management, recommendations, and question answering. Since KGs often suffer from incompleteness and noise in their facts (links), a number of recent techniques have proposed models that embed each entity and relation into a vector space, and use these embeddings to predict facts. These dense representation models for link prediction include Sameer Singh University of California Irvine, CA sameer@uci.edu tensor factorization [Nickel et al., 2011, Socher et al., 2013, Yang et al., 2015], algebraic operations [Bordes et al., 2011, 2013b, Dasgupta et al., 2018], multiple embeddings [Wang et al., 2014, Lin et al., 2015, Ji et al., 2015, Zhang et al., 2018], and complex neural models [Dettmers et al., 2018, Nguyen et al., 2018]. However, there are only a few studies [Kadlec et al., 2017, Sharma et al., 2018] that investigate the quality of the different KG models. There is a need to go beyond just the accuracy on link prediction, and instead focus on whether these representations are robust and stable, and what facts they make use of for their predictions. In this paper, our goal is to design approaches that minimally change the graph structure such"
N19-1337,P15-1067,0,0.0367502,"ions, and question answering. Since KGs often suffer from incompleteness and noise in their facts (links), a number of recent techniques have proposed models that embed each entity and relation into a vector space, and use these embeddings to predict facts. These dense representation models for link prediction include Sameer Singh University of California Irvine, CA sameer@uci.edu tensor factorization [Nickel et al., 2011, Socher et al., 2013, Yang et al., 2015], algebraic operations [Bordes et al., 2011, 2013b, Dasgupta et al., 2018], multiple embeddings [Wang et al., 2014, Lin et al., 2015, Ji et al., 2015, Zhang et al., 2018], and complex neural models [Dettmers et al., 2018, Nguyen et al., 2018]. However, there are only a few studies [Kadlec et al., 2017, Sharma et al., 2018] that investigate the quality of the different KG models. There is a need to go beyond just the accuracy on link prediction, and instead focus on whether these representations are robust and stable, and what facts they make use of for their predictions. In this paper, our goal is to design approaches that minimally change the graph structure such that the prediction of a target fact changes the most after the embeddings a"
N19-1337,N16-1054,0,0.0173989,"the best of our knowledge, this is the first work on conducting adversarial modifications on the link prediction task. Knowledge graph embedding There is a rich literature on representing knowledge graphs in vector spaces that differ in their scoring functions [Wang et al., 2017, Goyal and Ferrara, 2018, Fooshee et al., 2018]. Although CRIAGE is primarily applicable to multiplicative scoring functions [Nickel et al., 2011, Socher et al., 2013, Yang et al., 2015, Trouillon et al., 2016], these ideas apply to additive scoring functions [Bordes et al., 2013a, Wang et al., 2014, Lin et al., 2015, Nguyen et al., 2016] as well, as we show in Appendix A.3. Furthermore, there is a growing body of literature that incorporates an extra types of evidence for more informed embeddings such as numerical values [Garcia-Duran and Niepert, 2017], images [Oñoro-Rubio et al., 2017], text [Toutanova et al., 2015, 2016, Tu et al., 2017], and their combinations [Pezeshkpour et al., 2018]. Using CRIAGE, we can gain a deeper understanding of these methods, especially those that build their embeddings wit hmultiplicative scoring functions. Interpretability and Adversarial Modification There has been a significant recent inte"
N19-1337,D18-1359,1,0.901873,"s on multiplicative models of link prediction1 , specifically DistMult [Yang et al., 2015] because of its simplicity and popularity, and ConvE [Dettmers et al., 2018] because of its high accuracy. We can represent the scoring function of such methods as ψ(s, r, o) = f (es , er ) · eo , where es , er , eo ∈ Rd are embeddings of the subject, relation, and object respectively. In DistMult, f (es , er ) = es er , where is element-wise multiplication operator. Similarly, in ConvE, f (es , er ) is computed by a convolution on the concatenation of es and er . We use the same setup as Dettmers et al. [2018] for training, i.e., incorporate binary cross-entropy loss over the triple scores. In particular, for subjectrelation pairs (s, r) in the training data G, we use binary yos,r to represent negative and positive facts. Using the model’s probability of truth as σ(ψ(s, r, o)) for hs, r, oi, the loss is defined as: XX L(G) = yos,r log(σ(ψ(s, r, o))) We evaluate our proposed methods through following experiments. First, on relatively small KGs, we show that our approximations are accurate compared to the true change in the score. Second, we show that our additive attacks can effectively reduce the p"
N19-1337,P18-1012,0,0.0300314,"Missing"
N19-1337,D15-1174,0,0.0343603,"yal and Ferrara, 2018, Fooshee et al., 2018]. Although CRIAGE is primarily applicable to multiplicative scoring functions [Nickel et al., 2011, Socher et al., 2013, Yang et al., 2015, Trouillon et al., 2016], these ideas apply to additive scoring functions [Bordes et al., 2013a, Wang et al., 2014, Lin et al., 2015, Nguyen et al., 2016] as well, as we show in Appendix A.3. Furthermore, there is a growing body of literature that incorporates an extra types of evidence for more informed embeddings such as numerical values [Garcia-Duran and Niepert, 2017], images [Oñoro-Rubio et al., 2017], text [Toutanova et al., 2015, 2016, Tu et al., 2017], and their combinations [Pezeshkpour et al., 2018]. Using CRIAGE, we can gain a deeper understanding of these methods, especially those that build their embeddings wit hmultiplicative scoring functions. Interpretability and Adversarial Modification There has been a significant recent interest in conducting an adversarial attacks on different machine 3343 learning models [Biggio et al., 2014, Papernot et al., 2016, Dong et al., 2017, Zhao et al., 2018a,b, Brunet et al., 2018] to attain the interpretability, and further, evaluate the robustness of those models. Koh and L"
N19-1337,P16-1136,0,0.0606863,"Missing"
N19-1337,P17-1158,0,0.0307227,"et al., 2018]. Although CRIAGE is primarily applicable to multiplicative scoring functions [Nickel et al., 2011, Socher et al., 2013, Yang et al., 2015, Trouillon et al., 2016], these ideas apply to additive scoring functions [Bordes et al., 2013a, Wang et al., 2014, Lin et al., 2015, Nguyen et al., 2016] as well, as we show in Appendix A.3. Furthermore, there is a growing body of literature that incorporates an extra types of evidence for more informed embeddings such as numerical values [Garcia-Duran and Niepert, 2017], images [Oñoro-Rubio et al., 2017], text [Toutanova et al., 2015, 2016, Tu et al., 2017], and their combinations [Pezeshkpour et al., 2018]. Using CRIAGE, we can gain a deeper understanding of these methods, especially those that build their embeddings wit hmultiplicative scoring functions. Interpretability and Adversarial Modification There has been a significant recent interest in conducting an adversarial attacks on different machine 3343 learning models [Biggio et al., 2014, Papernot et al., 2016, Dong et al., 2017, Zhao et al., 2018a,b, Brunet et al., 2018] to attain the interpretability, and further, evaluate the robustness of those models. Koh and Liang [2017] uses influe"
N19-1337,D18-1358,0,0.0172679,"n answering. Since KGs often suffer from incompleteness and noise in their facts (links), a number of recent techniques have proposed models that embed each entity and relation into a vector space, and use these embeddings to predict facts. These dense representation models for link prediction include Sameer Singh University of California Irvine, CA sameer@uci.edu tensor factorization [Nickel et al., 2011, Socher et al., 2013, Yang et al., 2015], algebraic operations [Bordes et al., 2011, 2013b, Dasgupta et al., 2018], multiple embeddings [Wang et al., 2014, Lin et al., 2015, Ji et al., 2015, Zhang et al., 2018], and complex neural models [Dettmers et al., 2018, Nguyen et al., 2018]. However, there are only a few studies [Kadlec et al., 2017, Sharma et al., 2018] that investigate the quality of the different KG models. There is a need to go beyond just the accuracy on link prediction, and instead focus on whether these representations are robust and stable, and what facts they make use of for their predictions. In this paper, our goal is to design approaches that minimally change the graph structure such that the prediction of a target fact changes the most after the embeddings are relearned, which"
N19-1337,D18-1316,0,0.0625858,"Missing"
N19-5001,P18-1044,0,0.0297444,"2018) Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the future of adversarial learning for solvi"
N19-5001,C18-1055,0,0.0668385,"th case study of Generative Adversarial Networks for NLP, with a focus on dialogue generation (Li et al., 2017). This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Good"
N19-5001,P18-2006,0,0.13551,"th case study of Generative Adversarial Networks for NLP, with a focus on dialogue generation (Li et al., 2017). This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Good"
N19-5001,N19-1337,1,0.835986,"ks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Goodfellow et al., 2014). We will discuss why it is challenging to deploy GANs for NLP problems, comparing to vision problems. We then focus on introducing SeqGAN (Yu et al., 2017), an early solution of textual models of GAN, with a focus on policy gradient and Monte Carlo Tree Search. Finally, we provide an in-depth case study of deploying two-agent GAN models for conversational AI (Li et al., 2017). We will summarize the lessons lea"
N19-5001,D17-1215,0,0.0493758,"for NLP, with a focus on dialogue generation (Li et al., 2017). This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Goodfellow et al., 2014). We will discuss why it is"
N19-5001,D18-1026,0,0.0654804,"Missing"
N19-5001,P18-1225,0,0.0266302,"rning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP p"
N19-5001,P18-1046,1,0.924824,"Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP. In particular, we start with the gentle introduction to the fundamentals of adver"
N19-5001,P17-1119,0,0.0214364,"s of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learni"
N19-5001,P18-1079,1,0.824017,"adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Goodfellow et al., 2014). We will discuss why it is challenging to deploy GANs for NLP problems, comparing to vision problems. We then focus on introducing SeqGAN (Yu et al., 2017), an early solution of textual models of GAN, with a focus on policy gradient and Monte Carlo"
N19-5001,D17-1187,0,0.141197,"018a,b; Shi et al., 2018b; Chen et al., 2018; Farag et al., 2018; Ribeiro et al., 2018; Zhao et al., 2018) Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pro"
N19-5001,D18-1077,0,0.122559,"al focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP. In particular, w"
N19-5001,D18-1428,0,0.138691,"ial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data clean"
N19-5001,D18-1131,0,0.153172,"oduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP"
N19-5001,N18-1122,0,0.0638405,"Missing"
N19-5001,D18-1125,0,0.0412213,"Missing"
N19-5001,N18-1089,0,0.0232576,", 2018; Ribeiro et al., 2018; Zhao et al., 2018) Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the"
N19-5001,C18-1315,0,0.127664,"ation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP. In particular, we start with the gentle introduction to the fundamentals of adversarial learning. We further Tutorial Description Adversar"
N19-5001,C18-1103,0,0.030798,"et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tuto"
N19-5001,D18-1009,0,0.140633,"negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP problems is the model design issue. This tutorial draws connections from theories of deep adversarial learning to practical applications in NLP. In particular, we start with the gentle"
N19-5001,D18-1031,0,0.154184,"aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information re"
N19-5001,D18-1316,0,0.0180701,"ation (Li et al., 2017). This tutorial aims at introducing deep adversarial learning methods to researchers in the NLP community. We do not assume any particular prior knowledge in adversarial learning. The intended length of the tutorial is 3.5 hours, including a coffee break. 2 2018) or word-based (e.g. Alzantot et al., 2018), and the tasks they have been applied to. We will also provide an in-depth analysis of some of the general techniques for creating adversarial examples, such as gradient-based (e.g. Ebrahimi et al., 2018b), manually-designed (e.g. Jia and Liang, 2017), or learned (e.g. Zhao et al., 2018) perturbation techniques. Next, we will focus on practical applications of adversarial examples, such as existing work on adversarial rules for interpretable NLP (Ribeiro et al., 2018). To conclude this part, we discuss future directions and novel application areas for adversarial examples in NLP, including KB completion (Pezeshkpour et al., 2019). • An In-depth Case Study of GANs in NLP. Third, we switch from the focuses of adversarial training and adversarial examples to generative adversarial networks (Goodfellow et al., 2014). We will discuss why it is challenging to deploy GANs for NLP pr"
N19-5001,C18-1099,0,0.133892,"aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information re"
N19-5001,C18-1037,0,0.0290058,"on natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information retrieval (Li and Cheng, 2018). Adversarial learning methods could easily combine any representation learning based neural networks, and optimize for complex problems in NLP. However, a key challenge for applying deep adversarial learning techniques to real-world sized NLP"
N19-5001,P18-1083,1,0.909559,"aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al., 2018b), data cleaning (Elazar and Goldberg, 2018; Shah et al., 2018; Ryu et al., 2018; Zellers et al., 2018), information extraction (Qin et al., 2018; Hong et al., 2018; Wang et al., 2018b; Shi et al., 2018a; Bekoulis et al., 2018), and information re"
N19-5001,D18-1451,0,0.0867524,"ces in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to provide some practical perspectives on the future of adversarial learning for solving real-world NLP problems. 1 Jiwei Li Shannon.ai jiwei li@shannonai.com • Adversarial Generation, which primarily includes practical solutions of GANs for processing and generation natural language. (Yu et al., 2017; Li et al., 2017; Yang et al., 2018; Wang and Lee, 2018; Xu et al., 2018) Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018), adversarial evaluation (Elliott, 2018), and reward learning (Wang et al., 2018c). In particular, we will also provide a gentle introduction to the applications of adversarial learning in different NLP problems, including social media (Wang et al., 2018a; Carton et al., 2018), domain adaptation (Kim et al., 2017; Alam et al., 2018; Zou et al., 2018; Chen and Cardie, 2018; Tran and Nguyen, 2018; Cao et al., 2018; Li et al.,"
N19-5001,N18-2091,0,0.0348511,"., 2018b; Chen et al., 2018; Farag et al., 2018; Ribeiro et al., 2018; Zhao et al., 2018) Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. It is a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. • Adversarial Training, which focuses on adding noise, randomness, or adversarial loss during optimization. (Wu et al., 2017; Wang and Bansal, 2018; Li et al., 2018a; Yasunaga et al., 2018; Ponti et al., 2018; Kurita et al., 2018; Kang et al., 2018; Li et al., 2018c; Masumura et al., 2018) In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming to p"
P11-1080,P98-1012,0,0.8945,"reference grows superexponentially with the number of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years. Such a corpus would have billions of mentions. In this paper we propose a model and inference algorithms that can scale the cross-document coreference problem to corpora of that size. Much of the previous work in cross-document coreference (Bagga and Baldwin, 1998; Ravin and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pairwise mention similarity or distance function based on mention text, context, and document-level statistics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6. Other previous work attempts to address some of the above concerns by mapping coreference to inference on an undirected graphical"
P11-1080,D08-1029,0,0.392855,"clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) integrate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clustering. Niu et al. (2004) incorporate information extraction into the context similarity model, and annotate a small dataset to learn the parameters. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and the clustering algorithm used. Daum´e III and Marcu (2005) propose a generative approach to supervised clustering, and Haghighi and Klein (2010) use entity profiles to assist within-document coreference. Since many related methods use clustering, there are a number of distributed clustering algorithms that may help scale these approaches. Datta et al. (2006) propose an algorithm for distributed kmeans. Chen et al. (2010) describe a parallel spectral clustering algorithm. We use the Subsqu"
P11-1080,D08-1031,0,0.029693,"Missing"
P11-1080,W02-1001,0,0.0443813,"chosen mention l from its current entity es to a randomly chosen entity et . For such a proposal, the log-model ratio is: X X p(e0 ) log = ψa (l, m) + ψr (l, n) p(e) m∈et n∈es X X − ψa (l, n) − ψr (l, m) (2) n∈es m∈et Note that since only the factors between mention l and mentions in es and et are involved in this computation, the acceptance probability of each proposal is calculated efficiently. In general, the model may contain arbitrarily complex set of features over pairs of mentions, with parameters associated with them. Given labeled data, these parameters can be learned by Perceptron (Collins, 2002), which uses the MAP configuration according to the model (ˆ e). There also exist more efficient training algorithms such as SampleRank (McCallum et al., 2009; Wick et al., 2009b) that update parameters during inference. However, we only focus on inference in this work, and the only parameter that we set manually is the bias b, which indirectly influences the number of entities in ˆ e. Unless specified otherwise, in this work the initial configuration for MCMC is the singleton configuration, i.e. all entities have a size of 1. This MCMC inference technique, which has been used in McCallum and"
P11-1080,N07-1011,1,0.856675,"and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pairwise mention similarity or distance function based on mention text, context, and document-level statistics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6. Other previous work attempts to address some of the above concerns by mapping coreference to inference on an undirected graphical model (Culotta et al., 2007; Poon et al., 2008; Wellner et al., 2004; Wick et al., 2009a). These models contain pairwise factors between all pairs of mentions capturing similarity between them. Many of these models also enforce transitivity and enable features over 793 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 793–803, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics ... The Physiological Basis of Politics,” by Kevin B. Smith, Douglas Oxley, Matthew Hibbing... ...during the late 60's and early 70's, Kevin Smith worked with several l"
P11-1080,P07-1094,0,0.00627756,"p is accepted with the following Metropolis-Hastings acceptance probability: !   0 ) 1/t q(e) p(e α(e, e0 ) = min 1, (1) p(e) q(e0 ) 1 Number of possible entities is Bell(n) in the number of mentions, i.e. number of partitions of n items where t is the annealing temperature parameter. MCMC chains efficiently explore the highdensity regions of the probability distribution. By slowly reducing the temperature, we can decrease the entropy of the distribution to encourage convergence to the MAP configuration. MCMC has been used for optimization in a number of related work (McCallum et al., 2009; Goldwater and Griffiths, 2007; Changhe et al., 2004). The proposal function moves a randomly chosen mention l from its current entity es to a randomly chosen entity et . For such a proposal, the log-model ratio is: X X p(e0 ) log = ψa (l, m) + ψr (l, n) p(e) m∈et n∈es X X − ψa (l, n) − ψr (l, m) (2) n∈es m∈et Note that since only the factors between mention l and mentions in es and et are involved in this computation, the acceptance probability of each proposal is calculated efficiently. In general, the model may contain arbitrarily complex set of features over pairs of mentions, with parameters associated with them. Give"
P11-1080,N04-1002,0,0.713737,"ber of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years. Such a corpus would have billions of mentions. In this paper we propose a model and inference algorithms that can scale the cross-document coreference problem to corpora of that size. Much of the previous work in cross-document coreference (Bagga and Baldwin, 1998; Ravin and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pairwise mention similarity or distance function based on mention text, context, and document-level statistics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6. Other previous work attempts to address some of the above concerns by mapping coreference to inference on an undirected graphical model (Culotta et al., 2007; Poon et al., 2"
P11-1080,P07-1107,0,0.135709,"Missing"
P11-1080,D09-1120,0,0.0130614,"arge dataset, demonstrating the scalability of our approach. 1 Introduction Given a collection of mentions of entities extracted from a body of text, coreference or entity resolution consists of clustering the mentions such that two mentions belong to the same cluster if and only if they refer to the same entity. Solutions to this problem are important in semantic analysis and knowledge discovery tasks (Blume, 2005; Mayfield et al., 2009). While significant progress has been made in within-document coreference (Ng, 2005; Culotta et al., 2007; Haghighi and Klein, 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010), the larger problem of cross-document coreference has not received as much attention. Unlike inference in other language processing tasks that scales linearly in the size of the corpus, the hypothesis space for coreference grows superexponentially with the number of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years."
P11-1080,N10-1061,0,0.223887,"g the scalability of our approach. 1 Introduction Given a collection of mentions of entities extracted from a body of text, coreference or entity resolution consists of clustering the mentions such that two mentions belong to the same cluster if and only if they refer to the same entity. Solutions to this problem are important in semantic analysis and knowledge discovery tasks (Blume, 2005; Mayfield et al., 2009). While significant progress has been made in within-document coreference (Ng, 2005; Culotta et al., 2007; Haghighi and Klein, 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010), the larger problem of cross-document coreference has not received as much attention. Unlike inference in other language processing tasks that scales linearly in the size of the corpus, the hypothesis space for coreference grows superexponentially with the number of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years. Such a corpus would have bi"
P11-1080,P08-1099,1,0.562065,"esent in real-world applications. By utilizing parallelism across machines, our method can run on very large datasets simply by increasing the number of machines used. Second, approaches that use clustering are limited to using pairwise distance functions for which additional supervision and features are difficult to incorporate. In addition to representing features from all of the related work, graphical models can also use more complex entity-wide features (Culotta et al., 2007; Wick et al., 2009a), and parameters can be learned using supervised (Collins, 2002) or semisupervised techniques (Mann and McCallum, 2008). Finally, the inference for most of the related approaches is greedy, and earlier decisions are not revisited. Our technique is based on MCMC inference and simulated annealing, which are able to escape local maxima. 801 7 Conclusions Motivated by the problem of solving the coreference problem on billions of mentions from all of the newswire documents from the past few decades, we make the following contributions. First, we introduce distributed version of MCMC-based inference technique that can utilize parallelism to enable scalability. Second, we augment the model with hierarchical variables"
P11-1080,W03-0405,0,0.174727,"ent coreference (Bagga and Baldwin, 1998) uses an idf-based cosine-distance scoring function for pairs of contexts, similar to the one we use. Ravin and Kazi (1999) extend this work to be somewhat scalable by comparing pairs of contexts only if the mentions are deemed “ambiguous” using a heuristic. Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) integrate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clustering. Niu et al. (2004) incorporate information extraction into the context similarity model, and annotate a small dataset to learn the parameters. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and the clustering algorithm used. Daum´e III and Marcu (2005) propose a generative approach to supervis"
P11-1080,D10-1004,0,0.0166425,"Missing"
P11-1080,P05-1020,0,0.0171755,"Missing"
P11-1080,P04-1076,0,0.246817,"ion for pairs of contexts, similar to the one we use. Ravin and Kazi (1999) extend this work to be somewhat scalable by comparing pairs of contexts only if the mentions are deemed “ambiguous” using a heuristic. Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) integrate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clustering. Niu et al. (2004) incorporate information extraction into the context similarity model, and annotate a small dataset to learn the parameters. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and the clustering algorithm used. Daum´e III and Marcu (2005) propose a generative approach to supervised clustering, and Haghighi and Klein (2010) use entity profiles to assist within-"
P11-1080,W04-2406,0,0.00791484,"between pairs of contexts, which are then used for clustering. One of the first approaches to cross-document coreference (Bagga and Baldwin, 1998) uses an idf-based cosine-distance scoring function for pairs of contexts, similar to the one we use. Ravin and Kazi (1999) extend this work to be somewhat scalable by comparing pairs of contexts only if the mentions are deemed “ambiguous” using a heuristic. Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) integrate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clustering. Niu et al. (2004) incorporate information extraction into the context similarity model, and annotate a small dataset to learn the parameters. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and"
P11-1080,C10-2121,0,0.735477,"ng the entities. mawat, 2004) to manage the distributed computation. This approach to distribution is equivalent to inference with all mentions and entities on a single machine with a restricted proposer, but is faster since it exploits independencies to propose multiple jumps simultaneously. By restricting the jumps as described above, the acceptance probability calculation is exact. Partitioning the entities and proposing local jumps are restrictions to the single-machine proposal distribution; redistribution stages ensure the equivalent Markov chains are still irreducible. See Singh et al. (2010) for more details. 4 Hierarchical Coreference Model The proposal function for MCMC-based MAP inference presents changes to the current entities. Since we use MCMC to reach high-scoring regions of the hypothesis space, we are interested in the changes that improve the current configuration. But as the number of mentions and entities increases, these fruitful samples become extremely rare due to the blowup in the possible space of configurations, resulting in rejection of a large number of proposals. By distributing as described in the previous section, we propose samples in parallel, improving"
P11-1080,W99-0202,0,0.773167,"nentially with the number of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years. Such a corpus would have billions of mentions. In this paper we propose a model and inference algorithms that can scale the cross-document coreference problem to corpora of that size. Much of the previous work in cross-document coreference (Bagga and Baldwin, 1998; Ravin and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pairwise mention similarity or distance function based on mention text, context, and document-level statistics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6. Other previous work attempts to address some of the above concerns by mapping coreference to inference on an undirected graphical model (Culotta et al."
P11-1080,D10-1001,0,0.0176713,"Missing"
P11-1080,C98-1012,0,\N,Missing
P12-1040,W99-0201,0,0.0416223,"earch, question answering, and knowledge base construction. For example, coreference is vital for determining author publication lists in bibliographic knowledge bases such as CiteSeer and Google Scholar, where the repository must know if the “R. Hamming” who authored “Error detecting and error correcting codes” is the same” “R. Over the years, various machine learning techniques have been applied to different variations of the coreference problem. A commonality in many of these approaches is that they model the problem of entity coreference as a collection of decisions between mention pairs (Bagga and Baldwin, 1999; Soon et al., 2001; McCallum and Wellner, 2004; Singla and Domingos, 2005; Bengston and Roth, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level"
P12-1040,D08-1031,0,0.0397098,"etermining author publication lists in bibliographic knowledge bases such as CiteSeer and Google Scholar, where the repository must know if the “R. Hamming” who authored “Error detecting and error correcting codes” is the same” “R. Over the years, various machine learning techniques have been applied to different variations of the coreference problem. A commonality in many of these approaches is that they model the problem of entity coreference as a collection of decisions between mention pairs (Bagga and Baldwin, 1999; Soon et al., 2001; McCallum and Wellner, 2004; Singla and Domingos, 2005; Bengston and Roth, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005"
P12-1040,W06-3606,1,0.606287,"s are the factors of the graphical model that encode the pairwise compatibility functions. constraint that the setting to the coreference variables obey transitivity;2 this is the maximum probability estimate (MPE) setting. However, the solution to this problem is intractable, and even approximate inference methods such as loopy belief propagation can be difficult due to the cubic number of deterministic transitivity constraints. 2.2 Approximate Inference An approximate inference framework that has successfully been used for coreference models is Metropolis-Hastings (MH) (Milch et al. (2006), Culotta and McCallum (2006), Poon and Domingos (2007), amongst others), a Markov chain Monte Carlo algorithm traditionally used for marginal inference, but which can also be tuned for MPE inference. MH is a flexible framework for specifying customized local-search transition functions and provides a principled way of deciding which local search moves to accept. A proposal function q takes the current coreference hypothesis and proposes a new hypothesis by modifying a subset of the decision variables. The proposed change is accepted with probability α:   P r(y0 ) q(y|y0 ) α = min 1, (2) P r(y) q(y0 |y) 2 We say that a"
P12-1040,N07-1011,1,0.864923,"That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Li"
P12-1040,P07-1107,0,0.047354,"ndicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Name:,Jamie,Callan, Ins(tu(ons:-CMU,LTI., Topics:{WWW,,IR,,SIGIR}, Name:Jamie,Callan, Ins(tu(ons:, Topics:-IR, Jamie,Callan, Jamie,Callan, Topics:-IR, Topics:-IR, Coref?Name:"
P12-1040,N10-1061,0,0.126678,"B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Name:,Jamie,Callan, Ins(tu(ons:-CMU,LTI., Topics:{WWW,,IR,,SIGIR}, Name:Jamie,Callan, Ins(tu(ons:, Topics:-IR, Jamie,Callan, Jamie,Callan, Topics:-IR, Topics:-IR, Coref?Name:,James,Callan, Ins(tu(ons:-"
P12-1040,P05-1020,0,0.0278145,"h, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Associatio"
P12-1040,D09-1101,0,0.0236021,"ng a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Name:,Jamie,Callan, Ins(tu(ons"
P12-1040,C10-2121,0,0.501252,"Missing"
P12-1040,W99-0202,0,0.0690766,"oiding quadratic blow-up. Corresponding decision variables (open circles) indicate whether one node is the child of another. Mentions (gray boxes) are leaves. Deciding whether to merge these two entities requires evaluating just a single factor (red square), corresponding to the new child-parent relationship. number of mentions in each entity cluster is also large. Current systems cope with this by either dividing the data into blocks to reduce the search space (Hern´andez and Stolfo, 1995; McCallum et al., 2000; Bilenko et al., 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al., 2010), employing specialized Markov chain Monte Carlo procedures (Milch et al., 2006; Richardson and Domingos, 2006; Singh et al., 2010), or introducing shallow hierarchies of sub-entities for MCMC block moves and superentities for adaptive distributed inference (Singh et al., 2011). However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an entity, resulting in prohibitive computational costs associated with large datasets. This scaling with"
P12-1040,P11-1080,1,0.744781,"elationship. number of mentions in each entity cluster is also large. Current systems cope with this by either dividing the data into blocks to reduce the search space (Hern´andez and Stolfo, 1995; McCallum et al., 2000; Bilenko et al., 2006), using fixed heuristics to greedily compress the mentions (Ravin and Kazi, 1999; Rao et al., 2010), employing specialized Markov chain Monte Carlo procedures (Milch et al., 2006; Richardson and Domingos, 2006; Singh et al., 2010), or introducing shallow hierarchies of sub-entities for MCMC block moves and superentities for adaptive distributed inference (Singh et al., 2011). However, while these methods help manage the search space for medium-scale data, evaluating each coreference decision in many of these systems still scales linearly with the number of mentions in an entity, resulting in prohibitive computational costs associated with large datasets. This scaling with the number of mentions per entity seems particularly wasteful because although it is common for an entity to be referenced by a large number of mentions, many of these coreferent mentions are highly similar to each other. For example, in author coreference the two most common strings that refer"
P12-1040,J01-4004,0,0.36667,", and knowledge base construction. For example, coreference is vital for determining author publication lists in bibliographic knowledge bases such as CiteSeer and Google Scholar, where the repository must know if the “R. Hamming” who authored “Error detecting and error correcting codes” is the same” “R. Over the years, various machine learning techniques have been applied to different variations of the coreference problem. A commonality in many of these approaches is that they model the problem of entity coreference as a collection of decisions between mention pairs (Bagga and Baldwin, 1999; Soon et al., 2001; McCallum and Wellner, 2004; Singla and Domingos, 2005; Bengston and Roth, 2008). That is, coreference is solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow sy"
P12-1040,P08-1096,0,0.0246889,"s solved by answering a quadratic number of questions of the form “does mention A refer to the same entity as mention B?” with a compatibility function that indicates how likely A and B are coreferent. While these models have been successful in some domains, they also exhibit several undesirable characteristics. The first is that pairwise models lack the expressivity required to represent aggregate properties of the entities. Recent work has shown that these entity-level properties allow systems to correct coreference errors made from myopic pairwise decisions (Ng, 2005; Culotta et al., 2007; Yang et al., 2008; Rahman and Ng, 2009; Wick et al., 2009), and can even provide a strong signal for unsupervised coreference (Bhattacharya and Getoor, 2006; Haghighi and Klein, 2007; Haghighi and Klein, 2010). A second problem, that has received significantly less attention in the literature, is that the pairwise coreference models scale poorly to large collections of mentions especially when the expected 379 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 379–388, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Name:,Jam"
P12-1040,D08-1067,0,\N,Missing
P16-1030,D09-1020,0,0.0243763,"Missing"
P16-1030,baccianella-etal-2010-sentiwordnet,0,0.0164475,"Missing"
P16-1030,P98-1013,0,0.592476,"re 1 shows an example of a connotation frame for the predicate violate. We define four different typed relations: P(x → y) for perspective of x towards y, E(x) for effect on x, V(x) for value of x, and S(x) for mental state of x. These relationships can all be either positive (+), neutral (=), or negative (-). Our work is the first study to investigate frames as a representation formalism for connotative meanings. This contrasts with previous computational studies and resource development for frame semantics, where the primary focus was almost exclusively on denotational meanings of language (Baker et al., 1998; Palmer et al., 2005). Our formalism draws inspirations from the earlier work of frame semantics, however, in that we investigate the connection between a word and the related world knowledge associated with the word (Fillmore, 1976), which is essential for the readers to interpret many layers of the implied sentiment and presupposed value judgments. We also build upon the extensive amount of literature in sentiment analysis (Pang and Lee, 2008; Liu and Zhang, 2012), especially the recent emerging efforts on implied sentiment analysis (Feng et al., 2013; Greene and Resnik, 2009), entityentity"
P16-1030,bethard-etal-2008-building,0,0.0580396,"Missing"
P16-1030,W10-0203,0,0.0581481,"Missing"
P16-1030,P09-1068,0,0.104949,"Missing"
P16-1030,N09-1057,0,0.0222102,"anings of language (Baker et al., 1998; Palmer et al., 2005). Our formalism draws inspirations from the earlier work of frame semantics, however, in that we investigate the connection between a word and the related world knowledge associated with the word (Fillmore, 1976), which is essential for the readers to interpret many layers of the implied sentiment and presupposed value judgments. We also build upon the extensive amount of literature in sentiment analysis (Pang and Lee, 2008; Liu and Zhang, 2012), especially the recent emerging efforts on implied sentiment analysis (Feng et al., 2013; Greene and Resnik, 2009), entityentity sentiment inference (Wiebe and Deng, 2014), assuming it is an entity that can have a mental state. opinion role induction (Wiegand and Ruppenhofer, 2015) and effect analysis (Choi and Wiebe, 2014). However, our work is the first to organize various aspects of the connotative information into coherent frames. More concretely, our contributions are threefold: (1) a new formalism, model, and annotated dataset for studying connotation frames from large-scale natural language data and statistics, (2) new datadriven insights into the dynamics among different typed relations within eac"
P16-1030,W13-3514,0,0.0607865,"Missing"
P16-1030,D14-1125,0,0.0729271,"Missing"
P16-1030,E14-1040,0,0.160222,"Missing"
P16-1030,D07-1115,0,0.0457368,"Missing"
P16-1030,P13-1174,1,0.862919,"on denotational meanings of language (Baker et al., 1998; Palmer et al., 2005). Our formalism draws inspirations from the earlier work of frame semantics, however, in that we investigate the connection between a word and the related world knowledge associated with the word (Fillmore, 1976), which is essential for the readers to interpret many layers of the implied sentiment and presupposed value judgments. We also build upon the extensive amount of literature in sentiment analysis (Pang and Lee, 2008; Liu and Zhang, 2012), especially the recent emerging efforts on implied sentiment analysis (Feng et al., 2013; Greene and Resnik, 2009), entityentity sentiment inference (Wiebe and Deng, 2014), assuming it is an entity that can have a mental state. opinion role induction (Wiegand and Ruppenhofer, 2015) and effect analysis (Choi and Wiebe, 2014). However, our work is the first to organize various aspects of the connotative information into coherent frames. More concretely, our contributions are threefold: (1) a new formalism, model, and annotated dataset for studying connotation frames from large-scale natural language data and statistics, (2) new datadriven insights into the dynamics among different"
P16-1030,E14-1006,0,0.0186263,"Missing"
P16-1030,S13-1035,0,0.0387423,"Missing"
P16-1030,P14-2050,0,0.0304518,"Missing"
P16-1030,J05-1004,0,0.258364,"Missing"
P16-1030,K15-1022,0,0.0128664,"tigate the connection between a word and the related world knowledge associated with the word (Fillmore, 1976), which is essential for the readers to interpret many layers of the implied sentiment and presupposed value judgments. We also build upon the extensive amount of literature in sentiment analysis (Pang and Lee, 2008; Liu and Zhang, 2012), especially the recent emerging efforts on implied sentiment analysis (Feng et al., 2013; Greene and Resnik, 2009), entityentity sentiment inference (Wiebe and Deng, 2014), assuming it is an entity that can have a mental state. opinion role induction (Wiegand and Ruppenhofer, 2015) and effect analysis (Choi and Wiebe, 2014). However, our work is the first to organize various aspects of the connotative information into coherent frames. More concretely, our contributions are threefold: (1) a new formalism, model, and annotated dataset for studying connotation frames from large-scale natural language data and statistics, (2) new datadriven insights into the dynamics among different typed relations within each frame, and (3) an analytic study showing the potential use of connotation frames for analyzing subtle biases in journalism. The rest of the paper is organized as foll"
P16-1030,P79-1000,0,0.72901,"Missing"
P16-1030,W10-0723,0,0.0385888,"ssed by the event. = Value: not clear if agent is valuable E(agent) S(agent) State: the agent feels indifferent = E(agent) Effect: the agent is not really affected by the violation + V(theme) Value: the theme must be valuable - E(theme) S(agent) State: the theme will be unhappy - E(theme) Effect: the theme has been hurt Figure 1: An example connotation frame of “violate” as a set of typed relations: perspective P(x → y), effect E(x), value V(x), and mental state S(x). Introduction People commonly express their opinions through subtle and nuanced language (Thomas et al., 2006; Somasundaran and Wiebe, 2010). Often, through seemingly objective statements, the writer can influence the readers’ judgments toward an event and their participants. Even by choosing a particular predicate, the writer can indicate rich connotative information about the entities that interact through the predicate. More specifically, through a simple statement such as “x violated y”, the writer can convey: (1) writer’s perspective: the writer is projecting x as an “antagonist” and y as a “victim”, eliciting negative perspective from readers toward x (i.e., blaming x) and positive perspective toward y (i.e., sympathetic or"
P16-1030,E06-1027,0,\N,Missing
P16-1030,kamps-etal-2004-using,0,\N,Missing
P16-1030,N10-1119,0,\N,Missing
P16-1030,W10-0214,0,\N,Missing
P16-1030,C98-1013,0,\N,Missing
P16-1030,P13-1162,0,\N,Missing
P16-1030,W10-0204,0,\N,Missing
P16-1030,W12-3018,0,\N,Missing
P16-1030,P05-1017,0,\N,Missing
P18-1079,N18-1170,0,0.0887446,"t is on in the background? What is on? in VBP→is Where are is the water bottles Table Vending Maching Where are is the people gathered living room kitchen VERB on What is on the background? → VERB What are the planes parked on? A building Mountains Lights The television A building Mountains Concrete landing strip Table 7: SEARs for VQA that are rejected by users Other paraphrase limitations: Paraphrase models based on neural machine translation are biased towards maintaining the sentence structure, and thus do not produce certain adversaries (e.g. Table 5b), which recent work on paraphrasing (Iyyer et al., 2018) or generation using GANs (Zhao et al., 2018) may address. More critically, existing models are inaccurate for long texts, restricting SEAs and SEARs to sentences. Better bug fixing: Our data augmentation has the human users accept/reject rules based on whether or not they preserve semantics. Developing more effective ways of leveraging the expert’s time to close the loop, and facilitating more interactive collaboration between humans and SEARs are exciting areas for future work. 8 Conclusion We introduced SEAs and SEARs – adversarial examples and rules that preserve semantics, while causing m"
P18-1079,D17-1215,0,0.279128,"ex models are prone to brittleness: different ways of phrasing the same sentence can often cause the model to output different predictions. While held-out accuracy is often useful, it is not sufficient: practitioners consistently overestimate their model’s generalization (Patel et al., 2008) since test data is usually gathered in the same manner as training and validation. When deployed, these seemingly accurate models encounter sentences that are written very differently than the ones in the training data, thus making them prone to mistakes, and fragile with respect to distracting additions (Jia and Liang, 2017). These problems are exacerbated by the variability in language, and by cost and noise in annotations, making such bugs challenging to detect and fix. A particularly challenging issue is oversensitivity (Jia and Liang, 2017): a class of bugs where models output different predictions for very similar inputs. These bugs are prevalent in image classifi856 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 856–865 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Transformation Rules (WP is→WP’s) (?→"
P18-1079,P17-4012,0,0.0689725,"Missing"
P18-1079,P05-1015,0,0.333388,"use it for the remaining experiments. Source code and pretrained language models are available at https: //github.com/marcotcr/sears. For VQA, we use the multiple choice telling system and dataset of Zhu et al. (2016), using their implementation, with default parameters. The training data consists of questions that begin with “What”, “Where”, “When”, “Who”, “Why”, and “How”. The task is multiple choice, with four possible answers per instance. For sentiment analysis, we train a fastText (Joulin et al., 2016) model with unigrams and bigrams (embedding size of 50) on RottenTomato movie reviews (Pang and Lee, 2005), and evaluate it on IMDB sentence-sized reviews (Kotzias et al., 2015), simulating the common case where a model trained on a public dataset is applied to new data from a similar domain. 5.2 Can humans find good adversaries? In this experiment, we compare our method for generating SEAs with user’s ability to discover semantic-preserving adversaries. We take a random sample of 100 correctly-predicted instances for each task. In the first condition (human), we display each instance to 3 Amazon Mechanical Turk workers, and give them 10 attempts at creating semantically equivalent adversaries (wi"
P18-1079,N16-3020,1,0.378211,"ated examples. We take the rules that are accepted by ≥ 20 subjects as accepted bugs, a total of 4 rules (in Table 2) for VQA, and 16 rules for sentiment (including ones in Table 3). We then augment the training data by applying these rules to it, and retrain the models. To check if the bugs are still present, we create a sensitivity dataset by applying these SEARs to instances predicted correctly on the validation. A model not prone to the bugs described by these 6 Related Work Previous work on debugging primarily focuses on explaining predictions in validation data in order to uncover bugs (Ribeiro et al., 2016, 2018; Kulesza et al., 2011), or find labeling errors (Zhang et al., 2018; Koh and Liang, 2017). Our work is complementary to these techniques, as they provide no mechanism to detect oversensitivity bugs. We are able to uncover these bugs even when they are not present in the data, since we generate sentences. Adversarial examples for image recognition are typically indistinguishable to the human eye (Szegedy et al., 2014). These are more of a security concern than bugs per se, as images with adversarial noise are not “natural”, and not expected to occur in the real world outside of targeted"
P18-1079,tiedemann-2012-parallel,0,0.0196153,"Missing"
P18-1079,P17-1190,0,0.0147601,"x, and getting predictions from f until the original prediction is changed. Given an indicator function SemEq(x, x0 ) that is 1 if x is semantically equivalent to x0 and 0 otherwise, we define a semantically equivalent adversary (SEA) as a semantically equivalent instance that changes the model prediction in Eq (1). Such adversaries are important in evaluating the robustness of f , as each is an undesirable bug.   0 0 0 SEA(x, x ) = 1 SemEq(x, x )∧f (x) 6= f (x ) (1) While there are various ways of scoring semantic similarity between pairs of texts based on embeddings (Le and Mikolov, 2014; Wieting and Gimpel, 2017), they do not explicitly penalize unnatural sentences, and generating sentences requires surrounding context (Le and Mikolov, 2014) or training a separate model. We turn instead to paraphrasing based on neural machine translation (Lapata et al., 2017), where P (x0 |x) (the probability of a paraphrase x0 given original sentence x) is proportional to translating x into multiple pivot languages 857 and then taking the score of back-translating the translations into the original language. This approach scores semantics and “plausibility” simultaneously (as translation models have “built in” langua"
P18-1079,E17-1083,0,0.016895,"ivalent instance that changes the model prediction in Eq (1). Such adversaries are important in evaluating the robustness of f , as each is an undesirable bug.   0 0 0 SEA(x, x ) = 1 SemEq(x, x )∧f (x) 6= f (x ) (1) While there are various ways of scoring semantic similarity between pairs of texts based on embeddings (Le and Mikolov, 2014; Wieting and Gimpel, 2017), they do not explicitly penalize unnatural sentences, and generating sentences requires surrounding context (Le and Mikolov, 2014) or training a separate model. We turn instead to paraphrasing based on neural machine translation (Lapata et al., 2017), where P (x0 |x) (the probability of a paraphrase x0 given original sentence x) is proportional to translating x into multiple pivot languages 857 and then taking the score of back-translating the translations into the original language. This approach scores semantics and “plausibility” simultaneously (as translation models have “built in” language models) and allows for easy paraphrase generation, by linearly combining the paths of each back-decoder when back-translating. Unfortunately, given source sentences x and z, P (x0 |x) is not comparable to P (z 0 |z), as each has a different normali"
P18-1079,D18-1316,0,0.58834,"mantically meaningful to humans, or actionable. “Imperceptible” adversarial noise does not carry over from images to text, as adding or changing a single word in a sentence can drastically alter its meaning. Jia and Liang (2017) recognize that a true analog to detect oversensitivity would need semantic-preserving perturbations, but do not pursue an automated solution due to the difficulty of paraphrase generation. Their adversaries are whole sentence concatenations, generated by manually defined rules tailored to reading comprehension, and each adversary is specific to an individual instance. Zhao et al. (2018) generate natural text adversaries by projecting the input data to a latent space using a generative adversarial networks (GANs), and searching for adversaries close to the original instance in this latent space. Apart from the challenge of training GANs to generate high 863 quality text, there is no guarantee that an example close in the latent space is semantically equivalent. Ebrahimi et al. (2018), along with proposing character-level changes that are not semanticpreserving, also propose a heuristic that replaces single words adversarially to preserve semantics. This approach not only depe"
P19-1416,P17-1147,1,0.809277,"r most of the original single-hop accuracy, indicating that these distractors are still insufficient. Another method is to consider very large distractor sets such as all of Wikipedia or the entire Web, as done in open-domain H OTPOT QA and ComplexWebQuestions (Talmor and Berant, 2018). However, this introduces additional computational challenges and/or the need for retrieval systems. Finding a small set of distractors that induce multihop reasoning remains an open challenge that is worthy of follow up work. 2 Related Work Large-scale RC datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) have enabled rapid advances in neural QA models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2018). To foster research on reasoning across multiple pieces of text, multi-hop QA has been introduced (Koˇcisk`y et al., 2018; Talmor and Berant, 2018; Yang et al., 2018). These datasets contain compositional or “complex” questions. We demonstrate that these questions do not necessitate multi-hop reasoning. Existing multi-hop QA datasets are constructed using knowledge bases, e.g., W IKI H OP (Welbl et al., 2017) and C OMPLEX W EB Q UESTIONS (Talmor and Berant, 2018), or us"
P19-1416,Q18-1023,0,0.0788822,"Missing"
P19-1416,D16-1264,0,0.0696953,"e distractors can recover most of the original single-hop accuracy, indicating that these distractors are still insufficient. Another method is to consider very large distractor sets such as all of Wikipedia or the entire Web, as done in open-domain H OTPOT QA and ComplexWebQuestions (Talmor and Berant, 2018). However, this introduces additional computational challenges and/or the need for retrieval systems. Finding a small set of distractors that induce multihop reasoning remains an open challenge that is worthy of follow up work. 2 Related Work Large-scale RC datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) have enabled rapid advances in neural QA models (Seo et al., 2017; Xiong et al., 2018; Yu et al., 2018; Devlin et al., 2018). To foster research on reasoning across multiple pieces of text, multi-hop QA has been introduced (Koˇcisk`y et al., 2018; Talmor and Berant, 2018; Yang et al., 2018). These datasets contain compositional or “complex” questions. We demonstrate that these questions do not necessitate multi-hop reasoning. Existing multi-hop QA datasets are constructed using knowledge bases, e.g., W IKI H OP (Welbl et al., 2017) and C OMPLEX W EB Q UESTIONS (Talmor and"
P19-1416,N18-1059,0,0.20532,"difficulty. However, since only one of the ten paragraphs is about an animal, one can immediately locate the answer in Paragraph 1 using one hop. The full example is provided in Appendix A. established to protect?”, and then answer “What is the former name of that animal?”. However, when considering the evidence paragraphs, the question is solvable in a single hop by finding the only paragraph that describes an animal. Introduction Multi-hop reading comprehension (RC) requires reading and aggregating information over multiple pieces of textual evidence (Welbl et al., 2017; Yang et al., 2018; Talmor and Berant, 2018). In this work, we argue that it can be difficult to construct large multi-hop RC datasets. This is because multi-hop reasoning is a characteristic of both the question and the provided evidence; even highly compositional questions can be answered with a single hop if they target specific entity types, or the facts needed to answer them are redundant. For example, the question in Figure 1 is compositional: a plausible solution is to find “What animal’s habitat was the R´eserve Naturelle Lomako Yokokala ∗ Equal Contribution. Our analysis is centered on H OTPOT QA (Yang et al., 2018), a dataset"
P19-1416,D13-1160,0,\N,Missing
P19-1416,D18-1453,0,\N,Missing
P19-1416,D18-1259,0,\N,Missing
P19-1416,P18-1078,1,\N,Missing
P19-1416,N19-1423,0,\N,Missing
P19-1416,Q18-1021,0,\N,Missing
P19-1598,N16-1024,0,0.0365848,"ring evaluation. Furthermore, as discussed in Section 2.2, the goal in language modelling P is to measure the marginal probability p(x) = E p(x, E) not the joint probability. However, this sum is intractable to compute due to the large combinatorial space of possible annotations. We address this problem by approximating the marginal distribution using importance sampling. Given samples from a proposal distribution q(E|x) the marginal distribution is: p(x) = X p (x, E) = E X p (x, E) 1 X p (x, E) ≈ N q (E|x) E∼q E q (E|x) q (E|x) This approach is used to evaluate models in Ji et al. (2017) and Dyer et al. (2016). Following Ji et al. (2017), we compute q (E|x) using a discriminative version of our model that predicts annotations for the current token instead of for the next token. 5 Experiments To evaluate the proposed language model, we first introduce the baselines, followed by an evaluation using perplexity of held-out corpus, accuracy on fact completion, and an illustration of how the model uses the knowledge graph. 5.1 Evaluation Setup Baseline Models We compare KGLM to the following baseline models: • AWD-LSTM (Merity et al., 2018): strong LSTM-based model used as the foundation of most state-of"
P19-1598,W18-6521,0,0.0146107,"iers to incorporating factual knowledge into language models is that training data is hard to obtain. Standard language modeling corpora consist only of text, and thus are unable to describe which entities or facts each token is referring to. In contrast, while relation extraction datasets link text to a knowledge graph, the text is made up of disjoint sentences that do not provide sufficient context to train a powerful language model. Our goals are much more aligned to the data-to-text task (Ahn et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Yang et al., 2017; Gardent et al., 2017; Ferreira et al., 2018), where a small table-sized KB is provided to generate a short piece of text; we are interested in language models that dynamically decide the facts to incorporate from the knowledge graph, guided by the discourse. For these reasons we introduce the Linked WikiText-2 dataset, consisting of (approximately) the same articles appearing in the WikiText-2 language modeling corpus, but linked to the Wikidata (Vrandeˇci´c and Krötzsch, 2014) knowledge graph. Because the text closely matches, models trained on Linked WikiText-2 can be compared to models trained on WikiText-2. Furthermore, because many"
P19-1598,W17-3518,0,0.0311906,"ne of the primary barriers to incorporating factual knowledge into language models is that training data is hard to obtain. Standard language modeling corpora consist only of text, and thus are unable to describe which entities or facts each token is referring to. In contrast, while relation extraction datasets link text to a knowledge graph, the text is made up of disjoint sentences that do not provide sufficient context to train a powerful language model. Our goals are much more aligned to the data-to-text task (Ahn et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Yang et al., 2017; Gardent et al., 2017; Ferreira et al., 2018), where a small table-sized KB is provided to generate a short piece of text; we are interested in language models that dynamically decide the facts to incorporate from the knowledge graph, guided by the discourse. For these reasons we introduce the Linked WikiText-2 dataset, consisting of (approximately) the same articles appearing in the WikiText-2 language modeling corpus, but linked to the Wikidata (Vrandeˇci´c and Krötzsch, 2014) knowledge graph. Because the text closely matches, models trained on Linked WikiText-2 can be compared to models trained on WikiText-2. F"
P19-1598,P16-1154,0,0.0413536,"same distribution over the vocabulary as in Eqn (1) - a softmax using ht,x . If there is an entity to render, we construct the distribution over the original vocabulary and a vocabulary containing all the tokens that appear in aliases of et . This distribution is conditioned on et in addition to xt . To compute the scores over the original vocabulary, ht,x is replaced by h′t,x = Wproj [ht,x ; vet ] where Wproj is a learned weight matrix that projects the concatenated vector into the same vector space as ht,x . To obtain probabilities for words in the alias vocabulary, we use a copy mechanism Gu et al. (2016). The token sequences comprising each alias {aj } are embedded then encoded using an LSTM to form vectors aj . Copy scores are computed as: h   i T p(xt = aj ) ∝ exp σ h′t,x Wcopy aj 3 Linked WikiText-2 Modeling aside, one of the primary barriers to incorporating factual knowledge into language models is that training data is hard to obtain. Standard language modeling corpora consist only of text, and thus are unable to describe which entities or facts each token is referring to. In contrast, while relation extraction datasets link text to a knowledge graph, the text is made up of disjoint"
P19-1598,D17-1284,1,0.776783,"ing paragraphs describe each step in detail. Initial entity annotations We begin by identifying an initial set of entity mentions within the text. The primary source of these mentions is the humanprovided links between Wikipedia articles. Whenever a span of text is linked to another Wikipedia article, we associate its corresponding Wikidata entity with the span. While article links provide a large number of gold entity annotations, they are insufficient for capturing all of the mentions in the article since entities are only linked the first time they occur. Accordingly, we use the neural-el (Gupta et al., 2017) entity linker to identify additional links to Wikidata, and identify coreferences using Stanford CoreNLP2 to cover pronouns, nominals, and other tokens missed by the linker. Local knowledge graph The next step iteratively creates a generative story for the entities using relations in the knowledge graph as well as identifies new entities. To do this, we process the text token by token. Each time an entity is encountered, we add all of the related entities in Wikidata as candi5965 2 https://stanfordnlp.github.io/CoreNLP/ Tokens xt Super Mario Land is a Mention type Entity Mentioned Relation Pa"
P19-1598,D17-1195,0,0.355854,"ess to annotations during evaluation. Furthermore, as discussed in Section 2.2, the goal in language modelling P is to measure the marginal probability p(x) = E p(x, E) not the joint probability. However, this sum is intractable to compute due to the large combinatorial space of possible annotations. We address this problem by approximating the marginal distribution using importance sampling. Given samples from a proposal distribution q(E|x) the marginal distribution is: p(x) = X p (x, E) = E X p (x, E) 1 X p (x, E) ≈ N q (E|x) E∼q E q (E|x) q (E|x) This approach is used to evaluate models in Ji et al. (2017) and Dyer et al. (2016). Following Ji et al. (2017), we compute q (E|x) using a discriminative version of our model that predicts annotations for the current token instead of for the next token. 5 Experiments To evaluate the proposed language model, we first introduce the baselines, followed by an evaluation using perplexity of held-out corpus, accuracy on fact completion, and an illustration of how the model uses the knowledge graph. 5.1 Evaluation Setup Baseline Models We compare KGLM to the following baseline models: • AWD-LSTM (Merity et al., 2018): strong LSTM-based model used as the foun"
P19-1598,D16-1128,0,0.220907,") ∝ exp σ h′t,x Wcopy aj 3 Linked WikiText-2 Modeling aside, one of the primary barriers to incorporating factual knowledge into language models is that training data is hard to obtain. Standard language modeling corpora consist only of text, and thus are unable to describe which entities or facts each token is referring to. In contrast, while relation extraction datasets link text to a knowledge graph, the text is made up of disjoint sentences that do not provide sufficient context to train a powerful language model. Our goals are much more aligned to the data-to-text task (Ahn et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Yang et al., 2017; Gardent et al., 2017; Ferreira et al., 2018), where a small table-sized KB is provided to generate a short piece of text; we are interested in language models that dynamically decide the facts to incorporate from the knowledge graph, guided by the discourse. For these reasons we introduce the Linked WikiText-2 dataset, consisting of (approximately) the same articles appearing in the WikiText-2 language modeling corpus, but linked to the Wikidata (Vrandeˇci´c and Krötzsch, 2014) knowledge graph. Because the text closely matches, models trained on Linke"
P19-1598,P18-1196,0,0.0176554,"are mapped to a single UNK token. This is problematic for comparing the performance of the KGLM to traditional language models on Linked WikiText-2 since there are a large number of rare entities whose alias tokens are outof-vocabulary. That is, even if the KGLM identifies the correct entity and copies the correct alias token with high probability, other models can attain better perplexity by assigning a higher probability to UNK. Accordingly, we also measure unknown penalized perplexity (UPP) (a.k.a adjusted perplexity) introduced by Ueberla (1994), and used recently by Ahn et al. (2016) and Spithourakis and Riedel (2018). This metric penalizes the probability of UNK tokens by evenly dividing their probability mass over U , the set of tokens that get mapped to UNK . We can be compute UPP by replacing p(UNK) in the perplexity above by |U1 |p(UNK), where |U |is estimated from the data. We present the model perplexities in Table 3. To marginalize over annotations, perplexities for the E NTITY NLM, EntityCopyNet, and KGLM are estimated using the importance sampling approach described in Section 4. We observe that the KGLM attains substantially lower perplexity than the other entity-based language models (44.1 vs."
P19-1598,D17-1239,0,0.0535341,"Missing"
P19-1598,D17-1197,0,0.180297,"2 Modeling aside, one of the primary barriers to incorporating factual knowledge into language models is that training data is hard to obtain. Standard language modeling corpora consist only of text, and thus are unable to describe which entities or facts each token is referring to. In contrast, while relation extraction datasets link text to a knowledge graph, the text is made up of disjoint sentences that do not provide sufficient context to train a powerful language model. Our goals are much more aligned to the data-to-text task (Ahn et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Yang et al., 2017; Gardent et al., 2017; Ferreira et al., 2018), where a small table-sized KB is provided to generate a short piece of text; we are interested in language models that dynamically decide the facts to incorporate from the knowledge graph, guided by the discourse. For these reasons we introduce the Linked WikiText-2 dataset, consisting of (approximately) the same articles appearing in the WikiText-2 language modeling corpus, but linked to the Wikidata (Vrandeˇci´c and Krötzsch, 2014) knowledge graph. Because the text closely matches, models trained on Linked WikiText-2 can be compared to models tr"
P19-1621,N18-1202,0,0.115538,"Missing"
P19-1621,P18-2124,0,0.0493789,"consistent when measured by our implications and by human generated implications (and thus expected to generalize better in the real world). 2 Related Work Since QA models often exploit shortcuts to be accurate without really understanding questions and contexts, alternative evaluations have been proposed, consisting of solutions that mitigate known biases or propose separate diagnostic datasets. Examples of the former include adding multiple images for which the answer to the same question is different (Goyal et al., 2017; Zhang et al., 2016), or questions for which an answer is not present (Rajpurkar et al., 2018). While useful, these do not take the relationship between predictions into account, and thus do not capture problems like the ones in Figure 1. Exceptions exist when trying to gauge robustness: Ribeiro et al. (2018) consider the robustness of QA models to automatically generated input rephrasings, while Shah et al. (2019) evaluate VQA models on crowdsourced rephrasings for robustness. While important for evaluation, these efforts are orthogonal to our focus on consistency. Various automatically generated diagnostic datasets have been proposed (Weston et al., 2015; Johnson et al., 2017). While"
Q15-1023,D13-1184,0,0.39948,"l., 2011; Koch et al., 2014) that uses EL to create training data, and some coreference systems that use EL for disambiguation (Hajishirzi et al., 2013; Zheng et al., 2013; Durrett and Klein, 2014). Unfortunately, in spite of numerous papers on the topic and several published data sets, there is surprisingly little understanding about state-of-the-art performance. We argue that there are three reasons for this confusion. First, there is no standard definition of the problem. A few variants have been studied in the literature, such as Wikification (Milne and Witten, 2008; Ratinov et al., 2011; Cheng and Roth, 2013) which aims at linking noun phrases to Wikipedia entities and Named Entity Linking (aka Named Entity Disambiguation) (McNamee and Dang, 2009; Hoffart et al., 2011) which targets only named entities. Here we use the term Entity Linking as a unified name for both problems, and Named Entity Linking (NEL) for the subproblem of linking only named entities. But names are just one part of the problem. For many variants there are no annotation guidelines for scoring links. What types of entities are valid targets? When multiple entities are plausible for annotating a mention, which one should be chose"
Q15-1023,Q15-1011,0,0.0611063,"rity and relational triples are insufficient for capturing the context. For example, in “... while returning from Freeport to Portland. (TAC)”, the mention “Freeport”is unbounded by the state, one needs to know that it’s more likely to have both “Freeport” and “Portland” in the same state (i.e. Maine) to make a correct prediction 19 . Another reason may be TAC’s higher percentage of Web documents; since contextual information is more scattered in Web text than in newswire documents, this increases the difficulty of context modeling. We leave a more sophisticated context model for future work (Chisholm and Hachey, 2015; Singh et al., 2012). Since “Specific Labels”, “Metonymy”, and “Wrong Entity Types” correspond to the annotation issues discussed in Sections 3.2, 3.3, and 3.4, the distribution of errors are also useful in studying annotation inconsistencies. The fact that the errors vary considerably across the datasets, for instance, V INCULUM makes many more “Specific Labels” mistakes in ACE and MSNBC, strongly suggests that annotation guidelines have a considerable impact on the final performance. We also observe that annotation inconsistencies also cause reasonable predictions to be treated as a mistake"
Q15-1023,D07-1074,0,0.933858,"ndly, it is almost impossible to assess approaches, because systems are rarely compared using the same data sets. For instance, Hoffart et al. (2011) 315 Transactions of the Association for Computational Linguistics, vol. 3, pp. 315–328, 2015. Action Editor: Kristina Toutanova. Submission batch: 11/2014; Revision batch 3/2015; Published 6/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY-NC-SA 4.0 license. developed a new data set (AIDA) based on the CoNLL 2003 Named Entity Recognition data set but failed to evaluate their system on MSNBC previously created by (Cucerzan, 2007); Wikifier (Cheng and Roth, 2013) compared to the authors’ previous system (Ratinov et al., 2011) using the originally selected datasets but didn’t evaluate using AIDA data. Finally, when two end-to-end systems are compared, it is rarely clear which aspect of a system makes one better than the other. This is especially problematic when authors introduce complex mechanisms or nondeterministic methods that involve learning-based reranking or joint inference. To address these problems, we analyze several significant inconsistencies among the data sets. To have a better understanding of the import"
Q15-1023,Q14-1037,0,0.0759078,"t, e.g., KB:Entity, to indicate an entity in a particular KB, and quotes, e.g., “Mention”, to denote textual mentions. provide semantic annotations to human readers but also a machine-consumable representation of the most basic semantic knowledge in the text. Many other NLP applications can benefit from such links, such as distantly-supervised relation extraction (Craven and Kumlien, 1999; Riedel et al., 2010; Hoffmann et al., 2011; Koch et al., 2014) that uses EL to create training data, and some coreference systems that use EL for disambiguation (Hajishirzi et al., 2013; Zheng et al., 2013; Durrett and Klein, 2014). Unfortunately, in spite of numerous papers on the topic and several published data sets, there is surprisingly little understanding about state-of-the-art performance. We argue that there are three reasons for this confusion. First, there is no standard definition of the problem. A few variants have been studied in the literature, such as Wikification (Milne and Witten, 2008; Ratinov et al., 2011; Cheng and Roth, 2013) which aims at linking noun phrases to Wikipedia entities and Named Entity Linking (aka Named Entity Disambiguation) (McNamee and Dang, 2009; Hoffart et al., 2011) which target"
Q15-1023,P05-1045,0,0.103071,"ction 4.3) uses the sentence, while coreference (Section 4.4) and coherence (Section 4.5) use the full document and Web respectively. Our pipeline mimics the sieve structure introduced in (Lee et al., 2013), but instead of merging coreference clusters, we adjust the probability of candidate entities at each stage. The modularity of V INCULUM enables us to study the relative impact of its subcomponents. 4.1 Mention Extraction The first step of EL extracts potential mentions from the document. Since V INCULUM restricts attention to named entities, we use a Named Entity Recognition (NER) system (Finkel et al., 2005). Alternatively, an NP chunker may be used to identify the mentions. 4.2 Dictionary-based Candidate Generation While in theory a mention could link to any entity in the KB, in practice one sacrifices little by restricting attention to a subset (dozens) precompiled using a dictionary. A common way to build such a dictionary D is by crawling Web pages and aggregating anchor links that point to Wikipedia pages. The frequency with which a mention (anchor text), m, links to a particular entity (anchor link), c, allows one to estimate the conditional probability p(c|m). We adopt the 320 CrossWikis d"
Q15-1023,P14-2076,0,0.0235063,"Missing"
Q15-1023,D13-1029,1,0.791669,". The links not only 1 We use typewriter font, e.g., KB:Entity, to indicate an entity in a particular KB, and quotes, e.g., “Mention”, to denote textual mentions. provide semantic annotations to human readers but also a machine-consumable representation of the most basic semantic knowledge in the text. Many other NLP applications can benefit from such links, such as distantly-supervised relation extraction (Craven and Kumlien, 1999; Riedel et al., 2010; Hoffmann et al., 2011; Koch et al., 2014) that uses EL to create training data, and some coreference systems that use EL for disambiguation (Hajishirzi et al., 2013; Zheng et al., 2013; Durrett and Klein, 2014). Unfortunately, in spite of numerous papers on the topic and several published data sets, there is surprisingly little understanding about state-of-the-art performance. We argue that there are three reasons for this confusion. First, there is no standard definition of the problem. A few variants have been studied in the literature, such as Wikification (Milne and Witten, 2008; Ratinov et al., 2011; Cheng and Roth, 2013) which aims at linking noun phrases to Wikipedia entities and Named Entity Linking (aka Named Entity Disambiguation) (McNamee and"
Q15-1023,D12-1010,0,0.0185177,"Missing"
Q15-1023,P13-2006,0,0.518902,"Missing"
Q15-1023,D13-1041,0,0.0558743,"Missing"
Q15-1023,D11-1072,0,0.752275,"Missing"
Q15-1023,P11-1055,1,0.0756508,"ity KB:JetBlue, “Barnstable Airport” to KB:Barnstable Municipal Airport, and “JFK International” to KB:John F. Kennedy International Airport1 . The links not only 1 We use typewriter font, e.g., KB:Entity, to indicate an entity in a particular KB, and quotes, e.g., “Mention”, to denote textual mentions. provide semantic annotations to human readers but also a machine-consumable representation of the most basic semantic knowledge in the text. Many other NLP applications can benefit from such links, such as distantly-supervised relation extraction (Craven and Kumlien, 1999; Riedel et al., 2010; Hoffmann et al., 2011; Koch et al., 2014) that uses EL to create training data, and some coreference systems that use EL for disambiguation (Hajishirzi et al., 2013; Zheng et al., 2013; Durrett and Klein, 2014). Unfortunately, in spite of numerous papers on the topic and several published data sets, there is surprisingly little understanding about state-of-the-art performance. We argue that there are three reasons for this confusion. First, there is no standard definition of the problem. A few variants have been studied in the literature, such as Wikification (Milne and Witten, 2008; Ratinov et al., 2011; Cheng an"
Q15-1023,D14-1203,1,0.770824,"able Airport” to KB:Barnstable Municipal Airport, and “JFK International” to KB:John F. Kennedy International Airport1 . The links not only 1 We use typewriter font, e.g., KB:Entity, to indicate an entity in a particular KB, and quotes, e.g., “Mention”, to denote textual mentions. provide semantic annotations to human readers but also a machine-consumable representation of the most basic semantic knowledge in the text. Many other NLP applications can benefit from such links, such as distantly-supervised relation extraction (Craven and Kumlien, 1999; Riedel et al., 2010; Hoffmann et al., 2011; Koch et al., 2014) that uses EL to create training data, and some coreference systems that use EL for disambiguation (Hajishirzi et al., 2013; Zheng et al., 2013; Durrett and Klein, 2014). Unfortunately, in spite of numerous papers on the topic and several published data sets, there is surprisingly little understanding about state-of-the-art performance. We argue that there are three reasons for this confusion. First, there is no standard definition of the problem. A few variants have been studied in the literature, such as Wikification (Milne and Witten, 2008; Ratinov et al., 2011; Cheng and Roth, 2013) which"
Q15-1023,J13-4004,0,0.039274,"nks around this mention. The candidate entity with the maximum score, i.e. l = arg max s(c|m, d), is picked as c∈Cm the predicted link of m. Figure 2 illustrates the linking pipeline that follows mention extraction. For each mention, V INCULUM ranks the candidates at each stage based on an ever widening context. For example, candidate generation (Section 4.2) merely uses the mention string, entity typing (Section 4.3) uses the sentence, while coreference (Section 4.4) and coherence (Section 4.5) use the full document and Web respectively. Our pipeline mimics the sieve structure introduced in (Lee et al., 2013), but instead of merging coreference clusters, we adjust the probability of candidate entities at each stage. The modularity of V INCULUM enables us to study the relative impact of its subcomponents. 4.1 Mention Extraction The first step of EL extracts potential mentions from the document. Since V INCULUM restricts attention to named entities, we use a Named Entity Recognition (NER) system (Finkel et al., 2005). Alternatively, an NP chunker may be used to identify the mentions. 4.2 Dictionary-based Candidate Generation While in theory a mention could link to any entity in the KB, in practice o"
Q15-1023,Q14-1019,0,0.0569538,"cult to decide how specific the gold link should be. Given a static knowledge base, which is often incomplete, one cannot always find the most specific entity. For instance, there is no Wikipedia page for the KB:116th U.S. Congress because the Congress has not been elected yet. On the other hand, using general concepts can cause troubles for machine reading. Consider president-of relation extraction on the following sentence. Example 3 Joe Biden is the Senate President in the 113th United States Congress. 5 Note that linking common noun phrases is closely related to Word Sense Disambiguation (Moro et al., 2014). 6 We define named entity mention extensionally: any name uniquely referring to one entity of a predefined class, e.g. a specific person or location. Person Location TAC GPE (Geopolitical Entities) TAC Person Common Concepts E.g. Brain_Tumor, Desk, Water, etc. TAC Organization Organization Misc. Figure 1: Entities divided by their types. For named entities, the solid squares represent 4 CoNLL(AIDA) classes; the red dashed squares display 3 TAC classes; the shaded rectangle depicts common concepts. Failure to distinguish different Congress iterations would cause an information extraction syste"
Q15-1023,P11-1138,0,0.142648,"., 2010; Hoffmann et al., 2011; Koch et al., 2014) that uses EL to create training data, and some coreference systems that use EL for disambiguation (Hajishirzi et al., 2013; Zheng et al., 2013; Durrett and Klein, 2014). Unfortunately, in spite of numerous papers on the topic and several published data sets, there is surprisingly little understanding about state-of-the-art performance. We argue that there are three reasons for this confusion. First, there is no standard definition of the problem. A few variants have been studied in the literature, such as Wikification (Milne and Witten, 2008; Ratinov et al., 2011; Cheng and Roth, 2013) which aims at linking noun phrases to Wikipedia entities and Named Entity Linking (aka Named Entity Disambiguation) (McNamee and Dang, 2009; Hoffart et al., 2011) which targets only named entities. Here we use the term Entity Linking as a unified name for both problems, and Named Entity Linking (NEL) for the subproblem of linking only named entities. But names are just one part of the problem. For many variants there are no annotation guidelines for scoring links. What types of entities are valid targets? When multiple entities are plausible for annotating a mention, wh"
Q15-1023,spitkovsky-chang-2012-cross,0,0.374856,"veral significant inconsistencies among the data sets. To have a better understanding of the importance of various techniques, we develop a simple and modular, unsupervised EL system, V INCULUM. We compare V INCULUM to the two leading sophisticated EL systems on a comprehensive set of nine datasets. While our system does not consistently outperform the best EL system, it does come remarkably close and serves as a simple and competitive baseline for future research. Furthermore, we carry out an extensive ablation analysis, whose results illustrate 1) even a near-trivial model using CrossWikis (Spitkovsky and Chang, 2012) performs surprisingly well, and 2) incorporating a fine-grained set of entity types raises that level even higher. In summary, we make the following contributions: • We analyze the differences among several versions of the entity linking problem, compare existing data sets and discuss annotation inconsistencies between them. (Sections 2 & 3) • We present a simple yet effective, modular, unsupervised system, V INCULUM, for entity linking. We make the implementation open source and publicly available for future research.2 (Section 4) • We compare V INCULUM to 2 state-of-the-art systems on an ex"
Q15-1023,W13-3517,1,0.748446,"e use typewriter font, e.g., KB:Entity, to indicate an entity in a particular KB, and quotes, e.g., “Mention”, to denote textual mentions. provide semantic annotations to human readers but also a machine-consumable representation of the most basic semantic knowledge in the text. Many other NLP applications can benefit from such links, such as distantly-supervised relation extraction (Craven and Kumlien, 1999; Riedel et al., 2010; Hoffmann et al., 2011; Koch et al., 2014) that uses EL to create training data, and some coreference systems that use EL for disambiguation (Hajishirzi et al., 2013; Zheng et al., 2013; Durrett and Klein, 2014). Unfortunately, in spite of numerous papers on the topic and several published data sets, there is surprisingly little understanding about state-of-the-art performance. We argue that there are three reasons for this confusion. First, there is no standard definition of the problem. A few variants have been studied in the literature, such as Wikification (Milne and Witten, 2008; Ratinov et al., 2011; Cheng and Roth, 2013) which aims at linking noun phrases to Wikipedia entities and Named Entity Linking (aka Named Entity Disambiguation) (McNamee and Dang, 2009; Hoffart"
W12-3021,D07-1101,0,0.0688272,"Missing"
W12-3021,N07-1011,1,0.875559,"Missing"
W12-3021,P11-1055,0,0.0562764,"Missing"
W12-3021,P11-1080,1,0.861392,"Missing"
W12-3021,D10-1099,1,0.883234,"Missing"
W13-3517,N10-1061,0,0.151151,"es iteratively applied to identify the chains, such as Haghighi and Klein (2009), Raghunathan et al. (2010), Stoyanov et al. (2010). Alternatively (and similar to our approach), others represent this knowledge as features in a machine learning model. Early applications of such models include Soon et al. (2001), Ng and Cardie (2002) and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009), cross-document coreference (Finin et al., 2009; Singh et al., 2010), and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011). This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference. Ponzetto and Strube (2006) identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in"
W13-3517,P98-1013,0,0.00899288,"irs. Coreference resolution forms an important component for natural language processing and information extraction pipelines due to its utility in relation extraction, cross-document coreference, text summarization, and question answering. The task of coreference is challenging for automated systems as the local information contained in the document is often not enough to accurately disambiguate mentions, for example, coreferencing (m1 , m2 ) requires identifying that George W. Bush (m1 ) is the governor of Texas (m2 ), and similarly for (m3 , m4 ). External knowledge-bases such as FrameNet (Baker et al., 1998), Wikipedia, Yago (Suchanek et al., 2007), and Freebase (Bollacker et al., 2008), can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation. Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of men"
W13-3517,D08-1031,0,0.287666,"stem (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010). Given a document with its mentions, the system iteratively checks each mention mj for coreference with preceding mentions using a classifier. A coreference link may be created between mj and one of these preceding mentions using one of the following strategies. The C LOSEST L INK (Soon et al., 2001) method picks the closest mention to mj that is positively classified, while the B EST L INK (Ng and Cardie, 2002) method links mj to the precedTypes StringSimilarity Syntax Semantic Other Features mention string match, head string match, head substring match, head word pai"
W13-3517,D07-1074,0,0.0791348,"rs represent this knowledge as features in a machine learning model. Early applications of such models include Soon et al. (2001), Ng and Cardie (2002) and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009), cross-document coreference (Finin et al., 2009; Singh et al., 2010), and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011). This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference. Ponzetto and Strube (2006) identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in a pairwise model. Our method differs in that we draw such features from entity candidates during inference, and also maintain and update a set of candidate entity links ins"
W13-3517,N07-1011,1,0.351235,"nts in the development set. The data is processed using standard open source tools to segment the sentences and tokenize the corpus, and using the OpenNLP2 tagger to obtain the POS tags. The hyperparameters of our system, such as regularization, initial number of candidates, and the number of compar2 http://opennlp.apache.org/ isons during training (k in Section 2.3) are tuned on the development data when trained on the train set. The models we use to evaluate on the test data set are trained on the training and development sets, following the standard evaluation for coreference first used by Culotta et al. (2007). To provide the initial ranked list of entity candidates from Wikipedia, we query the KB Bridge system (Dalton and Dietz, 2013) with the proper name mentions. KB Bridge is an information-retrievalbased entity linking system that connects the query mentions to Wikipedia entities using a sequential dependence model. This system has been shown to match or outperform the top performing systems in the 2012 TAC KBP entity linking task. 4.2 Methods Our experiments investigate a number of baselines that are similar or identical to existing approaches. Wikipedia Linking: As a simple baseline, we direc"
W13-3517,doddington-etal-2004-automatic,0,0.0111976,"ashington State does appear in the candidate entities of the first two mentions, albeit with a lower rank. In our approach, clustering the first two mentions causes the shared candidate Washington State to move to the top of the list. The coreference system is now able to easily identify that the “Washington State” mention is compatible with the Washington State entity formed by the previous two mentions, providing evidence that the final mention should be clustered with either of them in subsequent comparisons. 4 Experiments 4.1 Setup We evaluate our system on the ACE 2004 annotated dataset (Doddington et al., 2004). Following the setup in Bengston and Roth (2008), we split the corpus into training, development, and test sets, resulting in 268 documents in the train set, 107 documents in the test set, and 68 documents in the development set. The data is processed using standard open source tools to segment the sentences and tokenize the corpus, and using the OpenNLP2 tagger to obtain the POS tags. The hyperparameters of our system, such as regularization, initial number of candidates, and the number of compar2 http://opennlp.apache.org/ isons during training (k in Section 2.3) are tuned on the developmen"
W13-3517,D09-1120,0,0.222949,"phrase mentions during training. We obtain B3 F1 of 65.3, 67.6, and 67.7 for our baseline, static linking, and dynamic linking respectively.4 When compared to the participants of the closed task, the dynamic linking system outperforms all but two on this metric, suggesting that dynamic alignment is beneficial even when the features have not been engineered for events or for different genres. 6 Related Work Within-document coreference has been wellstudied for a number of years. A variety of approaches incorporate linguistic knowledge as rules iteratively applied to identify the chains, such as Haghighi and Klein (2009), Raghunathan et al. (2010), Stoyanov et al. (2010). Alternatively (and similar to our approach), others represent this knowledge as features in a machine learning model. Early applications of such models include Soon et al. (2001), Ng and Cardie (2002) and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improve"
W13-3517,P11-1055,0,0.051117,"and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009), cross-document coreference (Finin et al., 2009; Singh et al., 2010), and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011). This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference. Ponzetto and Strube (2006) identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in a pairwise model. Our method differs in that we draw such features from entity candidates during inference, and also maintain and update a set of candidate entity links instead of selecting only one. Rahman and Ng (2011) introduce similar features from a more extensive set of knowledge sources (such as YAGO and FrameNet) into a cl"
W13-3517,W11-1902,0,0.0754511,"Missing"
W13-3517,P02-1014,0,0.297829,"outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010). Given a document with its mentions, the system iteratively checks each mention mj for coreference with preceding mentions using a classifier. A coreference link may be created between mj and one of these preceding mentions using one of the following strategies. The C LOSEST L INK (Soon et al., 2001) method picks the closest mention to mj that is positively classified, while the B EST L INK (Ng and Cardie, 2002) method links mj to the precedTypes StringSimilarity Syntax Semantic Other Features mention string match, he"
W13-3517,P10-1142,0,0.041068,"Missing"
W13-3517,N06-1025,0,0.429605,"ften not enough to accurately disambiguate mentions, for example, coreferencing (m1 , m2 ) requires identifying that George W. Bush (m1 ) is the governor of Texas (m2 ), and similarly for (m3 , m4 ). External knowledge-bases such as FrameNet (Baker et al., 1998), Wikipedia, Yago (Suchanek et al., 2007), and Freebase (Bollacker et al., 2008), can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation. Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking. Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in Ratinov and Roth (2012)). Alternatively, Rahman and Ng (2011) link each mention to multipl"
W13-3517,W11-1901,0,0.0115112,"ur baseline is shown in Figure 3. Our static linking matches the performance of Ratinov and Roth (2012) on the non-transcripts. Further, the improvement of static linking on the transcripts over the baseline is lower than that on the non-transcript data, suggesting that noisy mentions and text result in poor quality alignment. Dynamic linking, on the other hand, not only outperforms all other systems, but also shows a higher improvement over the baseline on the transcripts than OntoNotes We also run our systems on the OntoNotes dataset, which was used for evaluation in CoNLL 2011 Shared Task (Pradhan et al., 2011). The dataset consists of 2083 documents from a much larger variety of genres, such as conversations, magazines, web text, etc. Further, the dataset also consists of mentions that refer to events, most of which do not appear as Wikipedia pages. Since only the nonsingleton mentions are annotated in the training set, we also include additional noun phrase mentions during training. We obtain B3 F1 of 65.3, 67.6, and 67.7 for our baseline, static linking, and dynamic linking respectively.4 When compared to the participants of the closed task, the dynamic linking system outperforms all but two on t"
W13-3517,D10-1048,0,0.807132,"approach stays fairly even as X is varied. Even though the experiments suggest that the larger documents are tougher to coreference,3 dynamic linking provides higher improvements when the documents contain a larger number of mentions. 5.2 Performance on Transcripts The quality of alignment and the coreference predictions for a document is influenced by the quality of the mentions in the document. In particular, 158 3 i.e., the absolute values are lower for these splits. The baseline system obtains 83.08, 79.29, 79.64, and 79.77 respectively for X = 10, 33, 40, 50. Method Culotta et al. (2007) Raghunathan et al. (2010) Stoyanov and Eisner (2012) Wiki-linking Bengston and Roth (2008) Baseline Static Linking Dynamic Linking Pairwise P/R F1 71.6 46.2 56.1 64.15 14.99 66.56 47.07 82.53 40.80 72.20 47.40 24.30 55.14 54.61 57.23 MUC P/R 80.4 71.8 74.41 82.7 82.84 88.39 85.07 28.39 69.9 72.02 66.93 72.02 F1 75.8 80.1 41.10 75.8 77.05 76.18 78.01 CEAF P/R 58.54 58.4 75.58 75.40 75.33 75.35 76.55 76.37 F1 58.47 75.49 75.44 76.46 B3 P/R 86.7 73.2 86.3 75.4 92.89 88.3 87.02 93.10 89.37 57.21 74.5 75.97 72.72 76.12 F1 79.3 80.4 81.8 70.81 80.8 81.12 81.66 82.21 Table 2: Evaluation on the ACE test data, with the system"
W13-3517,P11-1082,0,0.568436,"ctive recent research. Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking. Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in Ratinov and Roth (2012)). Alternatively, Rahman and Ng (2011) link each mention to multiple entities in the knowledge base, improving recall at the cost of lower precision; the attributes of all the linked entities are aggregated as features. Although this approach is more robust to noise in the documents, the features of a mention merge the different aspects of the entities, for example a “Michael Jordan” mention will contain features for both the scientist and basketball personas. Instead of fixing the alignment of the mentions to the knowledge base, our proposed approach maintains a ranked list of candidate entities for each mention. To expand the se"
W13-3517,D12-1113,0,0.675058,"isambiguate mentions, for example, coreferencing (m1 , m2 ) requires identifying that George W. Bush (m1 ) is the governor of Texas (m2 ), and similarly for (m3 , m4 ). External knowledge-bases such as FrameNet (Baker et al., 1998), Wikipedia, Yago (Suchanek et al., 2007), and Freebase (Bollacker et al., 2008), can be used to provide global context, and there is a strong need for coreference resolution systems to accurately use such sources for disambiguation. Incorporating external knowledge bases into coreference has been the subject of active recent research. Ponzetto and Strube (2006) and Ratinov and Roth (2012) precompute a fixed alignment of the mentions to the knowledge base entities. The attributes of these entities are used during coreference by incorporating them in the mention features. Since alignment of mentions to the external entities is itself a difficult task, these systems favor high-precision linking. Unfortunately, this results in fewer alignments, and improvements are only shown on mentions that are easier to align and corefer (such as the non-transcript documents in Ratinov and Roth (2012)). Alternatively, Rahman and Ng (2011) link each mention to multiple entities in the knowledge"
W13-3517,P11-1138,0,0.0541057,"a. The high-precision mention-candidate pairings are precomputed and fixed; additionally, the features for an entity are based on the predictions of the previous sieves, thus fixed while a sieve is applied. With these restrictions, they show improvements over the state-ofthe-art on a subset of ACE mentions that are more easily aligned to Wikipedia, while our approach demonstrates improvements on the complete set of mentions including the tougher to link mentions from the transcripts. There are a number of approaches that provide an alignment from mentions in a document to Wikipedia. Wikifier (Ratinov et al., 2011) analyzes the context around the mentions and the entities jointly, and was used to align mentions for coreference in Ratinov and Roth (2012). Dalton and Dietz (2013) introduce an approximation to the above approach, but incorporate retrieval-based supervised reranking that provides multiple candidates and scores; this approach performed competitively on previous TAC-KBP entity linking benchmarks (Dietz and Dalton, 2012). Alignment to an external Conclusions A number of possible avenues for future study are apparent. First, our alignment to a knowledgebase can benefit from more document-aware"
W13-3517,D10-1099,1,0.101154,"Ng and Cardie (2002) and (Bengston and Roth, 2008). There are also a number of techniques that represent entities explicitly (Culotta et 159 4 with MUC 46.1, 49.9 & 50.1, and CEAF(m) 47.9, 49.6 & 49.8, respectively for baseline, static and dynamic linking. al., 2007; Wick et al., 2009; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012). knowledge-base has improved performance for a number of NLP and information extraction tasks, such as named-entity recognition (Cucerzan, 2007; Han and Zhao, 2009), cross-document coreference (Finin et al., 2009; Singh et al., 2010), and relation-extraction (Riedel et al., 2010; Hoffmann et al., 2011). This work is an extension of recent approaches that incorporate external knowledge sources to improve within-document coreference. Ponzetto and Strube (2006) identify Wikipedia candidates for each mention as a preprocessing step, and incorporate them as features in a pairwise model. Our method differs in that we draw such features from entity candidates during inference, and also maintain and update a set of candidate entity links instead of selecting only one. Rahman and Ng (2011) introduce similar features from a more extensive set of knowledge sources (such as YAGO"
W13-3517,J01-4004,0,0.231353,"ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010). Given a document with its mentions, the system iteratively checks each mention mj for coreference with preceding mentions using a classifier. A coreference link may be created between mj and one of these preceding mentions using one of the following strategies. The C LOSEST L INK (Soon et al., 2001) method picks the closest mention to mj that is positively classified, while the B EST L INK (Ng and Cardie, 2002) method links mj to the precedTypes StringSimilarity Syntax Semantic Other Features men"
W13-3517,spitkovsky-chang-2012-cross,0,0.0297459,"he large set of surface string variations and constant reranking of the entity candidates during inference allows our approach to correct mistakes in alignment and makes external information applicable to a wider variety of mentions. Our paper provides the following contributions: (1) an approach that jointly reasons about both within-doc entities and their alignment to KBentities by dynamically adjusting a ranked list of candidate alignments, during coreference, (2) Utilization of a larger set of surface string variations for each entity candidate by using links that appear all over the web (Spitkovsky and Chang, 2012), (3) A combination of these approaches that improves upon a competitive baseline without a knowledge base by 1.09 B3 F1 points on the ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system"
W13-3517,C12-1154,0,0.168314,"Our paper provides the following contributions: (1) an approach that jointly reasons about both within-doc entities and their alignment to KBentities by dynamically adjusting a ranked list of candidate alignments, during coreference, (2) Utilization of a larger set of surface string variations for each entity candidate by using links that appear all over the web (Spitkovsky and Chang, 2012), (3) A combination of these approaches that improves upon a competitive baseline without a knowledge base by 1.09 B3 F1 points on the ACE 2004 data, and outperforms the state-of-the-art coreference system (Stoyanov and Eisner, 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyano"
W13-3517,P10-2029,0,0.230993,", 2012) by 0.41 B3 F1 points, and (4) Accurate predictions on documents that are difficult for coreference, such as the transcript documents that were omitted from the evaluation in Ratinov and Roth (2012), and documents that contain a large number of mentions. 2 Baseline Pairwise System In this section we describe a variant of a commonlyused coreference resolution system that does not utilize external knowledge sources. This widely adopted model casts the problem as a series of binary classifications (Soon et al., 2001; Ng and Cardie, 2002; Ponzetto and Strube, 2006; Bengston and Roth, 2008; Stoyanov et al., 2010). Given a document with its mentions, the system iteratively checks each mention mj for coreference with preceding mentions using a classifier. A coreference link may be created between mj and one of these preceding mentions using one of the following strategies. The C LOSEST L INK (Soon et al., 2001) method picks the closest mention to mj that is positively classified, while the B EST L INK (Ng and Cardie, 2002) method links mj to the precedTypes StringSimilarity Syntax Semantic Other Features mention string match, head string match, head substring match, head word pair, mention substring mat"
W13-3517,C98-1013,0,\N,Missing
W14-2409,N13-1008,1,0.326335,"have often shown low recall (e.g. Bos and Markert, 2005) as they are affected by the limited coverage of ontologies such as WordNet. Moreover, due to their deterministic nature they often cannot cope with noise and uncertainty inherent to real world data, and inference with such representations is difficult to scale up. Embedding-based approaches address some of the concerns above. Here relational worlds are described using low-dimensional embeddings of entities and relations based on relational evidence in knowledge bases (Bordes et al., 2011) or surfaceform relationships mentioned in text (Riedel et al., 2013). To overcome the generalization bottleneck, these approaches learn to embed similar entities and relations as vectors close in distance. Subsequently, unseen facts can be inferred by simple and efficient linear algebra operations (e.g. dot products). The core argument against embeddings is their supposed inability to capture deeper semantics, and more complex patterns of reasoning such as those enabled by first-order logic (Lewis and Steedman, 2013). Here we argue that this does not need to be true. We present an approach that enables us to learn low-dimensional embeddings such that the model"
W14-2409,D12-1110,0,0.00765774,"we optimize this objective with SGD to learn low-dimensional embeddings that indeed follow the behavior of the knowledge base. 6 Related Work The idea of bringing together distributional semantics and formal logic is not new. Lewis and Steedman (2013) improve the generalization performance of a semantic parser via the use of distributional representations. However, their target representation language is still symbolic, and it is unclear how this approach can cope with noise and uncertainty in data. Another line of work (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Coecke et al., 2010; Socher et al., 2012; Hermann and Blunsom, 2013) uses symbolic representations to guide the composition of distributional representations. Reading a sentence or logical formula there amounts to compositionally mapping it to a k-dimensional vector that then can be used for downstream tasks. We propose a very different approach: Reading a sentence amounts to updating the involved entity pair and relation embeddings such that the sentence evaluates to true. Afterwards we cannot use the embeddings to calculate sentence similarities, but to answer relational questions about the world. Similar to our work, Bowman (2014"
W14-2409,H05-1079,0,0.216959,"Missing"
W14-2409,S13-1001,0,0.371733,"ion where the number of dimensions equals to the number of entities in the domain. worksFor A C Algebra worksFor(B) worksFor(B) D Figure 1: Information extraction (IE) and semantic parsing (SP) extract factual and more general logical statements from text, respectively. Humans can manually curate this knowledge. Instead of reasoning with this knowledge directly (A) we inject it into low dimensional representations of entities and relations (B). Linear algebra operations manipulate embeddings to derive truth vectors (C), which can be discretized or thresholded to retrieve truth values (D). 2.1 Grefenstette (2013) presents an isomorphism between statements in predicate logic and expressions in tensor calculus. Let [·] denote this mapping from a logical expression F to an expression in tensor algebra. Here, logical statements evaluating to true or false are mapped to [true] :=  T  T > = 1 0 and [false] := ⊥ = 0 1 respectively. Entities are represented by logical constants and mapped to one-hot vectors where each component represents a unique entity. For example, let k = 3 be the number of entities in a domain, then  T S MITH may be mapped to [S MITH] = 1 0 0 . Unary predicates are represented as 2"
W14-2409,P13-1088,0,0.0261226,"ective with SGD to learn low-dimensional embeddings that indeed follow the behavior of the knowledge base. 6 Related Work The idea of bringing together distributional semantics and formal logic is not new. Lewis and Steedman (2013) improve the generalization performance of a semantic parser via the use of distributional representations. However, their target representation language is still symbolic, and it is unclear how this approach can cope with noise and uncertainty in data. Another line of work (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Coecke et al., 2010; Socher et al., 2012; Hermann and Blunsom, 2013) uses symbolic representations to guide the composition of distributional representations. Reading a sentence or logical formula there amounts to compositionally mapping it to a k-dimensional vector that then can be used for downstream tasks. We propose a very different approach: Reading a sentence amounts to updating the involved entity pair and relation embeddings such that the sentence evaluates to true. Afterwards we cannot use the embeddings to calculate sentence similarities, but to answer relational questions about the world. Similar to our work, Bowman (2014) provides further evidence"
W14-2409,Q13-1015,0,0.188105,"ddings of entities and relations based on relational evidence in knowledge bases (Bordes et al., 2011) or surfaceform relationships mentioned in text (Riedel et al., 2013). To overcome the generalization bottleneck, these approaches learn to embed similar entities and relations as vectors close in distance. Subsequently, unseen facts can be inferred by simple and efficient linear algebra operations (e.g. dot products). The core argument against embeddings is their supposed inability to capture deeper semantics, and more complex patterns of reasoning such as those enabled by first-order logic (Lewis and Steedman, 2013). Here we argue that this does not need to be true. We present an approach that enables us to learn low-dimensional embeddings such that the model behaves as if it follows a complex first-order reasoning process—but still operates in terms of simple vector and matrix representations. In this view, machine reading becomes the process of taking (inherently symbolic) knowledge in language and injecting this knowledge into a sub-symbolic distributional world model. For example, one could envision a semantic parser that turns a sentence into a first-order logic statement, Many machine reading appro"
W14-2409,P09-1113,0,0.0122529,"is unclear how reasoning with embeddings could support the full power of symbolic representations such as first-order logic. In this proof-ofconcept paper we address this by learning embeddings that simulate the behavior of first-order logic. 1 Introduction Much of the work in machine reading follows an approach that is, at its heart, symbolic: language is transformed, possibly in a probabilistic way, into a symbolic world model such as a relational database or a knowledge base of first-order formulae. For example, a statistical relation extractor reads texts and populates relational tables (Mintz et al., 2009). Likewise, a semantic parser can turn sentences into complex first-order logic statements (Zettlemoyer and Collins, 2005). Several properties make symbolic representations of knowledge attractive as a target of machine reading. They support a range of well understood symbolic reasoning processes, capture semantic concepts such as determiners, negations 45 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 45–49, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics Evidence Logic IE Embedded Logic worksFor(A), profAt(A) profAt(B) ... SP B profAt"
W14-2409,W08-2222,0,\N,Missing
W14-2409,S13-1002,0,\N,Missing
W14-2409,W11-0112,0,\N,Missing
W15-1519,D14-1165,0,0.52052,"ly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn ∗ First two authors contributed equally to the paper. and surface form relations such as “was born in” extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations (Riedel et al., 2013; Fan et al., 2014; Chang et al., 2014). Matrix factorization is at the core of this completion: Riedel et al. (2013) convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns. Factorization of this matrix results in low-dimensional factors for entity-pairs and relations, which are able to effectively combine multiple evidence for each entity pair to predict unseen relations. An important shortcoming of this matrix factorization model for universal schema is that no information is shared between the rows that contain the same entity. This can significantly impact accuracy on pairs of e"
W15-1519,P14-1079,0,0.0171699,"troduction Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn ∗ First two authors contributed equally to the paper. and surface form relations such as “was born in” extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations (Riedel et al., 2013; Fan et al., 2014; Chang et al., 2014). Matrix factorization is at the core of this completion: Riedel et al. (2013) convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns. Factorization of this matrix results in low-dimensional factors for entity-pairs and relations, which are able to effectively combine multiple evidence for each entity pair to predict unseen relations. An important shortcoming of this matrix factorization model for universal schema is that no information is shared between the rows that contain the same entity. This can significantly impact a"
W15-1519,N13-1008,1,0.856214,"de entity types. 1 Introduction Distantly-supervised relation extraction has gained prominence as it utilizes automatically aligned data to train accurate extractors. Universal schema, in particular, has found impressive accuracy gains by (1) treating the distant-supervision as a knowledge-base (KB) containing both structured relations such as bornIn ∗ First two authors contributed equally to the paper. and surface form relations such as “was born in” extracted from text, and (2) by completing the entries in such a KB using joint and compact encoding of the dependencies between the relations (Riedel et al., 2013; Fan et al., 2014; Chang et al., 2014). Matrix factorization is at the core of this completion: Riedel et al. (2013) convert the KB into a binary matrix with entity-pairs forming the rows and relations forming the columns. Factorization of this matrix results in low-dimensional factors for entity-pairs and relations, which are able to effectively combine multiple evidence for each entity pair to predict unseen relations. An important shortcoming of this matrix factorization model for universal schema is that no information is shared between the rows that contain the same entity. This can sign"
