2021.emnlp-tutorials.2,Financial Opinion Mining,2021,-1,-1,2,1,8626,chungchi chen,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts,0,"In this tutorial, we will show where we are and where we will be to those researchers interested in this topic. We divide this tutorial into three parts, including coarse-grained financial opinion mining, fine-grained financial opinion mining, and possible research directions. This tutorial starts by introducing the components in a financial opinion proposed in our research agenda and summarizes their related studies. We also highlight the task of mining customers{'} opinions toward financial services in the FinTech industry, and compare them with usual opinions. Several potential research questions will be addressed. We hope the audiences of this tutorial will gain an overview of financial opinion mining and figure out their research directions."
2021.emnlp-main.362,Semantics-Preserved Data Augmentation for Aspect-Based Sentiment Analysis,2021,-1,-1,3,0,9451,tingwei hsu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Both the issues of data deficiencies and semantic consistency are important for data augmentation. Most of previous methods address the first issue, but ignore the second one. In the cases of aspect-based sentiment analysis, violation of the above issues may change the aspect and sentiment polarity. In this paper, we propose a semantics-preservation data augmentation approach by considering the importance of each word in a textual sequence according to the related aspects and sentiments. We then substitute the unimportant tokens with two replacement strategies without altering the aspect-level polarity. Our approach is evaluated on several publicly available sentiment analysis datasets and the real-world stock price/risk movement prediction scenarios. Experimental results show that our methodology achieves better performances in all datasets."
2021.eacl-main.122,Dynamic Graph Transformer for Implicit Tag Recognition,2021,-1,-1,3,0,10692,yiting liou,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Textual information extraction is a typical research topic in the NLP community. Several NLP tasks such as named entity recognition and relation extraction between entities have been well-studied in previous work. However, few works pay their attention to the implicit information. For example, a financial news article mentioned {``}Apple Inc.{''} may be also related to Samsung, even though Samsung is not explicitly mentioned in this article. This work presents a novel dynamic graph transformer that distills the textual information and the entity relations on the fly. Experimental results confirm the effectiveness of our approach to implicit tag recognition."
2020.semeval-1.279,{NTU}{\\_}{NLP} at {S}em{E}val-2020 Task 12: Identifying Offensive Tweets Using Hierarchical Multi-Task Learning Approach,2020,-1,-1,2,0,15380,pochun chen,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper presents our hierarchical multi-task learning (HMTL) and multi-task learning (MTL) approaches for improving the text encoder in Sub-tasks A, B, and C of Multilingual Offensive Language Identification in Social Media (SemEval-2020 Task 12). We show that using the MTL approach can greatly improve the performance of complex problems, i.e. Sub-tasks B and C. Coupled with a hierarchical approach, the performances are further improved. Overall, our best model, HMTL outperforms the baseline model by 3{\%} and 2{\%} of Macro F-score in Sub-tasks B and C of OffensEval 2020, respectively."
2020.lrec-1.76,{MPDD}: A Multi-Party Dialogue Dataset for Analysis of Emotions and Interpersonal Relationships,2020,-1,-1,2,0,16758,yiting chen,Proceedings of the 12th Language Resources and Evaluation Conference,0,"A dialogue dataset is an indispensable resource for building a dialogue system. Additional information like emotions and interpersonal relationships labeled on conversations enables the system to capture the emotion flow of the participants in the dialogue. However, there is no publicly available Chinese dialogue dataset with emotion and relation labels. In this paper, we collect the conversions from TV series scripts, and annotate emotion and interpersonal relationship labels on each utterance. This dataset contains 25,548 utterances from 4,142 dialogues. We also set up some experiments to observe the effects of the responded utterance on the current utterance, and the correlation between emotion and relation types in emotion and relation classification tasks."
2020.lrec-1.128,{C}hinese Discourse Parsing: Model and Evaluation,2020,-1,-1,3,0,16872,lin chuanan,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Chinese discourse parsing, which aims to identify the hierarchical relationships of Chinese elementary discourse units, has not yet a consistent evaluation metric. Although Parseval is commonly used, variations of evaluation differ from three aspects: micro vs. macro F1 scores, binary vs. multiway ground truth, and left-heavy vs. right-heavy binarization. In this paper, we first propose a neural network model that unifies a pre-trained transformer and CKY-like algorithm, and then compare it with the previous models with different evaluation scenarios. The experimental results show that our model outperforms the previous systems. We conclude that (1) the pre-trained context embedding provides effective solutions to deal with implicit semantics in Chinese texts, and (2) using multiway ground truth is helpful since different binarization approaches lead to significant differences in performance."
2020.lrec-1.711,{MSD}-1030: A Well-built Multi-Sense Evaluation Dataset for Sense Representation Models,2020,-1,-1,4,0,18054,tingyu yen,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Sense embedding models handle polysemy by giving each distinct meaning of a word form a separate representation. They are considered improvements over word models, and their effectiveness is usually judged with benchmarks such as semantic similarity datasets. However, most of these datasets are not designed for evaluating sense embeddings. In this research, we show that there are at least six concerns about evaluating sense embeddings with existing benchmark datasets, including the large proportions of single-sense words and the unexpected inferior performance of several multi-sense models to their single-sense counterparts. These observations call into serious question whether evaluations based on these datasets can reflect the sense model{'}s ability to capture different meanings. To address the issues, we propose the Multi-Sense Dataset (MSD-1030), which contains a high ratio of multi-sense word pairs. A series of analyses and experiments show that MSD-1030 serves as a more reliable benchmark for sense embeddings. The dataset is available at http://nlg.csie.ntu.edu.tw/nlpresource/MSD-1030/."
2020.lrec-1.749,"Issues and Perspectives from 10,000 Annotated Financial Social Media Data",2020,-1,-1,2,1,8626,chungchi chen,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we investigate the annotation of financial social media data from several angles. We present Fin-SoMe, a dataset with 10,000 labeled financial tweets annotated by experts from both the front desk and the middle desk in a bank{'}s treasury. These annotated results reveal that (1) writer-labeled market sentiment may be a misleading label; (2) writer{'}s sentiment and market sentiment of an investor may be different; (3) most financial tweets provide unfounded analysis results; and (4) almost no investors write down the gain/loss results for their positions, which would otherwise greatly facilitate detailed evaluation of their performance. Based on these results, we address various open problems and suggest possible directions for future work on financial social media data. We also provide an experiment on the key snippet extraction task to compare the performance of using a general sentiment dictionary and using the domain-specific dictionary. The results echo our findings from the experts{'} annotations."
2020.fnp-1.11,"{NTUNLPL} at {F}in{C}ausal 2020, Task 2:Improving Causality Detection Using {V}iterbi Decoder",2020,-1,-1,3,0,19326,peiwei kao,Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation,0,"In order to provide an explanation of machine learning models, causality detection attracts lots of attention in the artificial intelligence research community. In this paper, we explore the cause-effect detection in financial news and propose an approach, which combines the BIO scheme with the Viterbi decoder for addressing this challenge. Our approach is ranked the first in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our code for academic usage."
2020.coling-main.199,Heterogeneous Recycle Generation for {C}hinese Grammatical Error Correction,2020,-1,-1,2,0,21294,charles hinson,Proceedings of the 28th International Conference on Computational Linguistics,0,"Most recent works in the field of grammatical error correction (GEC) rely on neural machine translation-based models. Although these models boast impressive performance, they require a massive amount of data to properly train. Furthermore, NMT-based systems treat GEC purely as a translation task and overlook the editing aspect of it. In this work we propose a heterogeneous approach to Chinese GEC, composed of a NMT-based model, a sequence editing model, and a spell checker. Our methodology not only achieves a new state-of-the-art performance for Chinese GEC, but also does so without relying on data augmentation or GEC-specific architecture changes. We further experiment with all possible configurations of our system with respect to model composition order and number of rounds of correction. A detailed analysis of each model and their contributions to the correction process is performed by adapting the ERRANT scorer to be able to score Chinese sentences."
2020.acl-main.13,A Complete Shift-Reduce {C}hinese Discourse Parser with Robust Dynamic Oracle,2020,-1,-1,2,0,16873,shyhshiun hung,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"This work proposes a standalone, complete Chinese discourse parser for practical applications. We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies. We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition. Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance."
W19-4508,Lexicon Guided Attentive Neural Network Model for Argument Mining,2019,0,1,3,0,24132,jianfu lin,Proceedings of the 6th Workshop on Argument Mining,0,"Identification of argumentative components is an important stage of argument mining. Lexicon information is reported as one of the most frequently used features in the argument mining research. In this paper, we propose a methodology to integrate lexicon information into a neural network model by attention mechanism. We conduct experiments on the UKP dataset, which is collected from heterogeneous sources and contains several text types, e.g., microblog, Wikipedia, and news. We explore lexicons from various application scenarios such as sentiment analysis and emotion detection. We also compare the experimental results of leveraging different lexicons."
P19-1635,Numeracy-600{K}: Learning Numeracy for Detecting Exaggerated Information in Market Comments,2019,0,5,2,1,8626,chungchi chen,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we attempt to answer the question of whether neural network models can learn numeracy, which is the ability to predict the magnitude of a numeral at some specific position in a text description. A large benchmark dataset, called Numeracy-600K, is provided for the novel task. We explore several neural network models including CNN, GRU, BiGRU, CRNN, CNN-capsule, GRU-capsule, and BiGRU-capsule in the experiments. The results show that the BiGRU model gets the best micro-averaged F1 score of 80.16{\%}, and the GRU-capsule model gets the best macro-averaged F1 score of 64.71{\%}. Besides discussing the challenges through comprehensive experiments, we also present an important application scenario, i.e., detecting exaggerated information, for the task."
S18-1171,{NTU} {NLP} Lab System at {S}em{E}val-2018 Task 10: Verifying Semantic Differences by Integrating Distributional Information and Expert Knowledge,2018,0,2,2,1,5402,yowting shiue,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper presents the NTU NLP Lab system for the SemEval-2018 Capturing Discriminative Attributes task. Word embeddings, pointwise mutual information (PMI), ConceptNet edges and shortest path lengths are utilized as input features to build binary classifiers to tell whether an attribute is discriminative for a pair of concepts. Our neural network model reaches about 73{\%} F1 score on the test set and ranks the 3rd in the task. Though the attributes to deal with in this task are all visual, our models are not provided with any image data. The results indicate that visual information can be derived from textual data."
P18-2122,Disambiguating False-Alarm Hashtag Usages in Tweets for Irony Detection,2018,0,5,1,1,8627,henhsen huang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models. This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection. We analyze the ambiguity of hashtag usages and propose a novel neural network-based model, which incorporates linguistic information from different aspects, to disambiguate the usage of three hashtags that are widely used to collect the training data for irony detection. Furthermore, we apply our model to prune the self-labeled training data. Experimental results show that the irony detection model trained on the less but cleaner training instances outperforms the models trained on all data."
L18-1139,Transfer of Frames from {E}nglish {F}rame{N}et to Construct {C}hinese {F}rame{N}et: A Bilingual Corpus-Based Approach,2018,0,2,2,0,12615,tsunghan yang,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1541,Learning to Map Natural Language Statements into Knowledge Base Representations for Knowledge Base Construction,2018,0,2,2,0,30112,chinho lin,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-2016,A Unified {R}v{NN} Framework for End-to-End {C}hinese Discourse Parsing,2018,0,0,2,0,16872,lin chuanan,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"This paper demonstrates an end-to-end Chinese discourse parser. We propose a unified framework based on recursive neural network (RvNN) to jointly model the subtasks including elementary discourse unit (EDU) segmentation, tree structure construction, center labeling, and sense labeling. Experimental results show our parser achieves the state-of-the-art performance in the Chinese Discourse Treebank (CDTB) dataset. We release the source code with a pre-trained model for the NLP community. To the best of our knowledge, this is the first open source toolkit for Chinese discourse parsing. The standalone toolkit can be integrated into subsequent applications without the need of external resources such as syntactic parser."
C18-2030,A {C}hinese Writing Correction System for Learning {C}hinese as a Foreign Language,2018,0,0,2,1,5402,yowting shiue,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"We present a Chinese writing correction system for learning Chinese as a foreign language. The system takes a wrong input sentence and generates several correction suggestions. It also retrieves example Chinese sentences with English translations, helping users understand the correct usages of certain grammar patterns. This is the first available Chinese writing error correction system based on the neural machine translation framework. We discuss several design choices and show empirical results to support our decisions."
C18-1141,{G}en{S}ense: A Generalized Sense Retrofitting Model,2018,0,1,3,0,10983,yangyin lee,Proceedings of the 27th International Conference on Computational Linguistics,0,"With the aid of recently proposed word embedding algorithms, the study of semantic similarity has progressed and advanced rapidly. However, many natural language processing tasks need sense level representation. To address this issue, some researches propose sense embedding learning algorithms. In this paper, we present a generalized model from existing sense retrofitting model. The generalization takes three major components: semantic relations between the senses, the relation strength and the semantic strength. In the experiment, we show that the generalized model can outperform previous approaches in three types of experiment: semantic relatedness, contextual word similarity and semantic difference."
C18-1204,Correcting {C}hinese Word Usage Errors for Learning {C}hinese as a Second Language,2018,0,0,2,1,5402,yowting shiue,Proceedings of the 27th International Conference on Computational Linguistics,0,"With more and more people around the world learning Chinese as a second language, the need of Chinese error correction tools is increasing. In the HSK dynamic composition corpus, word usage error (WUE) is the most common error type. In this paper, we build a neural network model that considers both target erroneous token and context to generate a correction vector and compare it against a candidate vocabulary to propose suitable corrections. To deal with potential alternative corrections, the top five proposed candidates are judged by native Chinese speakers. For more than 91{\%} of the cases, our system can propose at least one acceptable correction within a list of five candidates. To the best of our knowledge, this is the first research addressing general-type Chinese WUE correction. Our system can help non-native Chinese learners revise their sentences by themselves."
S17-2144,{NLG}301 at {S}em{E}val-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News,2017,0,2,2,1,8626,chungchi chen,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"Short length, multi-targets, target relation-ship, monetary expressions, and outside reference are characteristics of financial tweets. This paper proposes methods to extract target spans from a tweet and its referencing web page. Total 15 publicly available sentiment dictionaries and one sentiment dictionary constructed from training set, containing sentiment scores in binary or real numbers, are used to compute the sentiment scores of text spans. Moreover, the correlation coeffi-cients of the price return between any two stocks are learned with the price data from Bloomberg. They are used to capture the relationships between the interesting tar-get and other stocks mentioned in a tweet. The best result of our method in both sub-task are 56.68{\%} and 55.43{\%}, evaluated by evaluation method 2."
S17-2177,{NTU}-1 at {S}em{E}val-2017 Task 12: Detection and classification of temporal events in clinical data with domain adaptation,2017,0,1,2,0,32393,poyu huang,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This study proposes a system to participate in the Clinical TempEval 2017 shared task, a part of the SemEval 2017 Tasks. Domain adaptation was the main challenge this year. We took part in the supervised domain adaption where data of 591 records of colon cancer patients and 30 records of brain cancer patients from Mayo clinic were given and we are asked to analyze the records from brain cancer patients. Based on the THYME corpus released by the organizer of Clinical TempEval, we propose a framework that automatically analyzes clinical temporal events in a fine-grained level. Support vector machine (SVM) and conditional random field (CRF) were implemented in our system for different subtasks, including detecting clinical relevant events and time expression, determining their attributes, and identifying their relations with each other within the document. The results showed the capability of domain adaptation of our system."
P17-2064,Detection of {C}hinese Word Usage Errors for Non-Native {C}hinese Learners with Bidirectional {LSTM},2017,9,2,2,1,5402,yowting shiue,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Selecting appropriate words to compose a sentence is one common problem faced by non-native Chinese learners. In this paper, we propose (bidirectional) LSTM sequence labeling models and explore various features to detect word usage errors in Chinese sentences. By combining CWINDOW word embedding features and POS information, the best bidirectional LSTM model achieves accuracy 0.5138 and MRR 0.6789 on the HSK dataset. For 80.79{\%} of the test data, the model ranks the ground-truth within the top two at position level."
I17-1098,"Integrating Subject, Type, and Property Identification for Simple Question Answering over Knowledge Base",2017,0,1,2,0,32927,weichuan hsiao,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"This paper presents an approach to identify subject, type and property from knowledge base (KB) for answering simple questions. We propose new features to rank entity candidates in KB. Besides, we split a relation in KB into type and property. Each of them is modeled by a bi-directional LSTM. Experimental results show that our model achieves the state-of-the-art performance on the SimpleQuestions dataset. The hard questions in the experiments are also analyzed in detail."
L16-1164,Fine-Grained {C}hinese Discourse Relation Labelling,2016,1,0,3,0,34396,huanyuan chen,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper explores several aspects together for a fine-grained Chinese discourse analysis. We deal with the issues of ambiguous discourse markers, ambiguous marker linkings, and more than one discourse marker. A universal feature representation is proposed. The pair-once postulation, cross-discourse-unit-first rule and word-pair-marker-first rule select a set of discourse markers from ambiguous linkings. Marker-Sum feature considers total contribution of markers and Marker-Preference feature captures the probability distribution of discourse functions of a representative marker by using preference rule. The HIT Chinese discourse relation treebank (HIT-CDTB) is used to evaluate the proposed models. The 25-way classifier achieves 0.57 micro-averaged F-score."
C16-2059,{NL}2{KB}: Resolving Vocabulary Gap between Natural Language and Knowledge Base in Knowledge Base Construction and Retrieval,2016,5,0,3,0,35678,shenglun wei,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"Words to express relations in natural language (NL) statements may be different from those to represent properties in knowledge bases (KB). The vocabulary gap becomes barriers for knowledge base construction and retrieval. With the demo system called NL2KB in this paper, users can browse which properties in KB side may be mapped to for a given relational pattern in NL side. Besides, they can retrieve the sets of relational patterns in NL side for a given property in KB side. We describe how the mapping is established in detail. Although the mined patterns are used for Chinese knowledge base applications, the methodology can be extended to other languages."
C16-1085,{C}hinese Preposition Selection for Grammatical Error Diagnosis,2016,0,2,1,1,8627,henhsen huang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Misuse of Chinese prepositions is one of common word usage errors in grammatical error diagnosis. In this paper, we adopt the Chinese Gigaword corpus and HSK corpus as L1 and L2 corpora, respectively. We explore gated recurrent neural network model (GRU), and an ensemble of GRU model and maximum entropy language model (GRU-ME) to select the best preposition from 43 candidates for each test sentence. The experimental results show the advantage of the GRU models over simple RNN and n-gram models. We further analyze the effectiveness of linguistic information such as word boundary and part-of-speech tag in this task."
C16-1210,{C}hinese Tense Labelling and Causal Analysis,2016,0,0,1,1,8627,henhsen huang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This paper explores the role of tense information in Chinese causal analysis. Both tasks of causal type classification and causal directionality identification are experimented to show the significant improvement gained from tense features. To automatically extract the tense features, a Chinese tense predictor is proposed. Based on large amount of parallel data, our semi-supervised approach improves the dependency-based convolutional neural network (DCNN) models for Chinese tense labelling and thus the causal analysis."
O14-4003,Modeling Human Inference Process for Textual Entailment Recognition,2014,0,0,1,1,8627,henhsen huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 19, Number 3, September 2014",0,None
huang-etal-2014-sentence,Sentence Rephrasing for Parsing Sentences with {OOV} Words,2014,11,2,1,1,8627,henhsen huang,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper addresses the problems of out-of-vocabulary (OOV) words, named entities in particular, in dependency parsing. The OOV words, whose word forms are unknown to the learning-based parser, in a sentence may decrease the parsing performance. To deal with this problem, we propose a sentence rephrasing approach to replace each OOV word in a sentence with a popular word of the same named entity type in the training set, so that the knowledge of the word forms can be used for parsing. The highest-frequency-based rephrasing strategy and the information-retrieval-based rephrasing strategy are explored to select the word to replace, and the Chinese Treebank 6.0 (CTB6) corpus is adopted to evaluate the feasibility of the proposed sentence rephrasing strategies. Experimental results show that rephrasing some specific types of OOV words such as Corporation, Organization, and Competition increases the parsing performances. This methodology can be applied to domain adaptation to deal with OOV problems."
C14-1060,Interpretation of {C}hinese Discourse Connectives for Explicit Discourse Relation Recognition,2014,28,3,1,1,8627,henhsen huang,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper addresses the specific features of Chinese discourse connectives, including types (word-pair and single-word), linking directions (forward and backward linking), positions and ambiguous degrees, and discusses how they affect the discourse relation recognition. A semisupervised learning method is proposed to learn the probability distributions of discourse functions of connectives from a small labeled dataset and a big unlabeled dataset. The statistics learned from the dataset demonstrates some interesting linguistic phenomena such as connective synonyms sharing similar distributions, multiple discourse functions of connectives, and couple-linking elements providing strong clues for discourse relation resolution."
W13-2817,Uses of Monolingual In-Domain Corpora for Cross-Domain Adaptation with Hybrid {MT} Approaches,2013,13,3,2,0,40904,anchang hsieh,Proceedings of the Second Workshop on Hybrid Approaches to Translation,0,"Resource limitation is challenging for crossdomain adaption. This paper employs patterns identified from a monolingual in-domain corpus and patterns learned from the post-edited translation results, and translation model as well as language model learned from pseudo bilingual corpora produced by a baseline MT system. The adaptation from a government document domain to a medical record domain shows the rules mined from the monolingual in-domain corpus are useful, and the effect of using the selected pseudo bilingual corpus is significant."
W13-2309,Analyses of the Association between Discourse Relation and Sentiment Polarity with a {C}hinese Human-Annotated Corpus,2013,17,5,1,1,8627,henhsen huang,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"Discourse relation may entail sentiment information. In this work, we annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annotation, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Finally, we highlight some language phenomena and give some remarks."
P13-2079,Modeling Human Inference Process for Textual Entailment Recognition,2013,15,5,1,1,8627,henhsen huang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"To prepare an evaluation dataset for textual entailment (TE) recognition, human annotators label rich linguistic phenomena on text and hypothesis expressions. These phenomena illustrate implicit human inference process to determine the relations of given text-hypothesis pairs. This paper aims at understanding what human think in TE recognition process and modeling their thinking process to deal with this problem. At first, we analyze a labelled RTE-5 test set which has been annotated with 39 linguistic phenomena of 5 aspects by Mark Sammons et al., and find that the negative entailment phenomena are very effective features for TE recognition. Then, a rule-based method and a machine learning method are proposed to extract this kind of phenomena from text-hypothesis pairs automatically. Though the systems with the machine-extracted knowledge cannot be comparable to the systems with human-labelled knowledge, they provide a new direction to think TE problems. We further annotate the negative entailment phenomena on Chinese text-hypothesis pairs in NTCIR-9 RITE-1 task, and conclude the same findings as that on the English RTE-5 datasets."
W12-1636,Contingency and Comparison Relation Labeling and Structure Prediction in {C}hinese Sentences,2012,10,10,1,1,8627,henhsen huang,Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Unlike in English, the sentence boundaries in Chinese are fuzzy and not well-defined. As a result, Chinese sentences tend to be long and consist of complex discourse relations. In this paper, we focus on two important relations, Contingency and Comparison, which occur often inside a sentence. We construct a moderate-sized corpus for the investigation of intra-sentential relations and propose models to label the relation structure. A learning based model is evaluated with various features. Experimental results show our model achieves accuracies of 81.63% in the task of relation labeling and 74.8% in the task of relation structure prediction."
C12-3028,An Annotation System for Development of {C}hinese Discourse Corpus,2012,7,6,1,1,8627,henhsen huang,Proceedings of {COLING} 2012: Demonstration Papers,0,"Well-annotated discourse corpora facilitate the discourse researches. Unlike English, the Chinese discourse corpus is not widely available yet. In this paper, we present a webbased annotation system to develop a Chinese discourse corpus with much finer annotation. We first review our previous corpora from the practical point of view, then propose a flexible annotation framework, and finally demonstrate the web-based annotation system. Under the proposed annotation scheme, both the explicit and the implicit discourse relations occurring on various linguistic levels will be captured and labelled with three-level PDTB tags. Besides, the sentiment information of each instance is also annotated for advanced study."
C12-1034,A Simplification-Translation-Restoration Framework for Cross-Domain {SMT} Applications,2012,28,11,2,1,43741,hanbin chen,Proceedings of {COLING} 2012,0,"Integration of domain specific knowledge into a general purpose statistical machine translation (SMT) system poses challenges due to insufficient bilingual corpora. In this paper we propose a simplification-translation-restoration (STR) framework for domain adaptation in SMT by simplifying domain specific segments of a text. For an in-domain text, we identify the critical segments and modify them to alleviate the data sparseness problem in the out-domain SMT system. After we receive the translation result, these critical segments are then restored according to the provided in-domain knowledge. We conduct experiments on an English-toChinese translation task in the medical domain and evaluate each step of the STR framework. The translation results show significant improvement of our approach over the out-domain and the naive in-domain SMT systems."
R11-1021,Pause and Stop Labeling for {C}hinese Sentence Boundary Detection,2011,14,3,1,1,8627,henhsen huang,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"The fuzziness of Chinese sentence boundary makes discourse analysis more challenging. Moreover, many articles posted on the Internet are even lack of punctuation marks. In this paper, we collect documents written by masters as a reference corpus and propose a model to label the punctuation marks for the given text. Conditional random field (CRF) models trained with the corpus determine the correct delimiter (a comma or a full-stop) between each pair of successive clauses. Different tagging schemes and various features from different linguistic levels are explored. The results show that our segmenter achieves an accuracy of 77.48% for plain text, which is close to the human performance 81.18%. For the rich formatted text, our segmenter achieves an even better accuracy of 82.93%."
I11-1170,{C}hinese Discourse Relation Recognition,2011,6,23,1,1,8627,henhsen huang,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"The challenging issues of discourse relation recognition in Chinese are addressed. Due to the lack of Chinese discourse corpora, we construct a moderate corpus with humanannotated discourse relations. Based on the corpus, a statistical classifier is proposed, and various features are explored in the experiments. The experimental results show that our method achieves an accuracy of 88.28% and an F-Score of 63.69% in four-class classification and achieves an F-Score of 93.57% in the best case."
2011.mtsummit-papers.31,Identification and Translation of Significant Patterns for Cross-Domain {SMT} Applications,2011,-1,-1,2,1,43741,hanbin chen,Proceedings of Machine Translation Summit XIII: Papers,0,None
W10-4103,Classical {C}hinese Sentence Segmentation,2010,9,4,1,1,8627,henhsen huang,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
