2020.coling-main.163,2020.lrec-1.223,0,0.0109328,"age.com 1810 Corpus M-index Normalized I-index Burstiness DTA 0.0008 0.6827 0.2320 LinCE (SPA-ENG) 0.9633 0.0489 -0.0709 (NEP-ENG) 0.9668 0.1449 -0.0780 (HIN-ENG) 0.6243 0.0637 -0.0068 (MSA-EA) 0.4396 0.0051 -0.0967 Table 2: Metrics reveal the divergence of the Deutsches Textarchiv from informal corpora. to label the “Antiqua” text spans with their corresponding languages. To eliminate errors made by the API, we then perform manual correction on all the labelled spans. We easily identify spans of embedded Greek by locating Greek UTF-8 characters. For comparison, we also use the LinCE corpora (Aguilar et al., 2020) to characterize differences between formal and informal code-switched text. LinCE combines Twitter and Facebook data from ten corpora, and the language in these corpora is more informal. The corpora cover four different codeswitched language pairs: Spanish-English, Nepali-English, Hindi-English, and Modern Standard ArabicEgyptian Arabic. Overall, LinCE contains 64,326 posts with 953,813 tokens. Although we realize that the two corpora differ in many other dimensions, such as cultural context, topic, text length, language pairs and so on, we believe that formality is a crucial factor that need"
2020.coling-main.163,W19-2515,0,0.0270476,"Missing"
2020.coling-main.163,W18-3204,0,0.0227825,"Missing"
2020.coling-main.163,W16-2105,0,0.0163148,"ifically, we are interested in: 1) how “unequal” the distribution of different languages is in a corpus; 2) how frequently the switching occurs; and 3) whether the switching happens in a periodic or aperiodic manner. We want to obtain metrics to answer the questions above quantitatively, and we employ these metrics to get a sense of how code-switching characteristics differ among corpora. Besides this descriptive task, we are interested in practical tasks for predicting code-switching. There has been previous work formalizing code-switching detection in historical texts as a language ID task (Schulz and Keller, 2016; Sprugnoli et al., 2017), and models such as Conditional Random Fields (CRF) have been deployed to classify words as in one language or another. However, such approaches fail to work in the following scenario: when large collections of page images are transcribed with optical character recognition (OCR) or when large audio collections are transcribed by speech recognition, we do not always know a priori which languages will be included. Including hypotheses from multiple languages in a transcription model can reduce accuracy in the majority matrix language. Including a model trained only on t"
2020.coling-main.163,W18-2405,0,0.0134882,"a focus on the social context in conversations where code-switching happens. For example, Blom and Gumperz (1972) proposes the dichotomy of situational and metaphorical code-switching, which serve as indicators of whether different languages or language varieties are used in different social situations. In natural language processing, there is extensive work studying code-switching from an engineering perspective. Different NLP tasks have been proposed on code-switching corpora, such as language ID (Solorio et al., 2014; Sequiera et al., 2015), named entity recognition (Aguilar et al., 2018; Singh et al., 2018) , POS tagging (Solorio and Liu, 2008; Vyas et al., 2014), sentiment analysis (Vilares et al., 2015), automatic speech recognition (Chan et al., 2014; Weiner et al., 2012), question answering (Chandu et al., 2018), etc. Indeed, with the abundance of code-switched speech and (informal) text data, there are many choices of NLP directions that one can pursue. Code-switching, as a language phenomenon, is usually considered informal. It is often found in speech and in casual text, such as social media (Sitaram et al., 2019); however, code-switching also appears in formal settings, such as newspaper"
2020.coling-main.163,D08-1110,0,0.0275843,"onversations where code-switching happens. For example, Blom and Gumperz (1972) proposes the dichotomy of situational and metaphorical code-switching, which serve as indicators of whether different languages or language varieties are used in different social situations. In natural language processing, there is extensive work studying code-switching from an engineering perspective. Different NLP tasks have been proposed on code-switching corpora, such as language ID (Solorio et al., 2014; Sequiera et al., 2015), named entity recognition (Aguilar et al., 2018; Singh et al., 2018) , POS tagging (Solorio and Liu, 2008; Vyas et al., 2014), sentiment analysis (Vilares et al., 2015), automatic speech recognition (Chan et al., 2014; Weiner et al., 2012), question answering (Chandu et al., 2018), etc. Indeed, with the abundance of code-switched speech and (informal) text data, there are many choices of NLP directions that one can pursue. Code-switching, as a language phenomenon, is usually considered informal. It is often found in speech and in casual text, such as social media (Sitaram et al., 2019); however, code-switching also appears in formal settings, such as newspaper reports, or in this paper, books. Ta"
2020.coling-main.163,W14-3907,0,0.0246006,"ntactic constraints on language alternation (Nilep, 2006). In sociolinguistics, there has been a focus on the social context in conversations where code-switching happens. For example, Blom and Gumperz (1972) proposes the dichotomy of situational and metaphorical code-switching, which serve as indicators of whether different languages or language varieties are used in different social situations. In natural language processing, there is extensive work studying code-switching from an engineering perspective. Different NLP tasks have been proposed on code-switching corpora, such as language ID (Solorio et al., 2014; Sequiera et al., 2015), named entity recognition (Aguilar et al., 2018; Singh et al., 2018) , POS tagging (Solorio and Liu, 2008; Vyas et al., 2014), sentiment analysis (Vilares et al., 2015), automatic speech recognition (Chan et al., 2014; Weiner et al., 2012), question answering (Chandu et al., 2018), etc. Indeed, with the abundance of code-switched speech and (informal) text data, there are many choices of NLP directions that one can pursue. Code-switching, as a language phenomenon, is usually considered informal. It is often found in speech and in casual text, such as social media (Sita"
2020.coling-main.163,W15-2902,0,0.0333531,"Missing"
2020.coling-main.163,D14-1105,0,0.0215991,"-switching happens. For example, Blom and Gumperz (1972) proposes the dichotomy of situational and metaphorical code-switching, which serve as indicators of whether different languages or language varieties are used in different social situations. In natural language processing, there is extensive work studying code-switching from an engineering perspective. Different NLP tasks have been proposed on code-switching corpora, such as language ID (Solorio et al., 2014; Sequiera et al., 2015), named entity recognition (Aguilar et al., 2018; Singh et al., 2018) , POS tagging (Solorio and Liu, 2008; Vyas et al., 2014), sentiment analysis (Vilares et al., 2015), automatic speech recognition (Chan et al., 2014; Weiner et al., 2012), question answering (Chandu et al., 2018), etc. Indeed, with the abundance of code-switched speech and (informal) text data, there are many choices of NLP directions that one can pursue. Code-switching, as a language phenomenon, is usually considered informal. It is often found in speech and in casual text, such as social media (Sitaram et al., 2019); however, code-switching also appears in formal settings, such as newspaper reports, or in this paper, books. Table 1 shows examples"
2020.lrec-1.473,pasha-etal-2014-madamira,0,0.103775,"Missing"
2020.lrec-1.473,E09-2008,0,0.0243723,"e describe the constructs in generational language rather than decomposition language. Another advantage of finite state automata is that FST and FSA corresponds to regular expressions that closely match Arabic concatenative and templatic morphological rules. This obliviates the need for higher level formal grammars such as context free grammars, that are not bidirectional. Algorithms exist for the compilation of rules into automata (Kaplan and Kay, 1994). Computationally, parsers and compilers for regular expressions are O(n) where n is the length of the input string. We decided to use Foma (Hulden, 2009) for this project since it is non-probabilistic and opensource. Other tools used to compile rules into automata (Bird et al., 2009) are: Lex, Flex, xfst (Beesley and Karttunen, 2003) from Xerox; HFST (Lind´en et al., 2013); and OpenFST (Allauzen et al., 2007). 2.1. Morphological Automata The morphological automata, serving as an acceptor, synthesizer and analyzer, has the same architecture for both diacritized and undiacritized words. The diacritized version has diacritized morphemes and the undiacritized version has undiacritized morphemes. The morphemes and allowable combinations are derived"
2020.lrec-1.473,J94-3001,0,0.745771,"generation rules without a need to write the more complex analysis rules, that are exercised automatically due to the bidirectional nature of FST. Consequently, we describe the constructs in generational language rather than decomposition language. Another advantage of finite state automata is that FST and FSA corresponds to regular expressions that closely match Arabic concatenative and templatic morphological rules. This obliviates the need for higher level formal grammars such as context free grammars, that are not bidirectional. Algorithms exist for the compilation of rules into automata (Kaplan and Kay, 1994). Computationally, parsers and compilers for regular expressions are O(n) where n is the length of the input string. We decided to use Foma (Hulden, 2009) for this project since it is non-probabilistic and opensource. Other tools used to compile rules into automata (Bird et al., 2009) are: Lex, Flex, xfst (Beesley and Karttunen, 2003) from Xerox; HFST (Lind´en et al., 2013); and OpenFST (Allauzen et al., 2007). 2.1. Morphological Automata The morphological automata, serving as an acceptor, synthesizer and analyzer, has the same architecture for both diacritized and undiacritized words. The dia"
2020.lrec-1.473,J97-2003,0,0.10543,"gories is not compared because UD PADT, MADAMIRA, or other systems or resources 2 https://github.com/ UniversalDependencies/UD_Arabic-PADT 3 https://camel.abudhabi.nyu.edu/madamira/ lack these finer level categorizations. Tables 1, 2, 3, 4, 5, 6, and 7 summarize the results. 2. Architecture Our goal is to design a morphological system (morphologizer) that acts as a synthesizer, analyzer, and diacritizer solely based on Arabic language rules that avoids very large listings of stems and training on data. Towards achieving this goal, our morphologizer is built using Finite State Machines (FSMs) (Mohri, 1997; Sipser, 2005) . FSMs are either finite state automata (FSA) that are acceptors of strings that we build to define sets of characters or finite state transducers (FSTs) that convert an input string into an output string using contextual or non-contextual replacement, insertion, or deletion. FSA and FSTs are defined using regular expressions, and are closed under operations such as concatenation and union. FST is bidirectional and hence input and output can be inverted for the same FST, thereby constructing a unified synthesizer and analyzer. An FST allows writing of generation rules without a"
2020.wanlp-1.12,W19-5605,0,0.208943,". 1 Introduction In classical Arabic texts, lists of the names of authorities that transmitted a piece of information (isnads) are often attached to a statement or report (the matn) to confirm its reliability. The study of isnads is an integral part of the study of hadith and the history of the Arabic written tradition in general. With the increasing availability of digitized texts, new methods are required to automatically locate and analyze isnads at scale in a wider variety of text than the canonical hadith collections used in smaller-scale studies (Harrag et al, 2014; Maraoui et al, 2019, Altammami et al, 2019). Isnads are often seamlessly integrated into running text, and are therefore difficult to distinguish from the surrounding text based on visual layout or punctuation information alone. The textual content of the isnad itself must therefore be used to determine its location. For instance, in the example hadith below, the names and transmissive terms indicate that the underlined section in the beginning of the text is the isnad, while the remainder is the matn.  قال: أن النبي صلى هللا عليه وسلم، عن الحسن عن سمرة، عن قتادة، حدثنا هشام:حدثنا أبو داود قال من قتل عبده قتلناه ومن جدعه ج"
2020.wanlp-1.12,N16-1030,0,0.0768948,"Missing"
2020.wanlp-1.12,D14-1162,0,0.0795585,"o isnad and background sections using a Viterbi-like inference method rather than making decisions at the level of individual tokens. However, such a model’s inability to understand context would result in a model with no ability to determine when common words in the embedded genre should and should not be treated as part of one, giving many false positives as single words common in the embedded genre would often be labeled as part of one, regardless of their context. We also train CRFs using 200-dimensional GloVe embeddings trained on the complete OpenITI corpus using the default parameters (Pennington et al, 2014) as features, concatenating the embeddings of the words within a window around the token to be tagged and using that as input to the model. Since different authors have different styles of citation, using different terms to indicate transmission, and draw from different sources, thus including different names in their chains, the unseen texts in the test set will contain chains that have a distinct vocabulary from those seen by the model in training. For the surface level token features described above, such mismatches in vocabulary are tokens that the model has no ability to understand the me"
2020.wanlp-1.12,D11-1141,0,0.0885683,"Missing"
2020.wanlp-1.12,W03-0419,0,0.0502192,"Missing"
2021.eacl-main.195,N16-1134,0,0.0258869,"d analyzing what makes specific source content popular, i.e. how many times a given source is quoted, cited, retweeted, etc. The source documents analyzed in prior work span a wide range of domains, from political speeches and debates (Tan et al., 2018; Niculae et al., 2015) to books (Bendersky and Smith, 2012), movie scripts (Danescu-Niculescu-Mizil et al., 2012), scientific articles (Guerini et al., 2012; Yogatama et al., 2011), tweets (Hong et al., 2011; Tan et al., 2014), and news articles (Bandari et al., 2012). The aligned derived documents also span multiple domains, from social media (Booten and Hearst, 2016; Danescu-Niculescu-Mizil et al., 2012; Bendersky and Smith, 2012; Bandari et al., 2012; Hong et al., 2011; Tan et al., 2014) to news articles (Tan et al., 2018; Niculae et al., 2015), and scientific papers (Guerini et al., 2012; Yogatama et al., 2011). Prior work has focused on both 1) predicting the popularity of an entire source document, e.g. a scientific article’s citation count (Yogatama et al., 2011) and 2) similar to our work, identifying which specific passages in a given source work will receive the most attention (Tan et al., 2018; DanescuNiculescu-Mizil et al., 2012; Bendersky and"
2021.eacl-main.195,D19-1383,0,0.024226,"ork and a single label for the pair based on which passage is quoted more. Each passage is fed into RoBERTA or XLMR separately and follows the standard finetuning setup as described for RoBERTAsingle . BERT-based BERT-based models (Devlin et al., 2019) have recently achieved state-of-the-art performance on multiple sentence classification tasks, including single sentence classification, e.g. sentiment analysis, and classification of a sequence of sentences into their corresponding categories, e.g. extractive summarization (Liu and Lapata, 2019) and scientific abstract sentence classification (Cohan et al., 2019). We work with two BERT-based models, RoBERTA (Liu et al., 2019) and XLM-RoBERTA (XLM-R) (Conneau et al., 2019). We fine-tune RoBERTA on our English datasets and XLM-R on the Latin dataset. We benchmark both models as both single passage and sequential passage predictors: • RoBERTAsingle / XLM-Rsingle : we finetune RoBERTA and XLM-R on individual passages. Thus, each training example contains a single passage and its quote count label. We follow the standard fine-tuning setup, using the final hidden state of the [CLS] token as the aggregate representation for a passage and feeding it into an o"
2021.eacl-main.195,2020.acl-main.747,0,0.0368026,"Missing"
2021.eacl-main.195,D19-1387,0,0.0264437,"Each training example consists of two passages sampled from the same work and a single label for the pair based on which passage is quoted more. Each passage is fed into RoBERTA or XLMR separately and follows the standard finetuning setup as described for RoBERTAsingle . BERT-based BERT-based models (Devlin et al., 2019) have recently achieved state-of-the-art performance on multiple sentence classification tasks, including single sentence classification, e.g. sentiment analysis, and classification of a sequence of sentences into their corresponding categories, e.g. extractive summarization (Liu and Lapata, 2019) and scientific abstract sentence classification (Cohan et al., 2019). We work with two BERT-based models, RoBERTA (Liu et al., 2019) and XLM-RoBERTA (XLM-R) (Conneau et al., 2019). We fine-tune RoBERTA on our English datasets and XLM-R on the Latin dataset. We benchmark both models as both single passage and sequential passage predictors: • RoBERTAsingle / XLM-Rsingle : we finetune RoBERTA and XLM-R on individual passages. Thus, each training example contains a single passage and its quote count label. We follow the standard fine-tuning setup, using the final hidden state of the [CLS] token a"
2021.eacl-main.195,2021.ccl-1.108,0,0.0887637,"Missing"
2021.eacl-main.195,2020.acl-demos.14,0,0.02637,"n all five datasets, no single model consistently outperforms the others in NDCG, with PR, SVMrank , and λMART each achieving the highest scores on different datasets. However, only one feature-based model, λMART, ever outperforms the neural models, achieving the highest NDCG scores on ABL-JA. We hypothesize that λMART’s strong performance on ABL-JA might be due, in part, to differences in the accuracy of our feature-extraction pipeline. Many of our features depend on accurate parsing (e.g. POS and verb tense counts). However, for our English datasets, the Stanza Universal Dependencies model (Qi et al., 2020) we use to process each sentence is trained on web-media data (UD English EWT). Thus, our English datasets (KJB, ABL, SHAK) are all out-of-domain. We hypothesize that Stanza is more accurate on ABL since it contains the most modern language similar to its training data. With these higher quality inputs, therefore, feature-based models can achieve higher performance, relative to the neural models. As one might note, our datasets are also out-of-domain for RoBERTa and XLM-R. However, as shown by Han and Eisenstein (2019), BERT-based models can adapt to new domains when provided with in-domain fi"
2021.eacl-main.195,W14-2707,1,0.865713,"Missing"
2021.eacl-main.195,D11-1055,0,0.0511313,"Missing"
2021.eacl-main.201,2020.acl-main.398,0,0.0723277,"Missing"
2021.eacl-main.201,P17-1167,0,0.050777,"Missing"
2021.eacl-main.201,D19-6609,0,0.0451332,"Missing"
2021.eacl-main.201,2020.acl-main.708,0,0.0196148,"ical information, and add two classification layers to predict aggregation functions and corresponding table cells to generate an answer for a given question. Much current table-to-text generation work focus on generating biographies from Wikipedia infoboxes (Lebret et al., 2016; Liu et al., 2018; Sha et al., 2018; Bao et al., 2018) or summarizing basketball games (Wiseman et al., 2017; Puduppully et al., 2019) according to the box- and line-score tables. Parikh et al. (2020) release a dataset for controlled table-to-text generation, where table cells are highlighted for the target sentences. Chen et al. (2020) propose a new natural language generation task, where the model tasked with generating statements that entailed by a given table. Chen et al. (2019) introduce a dataset for fact verification given tabular data as evidence and propose a BERT-based Table-BERT model and a latent program algorithm (LPA) model for this task. Zhong et al. (2020) propose to first drive the program from the table and statement, and then learn graph-enhanced contextual representations for both the table tokens and the program to classify the statements. After this paper was submitted, four related papers were publishe"
2021.eacl-main.201,2020.findings-emnlp.27,0,0.124097,"2020) propose to first drive the program from the table and statement, and then learn graph-enhanced contextual representations for both the table tokens and the program to classify the statements. After this paper was submitted, four related papers were published, which explore additional questions in table-based fact verification. Zhang et al. (2020) propose to utilize masking in the self attention layer to model table structure. Shi et al. (2020) and Yang et al. (2020) explore how to effectively combine both linguistic information and symbolic information for table-based fact verification. Eisenschlos et al. (2020) generate synthetic datasets to pre-train a TAPAS model to better understand tables for downstream tasks such as table-based fact verification and question answering. 3 Methods We describe adapting the TAPAS model to fact verification over tables and then introduce different pre-trained models on which we fine-tune the TAPAS for the table verification task. TAPAS for table-based fact verification Similar to the Table-BERT model, the TAPAS model also flattens the input table into a sequence of tokens. It concatenates all the tokenized table cells in a row and then concatenates all the row seque"
2021.eacl-main.201,D16-1128,0,0.0471391,"Missing"
2021.eacl-main.201,2020.coling-main.466,0,0.360004,"fact verification given tabular data as evidence and propose a BERT-based Table-BERT model and a latent program algorithm (LPA) model for this task. Zhong et al. (2020) propose to first drive the program from the table and statement, and then learn graph-enhanced contextual representations for both the table tokens and the program to classify the statements. After this paper was submitted, four related papers were published, which explore additional questions in table-based fact verification. Zhang et al. (2020) propose to utilize masking in the self attention layer to model table structure. Shi et al. (2020) and Yang et al. (2020) explore how to effectively combine both linguistic information and symbolic information for table-based fact verification. Eisenschlos et al. (2020) generate synthetic datasets to pre-train a TAPAS model to better understand tables for downstream tasks such as table-based fact verification and question answering. 3 Methods We describe adapting the TAPAS model to fact verification over tables and then introduce different pre-trained models on which we fine-tune the TAPAS for the table verification task. TAPAS for table-based fact verification Similar to the Table-BERT mo"
2021.eacl-main.201,N18-1074,0,0.159932,"Missing"
2021.eacl-main.201,D17-1239,0,0.0176647,"to paraphrases, and rank them according to their similarity to the original question to generate the answer for the questions. Herzig et al. (2020) add additional embedding layers to a BERT model to capture table structure and numerical information, and add two classification layers to predict aggregation functions and corresponding table cells to generate an answer for a given question. Much current table-to-text generation work focus on generating biographies from Wikipedia infoboxes (Lebret et al., 2016; Liu et al., 2018; Sha et al., 2018; Bao et al., 2018) or summarizing basketball games (Wiseman et al., 2017; Puduppully et al., 2019) according to the box- and line-score tables. Parikh et al. (2020) release a dataset for controlled table-to-text generation, where table cells are highlighted for the target sentences. Chen et al. (2020) propose a new natural language generation task, where the model tasked with generating statements that entailed by a given table. Chen et al. (2019) introduce a dataset for fact verification given tabular data as evidence and propose a BERT-based Table-BERT model and a latent program algorithm (LPA) model for this task. Zhong et al. (2020) propose to first drive the"
2021.eacl-main.201,2020.emnlp-main.628,0,0.216597,"en tabular data as evidence and propose a BERT-based Table-BERT model and a latent program algorithm (LPA) model for this task. Zhong et al. (2020) propose to first drive the program from the table and statement, and then learn graph-enhanced contextual representations for both the table tokens and the program to classify the statements. After this paper was submitted, four related papers were published, which explore additional questions in table-based fact verification. Zhang et al. (2020) propose to utilize masking in the self attention layer to model table structure. Shi et al. (2020) and Yang et al. (2020) explore how to effectively combine both linguistic information and symbolic information for table-based fact verification. Eisenschlos et al. (2020) generate synthetic datasets to pre-train a TAPAS model to better understand tables for downstream tasks such as table-based fact verification and question answering. 3 Methods We describe adapting the TAPAS model to fact verification over tables and then introduce different pre-trained models on which we fine-tune the TAPAS for the table verification task. TAPAS for table-based fact verification Similar to the Table-BERT model, the TAPAS model al"
2021.eacl-main.201,2020.emnlp-main.126,0,0.266643,"Missing"
2021.eacl-main.201,2020.acl-main.539,0,0.181361,"y Mccray”, we need first to find in which order each was picked and subtract them. Table representation learning is important for utilizing table data as evidence for fact verification. Most existing methods apply BERT (Devlin et al., 2018) model to learn table representations. Table-BERT (Chen et al., 2019) uses simple templates to transform tables into “somewhat natural” sentences, and fine-tunes BERT on pairs of statement sentence and corresponding table “sentence”. However, this model adds many extra tokens to the tables, sometimes doubling the length of the original table token sequence. Zhong et al. (2020) propose to first derive logical forms from the table and the statement. A heterogeneous graph is then constructed to capture connections between table cells, functions and arguments and statement tokens. A graph-enhanced contextual representation is learned for each token by only paying attention to the neighbor nodes in the graph when applying BERT model. However, as we show in §4.2, BERT is less effective when it is pre-trained on unstructured data but applied to structured data such as tables. Table-based fact verification also requires numerical reasoning over table records. Chen et al. ("
2021.scil-1.5,C18-1135,0,0.0309187,"Missing"
2021.scil-1.5,P14-5010,0,0.00254547,"; Biber and Finegan, 1989): the passive, the progressive, modals, semimodals, and pied-piping in relative clauses. We use the modals (will, would, can, could, may, might, should, shall, must, ought, need) found in Leech et al. (2009), p. 72. While semi-modals have no consistent definition, we use the “emergent modals” listed in Leech et al. (2009), p. 98: have got to, be going to, want to, have to, need to (which are also the top five most frequent semi-modals listed on p. 100 of the same source). Examples of these features are shown in Table 1. 3.1 We process the data using Stanford CoreNLP (Manning et al., 2014) to obtain labeled dependency parses, part-of-speech tags, and lemmas for all tokens. We then identify the relevant linguistic features as follows. Passives: We classify passive verb phrases as those with dependency labels “auxpass” and “nsubjpass”. Progressives: We find progressive constructions by looking for a POS tag beginning with “VB” that corresponds to the lemma “be” and that is followed by the POS tag “VBG”. Pied-piping: We detect pied-piping relative clauses by looking for a preposition (tagged “IN”) immediately followed by “who”, “whom”, “whose”, or “which”. Data The Canadian Hansar"
2021.scil-1.7,P10-1105,0,0.0287889,"uestions about language change has focused on a variety of tasks, the most common of which are the following: inferring linguistic phylogenies, identifying cognates, reconstructing proto-languages, tracking word senses and semantic change over time, and understanding morphological change. Establishing relationships between different languages through the reconstruction of linguistic phylogenies is the focus of several lines of work (Gray and Atkinson, 2003; Gray et al., 2009; Nakhleh et al., 2005). Other papers focus on the cognate identification task (Kondrak, 2001; Mackay and Kondrak, 2005; Hall and Klein, 2010). The reconstruction of proto-languages is another historical linguistics task for which computer scientists have proposed probabilistic methods; examples of work in this area include Bouchard-Cˆot´e et al. (2008) and Bouchard-Cˆot´e et al. (2013). In addition, there have been many attempts to detect lexical semantic change using word embeddings. Works such as Kim et al. (2014), Kulkarni et al. (2015), and Hamilton et al. (2016) use this approach to identify changes in lexical categories and word senses (e.g. “mouse” gained another meaning as computers became more widespread). Focusing on morp"
2021.scil-1.7,P16-1141,0,0.139338,"work (Gray and Atkinson, 2003; Gray et al., 2009; Nakhleh et al., 2005). Other papers focus on the cognate identification task (Kondrak, 2001; Mackay and Kondrak, 2005; Hall and Klein, 2010). The reconstruction of proto-languages is another historical linguistics task for which computer scientists have proposed probabilistic methods; examples of work in this area include Bouchard-Cˆot´e et al. (2008) and Bouchard-Cˆot´e et al. (2013). In addition, there have been many attempts to detect lexical semantic change using word embeddings. Works such as Kim et al. (2014), Kulkarni et al. (2015), and Hamilton et al. (2016) use this approach to identify changes in lexical categories and word senses (e.g. “mouse” gained another meaning as computers became more widespread). Focusing on morphological change, Lieberman et al. (2007) considers English verbs from the last 1200 years and examines the process by which these verbs have become regular. They conclude that the half-life of an irregular verb is correlated 71 Proceedings of the Society for Computation in Linguistics (SCiL) 2021, pages 71-80. Held on-line February 14-19, 2021 with its usage frequency and that “a verb that is 100 times less frequent regularizes"
2021.scil-1.7,W18-0304,1,0.645603,"Missing"
2021.scil-1.7,W14-2517,0,0.0178108,"c phylogenies is the focus of several lines of work (Gray and Atkinson, 2003; Gray et al., 2009; Nakhleh et al., 2005). Other papers focus on the cognate identification task (Kondrak, 2001; Mackay and Kondrak, 2005; Hall and Klein, 2010). The reconstruction of proto-languages is another historical linguistics task for which computer scientists have proposed probabilistic methods; examples of work in this area include Bouchard-Cˆot´e et al. (2008) and Bouchard-Cˆot´e et al. (2013). In addition, there have been many attempts to detect lexical semantic change using word embeddings. Works such as Kim et al. (2014), Kulkarni et al. (2015), and Hamilton et al. (2016) use this approach to identify changes in lexical categories and word senses (e.g. “mouse” gained another meaning as computers became more widespread). Focusing on morphological change, Lieberman et al. (2007) considers English verbs from the last 1200 years and examines the process by which these verbs have become regular. They conclude that the half-life of an irregular verb is correlated 71 Proceedings of the Society for Computation in Linguistics (SCiL) 2021, pages 71-80. Held on-line February 14-19, 2021 with its usage frequency and that"
2021.scil-1.7,W16-2015,0,0.0696639,"Missing"
2021.scil-1.7,N01-1014,0,0.0973049,"sing computational approaches to answer questions about language change has focused on a variety of tasks, the most common of which are the following: inferring linguistic phylogenies, identifying cognates, reconstructing proto-languages, tracking word senses and semantic change over time, and understanding morphological change. Establishing relationships between different languages through the reconstruction of linguistic phylogenies is the focus of several lines of work (Gray and Atkinson, 2003; Gray et al., 2009; Nakhleh et al., 2005). Other papers focus on the cognate identification task (Kondrak, 2001; Mackay and Kondrak, 2005; Hall and Klein, 2010). The reconstruction of proto-languages is another historical linguistics task for which computer scientists have proposed probabilistic methods; examples of work in this area include Bouchard-Cˆot´e et al. (2008) and Bouchard-Cˆot´e et al. (2013). In addition, there have been many attempts to detect lexical semantic change using word embeddings. Works such as Kim et al. (2014), Kulkarni et al. (2015), and Hamilton et al. (2016) use this approach to identify changes in lexical categories and word senses (e.g. “mouse” gained another meaning as co"
2021.scil-1.7,D14-1082,0,0.17639,"Missing"
2021.scil-1.7,W05-0606,0,0.114155,"nal approaches to answer questions about language change has focused on a variety of tasks, the most common of which are the following: inferring linguistic phylogenies, identifying cognates, reconstructing proto-languages, tracking word senses and semantic change over time, and understanding morphological change. Establishing relationships between different languages through the reconstruction of linguistic phylogenies is the focus of several lines of work (Gray and Atkinson, 2003; Gray et al., 2009; Nakhleh et al., 2005). Other papers focus on the cognate identification task (Kondrak, 2001; Mackay and Kondrak, 2005; Hall and Klein, 2010). The reconstruction of proto-languages is another historical linguistics task for which computer scientists have proposed probabilistic methods; examples of work in this area include Bouchard-Cˆot´e et al. (2008) and Bouchard-Cˆot´e et al. (2013). In addition, there have been many attempts to detect lexical semantic change using word embeddings. Works such as Kim et al. (2014), Kulkarni et al. (2015), and Hamilton et al. (2016) use this approach to identify changes in lexical categories and word senses (e.g. “mouse” gained another meaning as computers became more widesp"
2021.scil-1.7,P14-5010,0,0.00811078,"Missing"
2021.scil-1.7,W19-4721,0,0.01301,"edings of the Society for Computation in Linguistics (SCiL) 2021, pages 71-80. Held on-line February 14-19, 2021 with its usage frequency and that “a verb that is 100 times less frequent regularizes 10 times as fast.” Another related work in morphology is Kisselew et al. (2016), focusing on nouns that changed to become verbs as well as verbs that became nouns; their goal was predicting the form that appeared first. Although the task of automatically detecting syntactic change has not received much attention in the literature, syntactic change detection using a singlelayer LSTM is the topic of Merrill et al. (2019). Because the LSTM achieved higher accuracy for the task of predicting the year of composition of novel sentences compared to their baseline model consisting of feedforward networks, and because the latter do not take relations between words into account whereas LSTMs do, Merrill et al. (2019) conclude that syntactic change (as opposed to only lexical change) was detected. 3 parser, we create a new dobj relation whenever two verbs are coordinated and the first of the two verbs appears with a direct object, since the parser fails to identify the dobj relation that exists between the second verb"
2021.scil-1.7,N03-1033,0,0.270934,"Missing"
2021.scil-1.7,N03-1000,0,0.143534,"Missing"
2021.starsem-1.5,N12-1009,0,0.0298716,"contains the source document, split into sentences, with each sentence separated by a special [SSS] token (“source sentence start”) and ti is a single target sentence. We feed the [CLS] representation into a final layer 5 Datasets We benchmark the proposed models on four different datasets (Table 1). See Appendix A for further dataset stastics and preprocessing details. 55 5.1 PAN (Zhou et al., 2020) summarize some portion of the cited paper, which is, in turn, summarized by its abstract S. This assumption, however, may be incorrect if the citing sentence is a poor summary of the cited paper (Abu-Jbara and Radev, 2012) or it refers to content in the cited paper which is not included in the abstract. Nevertheless, this assumption allows for easy creation of large-scale, real-world LTRD datasets. This is in contrast to Pr2News, which is substantially smaller due its reliance on humanannotated val and test labels, and PAN, which uses automatic methods to generate synthetic examples. We discuss the trade-offs of using citation marks to generate LTRD datasets in §5.4. PAN contains pairs of (S, T) web documents where T has potentially plagiarized S. Positive pairs contain synthetic plagiarism, generated by method"
2021.starsem-1.5,P02-1020,0,0.708138,"ocal text reuse, where both source and target documents contain lexically or semantically similar passages, surrounded by text that is unrelated or dissimilar. Often, reused text is presented without explicit links or citations, making it hard to track information flow. While many state-of-the-art (SoTA) NLP architectures have been trained on the closely-related tasks of document- and sentence-pair similarity detection (Reimers and Gurevych, 2019) and ad-hoc retrieval (Dai and Callan, 2019), prior methods for local text-reuse detection (LTRD) are mostly limited to lexical matching (Lee, 2007; Clough et al., 2002; Leskovec et al., 2009; Wilkerson et al., 2015; Smith et al., 2014) with some dictionary expansion (Moritz et al., 2016). To our knowledge, only Zhou et al. (2020) has applied neural models to this problem, proposing hierarchical neural models that use a cross-document attention mechanism to model local similarities between two candidate documents. In this paper, we conduct a large-scale evaluation of several lexical overlap and SoTA neural models for LTRD. Among the neural models, we benchmark not only the hierarchical neural models proposed by Zhou et al. (2020), but also study the effectiv"
2021.starsem-1.5,S17-2001,0,0.0741781,"l overlap and feature-based neural models and slower pairwise neural models. 2 addition to evaluating models on the benchmark LTRD datasets introduced by Zhou et al. (2020), we conduct experiments on two more challenging datasets: ARC-Sim, a new citation localization dataset with hard negative examples, and Pr2News (MacLaughlin et al., 2020), a dataset of text reuse in science news articles with heuristically-labeled training data. Also related to our work is research studying sentence-pair problems, e.g. paraphrase detection (PD) (Dolan and Brockett, 2005), semantic textual similarity (STS) (Cer et al., 2017) and textual entailment, (Bowman et al., 2015), and documentranking problems, e.g. ad-hoc retrieval (Croft et al., 2009). In fact, it is trivial to adapt existing approaches to sentence-pair and document ranking problems to LTRD. As discussed in §3, we cast LTRD as sentence classification and ranking, identifying which sentences in a target text are lexically or semantically reused from some portion of the source. Thus, in order to adapt sentence-pair models to this task, we simply compute scores for all pairs of (source sentence, target sentence), and use some function (e.g. max) to aggregate"
2021.starsem-1.5,D19-1383,0,0.0282693,"ls: As shown by Gururangan et al. (2020), further pretraining BERT-based models on in-domain text improves performance on a variety of tasks. We explore the effects of DAPT for LTRD, testing Roberta models domain-adapted on either biomedical publications, computer science publications or news data. We fine-tune these models as above. Sequential Sequence Pair Models: Since the fine-tuned models discussed above operate on a single ti at a time, they cannot leverage information from the surrounding target context. Following the success of BERT-based models for sequential sentence classification (Cohan et al., 2019), we construct new input examples containing the full source and target documents, split into sentences: [CLS]&lt; s1 , ..., sn &gt;[SEP]&lt; t1 , ..., tn &gt;[SEP] Again, &lt; s1 , ..., sn &gt; contains the source sentences. Similarly, &lt; t1 , ..., tn &gt; contains the target sentences, with each separated by a special [TSS] token (“target sentence start”). We feed the final [TSS] representations into a multi-layer feedforward network to make a prediction for each target sentence. Each pair is labeled with all corresponding target sentence labels. Since many pairs exceed Roberta’s 512 Wordpiece length limit, we us"
2021.starsem-1.5,D19-3004,0,0.0284382,"ke the other three classes of models described above, features for these fine-tuned models cannot be precomputed. Instead, at test time, a separate forward pass is required for each (S, T) or (S, ti ) pair. Thus, though these models might achieve better performance than feature-based alternatives (Peters et al., 2019), it may be unfeasible to test them on large collections where many pairwise computations would be required. Sequence Pair Models: We fine-tune RobertaBase (Liu et al., 2019) using the standard setup for sequence-pair tasks such as PD, STS, and IR (Devlin et al., 2019; Akkalyoncu Yilmaz et al., 2019). We create an input example for each (source document S, target sentence ti ) pair: [CLS] &lt; s1 , ..., sn &gt; [SEP] ti [SEP] where &lt; s1 , ..., sn &gt; contains the source document, split into sentences, with each sentence separated by a special [SSS] token (“source sentence start”) and ti is a single target sentence. We feed the [CLS] representation into a final layer 5 Datasets We benchmark the proposed models on four different datasets (Table 1). See Appendix A for further dataset stastics and preprocessing details. 55 5.1 PAN (Zhou et al., 2020) summarize some portion of the cited paper, which i"
2021.starsem-1.5,councill-etal-2008-parscit,0,0.0687123,"Missing"
2021.starsem-1.5,bird-etal-2008-acl,0,0.0761745,"media, journalists often paraphrase or quote speeches, press releases, and interviews (Niculae et al., 2015; ∗ Equal contribution. 52 Proceedings of the 10th Conference on Lexical and Computational Semantics, pages 52–66 August 5–6, 2021, Bangkok, Thailand (online) ©2021 Association for Computational Linguistics citation localization; 2) Pr2News (MacLaughlin et al., 2020), a dataset of text reuse in news articles labeled with a mix of expert, non-expert, and heuristic annotation; 3) ARC-Sim, a new, publicly available1 citation localization dataset created using citation links in the ACL ARC (Bird et al., 2008). Our experiments address a number of previouslyunexplored questions in the study of LTRD, including 1) the impact of training on weakly-supervised data on model accuracy; 2) the effectiveness of SoTA neural models trained on “general” semantic similarity data for LTRD tasks; 3) the importance of incorporating document-level context; 4) the effects of domain-adaptive pretraining (Gururangan et al., 2020) on the accuracy of fine-tuned BERT models; and 5) the trade-offs between more efficient lexical overlap and feature-based neural models and slower pairwise neural models. 2 addition to evaluat"
2021.starsem-1.5,P04-3031,0,0.286547,"Missing"
2021.starsem-1.5,P07-1060,0,0.506972,"semantic local text reuse, where both source and target documents contain lexically or semantically similar passages, surrounded by text that is unrelated or dissimilar. Often, reused text is presented without explicit links or citations, making it hard to track information flow. While many state-of-the-art (SoTA) NLP architectures have been trained on the closely-related tasks of document- and sentence-pair similarity detection (Reimers and Gurevych, 2019) and ad-hoc retrieval (Dai and Callan, 2019), prior methods for local text-reuse detection (LTRD) are mostly limited to lexical matching (Lee, 2007; Clough et al., 2002; Leskovec et al., 2009; Wilkerson et al., 2015; Smith et al., 2014) with some dictionary expansion (Moritz et al., 2016). To our knowledge, only Zhou et al. (2020) has applied neural models to this problem, proposing hierarchical neural models that use a cross-document attention mechanism to model local similarities between two candidate documents. In this paper, we conduct a large-scale evaluation of several lexical overlap and SoTA neural models for LTRD. Among the neural models, we benchmark not only the hierarchical neural models proposed by Zhou et al. (2020), but al"
2021.starsem-1.5,D14-1162,0,0.0935149,"datasets exist yet which contain such fine-grained annotation. 54 Table 1: Dataset statistics: the total number of (source S, target T) example pairs, the average # of sentences and words in each S and T, and the average # of positively labeled T sentences in each positive (S, T) pair. For Pr2News, we report the average # of T sentences with label &gt; 0 in the human-labeled val and test sets. ti ∈ T are extracted, then ranked by their similarity to the final hidden representation of the entire source document S. GRU-HAN (deep) (Zhou et al., 2020): this model mirrors BERT-HAN, except with GloVe (Pennington et al., 2014) embeddings and a HAN with CDA at both the word and sentence level. It follows the same training and testing regime. BCL-CDA: We adapt the BCL model from MacLaughlin et al. (2020) (originally designed for the task of intrinsic source attribution on Pr2News) for LTRD by adding a final CDA layer (Zhou et al., 2020). After generating contextualized representations of each source and target sentence with BCL, a CDA layer computes an attention-weighted representation of each target sentence, weighted by its similarity to the source sentences. The CDAweighted and original target sentence representat"
2021.starsem-1.5,W04-1013,0,0.052425,"Information Retrieval (IR): distilled RobertaBASE (Sanh et al., 2019) fine-tuned on MS MARCO (Campos et al., 2016). Note, these pretrained SBERT models are not trained for LTRD. Instead, they are trained on largescale datasets for other related tasks (PD, STS, IR). These experiments thus evaluate how well off-theshelf tools generalize to a new task and domain. Just as the lexical models, we evaluate in sp and ap settings. Following Reimers and Gurevych (2019), we embed each source document, source sentence, and target sentence separately, then compute cosine similarity for each pair. • Rouge (Lin, 2004): Since authors of derived documents often paraphrase and summarize source content, we evaluate Rouge, a popular summarization evaluation metric. We evaluate Rouge-{1, 2, L}, selecting the best configuration for each dataset using validation data. We compute two versions of each metric: singlepair (sp) and all-pairs (ap). In sp, for a given document pair (S, T), we compute a score for each sentence ti ∈ T by computing its similarity to the entire S. In ap, we compute a score for each sentence ti ∈ T by computing its similarity to each sentence si ∈ S, then selecting the maximum score over all"
2021.starsem-1.5,W19-4302,0,0.0304408,"Missing"
2021.starsem-1.5,2020.acl-main.447,0,0.156479,"lly related to S and may contain some spurious lexical or semantic overlap; 2) be more indicative of real-world performance, since real users may S2ORC (Zhou et al., 2020) S2ORC is a citation localization dataset, containing (abstract S, paper section T) pairs. Citation localization consists of identifying which ti ∈ T, if any, cite the source. All citation marks are removed from the texts, so models can only make predictions by comparing the language of S and T, not just simply identify citation marks. Positive examples are created by sampling scientific papers from the broader S2ORC corpus (Lo et al., 2020), finding sections in those papers that contain citation(s) to another paper in the corpus, and pairing together the (cited source abstract S, citing section T). Negative pairs are created by pairing T with ˜ the abstract of a paper it does not cite. The D2D S, labels are 0 for negative pairs, 1 for positive. The S2D labels for ti ∈ T are 1 if ti contains a citation of S, else 0. S2D labels for negative pairs are all 0. The design of this dataset follows the assumption that the citing sentence(s) in T often paraphrase or 3 Available for download here. We sample 1 negative pair per (source abst"
2021.starsem-1.5,D15-1166,0,0.173473,"Missing"
2021.starsem-1.5,P10-1057,0,0.040933,"2 69.9 BERT – – – – – – 60.6 93.4 68.3 67.8 70.6 News-Roberta CS-Roberta 81.6 44.0 69.5 56.7 81.7 89.8 – – – – – Longformersequential 84.5 46.5 68.3 55.0 80.9 88.8 62.7 95.9 69.5 70.4 70.8 Setting and ARC-Sim. At the corpus-level, Roberta mostly makes false positives (FP) errors, while Longformer makes roughly equal FP and FN errors (and fewer errors overall). For both models, most of these FPs occur in positive (S, T) pairs, i.e. pairs where at least one ti cites S. As discussed in §5, these errors are reasonable, since T often only cites S once, even if it discusses S in multiple sentences (Qazvinian and Radev, 2010). Roberta’s more-frequent FP errors, however, do not affect its document-level scores as much. Since, at the document-level, we evaluate how well models rank the ti in each T w.r.t each other, models perform well if they score positive sentences higher than negatives (no reuse). Indeed, though Roberta predicts high scores for many negatives, it does better than Longformer at scoring positives higher, leading to better ranking performance. Next, we first perform error analysis on PAN, the only dataset where Roberta outperforms Longformer across all metrics. We find that Roberta makes few D2D er"
2021.starsem-1.5,N18-1101,0,0.0333021,"f it is possibly unrelated. In S2D, given an (S, T) pair, we aim to predict which specific sentences ti ∈ T contain reused S content. Thus, each pair has n corresponding labels, one label for each sentence ti ∈ T.2 4 or triplets of passages to learn semantically meaningful passage representations; 3) at test time, computing the similarity between two passages as the cosine similarity between their pooled representations. We evaluate three SBERTs trained for different tasks: • Semantic Textual Similarity (STS): RobertaLARGE (Liu et al., 2019) trained on SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) then fine-tuned on the STS-B (Cer et al., 2017) train set. Models We benchmark four classes of models on this task: 4.1 Lexical Overlap Models • Paraphrase Detection (PD): distilled RobertaBASE (Sanh et al., 2019) fine-tuned on a large-scale paraphrase detection corpus. We evaluate two unsupervised metrics: • TF-IDF Cosine Similarity: Simple word overlap metrics are commonly-used baselines to measure the similarity between two passages for PD (Dolan and Brockett, 2005), STS (Reimers and Gurevych, 2019), document retrieval (Croft et al., 2009), and LTRD (Tan et al., 2016; Lee, 2007; Clough et"
2021.starsem-1.5,N16-1174,0,0.0410117,"ti ∈ T. 4.2 4.3 Hierarchical Neural Models (HNM) We also benchmark three HNM. Similar to SBERT (§4.2), HNM operate on frozen embeddings (Peters et al., 2019) which are computationally efficient since they only need to be calculated once (i.e. only one BERT forward pass for each source or target sentence). Unlike SBERT, however, HNM also have task-specific model architectures that learn to contextualize and align sentences. BERT-HAN (shallow) (Zhou et al., 2020): this model mean pools frozen BERT embeddings to generate sentence representations, then uses a hierarchical attention network (HAN) (Yang et al., 2016) to add document-level context and a crossdocument attention (CDA) mechanism to align passages across documents. See Zhou et al. (2020). At training time, BERT-HAN only calculates loss at the document-pair level, i.e. D2D classification. There is no sentence-level supervision (S2D). At inference, two sets of predictions are output: 1) the D2D prediction, as during training; 2) the intermediate hidden representations of the sentences Pretrained Sentence-BERT Encoders We evaluate Sentence-BERT (SBERT) (Reimers and Gurevych, 2019), a SoTA pretrained passage encoder for semantic-relatedness tasks."
2021.starsem-1.5,2020.emnlp-main.407,0,0.0746485,"Missing"
C12-1122,P11-1048,0,0.15241,"fining an interface for inserting special purpose logic within the standard inference algorithm. Instead of representing a tree constraint externally in the structure of the model, we can encapsulate similar logic in this factor where the computation can be done more efficiently with variants of standard dynamic programming algorithms (Figure 1). Previous work has applied this technique to non-projective and projective dependency parsing (Smith and Eisner, 2008; Naradowsky et al., 2012), and a similar belief propagation approach has been used to enforce well-formed productions in CCG parsing (Auli and Lopez, 2011). More specifically, inputs to this algorithm are the span weights u(i, j). As in earlier dependency parsing work, these weights are derived from the ratio of messages coming in from the S pan variables: u(i, j) = mS pan(i, j)→CKYTree (true) mS pan(i, j)→CKYTree (false) After running this inside-outside algorithm in O(n3 ) time, we calculate the O(n2 ) outgoing messages from CKYTree: mCKYTree→S pan(i, j) (true) = g(i, j) mCKYTree→S pan(i, j) (false) = 1 − u(i, j) · g(i, j) Here g(i, j) is the gradient of the sum of the weights of all trees with respect to the input weight u(i, j). Using the fa"
C12-1122,P11-2119,0,0.0520964,"Missing"
C12-1122,J03-4003,0,0.0502916,"aints coordinate between syntactic constituents and named entity spans, providing an additional layer of flexibility on how these models interact. With this architecture we achieve the best-reported results on both CRF-based parsing and named entity recognition on sections of the OntoNotes corpus, and outperform state-of-the-art parsers on an NP-identification task, while remaining asymptotically faster than traditional grammar-based parsers. 1 Introduction Research in statistical parsing has made significant progress on recovering the kinds of annotations found in treebank-annotated corpora (Collins, 2003; Petrov et al., 2006), but in practice parsing is rarely an end in itself. Instead, parsing must be integrated with other forms of markup (part-ofspeech tagging, named-entity recognition, coreference resolution) in order to perform complete end-to-end tasks like question answering. Historically, independent pursuit of these tasks has often been accompanied by the belief that improvements on a measure intrinsic to parsing (for our domain, often F1 on the test section of the Wall Street Journal) will accurately reflect improvements on an end task when incorporated into a complete system. Nevert"
C12-1122,P08-1109,0,0.0502003,"Missing"
C12-1122,N09-1037,0,0.0960468,"ll accurately reflect improvements on an end task when incorporated into a complete system. Nevertheless, errors propagate in NLP pipelines, and the need for a consistent analysis across several layers of linguistic structure has motivated the development of joint models in which uncertainty is shared between individual model components. Previous work has applied joint inference to parsing and named-entity recognition (NER), the task pursued in this paper, by augmenting the grammar with specialized non-terminals which couple syntactic and NER labels to represent an analysis over both domains (Finkel and Manning, 2009). This coupling proved to be beneficial to both tasks, increasing parsing performance by 0.47 F1 and NER performance by 3.27 F1 on average over the independently-trained models. Proceedings of COLING 2012: Technical Papers, pages 1995–2010, COLING 2012, Mumbai, December 2012. 1995 Yet there are many reasons one might be concerned with this approach. First, it is a problem-specific formulation of a joint problem, and not necessarily extensible to problems that require a looser or more flexible coupling between the two problem domains. For instance, the presence of an NER span indicates the pres"
C12-1122,N06-2015,0,0.0281113,"first demonstrate that parsers based on this architecture outperform previous CRF parsers, and that they provide both an asymptotic advantage over state-of-the-art parsers in decoding speed and an attractive compromise between speed and performance. We then show that the semi-CRF NER model is a comparable baseline to previous standalone models, and that it improves upon previous results when trained jointly. 5.1 Data We primarily evaluate all of our model configurations on the same data set: a selection of six corpora drawn from the English broadcast news section of the OntoNotes 2.0 dataset (Hovy et al., 2006). The data are partitioned to achieve an approximately 3:1 ratio between training and test sets. This is an exact reproduction of the partitioning found in (Finkel and Manning, 2009), where detailed corpus statistics may be found. As in that work, we remove empty leaf nodes, coarsen nonterminal labels (NP, not NP-PRD), and filter out sentences longer than 40 words. In supplementary parsing experiments we make us of the OntoNotes Wall Street Journal Corpus distribution, using the standard train/test split and sentences with between 3 and 40 words. 5.2 Features For each of the models presented h"
C12-1122,P02-1017,0,0.145519,"), C on jC ont ains(i, j, w)} NN IN WP VBD RB |[DT chapter of what was once |[a LEN-3, START-13, END-16 OUTER-RB-IN, INNER-DT-VBG-NN WORD-shining, TAG-VBG, WORD-POS-shining-VBG, CAP-FALSE SPOS0-EPOS0-DT-NN, SPOS+1-EWORD0-VBG-symbol, BG-EW-VBD-RB-DT-VBG-NN-symbol, ... CONTAINS-POS-VBG, CONTAINS-WORD-shining TAGSET- DT-VBG-NN, CONTAINS-WORD-SHINING, ... NN VBG NN]| IN DT future. shining symbol]| of the Table 1: Common feature templates. Instantiations of these templates on the 13-16 span of the example sentence are also provided. CCM refers to the features used by the constituent-context model (Klein and Manning, 2002). joint models. In this section we aim to demonstrate that these benefits are not merely conceptual or aesthetic in nature, but translate to practical performance improvements in the model. In the following sections we first demonstrate that parsers based on this architecture outperform previous CRF parsers, and that they provide both an asymptotic advantage over state-of-the-art parsers in decoding speed and an attractive compromise between speed and performance. We then show that the semi-CRF NER model is a comparable baseline to previous standalone models, and that it improves upon previous"
C12-1122,P03-1054,0,0.076134,"Missing"
C12-1122,J93-2004,0,0.0403375,"Missing"
C12-1122,H05-1066,0,0.043785,"rminal labels (NP, not NP-PRD), and filter out sentences longer than 40 words. In supplementary parsing experiments we make us of the OntoNotes Wall Street Journal Corpus distribution, using the standard train/test split and sentences with between 3 and 40 words. 5.2 Features For each of the models presented here, every boolean variable mentioned has a corresponding unary factor representing its likelihood of being true. This leads to a wide-variety of features, depending on the variable’s semantics. For parsing we rely on features comparable to those used in edge-factored dependency parsing (McDonald et al., 2005), consisting of combinations of unigram features (word, part-of-speech, capitalization, and presence of punctuation) between the tokens at or near the span boundaries, including tokens immediately outside and inside of the span. This allows us to capture very strong lexical indications of a span, such as the presence of a comma immediately outside the start and end of the indices. We also look for the occurrence of particular tags anywhere within the span, which might signify that a constituent should not span those indices unless it is sufficiently large. A previous generative model of span-b"
C12-1122,D12-1074,1,0.84054,"ages from such combinatorial factors to a variable could be computed from the factor’s posterior beliefs about that variable, thus defining an interface for inserting special purpose logic within the standard inference algorithm. Instead of representing a tree constraint externally in the structure of the model, we can encapsulate similar logic in this factor where the computation can be done more efficiently with variants of standard dynamic programming algorithms (Figure 1). Previous work has applied this technique to non-projective and projective dependency parsing (Smith and Eisner, 2008; Naradowsky et al., 2012), and a similar belief propagation approach has been used to enforce well-formed productions in CCG parsing (Auli and Lopez, 2011). More specifically, inputs to this algorithm are the span weights u(i, j). As in earlier dependency parsing work, these weights are derived from the ratio of messages coming in from the S pan variables: u(i, j) = mS pan(i, j)→CKYTree (true) mS pan(i, j)→CKYTree (false) After running this inside-outside algorithm in O(n3 ) time, we calculate the O(n2 ) outgoing messages from CKYTree: mCKYTree→S pan(i, j) (true) = g(i, j) mCKYTree→S pan(i, j) (false) = 1 − u(i, j) ·"
C12-1122,P06-1055,0,0.0220804,"e between syntactic constituents and named entity spans, providing an additional layer of flexibility on how these models interact. With this architecture we achieve the best-reported results on both CRF-based parsing and named entity recognition on sections of the OntoNotes corpus, and outperform state-of-the-art parsers on an NP-identification task, while remaining asymptotically faster than traditional grammar-based parsers. 1 Introduction Research in statistical parsing has made significant progress on recovering the kinds of annotations found in treebank-annotated corpora (Collins, 2003; Petrov et al., 2006), but in practice parsing is rarely an end in itself. Instead, parsing must be integrated with other forms of markup (part-ofspeech tagging, named-entity recognition, coreference resolution) in order to perform complete end-to-end tasks like question answering. Historically, independent pursuit of these tasks has often been accompanied by the belief that improvements on a measure intrinsic to parsing (for our domain, often F1 on the test section of the Wall Street Journal) will accurately reflect improvements on an end task when incorporated into a complete system. Nevertheless, errors propaga"
C12-1122,N07-1051,0,0.100868,"Missing"
C12-1122,D08-1016,1,0.836424,"matical parsing in a factor graph, we could impose a constraint which prohibits crossing brackets by examining variables in a pairwise manner and assigning zero probability to configurations in which two conflicting variables were both on, and this would be possible with a quartic number of constraining factors. But while both approaches would capture the gist of a useful constraint, they are both computationally inefficient and will still fail to prohibit all structures that are not valid trees. Instead we introduce a special-purpose combinatorial factor, CKYTree. It was observed previously (Smith and Eisner, 2008) that the outgoing messages from such combinatorial factors to a variable could be computed from the factor’s posterior beliefs about that variable, thus defining an interface for inserting special purpose logic within the standard inference algorithm. Instead of representing a tree constraint externally in the structure of the model, we can encapsulate similar logic in this factor where the computation can be done more efficiently with variants of standard dynamic programming algorithms (Figure 1). Previous work has applied this technique to non-projective and projective dependency parsing (S"
C12-1122,W00-1308,0,0.0149632,"indices unless it is sufficiently large. A previous generative model of span-based grammar induction (Klein and Manning, 2002) considered the probability of a span to depend on the part-of-speech tags of the two words immediately outside of it, and on the conjunction of the tags within it. For spans that are sufficiently small (here w < 10), 2002 the part-of-speech tags of all words inside of the span are concatenated. Examples of these feature sets are provided in Table 1. The reliance on part-of-speech tags as features is afforded through the use of an existing maxent part-of-speech tagger (Toutanvoa and Manning, 2000) which allows us to treat part-of-speech tags as observed while maintaining a fair comparison to other systems which also do not utilize gold part-of-speech tags. Features for the unary classifier portion of the model also consist of the same unigram features (word, tag, capitalization, etc.), but are taken over 5-word windows from the tokens of the sentence.5 Features for NER spans are generally identical to those used for syntactic spans, while the feature sets specifying span labels are much more lexically-oriented. In addition to a small set of contextuallydirected information we look prim"
C12-1122,P11-1086,0,0.0173593,"ented as a bipartite graph composed of variables, whose values represent the state of the world, and factors, which specify potentials over the belief of a particular state. For learning methods that require the marginal beliefs to compute a gradient, factor graphs provide an efficient way of computing such marginals via message passing inference algorithms. 1 In this particular case, one could compose the context-free grammar with the finite-state model—causing the grammar to grow even more. Similar problems arise in combining syntactic machine translation models with n-gram language models (Vaswani et al., 2011). 1996 The difficulty in representing grammars with factor graphs is that the complexity of inference in such a graph scales exponentially in the size of the largest clique. Naively representing a weighted grammar in Chomsky normal form requires variables to be densely connected with ternary factors 2 to represent the application of a weighted grammar rule. This is not only computationally problematic (on a per-iteration of inference basis), but could also further complicate convergence when performing belief propagation, an efficient message passing inference algorithm (Pearl, 1988), in the n"
D07-1014,W03-1019,0,0.0213394,"irst, there were generative, stochastic models like HMMs, PCFGs, and Eisner’s (1996) models. Local discriminative classifiers were proposed by McCallum et al. (2000) for sequence modeling, by Ratnaparkhi et al. (1994) for constituent parsing, and by Hall et al. (2006) (among others) for 132 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 132–140, Prague, June 2007. 2007 Association for Computational Linguistics dependencies. Large-margin whole-structure models were proposed for sequence labeling by Altun et al. (2003), for constituents by Taskar et al. (2004), and for dependency trees by McDonald et al. (2005a). In this paper, we propose a model most similar to the conditional random fields— interpretable as log-linear models—of Lafferty et al. (2001), which are now widely used for sequence labeling. Log-linear models have been used in parsing by Riezler et al. (2000) (for constraint-based grammars) and Johnson (2001) and Miyao and Tsujii (2002) (for CFGs). Like McDonald et al., we use an edge-factored model that permits nonprojective trees; like Lafferty et al., we argue for an alternative interpretation"
D07-1014,W06-2920,0,0.480413,"ncy parsing tasks and can be trained and applied without marginalization, “summing trees” permits some alternative techniques of interest. Using the summing algorithm, we present competitive experimental results on four nonprojective languages, for maximum conditional likelihood estimation, minimum Bayes-risk parsing, and hidden variable training. 1 Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA nasmith@cs.cmu.edu Introduction Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al., 2005) and information extraction (Culotta and Sorensen, 2004). Dependency parsing can be used to provide a “bare bones” syntactic structure that approximates semantics, and it has the additional advantage of admitting fast parsing algorithms (Eisner, 1996; McDonald et al., 2005b) with a negligible grammar constant in many cases. Here we consider weighted dependency parsing models that can be used to define well-formed conditional distributions p(y |x), for dependency trees y and a sentence x. Conditional distributions over outputs (here, tre"
D07-1014,C02-1126,0,0.0145015,"though it may be possible to develop efficient approximations. 5 Hidden Variables A third application of probability distributions over trees is hidden-variable learning. The ExpectationMaximization (EM) algorithm (Baum and Petrie, 1966; Dempster et al., 1977; Baker, 1979), for example, is a way to maximum the likelihood of training data, marginalizing out hidden variables. 137 This has been applied widely in unsupervised parsing (Carroll and Charniak, 1992; Klein and Manning, 2002). More recently, EM has been used to learn hidden variables in parse trees; these can be head-child annotations (Chiang and Bikel, 2002), latent head features (Matsuzaki et al., 2005; Prescher, 2005; Dreyer and Eisner, 2006), or hierarchicallysplit nonterminal states (Petrov et al., 2006). To date, we know of no attempts to apply hidden variables to supervised dependency tree models. If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over ("
D07-1014,P04-1054,0,0.00587294,"ve techniques of interest. Using the summing algorithm, we present competitive experimental results on four nonprojective languages, for maximum conditional likelihood estimation, minimum Bayes-risk parsing, and hidden variable training. 1 Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA nasmith@cs.cmu.edu Introduction Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al., 2005) and information extraction (Culotta and Sorensen, 2004). Dependency parsing can be used to provide a “bare bones” syntactic structure that approximates semantics, and it has the additional advantage of admitting fast parsing algorithms (Eisner, 1996; McDonald et al., 2005b) with a negligible grammar constant in many cases. Here we consider weighted dependency parsing models that can be used to define well-formed conditional distributions p(y |x), for dependency trees y and a sentence x. Conditional distributions over outputs (here, trees) given inputs (here, sentences) have certain advantages. They permit marginalization over trees to compute post"
D07-1014,W06-1638,0,0.0146662,"rd application of probability distributions over trees is hidden-variable learning. The ExpectationMaximization (EM) algorithm (Baum and Petrie, 1966; Dempster et al., 1977; Baker, 1979), for example, is a way to maximum the likelihood of training data, marginalizing out hidden variables. 137 This has been applied widely in unsupervised parsing (Carroll and Charniak, 1992; Klein and Manning, 2002). More recently, EM has been used to learn hidden variables in parse trees; these can be head-child annotations (Chiang and Bikel, 2002), latent head features (Matsuzaki et al., 2005; Prescher, 2005; Dreyer and Eisner, 2006), or hierarchicallysplit nonterminal states (Petrov et al., 2006). To date, we know of no attempts to apply hidden variables to supervised dependency tree models. If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over (X, Y). We have already shown in Section 3 how to solve (a); here we avoid (b) by maximiz"
D07-1014,C96-1058,0,0.712912,"nd hidden variable training. 1 Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA nasmith@cs.cmu.edu Introduction Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al., 2005) and information extraction (Culotta and Sorensen, 2004). Dependency parsing can be used to provide a “bare bones” syntactic structure that approximates semantics, and it has the additional advantage of admitting fast parsing algorithms (Eisner, 1996; McDonald et al., 2005b) with a negligible grammar constant in many cases. Here we consider weighted dependency parsing models that can be used to define well-formed conditional distributions p(y |x), for dependency trees y and a sentence x. Conditional distributions over outputs (here, trees) given inputs (here, sentences) have certain advantages. They permit marginalization over trees to compute posteriors of interesting sub-events (e.g., the probability that two noun tokens bear a relation, regardless of which tree is correct). A probability model permits alternative decoding procedures (G"
D07-1014,P96-1024,0,0.549313,"6; McDonald et al., 2005b) with a negligible grammar constant in many cases. Here we consider weighted dependency parsing models that can be used to define well-formed conditional distributions p(y |x), for dependency trees y and a sentence x. Conditional distributions over outputs (here, trees) given inputs (here, sentences) have certain advantages. They permit marginalization over trees to compute posteriors of interesting sub-events (e.g., the probability that two noun tokens bear a relation, regardless of which tree is correct). A probability model permits alternative decoding procedures (Goodman, 1996). Well-motivated hidden variable training procedures (such as EM and conditional EM) are also readily available for probabilistic models. Finally, probability models can be chained together (as in a noisy channel model), mixed, or combined in a product-of-experts. Sequence models, context-free models, and dependency models have appeared in several guises; a cross-model comparison clarifies the contribution of this paper. First, there were generative, stochastic models like HMMs, PCFGs, and Eisner’s (1996) models. Local discriminative classifiers were proposed by McCallum et al. (2000) for sequ"
D07-1014,P01-1042,0,0.0489209,"Language Learning, pp. 132–140, Prague, June 2007. 2007 Association for Computational Linguistics dependencies. Large-margin whole-structure models were proposed for sequence labeling by Altun et al. (2003), for constituents by Taskar et al. (2004), and for dependency trees by McDonald et al. (2005a). In this paper, we propose a model most similar to the conditional random fields— interpretable as log-linear models—of Lafferty et al. (2001), which are now widely used for sequence labeling. Log-linear models have been used in parsing by Riezler et al. (2000) (for constraint-based grammars) and Johnson (2001) and Miyao and Tsujii (2002) (for CFGs). Like McDonald et al., we use an edge-factored model that permits nonprojective trees; like Lafferty et al., we argue for an alternative interpretation as a log-linear model over structures, conditioned on the observed sentence. In Section 2 we point out what would be required, computationally, for conditional training of nonprojective dependency models. The solution to the conditionalization problem is given in Section 3, using a widely-known but newly-applied Matrix Tree Theorem due to Tutte (1984), and experimental results are presented with a compari"
D07-1014,P02-1017,0,0.0212776,"al model is not edge-factored. All that is required are the marginals pθ~ (Y(i) = y(i) |x), which may be intractable to compute exactly, though it may be possible to develop efficient approximations. 5 Hidden Variables A third application of probability distributions over trees is hidden-variable learning. The ExpectationMaximization (EM) algorithm (Baum and Petrie, 1966; Dempster et al., 1977; Baker, 1979), for example, is a way to maximum the likelihood of training data, marginalizing out hidden variables. 137 This has been applied widely in unsupervised parsing (Carroll and Charniak, 1992; Klein and Manning, 2002). More recently, EM has been used to learn hidden variables in parse trees; these can be head-child annotations (Chiang and Bikel, 2002), latent head features (Matsuzaki et al., 2005; Prescher, 2005; Dreyer and Eisner, 2006), or hierarchicallysplit nonterminal states (Petrov et al., 2006). To date, we know of no attempts to apply hidden variables to supervised dependency tree models. If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojec"
D07-1014,H05-1064,0,0.0205116,"ective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over (X, Y). We have already shown in Section 3 how to solve (a); here we avoid (b) by maximizing conditional likelihood, marginalizing out the hidden variable, denoted z: X X p˜(x, y) log pθ~ (y, z |x) (17) max θ~ x,y z This sort of conditional training with hidden variables was carried out by Koo and Collins (2005), for example, in reranking; it is related to the information bottleneck method (Tishby et al., 1999) and contrastive estimation (Smith and Eisner, 2005). 5.1 Latent Dependency Labels Noting that our model is edge-factored (Eq. 2), we define our hidden variables to be edge-factored as well. We can think of the hidden variables as clusters on dependency tokens, and redefine the score of an edge to be: sx,θ~ (i, j) = X ~ ~ eθ·f (x,xi ,xj ,z) (18) z∈Z where Z is a set of dependency clusters. Note that keeping the model edge-factored means that the cluster of each dependency in a tree is condition"
D07-1014,D07-1015,0,0.689582,", using a widely-known but newly-applied Matrix Tree Theorem due to Tutte (1984), and experimental results are presented with a comparison to the MIRA learning algorithm used by McDonald et al. (2005a). We go on to describe and experiment with two useful applications of conditional modeling: minimum Bayesrisk decoding (Section 4) and hidden-variable training by conditional maximum likelihood estimation (Section 5). Discussion in Section 6 considers the implications of our experimental results. Two indepedent papers, published concurrently with this one, report closely related results to ours. Koo et al. (2007) and McDonald and Satta (2007) both describe how the Matrix Tree Theorem can be applied to computing the sum of scores of edgefactored dependency trees and the edge marginals. Koo et al. compare conditional likelihood training (as here) to the averaged perceptron and a maximum margin model trained using exponentiatedgradient (Bartlett et al., 2004); the latter requires the same marginalization calculations as conditional log-linear estimation. McDonald and Satta discuss a variety of applications (including minimum Bayesrisk decoding) and give complexity results for nonedge-factored models. Int"
D07-1014,N04-1022,0,0.0125304,"of probability distributions over trees is the alternative decoding algorithm known as minimum Bayes-risk (mBr) decoding. The more commonly used maximum a posteriori decoding (also known as “Viterbi” decoding) that we applied in Section 3.3 sought to minimize the expected whole-tree loss: ˆ = argmax pθ~ (y |x) = argmin Ep~(Y|x) [−δ(y, Y)] y θ y y (14) Minimum Bayes-risk decoding generalizes this idea to an arbitrary loss function ` on the proposed tree: ˆ = argmin Ep~(Y|x) [`(y, Y)] y θ (15) y This technique was originally applied in speech recognition (Goel and Byrne, 2000) and translation (Kumar and Byrne, 2004); Goodman (1996) proposed a similar idea in probabilistic context-free parsing, seeking to maximize expected recall. For more applications in parsing, see Petrov and Klein (2007). The most common loss function used to evaluate dependency parsers is the number of attachment errors, so we seek to decode using: &quot; n # X ˆ = argmin Ep~ (Y|x) y −δ(y(i), Y(i)) θ y = argmax y i=1 n X pθ~ (Y(i) = y(i) |x) (16) i=1 To apply this decoding method, we make use of Eq. 13, which gives us the posterior probabilities of edges under the model, and the same ChiuLiu-Edmonds maximum directed spanning tree algorith"
D07-1014,P05-1010,0,0.0246773,"approximations. 5 Hidden Variables A third application of probability distributions over trees is hidden-variable learning. The ExpectationMaximization (EM) algorithm (Baum and Petrie, 1966; Dempster et al., 1977; Baker, 1979), for example, is a way to maximum the likelihood of training data, marginalizing out hidden variables. 137 This has been applied widely in unsupervised parsing (Carroll and Charniak, 1992; Klein and Manning, 2002). More recently, EM has been used to learn hidden variables in parse trees; these can be head-child annotations (Chiang and Bikel, 2002), latent head features (Matsuzaki et al., 2005; Prescher, 2005; Dreyer and Eisner, 2006), or hierarchicallysplit nonterminal states (Petrov et al., 2006). To date, we know of no attempts to apply hidden variables to supervised dependency tree models. If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over (X, Y). We have already shown in Section 3 how"
D07-1014,E06-1011,0,0.540636,"e that the features are edge-factored: f~(x, y) = n X f~(x, xi , xy(i) ) (2) i=1 In other words, the dependencies between words in the tree are all conditionally independent of each other, given the sequence x and the fact that the parse is a spanning tree. Despite the constraints they impose on features, edge-factored models have the advantage of tractable O(n3 ) inference algorithms or, with some trickery, O(n2 ) maximum a posteriori (“best parse tree”) inference algorithms in the nonprojective case. Exact nonprojective inference and estimation become intractable if we break edge factoring (McDonald and Pereira, 2006). We wish to estimate the parameters θ~ by maximizing the conditional likelihood (like a CRF) rather 1 To be precise, every word has in-degree 1, with the sole edge pointing from the word’s parent, xy(i) → xi . x0 has indegree 0. By definition, trees are acyclic. The edges need not be planar and may “cross” in the plane, since we do not have a projectivity constraint. In some formulations, exactly one node in x can attach to x0 ; here we allow multiple nodes to attach to x0 , since this occurs with some frequency in many existing datasets. Summation over trees where x0 has exactly one child is"
D07-1014,W07-2216,0,0.424405,"but newly-applied Matrix Tree Theorem due to Tutte (1984), and experimental results are presented with a comparison to the MIRA learning algorithm used by McDonald et al. (2005a). We go on to describe and experiment with two useful applications of conditional modeling: minimum Bayesrisk decoding (Section 4) and hidden-variable training by conditional maximum likelihood estimation (Section 5). Discussion in Section 6 considers the implications of our experimental results. Two indepedent papers, published concurrently with this one, report closely related results to ours. Koo et al. (2007) and McDonald and Satta (2007) both describe how the Matrix Tree Theorem can be applied to computing the sum of scores of edgefactored dependency trees and the edge marginals. Koo et al. compare conditional likelihood training (as here) to the averaged perceptron and a maximum margin model trained using exponentiatedgradient (Bartlett et al., 2004); the latter requires the same marginalization calculations as conditional log-linear estimation. McDonald and Satta discuss a variety of applications (including minimum Bayesrisk decoding) and give complexity results for nonedge-factored models. Interested readers are referred t"
D07-1014,P05-1012,0,0.74593,"005a) or local parsing decision scores (Hall et al., 2006). In the works cited, these scores are not intended to be interpreted as probabilistic quantities. A notable gap in research on statistical dependency parsing is a proper conditional probability distribution over nonprojective dependency trees for a given sentence. We exploit the Matrix Tree Theorem (Tutte, 1984) to derive an algorithm that efficiently sums the scores of all nonprojective trees in a sentence, permitting the definition of a conditional log-linear model over trees. While discriminative methods, such as those presented in McDonald et al. (2005b), obtain very high accuracy on standard dependency parsing tasks and can be trained and applied without marginalization, “summing trees” permits some alternative techniques of interest. Using the summing algorithm, we present competitive experimental results on four nonprojective languages, for maximum conditional likelihood estimation, minimum Bayes-risk parsing, and hidden variable training. 1 Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA nasmith@cs.cmu.edu Introduction Recently dependency parsing has received r"
D07-1014,H05-1066,0,0.42133,"Missing"
D07-1014,P93-1024,0,0.0601716,"res. We propose several interpretations. First, it may simply be that many more clusters may be required. Note that the label-set sizes for the labeled versions of these datasets are larger than 32 (e.g., 50 for Danish). This has the unfortunate effect of blowing up the feature space beyond the memory capacity of our machines (hence our attempts at squeezing high-dimensional features through the clusters). Of course, improved clustering methods may also improve performance. In particular, a clusterlearning algorithm that permits clusters to split and/or merge, as in Petrov et al. (2006) or in Pereira et al. (1993), may be appropriate. Given the relative simplicity of clustering methods for context-free parsing to date (gains were found just by using Expectation-Maximization), we believe the fundamental reason clustering was not particularly helpful here is a structural one. In context-free parsing, the latent features are (in published work to date) on nonterminal states, which are the stuctural “bridge” between context-free rules. Adding features to those states is a way of pushing information—encoded indirectly, perhaps—farther around the tree, and therefore circumventing the strict independence assu"
D07-1014,N07-1051,0,0.0074192,"also known as “Viterbi” decoding) that we applied in Section 3.3 sought to minimize the expected whole-tree loss: ˆ = argmax pθ~ (y |x) = argmin Ep~(Y|x) [−δ(y, Y)] y θ y y (14) Minimum Bayes-risk decoding generalizes this idea to an arbitrary loss function ` on the proposed tree: ˆ = argmin Ep~(Y|x) [`(y, Y)] y θ (15) y This technique was originally applied in speech recognition (Goel and Byrne, 2000) and translation (Kumar and Byrne, 2004); Goodman (1996) proposed a similar idea in probabilistic context-free parsing, seeking to maximize expected recall. For more applications in parsing, see Petrov and Klein (2007). The most common loss function used to evaluate dependency parsers is the number of attachment errors, so we seek to decode using: &quot; n # X ˆ = argmin Ep~ (Y|x) y −δ(y(i), Y(i)) θ y = argmax y i=1 n X pθ~ (Y(i) = y(i) |x) (16) i=1 To apply this decoding method, we make use of Eq. 13, which gives us the posterior probabilities of edges under the model, and the same ChiuLiu-Edmonds maximum directed spanning tree algorithm used for maximum a posteriori decoding. Note that this decoding method can be applied regardless of how the model is trained. It merely requires assuming that the tree scores u"
D07-1014,P06-1055,0,0.116612,"able learning. The ExpectationMaximization (EM) algorithm (Baum and Petrie, 1966; Dempster et al., 1977; Baker, 1979), for example, is a way to maximum the likelihood of training data, marginalizing out hidden variables. 137 This has been applied widely in unsupervised parsing (Carroll and Charniak, 1992; Klein and Manning, 2002). More recently, EM has been used to learn hidden variables in parse trees; these can be head-child annotations (Chiang and Bikel, 2002), latent head features (Matsuzaki et al., 2005; Prescher, 2005; Dreyer and Eisner, 2006), or hierarchicallysplit nonterminal states (Petrov et al., 2006). To date, we know of no attempts to apply hidden variables to supervised dependency tree models. If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over (X, Y). We have already shown in Section 3 how to solve (a); here we avoid (b) by maximizing conditional likelihood, marginalizing out the hidden variable"
D07-1014,W05-1512,0,0.0168306,"Variables A third application of probability distributions over trees is hidden-variable learning. The ExpectationMaximization (EM) algorithm (Baum and Petrie, 1966; Dempster et al., 1977; Baker, 1979), for example, is a way to maximum the likelihood of training data, marginalizing out hidden variables. 137 This has been applied widely in unsupervised parsing (Carroll and Charniak, 1992; Klein and Manning, 2002). More recently, EM has been used to learn hidden variables in parse trees; these can be head-child annotations (Chiang and Bikel, 2002), latent head features (Matsuzaki et al., 2005; Prescher, 2005; Dreyer and Eisner, 2006), or hierarchicallysplit nonterminal states (Petrov et al., 2006). To date, we know of no attempts to apply hidden variables to supervised dependency tree models. If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over (X, Y). We have already shown in Section 3 how to solve (a); he"
D07-1014,P05-1034,0,0.0195596,"lization, “summing trees” permits some alternative techniques of interest. Using the summing algorithm, we present competitive experimental results on four nonprojective languages, for maximum conditional likelihood estimation, minimum Bayes-risk parsing, and hidden variable training. 1 Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA nasmith@cs.cmu.edu Introduction Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al., 2005) and information extraction (Culotta and Sorensen, 2004). Dependency parsing can be used to provide a “bare bones” syntactic structure that approximates semantics, and it has the additional advantage of admitting fast parsing algorithms (Eisner, 1996; McDonald et al., 2005b) with a negligible grammar constant in many cases. Here we consider weighted dependency parsing models that can be used to define well-formed conditional distributions p(y |x), for dependency trees y and a sentence x. Conditional distributions over outputs (here, trees) given inputs (here, sentences) have certain advantages"
D07-1014,P00-1061,0,0.0318368,"n Natural Language Processing and Computational c Natural Language Learning, pp. 132–140, Prague, June 2007. 2007 Association for Computational Linguistics dependencies. Large-margin whole-structure models were proposed for sequence labeling by Altun et al. (2003), for constituents by Taskar et al. (2004), and for dependency trees by McDonald et al. (2005a). In this paper, we propose a model most similar to the conditional random fields— interpretable as log-linear models—of Lafferty et al. (2001), which are now widely used for sequence labeling. Log-linear models have been used in parsing by Riezler et al. (2000) (for constraint-based grammars) and Johnson (2001) and Miyao and Tsujii (2002) (for CFGs). Like McDonald et al., we use an edge-factored model that permits nonprojective trees; like Lafferty et al., we argue for an alternative interpretation as a log-linear model over structures, conditioned on the observed sentence. In Section 2 we point out what would be required, computationally, for conditional training of nonprojective dependency models. The solution to the conditionalization problem is given in Section 3, using a widely-known but newly-applied Matrix Tree Theorem due to Tutte (1984), an"
D07-1014,P05-1044,1,0.0822349,"oving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over (X, Y). We have already shown in Section 3 how to solve (a); here we avoid (b) by maximizing conditional likelihood, marginalizing out the hidden variable, denoted z: X X p˜(x, y) log pθ~ (y, z |x) (17) max θ~ x,y z This sort of conditional training with hidden variables was carried out by Koo and Collins (2005), for example, in reranking; it is related to the information bottleneck method (Tishby et al., 1999) and contrastive estimation (Smith and Eisner, 2005). 5.1 Latent Dependency Labels Noting that our model is edge-factored (Eq. 2), we define our hidden variables to be edge-factored as well. We can think of the hidden variables as clusters on dependency tokens, and redefine the score of an edge to be: sx,θ~ (i, j) = X ~ ~ eθ·f (x,xi ,xj ,z) (18) z∈Z where Z is a set of dependency clusters. Note that keeping the model edge-factored means that the cluster of each dependency in a tree is conditionally independent of all the others, given the words. This is computationally advantageous (we can factor out the marginalization of the hidden variable b"
D07-1014,W04-3201,0,0.0335905,"odels like HMMs, PCFGs, and Eisner’s (1996) models. Local discriminative classifiers were proposed by McCallum et al. (2000) for sequence modeling, by Ratnaparkhi et al. (1994) for constituent parsing, and by Hall et al. (2006) (among others) for 132 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 132–140, Prague, June 2007. 2007 Association for Computational Linguistics dependencies. Large-margin whole-structure models were proposed for sequence labeling by Altun et al. (2003), for constituents by Taskar et al. (2004), and for dependency trees by McDonald et al. (2005a). In this paper, we propose a model most similar to the conditional random fields— interpretable as log-linear models—of Lafferty et al. (2001), which are now widely used for sequence labeling. Log-linear models have been used in parsing by Riezler et al. (2000) (for constraint-based grammars) and Johnson (2001) and Miyao and Tsujii (2002) (for CFGs). Like McDonald et al., we use an edge-factored model that permits nonprojective trees; like Lafferty et al., we argue for an alternative interpretation as a log-linear model over structures, con"
D07-1014,D07-1070,1,\N,Missing
D07-1070,P06-1121,0,0.0299677,"hand-labeled treebank in the target domain, together with a large unannotated collection of indomain sentences. Additional resources such as parsers for other domains or languages can be integrated naturally. Dependency parsing is important as a key component in leading systems for information extraction (Weischedel, 2004)1 and question answering (Peng et al., 2005). These systems rely on edges or paths in dependency parse trees to define their extraction patterns and classification features. Parsing is also key to the latest advances in machine translation, which translate syntactic phrases (Galley et al., 2006; Marcu et al., 2006; Cowan et al., 2006). 2 Our Approach Our approach rests on three observations: • Recent “feature-based” parsing models are an excellent fit for bootstrapping, because the parse is often overdetermined by many redundant features. • The feature-based framework is flexible enough to incorporate other sources of guidance during training or testing—such as the knowledge contained in a parser for another language or domain. • Maximizing a combination of likelihood on labeled data and confidence on unlabeled data is a principled approach to bootstrapping. 2.1 Feature-Based Parsin"
D07-1070,W06-1615,0,0.0476143,"ave a treebank for a different domain or genre of the target language. One could simply include these trees in the initial supervised training, and hope that bootstrapping corrects any learned weights that are inappropriate to the target domain, as discussed above. In fact, McClosky et al. (2006) found a similar technique to be effective—though only in a model with a large feature space (“PCFG + reranking”), as we would predict. However, another approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data (Blitzer et al., 2006). Bootstrapping now teaches us where to trust the out-of-domain parser. If our basic model has 100 features, we could add features 101 through 200, where for example ˜ ˜ f123 (e) = f23 · log Pr(e) and Pr(e) is the posterior edge probability according to the out-of-domain parser. Learning that this feature has a high weight means learning to trust the out-of-domain parser’s decision on edges where in-domain feature 23 fires. Even more sensibly, we could add features such as P ˜ ˜ ˜ ˜ f201 (e) = 10 i=1 fi (e) · θi , where f and θ are the feature and weight vectors for the out-of-domain parser. L"
D07-1070,J04-3004,0,0.0541045,"ach iteration retrains the model on the examples where it is currently most confident. This kind of “confidence thresholding” has been popular in previous bootstrapping work (as cited in §2.2). It attempts to maintain high accuracy while gradually expanding coverage. The assumption is that throughout the training procedure, the parser’s confidence is a trustworthy guide to its correctness. Different bootstrapping procedures use different learners, smoothing methods, confidence measures, and procedures for “forgetting” the labelings from previous iterations. In his analysis of Yarowsky (1995), Abney (2004) formulates several variants of bootstrapping. These are shown to increase either the likelihood of the training data, or a lower bound on that likelihood. In particular, Abney defines a function K that is an upper bound on the negative log-likelihood, and shows his bootstrapping algorithms locally minimize K. We now present a generalization of Abney’s K function and relate it to another semi-supervised learning technique, entropy regularization (Brand, 1999; Grandvalet and Bengio, 2005; Jiao et al., 2006). Our experiments will tune the feature weight vector, θ, to minimize our function. We wi"
D07-1070,W06-2920,0,0.057846,"experiments on small corpora, using the features from (McDonald et al., 2005). After discussing experimental setup (§3.1), we look at the correlation of confidence with accuracy and with oracle likelihood, and at the fine-grained behaviour of models’ dependency edge posteriors (§3.2). We then compare our confidence-maximizing bootstrapping to EM, which has been widely used in semi-supervised learning (§3.4). Section 3.3 presents overall bootstrapping accuracy. 3.1 Experimental Design We bootstrapped non-projective parsers for languages assembled for the CoNLL dependency parsing competitions (Buchholz and Marsi, 2006). We selected German, Spanish, and Czech (Brants et al., 2002; Civit Torruella and Mart´ı Anton´ın, 2002; B¨ohmov´a et al., 2003). After removing sentences more than 60 words long, we randomly divided each corpus into small seed sets of 100 and 1000 trees; development and test sets of 200 trees each; and an unlabeled training set from the rest. These treebanks contain strict dependency trees, in the sense that their only nodes are the words and a distinguished root node. In the Czech dataset, more than one word can attach to the root; also, the trees in German, Spanish, and Czech may be nonpro"
D07-1070,W00-1306,0,0.0302444,"sing, the analogy to the inside algorithm is the O(n3 ) “matrix-tree algorithm,” which is dominated asymptotically by a matrix determinant (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007). The gradient of a determinant may be computed by matrix inversion, so evaluating the gradient again has the same O(n3 ) complexity as evaluating the function. The second term of (4) is the Shannon entropy of the posterior distribution over parses. Computing this for projective parsing takes O(n3 ) time, using a dynamic programming algorithm that is closely related to the inside algorithm (Hwa, 2000).6 For nonprojective parsing, unfortunately, the runtime rises to O(n4 ), since it requires determinants of n distinct matrices (each incorporating a log factor in a different column; we omit the details). The gradient evaluation in both cases is again about as expensive as the function evaluation. A convenient speedup is to replace Shannon entropy with R´enyi entropy. The family of R´enyi entropy measures is parameterized by α: Rα (p) = X 1 log p(y)α 1−α y ! (6) In our setting, where p = pθ,i , the events y are the possible parses yi,k of xi . Observe that under our P P P definition of p, y p"
D07-1070,W05-1508,0,0.0287936,"ill tune the feature weight vector, θ, to minimize our function. We will do so simply by applying a generic function minimization method (stochastic gradient descent), rather than by crafting a new Yarowsky-style or Abney-style iterative procedure for our specific function. Suppose we have examples xi and corresponding possible labelings yi,k . We are trying to learn a parametric model pθ (yi,k |xi ). If p˜(yi,k |xi ) is a “labeling distribution” that reflects our uncertainty about the true labels, then our expected negative loglikelihood of the model is def K = − XX i = XX = X i k Similarly, Jansche (2005) imputes “missing” trees by using comparable corpora. 670 p˜(yi,k |xi ) log p˜(yi,k |xi ) pθ (yi,k |xi )˜ p(yi,k |xi ) D(˜ pi kpθ,i ) + H(˜ pi ) (3) i def 3 p˜(yi,k |xi ) log pθ (yi,k |xi ) k def where p˜i (·) = p˜(· |xi ) and pθ,i (·) = pθ (· |xi ). Note that K is a function not only of θ but also of the labeling distribution p˜; a learner might be allowed to manipulate either in order to decrease K. The summands of K in equation (3) can be divided into two cases, according to whether xi is labeled or not. For the labeled examples {xi : i ∈ L}, the labeling distribution p˜i is a point distrib"
D07-1070,P06-1027,0,0.268878,"s for “forgetting” the labelings from previous iterations. In his analysis of Yarowsky (1995), Abney (2004) formulates several variants of bootstrapping. These are shown to increase either the likelihood of the training data, or a lower bound on that likelihood. In particular, Abney defines a function K that is an upper bound on the negative log-likelihood, and shows his bootstrapping algorithms locally minimize K. We now present a generalization of Abney’s K function and relate it to another semi-supervised learning technique, entropy regularization (Brand, 1999; Grandvalet and Bengio, 2005; Jiao et al., 2006). Our experiments will tune the feature weight vector, θ, to minimize our function. We will do so simply by applying a generic function minimization method (stochastic gradient descent), rather than by crafting a new Yarowsky-style or Abney-style iterative procedure for our specific function. Suppose we have examples xi and corresponding possible labelings yi,k . We are trying to learn a parametric model pθ (yi,k |xi ). If p˜(yi,k |xi ) is a “labeling distribution” that reflects our uncertainty about the true labels, then our expected negative loglikelihood of the model is def K = − XX i = XX"
D07-1070,N07-1032,1,0.825026,"o a theoretically attractive generalization. It can be shown that limα→1 Rα (p) 5 The numerator of pθ,i (yi∗ ) (see definition above) is trivial since yi∗ is a single known parse. But the denominator Zi is a normalizing constant that sums over all parses; it is found by a dependency-parsing variant of the inside algorithm, following (Eisner, 1996). 6 See also (Mann and McCallum, 2007) for similar results on conditional random fields. 672 is in fact the Shannon entropy H(p) and that limα→∞ Rα (p) = − log maxy p(y), i.e. the negative log probability of the modal or “Viterbi” label (Arndt, 2001; Karakos et al., 2007). The α = 2 case, widely used as a measure of purity in decision tree learning, is often called the “Gini index.” Finally, when α = 0, we get the log of the number of labels, which equals the H(uniform distribution) that Abney used in equation (3). 3 Evaluation For this paper, we performed some initial bootstrapping experiments on small corpora, using the features from (McDonald et al., 2005). After discussing experimental setup (§3.1), we look at the correlation of confidence with accuracy and with oracle likelihood, and at the fine-grained behaviour of models’ dependency edge posteriors (§3."
D07-1070,P04-1061,0,0.206471,"Missing"
D07-1070,P06-1103,0,0.0183451,"not be able to confidently parse without an English translation. Parses of comparable English sentences. World knowledge can be useful in parsing. Suppose you see a French sentence that contains mangeons and pommes, and you know that manger=eat and pomme=apple. You might reasonably guess that pommes is the direct object of mangeons, because you know that apple is a plausible direct object for eat. We can discover this last bit of world knowledge from comparable English text. Translation dictionaries can themselves be induced from comparable corpora (Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006), or extracted from bitext or digitized versions of human-readable dictionaries if these are available. The above inference pattern can be captured by features similar to those in equation (1). For example, one can define a feature j by poss def prep pobj (2) fi (c −→ p) = log Pr (p0 ←− of ←− c0 |p0 translates p, c0 translates c) where each event in the event space is a pair (c0 , p0 ) of same-sentence tokens in comparable English text, all pairs being equally likely. Thus, to estimate Pr(· |·), the denominator counts same-sentence token pairs (c0 , p0 ) in the comparable English corpus that t"
D07-1070,W06-1628,0,0.059052,"Missing"
D07-1070,D07-1015,0,0.108661,"∈yi,k f (e) · θ)/Zi . With exponentially many parses of xi , does our objective function (4) now have prohibitive computational complexity? The complexity is actually similar to that of the inside algorithm for parsing. In fact, the first term of (4) for projective parsing is found by running the O(n3 ) inside algorithm on supervised data,5 and its gradient is found by the corresponding O(n3 ) outside algorithm. For nonprojective parsing, the analogy to the inside algorithm is the O(n3 ) “matrix-tree algorithm,” which is dominated asymptotically by a matrix determinant (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007). The gradient of a determinant may be computed by matrix inversion, so evaluating the gradient again has the same O(n3 ) complexity as evaluating the function. The second term of (4) is the Shannon entropy of the posterior distribution over parses. Computing this for projective parsing takes O(n3 ) time, using a dynamic programming algorithm that is closely related to the inside algorithm (Hwa, 2000).6 For nonprojective parsing, unfortunately, the runtime rises to O(n4 ), since it requires determinants of n distinct matrices (each incorporating a log factor in a dif"
D07-1070,N07-2028,0,0.0318561,"the inside algorithm, and we can find the numerator by running the inside algorithm again with θ scaled by α. Thus with R´enyi entropy, all computations and their gradients are O(n3 )—even in the nonprojective case. R´enyi entropy is also a theoretically attractive generalization. It can be shown that limα→1 Rα (p) 5 The numerator of pθ,i (yi∗ ) (see definition above) is trivial since yi∗ is a single known parse. But the denominator Zi is a normalizing constant that sums over all parses; it is found by a dependency-parsing variant of the inside algorithm, following (Eisner, 1996). 6 See also (Mann and McCallum, 2007) for similar results on conditional random fields. 672 is in fact the Shannon entropy H(p) and that limα→∞ Rα (p) = − log maxy p(y), i.e. the negative log probability of the modal or “Viterbi” label (Arndt, 2001; Karakos et al., 2007). The α = 2 case, widely used as a measure of purity in decision tree learning, is often called the “Gini index.” Finally, when α = 0, we get the log of the number of labels, which equals the H(uniform distribution) that Abney used in equation (3). 3 Evaluation For this paper, we performed some initial bootstrapping experiments on small corpora, using the features"
D07-1070,W06-1606,0,0.0177912,"in the target domain, together with a large unannotated collection of indomain sentences. Additional resources such as parsers for other domains or languages can be integrated naturally. Dependency parsing is important as a key component in leading systems for information extraction (Weischedel, 2004)1 and question answering (Peng et al., 2005). These systems rely on edges or paths in dependency parse trees to define their extraction patterns and classification features. Parsing is also key to the latest advances in machine translation, which translate syntactic phrases (Galley et al., 2006; Marcu et al., 2006; Cowan et al., 2006). 2 Our Approach Our approach rests on three observations: • Recent “feature-based” parsing models are an excellent fit for bootstrapping, because the parse is often overdetermined by many redundant features. • The feature-based framework is flexible enough to incorporate other sources of guidance during training or testing—such as the knowledge contained in a parser for another language or domain. • Maximizing a combination of likelihood on labeled data and confidence on unlabeled data is a principled approach to bootstrapping. 2.1 Feature-Based Parsing McDonald et al. (2"
D07-1070,C96-1058,1,0.864752,"schedel (p.c.) reports that this system’s performance degrades considerably when only phrase chunking is available rather than full parsing. 667 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 667–677, Prague, June 2007. 2007 Association for Computational Linguistics Given an n-word input sentence, the parser begins by scoring each of the O(n2 ) possible edges, and then seeks the highest-scoring legal dependency tree formed by any n − 1 of these edges, using an O(n3 ) dynamic programming algorithm (Eisner, 1996) for projective trees. For non-projective parsing, O(n3 ), or with some trickery O(n2 ), greedy algorithms exist (Chu and Liu, 1965; Edmonds, 1967; Gabow et al., 1986). The feature function f may pay attention to many properties of the directed edge e. Of course, features may consider the parent and child words connected by e, and their parts of speech.2 But some features used by McDonald et al. (2005) also consider the parts of speech of words adjacent to the parent and child, or between the parent and child, as well as the number of words between the parent and child. In general, these featu"
D07-1070,P06-1043,0,0.0290869,"domain (plus a large unsupervised corpus). We now consider other, more dubious, knowledge sources that might supplement or replace this small treebank. In each case, we can use these knowledge sources to derive features that may—or may not— prove trustworthy during bootstrapping. Parses from a different domain. One might have a treebank for a different domain or genre of the target language. One could simply include these trees in the initial supervised training, and hope that bootstrapping corrects any learned weights that are inappropriate to the target domain, as discussed above. In fact, McClosky et al. (2006) found a similar technique to be effective—though only in a model with a large feature space (“PCFG + reranking”), as we would predict. However, another approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data (Blitzer et al., 2006). Bootstrapping now teaches us where to trust the out-of-domain parser. If our basic model has 100 features, we could add features 101 through 200, where for example ˜ ˜ f123 (e) = f23 · log Pr(e) and Pr(e) is the posterior edge probability according to the out-of-domain pars"
D07-1070,E06-1011,0,0.0488428,"its ML baseline. Since these models were only trained and tested on sentences of 10 words or fewer, accuracy is much higher than the full results in Table 2. 4 We hypothesize that qualitatively better bootstrapping results will require much larger unlabeled data sets. In scaling up bootstrapping to larger unlabeled training sets, we must carefully weight tradeoffs between expanding coverage and introducing noise from out-of-domain data. We could also better exploit the data we have with richer models of syntax. In supervised dependency parsing, secondorder edge features provide improvements (McDonald and Pereira, 2006; Riedel and Clarke, 2006); moreover, the feature-based approach is not limited to dependency parsing. Similar techniques could score parses in other formalisms, such as CFG or TAG. In this case, f extracts features from each of the derivation tree’s rewrite rules (CFG) or elementary trees (TAG). In lexicalized formalisms, f will still be able to score lexical dependencies that are implicitly represented in the parse. Finally, we want to investigate whether larger training sets will provide traction for sparser cross-lingual and crossdomain features. 5 to sentences of ten words or fewer and to"
D07-1070,W07-2216,0,0.0484585,"i . With exponentially many parses of xi , does our objective function (4) now have prohibitive computational complexity? The complexity is actually similar to that of the inside algorithm for parsing. In fact, the first term of (4) for projective parsing is found by running the O(n3 ) inside algorithm on supervised data,5 and its gradient is found by the corresponding O(n3 ) outside algorithm. For nonprojective parsing, the analogy to the inside algorithm is the O(n3 ) “matrix-tree algorithm,” which is dominated asymptotically by a matrix determinant (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007). The gradient of a determinant may be computed by matrix inversion, so evaluating the gradient again has the same O(n3 ) complexity as evaluating the function. The second term of (4) is the Shannon entropy of the posterior distribution over parses. Computing this for projective parsing takes O(n3 ) time, using a dynamic programming algorithm that is closely related to the inside algorithm (Hwa, 2000).6 For nonprojective parsing, unfortunately, the runtime rises to O(n4 ), since it requires determinants of n distinct matrices (each incorporating a log factor in a different column; we omit the"
D07-1070,P05-1012,0,0.538306,"Marcu et al., 2006; Cowan et al., 2006). 2 Our Approach Our approach rests on three observations: • Recent “feature-based” parsing models are an excellent fit for bootstrapping, because the parse is often overdetermined by many redundant features. • The feature-based framework is flexible enough to incorporate other sources of guidance during training or testing—such as the knowledge contained in a parser for another language or domain. • Maximizing a combination of likelihood on labeled data and confidence on unlabeled data is a principled approach to bootstrapping. 2.1 Feature-Based Parsing McDonald et al. (2005) introduced a simple, flexible framework for scoring dependency parses. Each directed edge e in the dependency tree is described with a high-dimensional feature vector f (e). The edge’s score is the dot product f (e) · θ, where θ is a learned weight vector. The overall score of a dependency tree is the sum of the scores of all edges in the tree. 1 Ralph Weischedel (p.c.) reports that this system’s performance degrades considerably when only phrase chunking is available rather than full parsing. 667 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and"
D07-1070,J94-2001,0,0.0583027,"words or fewer and to part-ofspeech sequences alone, without any lexical information. Since the DMV models projective trees, we ran experiments on three CoNLL corpora that had augmented their primary non-projective parses with alternate projective annotations: Bulgarian (Simov et al., 2005), German, and Spanish. We performed supervised maximum likelihood and conditional likelihood estimation on a seed set of 100 sentences for each language. These models respectively initialized EM and confidence training on unlabeled data. We see (Table 4) that EM degrades the performance of its ML baseline. Merialdo (1994) saw a similar degradation over small (and large) seed sets in HMM POS tagging. We tried fixing and not fixing the feature expectations on the seed set during EM and show the former, better numbers. Confidence maximization improved over both its own conditional likelihood initializer and also over ML. We selected optimal smoothing parameters for all models and optimal α (equation (6)) and γ (equation (4)) for the confidence model on labeled held-out data. 675 Future Work Conclusions Feature-rich dependency models promise to help bootstrapping by providing many redundant features for the learne"
D07-1070,H05-1039,0,0.0285249,"slations. Yet an adequate labeled training corpus—a large treebank of manually constructed parse trees of typical sentences—is rarely available and would be prohibitively expensive to develop. We show how it is possible to train instead from a small hand-labeled treebank in the target domain, together with a large unannotated collection of indomain sentences. Additional resources such as parsers for other domains or languages can be integrated naturally. Dependency parsing is important as a key component in leading systems for information extraction (Weischedel, 2004)1 and question answering (Peng et al., 2005). These systems rely on edges or paths in dependency parse trees to define their extraction patterns and classification features. Parsing is also key to the latest advances in machine translation, which translate syntactic phrases (Galley et al., 2006; Marcu et al., 2006; Cowan et al., 2006). 2 Our Approach Our approach rests on three observations: • Recent “feature-based” parsing models are an excellent fit for bootstrapping, because the parse is often overdetermined by many redundant features. • The feature-based framework is flexible enough to incorporate other sources of guidance during tr"
D07-1070,W06-1616,0,0.0124744,"e models were only trained and tested on sentences of 10 words or fewer, accuracy is much higher than the full results in Table 2. 4 We hypothesize that qualitatively better bootstrapping results will require much larger unlabeled data sets. In scaling up bootstrapping to larger unlabeled training sets, we must carefully weight tradeoffs between expanding coverage and introducing noise from out-of-domain data. We could also better exploit the data we have with richer models of syntax. In supervised dependency parsing, secondorder edge features provide improvements (McDonald and Pereira, 2006; Riedel and Clarke, 2006); moreover, the feature-based approach is not limited to dependency parsing. Similar techniques could score parses in other formalisms, such as CFG or TAG. In this case, f extracts features from each of the derivation tree’s rewrite rules (CFG) or elementary trees (TAG). In lexicalized formalisms, f will still be able to score lexical dependencies that are implicitly represented in the parse. Finally, we want to investigate whether larger training sets will provide traction for sparser cross-lingual and crossdomain features. 5 to sentences of ten words or fewer and to part-ofspeech sequences a"
D07-1070,W02-2026,0,0.0216572,"e some unsupervised sentences that we would not be able to confidently parse without an English translation. Parses of comparable English sentences. World knowledge can be useful in parsing. Suppose you see a French sentence that contains mangeons and pommes, and you know that manger=eat and pomme=apple. You might reasonably guess that pommes is the direct object of mangeons, because you know that apple is a plausible direct object for eat. We can discover this last bit of world knowledge from comparable English text. Translation dictionaries can themselves be induced from comparable corpora (Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006), or extracted from bitext or digitized versions of human-readable dictionaries if these are available. The above inference pattern can be captured by features similar to those in equation (1). For example, one can define a feature j by poss def prep pobj (2) fi (c −→ p) = log Pr (p0 ←− of ←− c0 |p0 translates p, c0 translates c) where each event in the event space is a pair (c0 , p0 ) of same-sentence tokens in comparable English text, all pairs being equally likely. Thus, to estimate Pr(· |·), the denominator counts same-sentence token pairs (c0 , p"
D07-1070,W06-3104,1,0.841037,"ulting noisy treebank. They used only 1-best translations, 1-best alignments, dependency paths of length 1, and no labeled data in Spanish or Chinese. Hwa et al. (2005) used a manually written postprocessor to correct some of the many incorrect projections. By contrast, our framework uses the projected dependencies only as one source of features. They may be overridden by other features in particular cases, and will be given a high weight only if they tend to agree with other features during bootstrapping. A similar soft projection of dependencies was used in supervised machine translation by Smith and Eisner (2006), who used a source sentence’s dependency paths to bias the generation of its translation. Note that these bilingual features will only fire on those supervised or unsupervised sentences for which we have an English translation. In particular, they will usually be unavailable on the test set. However, we hope that they will seed and facilitate the bootstrapping process, by helping us confidently parse some unsupervised sentences that we would not be able to confidently parse without an English translation. Parses of comparable English sentences. World knowledge can be useful in parsing. Suppos"
D07-1070,D07-1014,1,0.85215,"f pθ,i (yi,k ) = exp( e∈yi,k f (e) · θ)/Zi . With exponentially many parses of xi , does our objective function (4) now have prohibitive computational complexity? The complexity is actually similar to that of the inside algorithm for parsing. In fact, the first term of (4) for projective parsing is found by running the O(n3 ) inside algorithm on supervised data,5 and its gradient is found by the corresponding O(n3 ) outside algorithm. For nonprojective parsing, the analogy to the inside algorithm is the O(n3 ) “matrix-tree algorithm,” which is dominated asymptotically by a matrix determinant (Smith and Smith, 2007; Koo et al., 2007; McDonald and Satta, 2007). The gradient of a determinant may be computed by matrix inversion, so evaluating the gradient again has the same O(n3 ) complexity as evaluating the function. The second term of (4) is the Shannon entropy of the posterior distribution over parses. Computing this for projective parsing takes O(n3 ) time, using a dynamic programming algorithm that is closely related to the inside algorithm (Hwa, 2000).6 For nonprojective parsing, unfortunately, the runtime rises to O(n4 ), since it requires determinants of n distinct matrices (each incorporating a l"
D07-1070,E03-1008,0,0.0283579,"the more robust it will be against errors even when θ is incorrectly trained. (Some feature weights may be too strong or have the wrong sign, because of overfitting or mistaken parses during bootstrapping.) In the former case, strong features lend their strength to weak ones. In the latter case, a conflict among strong features weakens the ones that depart from the consensus, or discounts the example sentence if there is no consensus. Previous work on parser bootstrapping has not been able to exploit this redundancy among features, because it has used PCFG-like models with far fewer features (Steedman et al., 2003). 2.3 Adaptation and Projection via Features The previous section assumed that we had a small supervised treebank in the target language and domain (plus a large unsupervised corpus). We now consider other, more dubious, knowledge sources that might supplement or replace this small treebank. In each case, we can use these knowledge sources to derive features that may—or may not— prove trustworthy during bootstrapping. Parses from a different domain. One might have a treebank for a different domain or genre of the target language. One could simply include these trees in the initial supervised t"
D07-1070,P00-1027,0,0.0153504,"eans accepting all other features on that edge. It also means rejecting various other edges, because of the global constraints that a legal parse tree must give each word only one parent and must be free of cycles and, in 2 Note that since we are not trying to predict parts of speech, we treat the output of one or more automatic taggers as yet more inputs to edge feature functions. 668 the projective case, crossings. Our observation is that this situation is ideal for so-called “bootstrapping,” “co-training,” or “minimally supervised” learning methods (Yarowsky, 1995; Blum and Mitchell, 1998; Yarowsky and Wicentowski, 2000). Such methods should thrive when the right answer is overdetermined owing to redundant features and/or global constraints. Concretely, suppose we start by training a supervised parser on only 100 examples, using some regularization method to prevent overfitting to this set. While many features might truly be relevant to the task, only a few appear often enough in this small training set to acquire significantly positive or negative weights. Even this lightly trained parser may be quite sure of itself on some test sentences in a large unannotated corpus, when one parse scores far higher than a"
D07-1070,P95-1026,0,0.291664,"f one another. Selecting a good feature means accepting all other features on that edge. It also means rejecting various other edges, because of the global constraints that a legal parse tree must give each word only one parent and must be free of cycles and, in 2 Note that since we are not trying to predict parts of speech, we treat the output of one or more automatic taggers as yet more inputs to edge feature functions. 668 the projective case, crossings. Our observation is that this situation is ideal for so-called “bootstrapping,” “co-training,” or “minimally supervised” learning methods (Yarowsky, 1995; Blum and Mitchell, 1998; Yarowsky and Wicentowski, 2000). Such methods should thrive when the right answer is overdetermined owing to redundant features and/or global constraints. Concretely, suppose we start by training a supervised parser on only 100 examples, using some regularization method to prevent overfitting to this set. While many features might truly be relevant to the task, only a few appear often enough in this small training set to acquire significantly positive or negative weights. Even this lightly trained parser may be quite sure of itself on some test sentences in a large u"
D07-1102,P05-1022,0,0.0711624,"3). We show that oracle parse accuracy1 of the output of our k-best parser is generally higher than the best reported results. We also present the results of a reranker based on a rich set of structural features, including features explicitly targeted at modeling non-projective configurations. Labeling of the dependency edges is accomplished by an edge labeler based on the same feature set as used in training the k-best MST parser. Introduction 2 Parser Description Reranking the output of a k-best parser has been shown to improve upon the best results of a stateof-the-art constituency parser (Charniak and Johnson, 2005). This is primarily due to the ability to incorporate complex structural features that cannot be modeled under a CFG. Recent work shows that k-best maximum spanning tree (MST) parsing and reranking is also viable (Hall, 2007). In the current work, we explore the k-best MST parsing paradigm along with a tree-based reranker. A system using the parsing techniques presented in this paper was entered in the CoNLL 2007 shared task competition (Nivre et al., 2007). This task evaluated parsing performance on 10 languages: Arabic, Basque, Our parser is composed of three components: a kbest MST parser,"
D07-1102,P07-1050,1,0.940587,"y targeted at modeling non-projective configurations. Labeling of the dependency edges is accomplished by an edge labeler based on the same feature set as used in training the k-best MST parser. Introduction 2 Parser Description Reranking the output of a k-best parser has been shown to improve upon the best results of a stateof-the-art constituency parser (Charniak and Johnson, 2005). This is primarily due to the ability to incorporate complex structural features that cannot be modeled under a CFG. Recent work shows that k-best maximum spanning tree (MST) parsing and reranking is also viable (Hall, 2007). In the current work, we explore the k-best MST parsing paradigm along with a tree-based reranker. A system using the parsing techniques presented in this paper was entered in the CoNLL 2007 shared task competition (Nivre et al., 2007). This task evaluated parsing performance on 10 languages: Arabic, Basque, Our parser is composed of three components: a kbest MST parser, a tree-labeler, and a tree-reranker. Log-linear models are used for each of the components independently. In this section we give an overview of the models, the training techniques, and the decoders. 2.1 MST Parsing, Rerankin"
D07-1102,P07-1077,1,0.833186,"calstring attributes. Additionally, we use surface-string distance between the parent and child, buckets of features indicating if a particular form/lemma/tag occurred between or next to the parent and child, and a branching feature indicating whether the child is to the left or right of the parent. Composite features, combining the above features are also included (e.g., a single feature combining branching, parent & child form, parent & child tag). The tree-based reranker includes the features described in (Hall, 2007) as well as features based on non-projective edge attributes explored in (Havelka, 2007a; Havelka, 2007b). One set of features models relationships of nodes with their siblings, including valency and subcategorization. A second 964 set of features models global tree structure and includes features based on a node’s ancestors and the depth and size of its subtree. A third set of features models the interaction of word order and tree structure as manifested on individual edges, i.e., the features model language specific projective and nonprojective configurations. They include edge-based features corresponding to the global constraints of projectivity, planarity and well-nestednes"
D07-1102,N07-2016,1,0.797822,"calstring attributes. Additionally, we use surface-string distance between the parent and child, buckets of features indicating if a particular form/lemma/tag occurred between or next to the parent and child, and a branching feature indicating whether the child is to the left or right of the parent. Composite features, combining the above features are also included (e.g., a single feature combining branching, parent & child form, parent & child tag). The tree-based reranker includes the features described in (Hall, 2007) as well as features based on non-projective edge attributes explored in (Havelka, 2007a; Havelka, 2007b). One set of features models relationships of nodes with their siblings, including valency and subcategorization. A second 964 set of features models global tree structure and includes features based on a node’s ancestors and the depth and size of its subtree. A third set of features models the interaction of word order and tree structure as manifested on individual edges, i.e., the features model language specific projective and nonprojective configurations. They include edge-based features corresponding to the global constraints of projectivity, planarity and well-nestednes"
D07-1102,W07-2416,0,0.028242,"llows us to explicitly model global syntactic phenomena. We describe the reranker features which include non-projective edge attributes. We provide an analysis of the errors made by our system and suggest changes to the models and features that might rectify the current system. Catalan, Chinese, Czech, English, Greek, Hungarian, Italian, and Turkish using data originating from a wide variety of dependency treebanks, and transformations of constituency-based treebanks (Hajiˇc et al., 2004; Aduriz et al., 2003; Mart´ı et al., 2007; Chen et al., 2003; B¨ohmov´a et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003). We show that oracle parse accuracy1 of the output of our k-best parser is generally higher than the best reported results. We also present the results of a reranker based on a rich set of structural features, including features explicitly targeted at modeling non-projective configurations. Labeling of the dependency edges is accomplished by an edge labeler based on the same feature set as used in training the k-best MST parser. Introduction 2 Parser Description Reranking the output of a k-best pars"
D07-1102,J93-2004,0,0.0282832,"sed reranking model allows us to explicitly model global syntactic phenomena. We describe the reranker features which include non-projective edge attributes. We provide an analysis of the errors made by our system and suggest changes to the models and features that might rectify the current system. Catalan, Chinese, Czech, English, Greek, Hungarian, Italian, and Turkish using data originating from a wide variety of dependency treebanks, and transformations of constituency-based treebanks (Hajiˇc et al., 2004; Aduriz et al., 2003; Mart´ı et al., 2007; Chen et al., 2003; B¨ohmov´a et al., 2003; Marcus et al., 1993; Johansson and Nugues, 2007; Prokopidis et al., 2005; Csendes et al., 2005; Montemagni et al., 2003; Oflazer et al., 2003). We show that oracle parse accuracy1 of the output of our k-best parser is generally higher than the best reported results. We also present the results of a reranker based on a rich set of structural features, including features explicitly targeted at modeling non-projective configurations. Labeling of the dependency edges is accomplished by an edge labeler based on the same feature set as used in training the k-best MST parser. Introduction 2 Parser Description Reranking"
D07-1102,P05-1012,0,0.0460943,"subcategorization, ancestry relationships, and sibling interactions, as well as features capturing the global structure of dependency trees, aimed primarily at modeling language specific non-projective configurations. We assign dependency labels to entire trees, rather than predicting the labels during tree construction. Given that we have a reranking process, we can label the k-best tree hypotheses output from our MST parser, and rerank the labeled trees. We have explored both labeled and unlabeled reranking. In the latter case, we simply label the maximal unlabeled tree. 2.1.1 MST Training McDonald et al. (2005) present a technique for training discriminative models for dependency parsing. The edge-factored models we use for MST parsing are closely related to those described in the previous work, but allow for the efficient computation of normalization factors which are required for first and second-order (gradient-based) training techniques. We consider two estimation procedures for parent-prediction models. A parent-prediction model assigns a conditional score s(g|d) for every parent-child pair (we denote the parent/governor g, and the d), where s(g|d) = P child/dependent 0 s(g, d)/ g0 s(g , d). In"
D07-1102,D07-1014,1,0.878858,"Missing"
D07-1102,D07-1096,0,\N,Missing
D08-1016,P05-1022,0,0.0856779,"Missing"
D08-1016,J07-2003,0,0.00538807,"as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008). We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links Mij (not constrained by T REE/PT REE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; Buch-Kromann, 2006). Other parsing-like problems that could be attacked with BP appear in syntax-based machine translation. Decoding is very expensive with a synchronous grammar composed with an n-gram language model (Chiang, 2007)—but our footnote 10 suggests that BP might incorporate a language model rapidly. String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG (Wu, 1997)—but Duchi et al. (2007) show how to incorporate bipartite matching into max-product BP. Finally, we can take advantage of improvements to BP proposed in the context of other applications. For example, instead of updating all messages in parallel at every iteration, it is empirically faster to serialize updates using a priority queue (Elidan et al., 2006; Sutton and McCallum, 2007).31 31 These m"
D08-1016,P99-1059,1,0.280355,"projective parsing, we might prefer (but not require) that a parent and child be on the same side of the grandparent. S IB. Shorthand for the family of O(n3 ) binary factors {PAIRij,ik }, which judge whether two children of the same parent are compatible. E.g., a given verb may not like to have two noun children both to its left.6 The children do not need to be adjacent. C HILD S EQ. A family of O(n) global factors. C HILD S EQi scores i’s sequence of children; hence it consults all variables of the form Lij . The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999). If 5 has children 2, 7, 9 under A, then C HILD S EQi is a product of subfactors of the form PAIR5#,57 , PAIR 57,59 , PAIR 59,5# (right child sequence) and PAIR 5#,52 , PAIR 52,5# (left child sequence). N O C ROSS. A family of O(n2 ) global constraints. If the parent-to-j link crosses the parent-to-` link, then N O C ROSSj` fires with a value that depends only on j and `. (If j and ` do not each have exactly one parent, N O C ROSSj` fires with value 0; i.e., it incorporates E XACTLY 1j and E XACTLY 1` .)7 TAGi is a unary factor that evaluates whether Ti ’s value is consistent with W (especial"
D08-1016,H05-1036,1,0.772834,"gation was written in C++. The BP parser averaged 1.8 seconds per sentence for non-projective parsing and 1.5 seconds per sentence for projective parsing (1.2 and 0.9 seconds/sentence for ≤ 40 words), using our standard setup, which included five iterations of BP and the final MBR tree decoding pass. In our tables, we boldface the best result in each column along with any results that are not significantly worse (paired permutation test, p &lt; .05). 27 This dominates runtime, and probably slows down all our parsers by a factor of 4–11 owing to known inefficiencies in the Dyna prototype we used (Eisner et al., 2005). 0.3 (a) 0.2 Input length (b) 0.1 40 words 50 words 60 words 70 words 0.0 Error relative to exact MBR 0.4 2 iterations of BP 3 iterations of BP 5 iterations of BP 10 iterations of BP MBR DP 0 20 40 60 Parsing time in seconds Figure 4: Runtime vs. search error after different numbers of BP iterations. This shows the simpler model of Fig. 2, where DP is still relatively fast. 8.4 Faster higher-order projective parsing We built a first-order projective parser—one that uses only factors PT REE and L INK—and then compared the cost of incorporating second-order factors, G RAND and C HILD S EQ, by B"
D08-1016,C96-1058,1,0.911972,"ees having edge i → j. We perform these combinatorial sums by calling a first-order parsing algorithm, with edge weights q¯ij . Thus, as outlined in §2, a first-order parser is called each time we propagate through the global T REE or PT REE constraint, using edge weights that include the first-order L INK factors but also multiply in any current messages from higher-order factors. The parsing algorithm simultaneously computes the partition function b(), and all O(n2 ) marginal beliefs b(Lij = true). For PT REE (projective), it is the inside-outside version of a dynamic programming algorithm (Eisner, 1996). For T REE (nonprojective), Koo et al. (2007) and Smith and Smith (2007) show how to employ the matrix-tree theorem. In both cases, the total time is O(n3 ).19 N O C ROSSj` must sum over assignments to O(n) neighboring variables {Lij } and {Lk` }. The nonzero summands are assignments where j and ` def each Q have exactly Q one parent. At step 1, π = At step 2, the i qLij (false) · k qLk` (false). marginal belief b(Lij = true) sums over the n nonzero P assignments containing i → j. It is π · q¯Lij · ¯Lk` · PAIRij,k` , where PAIRij,k` is xj` if i → j kq crosses k → ` and is 1 otherwise. xj` is"
D08-1016,P96-1024,0,0.334433,"eration, via serial forward and backward sweeps through the subfactors. Handling the subfactors in parallel, (3)–(4), would need O(n) iterations. 23 Ignoring the treatment of boundary symbols “#” (see §3.4). 22 In our experiments, we actually take the edge weights to be not the messages q¯Lij from the links, def but the full beliefs ¯bL at the links (where ¯bL = ij ij log bLij (true)/bLij (false)). These are passed into a fast algorithm for maximum spanning tree (Tarjan, 1977) or maximum projective spanning tree (Eisner, 1996). This procedure is equivalent to minimum Bayes risk (MBR) parsing (Goodman, 1996) with a dependency accuracy loss function. Notice that the above decoding approaches do not enforce any hard constraints other than T REE in the final output. In addition, they only recover values of the Lij variables. They marginalize over other variables such as tags and link roles. This solves the problem of “nuisance” variables (which merely fragment probability mass among refinements of a parse). On the other hand, it may be undesirable for variables whose values we desire to recover.24 7 Training Our training method also uses beliefs computed by BP, but at the factors. We choose the weig"
D08-1016,J99-4004,0,0.0286991,"lso uses beliefs computed by BP, but at the factors. We choose the weight vector θ by maximizing the log-probability of training data 24 An alternative is to attempt to find the most probable (“MAP”) assignment to all variables—using the max-product algorithm (footnote 13) or one of its recent variants. The estimated marginal beliefs become “max marginals,” which assess the 1-best assignment consistent with each value of the variable. We can indeed build max-product propagators for our global constraints. PT REE still propagates in O(n3 ) time: simply change the first-order parser’s semiring (Goodman, 1999) to use max instead of sum. T REE requires O(n4 ) time: it seems that the O(n2 ) max marginals must be computed separately, each requiring a separate call to an O(n2 ) maximum spanning tree algorithm (Tarjan, 1977). If max-product BP converges, we may simply output each variable’s favorite value (according to its belief), if unique. However, max-product BP tends to be unstable on loopy graphs, and we may not wish to wait for full convergence in any case. A more robust technique for extracting an assignment is to mimic Viterbi decoding, and “follow backpointers” of the max-product computation a"
D08-1016,P08-1067,0,0.0828718,"proximation technique from machine learning, namely, loopy belief propagation (BP). In this paper, we show that BP can be used to train and decode complex parsing models. Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem.1 2 Overview and Related Work We wish to make a dependency parse’s score depend on higher-order features, which consider ar∗ This work was supported by the Human Language Technology Center of Excellence. 1 As do constraint relaxation (Tromble and Eisner, 2006) and forest reranking (Huang, 2008). In contrast, generic NP-hard solution techniques like Integer Linear Programming (Riedel and Clarke, 2006) know nothing about optimal substructure. bitrary interactions among two or more edges in the parse (and perhaps also other latent variables such as part-of-speech tags or edge labels). Such features can help accuracy—as we show. Alas, they raise the polynomial runtime of projective parsing, and render non-projective parsing NP-hard. Hence we seek approximations. We will show how BP’s “message-passing” discipline offers a principled way for higher-order features to incrementally adjust t"
D08-1016,D07-1015,0,0.0469871,"mbinatorial sums by calling a first-order parsing algorithm, with edge weights q¯ij . Thus, as outlined in §2, a first-order parser is called each time we propagate through the global T REE or PT REE constraint, using edge weights that include the first-order L INK factors but also multiply in any current messages from higher-order factors. The parsing algorithm simultaneously computes the partition function b(), and all O(n2 ) marginal beliefs b(Lij = true). For PT REE (projective), it is the inside-outside version of a dynamic programming algorithm (Eisner, 1996). For T REE (nonprojective), Koo et al. (2007) and Smith and Smith (2007) show how to employ the matrix-tree theorem. In both cases, the total time is O(n3 ).19 N O C ROSSj` must sum over assignments to O(n) neighboring variables {Lij } and {Lk` }. The nonzero summands are assignments where j and ` def each Q have exactly Q one parent. At step 1, π = At step 2, the i qLij (false) · k qLk` (false). marginal belief b(Lij = true) sums over the n nonzero P assignments containing i → j. It is π · q¯Lij · ¯Lk` · PAIRij,k` , where PAIRij,k` is xj` if i → j kq crosses k → ` and is 1 otherwise. xj` is some factor value defined by equation (2) to p"
D08-1016,D08-1017,0,0.127616,"elf. BP’s behavior in our setup can be understood intuitively as follows. Inasmuch as the first-order parser finds that edge e is probable, the higher-order features will kick in and discourage other edges e0 to the extent that they prefer not to coexist with e.2 Thus, the next call to the first-order parser assigns lower probabilities to parses that contain these e0 . (The method is approximate because a first-order parser must equally penalize all parses containing e0 , even those that do not in fact contain e.) This behavior is somewhat similar to parser stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a first-order parser derives some of its input features from the full 1-best output of another parser. In our method, a first-order parser derives such input features from its own previous full output (but probabilistic output rather than just 1best). This circular process is iterated to convergence. Our method also permits the parse to interact cheaply with other variables. Thus first-order parsing, part-of-speech tagging, and other tasks on a common input could mutually influence one another. Our method and its numerical details emerge naturally as an instance of the well-studied"
D08-1016,P05-1010,0,0.0182072,"ns and Future Work Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable. For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008). We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links Mij (not constrained by T REE/PT REE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; Buch-Kromann, 2006). Other parsing-like problems that could be attacked with BP appear in syntax-based machine translation. Decoding is very expensive with a synchronous grammar composed with an n-gram language model (Chiang, 2007)—but our footnote 10 suggests that BP might incorporate a language model rapidly. String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG (Wu, 1997)—but Duc"
D08-1016,E06-1011,0,0.341435,"E factor to train conditional log-linear parsing models of two highly non-projective languages, Danish and Dutch, as well as slightly non-projective English (§8.1). In all three languages, the first-order non-projective parser greatly overpredicts the number of crossing links. We thus added N O C ROSS factors, as well as G RAND and C HILD S EQ as before. All of these significantly improve the first-order baseline, though not necessarily cumulatively (Table 2). Finally, Table 2 compares loopy BP to a previously proposed “hill-climbing” method for approximate inference in non-projective parsing McDonald and Pereira (2006). Hill-climbing decodes our richest non-projective model by finding the best projective parse under that model—using slow, higherorder DP—and then greedily modifies words’ parents until the parse score (1) stops improving. Decoding N OT 2 AT M OST 1 E XACTLY 1 + N O 2C YCLE T REE PT REE Danish 81.8 (76.7) 85.4 (82.2) 85.7 (85.0) 85.0 (85.2) 85.5 (85.5) 85.8 Dutch 83.3 (75.0) 87.3 (86.3) 87.0 (86.7) 86.2 (86.7) 87.3 (87.3) 83.9 English 87.5 (66.4) 88.5 (84.6) 88.6 (86.0) 88.5 (86.2) 88.6 (88.6) 88.8 Table 3: After training a non-projective first-order model with T REE, decoding it with weaker c"
D08-1016,P05-1012,0,0.464359,"exp θh fh (A, W, m) (2) h∈features(Fm ) where θ is a learned, finite collection of weights and f is a corresponding collection of feature functions, some of which are used by Fm . (Note that fh is permitted to consult the observed input W . It also sees which factor Fm it is scoring, to support reuse of a single feature function fh and its weight θh by unboundedly many factors in a model.) L INK. A family of unary soft factors that judge the links in a parse A individually. L INKij fires iff Lij = true, and then its value depends on (i, j), W , and θ. Our experiments use the same features as McDonald et al. (2005). A first-order (or “edge-factored”) parsing model (McDonald et al., 2005) contains only L INK factors, along with a global T REE or PT REE factor. Though there are O(n2 ) link factors (one per Lij ), only n of them fire on any particular parse, since the global factor ensures that exactly n are true. We’ll consider various higher-order soft factors: PAIR . The binary factor PAIRij,k` fires with some value iff Lij and Lk` are both true. Thus, it penalizes or rewards a pair of links for being simultaneously present. This is a soft version of NAND. G RAND. Shorthand for the family of O(n3 ) bina"
D08-1016,P08-1108,0,0.0239291,"ering the interactions itself. BP’s behavior in our setup can be understood intuitively as follows. Inasmuch as the first-order parser finds that edge e is probable, the higher-order features will kick in and discourage other edges e0 to the extent that they prefer not to coexist with e.2 Thus, the next call to the first-order parser assigns lower probabilities to parses that contain these e0 . (The method is approximate because a first-order parser must equally penalize all parses containing e0 , even those that do not in fact contain e.) This behavior is somewhat similar to parser stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a first-order parser derives some of its input features from the full 1-best output of another parser. In our method, a first-order parser derives such input features from its own previous full output (but probabilistic output rather than just 1best). This circular process is iterated to convergence. Our method also permits the parse to interact cheaply with other variables. Thus first-order parsing, part-of-speech tagging, and other tasks on a common input could mutually influence one another. Our method and its numerical details emerge naturally as an instan"
D08-1016,W06-1616,0,0.362548,"r, we show that BP can be used to train and decode complex parsing models. Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem.1 2 Overview and Related Work We wish to make a dependency parse’s score depend on higher-order features, which consider ar∗ This work was supported by the Human Language Technology Center of Excellence. 1 As do constraint relaxation (Tromble and Eisner, 2006) and forest reranking (Huang, 2008). In contrast, generic NP-hard solution techniques like Integer Linear Programming (Riedel and Clarke, 2006) know nothing about optimal substructure. bitrary interactions among two or more edges in the parse (and perhaps also other latent variables such as part-of-speech tags or edge labels). Such features can help accuracy—as we show. Alas, they raise the polynomial runtime of projective parsing, and render non-projective parsing NP-hard. Hence we seek approximations. We will show how BP’s “message-passing” discipline offers a principled way for higher-order features to incrementally adjust the numerical edge weights that are fed to a fast first-order parser. Thus the first-order parser is influenc"
D08-1016,1993.iwpt-1.22,0,0.132554,"e parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008). We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links Mij (not constrained by T REE/PT REE) that augment the parse with representations of control, binding, etc. (Sleator and Temperley, 1993; Buch-Kromann, 2006). Other parsing-like problems that could be attacked with BP appear in syntax-based machine translation. Decoding is very expensive with a synchronous grammar composed with an n-gram language model (Chiang, 2007)—but our footnote 10 suggests that BP might incorporate a language model rapidly. String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG (Wu, 1997)—but Duchi et al. (2007) show how to incorporate bipartite matching into max-product BP. Finally, we can take advantage of improvements to BP proposed in the context"
D08-1016,D07-1014,1,0.244897,"lling a first-order parsing algorithm, with edge weights q¯ij . Thus, as outlined in §2, a first-order parser is called each time we propagate through the global T REE or PT REE constraint, using edge weights that include the first-order L INK factors but also multiply in any current messages from higher-order factors. The parsing algorithm simultaneously computes the partition function b(), and all O(n2 ) marginal beliefs b(Lij = true). For PT REE (projective), it is the inside-outside version of a dynamic programming algorithm (Eisner, 1996). For T REE (nonprojective), Koo et al. (2007) and Smith and Smith (2007) show how to employ the matrix-tree theorem. In both cases, the total time is O(n3 ).19 N O C ROSSj` must sum over assignments to O(n) neighboring variables {Lij } and {Lk` }. The nonzero summands are assignments where j and ` def each Q have exactly Q one parent. At step 1, π = At step 2, the i qLij (false) · k qLk` (false). marginal belief b(Lij = true) sums over the n nonzero P assignments containing i → j. It is π · q¯Lij · ¯Lk` · PAIRij,k` , where PAIRij,k` is xj` if i → j kq crosses k → ` and is 1 otherwise. xj` is some factor value defined by equation (2) to penalize or reward the cross"
D08-1016,N06-1054,1,0.753759,"roximations. We propose borrowing a different approximation technique from machine learning, namely, loopy belief propagation (BP). In this paper, we show that BP can be used to train and decode complex parsing models. Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem.1 2 Overview and Related Work We wish to make a dependency parse’s score depend on higher-order features, which consider ar∗ This work was supported by the Human Language Technology Center of Excellence. 1 As do constraint relaxation (Tromble and Eisner, 2006) and forest reranking (Huang, 2008). In contrast, generic NP-hard solution techniques like Integer Linear Programming (Riedel and Clarke, 2006) know nothing about optimal substructure. bitrary interactions among two or more edges in the parse (and perhaps also other latent variables such as part-of-speech tags or edge labels). Such features can help accuracy—as we show. Alas, they raise the polynomial runtime of projective parsing, and render non-projective parsing NP-hard. Hence we seek approximations. We will show how BP’s “message-passing” discipline offers a principled way for higher-order"
D08-1016,J97-3002,0,0.0486573,"Missing"
D08-1016,W07-2216,0,\N,Missing
D08-1016,D07-1096,0,\N,Missing
D08-1028,C96-1043,0,0.0299051,"ents. In Section 5, we present a new approach to automatically identify emerging templates – texts that are repeatedly created by agents and are similar to each other but distinct from the current template text. We use AHT as the metric to minimize for automatic identification of emerging templates. We discuss some of the issues concerning this work in Section 6 and conclude in Section 7. 2 Related Work There are few threads of research that are relevant to the work presented in this paper. First, the topic of email response generation in the context of customer care has been investigated by (Coch, 1996; Lapalme and Kosseim, 2003; Zukerman and Marom, 2007). In (Coch, 1996), the authors model multi-sentence generation of response letters to customer complaints in French. The generation model is carefully crafted for the domain using domain-specific rules for conceptual planning, rhetorical relations and surface word order operators. They show that their 265 approach performs better than predefined templates and slightly worse than human generated responses. In (Lapalme and Kosseim, 2003), the authors explore three different approaches based on classification, case-based reasoning and question"
D08-1028,W02-1020,0,0.0332681,"sitory of documents or the return results of a search query. These maps are primarily designed for exploration and navigation through the document space. While the underlying algorithm we use to illustrate the text edits is similar to the one used in text map visualizations, our focus in this paper is to provide a mechanism for template designers to quickly identify the variants of a template sentence created by the agents. A third thread is in the context of humanassisted machine translation, where a human translator post-edits the output of a machine translation system (Foster et al., 1997; Foster et al., 2002; Och et al., 2003). In order to improve the efficiency of a human translator, the k-best output of a translation system could be displayed as word or phrase choices which are color coded based on the confidence value assigned by the translation model. While the approach we follow is partly motivated by the postediting paradigm, there are significant differences in the context we apply this approach. In the context of this paper, the template designer is presented a summary of the set of variants created by each agent for each sentence of the template. The task of the template designer is to u"
D08-1028,E03-1032,0,0.0137326,"r the return results of a search query. These maps are primarily designed for exploration and navigation through the document space. While the underlying algorithm we use to illustrate the text edits is similar to the one used in text map visualizations, our focus in this paper is to provide a mechanism for template designers to quickly identify the variants of a template sentence created by the agents. A third thread is in the context of humanassisted machine translation, where a human translator post-edits the output of a machine translation system (Foster et al., 1997; Foster et al., 2002; Och et al., 2003). In order to improve the efficiency of a human translator, the k-best output of a translation system could be displayed as word or phrase choices which are color coded based on the confidence value assigned by the translation model. While the approach we follow is partly motivated by the postediting paradigm, there are significant differences in the context we apply this approach. In the context of this paper, the template designer is presented a summary of the set of variants created by each agent for each sentence of the template. The task of the template designer is to use this tool to sel"
D08-1028,P02-1040,0,0.0822897,"se. The time taken for each of these phases typically varies depending on the customer’s account and the problem category. Nevertheless, we assume that the times for these phases is mostly a constant for a given problem category, and hence the results presented in this paper need to be interpreted on a per problem category basis. A second limitation of the approach presented in this paper is that the metric used to measure the similarity between strings (n-gram overlap) is only a crude approximation of an ideal semantic similarity metric. There are however other similarity metrics (e.g. BLEU (Papineni et al., 2002)) which could be used equally well. The purpose of this paper is to illustrate the possibility of analysis of responses using one particular instantiation of the similarity metric. In spite of the several directions that this work can 272 Acknowledgments We would like to thanks Mazin Gilbert, Junlan Feng, Narendra Gupta and Wenling Hsu for the discussions during the course of this work. We also thank the members who generously offered to their support to provide us with data used in this study without which this work would not have been possible. We thank the anonymous reviewers for their usef"
D09-1086,P05-1012,0,0.402187,"e.1 Given the two sentences w, w0 , our probability distribution over possible graphs considers local features of the parses, the alignment, and both jointly. Thus, we learn what local syntactic configurations tend to occur in each language and how they correspond across languages. As a result, we might learn that parses are “mostly synchronous,” but that there are some systematic cross-linguistic 1 i + X wj gj (t, t0 , a, w, w0 ) j The features f look only at target words and dependencies. In the conditional models of §3 and §6, these features are those of an edge-factored dependency parser (McDonald et al., 2005). In the generative models of §5, f has the form of a dependency model with valence (Klein and Manning, 2004). All models, for instance, have a feature template that considers the parts of speech of a potential parent-child relation. In order to benefit from the source language, we also need to include bilingual features g. When scoring a candidate target dependency link from word x → y, these features consider the relationship of their corresponding source words x0 and y 0 . (The correspondences are determined by the alignment a.) For instance, the source tree t0 may contain the link x0 → y 0"
D09-1086,D08-1092,0,0.119978,"oordination show up in human-produced dependency treebanks. (The other possibility is a reverse Mel’ˇcuk scheme.) These treebanks also differ on other conventions. Figure 2: With the English tree and alignment provided by a parser and aligner at test time, the Chinese parser finds the correct dependencies (see §6). A monolingual parser’s incorrect edges are shown with dashed lines. This model should enable us to convert the original large source corpus to target style, giving us additional training data in the target style. 1.2 recover t), as well as full joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008). In this paper, we condition on the 1-best source tree t0 . As for the alignment a, our models either condition on the 1-best alignment or integrate the alignment out. Our models are thus of the form p(t |w, w0 , t0 , a) or, in the generative case, p(w, t, a |w0 , t0 ). We intend to consider other formulations in future work. So far, this is very similar to the monolingual parser adaptation scenario, but there are a few key differences. Since the source and target sentences in the bitext are in different languages, there is no longer a trivial alignment between the words of the source and tar"
D09-1086,W06-3104,1,0.752311,"t, a, t0 |w, w0 ). Such a joint model captures how t, a, t0 mutually constrain each other, so that even partial knowledge of some of these three variables can help us to recover the others when training or decoding on bilingual text. This idea underlies a number of recent papers on syntax-based alignment (using t and t0 to better recover a), grammar induction from bitext (using a to better recover t and t0 ), parser projection (using t0 and a to better 823 divergences and some instances of sloppy (nonparallel or inexact) translation. Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). All the models in this paper are conditioned on the source tree t0 . Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to w0 and thus have the form p(t |w, w0 , t0 , a); the unsupervised, generative projection models in §5 have the form p(w, t, a |w0 , t0 ). The score s of a given tuple of trees, words, and ali"
D09-1086,P09-1053,0,0.0520215,"cover the others when training or decoding on bilingual text. This idea underlies a number of recent papers on syntax-based alignment (using t and t0 to better recover a), grammar induction from bitext (using a to better recover t and t0 ), parser projection (using t0 and a to better 823 divergences and some instances of sloppy (nonparallel or inexact) translation. Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). All the models in this paper are conditioned on the source tree t0 . Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to w0 and thus have the form p(t |w, w0 , t0 , a); the unsupervised, generative projection models in §5 have the form p(w, t, a |w0 , t0 ). The score s of a given tuple of trees, words, and alignment can thus be written as a dot product of weights w with features f and g: X s(t, t0 , a, w, w0 ) = wi fi (t, w) phrase that confused the undertrained monolingual pa"
D09-1086,P07-1033,0,0.0319557,"Missing"
D09-1086,P06-1072,1,0.684427,"t, a, t0 |w, w0 ). Such a joint model captures how t, a, t0 mutually constrain each other, so that even partial knowledge of some of these three variables can help us to recover the others when training or decoding on bilingual text. This idea underlies a number of recent papers on syntax-based alignment (using t and t0 to better recover a), grammar induction from bitext (using a to better recover t and t0 ), parser projection (using t0 and a to better 823 divergences and some instances of sloppy (nonparallel or inexact) translation. Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). All the models in this paper are conditioned on the source tree t0 . Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to w0 and thus have the form p(t |w, w0 , t0 , a); the unsupervised, generative projection models in §5 have the form p(w, t, a |w0 , t0 ). The score s of a given tuple of trees, words, and ali"
D09-1086,D07-1070,1,0.836522,"ser corrects additional errors in the second half of this sentence, which has been removed to improve legibility.) The parser is able to recover the long-distance dependency from the first Chinese word (China) to the last (begun), while skipping over the intervening noun Parser Projection For many target languages, however, we do not have the luxury of a large parsed “source corpus” in the language, even one in a different style or domain as above. Thus, we may seek other forms of data to augment our small target corpus. One option would be to leverage unannotated text (McClosky et al., 2006; Smith and Eisner, 2007). But we can also try to transfer syntactic information from a parsed source corpus in another language. This is an extreme case of out-of-domain data. This leads to the second task of this paper: learning a statistical model to transform a syntactic analysis of a sentence in one language into an analysis of its translation. Tree transformations are often modeled with synchronous grammars. Suppose we are given a sentence w0 in the “source” language and its translation w into the “target” language. Their syntactic parses t0 and t are presumably not independent, but will tend to have some parall"
D09-1086,D08-1016,1,0.0785439,"Missing"
D09-1086,P09-1042,0,0.530933,"Missing"
D09-1086,W04-3207,1,0.637558,"chemes for annotating coordination show up in human-produced dependency treebanks. (The other possibility is a reverse Mel’ˇcuk scheme.) These treebanks also differ on other conventions. Figure 2: With the English tree and alignment provided by a parser and aligner at test time, the Chinese parser finds the correct dependencies (see §6). A monolingual parser’s incorrect edges are shown with dashed lines. This model should enable us to convert the original large source corpus to target style, giving us additional training data in the target style. 1.2 recover t), as well as full joint parsing (Smith and Smith, 2004; Burkett and Klein, 2008). In this paper, we condition on the 1-best source tree t0 . As for the alignment a, our models either condition on the 1-best alignment or integrate the alignment out. Our models are thus of the form p(t |w, w0 , t0 , a) or, in the generative case, p(w, t, a |w0 , t0 ). We intend to consider other formulations in future work. So far, this is very similar to the monolingual parser adaptation scenario, but there are a few key differences. Since the source and target sentences in the bitext are in different languages, there is no longer a trivial alignment between the w"
D09-1086,D09-1023,0,0.0343813,"ilingual text. This idea underlies a number of recent papers on syntax-based alignment (using t and t0 to better recover a), grammar induction from bitext (using a to better recover t and t0 ), parser projection (using t0 and a to better 823 divergences and some instances of sloppy (nonparallel or inexact) translation. Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). All the models in this paper are conditioned on the source tree t0 . Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to w0 and thus have the form p(t |w, w0 , t0 , a); the unsupervised, generative projection models in §5 have the form p(w, t, a |w0 , t0 ). The score s of a given tuple of trees, words, and alignment can thus be written as a dot product of weights w with features f and g: X s(t, t0 , a, w, w0 ) = wi fi (t, w) phrase that confused the undertrained monolingual parser. Although, due to the auxiliary verb, “China”"
D09-1086,P09-1009,0,0.106485,"Missing"
D09-1086,D09-1127,0,0.0627136,"Missing"
D09-1086,D07-1003,0,0.0329947,"of these three variables can help us to recover the others when training or decoding on bilingual text. This idea underlies a number of recent papers on syntax-based alignment (using t and t0 to better recover a), grammar induction from bitext (using a to better recover t and t0 ), parser projection (using t0 and a to better 823 divergences and some instances of sloppy (nonparallel or inexact) translation. Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). All the models in this paper are conditioned on the source tree t0 . Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to w0 and thus have the form p(t |w, w0 , t0 , a); the unsupervised, generative projection models in §5 have the form p(w, t, a |w0 , t0 ). The score s of a given tuple of trees, words, and alignment can thus be written as a dot product of weights w with features f and g: X s(t, t0 , a, w, w0 ) = wi fi (t, w) phrase t"
D09-1086,P02-1017,0,0.0603758,"ntain posterior probabilities or null links, nor do they distinguish which links are in the IBM Model intersection, we treated all links as equally likely when learning the QG. Target language words unaligned to any source language words were the only nodes allowed to align to NULL in QG derivations. We parsed the English side of the bitext with the projective dependency parser described by McDonald et al. (2005) trained on the Penn Treebank §§2–20. Much previous work on unsupervised grammar induction has used gold-standard partof-speech tags (Smith and Eisner, 2006b; Klein and Manning, 2004; Klein and Manning, 2002). While there are no gold-standard tags for the Europarl bitext, we did train a conditional Markov Details of EM Training As in previous work on grammar induction, we learn the DMV from part-of-speech-tagged targetlanguage text. We use expectation maximization (EM) to maximize the likelihood of the data. Since the likelihood function is nonconvex in the unsupervised case, our choice of initial parameters can have a significant effect on the outcome. Although we could also try many random starting points, the initializer in Klein and Manning (2004) performs quite well. The base dependency parse"
D09-1086,P04-1061,0,0.878939,"res of the parses, the alignment, and both jointly. Thus, we learn what local syntactic configurations tend to occur in each language and how they correspond across languages. As a result, we might learn that parses are “mostly synchronous,” but that there are some systematic cross-linguistic 1 i + X wj gj (t, t0 , a, w, w0 ) j The features f look only at target words and dependencies. In the conditional models of §3 and §6, these features are those of an edge-factored dependency parser (McDonald et al., 2005). In the generative models of §5, f has the form of a dependency model with valence (Klein and Manning, 2004). All models, for instance, have a feature template that considers the parts of speech of a potential parent-child relation. In order to benefit from the source language, we also need to include bilingual features g. When scoring a candidate target dependency link from word x → y, these features consider the relationship of their corresponding source words x0 and y 0 . (The correspondences are determined by the alignment a.) For instance, the source tree t0 may contain the link x0 → y 0 , which would cause a feature for monotonic projection to fire for the x → y edge. If, on the other hand, y"
D09-1086,P04-1060,0,0.214054,"Missing"
D09-1086,D08-1017,0,0.0908269,"Missing"
D09-1086,P06-1043,0,0.0449812,"nes. (The bilingual parser corrects additional errors in the second half of this sentence, which has been removed to improve legibility.) The parser is able to recover the long-distance dependency from the first Chinese word (China) to the last (begun), while skipping over the intervening noun Parser Projection For many target languages, however, we do not have the luxury of a large parsed “source corpus” in the language, even one in a different style or domain as above. Thus, we may seek other forms of data to augment our small target corpus. One option would be to leverage unannotated text (McClosky et al., 2006; Smith and Eisner, 2007). But we can also try to transfer syntactic information from a parsed source corpus in another language. This is an extreme case of out-of-domain data. This leads to the second task of this paper: learning a statistical model to transform a syntactic analysis of a sentence in one language into an analysis of its translation. Tree transformations are often modeled with synchronous grammars. Suppose we are given a sentence w0 in the “source” language and its translation w into the “target” language. Their syntactic parses t0 and t are presumably not independent, but will"
D09-1086,D07-1112,0,\N,Missing
D09-1092,J93-2003,0,0.0347959,"Missing"
D09-1092,P08-1088,0,0.452554,"Missing"
D09-1092,D08-1038,0,0.102748,"ble, calling for tools that can characterize content in many languages. We introduce a polylingual topic model that discovers topics aligned across multiple languages. We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages. 1 Introduction Statistical topic models have emerged as an increasingly useful analysis tool for large text collections. Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006). Much of this work, however, has occurred in monolingual contexts. In an increasingly connected world, the ability to access documents in many languages has become both a strategic asset and a personally enriching experience. In this paper, we present the polylingual topic model (PLTM). We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedin"
D09-1092,W02-0902,0,0.175315,"Missing"
D09-1092,2007.mtsummit-tutorials.1,0,0.0100598,"anguages). There are many potential applications for polylingual topic models. Although research literature is typically written in English, bibliographic 880 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 880–889, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP the contents of collections in unfamiliar languages and identify trends in topic prevalence. 2 z α Related Work ... z Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing, 2007). Tam, Lane and Schultz (Tam et al., 2007) also show improvements in machine translation using bilingual topic models. Both of these translation-focused topic models infer word-to-word alignments as part of their inference procedures, which would become exponentially more complex if additional languages were added. We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages. A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented"
D12-1067,W06-2920,0,0.0660709,"And what kind of reduction in runtime does this reduction in edges lead to? We have also pointed out that our outer bound on the grandparent polytope of legal edge and grandparent vectors is tighter than the one presented by Martins et al. (2009). What effect does this bound have on the number of fractional solutions and the overall accuracy? To answer these questions we will focus on a set of non-projective grandparent models, but point out that our method and formulation can be easily extended to projective parsing as well as other types of higher order edges. We use the Danish test data of Buchholz and Marsi (2006) and the Italian and Hungarian test datasets of Nivre et al. (2007). 6.1 Impact of Price and Cut Table 1 compares brute force optimization (BF) with the full model, in spirit of Martins et al. (2009), to running parse, price and cut (PPC) on the same model. This model contains all constraints presented in 3.2. The table shows the average number of parsed sentences per second, the average objective, number of grandparent edges scored and added, all relative to the brute force approach. We also present the average unlabeled accuracy, and the percentage of sentences with integer solutions. This n"
D12-1067,P05-1022,0,0.0635364,"ss, and utility for column generation. Other recent LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation"
D12-1067,D07-1001,0,0.0237009,"Missing"
D12-1067,N07-1011,1,0.832712,"maller integrality gap and higher speed. 1 Introduction Many problems in NLP, and structured prediction in general, can be cast as finding high-scoring structures based on a large set of candidate parts. For example, in second order graph-based dependency parsing (Kübler et al., 2009) we have to choose a quadratic number of first order and a cubic number of second order edges such that the graph is both high-scoring and a tree. In coreference, we have to select high-scoring clusters of mentions from an exponential number of candidate clusters, such that each mention is in exactly one cluster (Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008)"
D12-1067,N03-1016,0,0.0334835,"ed prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation between both A* and Column Generation based on an LP formulation of the shortest path problem. Roughly speaking, in this formulation any feasible dual assignments correspond to a consistent (and thus admissible) heuristic, and the corresponding reduced costs can be used as edge weights. Running Dijkstra’s algorithm with these weights then amounts to A*. Column generation for the shortest path problem can then be understood as a method to lazily construct a consistent heuristic. In every step this method finds edge"
D12-1067,P10-1001,0,0.313816,"2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (Koo and Collins, 2010; Martins et al., 2009; Riedel and Clarke, 2006). While such methods can be effective, they are more convoluted, often require training of addition models as well as tuning of thresholding hyper-parameters, and usually provide no guarantees of optimality. We present an approach that can solve problems with large sets of candidate parts without considering all of these parts in either optimization or scor732 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 732–743, Jeju Island, Korea, 12–14 July 201"
D12-1067,D10-1125,0,0.44312,"dependency parsing can be framed as Integer Linear Program (ILP), and efficiently solved using an off-theshelf optimizer if a cutting plane approach is used.1 Compared to tailor made dynamic programs, such generic solvers give the practitioner more modeling flexibility (Martins et al., 2009), albeit at the cost of efficiency. Likewise, compared to approximate solvers, ILP and Linear Program (LP) formulations can give strong guarantees of optimality. The study of Linear LP relaxations of dependency parsing has also lead to effective alternative methods for parsing, such as dual decomposition (Koo et al., 2010; Rush et al., 2010). As we see later, the capability of LP solvers to calculate dual solutions is also crucial for efficient and exact pruning. Note, however, that dynamic programs provide dual solutions as well (see section 4.5 for more details). 3.1 Arc-Factored Models To represent a parse y ∈ Y we first introduce an vector of variables z , hza ia where za is 1 if a ∈ y and 0 otherwise. With this representation parsing amounts to finding a vector z that corresponds to a P legal parse tree and that maximizes a za sa . One way to achieve this is to search through the convex hull of all legal"
D12-1067,P09-1039,0,0.244399,"of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (Koo a"
D12-1067,D11-1022,0,0.0338713,"with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clarke (2006). For the case of grandparent edges, our formulation also improves upon the outer bound of Martins et al. (2009) in terms of speed, tightness, and utility for column generation. Other recent LP relaxations are based on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in whi"
D12-1067,E06-1011,0,0.641719,"such that each mention is in exactly one cluster (Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prun"
D12-1067,P05-1012,0,0.0682936,"ected edges. Then a directed graph y ⊆ C is a legal dependency parse if and only if it is a tree over V rooted at vertex 0. Given a sentence x, we use Y to denote the set of its legal parses. Note that all of the above definitions depend on x, but for simplicity we omit this dependency in our notation. 2.1 Graph-based models define parametrized scoring functions that are trained to discriminate between correct and incorrect parse trees. So called arcfactored or first order models are the most basic variant of such functions: they assess the quality of a tree by scoring each edge in isolation (McDonald et al., 2005b; McDonald et al., 2005a). Formally, arcfactored models are scoring functions of the form X shh,mi (x, w) (1) s (y; x, w) = hh,mi∈y where w is a weight vector and shh,mi (x, w) scores the edge hh, mi with respect to sentence x and weights w. From here on we will omit both x and w from our notation if they are clear from the context. Given such a scoring function, parsing amounts to solving: X maximize shh,mi y hh,mi∈y (2) subject to y ∈ Y. 2.2 Dependency trees are representations of the syntactic structure of a sentence (Nivre et al., 2007). They determine, for each token of a sentence, the s"
D12-1067,H05-1066,0,0.482353,"Missing"
D12-1067,W06-1616,1,0.90248,"o evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all corresponding higher order edges (Koo and Collins, 2010; Martins et al., 2009; Riedel and Clarke, 2006). While such methods can be effective, they are more convoluted, often require training of addition models as well as tuning of thresholding hyper-parameters, and usually provide no guarantees of optimality. We present an approach that can solve problems with large sets of candidate parts without considering all of these parts in either optimization or scor732 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 732–743, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistic"
D12-1067,N10-1117,1,0.820419,"rams. It is well known that for each dynamic program there is an equivalent polynomial LP formulation (Martin et al., 1990). Roughly speaking, in this formulation primal variables correspond to state transitions, and dual variables to value functions (e.g., the forward scores in the Viterbi algorithm). In pilot studies we have already used DCG to speed up (exact) Viterbi on linear chains (Belanger et al., 2012). We believe it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row genera739 tion (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By us"
D12-1067,W04-2401,0,0.0270605,"Missing"
D12-1067,N12-1054,0,0.0281506,"sed on dual decomposition (Rush et al., 2010; Koo et al., 2010; Martins et al., 2011). These relaxations allow the practitioner to utilize tailor-made dynamic programs for tractable substructure, but still every edge needs to be scored. Given that column generation can also be applied in dynamic programs (see section 4.5), our algorithm could in fact accelerate dual decomposition parsing as well. Pruning methods are a major part of many structured prediction algorithms in general, and of parsing algorithms in particular (Charniak and Johnson, 2005; Martins et al., 2009; Koo and Collins, 2010; Rush and Petrov, 2012). Generally these methods follow a coarse-to-fine scheme in which simpler models filter out large fractions of edges. Such methods are effective, but require tuning of threshold parameters, training of additional models, and generally lead to more complex pipelines that are harder to analyze and have fewer theoretical guarantees. A* search (Ahuja et al., 1993) has been used to search for optimal parse trees, for example by Klein and Manning (2003) or, for dependency parsing, by Dienes et al. (2003). There is a direct relation between both A* and Column Generation based on an LP formulation of"
D12-1067,D10-1001,0,0.248793,"g can be framed as Integer Linear Program (ILP), and efficiently solved using an off-theshelf optimizer if a cutting plane approach is used.1 Compared to tailor made dynamic programs, such generic solvers give the practitioner more modeling flexibility (Martins et al., 2009), albeit at the cost of efficiency. Likewise, compared to approximate solvers, ILP and Linear Program (LP) formulations can give strong guarantees of optimality. The study of Linear LP relaxations of dependency parsing has also lead to effective alternative methods for parsing, such as dual decomposition (Koo et al., 2010; Rush et al., 2010). As we see later, the capability of LP solvers to calculate dual solutions is also crucial for efficient and exact pruning. Note, however, that dynamic programs provide dual solutions as well (see section 4.5 for more details). 3.1 Arc-Factored Models To represent a parse y ∈ Y we first introduce an vector of variables z , hza ia where za is 1 if a ∈ y and 0 otherwise. With this representation parsing amounts to finding a vector z that corresponds to a P legal parse tree and that maximizes a za sa . One way to achieve this is to search through the convex hull of all legal incidence vectors, k"
D12-1067,D08-1016,1,0.959552,"(Culotta et al., 2007). In segmentation of citation strings, we need to consider a quadratic number of possible segments such that every token is part of exactly one segment (Poon and Domingos, 2007). What makes such problems challenging is the large number of possible parts to consider. This number not only affects the cost of search or optimization but also slows down the process of scoring parts before they enter the optimization problem. For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al., 2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. But to even calculate the score for each part we need a cubic number of operations that usually involve expensive feature extraction. This step often becomes a major bottleneck in parsing, and structured prediction in general. Candidate parts can often be heuristically pruned. In the case of dependency parsing, previous work has used coarse-to-fine strategies where simpler first order models are used to prune unlikely first order edges, and hence all c"
D12-1067,N06-1054,0,0.0303788,"i on linear chains (Belanger et al., 2012). We believe it could be equally applied to dynamic programs for higher order dependency parsing. 5 Related Work Our work is most similar in spirit to the relaxation method presented by Riedel and Smith (2010) that incrementally adds second order edges to a graphical model based on a gain measure—the analog of our reduced cost. However, they always score every higher order edge, and also provide no certificates of optimality. Several works in parsing, and in MAP inference in general, perform some variant of row genera739 tion (Riedel and Clarke, 2006; Tromble and Eisner, 2006; Sontag and Jaakkola, 2007; Sontag et al., 2008). However, none of the corresponding methods lazily add columns, too. The cutting plane method of Riedel (2008) can omit columns, but only if their coefficient is negative. By using the notion of reduced costs we can also omit columns with positive coefficient. Niepert (2010) applies column generation, but his method is limited to the case of kBounded MAP Inference. Several ILP and LP formulations of dependency parsing have been proposed. Our formulation is inspired by Martins et al. (2009), and hence uses fewer constraints than Riedel and Clark"
D12-1067,D07-1096,1,\N,Missing
D12-1074,P11-1048,0,0.0509764,"al. (2009) proposed a variational method based on n-gram loss functions. More recently, Liu and Ihler (2011) analyzed message-passing algorithms for marginal MAP. In this paper, we adopt a simple minimum Bayes risk decoding scheme. First, we perform sumproduct belief propagation on the full factor graph. Then, we maximize the expected accuracy of the variables of interest, subject to any hard constraints on them (such as mutual exclusion among labels). In some cases with complex combinatorial constraints, this simple MBR scheme has proved more effective than exact decoding over all variables (Auli and Lopez, 2011). 3 Relation Extraction Performing a syntax-based NLP task in most realworld scenarios requires that the incoming data first be parsed using a pre-trained parsing model. For some tasks, like relation extraction, many data sets lack syntactic annotation and these circumstances persist even into the training phase. In this section we explore such scenarios and contrast the use of parser-provided syntactic annotation to marginalizing over latent representations of constituency or dependency syntax. We show the hidden syntactic models are not just competitive with these “oracle” models, but in som"
D12-1074,P07-1073,0,0.0112204,"ation. Indeed, syntactic features have long been an extremely useful source of information for relation extraction systems (Culotta and Sorensen, 2004; Mintz et al., 2009). Secondly, relation extraction has been a common task for pioneering efforts in processing data mined from the internet, and otherwise noisy or out-of-domain data. In particular, large noisily-annotated data sets have been generated by leveraging freely available knowledge bases such as Freebase (Bollacker et al., 2008; Mintz et al., 2009). Such data sets have been utilized successfully for relation extraction from the web (Bunescu and Mooney, 2007). 3.1 Model We present a simple model for representing relational structure, with the only variables present being a set of boolean-valued variables representing an undirected dependency between two entities, and an additional set of boolean label variables representing the type label of the relation. O(n2 ) • Let {Rel(i, j : 0 ≤ i &lt; j ≤ n} be boolean variables such that Rel(i, j) = true iff there is a relation spanning i to j. • Let {Rel-Label(i, j, λ) : λ ∈ L, and 0 ≤ i &lt; j ≤ n} be O(|L|n2 ) boolean variables such that Rel-Label(i, j, λ) = true iff there is a relation spanning i to j with re"
D12-1074,N10-1066,0,0.0121246,"formance between their parsing F1 and their SRL F1 were Japanese and German. As illustrated in Fig. 3, the correspondence between syntax and SRL are extremely, and systematically, poor. In this example our hidden structure model was able to assign strong beliefs to the latent syntactic variables which correspond to the correct predicate/argument pairs, allowing it to correctly identify three of the four SRL arguments when the joint model failed to recover one. 5 Related Work This work is perhaps mostly closely related to the Learning over Constrained Latent Representations (LCLR) framework of Chang et al. (2010). Their abstract problem formulation is identical: both paradigms seek to couple the end task to an intermediate representation which is not accessible to the learning algorithm. However much of the intent, scale, and methodology is different. LCLR aims to provide a flexible latent structure for increasing the representational power of the model in a useful way, and is demonstrated on tasks and domains where data availability is not a key concern. In contrast, while our hidden structure models may outperform their observed syntax counterparts, our focus is as much on alleviating the burden of"
D12-1074,P04-1054,0,0.00776923,"ow the hidden syntactic models are not just competitive with these “oracle” models, but in some configurations can actually outperform them. Relation extraction is the task of identifying semantic relations between sets of entities in text (as illustrated in Fig. 1b), and a good proving ground for latent syntactic methods for two reasons. First, because entities share a semantic relationship, under most linguistic analyses these entities will also share some syntactic relation. Indeed, syntactic features have long been an extremely useful source of information for relation extraction systems (Culotta and Sorensen, 2004; Mintz et al., 2009). Secondly, relation extraction has been a common task for pioneering efforts in processing data mined from the internet, and otherwise noisy or out-of-domain data. In particular, large noisily-annotated data sets have been generated by leveraging freely available knowledge bases such as Freebase (Bollacker et al., 2008; Mintz et al., 2009). Such data sets have been utilized successfully for relation extraction from the web (Bunescu and Mooney, 2007). 3.1 Model We present a simple model for representing relational structure, with the only variables present being a set of b"
D12-1074,N09-1037,0,0.0364923,"tactic in nature introduces its own unique set of problems. Large amounts of syntactically annotated data is difficult to obtain, costly to produce, and often tied to a particular domain that may vary greatly from that of the desired end task. Additionally, current systems often utilize only a small amount of the annotation for any particular task. For instance, performing named entity recognition (NER) jointly with constituent parsing has been shown to improve performance on both tasks, but the only aspect of the syntax which is leveraged by the NER component is the location of noun phrases (Finkel and Manning, 2009). By instead discovering a latent representation jointly with the end task we address all of these concerns, alleviating the need for any syntactic annotations, while simultaneously attempting to learn a latent syntax relevant to both the particular domain and structure of the end task. We propose a novel method which avoids the need for any syntactically annotated data when predicting a related NLP task. Our method couples latent syntactic representations, constrained to form valid dependency graphs or constituency parses, with the prediction task via specialized factors in a Markov random fi"
D12-1074,P96-1024,0,0.0523986,"n as consensus decoding—which has been shown to be NP-hard and without a polynomial time approximation scheme (Sima’an, 1996; 812 Casacuberta and Higuera, 2000). In the NLP community, these inference problems often arise when dealing with spurious ambiguity where multiple derivations can lead to the same derived structure. In tree substitution grammars, for instance, there may be many ways of combining elementary trees to produce the same output tree; in machine translation, many different elementary phrases or elementary tree pairs might produce the same output string. For syntactic parsing, Goodman (1996) proposed a variational method for summing out spurious ambiguity that was equivalent to minimum Bayes risk decoding (Goel and Byrne, 2000; Kumar and Byrne, 2004) with a constituent-recall loss function. For MT, May and Knight (2006) proposed methods for determinizing tree automata to reduce ambiguity, and Li et al. (2009) proposed a variational method based on n-gram loss functions. More recently, Liu and Ihler (2011) analyzed message-passing algorithms for marginal MAP. In this paper, we adopt a simple minimum Bayes risk decoding scheme. First, we perform sumproduct belief propagation on the"
D12-1074,N04-1022,0,0.0153013,"2000). In the NLP community, these inference problems often arise when dealing with spurious ambiguity where multiple derivations can lead to the same derived structure. In tree substitution grammars, for instance, there may be many ways of combining elementary trees to produce the same output tree; in machine translation, many different elementary phrases or elementary tree pairs might produce the same output string. For syntactic parsing, Goodman (1996) proposed a variational method for summing out spurious ambiguity that was equivalent to minimum Bayes risk decoding (Goel and Byrne, 2000; Kumar and Byrne, 2004) with a constituent-recall loss function. For MT, May and Knight (2006) proposed methods for determinizing tree automata to reduce ambiguity, and Li et al. (2009) proposed a variational method based on n-gram loss functions. More recently, Liu and Ihler (2011) analyzed message-passing algorithms for marginal MAP. In this paper, we adopt a simple minimum Bayes risk decoding scheme. First, we perform sumproduct belief propagation on the full factor graph. Then, we maximize the expected accuracy of the variables of interest, subject to any hard constraints on them (such as mutual exclusion among"
D12-1074,N10-1137,0,0.0154012,"and the connective factors between syntax and SRL. In the observed syntax model the Link variables are clamped to their correct values, with no need for a factor to coordinate them to form a valid tree. Finally, the hidden model comprises all layers, including a combinatorial syntactic constraint (a.) over syntactic variables. In this scenario all labels in (b.) are hidden at both training and test time. feature-rich classifiers to the parsed trees. Related work has recognized the large annotation burden the task demands, but aimed to keep the syntactic annotations and induce semantic roles (Lang and Lapata, 2010). In this section we will take the opposite approach, disregarding the syntactic annotations which we argue are more costly to acquire, as they require more formal linguistic training to produce. 4.1 Model We present a simple, flexible model for SRL in which sense predictions are made independently of the rest of the model, and argument predictions are made independently of each other. The model structure is composed as depicted in Fig. 2. • Let {Arg(i, j) : 0 ≤ i &lt; j ≤ n} be O(n2 ) boolean variables such that Arg(i, j) = true 816 iff predicate i takes token j as an argument. • Let {Role(i, j,"
D12-1074,P09-1067,0,0.0145018,"e. In tree substitution grammars, for instance, there may be many ways of combining elementary trees to produce the same output tree; in machine translation, many different elementary phrases or elementary tree pairs might produce the same output string. For syntactic parsing, Goodman (1996) proposed a variational method for summing out spurious ambiguity that was equivalent to minimum Bayes risk decoding (Goel and Byrne, 2000; Kumar and Byrne, 2004) with a constituent-recall loss function. For MT, May and Knight (2006) proposed methods for determinizing tree automata to reduce ambiguity, and Li et al. (2009) proposed a variational method based on n-gram loss functions. More recently, Liu and Ihler (2011) analyzed message-passing algorithms for marginal MAP. In this paper, we adopt a simple minimum Bayes risk decoding scheme. First, we perform sumproduct belief propagation on the full factor graph. Then, we maximize the expected accuracy of the variables of interest, subject to any hard constraints on them (such as mutual exclusion among labels). In some cases with complex combinatorial constraints, this simple MBR scheme has proved more effective than exact decoding over all variables (Auli and L"
D12-1074,N06-1045,0,0.0206681,"dealing with spurious ambiguity where multiple derivations can lead to the same derived structure. In tree substitution grammars, for instance, there may be many ways of combining elementary trees to produce the same output tree; in machine translation, many different elementary phrases or elementary tree pairs might produce the same output string. For syntactic parsing, Goodman (1996) proposed a variational method for summing out spurious ambiguity that was equivalent to minimum Bayes risk decoding (Goel and Byrne, 2000; Kumar and Byrne, 2004) with a constituent-recall loss function. For MT, May and Knight (2006) proposed methods for determinizing tree automata to reduce ambiguity, and Li et al. (2009) proposed a variational method based on n-gram loss functions. More recently, Liu and Ihler (2011) analyzed message-passing algorithms for marginal MAP. In this paper, we adopt a simple minimum Bayes risk decoding scheme. First, we perform sumproduct belief propagation on the full factor graph. Then, we maximize the expected accuracy of the variables of interest, subject to any hard constraints on them (such as mutual exclusion among labels). In some cases with complex combinatorial constraints, this sim"
D12-1074,H05-1066,0,0.0477086,"voting on the same unknown head word. • Oracle D-Parse, in which we also instantiate a full set of latent dependency syntax variables, and connect them to the baseline model using D-C ONNECT factors. Syntax variables are clamped to their true values. • Oracle C-Parse, the constituency syntax analogue of Oracle D-Parse. • Hidden D-Parse, which is an extension of Oracle D-Parse in which we connect all syntax variables to a DEP-T REE factor, syntax variables are unobserved, and are learned jointly with the end task. The features for latent syntax are a subset of those used in dependency parsing (McDonald et al., 2005). • Hidden C-Parse, the constituency syntax analogue of Hidden D-Parse. The feature set is similar but bigrams are taken over the words defining the constituent span, rather than the words defining the head/modifier relation. Figure 1: Latent Dependency coupling for the RE task. The D-C ONNECT factor expresses ternary connection relations because the shared head word of the proposed relation is unknown. As is convention, variables are represented by circles, factors by rectangles. We introduce six model scenarios. • Baseline, simply the arc-factored model consisting only of Rel and correspondi"
D12-1074,N09-1018,1,0.712589,"nglish, not only is the hidden marginalization method a suitable replacement for the syntactic trees provided by pre-trained, state-of-the-art models, but in both configurations we find that inducing an optimal hidden structure is preferable to the parser-produced annotations. On Chinese, where the data set is atypically small, we still observe improved performance 815 4 Semantic Role Labeling The task of semantic role labeling (SRL) aims to detect and label the semantic relationships between particular words, most commonly verbs (referred to in the domain as predicates), and their arguments (Meza-Ruiz and Riedel, 2009). In a manner similar to RE, there is a strong correlation between the presence of an SRL relation and there existing an underlying syntactic dependency, though this is not always expressed as directly as a 1-to-1 correspondence. This has historically motivated a reliance on syntactic annotation, and some of the most successful methods have simply applied a.) Syntactic Combinatorial Constraint DEP-Tree b.) Syntactic Layer Link 5, 1 Link 5, 2 Link 5, 3 D-Connect 5, 1 D-Connect 5, 2 D-Connect 5, 3 Arg 5, 2 role A1 Link 5, n . D-Connect 5, n Arg 5, 3 At Most 1 At Most 1 role A0 . d.) Sense Predic"
D12-1074,P09-1113,0,0.0117712,"ls are not just competitive with these “oracle” models, but in some configurations can actually outperform them. Relation extraction is the task of identifying semantic relations between sets of entities in text (as illustrated in Fig. 1b), and a good proving ground for latent syntactic methods for two reasons. First, because entities share a semantic relationship, under most linguistic analyses these entities will also share some syntactic relation. Indeed, syntactic features have long been an extremely useful source of information for relation extraction systems (Culotta and Sorensen, 2004; Mintz et al., 2009). Secondly, relation extraction has been a common task for pioneering efforts in processing data mined from the internet, and otherwise noisy or out-of-domain data. In particular, large noisily-annotated data sets have been generated by leveraging freely available knowledge bases such as Freebase (Bollacker et al., 2008; Mintz et al., 2009). Such data sets have been utilized successfully for relation extraction from the web (Bunescu and Mooney, 2007). 3.1 Model We present a simple model for representing relational structure, with the only variables present being a set of boolean-valued variabl"
D12-1074,C96-2215,0,0.0587693,"Missing"
D12-1074,D08-1016,1,0.822797,"r. 2.1 Latent Dependency Structure Dependency grammar is a lexically-oriented syntactic formalism in which syntactic relationships are expressed as dependencies between individual words. Each non-root word specifies another as its head, provided that the resulting structure forms 811 a valid directed graph, ie. there are no cycles in the graph. Due to the flexibility of this representation it is often used to describe free-word-order languages, and increasingly preferred in NLP for more language-in-use scenarios. A dependency graph can be modeled with the following nodes, as first proposed by Smith and Eisner (2008): • Let {Link(i, j) : 0 ≤ i ≤ j ≤ n, n 6= j} be O(n2 ) boolean variables corresponding to the possible links in a dependency parse. Li,j = true implies that there is a dependency from parent i to child j. • Let {LIN K(i, j) : 0 ≤ i ≤ j ≤ n, n 6= j} be O(n2 ) unary factors, each paired with a corresponding Link(i, j) variable and expressing the independent belief that Link(i, j) = true. 2.2 Latent Constituency Structure Alternatively we can describe the more structured constituency formalism by setting up a representation over span variables: • Let {Span(i, j) : 0 ≤ i &lt; j ≤ n} be O(n2 ) boolean"
D12-1074,W00-1308,0,0.0611931,"29.4 37.4 60.0 32.6 42.2 58.1 31.3 40.7 48.0 32.0 38.4 47.2 30.0 36.7 66.8 37.8 48.3 63.8 37.0 46.8 56.3 32.3 41.0 53.4 31.6 39.7 Table 1: Relation Extraction Results. Models using hidden constituency syntax provide significant gains over the syntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated on the smaller Chinese data set. are also tokenized according to Penn Chinese Treebank standards (Xue et al., 2005). The sentences are then tagged and parsed using the Stanford CoreNLP tools, using the standard pre-trained models for tagging (Toutanvoa and Manning, 2000), and the factored parsing model of Klein and Manning (2002). The distributed grammar is trained on a variety of sources, including the standard Wallstreet Journal corpus, but also biomedical, translation, and questions. We then apply entity and relation annotations noisily to the data, collapsing multi-word entities into one term. We filter out sentences with fewer than two entities (and are thus incapable of containing relations) and sentences with more than 40 words (to keep the parses more reliable). This yields 6966 sentences for English data, but unfortunately only 747 sentences for the"
D12-1074,N06-1037,0,0.0110315,"ized factors in a Markov random field. At both training and test time we marginalize over this hidden structure, learning the optimal latent representations for the problem. Results show that this approach provides significant gains over a syntactically uninformed baseline, outperforming models that observe syntax on an English relation extraction task, and performing comparably to them in semantic role labeling. 1 Introduction Many NLP tasks are inherently tied to syntax, and state-of-the-art solutions to these tasks often rely on syntactic annotations as either a source for useful features (Zhang et al., 2006, path features in relation extraction) or as a scaffolding upon which a more narrow, specialized classification can occur (as often done in semantic role labeling). This decouWe phrase the joint model as factor graph and marginalize over the hidden structure of the intermediate representation at both training and test time, to optimize performance on the end task. Inference is done via loopy belief propagation, making this framework trivially extensible to most graph structures. Computation over latent syntactic rep810 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural L"
D12-1074,P05-1053,0,0.0127568,"is convention, variables are represented by circles, factors by rectangles. We introduce six model scenarios. • Baseline, simply the arc-factored model consisting only of Rel and corresponding Label variables for each entity. Features on the relation factors, which are common to all model configurations, are combinations of lexical information (i.e., the words that form the entity, the pos-tags of the entities, etc.) as well as the distance between the relation. This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information. • Baseline-Ent, a variant of Baseline with additional features which include combinations of mention type, entity type, and entity sub-type. 814 Coordination factor features for the syntacticallyinformed models are particularly important. This became evident in initial experiments where the baseline was often able to outperform the hidden syntactic model. However, inclusion of entity and mention label features into the connection factors provides the model with grea"
H05-1060,W98-1110,0,0.0718073,"Missing"
H05-1060,W02-0506,0,0.0624393,"Missing"
H05-1060,W04-3246,0,0.0645424,"Missing"
H05-1060,N04-4038,0,0.0539525,"Missing"
H05-1060,P05-1071,0,0.26518,"Missing"
H05-1060,P01-1035,0,0.648454,"Missing"
H05-1060,C00-1042,0,0.24983,"Missing"
H05-1060,J00-1006,0,0.0185782,"Missing"
H05-1060,W04-3230,0,0.0214129,", θ~ = log P ps y0 |θ~ 0 y ∈d(x) The sum in the denominator is computed using a dynamic programming algorithm (akin to the forward algorithm); it involves computing the sum of all paths through the “sausage” lattice of possible analyses for x. By doing this, we allow knowledge of the support of the channel model to enter into our estimation of the source model. It is important to note that the estimation of the model (the objective function used in training, Eq. 3) is distinct from the source-channel structure of the model (Eq. 1). The lattice-conditional estimation approach was first used by Kudo et al. (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. The resulting model is an instance of a conditional random field (CRF; Lafferty et al., 2001). When training a CRF for POS tagging, IOB chunking (Sha and Pereira, 2003), or word segmentation (Peng et al., 2004), one typically structures the conditional probabilities (in the objective function) using domain knowledge: in POS tagging, the set of allowed tags for a word is used; in IOB chunking, the bigram “O I” is disallowed; and in segmentation, a lexicon is used to enu"
H05-1060,J95-3004,0,0.151142,"r sentences in three languages. In this paper, the dictionary defines the support set of the channel model. That is, pc (x |y) &gt; 0 if and only if y ∈ d(x). This is a clean way to incorporate domain knowledge into the probabilistic model; this kind of constraint has been applied in previous work at decoding time (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001). In such a model, each word is independent of its neighbors (because the dictionary ignores context). Estimation. A unigram channel model defines 2 Probabilistic modeling of what we call the morphological channel was first carried out by Levinger et al. (1995), who used unlabeled data to estimate p(~ y |x) for Hebrew, with the support defined by a dictionary. 476 def pc (x |y) = |x| Y p(xi |yi ) (2) i=1 The simplest estimate of this model is to make p(·, ·) uniform over (x, ~y ) such that ~y ∈ d(x). Doing so and marginalizing to get p(x |~y ) makes the channel model encode categorial information only, leaving all learning to the source model.3 Another way to estimate this model is, of course, from data. This is troublesome, because—modulo optionality—x is expected to be known given ~y , resulting in a huge model with mostly 1-valued probabilities."
H05-1060,C04-1081,0,0.0254105,"ation of the source model. It is important to note that the estimation of the model (the objective function used in training, Eq. 3) is distinct from the source-channel structure of the model (Eq. 1). The lattice-conditional estimation approach was first used by Kudo et al. (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. The resulting model is an instance of a conditional random field (CRF; Lafferty et al., 2001). When training a CRF for POS tagging, IOB chunking (Sha and Pereira, 2003), or word segmentation (Peng et al., 2004), one typically structures the conditional probabilities (in the objective function) using domain knowledge: in POS tagging, the set of allowed tags for a word is used; in IOB chunking, the bigram “O I” is disallowed; and in segmentation, a lexicon is used to enumerate the possible word boundaries.4 4 This refinement is in the same vein as the move from maximum likelihood estimation to conditional estimation. MLE would make the sum in the denominator of Eq. 4 Y+ , which for log-linear models is often intractable to compute (and for sequence models may not converge). Conditional estimation limi"
H05-1060,N03-1028,0,0.00899842,"t of the channel model to enter into our estimation of the source model. It is important to note that the estimation of the model (the objective function used in training, Eq. 3) is distinct from the source-channel structure of the model (Eq. 1). The lattice-conditional estimation approach was first used by Kudo et al. (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. The resulting model is an instance of a conditional random field (CRF; Lafferty et al., 2001). When training a CRF for POS tagging, IOB chunking (Sha and Pereira, 2003), or word segmentation (Peng et al., 2004), one typically structures the conditional probabilities (in the objective function) using domain knowledge: in POS tagging, the set of allowed tags for a word is used; in IOB chunking, the bigram “O I” is disallowed; and in segmentation, a lexicon is used to enumerate the possible word boundaries.4 4 This refinement is in the same vein as the move from maximum likelihood estimation to conditional estimation. MLE would make the sum in the denominator of Eq. 4 Y+ , which for log-linear models is often intractable to compute (and for sequence models may"
H05-1060,W04-3207,1,0.901276,"ynamic programming algorithm (akin to the forward algorithm); it involves computing the sum of all paths through the “sausage” lattice of possible analyses for x. By doing this, we allow knowledge of the support of the channel model to enter into our estimation of the source model. It is important to note that the estimation of the model (the objective function used in training, Eq. 3) is distinct from the source-channel structure of the model (Eq. 1). The lattice-conditional estimation approach was first used by Kudo et al. (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. The resulting model is an instance of a conditional random field (CRF; Lafferty et al., 2001). When training a CRF for POS tagging, IOB chunking (Sha and Pereira, 2003), or word segmentation (Peng et al., 2004), one typically structures the conditional probabilities (in the objective function) using domain knowledge: in POS tagging, the set of allowed tags for a word is used; in IOB chunking, the bigram “O I” is disallowed; and in segmentation, a lexicon is used to enumerate the possible word boundaries.4 4 This refinement is in the same vein as the mo"
H05-1060,P05-1003,0,0.0053139,"etraining. Prior work (factored training). Separately training different models that predict the same variables (e.g., x and y) then combining them for consensus-based inference (either through a mixture or a product of probabilities) is an old idea (Genest and Zidek, 1986). Recent work in learning weights for the component “expert” models has turned to cooperative techniques (Hinton, 1999). Decoding that finds y (given x) to maximize some weighted average of log-probabilities is known as a logarithmic opinion pool (LOP). LOPs were applied to CRFs (for named entity recognition and tagging) by Smith et al. (2005), with an eye toward regularization. Their experts (each a CRF) contained overlapping feature sets, and the combined model achieved much the same effect as training a single model with smoothing. Note that our models, unlike theirs, partition the feature space; there is only one CRF, but some parameters are ignored when estimating other parameters. We have not estimated log-domain mixing coefficients—we weight all models’ contributions equally. Sutton and McCallum (2005) have applied factored estimation to CRFs, motivated (like us) by speed; they also describe how factored estimation 9 Lemma-t"
N04-1021,J00-1004,0,0.0540744,"Missing"
N04-1021,J93-2003,0,0.0210066,"e different types of features, including features based on syntactic analyses of the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based Anoop Sarkar Simon Fraser U. As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability P r(eI1 |f1J ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: PM exp[ m=1 λm hm (eI1 , f1J )] I J P r(e1 |f1 ) = P (2) PM J 0I e0I exp[ m=1 λm hm (e 1 , f1 )] 1 We obtain the following decision rule: eˆI1 = argmax eI1 M nX o λm hm (eI1 , f1J ) (3) m=1 The standard criterion for training such a log-linear"
N04-1021,P03-1011,1,0.218398,"of words in tree fragment and k for maximum height of with some probability, to transform one tree into another. tree fragment. We proceed from left to right in the ChiHowever, when training the model, trees for both the nese sentence and incrementally grow a pair of subtrees, source and target languages are provided, in our case one subtree in Chinese and the other in English, such that from the Chinese and English parsers. each word in the Chinese subtree is aligned to a word in We began with the tree-to-tree alignment model prethe English subtree. We grow this pair of subtrees unsented by Gildea (2003). The model was extended to hantil we can no longer grow either subtree without violatdle dependency trees, and to make use of the word-level ing the two parameter values n and k. Note that these alignments produced by the baseline MT system. The aligned subtree pairs have properties similar to alignment probability assigned by the tree-to-tree alignment model, templates. They can rearrange in complex ways between given the word-level alignment with which the candidate source and target. Figure 2 shows how subtree-pairs for translation was generated, was used as a feature in our parameters n ="
N04-1021,P03-1021,1,0.129951,"nsisting of S sentence pairs {(fs , es ) : s = 1, . . . , S}. However, this does not guarantee optimal performance on the metric of translation quality by which our system will ultimately be evaluated. For this reason, we optimize the parameters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. Figure 1: Example segmentation of Chinese sentence and its English translation into alignment templates. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. 2.1 Baseline MT System: Alignment Templates Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words f1J are grouped to phrases f˜1K . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment te"
N04-1021,P02-1038,1,0.203592,"the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based Anoop Sarkar Simon Fraser U. As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability P r(eI1 |f1J ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: PM exp[ m=1 λm hm (eI1 , f1J )] I J P r(e1 |f1 ) = P (2) PM J 0I e0I exp[ m=1 λm hm (e 1 , f1 )] 1 We obtain the following decision rule: eˆI1 = argmax eI1 M nX o λm hm (eI1 , f1J ) (3) m=1 The standard criterion for training such a log-linear model is to maximize the probability of the parallel training corpus consisting"
N04-1021,J04-4002,1,0.260908,"ters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. Figure 1: Example segmentation of Chinese sentence and its English translation into alignment templates. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. 2.1 Baseline MT System: Alignment Templates Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words f1J are grouped to phrases f˜1K . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to π1K ). Then, every phrase f˜ produces its translation e˜ (using the corresponding alignment template z). Finally, the sequence of phrases e˜K 1 constitutes the sequence of words eI1 . Our baseline system incorporat"
N04-1021,W99-0604,1,0.236444,"Missing"
N04-1021,W03-1002,0,0.0153043,"of the AT touches the upper right corner of the previous AT and the first word in the current AT immediately follows the last word in the previous AT. The total probability is the product over all alignment templates i, either P (ATi is right-continuous) or 1 − P (ATi is right-continuous). In both models, the probabilities P have been estimated from the full training data (train). 5 Shallow Syntactic Feature Functions By shallow syntax, we mean the output of the part-ofspeech tagger and chunkers. We hope that such features can combine the strengths of tag- and chunk-based translation systems (Schafer and Yarowsky, 2003) with our baseline system. 5.1 Projected POS Language Model This feature uses Chinese POS tag sequences as surrogates for Chinese words to model movement. Chinese words are too sparse to model movement, but an attempt to model movement using Chinese POS may be more successful. We hope that this feature will compensate for a weak model of word movement in the baseline system. Chinese POS sequences are projected to English using the word alignment. Relative positions are indicated for each Chinese tag. The feature function was also tried without the relative positions: CD +0 M +1 NN +3 NN -1 NN"
N04-1021,C96-2141,0,0.254737,"Missing"
N04-1021,P98-2230,0,0.0760244,"Missing"
N04-1021,P01-1067,1,0.0551165,"ical human translations. One reason for that is that the MT output uses fewer unseen words and typically more frequent words which lead to a higher language model probability. We also performed experiments to balance this effect by dividing the parser probability by the word unigram probability and using this ’normalized parser probability’ as a feature function, but also this did not yield improvements. 6.2 Tree-to-String Alignment A tree-to-string model is one of several syntaxbased translation models used. The model is a conditional probability p(f |T (e)). Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. First, it reorders the child nodes, such as changing VP → VB NP PP into VP → NP PP VB. Second, it inserts an optional word at each node. Third, it translates the leaf English words into Chinese words. These operations are stochastic and their probabilities are assumed to depend only on the node, and are independent of other operations on the node, or other nodes. The probability of each operation is automatically obtained by a training algorithm, using about 780,000 English par"
N04-1021,C98-2225,0,\N,Missing
N10-1117,P05-1045,0,0.0125325,"2007), we have non-projective languages such as Dutch using second order projective models if we want to apply DP. Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution (McDonald and Pereira, 2006). In order to explore richer model structures, the NLP community has recently started to investigate the use of other, well-known machine learning techniques for marginal inference. One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al., 2005), another is (loopy) sum-product belief propagation (Smith and Eisner, 2008). In both cases we usually work in the framework of graphical models—in our case, with factor graphs that describe our distributions through variables, factors, and factor potentials. In theory, methods such as belief propagation can take any graph and perform marginal inference. This means that we gain a great amount of flexibility to represent more global and joint distributions for NLP tasks. The graphical models of interest, however, are often too large and densely connected for efficient inference in them. For exa"
N10-1117,E06-1011,0,0.0422269,"is requires the model to factor in a way that lends itself to DP algorithms, we have to restrict the class of probabilistic models we consider. For example, since we cannot derive a dynamic program for marginal inference in second order non-projective dependency parsing (McDonald and Satta, 2007), we have non-projective languages such as Dutch using second order projective models if we want to apply DP. Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution (McDonald and Pereira, 2006). In order to explore richer model structures, the NLP community has recently started to investigate the use of other, well-known machine learning techniques for marginal inference. One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al., 2005), another is (loopy) sum-product belief propagation (Smith and Eisner, 2008). In both cases we usually work in the framework of graphical models—in our case, with factor graphs that describe our distributions through variables, factors, and factor potentials. In theory, methods such as belief propagation can take a"
N10-1117,W07-2216,0,0.0170166,"probabilities (perhaps subject to hard constraints) in order to predict a good variable assignment.1 1 With a loss function that decomposes on the variables, this amounts to Minimum Bayes Risk (MBR) decoding, which is Traditionally, marginal inference in NLP has been performed via dynamic programming (DP); however, because this requires the model to factor in a way that lends itself to DP algorithms, we have to restrict the class of probabilistic models we consider. For example, since we cannot derive a dynamic program for marginal inference in second order non-projective dependency parsing (McDonald and Satta, 2007), we have non-projective languages such as Dutch using second order projective models if we want to apply DP. Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution (McDonald and Pereira, 2006). In order to explore richer model structures, the NLP community has recently started to investigate the use of other, well-known machine learning techniques for marginal inference. One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel"
N10-1117,W06-1616,1,0.959755,"hods, and the advantage second order models give in accuracy is often not significant enough to offset the lack of speed in practice. Moreover, if we extend such parsing models to, say, penalizing all pairs of crossing edges or scoring syntax-based  alignments, we will need to inspect at least O n4 factors, increasing our efficiency concerns. When looking at the related task of finding the most likely assignment in large graphical models (i.e., MAP inference), we notice that several recent approaches have significantly sped up computation through relaxation methods (Tromble and Eisner, 2006; Riedel and Clarke, 2006). Here we start with a small subset of the full graph, and run inference for this simpler problem. Then we search for factors that are “violated” in the solution, and add them to the graph. This is repeated until no more new factors can be added. Empirically this approach has shown impressive success. It often dramatically reduces the effective network size, with no loss in accuracy. How can we extend or generalize MAP relaxation algorithms to the case of marginal inference? Roughly speaking, we answer it by introducing a notion of factor gain that is defined as the KL divergence between the c"
N10-1117,D08-1016,1,0.915567,"projective models if we want to apply DP. Some previous work has circumvented this problem for MAP inference by starting with a second-order projective solution and then greedily flipping edges to find a better nonprojective solution (McDonald and Pereira, 2006). In order to explore richer model structures, the NLP community has recently started to investigate the use of other, well-known machine learning techniques for marginal inference. One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al., 2005), another is (loopy) sum-product belief propagation (Smith and Eisner, 2008). In both cases we usually work in the framework of graphical models—in our case, with factor graphs that describe our distributions through variables, factors, and factor potentials. In theory, methods such as belief propagation can take any graph and perform marginal inference. This means that we gain a great amount of flexibility to represent more global and joint distributions for NLP tasks. The graphical models of interest, however, are often too large and densely connected for efficient inference in them. For example, in second order often very effective. 760 Human Language Technologies:"
N10-1117,N06-1054,0,0.299226,"simpler greedy parsing methods, and the advantage second order models give in accuracy is often not significant enough to offset the lack of speed in practice. Moreover, if we extend such parsing models to, say, penalizing all pairs of crossing edges or scoring syntax-based  alignments, we will need to inspect at least O n4 factors, increasing our efficiency concerns. When looking at the related task of finding the most likely assignment in large graphical models (i.e., MAP inference), we notice that several recent approaches have significantly sped up computation through relaxation methods (Tromble and Eisner, 2006; Riedel and Clarke, 2006). Here we start with a small subset of the full graph, and run inference for this simpler problem. Then we search for factors that are “violated” in the solution, and add them to the graph. This is repeated until no more new factors can be added. Empirically this approach has shown impressive success. It often dramatically reduces the effective network size, with no loss in accuracy. How can we extend or generalize MAP relaxation algorithms to the case of marginal inference? Roughly speaking, we answer it by introducing a notion of factor gain that is defined as the K"
N10-1117,D07-1096,1,\N,Missing
N16-1053,2005.mtsummit-papers.11,0,0.0741598,"he multi-level Dirchlet prior structure. Another way of evaluating topic models is through an extrinsic evaluation task which was not available for this collection. In the case of oLDA, article sections were treated as individual documents. In the original oLDA1 implementation the per document concentration parameter αd was set to K1 which we also use in our case for both the symmetric θd and asymmetric θs (same goes for PLTM 1 Modeling and Retrieving Speeches in Europarl Sessions We compared the modeling performance of oPLTM and mlhPLTM on a subset of the English-Spanish Europarl collection (Koehn, 2005). The subset consists of ∼64k training pairs of English-Spanish speeches that are translations of each other which originate from 374 sessions of the European Parliament (Europarl) and a test set of ∼14k speech translation pairs from 112 sessions. With oPLTM we modeled individual speech pairs while with mlhPLTM we utilized the session hierarchy and modeled pairs of speeches as document sections. Comparisons were performed intrinsically (using perplexity) and extrinsically on a cross-language information retrieval (CLIR) task. This task, along with the Europarl subset, have been previously defi"
N16-1053,W13-2232,1,0.794953,"A posterior distributions is Gibbs sampling (Griffiths and Steyvers, 2004). For example, this approach was used by Wallach et al. (2009) and was originally used for PLTM. On the other hand the VB approach (Blei et al., 2003) offers more efficient computation but as in the case of Gibbs sampling requires iterating over the whole collection multiple times (e.g. Kim et al. (2013)). More recently Hoffman et al. (2010) introduced online LDA (oLDA) that relies on online stochastic optimization and requires a single pass over the whole collection. The same approach was also extended to PLTM (oPLTM) (Krstovski and Smith, 2013). In our work we also utilize online VB to implement multi-level hyperprior (mlh) structure in LDA and PLTM. Similar to batch VB, in online T VB locally optimal values of the free variational pa  z w s rameters γ and φ, which are used to approximate d N  d the posterior θ and z, are computed in the E step s u d s of the algorithm but on a batch b of documents di   z w S  N D T S (rather than the whole collection D as in the case of D batch VB) while holding the topic-word variational Figure 2: mlhPLTM: Graphical model representation.  parameter λ fixed. In the M step, λ is update"
N16-1053,D09-1092,1,0.926318,"ubset consists of ∼64k training pairs of English-Spanish speeches that are translations of each other which originate from 374 sessions of the European Parliament (Europarl) and a test set of ∼14k speech translation pairs from 112 sessions. With oPLTM we modeled individual speech pairs while with mlhPLTM we utilized the session hierarchy and modeled pairs of speeches as document sections. Comparisons were performed intrinsically (using perplexity) and extrinsically on a cross-language information retrieval (CLIR) task. This task, along with the Europarl subset, have been previously defined by Mimno et al. (2009) and used across other publications (Platt et al., 2010; Krstovski and Smith, 2013). Given a query English speech, the CLIR task is to retrieve its Spanish translation equivalent. It involves performing comparison across topic representations of all Spanish speeches using Jensen-Shannon divergence and sorting the results. Models are evaluated using precision at rank one (P@1). Figure 6 shows the CLIR task performance comparisons results using 13 different topic configurations. We performed comparisons across three different settings of the concentration parameters αd and αs (αd =αs = K1 , 0.4"
N16-1053,D10-1025,0,0.113781,"speeches that are translations of each other which originate from 374 sessions of the European Parliament (Europarl) and a test set of ∼14k speech translation pairs from 112 sessions. With oPLTM we modeled individual speech pairs while with mlhPLTM we utilized the session hierarchy and modeled pairs of speeches as document sections. Comparisons were performed intrinsically (using perplexity) and extrinsically on a cross-language information retrieval (CLIR) task. This task, along with the Europarl subset, have been previously defined by Mimno et al. (2009) and used across other publications (Platt et al., 2010; Krstovski and Smith, 2013). Given a query English speech, the CLIR task is to retrieve its Spanish translation equivalent. It involves performing comparison across topic representations of all Spanish speeches using Jensen-Shannon divergence and sorting the results. Models are evaluated using precision at rank one (P@1). Figure 6 shows the CLIR task performance comparisons results using 13 different topic configurations. We performed comparisons across three different settings of the concentration parameters αd and αs (αd =αs = K1 , 0.4 and 1.0). 2 http://www.cs.princeton.edu/˜mdhoffma 457 h"
N16-1132,2010.iwslt-papers.3,0,0.0170087,"and/or a pretrained baseline MT system. This paper, in contrast, investigates building MT systems from comparable corpora without such resources. In a widely cited early paper, Munteanu and Marcu (2005) use a bilingual dictionary and a collection of parallel sentences to train IBM Model 1 and a maximum entropy classifier to determine whether two sentences are translations of each other. Tillmann and Xu (2009) and Smith et al. (2010) detect parallel sentences by training IBM Model 1 and maximum entropy classifiers, respectively. In later work on detecting sentence and phrase translation pairs, Cettolo et al. (2010) and Hoang et al. (2014) use SMT systems to translate candidate documents; Quirk et al. (2007) use parallel data to train a translation equivalence model; and Ture and Lin (2012) use a translation lexicon to build a scoring function for parallel documents. More recently, Ling et al. (2013) trained IBM Model 1 on bitext to detect translationally equivalent phrase pairs within single microblog posts. Abdul-Rauf and Schwenk (2009), Uszkoreit et al. (2010), and Gahbiche-Braham et al. (2011), 1127 Proceedings of NAACL-HLT 2016, pages 1127–1132, c San Diego, California, June 12-17, 2016. 2016 Associ"
N16-1132,N07-2008,0,0.24808,"nd Gahbiche-Braham et al. (2011), 1127 Proceedings of NAACL-HLT 2016, pages 1127–1132, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics rather than trying to detect translated sentence pairs directly, translate the entire source language side of a comparable corpus into the target language with a baseline SMT system and then search for corresponding documents. On the other hand, there exist approaches that mine comparable corpora without any prior translation information or parallel data. Examples of this approach are rarer, and we briefly mention two: Enright and Kondrak (2007) use singleton words (hapax legomena) to represent documents in a bilingual collection for the task of detecting document translation pairs, and Krstovski and Smith (2011) construct a vocabulary of overlapping words to represent documents in multilingual collections. The latter approach demonstrates high precision vs. recall values on various language pairs from different languages and writing systems when detecting translation pairs on a document level such as Europarl sessions. Recently proposed approaches, such as (Klementiev et al., 2012) use monolingual corpora to estimate phrase-based SM"
N16-1132,W04-3208,0,0.024329,"on system that outperforms one trained on the WMT’11 parallel training set. 1 2 Introduction In statistical machine translation (SMT), the quality of the translation model is highly dependent on the amount of parallel data used to build it. Parallel data has usually been generated through the process of human translation, which imposes significant costs when building systems for new languages and domains. To alleviate this problem, researchers have considered comparable corpora—a collection of multilingual documents that are only topically aligned but not necessary translations of each other (Fung and Cheung, 2004). While most previous approaches for mining comparable corpora heavily depend on initializing the learning process with some translation dictionaries or parallel text, we use multilingual topic models to detect document translation pairs and extract parallel sentences with only Prior Work on Comparable Corpora Most previous, if not all, approaches for mining comparable corpora heavily depend on bilingual resources, such as translation lexica, bitext, and/or a pretrained baseline MT system. This paper, in contrast, investigates building MT systems from comparable corpora without such resources."
N16-1132,W11-1207,0,0.0156343,"l 1 and maximum entropy classifiers, respectively. In later work on detecting sentence and phrase translation pairs, Cettolo et al. (2010) and Hoang et al. (2014) use SMT systems to translate candidate documents; Quirk et al. (2007) use parallel data to train a translation equivalence model; and Ture and Lin (2012) use a translation lexicon to build a scoring function for parallel documents. More recently, Ling et al. (2013) trained IBM Model 1 on bitext to detect translationally equivalent phrase pairs within single microblog posts. Abdul-Rauf and Schwenk (2009), Uszkoreit et al. (2010), and Gahbiche-Braham et al. (2011), 1127 Proceedings of NAACL-HLT 2016, pages 1127–1132, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics rather than trying to detect translated sentence pairs directly, translate the entire source language side of a comparable corpus into the target language with a baseline SMT system and then search for corresponding documents. On the other hand, there exist approaches that mine comparable corpora without any prior translation information or parallel data. Examples of this approach are rarer, and we briefly mention two: Enright and Kondrak (2007) use s"
N16-1132,E12-1014,0,0.0128858,"s approach are rarer, and we briefly mention two: Enright and Kondrak (2007) use singleton words (hapax legomena) to represent documents in a bilingual collection for the task of detecting document translation pairs, and Krstovski and Smith (2011) construct a vocabulary of overlapping words to represent documents in multilingual collections. The latter approach demonstrates high precision vs. recall values on various language pairs from different languages and writing systems when detecting translation pairs on a document level such as Europarl sessions. Recently proposed approaches, such as (Klementiev et al., 2012) use monolingual corpora to estimate phrase-based SMT parameters. Unlike our paper, however, they do not demonstrate an end-toend SMT system trained without any parallel data. Our approach differs from these and other previous approaches by not relying on any initial translation dictionary or any bitext to train a seed SMT system. Therefore, the primary experimental comparison that we perform is between no bitext at all and a system trained with some bitext. 3 Bootstrapping Approach Our bootstrapping approach (Figure 1) is a twostage system that used the Overlapping Cosine Distance (OCD) appro"
N16-1132,P07-2045,0,0.00829972,"xperiments and Results We demonstrate the performance of the bootstrapping approach on the task of extracting parallel sentences to train a translation system. We evaluate MT systems trained on extracted parallel sentences and 1 We use k-d tree implementation in the ANN library (Mount and Arya, 2010). compare their performance against MT systems created using clean parallel collections. MT systems were evaluated with the standard BLEU metric (Papineni et al., 2002) on two official WMT test sets that cover different domains: News (WMT’11) and Europarl (WMT’08). We trained the Moses SMT system (Koehn et al., 2007) following the WMT shared task guidelines for building a baseline system with one of two parallel training collections from WMT’11: English-Spanish News Commentary (v6) and Europarl (v6). MT systems were trained using test-domain specific language models (LM) — English News Commentary for News test and English Europarl for the Europarl test. Our comparable corpus consists of news stories from the English (LDC2011T07) and Spanish (LDC2011T12) Gigaword collections. We perform the following processing in each step of the pipeline. We run OCD on days of news originating from multiple news agencies"
N16-1132,W11-2125,1,0.921581,"stics rather than trying to detect translated sentence pairs directly, translate the entire source language side of a comparable corpus into the target language with a baseline SMT system and then search for corresponding documents. On the other hand, there exist approaches that mine comparable corpora without any prior translation information or parallel data. Examples of this approach are rarer, and we briefly mention two: Enright and Kondrak (2007) use singleton words (hapax legomena) to represent documents in a bilingual collection for the task of detecting document translation pairs, and Krstovski and Smith (2011) construct a vocabulary of overlapping words to represent documents in multilingual collections. The latter approach demonstrates high precision vs. recall values on various language pairs from different languages and writing systems when detecting translation pairs on a document level such as Europarl sessions. Recently proposed approaches, such as (Klementiev et al., 2012) use monolingual corpora to estimate phrase-based SMT parameters. Unlike our paper, however, they do not demonstrate an end-toend SMT system trained without any parallel data. Our approach differs from these and other previ"
N16-1132,W13-2232,1,0.846879,"Ture et al., 2011; Ture and Lin, 2012) but this approach requires special programming methods. In order to use the PLTM on large collections and avoid the bottleneck introduced by Gibbs sampling, we use the online variational Bayes (VB) approach originally developed by (Hoffman et al., 2010) for LDA model to develop a fast, online PLTM model. As in the regular VB approach, online VB approximates the hidden parameters θ, z and β using the free variational parameters: γ, φ and λ. Rather than going over the whole collection of documents to bring the variational parameters to a convergence point, Krstovski and Smith (2013) perform updates of the variational parameters γ and φL on document batches and update the λL variational parameter as a weighted average of its stochastic gradient based approximation and its value on the previous batch. The approximation is done through Expectation-Maximization (EM). Unlike the usual metric spaces where two vectors are compared using distance metrics such as Euclidean (Eu) or Cos distance, in the probability simplex similarity is computed using informationtheoretic measurements such as Kullback-Leibler, Jensen-Shannon divergence and He distance. We alleviate the O(N 2 ) wors"
N16-1132,P13-1018,0,0.0151217,"a maximum entropy classifier to determine whether two sentences are translations of each other. Tillmann and Xu (2009) and Smith et al. (2010) detect parallel sentences by training IBM Model 1 and maximum entropy classifiers, respectively. In later work on detecting sentence and phrase translation pairs, Cettolo et al. (2010) and Hoang et al. (2014) use SMT systems to translate candidate documents; Quirk et al. (2007) use parallel data to train a translation equivalence model; and Ture and Lin (2012) use a translation lexicon to build a scoring function for parallel documents. More recently, Ling et al. (2013) trained IBM Model 1 on bitext to detect translationally equivalent phrase pairs within single microblog posts. Abdul-Rauf and Schwenk (2009), Uszkoreit et al. (2010), and Gahbiche-Braham et al. (2011), 1127 Proceedings of NAACL-HLT 2016, pages 1127–1132, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics rather than trying to detect translated sentence pairs directly, translate the entire source language side of a comparable corpus into the target language with a baseline SMT system and then search for corresponding documents. On the other hand, there ex"
N16-1132,D09-1092,1,0.759581,"other previous approaches by not relying on any initial translation dictionary or any bitext to train a seed SMT system. Therefore, the primary experimental comparison that we perform is between no bitext at all and a system trained with some bitext. 3 Bootstrapping Approach Our bootstrapping approach (Figure 1) is a twostage system that used the Overlapping Cosine Distance (OCD) approach of Krstovski and Smith (2011) as its first step. OCD outputs a ranked list of candidate document pairs, which are then fed through a sentence-alignment system (Moore, 2002). A polylingual topic model (PLTM) (Mimno et al., 2009) is then trained on the aligned portions of these documents. Using the trained model, we infer topics on the whole comparable training set. Once represented as points in the topic space, documents are then compared for similarity using divergence based metrics such as Hellinger (He) distance. Results from these comparisons create a single ranked list of text translation pairs, which are on a sub docu1128 Figure 1: The bilingual collection processing pipeline. ment length level. From this single ranked list, using thresholding, we again extract the top n candidate translation pairs that are the"
N16-1132,moore-2002-fast,0,0.0624384,"allel data. Our approach differs from these and other previous approaches by not relying on any initial translation dictionary or any bitext to train a seed SMT system. Therefore, the primary experimental comparison that we perform is between no bitext at all and a system trained with some bitext. 3 Bootstrapping Approach Our bootstrapping approach (Figure 1) is a twostage system that used the Overlapping Cosine Distance (OCD) approach of Krstovski and Smith (2011) as its first step. OCD outputs a ranked list of candidate document pairs, which are then fed through a sentence-alignment system (Moore, 2002). A polylingual topic model (PLTM) (Mimno et al., 2009) is then trained on the aligned portions of these documents. Using the trained model, we infer topics on the whole comparable training set. Once represented as points in the topic space, documents are then compared for similarity using divergence based metrics such as Hellinger (He) distance. Results from these comparisons create a single ranked list of text translation pairs, which are on a sub docu1128 Figure 1: The bilingual collection processing pipeline. ment length level. From this single ranked list, using thresholding, we again ext"
N16-1132,J05-4003,0,0.0664881,"for mining comparable corpora heavily depend on initializing the learning process with some translation dictionaries or parallel text, we use multilingual topic models to detect document translation pairs and extract parallel sentences with only Prior Work on Comparable Corpora Most previous, if not all, approaches for mining comparable corpora heavily depend on bilingual resources, such as translation lexica, bitext, and/or a pretrained baseline MT system. This paper, in contrast, investigates building MT systems from comparable corpora without such resources. In a widely cited early paper, Munteanu and Marcu (2005) use a bilingual dictionary and a collection of parallel sentences to train IBM Model 1 and a maximum entropy classifier to determine whether two sentences are translations of each other. Tillmann and Xu (2009) and Smith et al. (2010) detect parallel sentences by training IBM Model 1 and maximum entropy classifiers, respectively. In later work on detecting sentence and phrase translation pairs, Cettolo et al. (2010) and Hoang et al. (2014) use SMT systems to translate candidate documents; Quirk et al. (2007) use parallel data to train a translation equivalence model; and Ture and Lin (2012) us"
N16-1132,P02-1040,0,0.096154,"xi and yi , xi = pi √ and yi = qi , and compute He distance using Eu based, approximate NN computation approaches such as k-d trees1 (Bentley, 1975). 4 Experiments and Results We demonstrate the performance of the bootstrapping approach on the task of extracting parallel sentences to train a translation system. We evaluate MT systems trained on extracted parallel sentences and 1 We use k-d tree implementation in the ANN library (Mount and Arya, 2010). compare their performance against MT systems created using clean parallel collections. MT systems were evaluated with the standard BLEU metric (Papineni et al., 2002) on two official WMT test sets that cover different domains: News (WMT’11) and Europarl (WMT’08). We trained the Moses SMT system (Koehn et al., 2007) following the WMT shared task guidelines for building a baseline system with one of two parallel training collections from WMT’11: English-Spanish News Commentary (v6) and Europarl (v6). MT systems were trained using test-domain specific language models (LM) — English News Commentary for News test and English Europarl for the Europarl test. Our comparable corpus consists of news stories from the English (LDC2011T07) and Spanish (LDC2011T12) Giga"
N16-1132,2007.mtsummit-papers.50,0,0.0263286,"s from comparable corpora without such resources. In a widely cited early paper, Munteanu and Marcu (2005) use a bilingual dictionary and a collection of parallel sentences to train IBM Model 1 and a maximum entropy classifier to determine whether two sentences are translations of each other. Tillmann and Xu (2009) and Smith et al. (2010) detect parallel sentences by training IBM Model 1 and maximum entropy classifiers, respectively. In later work on detecting sentence and phrase translation pairs, Cettolo et al. (2010) and Hoang et al. (2014) use SMT systems to translate candidate documents; Quirk et al. (2007) use parallel data to train a translation equivalence model; and Ture and Lin (2012) use a translation lexicon to build a scoring function for parallel documents. More recently, Ling et al. (2013) trained IBM Model 1 on bitext to detect translationally equivalent phrase pairs within single microblog posts. Abdul-Rauf and Schwenk (2009), Uszkoreit et al. (2010), and Gahbiche-Braham et al. (2011), 1127 Proceedings of NAACL-HLT 2016, pages 1127–1132, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics rather than trying to detect translated sentence pairs dir"
N16-1132,N10-1063,0,0.0597813,"h only Prior Work on Comparable Corpora Most previous, if not all, approaches for mining comparable corpora heavily depend on bilingual resources, such as translation lexica, bitext, and/or a pretrained baseline MT system. This paper, in contrast, investigates building MT systems from comparable corpora without such resources. In a widely cited early paper, Munteanu and Marcu (2005) use a bilingual dictionary and a collection of parallel sentences to train IBM Model 1 and a maximum entropy classifier to determine whether two sentences are translations of each other. Tillmann and Xu (2009) and Smith et al. (2010) detect parallel sentences by training IBM Model 1 and maximum entropy classifiers, respectively. In later work on detecting sentence and phrase translation pairs, Cettolo et al. (2010) and Hoang et al. (2014) use SMT systems to translate candidate documents; Quirk et al. (2007) use parallel data to train a translation equivalence model; and Ture and Lin (2012) use a translation lexicon to build a scoring function for parallel documents. More recently, Ling et al. (2013) trained IBM Model 1 on bitext to detect translationally equivalent phrase pairs within single microblog posts. Abdul-Rauf an"
N16-1132,N09-2024,0,0.020539,"ract parallel sentences with only Prior Work on Comparable Corpora Most previous, if not all, approaches for mining comparable corpora heavily depend on bilingual resources, such as translation lexica, bitext, and/or a pretrained baseline MT system. This paper, in contrast, investigates building MT systems from comparable corpora without such resources. In a widely cited early paper, Munteanu and Marcu (2005) use a bilingual dictionary and a collection of parallel sentences to train IBM Model 1 and a maximum entropy classifier to determine whether two sentences are translations of each other. Tillmann and Xu (2009) and Smith et al. (2010) detect parallel sentences by training IBM Model 1 and maximum entropy classifiers, respectively. In later work on detecting sentence and phrase translation pairs, Cettolo et al. (2010) and Hoang et al. (2014) use SMT systems to translate candidate documents; Quirk et al. (2007) use parallel data to train a translation equivalence model; and Ture and Lin (2012) use a translation lexicon to build a scoring function for parallel documents. More recently, Ling et al. (2013) trained IBM Model 1 on bitext to detect translationally equivalent phrase pairs within single microb"
N16-1132,N12-1079,0,0.0817459,"teanu and Marcu (2005) use a bilingual dictionary and a collection of parallel sentences to train IBM Model 1 and a maximum entropy classifier to determine whether two sentences are translations of each other. Tillmann and Xu (2009) and Smith et al. (2010) detect parallel sentences by training IBM Model 1 and maximum entropy classifiers, respectively. In later work on detecting sentence and phrase translation pairs, Cettolo et al. (2010) and Hoang et al. (2014) use SMT systems to translate candidate documents; Quirk et al. (2007) use parallel data to train a translation equivalence model; and Ture and Lin (2012) use a translation lexicon to build a scoring function for parallel documents. More recently, Ling et al. (2013) trained IBM Model 1 on bitext to detect translationally equivalent phrase pairs within single microblog posts. Abdul-Rauf and Schwenk (2009), Uszkoreit et al. (2010), and Gahbiche-Braham et al. (2011), 1127 Proceedings of NAACL-HLT 2016, pages 1127–1132, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics rather than trying to detect translated sentence pairs directly, translate the entire source language side of a comparable corpus into the tar"
N16-1132,C10-1124,0,0.0448422,"entences by training IBM Model 1 and maximum entropy classifiers, respectively. In later work on detecting sentence and phrase translation pairs, Cettolo et al. (2010) and Hoang et al. (2014) use SMT systems to translate candidate documents; Quirk et al. (2007) use parallel data to train a translation equivalence model; and Ture and Lin (2012) use a translation lexicon to build a scoring function for parallel documents. More recently, Ling et al. (2013) trained IBM Model 1 on bitext to detect translationally equivalent phrase pairs within single microblog posts. Abdul-Rauf and Schwenk (2009), Uszkoreit et al. (2010), and Gahbiche-Braham et al. (2011), 1127 Proceedings of NAACL-HLT 2016, pages 1127–1132, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics rather than trying to detect translated sentence pairs directly, translate the entire source language side of a comparable corpus into the target language with a baseline SMT system and then search for corresponding documents. On the other hand, there exist approaches that mine comparable corpora without any prior translation information or parallel data. Examples of this approach are rarer, and we briefly mention tw"
N16-1132,E09-1003,0,\N,Missing
P06-2101,P05-1022,0,0.0319783,"ch as (5) is really a function of γ ·θ, so the optimizer could exactly compensate for increased γ by decreasing the θ vector proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose γ and θ accordingly: 17.5 γ=∞ γ = 0.1 γ=1 γ = 10 −10 −5 0 5 10 Translation model 1 Figure 2: Loss and expected loss as one translation model’s weight varies: the gray line (γ = ∞) shows true B LEU (to be optimized in equation (2)). The black lines show the expected B LEU as γ in equation (5) increases from 0.1 toward ∞. for this honor, we follow Charniak and Johnson (2005) in summing their probabilities.2 Maximizing (4) is equivalent to minimizing P an upper bound on the expected 0/1 loss i (1 − pθ (yi∗ |xi )). Though the log makes it tractable, this remains a 0/1 objective that does not give partial credit to wrong answers, such as imperfect but useful translations. Most systems should be evaluated and preferably trained on less harsh metrics. 3 min Epγ,θ [L(yi,k )] − T · H(pγ,θ ) γ,θ In place of a schedule for raising γ, we now use a cooling schedule to lower T from ∞ to −∞, thereby weakening the preference for high entropy. The Lagrange multiplier T on entro"
P06-2101,W06-2929,1,0.739696,"ay subtract it from equation (4), which is equivalent to maximum a posteriori estimation with a diagonal Gaussian prior (Chen and Rosenfeld, 1999). The variance σd2 may reflect a prior belief about the potential usefulness of feature d, or may be tuned on heldout data. Another simple regularization method is to stop cooling before T reaches 0 (cf. Elidan and Friedman (2005)). If loss on heldout data begins to increase, we may be starting to overfit. This technique can be used along with annealing or quadratic regularization and can achieve additional accuracy gains, which we report elsewhere (Dreyer et al., 2006). 5 i i = E[log A] − E[log C] (8) P where the P integer random variables A = i ai and C = i ci count the number of posited and correctly posited elements over the whole corpus. To approximate E[g(A)], where g is any twicedifferentiable function (here g = log), we can approximate g locally by a quadratic, given by the Taylor expansion of g about A’s mean µA = E[A]: Computing Expected Loss E[g(A)] ≈ E[g(µA ) + (A − µA )g 0 (µA ) 1 + (A − µA )2 g 00 (µA )] 2 = g(µA ) + E[A − µA ]g 0 (µA ) 1 + E[(A − µA )2 ]g 00 (µA ) 2 1 2 00 = g(µA ) + σA g (µA ). 2 P 2 P 2 Here µA = i σai , since A i µai and σA"
P06-2101,P96-1024,0,0.0996297,"k annealing is significantly better than the other training methods, while minimum error is significantly worse. For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other. For Dutch, the three methods are indistinguishable. than either and in some cases significantly helped. Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do. The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000). We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder. Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer e"
P06-2101,N03-1017,0,0.00784793,") as correct if the occurs at most twice in any reference translation of xi . This “clipping” does not affect the rest of our method. 8 Reasonable for a large corpus, by Lyapunov’s central limit theorem (allows non-identically distributed summands). 9 The component whose optimization achieved the lowest loss is then updated. The process iterates until no lower loss can be found. In contrast, Papineni (1999) proposed a linear programming method that may search along diagonal lines. 791 Optimization Procedure Max. like. Min. error Ann. min. risk and score the alignment template model’s phrases (Koehn et al., 2003). The Pharaoh phrase-based decoder uses precisely the setup of this paper. It scores a candidate translation (including its phrasal alignment to the original text) as θ · f , where f is a vector of the following 8 features: FinnishEnglish 5.02 10.27 16.43 FrenchEnglish 5.31 26.16 27.31 GermanEnglish 7.43 20.94 21.30 Table 1: B LEU 4n1 percentage on translating 2000sentence test corpora, after training the 8 experts on 100,000 sentence pairs and fitting their weights θ on 200 more, using settings tuned on a further 200. The current minimum risk annealing method achieved significant improvements"
P06-2101,N04-1022,0,0.34905,"of θ. Maximum likelihood is not sensitive to the starting value of θ because it has only a global optimum; annealed minimum risk is not sensitive to it either, because initially γ ≈ 0, making equation (6) flat. • maximum likelihood: a Gaussian prior with all σd2 at 0.25, 0.5, 1, or ∞ • minimum error: 1, 5, or 10 different random starting points, drawn from a uniform 792 Bleu 20 22 approximation from §5. We found this to perform significantly better on B LEU evaluation than if we trained with a “linearized” B LEU that summed per-sentence B LEU scores (as used in minimum Bayes risk decoding by Kumar and Byrne (2004)). Dependency Parsing 18 6.2 We trained dependency parsers for three different languages: Bulgarian, Dutch, and Slovenian.11 Input sentences to the parser were already tagged for parts of speech. Each parser employed 10 experts, each parameterized as a globally normalized loglinear model (Lafferty et al., 2001). For example, the 9th component of the feature vector fi,k (which described the k th parse of the ith sentence) was the log of that parse’s normalized probability according to the 9th expert. Each expert was trained separately to maximize the conditional probability of the correct parse"
P06-2101,P03-1021,0,0.659302,"ble translations or parse trees; or only the Ki most probable under ∗ This work was supported by an NSF graduate research fellowship for the first author and by NSF ITR grant IIS0313193 and ONR grant N00014-01-1-0685. The views expressed are not necessarily endorsed by the sponsors. We thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and the reviewers for helpful discussions and comments. 787 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 787–794, c Sydney, July 2006. 2006 Association for Computational Linguistics constant, hence not amenable to gradient descent. Och (2003) observed, however, that the piecewiseconstant property could be exploited to characterize the function exhaustively along any line in parameter space, and hence to minimize it globally along that line. By calling this global line minimization as a subroutine of multidimensional optimization, he was able to minimize (2) well enough to improve over likelihood maximization for training factored machine translation systems. Instead of considering only the best hypothesis for any θ, we can minimize risk, i.e., the expected loss under pθ across all analyses yi : XX def min Epθ L(yi,k ) = min L(yi,k"
P06-2101,P02-1040,0,0.0894142,"mbinations of models for dependency parsing and for machine translation. In machine translation, annealed minimum risk training achieves significant improvements in B LEU over standard minimum error training. We also show improvements in labeled dependency parsing. 1 Direct Minimization of Error Researchers in empirical natural language processing have expended substantial ink and effort in developing metrics to evaluate systems automatically against gold-standard corpora. The ongoing evaluation literature is perhaps most obvious in the machine translation community’s efforts to better B LEU (Papineni et al., 2002). Despite this research, parsing or machine translation systems are often trained using the much simpler and harsher metric of maximum likelihood. One reason is that in supervised training, the log-likelihood objective function is generally convex, meaning that it has a single global maximum that can be easily found (indeed, for supervised generative models, the parameters at this maximum may even have a closed-form solution). In contrast to the likelihood surface, the error surface for discrete structured prediction is not only riddled with local minima, but piecewise constant 2 Training Log-"
P06-2101,P04-1062,1,0.688242,"ccording to some schedule and optimize θ again. When γ is low, the smooth objective might allow us to pass over local minima that could open up at higher γ. Figure 3 shows how the smoothing is gradually weakened to reach the risk objective (3) as γ → 1 and approach the true error objective (2) as γ → ∞. Our risk minimization most resembles the work of Rao and Rose (2001), who trained an isolatedword speech recognition system for expected word-error rate. Deterministic annealing has also been used to tackle non-convex likelihood surfaces in unsupervised learning with EM (Ueda and Nakano, 1998; Smith and Eisner, 2004). Other work on “generalized probabilistic descent” minimizes a similar objective function but with γ held constant (Katagiri et al., 1998). Although the entropy is generally higher at lower values of γ, it varies as the optimization changes θ. In particular, a pure unregularized loglinear model such as (5) is really a function of γ ·θ, so the optimizer could exactly compensate for increased γ by decreasing the θ vector proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose γ and θ accordingly: 17.5 γ=∞ γ = 0.1 γ=1 γ = 10"
P06-2101,P05-1003,0,0.0175745,"ised generative models, the parameters at this maximum may even have a closed-form solution). In contrast to the likelihood surface, the error surface for discrete structured prediction is not only riddled with local minima, but piecewise constant 2 Training Log-Linear Models In this work, we focus on rescoring with loglinear models. In particular, our experiments consider log-linear combinations of a relatively small number of features over entire complex structures, such as trees or translations, known in some previous work as products of experts (Hinton, 1999) or logarithmic opinion pools (Smith et al., 2005). A feature in the combined model might thus be a log probability from an entire submodel. Giving this feature a small or negative weight can discount a submodel that is foolishly structured, badly trained, or redundant with the other features. For each sentence xi in our training corpus S, we are given Ki possible analyses yi,1 , . . . yi,Ki . (These may be all of the possible translations or parse trees; or only the Ki most probable under ∗ This work was supported by an NSF graduate research fellowship for the first author and by NSF ITR grant IIS0313193 and ONR grant N00014-01-1-0685. The v"
P06-2101,W04-3201,0,0.0139034,"ins on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000). We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder. Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004). Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones. The distinction is in using a loss function to calculate the required margins. 8 Conclusions Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems. Different methods can be used to attempt this global, non-convex optimization. We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to op"
P11-1011,D08-1107,0,0.49834,"rmation retrieval (IR) applications. Accordingly, in this paper, we focus on annotating search queries submitted by the users to a search engine. There are several key differences between user queries and the documents used in NLP (e.g., news 102 David A. Smith Dept. of Computer Science University of Massachusetts Amherst, MA dasmith@cs.umass.edu articles or web pages). As previous research shows, these differences severely limit the applicability of standard NLP techniques for annotating queries and require development of novel annotation approaches for query corpora (Bergsma and Wang, 2007; Barr et al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li, 2010). The most salient difference between queries and documents is their length. Most search queries are very short, and even longer queries are usually shorter than the average written sentence. Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chunkers, which are trained on more syntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, tha"
P11-1011,D07-1086,0,0.279611,"much less common in information retrieval (IR) applications. Accordingly, in this paper, we focus on annotating search queries submitted by the users to a search engine. There are several key differences between user queries and the documents used in NLP (e.g., news 102 David A. Smith Dept. of Computer Science University of Massachusetts Amherst, MA dasmith@cs.umass.edu articles or web pages). As previous research shows, these differences severely limit the applicability of standard NLP techniques for annotating queries and require development of novel annotation approaches for query corpora (Bergsma and Wang, 2007; Barr et al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li, 2010). The most salient difference between queries and documents is their length. Most search queries are very short, and even longer queries are usually shorter than the average written sentence. Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chunkers, which are trained on more syntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009)"
P11-1011,N09-1037,0,0.0414793,"ation, the focus of the latter is on providing linguistic information about existing queries (after initial refinement has been performed). Such information is especially important for more verbose and gramatically complex queries. In addition, while all the methods proposed by Guo et al. (2008) require large amounts of training data (thousands of training examples), our joint annotation method can be effectively trained with a minimal human labeling effort (several hundred training examples). An additional research area which is relevant to this paper is the work on joint structure modeling (Finkel and Manning, 2009; Toutanova et al., 2008) and stacked classification (Nivre and McDonald, 2008; Martins et al., 2008) in natural language processing. These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al., 2008). Similarly to this work in NLP, we demonstrate that a joint approach for modeling the linguistic query structure can also be beneficial for IR applications. 6 Experiments 6.1 Experimental Setup For evaluating the performance of"
P11-1011,N07-1028,0,0.0304923,"mation about estimated partsof-speech tags and capitalization of query terms to improve the accuracy of query segmentation. We empirically evaluate the joint query annotation method on a range of query types. Instead of just focusing our attention on keyword queries, as is often done in previous work (Barr et al., 2008; Bergsma and Wang, 2007; Tan and Peng, 2008; Guo et al., 2008), we also explore the performance of our annotations with more complex natural language search queries such as verbal phrases and whquestions, which often pose a challenge for IR applications (Bendersky et al., 2010; Kumaran and Allan, 2007; Kumaran and Carvalho, 2009; Lease, 2007). 103 We show that even with a very limited amount of training data, our joint annotation method significantly outperforms annotations that were done independently for these queries. The rest of the paper is organized as follows. In Section 2 we demonstrate several examples of annotated search queries. Then, in Section 3, we introduce our joint query annotation method. In Section 4 we describe two types of independent query annotations that are used as input for the joint query annotation. Section 5 details the related work and Section 6 presents the e"
P11-1011,P10-1136,0,0.596858,"per, we focus on annotating search queries submitted by the users to a search engine. There are several key differences between user queries and the documents used in NLP (e.g., news 102 David A. Smith Dept. of Computer Science University of Massachusetts Amherst, MA dasmith@cs.umass.edu articles or web pages). As previous research shows, these differences severely limit the applicability of standard NLP techniques for annotating queries and require development of novel annotation approaches for query corpora (Bergsma and Wang, 2007; Barr et al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li, 2010). The most salient difference between queries and documents is their length. Most search queries are very short, and even longer queries are usually shorter than the average written sentence. Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chunkers, which are trained on more syntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, that despite their brevity, queries are grammatically d"
P11-1011,D09-1068,0,0.153284,"IR) applications. Accordingly, in this paper, we focus on annotating search queries submitted by the users to a search engine. There are several key differences between user queries and the documents used in NLP (e.g., news 102 David A. Smith Dept. of Computer Science University of Massachusetts Amherst, MA dasmith@cs.umass.edu articles or web pages). As previous research shows, these differences severely limit the applicability of standard NLP techniques for annotating queries and require development of novel annotation approaches for query corpora (Bergsma and Wang, 2007; Barr et al., 2008; Lu et al., 2009; Bendersky et al., 2010; Li, 2010). The most salient difference between queries and documents is their length. Most search queries are very short, and even longer queries are usually shorter than the average written sentence. Due to their brevity, queries often cannot be divided into sub-parts, and do not provide enough context for accurate annotations to be made using the standard NLP tools such as taggers, parsers or chunkers, which are trained on more syntactically coherent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, that despite their b"
P11-1011,P09-1097,0,0.538924,"ent textual units. A recent analysis of web query logs by Bendersky and Croft (2009) shows, however, that despite their brevity, queries are grammatically diverse. Some queries are keyword concatenations, some are semicomplete verbal phrases and some are wh-questions. It is essential for the search engine to correctly annotate the query structure, and the quality of these query annotations has been shown to be a crucial first step towards the development of reliable and robust query processing, representation and understanding algorithms (Barr et al., 2008; Guo et al., 2008; Guo et al., 2009; Manshadi and Li, 2009; Li, 2010). However, in current query annotation systems, even sentence-like queries are often hard to parse and annotate, as they are prone to contain misspellings and idiosyncratic grammatical structures. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 102–111, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Term who won the 2004 kentucky derby (a) CAP L L L L C C TAG X V X X N N SEG B I B B B I Term kindred where would i be (b) CAP C C C C C TAG N X X X V SEG B B I I I Term shih tzu health problems (c) CAP"
P11-1011,D08-1017,0,0.0499191,"a label and ZQ t ∗(J) Predict annotation zQh , using CRF(zQt ). ∗(J) ∗(J) ∗(J) ZQh ← ZQh ∪ zQh . Output: ZQh — predicted annotations for the held-out set of queries. Input: ∗(J) Figure 2: Algorithm for performing joint query annotation. Note that this formulation of joint query annotation can be viewed as a stacked classification, in which a second, more effective, classifier is trained using the labels inferred by the first classifier as features. Stacked classifiers were recently shown to be an efficient and effective strategy for structured classification in NLP (Nivre and McDonald, 2008; Martins et al., 2008). 4 Independent Query Annotations While the joint annotation method proposed in Section 3 is general enough to be applied to any set of independent query annotations, in this work we focus on two previously proposed independent annotation methods based on either the query itself, or the top sentences retrieved in response to the query (Bendersky et al., 2010). The main benefits of these two annotation methods are that they can be easily implemented using standard software tools, do not require any labeled data, and provide reasonable annotation accuracy. Next, we briefly describe these two ind"
P11-1011,P08-1108,0,0.164202,"del CRF(zQt ) using zQt as a label and ZQ t ∗(J) Predict annotation zQh , using CRF(zQt ). ∗(J) ∗(J) ∗(J) ZQh ← ZQh ∪ zQh . Output: ZQh — predicted annotations for the held-out set of queries. Input: ∗(J) Figure 2: Algorithm for performing joint query annotation. Note that this formulation of joint query annotation can be viewed as a stacked classification, in which a second, more effective, classifier is trained using the labels inferred by the first classifier as features. Stacked classifiers were recently shown to be an efficient and effective strategy for structured classification in NLP (Nivre and McDonald, 2008; Martins et al., 2008). 4 Independent Query Annotations While the joint annotation method proposed in Section 3 is general enough to be applied to any set of independent query annotations, in this work we focus on two previously proposed independent annotation methods based on either the query itself, or the top sentences retrieved in response to the query (Bendersky et al., 2010). The main benefits of these two annotation methods are that they can be easily implemented using standard software tools, do not require any labeled data, and provide reasonable annotation accuracy. Next, we briefly"
P11-1011,J08-2002,0,0.0360335,"Missing"
P11-1089,P08-1043,0,0.427248,"amantis lovers omnis all Figure 1: Dependency tree for the sentence “Una dies omnis potuit praecurrere amantis”. The word omnis is an adjective modifying the noun amantis. This information is key to the morphological disambiguation of both words, as shown in Table 1. such as Turkish (Hakkani-T¨ur et al., 2000) and Czech (Hajiˇc et al., 2001), did not consider syntactic relationships between words. In the literature on data-driven parsing, two recent studies attempted joint inference on morphology and syntax, and both considered phrase-structure trees for Modern Hebrew (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008). The primary focus of morphological processing in Modern Hebrew is splitting orthographic words into morphemes: clitics such as prepositions, pronouns, and the definite article must be separated from the core word. Each of the resulting morphemes is then tagged with an atomic “part-of-speech” to indicate word class and some morphological features. Similarly, the English POS tags in the Penn Treebank combine word class information with morphological attributes such as “plural” or “past tense”. Cohen and Smith (2007) separately train a discriminative conditional random field (CRF) for segmentat"
P11-1089,P96-1024,0,0.0375614,"number of belief propagation iterations and the number of training rounds, were tuned on the development set. 5.3 Decoding The output of the joint model is the assignment to the TAG and L INK variables. Loopy belief propagation (BP) was used to calculate the posterior probabilities of these variables. For TAG, we emit the tag with the highest posterior probability as computed by sum-product BP. We produced head attachments by first calculating the posteriors of the L INK variables with BP and then passing them to an edgefactored tree decoder. This is equivalent to minimum Bayes risk decoding (Goodman, 1996), which is used by Cohen and Smith (2007) and Smith and Eisner (2008). This MBR decoding procedure enforces the hard constraint that the output be a tree but sums over possible morphological assignments.5 5.4 Reducing Model Complexity In principle, the joint model should consider every possible combination of morphological attributes for every word. In practice, to reduce the complexity of the model, we used a pre-existing morphological database, M ORPHEUS (Crane, 1991), to constrain the range of possible values of the attributes listed in Table 2; more precisely, we add a hard constraint, req"
P11-1089,C00-1042,0,0.21484,"Missing"
P11-1089,H05-1066,0,0.0604475,"Missing"
P11-1089,P05-1012,0,0.660921,"d to a TAG variable. The factor fires when Ta,i,v is true. The features consist of the value v of the morphological attribute concerned, combined with the word identity of wi , with backoff using all suffixes of the word. The CASEUNIGRAM factors shown in Figure 2 are examples of this family of factors. • TAG -B IGRAM: There are O(nm2 ) of such binary factors, each connected to the TAG variables of a pair of neighboring words. The factor fires when Ta,i,v1 and Ta,i+1,v2 are both true. The CASE-BIGRAM factors shown in Figure 2 are examples of this family of factors. Beyond these basic features, McDonald et al. (2005) also utilize POS trigrams and POS 4grams. Both include the POS of two linked words, wi and wj . The third component in the trigrams is the POS of each word wk located between wi and wj , i < k < j. The two additional components that make up the 4-grams are subsets of the POS of words located to the immediate left and right of wi and wj . • TAG -C ONSISTENCY: For each word, the TAG variables representing the possible POS values are connected to those representing the values of other morphological attributes, yielding O(nm2 ) binary factors. They fire when Tpos,i,v1 and Ta,i,v2 are both true. T"
P11-1089,H05-1060,1,0.188147,"Missing"
P11-1089,D08-1016,1,0.756257,"8000 sentences from the Ancient Greek Dependency Treebank (Bamman et al., 2009), 5800 from the Hungarian Szeged Dependency Treebank (Vincze et al., 2010), and a subset of 3100 from the Prague Dependency Treebank (B¨ohmov´a et al., 2003). 890 Training We define each factor in (1) as a log-linear function: Fk (A) = exp X θh fh (A, W, k) (2) h Given an assignment A and words W , fh is an indicator function describing the presence or absence of the feature, and θh is the corresponding set of weights learned using stochastic gradient ascent, with the gradients inferred by loopy belief propagation (Smith and Eisner, 2008). The variance of the Gaussian prior is set to 1. The other two parameters in the training process, the number of belief propagation iterations and the number of training rounds, were tuned on the development set. 5.3 Decoding The output of the joint model is the assignment to the TAG and L INK variables. Loopy belief propagation (BP) was used to calculate the posterior probabilities of these variables. For TAG, we emit the tag with the highest posterior probability as computed by sum-product BP. We produced head attachments by first calculating the posteriors of the L INK variables with BP an"
P11-1089,N03-1033,0,0.0806641,"Missing"
P11-1089,P06-3009,0,0.112221,"Missing"
P11-1089,D07-1022,0,\N,Missing
P11-1089,W06-2920,0,\N,Missing
P11-1089,P01-1035,0,\N,Missing
P11-1089,vincze-etal-2010-hungarian,0,\N,Missing
P11-1089,D07-1096,0,\N,Missing
P18-1220,W11-2123,0,0.00863221,"h OCR’d line into the corresponding manual transcription, i.e., the human annotation. We call the correction model trained in this setting Seq2Seq-Super. Unsupervised Training. In the absence of ground truth transcriptions, we can use different methods to generate a noisy corrected version for each OCR’d line. (a) Noisy Training. In this setting, the correction model is trained to transform each OCR’d text line to a selected high-quality witness. The quality of the witnesses is measured by a 5-gram character language model built on the New York Time Corpus (Sandhaus, 2008) with KenLM toolkit (Heafield, 2011). For each OCR’d line with multiple witnesses, a score is assigned to each witness by the language model, divided by the number of characters in it to reduce the effect of the length of a witness. Then a witness with the highest score is chosen as the noisy ground truth for each line. Those lines with low score for all witnesses are removed. We call the correction model trained in this setting Seq2Seq-Noisy. (b) Synthetic Training. In this setting, the error correction model is trained to recover a manually corrupted out-of-domain corpus. We construct the synthetic dataset by injecting uniform"
P18-1220,P82-1020,0,0.73721,"Missing"
P18-1220,C16-1160,0,0.300306,"our model has 3 layers with 400 hidden units for each layer, where GRU is applied as the dynamic function. Adam optimizer with a learning rate of 0.0003 and default decay rates is used to train the correction model . We train up to 40 epochs with a minibatch size of 128 and select the model with the lowest perplexity on the development set. The decoder implements beam search with a beam width of 100. 2367 4.1.2 Baselines and Comparisons In preliminary experiments, we first compare the neural translation model (§3.2) with a commonly used Seq2Seq model, pruned conditional random fields (PCRF) (Schnober et al., 2016) on the single-input correction task. CRF models have been shown to be very competitve on tasks such as OCR post-correction, spelling correction, and lemmatization. After that, we compare the different multi-input attention strategies introduced in §3.3 on multi-input correction task to choose the best strategy for the main experiments. In the main experiment, we compare the performance of correction models trained in different training settings and decode with and without multiple witnesses. Two ensembles methods, language model ranking (LMR) and majority vote (Xu and Smith, 2017), are also c"
P18-1220,N03-1018,0,0.0369014,"OCR are exacerbated in library-scale digitization by commercial (e.g., Google Books, Newspapers.com), government (e.g., Library of Congress, Biblioth`eque nationale de France), and nonprofit (e.g., Internet Archive) organizations. The scale of these projects not only makes it difficult to adapt OCR models to their diverse layouts and typefaces but also makes it impractical to present any OCR output other than a single-best transcript. Existing methods for automatic OCR postcorrection are mostly supervised methods that correct recognition errors in a single OCR output (Kolak and Resnik, 2002; Kolak et al., 2003; Yamazoe et al., 2011). Those systems are not scalable since human annotations are expensive to acquire, and they are not capable of utilizing complementary sources of information. Another line of work is ensemble methods (Lund et al., 2013, 2014) combining OCR results from multiple scans of the same document. Most of these ensemble methods, however, require aligning multiple OCR outputs (Lund and Ringger, 2009; Lund et al., 2011), which is intractable in general and might introduce noise into the later correction stage. Furthermore, voting-based ensemble methods (Lund and Ringger, 2009; Wemh"
P18-1220,P17-2031,0,0.0886598,"Missing"
P18-1220,N16-1007,0,0.0167572,"und et al., 2011; Al Azawi et al., 2015), or unsupervised methods such as dictionary-based selection (Lund and Ringger, 2009) and voting (Wemhoener et al., 2013; Xu and Smith, 2017). While supervised ensemble methods require human annotation for training, unsupervised selection methods work only when the correct word or character exists in one of the inputs. Furthermore, those methods could not correct single inputs. Multi-Input Attention. Multi-input attention has already been explored in tasks such as machine translation (Zoph and Knight, 2016; Libovick´y and Helcl, 2017) and summarization (Wang and Ling, 2016). Wang and Ling (2016) propose to concatenate multiple inputs to generate a summary; this flat attention combination model might be affected by the order of input sequences. Zoph and Knight (2016) aims at developing a multisource translation model on a trilingual corpus where the encoder for each language is combined to pass to the decoder; however, it requires the same number of inputs at training and decoding time since the parameters depend on the number of inputs. Libovick´y and Helcl (2017) explore different attention combination strategies for multiple information sources such as image a"
P18-1220,N16-1004,0,0.38787,"erroneous OCR’d text unit to either its high-quality duplication or a consensus correction among its duplications via bootstrapping from an uniform error model. The baseline correction system is a sequence-to-sequence model with attention (Bahdanau et al., 2015), which has been shown to be effective in text correction tasks (Chollampatt et al., 2016; Xie et al., 2016). We also seek to improve the correction performance for duplicated texts by integrating multiple inputs. Previous work on combining multiple inputs in neural translation deal with data from different domains, e.g., multilingual (Zoph and Knight, 2016) or multimodal (Libovick´y and Helcl, 2017) data. Therefore, their models need to be trained on multiple inputs to learn parameters to combine inputs from each domain. Given that the inputs of our task are all from the same domain, our model is trained on a single input and introduces multi-input attention to generate a consensus result merely for decoding. It does not require learning extra parameters for attention combination and thus is more efficient to train. Furthermore, average attention combination, a simple multi-input attention mechanism, is proposed to improve both the effectiveness"
P18-1220,N12-1024,0,0.0217214,"anguage model ranking (LMR) and majority vote (Xu and Smith, 2017), are also considered as unsupervised baseline methods. LMR chooses a single high-quality witness for each OCR’d line by a language model as the correction for that line. Majority vote first aligns multiple input sequences using a greedy pairwise algorithm (since multiple sequence alignment is intractable) and then votes on each position in the alignment, with a slight advantage given to the original OCR output in case of ties. We also tried to use an exact unsupervised method for consensus decoding based on dual decomposition (Paul and Eisner, 2012). Their implementation, unfortunately, turned out not to return a certificate of completion on most lines in our data even after thousands of iterations. 4.1.3 Evaluation Metrics Word error rate (WER) and character error rate (CER) are used to compare the performance of each method. Case is ignored. Lattice word error rate (LWER) and lattice character error rate (LCER) are also computed as the oracle performance for each method, which could reveal the capability of each model to be applied to downstream tasks taking lattices as input, e.g., reranking or retrieval of the correction hypotheses ("
W03-0107,W03-0405,1,0.465632,"xample, we cannot find a classifier for “Oxford”, but can tell that a story is about Mississippi, we will still be able to disambiguate. We use a gazetteer to restrict the set of candidate states and countries for a given place name. In trying to disambiguate “Portland”, we would thus consider Oregon, Maine, and England, among other options, but not Maryland. As in the word sense disambiguation task as usually defined, we are classifying names and not clustering them. This approach is practical for geographic names, for which broad-coverage gazetteers exist, though less so for personal names (Mann and Yarowsky, 2003). System performance is measured with reference to the naive baseline where each ambiguous toponym is guessed to be the most commonly occurring place. London, England, would thus always be guessed rather than London, Ontario. Bootstrapping methods similar to ours have been shown to be competitive in word sense disambiguation (Yarowsky and Florian, 2003; Yarowsky, 1995). 3 Difficulty of the Task Our ability to disambiguate place names should be weighed against the ease or difficulty of the task. In a world where most toponyms referred unambiguously to one place, we would not be impressed by nea"
W03-0107,A97-1030,0,\N,Missing
W03-0107,P95-1026,0,\N,Missing
W04-3207,J00-1004,0,0.0292845,"Missing"
W04-3207,J90-2002,0,0.0283338,"Missing"
W04-3207,J93-2003,0,0.00752304,"Missing"
W04-3207,2003.mtsummit-papers.6,0,0.0904424,"Missing"
W04-3207,P03-1012,0,0.0208107,"Missing"
W04-3207,J94-4004,0,0.0257474,"Missing"
W04-3207,P99-1059,0,0.0198494,"ncluding our bilingual parsing algorithm. We close by reviewing prior work in areas related to this paper (§5). 2 Bilingual parsing The joint model used by our bilingual parser is an instance of a stochastic bilingual multitext grammar (2MTG), formally defined by Melamed (2003). The 2MTG formalism generates two strings such that each syntactic constituent—including individual words—in one side of the bitext corresponds either to a constituent in the other side or to ∅. Melamed defines bilexicalized MTG (L2 MTG), which is a synchronous extension of bilexical grammars such as those described in Eisner and Satta (1999) and applies the latter’s algorithmic speedups to L2 MTG-parsing. Our formalism is not a precise fit to either unlexicalized MTG or L2 MTG since we posit lexical dependency structure only in one of the languages (English). The primary rationale for this is that we are dealing with only a small quantity of labeled data in language L and therefore do not expect to be able to accurately estimate its lexical affinities. Further, synchronous parsing is in practice computationally expensive, and eliminating lexicalization on one side reduces the run-time of the parser from O(n8 ) to O(n7 ). Our pars"
W04-3207,P03-2041,0,0.0405277,"Missing"
W04-3207,W02-1039,0,0.0365437,"Missing"
W04-3207,P03-1011,0,0.0250551,"Missing"
W04-3207,P02-1050,0,0.101719,"We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data. 1 Introduction Consider the problem of parsing a language L for which annotated resources like treebanks are scarce. Suppose we have a small amount of text data with syntactic annotations and a fairly large corpus of parallel text, for which the other language (e.g., English) is not resourceimpoverished. How might we exploit English parsers to improve syntactic analysis tools for this language? One idea (Yarowsky and Ngai, 2001; Hwa et al., 2002) is to project English analysis onto L data, “through” word-aligned parallel text. To do this, we might use an English parser to analyze the English side of the parallel text and a word-alignment algorithm to induce word correspondences. By positing a coupling of English syntax with L syntax, we can induce structure on the L side of the parallel text that is in some sense isomorphic to the English parse. We might take the projection idea a step farther. A statistical English parser can tell us much more than the hypothesized best parse. It can be used to find every parse admitted by a grammar,"
W04-3207,P99-1069,0,0.0460784,"Missing"
W04-3207,N03-1021,0,0.0210817,"f joint English-parse, L-parse, and word-alignment inference. In §3 we describe parameter estimation for each of the factored models, including novel applications of log-linear models to English dependency parsing and Korean morphological analysis. §4 presents Korean parsing results with various monolingual and bilingual algorithms, including our bilingual parsing algorithm. We close by reviewing prior work in areas related to this paper (§5). 2 Bilingual parsing The joint model used by our bilingual parser is an instance of a stochastic bilingual multitext grammar (2MTG), formally defined by Melamed (2003). The 2MTG formalism generates two strings such that each syntactic constituent—including individual words—in one side of the bitext corresponds either to a constituent in the other side or to ∅. Melamed defines bilexicalized MTG (L2 MTG), which is a synchronous extension of bilexical grammars such as those described in Eisner and Satta (1999) and applies the latter’s algorithmic speedups to L2 MTG-parsing. Our formalism is not a precise fit to either unlexicalized MTG or L2 MTG since we posit lexical dependency structure only in one of the languages (English). The primary rationale for this i"
W04-3207,J03-1002,0,0.00410479,"Missing"
W04-3207,P02-1035,0,0.0608816,"Missing"
W04-3207,W02-2207,0,0.0439253,"Missing"
W04-3207,C90-3045,0,0.0272605,"Missing"
W04-3207,N03-1031,0,0.0192336,"Missing"
W04-3207,J97-3002,0,0.2764,"Missing"
W04-3207,W00-1208,0,0.0458292,"Missing"
W04-3207,P01-1067,0,0.0767938,"Missing"
W04-3207,N01-1026,0,0.108151,"te parameter estimation. We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data. 1 Introduction Consider the problem of parsing a language L for which annotated resources like treebanks are scarce. Suppose we have a small amount of text data with syntactic annotations and a fairly large corpus of parallel text, for which the other language (e.g., English) is not resourceimpoverished. How might we exploit English parsers to improve syntactic analysis tools for this language? One idea (Yarowsky and Ngai, 2001; Hwa et al., 2002) is to project English analysis onto L data, “through” word-aligned parallel text. To do this, we might use an English parser to analyze the English side of the parallel text and a word-alignment algorithm to induce word correspondences. By positing a coupling of English syntax with L syntax, we can induce structure on the L side of the parallel text that is in some sense isomorphic to the English parse. We might take the projection idea a step farther. A statistical English parser can tell us much more than the hypothesized best parse. It can be used to find every parse adm"
W04-3207,J98-4003,0,\N,Missing
W04-3207,W90-0102,0,\N,Missing
W06-2929,P96-1023,0,0.0130127,"set D. In this work, the graph is constrained to be a projective tree rooted at $: each word except $ has a single parent, and there are no cycles or crossing dependencies. Using a simple dynamic program to find the minimum-error projective parse, we find that assuming projectivity need not harm accuracy very much (Tab. 1, col. 3). 3 Unlabeled Parsing The first component of our system is an unlabeled parser that, given a sentence, finds the U best unlabeled trees under a probabilistic model using a bottom-up dynamic programming algorithm.2 The model is a probabilistic head automaton grammar (Alshawi, 1996) that assumes conditional indepen1 We used words and fine tags in our parser and labeler, with coarse tags in one backoff model. Other features are used in reranking; we never used the given morphological features or the “projective” annotations offered in the training data. 2 The execution model we use is best-first, exhaustive search, as described in Eisner et al. (2004). All of our dynamic programming algorithms are implemented concisely in the Dyna language. 201 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), c pages 201–205, New York City, June 200"
W06-2929,P05-1022,0,0.0897925,"e system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). The system is designed for fast training and decoding and for high precision. We describe sources of crosslingual error and ways to ameliorate them. We then provide a detailed error analysis of parses produced for sentences in German (much training data) and Arabic (little training data). 1 Introduction Standard state-of-the-art parsing systems (e.g., Charniak and Johnson, 2005) typically involve two passes. First, a parser produces a list of the most likely n parse trees under a generative, probabilistic model (usually some flavor of PCFG). A discriminative reranker then chooses among trees in this list by using an extended feature set (Collins, 2000). This paradigm has many advantages: PCFGs are fast to train, can be very robust, and perform better as more data is made available; and rerankers train quickly (compared to discriminative models), require few parameters, and permit arbitrary features. We describe such a system for dependency parsing. Our shared task en"
W06-2929,P99-1059,0,0.0617705,"Missing"
W06-2929,W05-1504,1,0.743677,"Missing"
W06-2929,1997.iwpt-1.10,0,0.159707,"Missing"
W06-2929,W05-1506,0,0.02202,"show oracle, 1-best, and reranked performance on the test set at different stages of the system. Boldface marks oracle performance that, given perfect downstream modules, would supercede the best system. Italics mark the few cases where the reranker increased error rate. Columns 8–10 show labeled accuracy; column 10 gives the final shared task evaluation scores. dence between the left yield and the right yield of a given head, given the head (Eisner, 1997).3 The best known parsing algorithm for such a model is O(n3 ) (Eisner and Satta, 1999). The U -best list is generated using Algorithm 3 of Huang and Chiang (2005). 3.1 Vine parsing (dependency length bounds) Following Eisner and N. Smith (2005), we also impose a bound on the string distance between every 3 To empirically test this assumption across languages, we measured the mutual information between different features of yleft (j) and yright (j), given xj . (Mutual information is a statistic that equals zero iff conditional independence holds.) A detailed discussion, while interesting, is omitted for space, but we highlight some of our findings. First, unsurprisingly, the splithead assumption appears to be less valid for languages with freer word ord"
W06-2929,H05-1066,0,0.181384,"Missing"
W06-2929,P06-2101,1,0.782705,"beled labeling model is a major source of error. 4 Our infrastructure provides a concise, interpreted language for expressing the models to be mixed, so large-scale combination and comparison are possible. 203 5 We tested first-order Markov models that conditioned on parent or MRSSS dependency labels. 5 Reranking We train a log-linear model combining many feature scores (see below), including the log-probabilities from the parser and labeler. Training minimizes the expected error under the model; we use deterministic annealing to smooth the error surface and avoid local minima (Rose, 1998; D. Smith and Eisner, 2006). We reserved 200 sentences in each language for training the reranker, plus 200 for choosing among rerankers trained on different feature sets and different (U × L)-best lists.6 Features Our reranking features predict tags, labels, lemmata, suffixes and other information given all or some of the following non-local conditioning context: bigrams and trigrams of tags or dependency labels; parent and grandparent dependency labels; subcategorization frames (in terms of tags or dependency labels); the occurrence of certain tags between head and child; surface features like the lemma7 and the 3-cha"
W06-2929,W03-2403,0,\N,Missing
W06-2929,dzeroski-etal-2006-towards,0,\N,Missing
W06-2929,W03-2405,0,\N,Missing
W06-2929,afonso-etal-2002-floresta,0,\N,Missing
W06-3104,J00-1004,0,0.0255115,"ransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V aligned to V0 and NP align"
W06-3104,J93-2003,0,0.0297629,"ition of the PP auf diese Frage either syntactically (by invoking scrambling) or semantically (by describing a deep analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V"
W06-3104,P05-1067,0,0.0992068,"equivalent conditional process for generating T2 , A given T1 . 24 deterministically assembles the latter rules into an actual tree T2 and reads off its yield S2 . What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1 . It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3 , or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1 , as IBM Models 1–5 do. Our alternative is to define a “quasi-synchronous” stochastic process. It generates T2 in a way that is not in thrall to T1 but is “inspired by it.” (A human tr"
W06-3104,J94-4004,0,0.0456684,"tactic alignment. Any portion of T2 can align to any portion of T1 , or to NULL. Nodes that are syntactically related in T1 do not have to translate into nodes that are syntactically related in T2 —although (1) is usually higher if they do. This property makes our approach especially promising for aligning freely, or erroneously, translated sentences, and for coping with syntactic diver23 Proceedings of the Workshop on Statistical Machine Translation, pages 23–30, c New York City, June 2006. 2006 Association for Computational Linguistics gences observed between even closely related languages (Dorr, 1994; Fox, 2002). We can patch together an alignment without accounting for all the details of the translation process. For instance, perhaps a source NP (figure 1) or PP (figure 2) appears “out of place” in the target sentence. A linguist might account for the position of the PP auf diese Frage either syntactically (by invoking scrambling) or semantically (by describing a deep analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it"
W06-3104,P03-2041,1,0.938449,"ead). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V aligned to V0 and NP aligned to NP0 . The process then continues"
W06-3104,W02-1039,0,0.013396,"ment. Any portion of T2 can align to any portion of T1 , or to NULL. Nodes that are syntactically related in T1 do not have to translate into nodes that are syntactically related in T2 —although (1) is usually higher if they do. This property makes our approach especially promising for aligning freely, or erroneously, translated sentences, and for coping with syntactic diver23 Proceedings of the Workshop on Statistical Machine Translation, pages 23–30, c New York City, June 2006. 2006 Association for Computational Linguistics gences observed between even closely related languages (Dorr, 1994; Fox, 2002). We can patch together an alignment without accounting for all the details of the translation process. For instance, perhaps a source NP (figure 1) or PP (figure 2) appears “out of place” in the target sentence. A linguist might account for the position of the PP auf diese Frage either syntactically (by invoking scrambling) or semantically (by describing a deep analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible"
W06-3104,P03-1011,0,0.0479976,"T researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V aligned to V0 and NP aligned to NP0 . The process then continues recursively by"
W06-3104,W04-3228,0,0.0127172,"h0 , p) = Y Y dir∈{l,r} c∈depsD (p,dir) 0 P (D(c) |a, a , c) × pstop (nostop |p, dir, adj) ×pchoose (c |p, dir) ×pconf ig (config) × ptrans (a |a0 ) pstop (stop |p, dir, adj) 4 Experiments We claim that for modeling human-translated bitext, it is better to project syntax only loosely. To evaluate this claim, we train quasi-synchronous dependency grammars that allow progressively more divergence from monotonic tree alignment. We evaluate these models on cross-entropy over held-out data and on error rate in a word-alignment task. One might doubt the use of dependency trees for alignment, since Gildea (2004) found that constituency trees aligned better. That experiment, however, aligned only the 1-best parse trees. We too will consider only the 1-best source tree T1 , but in constrast to Gildea, we will search for the target tree T2 that aligns best with T1 . Finding T2 and the alignment is simply a matter of parsing S2 with the QG derived from T1 . 4.1 Data and Training We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002). We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Mann"
W06-3104,P02-1050,0,0.0212717,"Missing"
W06-3104,P03-1054,0,0.00606541,"Gildea (2004) found that constituency trees aligned better. That experiment, however, aligned only the 1-best parse trees. We too will consider only the 1-best source tree T1 , but in constrast to Gildea, we will search for the target tree T2 that aligns best with T1 . Finding T2 and the alignment is simply a matter of parsing S2 with the QG derived from T1 . 4.1 Data and Training We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002). We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Manning, 2003). Initial estimates of lexical translation probabilities 28 came from the IBM Model 4 translation tables produced by G IZA ++ (Brown et al., 1993; Och and Ney, 2003). All text was lowercased and numbers of two or more digits were converted to an equal number of hash signs. The bitext was divided into training sets of 1K, 10K, and 100K sentence pairs. We held out one thousand sentences for evaluating the crossentropy of the various models and hand-aligned 100 sentence pairs to evaluate alignment error rate (AER). We trained the model parameters on bitext using the Expectation-Maximization (EM)"
W06-3104,P04-1061,0,0.166708,"linear model. When generating the children of a node in T2 , the process first generates their tags using monolingual parameters (fluency), and then fills in in the words using bilingual parameters (adequacy) that select and translate words from T1 .5 Concretely, each node in T2 is labeled by a triple (tag, word, aligned word). Given a parent node (p, h, h0 ) in T2 , we wish to generate sequences of left and right child nodes, of the form (c, a, a0 ). Our monolingual parameters come from a simple generative model of syntax used for grammar induction: the Dependency Model with Valence (DMV) of Klein and Manning (2004). In scoring dependency attachments, DMV uses tags rather than words. The parameters of the model are: 1. pchoose (c |p, dir): the probability of generating c as the next child tag in the sequence of dir children, where dir ∈ {lef t, right}. 2. pstop (s |h, dir, adj): the probability of generating no more child tags in the sequence of dir children. This is conditioned in part on the “adjacency” adj ∈ {true, f alse}, which indicates whether the sequence of dir children is empty so far. Our bilingual parameters score word-to-word translation and aligned dependency configurations. We thus use the"
W06-3104,P04-1084,0,0.0134172,"reads off its yield S2 . What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1 . It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3 , or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1 , as IBM Models 1–5 do. Our alternative is to define a “quasi-synchronous” stochastic process. It generates T2 in a way that is not in thrall to T1 but is “inspired by it.” (A human translator might be imagined to behave similarly.) When choosing how to expand nodes of T2 , we are influenced both by the structure of T1 and"
W06-3104,P04-1083,0,0.060538,"ay not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V aligned to V0 and NP aligned to NP0 . The process then continues recursively by choosing moves"
W06-3104,J03-1002,0,0.00527481,"e T1 , but in constrast to Gildea, we will search for the target tree T2 that aligns best with T1 . Finding T2 and the alignment is simply a matter of parsing S2 with the QG derived from T1 . 4.1 Data and Training We performed our modeling experiments with the German-English portion of the Europarl European Parliament transcripts (Koehn, 2002). We obtained monolingual parse trees from the Stanford German and English parsers (Klein and Manning, 2003). Initial estimates of lexical translation probabilities 28 came from the IBM Model 4 translation tables produced by G IZA ++ (Brown et al., 1993; Och and Ney, 2003). All text was lowercased and numbers of two or more digits were converted to an equal number of hash signs. The bitext was divided into training sets of 1K, 10K, and 100K sentence pairs. We held out one thousand sentences for evaluating the crossentropy of the various models and hand-aligned 100 sentence pairs to evaluate alignment error rate (AER). We trained the model parameters on bitext using the Expectation-Maximization (EM) algorithm. The T1 tree is fully observed, but we parse the target language. As noted, the initial lexical translation probabilities came from IBM Model 4. We initial"
W06-3104,P05-1034,0,0.0402607,"process for generating T2 , A given T1 . 24 deterministically assembles the latter rules into an actual tree T2 and reads off its yield S2 . What is worrisome about the synchronous process is that it can only produce trees T2 that are perfectly isomorphic to T1 . It is possible to relax this requirement by using synchronous grammar formalisms more sophisticated than CFG:2 one can permit unaligned nodes (Yamada and Knight, 2001), duplicated children (Gildea, 2003)3 , or alignment between elementary trees of differing sizes rather than between single rules (Eisner, 2003; Ding and Palmer, 2005; Quirk et al., 2005). However, one would need rather powerful and slow grammar formalisms (Shieber and Schabes, 1990; Melamed et al., 2004), often with discontiguous constituents, to account for all the linguistic divergences that could arise from different movement patterns (scrambling, wh-in situ) or free translation. In particular, a synchronous grammar cannot practically allow S2 to be any permutation of S1 , as IBM Models 1–5 do. Our alternative is to define a “quasi-synchronous” stochastic process. It generates T2 in a way that is not in thrall to T1 but is “inspired by it.” (A human translator might be ima"
W06-3104,C90-3045,0,0.0855646,"use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V aligned to V0 and NP aligned to NP0 . The process then continues recursively by choosing moves to expand these children. One can regard this stochastic process as an instance of analysis-transfer-synthesis MT. Analysis chooses a parse T1 given S1 . Transfer map"
W06-3104,J97-3002,0,0.150153,"analysistransfer-synthesis process in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V alig"
W06-3104,P01-1067,0,0.216614,"ess in the translator’s head). But an MT researcher may not have the wherewithal to design, adequately train, and efficiently compute with “deep” accounts of this sort. Under our approach, it is possible to use a simple, tractable syntactic model, but with some contextual probability of “sloppy” transfer. 1.2 From Synchronous to Quasi-Synchronous Grammars Because our approach will let anything align to anything, it is reminiscent of IBM Models 1–5 (Brown et al., 1993). It differs from the many approaches where (1) is defined by a stochastic synchronous grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003; Melamed, 2004) and from transfer-based systems defined by context-free grammars (Lavie et al., 2003). The synchronous grammar approach, originally due to Shieber and Schabes (1990), supposes that T2 is generated in lockstep to T1 .1 When choosing how to expand a certain VP node in T2 , a synchronous CFG process would observe that this node is aligned to a node VP0 in T1 , which had been expanded in T1 by VP0 → NP0 V0 . This might bias it toward choosing to expand the VP in T2 as VP → V NP, with the new children V aligned to V0 and NP aligned to NP0 . The process t"
W06-3104,W99-0604,0,\N,Missing
W06-3104,N04-1035,0,\N,Missing
W06-3104,J95-4002,0,\N,Missing
W06-3104,J06-2001,0,\N,Missing
W06-3104,N01-1026,0,\N,Missing
W06-3104,J03-4003,0,\N,Missing
W06-3104,P99-1059,1,\N,Missing
W06-3104,N03-1017,0,\N,Missing
W06-3104,N04-1021,1,\N,Missing
W06-3104,J99-4004,0,\N,Missing
W06-3104,W04-3312,0,\N,Missing
W06-3104,N03-1021,0,\N,Missing
W06-3104,W90-0102,0,\N,Missing
W11-2125,W11-1212,0,0.0131931,"we don’t utilize any existing MT type artifact. In other words, for a given language pair we don’t use translation lexicon by training an existing statistical machine translation system using sentence aligned parallel bilingual data in the same language or existing translation lexicon. Earlier work done by Enright and Kondrak (2007) uses only hapax words to represent and rank (based on the overlap number) translation documents pair in a parallel bilingual collection which is an easier task to evaluation due to the presence of a one-to-one matching among the bilingual documents. Most recently, Patry and Langlais (2011) show an improvement over this method by using an IR system to first retrieve translation document candidates and then identify translation document pairs by training a classifier. We start off by giving detailed explanation of the above mentioned data representation. We then test the feasibility of our approach using aligned parallel document data from three different bilingual collections in several languages and writing systems. Results from these tests are given in section 3. The goal of developing our approach was to utilize it as an initial filtering step in developing parallel corpora f"
W11-2125,J03-3002,0,0.0437937,"computational cost of O(n 2 ) document pair comparisons, we employ locality sensitive hashing (LSH) approximation algorithm for cosine similarity, which reduces our time complexity to O(n log n) . Introduction A dearth of parallel data has been, and still is, a major problem for developing highly reliable statistical machine translation systems in many languages and domains. There have been many proposed approaches for alleviating this problem by utilizing techniques for creating and extracting parallel documents, sentences or phrases from comparable bilingual data available on the open web (Resnik and Smith, 2003), such as Wikipedia articles (Smith et. al, 2010), to name a few, or through digitized archives from various sources (Zhao and Vogel, 2002), (Munteanu and Marcu, 2005). In general, in the process of utilizing comparable corpora to obtain sentence-aligned bilingual text, the first step involves performing initial filtering where text entities from both language collections are compared to each other and based on comparison score they are matched and grouped as potential translation candidate pairs. After this initial step, text entity pairs or tuples are further analyzed in order to extract par"
W11-2125,2005.mtsummit-papers.11,0,0.0414995,"d. We give min, max and median values over the number of words in each document. # doc. Lang. Pairs En Europarl 654 en-de De En Europarl 430 en-bg Bg En Europarl 642 en-es Es En Europarl 412 en-gr Gr En Newswire 230 en-ar Ar En UN en-ar 430 Ar Collection Figure 1. Process of creating and representing each document of a bilingual collection in an independent vector space. 3 3.1 Motivational Experiments Evaluation Collections We start off by evaluating the above proposed approach of determining candidate document translation pairs using three different parallel collections: Europarl, created by Koehn (2005), UN Arabic English Parallel Text (LDC2004E13) and the Arabic News Translation Part 1 (LDC2004T17). The purposes of first testing our approach using the Europarl corpus were twofold: This collection contains parallel documents (sessions of the European Parliament) that are further aligned at the speech and sentence level, which allows us to test alignment accuracy at several levels of granularity. Second, this collection contains parallel data from 209 Min 92 95 4872 4771 92 104 92 103 66 62 17672 15478 Max Median 109030 99753 59284 56907 109793 114770 93886 93304 47784 34272 71594 62448 46800"
W11-2125,P05-1077,0,0.327484,"Missing"
W11-2125,J05-4003,0,0.445169,"time complexity to O(n log n) . Introduction A dearth of parallel data has been, and still is, a major problem for developing highly reliable statistical machine translation systems in many languages and domains. There have been many proposed approaches for alleviating this problem by utilizing techniques for creating and extracting parallel documents, sentences or phrases from comparable bilingual data available on the open web (Resnik and Smith, 2003), such as Wikipedia articles (Smith et. al, 2010), to name a few, or through digitized archives from various sources (Zhao and Vogel, 2002), (Munteanu and Marcu, 2005). In general, in the process of utilizing comparable corpora to obtain sentence-aligned bilingual text, the first step involves performing initial filtering where text entities from both language collections are compared to each other and based on comparison score they are matched and grouped as potential translation candidate pairs. After this initial step, text entity pairs or tuples are further analyzed in order to extract parallel sentence pairs. In this paper we only focus on this initial step. We present a novel exploration of approaches that retrieve actual document translation pairs wi"
W11-2125,C10-1124,0,0.103212,"Missing"
W11-2125,N10-1063,0,0.0880011,"Missing"
W11-2125,N07-2008,0,0.272562,"late tf·idf score for the given document. Unlike other approaches, where documents or their word representations are first translated from foreign language to English using bilingual dictionary (Fung and Cheung, 2004), (Munteanu and Marcu, 2005) and (Uszkoreit et. al., 2010) in our approach we don’t utilize any existing MT type artifact. In other words, for a given language pair we don’t use translation lexicon by training an existing statistical machine translation system using sentence aligned parallel bilingual data in the same language or existing translation lexicon. Earlier work done by Enright and Kondrak (2007) uses only hapax words to represent and rank (based on the overlap number) translation documents pair in a parallel bilingual collection which is an easier task to evaluation due to the presence of a one-to-one matching among the bilingual documents. Most recently, Patry and Langlais (2011) show an improvement over this method by using an IR system to first retrieve translation document candidates and then identify translation document pairs by training a classifier. We start off by giving detailed explanation of the above mentioned data representation. We then test the feasibility of our appr"
W11-2125,D08-1090,0,0.0954721,"Missing"
W11-2125,W04-3208,0,0.0220433,"ty of all pairs, we use a randomized approximation algorithm based on locality sensitive hashing (LSH). For this joint approach, we represent each document in both languages using an n-dimensional feature vector template which consists of the set of intersecting words which are found across all documents in both language collections. For each dimension i.e. word, in the feature vector template we calculate tf·idf score for the given document. Unlike other approaches, where documents or their word representations are first translated from foreign language to English using bilingual dictionary (Fung and Cheung, 2004), (Munteanu and Marcu, 2005) and (Uszkoreit et. al., 2010) in our approach we don’t utilize any existing MT type artifact. In other words, for a given language pair we don’t use translation lexicon by training an existing statistical machine translation system using sentence aligned parallel bilingual data in the same language or existing translation lexicon. Earlier work done by Enright and Kondrak (2007) uses only hapax words to represent and rank (based on the overlap number) translation documents pair in a parallel bilingual collection which is an easier task to evaluation due to the prese"
W12-2510,P12-1094,0,0.0486069,"various aspects of writing including (but not lim8 In this work, we set n = 3, k = 50. This setting is done prior to seeing any labeled data. 73 Number of books Number of authors Total sentences After Na¨ıve Bayes filtering 21, 492 8, 889 4.45 · 107 1.75 · 107 Table 2: Summary of the Project Gutenberg corpus. ited to) humor and irony9 , use of metaphors10 , and hyperbole11 . It is important to note that our approach is conceptually different from the previous work on paraphrase and quote detection in book corpora (Kolak and Schilit, 2008), news stories (Liang et al., 2010) and movie scripts (Danescu-Niculescu-Mizil et al., 2012). While this previous work focuses on mining popular and oft-used quotations, we are mainly interested in discovering quotable phrases that might have never been quoted by others. 4 Experimental Setup To evaluate the quotable phrase extraction process in its entirety (see Figure 1), we use a collection of Project Gutenberg (PG) books12 – a popular digital library containing full texts of public domain books in a variety of formats. In particular, we harvest the entire corpus of 21,492 English books in textual format from the PG website. The breakdown of the PG corpus is shown in Table 2. The n"
W12-2510,P10-1015,0,0.0228348,"th the highest value of IG(X, Y )8 . We use the count in the sentence of the sequence seqn with the i-th highest information gain as the feature #IGSeq[i]. Intuitively, the features #IGSeq[i] measure how many POS tag sequences that are indicative of a quotable phrase (or, conversely, indicative of a non-quotable phrase) the sentence contains. 3 Related Work The increasing availability of large-scale digital libraries resulted in a recent surge of interest in computational approaches to literary analysis. To name just a few examples, Genzel et al. (2010) examined machine translation of poetry; Elson et al. (2010) extracted conversational networks from Victorian novels; and Faruqui and Pad´o (2011) predicted formal and informal address in English literature. In addition, computational methods are increasingly used for identification of complex aspects of writing such as humor (Mihalcea and Pulman, 2007), double-entendre (Kiddon and Brun, 2011) and sarcasm (Tsur et al., 2010). However, while successful, most of this work is still limited to an analysis of a single aspect of writing style. In this work, we propose a more general computational approach that attempts to extract quotable phrases. A quotabil"
W12-2510,P11-2082,0,0.0389349,"Missing"
W12-2510,D10-1016,0,0.0173875,"n the sentence, and Y ∈ {q, q}. We select k sequences seqn with the highest value of IG(X, Y )8 . We use the count in the sentence of the sequence seqn with the i-th highest information gain as the feature #IGSeq[i]. Intuitively, the features #IGSeq[i] measure how many POS tag sequences that are indicative of a quotable phrase (or, conversely, indicative of a non-quotable phrase) the sentence contains. 3 Related Work The increasing availability of large-scale digital libraries resulted in a recent surge of interest in computational approaches to literary analysis. To name just a few examples, Genzel et al. (2010) examined machine translation of poetry; Elson et al. (2010) extracted conversational networks from Victorian novels; and Faruqui and Pad´o (2011) predicted formal and informal address in English literature. In addition, computational methods are increasingly used for identification of complex aspects of writing such as humor (Mihalcea and Pulman, 2007), double-entendre (Kiddon and Brun, 2011) and sarcasm (Tsur et al., 2010). However, while successful, most of this work is still limited to an analysis of a single aspect of writing style. In this work, we propose a more general computational ap"
W12-2510,P11-2016,0,0.0186454,"e contains. 3 Related Work The increasing availability of large-scale digital libraries resulted in a recent surge of interest in computational approaches to literary analysis. To name just a few examples, Genzel et al. (2010) examined machine translation of poetry; Elson et al. (2010) extracted conversational networks from Victorian novels; and Faruqui and Pad´o (2011) predicted formal and informal address in English literature. In addition, computational methods are increasingly used for identification of complex aspects of writing such as humor (Mihalcea and Pulman, 2007), double-entendre (Kiddon and Brun, 2011) and sarcasm (Tsur et al., 2010). However, while successful, most of this work is still limited to an analysis of a single aspect of writing style. In this work, we propose a more general computational approach that attempts to extract quotable phrases. A quotability of a phrase can be affected by various aspects of writing including (but not lim8 In this work, we set n = 3, k = 50. This setting is done prior to seeing any labeled data. 73 Number of books Number of authors Total sentences After Na¨ıve Bayes filtering 21, 492 8, 889 4.45 · 107 1.75 · 107 Table 2: Summary of the Project Gutenber"
W12-2510,J06-4003,0,0.0386583,"Missing"
W12-3203,P11-1135,0,0.0202624,"are not clearly an accurate description of his work at that time. More fine-grained modeling of time and also accounting for the death and birth of factions might ameliorate 29 these inconsistencies with our background knowledge about Charniak. The model finds that Aravind Joshi was associated with the tagging/parsing faction in the 1990s and in recent years moved back towards discourse (Prasad et al., 2008). David Yarowsky, known for his early work on word sense disambiguation, has since focused on applying word sense disambiguation techniques in a multilingual context (Garera et al., 2009; Bergsma et al., 2011). As mentioned in the previous section, we observe that the extended model is able to capture Daniel Marcu’s shift from discourse-related work to MT with his work in phrase-based statistical MT (Marcu and Wong, 2002). 5 Related Work A number of algorithms use topic modeling to analyze the text in the articles. Topic models such as latent Dirichlet allocation (Blei et al., 2003) and its variations have been increasingly used to study trends in scientific literature (McCallum et al., 2006; Dietz et al., 2007; Hall et al., 2008; Gerrish and Blei, 2010), predict citation information (McNee et al.,"
W12-3203,A00-2018,0,0.0344748,"3,874, 3,786, and 8,105 papers, respectively. Here we used G = 20 factions for faster runtime, leading to diminished interpretability, though the sparsity of the deviation vectors mitigates this problem somewhat. Figure 4 shows graphical plots of selected authors and their faction membership posteriors over time (drawn from the final E-step). With a simple extension of the original model, we can learn shifts in the subject area the author is publishing about. Consider Eugene Charniak: the model observed a major change in faction alignment around 2000, when one of the popular Charniak parsers (Charniak, 2000) was released; this is somewhat later than Charniak’s interests shifted, and the earlier faction’s words are not clearly an accurate description of his work at that time. More fine-grained modeling of time and also accounting for the death and birth of factions might ameliorate 29 these inconsistencies with our background knowledge about Charniak. The model finds that Aravind Joshi was associated with the tagging/parsing faction in the 1990s and in recent years moved back towards discourse (Prasad et al., 2008). David Yarowsky, known for his early work on word sense disambiguation, has since f"
W12-3203,W09-1117,0,0.0192651,"rlier faction’s words are not clearly an accurate description of his work at that time. More fine-grained modeling of time and also accounting for the death and birth of factions might ameliorate 29 these inconsistencies with our background knowledge about Charniak. The model finds that Aravind Joshi was associated with the tagging/parsing faction in the 1990s and in recent years moved back towards discourse (Prasad et al., 2008). David Yarowsky, known for his early work on word sense disambiguation, has since focused on applying word sense disambiguation techniques in a multilingual context (Garera et al., 2009; Bergsma et al., 2011). As mentioned in the previous section, we observe that the extended model is able to capture Daniel Marcu’s shift from discourse-related work to MT with his work in phrase-based statistical MT (Marcu and Wong, 2002). 5 Related Work A number of algorithms use topic modeling to analyze the text in the articles. Topic models such as latent Dirichlet allocation (Blei et al., 2003) and its variations have been increasingly used to study trends in scientific literature (McCallum et al., 2006; Dietz et al., 2007; Hall et al., 2008; Gerrish and Blei, 2010), predict citation inf"
W12-3203,D08-1038,0,0.252651,"thology. We extend the model to reveal changes in some authors’ faction memberships over time. 1 Introduction The ACL Anthology presents an excellent dataset for studying both the language and the social connections in our evolving research field. Extensive studies using techniques from the field of bibliometrics have been applied to this dataset (Radev et al., 2009a), quantifying the importance and impact factor of both authors and articles in the community. Moreover, recent work has leveraged the availability of digitized publications to study trends and influences within the ACL community (Hall et al., 2008; Gerrish and Blei, 2010; Yogatama et al., 2011) and to analyze academic collaborations (Johri et al., 2011). To the best of our knowledge, however, existing work has mainly pursued “macroscopic” investigations of the interaction of authors in collaboration, citation networks, or the textual content of whole papers. We seek to complement these results with a David A. Smith Department of Computer Science University of Massachusetts Amherst, MA 01003, USA dasmith@cs.umass.edu “microscopic” investigation of authors’ interactions by considering the individual sentences authors use to cite each oth"
W12-3203,W11-1516,0,0.40676,"ion The ACL Anthology presents an excellent dataset for studying both the language and the social connections in our evolving research field. Extensive studies using techniques from the field of bibliometrics have been applied to this dataset (Radev et al., 2009a), quantifying the importance and impact factor of both authors and articles in the community. Moreover, recent work has leveraged the availability of digitized publications to study trends and influences within the ACL community (Hall et al., 2008; Gerrish and Blei, 2010; Yogatama et al., 2011) and to analyze academic collaborations (Johri et al., 2011). To the best of our knowledge, however, existing work has mainly pursued “macroscopic” investigations of the interaction of authors in collaboration, citation networks, or the textual content of whole papers. We seek to complement these results with a David A. Smith Department of Computer Science University of Massachusetts Amherst, MA 01003, USA dasmith@cs.umass.edu “microscopic” investigation of authors’ interactions by considering the individual sentences authors use to cite each other. In this paper, we present a joint model of who cites whom in computational linguistics, and also of how"
W12-3203,W02-1018,0,0.12387,"cal MT and parsing factions in the bottom left exhibit higher citation activity amongst each other. In addition, we note that factions tend to self-cite more often than out of their own factions; this is unsurprising given the prior we selected. The IFC between discourse and MT2 (as shown by the edge thickness in figure 2) is higher than expected, given our prior knowledge of the computational linguistics community. Further investigation revealed that, Daniel Marcu, posited by our model to be a member of the discourse faction, has coauthored numerous highly cited papers in MT in recent years (Marcu and Wong, 2002). However, the model split the translation field, which fragmented the counts of MT related citation words. Thus, assigning Daniel Marcu to the discourse faction, which also has a less diverse citation vocabulary, is more probable than assigning him to one of the MT factions. In §4.6, we consider a model of factions over time to mitigate this problem. 4.5 Comparison to Graph Clustering Work in the field of bibliometrics has largely focused on using the link structure of citation networks to study higher level structures. See Osareh (1996) for a review. Popular methods include bibliographic cou"
W12-3203,prasad-etal-2008-penn,0,0.00873884,"major change in faction alignment around 2000, when one of the popular Charniak parsers (Charniak, 2000) was released; this is somewhat later than Charniak’s interests shifted, and the earlier faction’s words are not clearly an accurate description of his work at that time. More fine-grained modeling of time and also accounting for the death and birth of factions might ameliorate 29 these inconsistencies with our background knowledge about Charniak. The model finds that Aravind Joshi was associated with the tagging/parsing faction in the 1990s and in recent years moved back towards discourse (Prasad et al., 2008). David Yarowsky, known for his early work on word sense disambiguation, has since focused on applying word sense disambiguation techniques in a multilingual context (Garera et al., 2009; Bergsma et al., 2011). As mentioned in the previous section, we observe that the extended model is able to capture Daniel Marcu’s shift from discourse-related work to MT with his work in phrase-based statistical MT (Marcu and Wong, 2002). 5 Related Work A number of algorithms use topic modeling to analyze the text in the articles. Topic models such as latent Dirichlet allocation (Blei et al., 2003) and its va"
W12-3203,W09-3607,0,0.248063,"re closely within their faction, cite within the faction using language distinct from citation outside the faction, and be largely understandable through the language used when cited from without. We conduct an exploratory data analysis on the ACL Anthology. We extend the model to reveal changes in some authors’ faction memberships over time. 1 Introduction The ACL Anthology presents an excellent dataset for studying both the language and the social connections in our evolving research field. Extensive studies using techniques from the field of bibliometrics have been applied to this dataset (Radev et al., 2009a), quantifying the importance and impact factor of both authors and articles in the community. Moreover, recent work has leveraged the availability of digitized publications to study trends and influences within the ACL community (Hall et al., 2008; Gerrish and Blei, 2010; Yogatama et al., 2011) and to analyze academic collaborations (Johri et al., 2011). To the best of our knowledge, however, existing work has mainly pursued “macroscopic” investigations of the interaction of authors in collaboration, citation networks, or the textual content of whole papers. We seek to complement these resul"
W12-3203,D11-1055,1,0.883961,"Missing"
W13-2232,D10-1005,0,0.0521679,"Missing"
W13-2232,2005.mtsummit-papers.11,0,0.154022,"Missing"
W13-2232,W11-2125,1,0.735396,"k of ﬁnding document translation pairs is formulated as ﬁnding similar probability distributions. While the nature of both works was exploratory, results shown on fairly large collections of bilingual documents (less than 20k documents) offer convincing argument of their potential. Expanding these approaches to much large collections of multilingual documents would require utilizing fast NN search for computing similarity in the probability simplex. While there are many other proposed approaches to the task of ﬁnding document translation pairs that represent documents in metric space, such as Krstovski and Smith (2011) which utilizes LSH for cosine distance, there is no evidence that they yield good results on documents of small lengths such as paragraphs and even senIn practice, we can often do much better than this theoretical bound. Figure 7 shows the empirical relation of JS and Hellinger on a translationdetection task. As will be described in §4, we computed the JS and Hellinger divergences between topic distributions of English and Spanish Europarl speeches for a total of 1 million document pairs. Each point in the ﬁgure represents one Spanish-English document pair that might or might not be translati"
W13-2232,D10-1124,0,0.0867295,"Missing"
W13-2232,D09-1092,1,0.946735,"Missing"
W13-2232,D10-1025,0,0.498679,"e analysis of relationships between documents written using different vocabularies. For instance, topic models have been used to identify scientiﬁc communities working on related problems in different disciplines, e.g., work on cancer funded by multiple Institutes within the NIH (Talley et al., 2011). While vocabulary mismatch occurs within the realm of one language, naturally this mismatch occurs across different languages. Therefore, mapping documents in different languages into a common latent topic space can be of great beneﬁt when detecting document translation pairs (Mimno et al., 2009; Platt et al., 2010). Aside from the beneﬁts that it offers in the task of detecting document translation pairs, topic models offer potential beneﬁts to the task of creating translation lexica, aligning passages, etc. The process of discovering relationship between documents using topic models involves: (1) representing documents in the latent space by inferring their topic distributions and (2) comparing pairs of topic distributions to ﬁnd close matches. Many widely used techniques do not scale efﬁciently, however, as the size of the document collection grows. Posterior inference by Gibbs sampling, for instance,"
W13-2232,D08-1038,0,0.505914,"ion for pairs of documents that are translations of each other. We present (1) efﬁcient, online inference for representing documents in several languages in a common topic space and (2) fast approximations for ﬁnding near neighbors in the probability simplex. Empirical evaluations show that these methods are as accurate as—and signiﬁcantly faster than— Gibbs sampling and brute-force all-pairs search. 1 Introduction Statistical topic models, such as latent Dirichlet allocation (LDA) (Blei et al., 2003), have proven to be highly effective at discovering hidden structure in document collections (Hall et al., 2008, e.g.). Often, these models facilitate exploratory data analysis, by revealing which collocations of terms are favored in different kinds of documents or which terms and topics rise and fall over time (Blei and Lafferty, 2006; Wang and McCallum, 2006). One of the greatest advantages in using topic models to analyze and process large document collections is their ability to represent documents as probability distributions over a small number of topics, thereby mapping documents into a low-dimensional latent space—the 252 Proceedings of the Eighth Workshop on Statistical Machine Translation, pa"
W14-2707,P08-2067,0,0.0295359,"ents containing each distinct n-gram, we output all pairs of documents in each list. We suppress repeated ngrams that appear in different issues of the same newspaper. These repetitions often occur in editorial boilerplate or advertisements, which, while interesting, are outside the scope of this project. We also suppress n-grams that generate more than u2 pairs, where u is a parameter.1 These frequent ngrams are likely to be common fixed phrases. Filtering terms with high document frequency has led to significant speed increases with small loss in accuracy in other document similarity work (Elsayed et al., 2008). We then sort the list of repeated ngrams by document pair, which allows us to assign a score to each pair based on the number of overlapping n-grams and the distinctiveness of those n-grams. Table 1 shows the parameters for trading off recall and precision at this stage. Efficient N-gram Indexing The first step is to build for each n-gram feature an inverted index of the documents where it appears. As in other duplicate detection and text reuse applications, we are only interested in the n-grams shared by two or more documents. The index, therefore, does not need to contain entries for the n"
W17-6939,P15-2020,0,0.0285248,"guistic information (for a brief review see the introduction in Zanini et al. (2016)). To evaluate 1 our hypothesis, we employ a computational model trained to classify objects in images. We test whether mass-substance images are internally (i.e. among the various regions of the same image) more homogeneous, and externally (i.e. among the various instances of the same entity) more consistent compared to entities denoted by count nouns (see Figure 1). In other words, ‘substances’ should be distinguished from ‘objects’ by means of the lower variance of their visual features (somewhat similar to Kiela et al. (2015) in a lexical entailment detection task). Though similar with respect to shape, entities denoted by count nouns are likely to be very different with respect to many other low-level visual features (surface, texture, color, etc.). As a consequence, they would require higher-level operations to be recognized and classified as belonging to a particular entity class. Figure 1: Left: images representing the count noun ‘building’. Right: images representing the mass noun ‘flour’. As can be noted, the former exhibits much more variability compared to the latter, both internally (i.e. among regions of"
W17-6939,L16-1447,0,0.0442887,"Missing"
W18-0304,P14-5010,0,0.00981904,". Salt Lake City, Utah, January 4-7, 2018 The majority of work on diachronic syntax has relied on manual annotations, and computational techniques in historical linguistics have mostly focused on phonology, morphology, and the lexicon (Lieberman et al., 2007; Ellison and Kirby, 2006; Bouchard-Cˆot´e et al., 2013, for example). One additional goal of this paper, therefore, is to employ automated methods to analyze the factors that affect passivization and that explain its decreasing frequency in English over the last two centuries. 2 bates using version 3.6.0 of the Stanford dependency parser (Manning et al., 2014; De Marneffe et al., 2006), we detect passive verb phrases by screening for two dependency relation types: those labeled with “auxpass” and with “nsubjpass”. (In transitive constructions, passives can also be detected by screening relations that have the label “nmod:agent”.) To identify the agent in a passive construction, we focus on the labels ending with “agent”. To identify the subject of each verb, we use the labels “nsubjpass” and “nsubj”. Finally, to identify the object in an active construction, we make use of the “dobj” relation. Although not all demoted subjects in passive construct"
W18-0304,J93-2004,0,0.07376,"eing realized in British English as well as American English (and especially the latter) when the length of the patient is longer. We therefore also consider the length of the agent and that of the patient when predicting passivization. Taking square roots of the lengths led to better performance on development data compared to using the raw lengths, so our two length features consist of the square root of the agent’s length and that of the patient’s length. 2.3 Wall Street Journal In addition, we report our model’s performance on the Wall Street Journal corpus from the Penn Treebank Project (Marcus et al., 1993; Marcus et al., 1994) and use the latter to test the significance of explanatory variables. Even though accurate constituency trees are provided in the Wall Street Journal corpus, we parsed the text with a dependency parser and processed it in a manner similar to the aforementioned procedure for the Hansard so that results from the two corpora would be comparable. 3 Modeling Passivization Given or New Information As a proxy for given information, we add a feature corresponding to whether the agent begins with the lemma “the”, “this”, “that” or a pronoun, as well as another feature indicating"
W18-0304,H94-1020,0,0.704549,"ish English as well as American English (and especially the latter) when the length of the patient is longer. We therefore also consider the length of the agent and that of the patient when predicting passivization. Taking square roots of the lengths led to better performance on development data compared to using the raw lengths, so our two length features consist of the square root of the agent’s length and that of the patient’s length. 2.3 Wall Street Journal In addition, we report our model’s performance on the Wall Street Journal corpus from the Penn Treebank Project (Marcus et al., 1993; Marcus et al., 1994) and use the latter to test the significance of explanatory variables. Even though accurate constituency trees are provided in the Wall Street Journal corpus, we parsed the text with a dependency parser and processed it in a manner similar to the aforementioned procedure for the Hansard so that results from the two corpora would be comparable. 3 Modeling Passivization Given or New Information As a proxy for given information, we add a feature corresponding to whether the agent begins with the lemma “the”, “this”, “that” or a pronoun, as well as another feature indicating whether the patient be"
W18-0304,de-marneffe-etal-2006-generating,0,0.20038,"Missing"
W18-0304,P06-1035,0,0.0431172,"nce of person-hierarchy effects in English passivization noted by Bresnan et al. (2001), with increasing strength of the constraint against passivizing clauses with local (i.e. first- or second-person) agents and the increasing prevalence of such agents. 34 Proceedings of the Society for Computation in Linguistics (SCiL) 2018, pages 34-43. Salt Lake City, Utah, January 4-7, 2018 The majority of work on diachronic syntax has relied on manual annotations, and computational techniques in historical linguistics have mostly focused on phonology, morphology, and the lexicon (Lieberman et al., 2007; Ellison and Kirby, 2006; Bouchard-Cˆot´e et al., 2013, for example). One additional goal of this paper, therefore, is to employ automated methods to analyze the factors that affect passivization and that explain its decreasing frequency in English over the last two centuries. 2 bates using version 3.6.0 of the Stanford dependency parser (Manning et al., 2014; De Marneffe et al., 2006), we detect passive verb phrases by screening for two dependency relation types: those labeled with “auxpass” and with “nsubjpass”. (In transitive constructions, passives can also be detected by screening relations that have the label “"
W18-1210,P12-3011,0,0.0118265,". lattice (Ftp , containing distributions over characters actually selected by the user), concatenating it a character “wild card” machine (σ, representing the next character that the user intends to select to continue Ftp ), and intersecting the resulting machine with Fsym (Equation 5). At this point, we are able to extract a probability distribution over possible character selections by performing weight pushing to the end of the machine, and then examining the final set of arcs in FOCLM . All of these FST operations are implemented with the OpenFst (Allauzen et al., 2007) and OpenGRMNGRAM (Roark et al., 2012) libraries. 2.2 2.4 While character deletion or insertion events are not auto-corrected in our approach, we address auto-correction to some degree. As described in the main process of computing the prior distribution, there is a frequent re-assessment the previously typed words. This history is recalculated as more EEG evidence of letter selections is introduced; this recalculation occurs mainly when the space character is introduced as it affects the word-boundaries and thus the history. As a result, at each time step, the updated history is rescored by the word language-model (FwLM ), as our"
W18-1210,D11-1065,0,0.0779101,"Missing"
W19-1308,S15-2078,1,0.81589,"icipants from seven different countries, including Germany, the Netherlands, and the USA. The questionnaire was divided into three subscales focusing on interaction, social influence, and emotion. In general, participants from the USA showed the most positive attitudes towards robots, particularly in their openness to interacting with robots, although they were more negative than the German or Dutch on the topic of robot emotion. Social media has proven to be a rich source of data for sentiment and emotion analysis on a variety of topics, using lexicon-based and machine learning methods (e.g. Rosenthal et al. (2015); Giachanou and Crestani (2016); Mohammad et al. (2018)). However, very little work has focused on the emotions expressed towards robots. Friedman et al. (2003) analyzed 3,119 forum posts relating to the AIBO robot dog. They developed (1) Were there differences in the type or scale of emotions expressed in each of the host countries? We compare the percentages of words associated with different emotions from the tweets produced during each trip, to examine any cultural factors in the public reaction to hitchBOT. (2) What emotions were triggered when hitchBOT was destroyed? We compare the perce"
W19-1308,P18-1017,1,0.901282,"Missing"
W19-1308,S14-2033,0,0.0681897,"Missing"
W19-1308,S18-1001,1,0.899238,"Missing"
W19-1707,D17-1145,0,0.0269174,"(Khandelwal et al., 2018). Although this might be a disadvantage with noisy histories, we will see that it is no worse than n-gram models with clean training and much better with noisy training. In addition, existing work mainly focuses on prediction given a single sequence of tokens in the history, but the signal classifier for BCI systems might not always correctly rank the users’ intent as the top candidate. Dudy et al. (2018) proposed incorporating ambiguous history into decoding with a joint word-character finite-state model, but typing prediction could not be further improved. Although (Sperber et al., 2017) considered lattice decoding for neural models, the task of integrating multiple candidate histories during online prediction has not been studied. To address these challenges, we propose to train a noise-tolerant neural language model for online predictive typing and to provide a richer understanding of the effect of the noise on recurrent neural network language models. We aim to anLanguage models have broad adoption in predictive typing tasks. When the typing history contains numerous errors, as in open-vocabulary predictive typing with braincomputer interface (BCI) systems, we observe sign"
W19-1707,W18-1210,1,0.801099,"nowledge, language models have rarely been evaluated in with such character-level noise. Recurrent language models, however, could effectively utilize contexts of 200 tokens on average (Khandelwal et al., 2018). Although this might be a disadvantage with noisy histories, we will see that it is no worse than n-gram models with clean training and much better with noisy training. In addition, existing work mainly focuses on prediction given a single sequence of tokens in the history, but the signal classifier for BCI systems might not always correctly rank the users’ intent as the top candidate. Dudy et al. (2018) proposed incorporating ambiguous history into decoding with a joint word-character finite-state model, but typing prediction could not be further improved. Although (Sperber et al., 2017) considered lattice decoding for neural models, the task of integrating multiple candidate histories during online prediction has not been studied. To address these challenges, we propose to train a noise-tolerant neural language model for online predictive typing and to provide a richer understanding of the effect of the noise on recurrent neural network language models. We aim to anLanguage models have broa"
W19-1707,W11-2123,0,0.121823,"Missing"
W19-1707,P82-1020,0,0.788731,"Missing"
W19-1707,P18-1027,0,0.0292475,"on language modeling for BCI employs n-gram language models although the past decade has seen recurrent and other neural architectures surpass these models for many tasks. Furthermore, most predictive typing methods, for BCI or other applications, depend on language models trained on clean text; however, BCI output often contains noise due to misclassification of EEG or other input signals. To the best of our knowledge, language models have rarely been evaluated in with such character-level noise. Recurrent language models, however, could effectively utilize contexts of 200 tokens on average (Khandelwal et al., 2018). Although this might be a disadvantage with noisy histories, we will see that it is no worse than n-gram models with clean training and much better with noisy training. In addition, existing work mainly focuses on prediction given a single sequence of tokens in the history, but the signal classifier for BCI systems might not always correctly rank the users’ intent as the top candidate. Dudy et al. (2018) proposed incorporating ambiguous history into decoding with a joint word-character finite-state model, but typing prediction could not be further improved. Although (Sperber et al., 2017) con"
