2020.coling-main.500,D19-1307,0,0.0339871,"Missing"
2020.coling-main.500,N19-1423,0,0.00517476,"y arbitrary document particle (s and d should be of the same lexical form such as bigram); (2) Extract top-k document particles based on similarity to form the anchor set, i.e. As = {ds1 , ds2 , ..., dsk }. Also, we record the similarity as the strength of anchor and denote the strength between s and dsi as qsi (1 ≤ i ≤ k). The embedding vector of the particle in this paper is obtained by averaging the contextualized embeddings of all tokens occurring in the particle. Specifically, in the following experiment, we will sum the last four hidden layers of the pretrained uncased BERT Base model2 (Devlin et al., 2019) to get the embedding for each token (dimension of embedding vector is 768). An example of anchor set can be found in Figure 2. Source document ----------huge success-----excellent result-------------------------------------remarkable achievement Summary (peer/reference) ------------------------great success------------------------- Anchor set (k=3) huge success excellent result remarkable achiev. Figure 2: An example of anchor set for the bigram “great success” when top-3 results are extracted. The anchored version of ROUGE can be defined as follows once all the anchor sets for summary partic"
2020.coling-main.500,2020.acl-main.124,0,0.115179,"off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019). The widely adopted metrics, e.g. ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary). The reference-free metrics are still not mature enough to be utilized for evaluation in a real-world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics, especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol of reference-based summarization metrics by rethinking the role of source document which is indeed a lost treasure neglected by most previous works. Furthermore, a specific implementation of the protocol (i.e. anchored version of ROUGE) will be discussed. The reference-based metrics that already exist typically pursue a kinda computation of overlap between the peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al., 2019). However, to our knowledge, few of th"
2020.coling-main.500,D15-1013,0,0.0295391,"On both datasets, anchored ROUGE has achieved the highest correlations according to all three correlation coefficients. More specifically, both AncR-1 and AncR-2 have a correlation higher than their original counterparts (i.e. R-1 and R-2) and the gaps are over 2.5 and 1.3 percent, respectively. Even the most recent metric based on advanced contextualized embeddings, i.e. Mover, has fallen behind our metric (by over one percent as compared with AncR-1 on TAC 2008 and AncR-2 on TAC 2009). For a more convincing comparison, we have conducted the pairwise Williams significance test recommended by Graham (2015) between our metric (more precisely AncR-1 on TAC 2008 and AncR-2 on TAC 2009) and other competitors and the result shows that the increases of our metric over 3 are statistically significant (p-value < 0.05). others except the supervised metric Sbest Hyperparameter effect & Robustness. Two extra tests have been performed to further analyze our metric. The effects of anchor set size k on Pearson correlations are illustrated in Figure 3, indicating that an anchor set with the proper size is needed to establish the efficacy of our metric. The correlations deteriorate when k is less than three an"
2020.coling-main.500,N18-1065,0,0.0130312,"with an extremely large k that causes more intensive computation. The effect of the number of reference summaries is shown in Table 2. We have used all available references to compute metrics when n is equal to four and used n randomly selected references with a smaller n (note that the average of n4 results is reported). The observation is that our metric is relatively robust to n and it demonstrates that our metric is less prone to the reference noise observed in Kryscinski et al. (2019) or the reference bias introduced when very few reference summaries are available (Hermann et al., 2015; Grusky et al., 2018). 4 Related Work There are various reference-based automatic evaluation metrics for the task of document summarization. The widely accepted metric is ROUGE (Lin, 2004) that focuses primarily on n-gram co-occurrence statistics. Some strategies are proposed to replace the “hard matching” of ROUGE, such as the adoption of WordNet (ShafieiBavani et al., 2018) and the fusion of ROUGE and word2vec (Ng and Abrecht, 2015). Another promising method of designing metrics is to directly compute the semantic similarity of peer and reference summary, including the metrics utilizing various word embeddings s"
2020.coling-main.500,D19-1051,0,0.0604432,"ground. Empirical results on benchmark datasets validate that source document helps to induce a higher correlation with human judgments for ROUGE metric. Being self-explanatory and easy-to-implement, the protocol can naturally foster various effective designs of reference-based metrics besides the anchored ROUGE introduced here. 1 Introduction Automatic evaluation metric plays a vital role in evaluating system performance for the task of document summarization. Challenges remain in the design of an ideal evaluation metric and the off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019). The widely adopted metrics, e.g. ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary). The reference-free metrics are still not mature enough to be utilized for evaluation in a real-world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics, especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol of reference-based summar"
2020.coling-main.500,P11-1052,0,0.0566795,"on results between reference-based automatic metrics and human judgments (k = 5 and n = 4). Best correlations are in bold and our proposed metrics are AncR-1 and AncR-2. P ROUGE-anchored = T (d, summ) = X s∈summ P min(T (d, peer), T (d, ref)) P , ref∈RefSumm d∈Cref T (d, ref) ref∈RefSumm d∈Cref P i=k X δd,dsi · ws · qsi , for summ ∈ {peer, ref}. (1) (2) i=1 dsi ∈As The anchored metric listed above has based the computation on anchor sets residing in the source document. For convenience, we will compare it with the original ROUGE metric, especially the equivalent definition of ROUGE-N given by Lin and Bilmes (2011) (Theorem 3 in the original paper). Function T replaces the count of summary particles, which is adopted in original ROUGE, and for a specific document particle sums the weighted contributions from different summary particles (the weight coefficient is the anchor strength qsi as shown in Eqn. 2). The factor ws is used to assess the effect of multiple occurrences of the same summary particle s. In Eqn. 1, the min function is utilized to compute the weighted matching degree based on document particle d (thus the overall metric will be less than one), which revises the exact count of matching sum"
2020.coling-main.500,W04-1013,0,0.328857,"ce document helps to induce a higher correlation with human judgments for ROUGE metric. Being self-explanatory and easy-to-implement, the protocol can naturally foster various effective designs of reference-based metrics besides the anchored ROUGE introduced here. 1 Introduction Automatic evaluation metric plays a vital role in evaluating system performance for the task of document summarization. Challenges remain in the design of an ideal evaluation metric and the off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019). The widely adopted metrics, e.g. ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary). The reference-free metrics are still not mature enough to be utilized for evaluation in a real-world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics, especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol of reference-based summarization metrics by rethinking the role of source doc"
2020.coling-main.500,J13-2002,0,0.0277847,"sets of topic-focused multi-document summarization (MDS), i.e. TAC 20083 and TAC 20094 , for two main reasons: (1) MDS is more challenging than single document summarization and summarizers tend to behave more differently for evaluation, which fits the purpose to examine various metrics; (2) Multiple reference summaries are offered, which makes it possible to perform robustness test (see Table 2). The two datasets consist of 48 and 44 topics, respectively, each of which has 10 source documents and 4 reference summaries, i.e. n is 4. We only use document set A of official datasets in line with Louis and Nenkova (2013) and Gao et al. (2020). Additionally, TAC 2008 has 57 peer summaries for each topic while TAC 2009 has 55. All summaries are at most 100 words and each peer summary is associated with a Pyramid score (Nenkova and Passonneau, 2004), which serves as the human judgment. For tuning the anchor set size (i.e. k in Section 2), another dataset (DUC 20075 ) will be used. Comparing metrics. These reference-based metrics are involved in the experiment. (1) ROUGE (Lin, 2004): a traditional metric for counting lexical-level overlap. For comparison, two variants are considered based on either unigram (R-1)"
2020.coling-main.500,N04-1019,0,0.224005,"evaluation, which fits the purpose to examine various metrics; (2) Multiple reference summaries are offered, which makes it possible to perform robustness test (see Table 2). The two datasets consist of 48 and 44 topics, respectively, each of which has 10 source documents and 4 reference summaries, i.e. n is 4. We only use document set A of official datasets in line with Louis and Nenkova (2013) and Gao et al. (2020). Additionally, TAC 2008 has 57 peer summaries for each topic while TAC 2009 has 55. All summaries are at most 100 words and each peer summary is associated with a Pyramid score (Nenkova and Passonneau, 2004), which serves as the human judgment. For tuning the anchor set size (i.e. k in Section 2), another dataset (DUC 20075 ) will be used. Comparing metrics. These reference-based metrics are involved in the experiment. (1) ROUGE (Lin, 2004): a traditional metric for counting lexical-level overlap. For comparison, two variants are considered based on either unigram (R-1) or bigram (R-2). (2) ROUGE-WE (Ng and Abrecht, 2015): a metric based on word2vec embeddings (Mikolov et al., 2013) to compute semantic similarity. ROUGE-WE with unigram (R-1-WE) and bigram (R-2-WE) are computed. (3) BERTScore (Zha"
2020.coling-main.500,D15-1222,0,0.348392,"ce-based metrics, especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol of reference-based summarization metrics by rethinking the role of source document which is indeed a lost treasure neglected by most previous works. Furthermore, a specific implementation of the protocol (i.e. anchored version of ROUGE) will be discussed. The reference-based metrics that already exist typically pursue a kinda computation of overlap between the peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al., 2019). However, to our knowledge, few of them consider the impact of source document (or documents in multi-document summarization) on the computation. This goes against common sense as source document is the true information source of both summaries and can be utilized to boost the discriminative power of metrics. Therefore, we advance a new protocol of reference-based metrics for the evaluation of document summarization. More specifically, the direct participation of source document is a necessity to compute any reference-based metric for document summa"
2020.coling-main.500,W17-4510,0,0.0132227,"uation metric and the off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019). The widely adopted metrics, e.g. ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary). The reference-free metrics are still not mature enough to be utilized for evaluation in a real-world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics, especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol of reference-based summarization metrics by rethinking the role of source document which is indeed a lost treasure neglected by most previous works. Furthermore, a specific implementation of the protocol (i.e. anchored version of ROUGE) will be discussed. The reference-based metrics that already exist typically pursue a kinda computation of overlap between the peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al., 2019). However, to our k"
2020.coling-main.500,E17-2007,0,0.0134001,"n on more solid ground. Empirical results on benchmark datasets validate that source document helps to induce a higher correlation with human judgments for ROUGE metric. Being self-explanatory and easy-to-implement, the protocol can naturally foster various effective designs of reference-based metrics besides the anchored ROUGE introduced here. 1 Introduction Automatic evaluation metric plays a vital role in evaluating system performance for the task of document summarization. Challenges remain in the design of an ideal evaluation metric and the off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019). The widely adopted metrics, e.g. ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary). The reference-free metrics are still not mature enough to be utilized for evaluation in a real-world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics, especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol"
2020.coling-main.500,D18-1085,0,0.0130258,"ur metric is relatively robust to n and it demonstrates that our metric is less prone to the reference noise observed in Kryscinski et al. (2019) or the reference bias introduced when very few reference summaries are available (Hermann et al., 2015; Grusky et al., 2018). 4 Related Work There are various reference-based automatic evaluation metrics for the task of document summarization. The widely accepted metric is ROUGE (Lin, 2004) that focuses primarily on n-gram co-occurrence statistics. Some strategies are proposed to replace the “hard matching” of ROUGE, such as the adoption of WordNet (ShafieiBavani et al., 2018) and the fusion of ROUGE and word2vec (Ng and Abrecht, 2015). Another promising method of designing metrics is to directly compute the semantic similarity of peer and reference summary, including the metrics utilizing various word embeddings such as ELMo (Sun and Nenkova, 2019) and BERT (Zhang et al., 2019; Zhao et al., 2019). Furthermore, Zhang et 5699 al. (2020) proposes a metric computing factual correctness based on information extraction. However, none of the above metrics fall into the newly-introduced protocol. The anchored ROUGE proposed by us is a refined metric that has followed the"
2020.coling-main.500,D19-1116,0,0.0530917,"cially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol of reference-based summarization metrics by rethinking the role of source document which is indeed a lost treasure neglected by most previous works. Furthermore, a specific implementation of the protocol (i.e. anchored version of ROUGE) will be discussed. The reference-based metrics that already exist typically pursue a kinda computation of overlap between the peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al., 2019). However, to our knowledge, few of them consider the impact of source document (or documents in multi-document summarization) on the computation. This goes against common sense as source document is the true information source of both summaries and can be utilized to boost the discriminative power of metrics. Therefore, we advance a new protocol of reference-based metrics for the evaluation of document summarization. More specifically, the direct participation of source document is a necessity to compute any reference-based metric for document summarization. This makes so"
2020.coling-main.500,2020.acl-main.458,0,0.0780863,"Missing"
2020.coling-main.500,D19-1053,0,0.0547772,".nist.gov/duc2007/tasks.html#pilot 4 5698 0.85 TAC 2008 r ρ Pearson Correlation 0.83 0.81 AncR-1 n=4 n=3 n=2 n=1 .772 .770 .769 .764 .690 .685 .686 .679 .837 .836 .832 .831 .730 .726 .724 .721 AncR-2 n=4 n=3 n=2 n=1 .756 .760 .754 .751 .653 .658 .654 .652 .842 .840 .835 .833 .738 .736 .732 .729 AncR-1TAC2008 AncR-2TAC2008 0.79 AncR-1TAC2009 0.77 AncR-2TAC2009 0.75 0.73 1 2 3 4 5 6 7 8 9 10 Anchor set size k Figure 3: Exploring anchor set size k. TAC 2009 r ρ Table 2: Correlations computed with n references. 2017): two learned metrics that combine different sets of existing metrics. (5) Mover (Zhao et al., 2019): a contextualized-embedding-based metric using Word Mover’s Distance (Kusner et al., 2015). We report its best version with the BERT embeddings and the certain methods for fine-tuning and aggregation of embeddings according to the original paper. (6) ROUGE-anchored: our metric proposed under the new protocol as formulated in Section 2. Similar to ROUGE, we consider two variants with different particle granularities, i.e. unigram (AncR-1) and bigram (AncR-2). Tuning on DUC 2007 sets the anchor set size to 5. Following the convention, we compute the average summary-level correlation with human"
2020.conll-1.48,S19-1028,0,0.0335433,"Missing"
2020.conll-1.48,P04-3031,0,0.339572,"Missing"
2020.conll-1.48,D15-1075,0,0.0339651,"ss function for a training batch with k examples is a weighted P sum of instance-level loss li : Lbatch = αi ∗ li /( ki=1 αi ). Bias Product Ensemble: an ensemble method that is a product of experts pˆi = sof tmax(log(pi ) + log(bi )). By doing so, the prime model would be encouraged to learn all the information except the specific bias. An intuitive justification from the probabilistic view can be found in Clark et al. (2019). Note that while training, only the prime model is updated while the bias-only model remains unchanged. 3 3.2 2018) in our testing. Training Resources: apart from SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018), we also incorporate Diverse NLI (DNLI) (Poliak et al., 2018a) and Adversarial NLI (ANLI) (Nie et al., 2020) datasets for training. For DNLI, we merge the subsets to form unified train/valid/test sets. Dataset Statistics are shown in Table 2. 2.3 Model Performance on the Benchmark Mixture of Experts (MoE) Debiasing Word overlap heuristics: To combat the word overlap heuristics (HANS (McCoy et al., 2019), renamed as IS-SD in Sec 2.1.2), Clark et al. (2019) used the following features to train a biasonly model: (1) whether the hypothesis is a subsequence of"
2020.conll-1.48,N19-1423,0,0.031017,"m guess while that of higher layers is about 4 point lower than random guess. 54.2 54.8 56.7 27.7 27.4 28.0 80.0 79.9 80.3 83.5 83.4 83.5 Table 8: The performance of BERT base model under different model selection strategies. 6 6.1 Experimental Settings Implementation Details We set up both pretrained and non-pretrained model baselines for the proposed evaluation bechmarks. We rerun their public available codebases (Wolf et al., 2019), including InferSent (Conneau et al., 2017) 6 (w/ and w/o Elmo (Peters et al., 2018)), DAM (Parikh et al., 2016) 7 , ESIM (Chen et al., 2017)8 , BERT (uncased) (Devlin et al., 2019), XLNet (cased) (Yang et al., 2019) and RoBERTa (Liu et al., 2019), 9 . we map the vector at the position of the ‘[CLS]’ token in the pretrained models to three-way NLI classification via linear transformation. We show the per-layer analyses for RoBERTa model in Table 2. We try to reduce the randomness of our experiments by 3 runs using different random seeds. We report the median of the 3 runs for all the tables except the ensemble-related (Sec 5.2) experiments in Table 6. Table 7 shows how we evaluate the test sets with only two labels in 3-way NLI classification. while in the single mode, w"
2020.conll-1.48,P17-2097,0,0.0216575,"e datasets. Our findings suggest model-level MoE ensemble, text swap DA and performance based dataset merging would effectively combat multiple (though not all) distinct biases. Although we haven’t found a debiasing strategy that can guarantee the NLI models to be more robust on every adversarial dataset used in this paper, we leave the question of whether such a debiasing method exists for future research. Related Work Bias in NLI: The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017; Schwartz et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from ne"
2020.conll-1.48,2020.emnlp-main.657,1,0.834756,"Missing"
2020.conll-1.48,P17-1152,0,0.0280573,"the first 4 layers are close to random guess while that of higher layers is about 4 point lower than random guess. 54.2 54.8 56.7 27.7 27.4 28.0 80.0 79.9 80.3 83.5 83.4 83.5 Table 8: The performance of BERT base model under different model selection strategies. 6 6.1 Experimental Settings Implementation Details We set up both pretrained and non-pretrained model baselines for the proposed evaluation bechmarks. We rerun their public available codebases (Wolf et al., 2019), including InferSent (Conneau et al., 2017) 6 (w/ and w/o Elmo (Peters et al., 2018)), DAM (Parikh et al., 2016) 7 , ESIM (Chen et al., 2017)8 , BERT (uncased) (Devlin et al., 2019), XLNet (cased) (Yang et al., 2019) and RoBERTa (Liu et al., 2019), 9 . we map the vector at the position of the ‘[CLS]’ token in the pretrained models to three-way NLI classification via linear transformation. We show the per-layer analyses for RoBERTa model in Table 2. We try to reduce the randomness of our experiments by 3 runs using different random seeds. We report the median of the 3 runs for all the tables except the ensemble-related (Sec 5.2) experiments in Table 6. Table 7 shows how we evaluate the test sets with only two labels in 3-way NLI cla"
2020.conll-1.48,P19-1554,0,0.0110103,"(Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan et al., 2018; Poliak et al., 2018b; He et al., 2019). Rudinger et al. (2017) showed hypotheses in SNLI has the evidence of gender, racial stereotypes, etc. Sanchez et al. (2018) analysed the behaviour of NLI models and the factors to be more robust. Feng et al. (2019) discussed how to use partial-input baseline in future dataset creation. Belinkov et al. (2019); Clark et al. (2019); He et al. (2019); Yaghoobzadeh et al. (2019); Ding et al. (2020) proposed efficient methods to mitigate a particular known bias in NLI. Benchmark collection in NLI: GLUE (Wang et al., 2019b,a) benchmark contains several NLIrelated benchmark datasets. However it does not include adversarial test sets, domain specific test (Romanov and Shivade, 2018; Ravichander et al., Acknowledgments We would like to thank Sam Wiseman and Kevin Gimpel for very thoughtful discussions, and the an"
2020.conll-1.48,D19-1418,0,0.304731,"g methods could increase the model performance on the paired adversarial dataset, they might hinder the model performance on other adversarial datasets, as well as hurt the model generalization power, i.e. deficient scores on cross-datasets or cross-domain settings. These phenomena motivate us to investigate if it exists a unified model-agnostic debiasing strategy which can mitigate distinct (or even all) known biases while keeping or strengthening the model generalization power. We begin with NLI debiasing models. To make our trials more generic, we adopt a mixture of experts (MoE) strategy (Clark et al., 2019), which is known for being model-agnostic and is adaptable to various kinds of known biases, as backbone. Specifically we treat three known biases, namely word overlap, length mismatch and partial input heuristics as independent experts and train corresponding debiasing models. Our results show that the debiasing methods tied to one particular known bias may not be sufficient to build a generalized, robust model. This motivates us to investigate a better solution to integrate the advantages of distinct debiasing models. We find model-level ensemble is more effective than other MoE ensemble met"
2020.conll-1.48,P18-2103,0,0.206059,"guage Learning, pages 596–608 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Datasets Paper Categories Labels Size PI-CD (a) 1k3k7 (E,N,C) 3.2k PI-SP (b) 1k3k7 (E,N,C) .37k IS-SD (c) 2k5k8 (¬E, E) 30k IS-CS (d) 2k3k7 (E,N,C) .65k (e)(f) 2k4k9 (E,C) 9.9K LI-LI LI-TS (g)(h) 2k6k10 (¬C, C) 9.8K ST-WO (e) 2k4k11 (E,N,C) 9.8K ST-NE (e) 2k4k11 (E,N,C) 9.8K ST-LM (e) 2k4k11 (E,N,C) 9.8K (e) 2k4k12 (E,N,C) 31K ST-SE (a) Gururangan et al. (2018) (b) Liu et al. (2020) (c) McCoy et al. (2019) (d) Nie et al. (2019) (e) Naik et al. (2018) (f) Glockner et al. (2018) (g) Wang et al. (2019c) (h) Minervini and Riedel Category First-level Second-level 1 (I) Partial input heuristics 2 (I) Inter-sentence heuristics 3 (II) Instance selection 4 (II) Single Sentence Modification 5 (II) Sentence Pair Modification 6 (II) Sentence Pair Swapping 7 (III) Lexical Statistical Irregularity 8 (III) Syntactic Statistical Irregularity 9 (III) Lexical Inference 10 (III) First Order Logic 11 (III) Stress Test - Distraction Test (III) Stress Test - Noise Test 12 (I) Where are the heuristics? (II) How did the dataset constructed? (III) Which aspect did the dataset detect? NLI d"
2020.conll-1.48,D17-1070,0,0.0225944,"early all test sets except ANLI get higher scores by using higher layers. On ANLI, the performance of the first 4 layers are close to random guess while that of higher layers is about 4 point lower than random guess. 54.2 54.8 56.7 27.7 27.4 28.0 80.0 79.9 80.3 83.5 83.4 83.5 Table 8: The performance of BERT base model under different model selection strategies. 6 6.1 Experimental Settings Implementation Details We set up both pretrained and non-pretrained model baselines for the proposed evaluation bechmarks. We rerun their public available codebases (Wolf et al., 2019), including InferSent (Conneau et al., 2017) 6 (w/ and w/o Elmo (Peters et al., 2018)), DAM (Parikh et al., 2016) 7 , ESIM (Chen et al., 2017)8 , BERT (uncased) (Devlin et al., 2019), XLNet (cased) (Yang et al., 2019) and RoBERTa (Liu et al., 2019), 9 . we map the vector at the position of the ‘[CLS]’ token in the pretrained models to three-way NLI classification via linear transformation. We show the per-layer analyses for RoBERTa model in Table 2. We try to reduce the randomness of our experiments by 3 runs using different random seeds. We report the median of the 3 runs for all the tables except the ensemble-related (Sec 5.2) experim"
2020.conll-1.48,N18-2017,0,0.0324523,"Missing"
2020.conll-1.48,D19-6115,0,0.0249291,"sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan et al., 2018; Poliak et al., 2018b; He et al., 2019). Rudinger et al. (2017) showed hypotheses in SNLI has the evidence of gender, racial stereotypes, etc. Sanchez et al. (2018) analysed the behaviour of NLI models and the factors to be more robust. Feng et al. (2019) discussed how to use partial-input baseline in future dataset creation. Belinkov et al. (2019); Clark et al. (2019); He et al. (2019); Yaghoobzadeh et al. (2019); Ding et al. (2020) proposed efficient methods to mitigate a particular known bias in NLI. Benchmark collection in NLI: GLUE (Wang et al., 2019b,a) benchmark contains several NLIrelated benchmark datasets. However it does"
2020.conll-1.48,N19-1090,0,0.0327223,"Missing"
2020.conll-1.48,N18-1170,0,0.0238641,"sarial dataset used in this paper, we leave the question of whether such a debiasing method exists for future research. Related Work Bias in NLI: The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017; Schwartz et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan et al., 2018; Poliak et al., 2018b; He et al., 2019). Rudinger et al. (2017) showed hypotheses in SNLI has the evidence of gender, racial stereotypes, etc. Sanchez et al. (2018) analysed the behaviour of NLI models and the factors to be more robust. Feng et al. (2019)"
2020.conll-1.48,P19-1334,0,0.207308,". Finally, we investigate several methods to merge heterogeneous training data (1.35M) and perform model ensembling, which are straightforward but effective to strengthen NLI models. 1 Introduction Natural language inference (NLI) (also known as recognizing textual entailment) is a widely studied task which aims to infer the relationship (e.g., entailment, contradiction, neutral) between two fragments of text, known as premise and hypothesis (Dagan et al., 2005, 2013). Recent works have found that NLI models are sensitive to the compositional features (Nie et al., 2019), syntactic heuristics (McCoy et al., 2019), stress test (Geiger et al., 2018; Naik et al., 2018) and human artifacts in the data collection phase (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018). ∗ 1 In this paper, we use the term ‘bias’ to refer to these known dataset biases in NLI following Clark et al. (2019). In other context, ‘bias’ may refer to systematic mishandling of gender or evidences of racial stereotypes (Rudinger et al., 2017) in NLI datasets or models. Equal contribution. 596 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 596–608 c Online, November 19-20, 2020. 2020 A"
2020.conll-1.48,K18-1007,0,0.0792749,"remise sentences to obtain the ’lexically misleading scores (LMS)’ for each instance in the test sets. We use CS0.7 in their paper which denotes the subsets whose LMS are larger that 0.7. 2.1.3 Logical Inference Ability (LI) Lexical Inference Test (LI-LI): A proper NLI system should recognize hypernyms and hyponyms; synonym and antonyms. We merge the “antonym” category in Naik et al. (2018) and Glockner et al. (2018) to assess the models’ capability to model lexical inference. Text-fragment Swap Test (LI-TS): NLI system should also follow the first-order logic constraints (Wang et al., 2019c; Minervini and Riedel, 2018). For example, if the premise sentence sp entails the hypothesis sentence sh , then sh must not be contradicted by sp . We then swap the two sentences in the original MultiNLI mismatched dev sets. If the gold label is ‘contradiction’, the corresponding label in the swapped instance remains unchanged, otherwise it becomes ‘non-contradicted’. 2.1.4 Insights within Adversarial Tests Stress Test (ST) 2.2 We also include the “word overlap” (ST-WO), “negation” (ST-NE), “length mismatch” (ST-LM) and “spelling errors” (ST-SE) in Naik et al. (2018), in which ST-WO aims at detecting lexical overlap heur"
2020.conll-1.48,C18-1198,0,0.285393,"rogeneous training data (1.35M) and perform model ensembling, which are straightforward but effective to strengthen NLI models. 1 Introduction Natural language inference (NLI) (also known as recognizing textual entailment) is a widely studied task which aims to infer the relationship (e.g., entailment, contradiction, neutral) between two fragments of text, known as premise and hypothesis (Dagan et al., 2005, 2013). Recent works have found that NLI models are sensitive to the compositional features (Nie et al., 2019), syntactic heuristics (McCoy et al., 2019), stress test (Geiger et al., 2018; Naik et al., 2018) and human artifacts in the data collection phase (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018). ∗ 1 In this paper, we use the term ‘bias’ to refer to these known dataset biases in NLI following Clark et al. (2019). In other context, ‘bias’ may refer to systematic mishandling of gender or evidences of racial stereotypes (Rudinger et al., 2017) in NLI datasets or models. Equal contribution. 596 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 596–608 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.o"
2020.conll-1.48,W19-5333,0,0.0613467,"Missing"
2020.conll-1.48,N15-1098,0,0.0322294,"model ensemble, and benchmark these methods on various adversarial and general purpose datasets. Our findings suggest model-level MoE ensemble, text swap DA and performance based dataset merging would effectively combat multiple (though not all) distinct biases. Although we haven’t found a debiasing strategy that can guarantee the NLI models to be more robust on every adversarial dataset used in this paper, we leave the question of whether such a debiasing method exists for future research. Related Work Bias in NLI: The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017; Schwartz et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics betwe"
2020.conll-1.48,2020.lrec-1.846,1,0.589335,"ls. Equal contribution. 596 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 596–608 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Datasets Paper Categories Labels Size PI-CD (a) 1k3k7 (E,N,C) 3.2k PI-SP (b) 1k3k7 (E,N,C) .37k IS-SD (c) 2k5k8 (¬E, E) 30k IS-CS (d) 2k3k7 (E,N,C) .65k (e)(f) 2k4k9 (E,C) 9.9K LI-LI LI-TS (g)(h) 2k6k10 (¬C, C) 9.8K ST-WO (e) 2k4k11 (E,N,C) 9.8K ST-NE (e) 2k4k11 (E,N,C) 9.8K ST-LM (e) 2k4k11 (E,N,C) 9.8K (e) 2k4k12 (E,N,C) 31K ST-SE (a) Gururangan et al. (2018) (b) Liu et al. (2020) (c) McCoy et al. (2019) (d) Nie et al. (2019) (e) Naik et al. (2018) (f) Glockner et al. (2018) (g) Wang et al. (2019c) (h) Minervini and Riedel Category First-level Second-level 1 (I) Partial input heuristics 2 (I) Inter-sentence heuristics 3 (II) Instance selection 4 (II) Single Sentence Modification 5 (II) Sentence Pair Modification 6 (II) Sentence Pair Swapping 7 (III) Lexical Statistical Irregularity 8 (III) Syntactic Statistical Irregularity 9 (III) Lexical Inference 10 (III) First Order Logic 11 (III) Stress Test - Distraction Test (III) Stress Test - Noise Test 12 (I) Where are the he"
2020.conll-1.48,2020.acl-main.441,0,0.171242,"an ensemble method that is a product of experts pˆi = sof tmax(log(pi ) + log(bi )). By doing so, the prime model would be encouraged to learn all the information except the specific bias. An intuitive justification from the probabilistic view can be found in Clark et al. (2019). Note that while training, only the prime model is updated while the bias-only model remains unchanged. 3 3.2 2018) in our testing. Training Resources: apart from SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018), we also incorporate Diverse NLI (DNLI) (Poliak et al., 2018a) and Adversarial NLI (ANLI) (Nie et al., 2020) datasets for training. For DNLI, we merge the subsets to form unified train/valid/test sets. Dataset Statistics are shown in Table 2. 2.3 Model Performance on the Benchmark Mixture of Experts (MoE) Debiasing Word overlap heuristics: To combat the word overlap heuristics (HANS (McCoy et al., 2019), renamed as IS-SD in Sec 2.1.2), Clark et al. (2019) used the following features to train a biasonly model: (1) whether the hypothesis is a subsequence of the premise, (2) whether all words in the hypothesis appear in the premise, (3) the percent of words from the hypothesis that appear in the premis"
2020.conll-1.48,2021.ccl-1.108,0,0.106003,"Missing"
2020.conll-1.48,D16-1244,0,0.094087,"Missing"
2020.conll-1.48,D14-1162,0,0.0845486,"Missing"
2020.conll-1.48,D18-1534,0,0.0152222,"debiasing strategy that can guarantee the NLI models to be more robust on every adversarial dataset used in this paper, we leave the question of whether such a debiasing method exists for future research. Related Work Bias in NLI: The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017; Schwartz et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan et al., 2018; Poliak et al., 2018b; He et al., 2019). Rudinger et al. (2017) showed hypotheses in SNLI has the evidence of gender, racial stereotypes, etc. Sanchez et al. (2018) analysed th"
2020.conll-1.48,N18-1202,0,0.031295,"scores by using higher layers. On ANLI, the performance of the first 4 layers are close to random guess while that of higher layers is about 4 point lower than random guess. 54.2 54.8 56.7 27.7 27.4 28.0 80.0 79.9 80.3 83.5 83.4 83.5 Table 8: The performance of BERT base model under different model selection strategies. 6 6.1 Experimental Settings Implementation Details We set up both pretrained and non-pretrained model baselines for the proposed evaluation bechmarks. We rerun their public available codebases (Wolf et al., 2019), including InferSent (Conneau et al., 2017) 6 (w/ and w/o Elmo (Peters et al., 2018)), DAM (Parikh et al., 2016) 7 , ESIM (Chen et al., 2017)8 , BERT (uncased) (Devlin et al., 2019), XLNet (cased) (Yang et al., 2019) and RoBERTa (Liu et al., 2019), 9 . we map the vector at the position of the ‘[CLS]’ token in the pretrained models to three-way NLI classification via linear transformation. We show the per-layer analyses for RoBERTa model in Table 2. We try to reduce the randomness of our experiments by 3 runs using different random seeds. We report the median of the 3 runs for all the tables except the ensemble-related (Sec 5.2) experiments in Table 6. Table 7 shows how we eva"
2020.conll-1.48,K17-1004,0,0.0191586,"ndings suggest model-level MoE ensemble, text swap DA and performance based dataset merging would effectively combat multiple (though not all) distinct biases. Although we haven’t found a debiasing strategy that can guarantee the NLI models to be more robust on every adversarial dataset used in this paper, we leave the question of whether such a debiasing method exists for future research. Related Work Bias in NLI: The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017; Schwartz et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan"
2020.conll-1.48,W18-5441,0,0.0395994,"Missing"
2020.conll-1.48,L18-1239,0,0.0471816,"Missing"
2020.conll-1.48,S18-2023,0,0.0335729,"Missing"
2020.conll-1.48,K19-1033,0,0.0257256,"Missing"
2020.conll-1.48,D18-1187,0,0.0260372,"Missing"
2020.conll-1.48,W17-1609,0,0.0514757,"Missing"
2020.conll-1.48,N18-1179,0,0.020067,"tations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan et al., 2018; Poliak et al., 2018b; He et al., 2019). Rudinger et al. (2017) showed hypotheses in SNLI has the evidence of gender, racial stereotypes, etc. Sanchez et al. (2018) analysed the behaviour of NLI models and the factors to be more robust. Feng et al. (2019) discussed how to use partial-input baseline in future dataset creation. Belinkov et al. (2019); Clark et al. (2019); He et al. (2019); Yaghoobzadeh et al. (2019); Ding et al. (2020) proposed efficient methods to mitigate a particular known bias in NLI. Benchmark collection in NLI: GLUE (Wang et al., 2019b,a) benchmark contains several NLIrelated benchmark datasets. However it does not include adversarial test sets, domain specific test (Romanov and Shivade, 2018; Ravichander et al., Acknowledgments We w"
2020.conll-1.48,P18-1042,0,0.0373074,"Missing"
2020.conll-1.48,N18-1101,0,0.0188457,"h k examples is a weighted P sum of instance-level loss li : Lbatch = αi ∗ li /( ki=1 αi ). Bias Product Ensemble: an ensemble method that is a product of experts pˆi = sof tmax(log(pi ) + log(bi )). By doing so, the prime model would be encouraged to learn all the information except the specific bias. An intuitive justification from the probabilistic view can be found in Clark et al. (2019). Note that while training, only the prime model is updated while the bias-only model remains unchanged. 3 3.2 2018) in our testing. Training Resources: apart from SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018), we also incorporate Diverse NLI (DNLI) (Poliak et al., 2018a) and Adversarial NLI (ANLI) (Nie et al., 2020) datasets for training. For DNLI, we merge the subsets to form unified train/valid/test sets. Dataset Statistics are shown in Table 2. 2.3 Model Performance on the Benchmark Mixture of Experts (MoE) Debiasing Word overlap heuristics: To combat the word overlap heuristics (HANS (McCoy et al., 2019), renamed as IS-SD in Sec 2.1.2), Clark et al. (2019) used the following features to train a biasonly model: (1) whether the hypothesis is a subsequence of the premise, (2) whether all words in"
2020.conll-1.48,P19-1435,0,0.0193032,"sks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017; Schwartz et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan et al., 2018; Poliak et al., 2018b; He et al., 2019). Rudinger et al. (2017) showed hypotheses in SNLI has the evidence of gender, racial stereotypes, etc. Sanchez et al. (2018) analysed the behaviour of NLI models and the factors to be more robust. Feng et al. (2019) discussed how to use partial-input baseline in future dataset creation. Belinkov et al. (2019); Clark et al. (2019); He et al. (2019); Yaghoobzadeh et al. (2019); Ding et al. (2020) proposed effic"
2020.emnlp-main.32,N19-1423,0,0.00705487,"comparison purposes, we consider the following three strategies of building sentence embeddings. Tf-isf: the simple tf-idf model with a finer granularity. More details can be found in Wan et al. (2007) and Wang et al. (2017). ESE: the enhanced feature embedding model (Yang et al., 2019). The embedding of each sentence is the concatenation of all components: paragraph vector, positional embedding and three feature embeddings (namely word-part-of-speech, bigram and trigram). BERT: the sentence encoder that learns vector representations by pre-training a deep bi-directional Transformer network (Devlin et al., 2019). The advantage is that BERT is context-sensitive when considering the word embedding. Notice that the leading sentences in each document should have priority in the summary extraction. For injecting this knowledge, aij is multiplied 437 by the average positional weight 1/(oi +oj ). This can differentiate the sentences across documents and preserve the symmetry of A. 2.4 Justifications of Hypothesis We validate our spectral-based hypothesis by the following three complementary perspectives: Semantic scaling: dominant eigenvalue of affinity matrix determines the vector scaling in semantic space"
2020.emnlp-main.32,P19-1102,0,0.477734,"nts about the same news event. In addition, four human-written summaries are offered for each cluster to be the reference (golden) summary. Yelp3 : an all-purpose dataset that can be utilized for MDS. We only use the subset that has the reference summary (the test split offered by Chu and Liu (2019)): 100 businesses (document clusters), each of which includes 8 reviews (documents). One reference summary was collected for each cluster using crowdsourcing. More details of building the dataset can be found in Chu and Liu (2019). Multi-News4 : a large-scale dataset collected from news aggregator (Fabbri et al., 2019). It has 5622 document clusters (in the test split offered by the original paper), and multiple documents are present 2 https://duc.nist.gov/duc2004/tasks.html https://www.yelp.com/dataset 4 https://github.com/Alex-Fabbri/Multi-News 3 1 The eigenvector v can be of arbitrary length, which differs from the normalized vector u in Eq. (3). 439 in each cluster. Furthermore, each cluster is attached with one human-written reference summary. DUC 2004 Domain #Clusters #Docs per cluster #Ref. per cluster #Doc sources News 50 10 4 2 Yelp Business review 100 8 1 1 Multi-News News 5622 2∼10 1 >1500 Table"
2020.emnlp-main.32,hong-etal-2014-repository,0,0.0214492,"ncatenated embedding is 800). For the strategy BERT, we used the uncased BERTBase model6 pre-trained on Wikipedia, through bert-as-service7 to obtain the sentence embedding of 768 dimensions. All the experiments are performed on a machine with two CPUs (3.5GHz) and one GPU (16G memory). The extractive MDS methods need a length limit of summary to terminate the extraction of summary sentences. We adopt 100 words as the length limit in the DUC dataset, instead of 665 bytes specified by the official task. The change has also been made to provide the same setting for evaluating various methods in Hong et al. (2014) and Zheng et al. (2019). For the Yelp dataset, we set the limit to be the 99.5th percentile less than the maximum length of any document; for Multi-News, the limit is set as 300 words. The same settings have been adopted in Chu and Liu (2019) and Fabbri et al. (2019), respectively. 5 3.3 Evaluation Metrics We adopt ROUGE (Lin, 2004) as the automatic metric, which has been observed in a good agreement with human judgment (Owczarzak et al., 2012). It measures the overlap of N -grams (R-N) and skipbigrams with a maximum distance of four words (R-SU4). Also, it can be computed based on the longes"
2020.emnlp-main.32,W04-1013,0,0.0865664,"ry to terminate the extraction of summary sentences. We adopt 100 words as the length limit in the DUC dataset, instead of 665 bytes specified by the official task. The change has also been made to provide the same setting for evaluating various methods in Hong et al. (2014) and Zheng et al. (2019). For the Yelp dataset, we set the limit to be the 99.5th percentile less than the maximum length of any document; for Multi-News, the limit is set as 300 words. The same settings have been adopted in Chu and Liu (2019) and Fabbri et al. (2019), respectively. 5 3.3 Evaluation Metrics We adopt ROUGE (Lin, 2004) as the automatic metric, which has been observed in a good agreement with human judgment (Owczarzak et al., 2012). It measures the overlap of N -grams (R-N) and skipbigrams with a maximum distance of four words (R-SU4). Also, it can be computed based on the longest common subsequence (R-L). Each version of ROUGE has their scores oriented to recall, precision and F1. In the experiments, we report the different combinations of ROUGE scores for each dataset, which have been recommended and adopted by previous works. Specifically, the recall scores of R-1,2,4 will be reported for the DUC 2004 dat"
2020.emnlp-main.32,P11-1052,0,0.0470702,"Another hypothesis in the sparse-codingbased methods (Li et al., 2015b; Liu et al., 2015; Yao et al., 2015) regards the original sentences as a linear combination of summary sentences. This leads to an intuitive reconstruction, whereas linear combination is more a simplification than a necessity. Our proposed hypothesis offers a spectral viewpoint and will be explicitly validated on the real dataset. (2) Optimization objective. Multi-criteria optimization is suitable for MDS as various criteria (goals) exist in the task, such as relevancy criterion and non-redundancy criterion. For instance, Lin and Bilmes (2011) is a bi-criteria case that imposes the submodularity constraint on each criterion. Multi-criteria loss functions in neuralnetwork-based methods (Ma et al., 2016; Chu and Liu, 2019; Zheng et al., 2019) include the reconstruction errors from different spaces. In the above cases, the overall objective functions used include some hyperparameters for gluing singletons. Comparatively, our proposed objective (spectral impact) has a compact form. It avoids the hyperparameter setting and simulates the non-separable processing of multiple MDS criteria by human beings. (3) Model complexity. There is a t"
2020.emnlp-main.32,D18-1446,0,0.0297214,"Missing"
2020.emnlp-main.32,C16-1143,0,0.0433498,"Missing"
2020.emnlp-main.32,D17-1221,0,0.127261,"Comparing Methods We compare our method with both traditional and state-of-the-art MDS methods. Lead: The documents in a cluster are randomly shuffled, and the first sentence of the document is added to the summary until the length limit is reached. LexRank (Erkan and Radev, 2004): It performs the sentence relevancy estimation by the random walk process on the sentence graph. CLASSY04 (Conroy et al., 2004): It ranked first in the official evaluation of DUC 2004. As a supervised method, it uses a Hidden Markov Model to rank sentences and a QR decomposition to produce the summary. C-Attention (Li et al., 2017a): The cascaded attention based auto-encoder is proposed for estimating the relevancy of words and sentences. GRU-GCN (Yasunaga et al., 2017): It is a supervised method that employs a Graph Convolutional Network on sentence graph. The sentence embedding obtained from a Recurrent Neural Network serves as the input node feature. ParaFuse (Nayeem et al., 2018): MDS is formulated as multi-sentence compression. As the statehttps://tartarus.org/martin/PorterStemmer/ https://github.com/google-research/bert 7 https://github.com/hanxiao/bert-as-service 6 8 https://github.com/andersjo/pyrouge/tree/mast"
2020.emnlp-main.32,C18-1102,0,0.457535,"Missing"
2020.emnlp-main.32,W12-2601,0,0.0240031,"C dataset, instead of 665 bytes specified by the official task. The change has also been made to provide the same setting for evaluating various methods in Hong et al. (2014) and Zheng et al. (2019). For the Yelp dataset, we set the limit to be the 99.5th percentile less than the maximum length of any document; for Multi-News, the limit is set as 300 words. The same settings have been adopted in Chu and Liu (2019) and Fabbri et al. (2019), respectively. 5 3.3 Evaluation Metrics We adopt ROUGE (Lin, 2004) as the automatic metric, which has been observed in a good agreement with human judgment (Owczarzak et al., 2012). It measures the overlap of N -grams (R-N) and skipbigrams with a maximum distance of four words (R-SU4). Also, it can be computed based on the longest common subsequence (R-L). Each version of ROUGE has their scores oriented to recall, precision and F1. In the experiments, we report the different combinations of ROUGE scores for each dataset, which have been recommended and adopted by previous works. Specifically, the recall scores of R-1,2,4 will be reported for the DUC 2004 dataset according to Hong et al. (2014), Wang et al. (2017) and Zheng et al. (2019); the F1 scores of R-1,2,L will be"
2020.emnlp-main.32,W17-1003,0,0.0128996,"urrent Neural Network serves as the input node feature. ParaFuse (Nayeem et al., 2018): MDS is formulated as multi-sentence compression. As the statehttps://tartarus.org/martin/PorterStemmer/ https://github.com/google-research/bert 7 https://github.com/hanxiao/bert-as-service 6 8 https://github.com/andersjo/pyrouge/tree/master/tools/ ROUGE-1.5.5 440 of-the-art on DUC 2004, however, it needs some extra resource and toolkit, such as paraphrase bank and keyword extractor. Best Review (Chu and Liu, 2019): A simple baseline selecting the best document to be summary based on word overlap. Centroid (Rossiello et al., 2017): Word embeddings are exploited to boost the performance of centroid-based methods. MeanSum (Chu and Liu, 2019): An end-to-end neural model is put forward to implement the abstractive summarization of business review documents. The summary is decoded from the mean of the representations of input reviews. PG (See et al., 2017): It introduces a pointergenerator (PG) network that motivates the summarizer to copy original words from input via pointing, while preserving the ability to generate new words. Hi-MAP (Fabbri et al., 2019): It proposes the integration of sentence-level MMR scores into the"
2020.emnlp-main.32,P17-1099,0,0.0346665,"s/ ROUGE-1.5.5 440 of-the-art on DUC 2004, however, it needs some extra resource and toolkit, such as paraphrase bank and keyword extractor. Best Review (Chu and Liu, 2019): A simple baseline selecting the best document to be summary based on word overlap. Centroid (Rossiello et al., 2017): Word embeddings are exploited to boost the performance of centroid-based methods. MeanSum (Chu and Liu, 2019): An end-to-end neural model is put forward to implement the abstractive summarization of business review documents. The summary is decoded from the mean of the representations of input reviews. PG (See et al., 2017): It introduces a pointergenerator (PG) network that motivates the summarizer to copy original words from input via pointing, while preserving the ability to generate new words. Hi-MAP (Fabbri et al., 2019): It proposes the integration of sentence-level MMR scores into the PG network in order to adapt the attention weights on a word-level. The MMR score is computed by the Maximal Marginal Relevance algorithm (Carbonell and Goldstein, 1998), which gives the goodness of the available sentence given already selected ones. Our method Spectral: This is our spectral-based method specified in Alg. 2."
2020.emnlp-main.32,D17-1020,1,0.927428,"element in the affinity matrix A is a pairwise affinity of two different sentences. Since our hypothesis depends on A, a better MDS performance can be expected by adjusting the building of A. Sentence embeddings play a vital role in the process of building A, since affinity aij can be set to be the cosine similarity of the embeddings of sentences si and sj (i.e. aij = aji and aii = 0). For comparison purposes, we consider the following three strategies of building sentence embeddings. Tf-isf: the simple tf-idf model with a finer granularity. More details can be found in Wan et al. (2007) and Wang et al. (2017). ESE: the enhanced feature embedding model (Yang et al., 2019). The embedding of each sentence is the concatenation of all components: paragraph vector, positional embedding and three feature embeddings (namely word-part-of-speech, bigram and trigram). BERT: the sentence encoder that learns vector representations by pre-training a deep bi-directional Transformer network (Devlin et al., 2019). The advantage is that BERT is context-sensitive when considering the word embedding. Notice that the leading sentences in each document should have priority in the summary extraction. For injecting this"
2020.emnlp-main.32,K17-1045,0,0.0157188,"andomly shuffled, and the first sentence of the document is added to the summary until the length limit is reached. LexRank (Erkan and Radev, 2004): It performs the sentence relevancy estimation by the random walk process on the sentence graph. CLASSY04 (Conroy et al., 2004): It ranked first in the official evaluation of DUC 2004. As a supervised method, it uses a Hidden Markov Model to rank sentences and a QR decomposition to produce the summary. C-Attention (Li et al., 2017a): The cascaded attention based auto-encoder is proposed for estimating the relevancy of words and sentences. GRU-GCN (Yasunaga et al., 2017): It is a supervised method that employs a Graph Convolutional Network on sentence graph. The sentence embedding obtained from a Recurrent Neural Network serves as the input node feature. ParaFuse (Nayeem et al., 2018): MDS is formulated as multi-sentence compression. As the statehttps://tartarus.org/martin/PorterStemmer/ https://github.com/google-research/bert 7 https://github.com/hanxiao/bert-as-service 6 8 https://github.com/andersjo/pyrouge/tree/master/tools/ ROUGE-1.5.5 440 of-the-art on DUC 2004, however, it needs some extra resource and toolkit, such as paraphrase bank and keyword extra"
2020.emnlp-main.32,D19-1311,0,0.0399039,"Missing"
2020.emnlp-main.657,N10-1112,1,0.874309,"g step after estimating the generative classifier distributions. They used log loss as their discriminative objective. We also consider using a discriminative fine-tuning step when training our model, specifically we compare log loss to four other discriminative losses: • Perceptron loss: the loss function underlying the perceptron algorithm (Rosenblatt, 1958) • Hinge loss: the loss function underlying support vector machines (SVMs) and structured SVMs (Wahba et al., 1999; Taskar et al., 2004) • Softmax-margin: which combines log loss with a cost function as in hinge loss (Povey et al., 2008; Gimpel and Smith, 2010) • Bayes risk: the expectation of the cost function with respect to the model’s conditional distribution (Kaiser et al., 2000; Smith and Eisner, 2006) Table 1 shows these discriminative losses.2 Some losses use a cost function, which can be chosen by the practitioner to penalize different errors differently. In our experiments, we define it as cost(y, y 0 ) = 1 for y 6= y 0 and cost(y, y 0 ) = 0 if y = y 0 , where y is the gold label and y 0 is a candidate label. In addition, we introduce a very simple loss that is inspired by these other discriminative losses while performing quite well overa"
2020.emnlp-main.657,P18-2103,0,0.0686436,"ich first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a natural language inference dataset consists of two natural language texts, known as the premise and the hypothesis, and a label indicating the relation between the two texts. Formally, we denote an instance hx(p) , x(h) , yi as a tuple consisting of a premise (p) (p) (p) x(p) = {x1 , x2 , ..., xN }, a hypothesis x(h) = (h) (h) (h) {x1 , x2 , ..., xT }, and a label y ∈ Y . Most existing NLI models are trained in a dis8190 criminative manner by maximizing the conditional log-"
2020.emnlp-main.657,P16-1154,0,0.177104,"simple model architectures and more complex ones. Prior work on document classification and question answering has shown that generative classifiers have advantages over their discriminative counterparts in non-ideal conditions (Yogatama et al., 2017; Lewis and Fan, 2019; Ding and Gimpel, 2019). In this paper, we develop generative classifiers for NLI. Our model, which we call GenNLI, defines the conditional probability of the hypothesis given the premise and the label, parameterizing the distribution using a sequence-to-sequence model with attention (Luong et al., 2015) and a copy mechanism (Gu et al., 2016). We explore training objectives for discriminative fine-tuning of our generative classifiers, comparing several classical discriminative criteria. We find that several losses, including hinge loss and softmax-margin, outperform log loss fine-tuning used in prior work (Lewis and Fan, 2019) while similarly retaining the advantages of generative classifiers. We also find strong results with a simple unbounded modification to log loss, which we call the “infinilog loss”. Our evaluation focuses on challenging experimental conditions: small training sets, imbalanced label distributions, and label n"
2020.emnlp-main.657,N18-2017,0,0.0373154,"uch methods can be roughly categorized into two classes: sentence embedding bottleneck methods which first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a natural language inference dataset consists of two natural language texts, known as the premise and the hypothesis, and a label indicating the relation between the two texts. Formally, we denote an instance hx(p) , x(h) , yi as a tuple consisting of a premise (p) (p) (p) x(p) = {x1 , x2 , ..., xN }, a hypothesis x(h) = (h) (h) (h) {x1 , x2 , ..., xT }, and a label y ∈ Y . Most exi"
2020.emnlp-main.657,2021.ccl-1.108,0,0.178082,"Missing"
2020.emnlp-main.657,D15-1166,0,0.217154,"ing discriminative models, including both simple model architectures and more complex ones. Prior work on document classification and question answering has shown that generative classifiers have advantages over their discriminative counterparts in non-ideal conditions (Yogatama et al., 2017; Lewis and Fan, 2019; Ding and Gimpel, 2019). In this paper, we develop generative classifiers for NLI. Our model, which we call GenNLI, defines the conditional probability of the hypothesis given the premise and the label, parameterizing the distribution using a sequence-to-sequence model with attention (Luong et al., 2015) and a copy mechanism (Gu et al., 2016). We explore training objectives for discriminative fine-tuning of our generative classifiers, comparing several classical discriminative criteria. We find that several losses, including hinge loss and softmax-margin, outperform log loss fine-tuning used in prior work (Lewis and Fan, 2019) while similarly retaining the advantages of generative classifiers. We also find strong results with a simple unbounded modification to log loss, which we call the “infinilog loss”. Our evaluation focuses on challenging experimental conditions: small training sets, imba"
2020.emnlp-main.657,marelli-etal-2014-sick,0,0.421086,"n its discriminative analogue, logistic regression. Yogatama et al. (2017) compared the performance of generative and discriminative classifiers and showed the advantages of neural generative classifiers in terms of sample complexity, data shift, and zero-shot and continual learning settings. Ding and Gimpel (2019) further improved the performance of generative classifiers on document classification by introducing discrete latent variables Natural Language Inference Early methods for NLI mainly relied on conventional, feature-based methods trained from smallscale datasets (Dagan et al., 2013; Marelli et al., 2014). The release of larger datasets, such as SNLI, made neural network methods feasible. Such methods can be roughly categorized into two classes: sentence embedding bottleneck methods which first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased t"
2020.emnlp-main.657,P19-1334,0,0.0854708,"Missing"
2020.emnlp-main.657,K18-1007,0,0.0945586,"o sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a natural language inference dataset consists of two natural language texts, known as the premise and the hypothesis, and a label indicating the relation between the two texts. Formally, we denote an instance hx(p) , x(h) , yi as a tuple consisting of a premise (p) (p) (p) x(p) = {x1 , x2 , ..., xN }, a hypothesis x(h) = (h) (h) (h) {x1 , x2 , ..., xT }, and a label y ∈ Y . Most existing NLI models are trained in a dis8190 criminative manner by maximizing the conditional log-likelihood of the label give"
2020.emnlp-main.657,W17-5308,0,0.0232681,"ved the performance of generative classifiers on document classification by introducing discrete latent variables Natural Language Inference Early methods for NLI mainly relied on conventional, feature-based methods trained from smallscale datasets (Dagan et al., 2013; Marelli et al., 2014). The release of larger datasets, such as SNLI, made neural network methods feasible. Such methods can be roughly categorized into two classes: sentence embedding bottleneck methods which first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a na"
2020.emnlp-main.657,D16-1244,0,0.214468,"Missing"
2020.emnlp-main.657,D14-1162,0,0.0901545,"strong on standard leaderboards.5 3 While MRPC is a binary paraphrase classification task rather than an NLI or entailment task, we treat it as a binary entailment task by choosing one of the sentences arbitrarily as the premise and using the other as the hypothesis. 4 MRPC and RTE have no public test set, so we report their performances on the development sets. 5 GLUE leaderboard: https://gluebenchmark. com/leaderboard/; SNLI leaderboard: https:// nlp.stanford.edu/projects/snli/ Training Details Both generative and discriminative models are initialized with GloVe pretrained word embeddings (Pennington et al., 2014).6 The word embedding dimension and the LSTM hidden state dimension are set to 300. All parameters, including the word embeddings, are updated during training. The label embedding dimensionality for GenNLI is set to 100. All the experiments are conducted 5 times with different random seeds and we report the median scores. GenNLI. The training includes two steps: the model is first trained with the generative objective only (Equation 1) for 20 epochs, followed by the discriminative fine-tuning objective only (one of the objectives in Table 1) for 15 epochs. Unless otherwise specified, we use in"
2020.emnlp-main.657,N18-1202,0,0.0641951,"ent uses a BiLSTM network with max pooling (Collobert and Weston, 2008) to learn generic sentence embeddings that perform well on several NLI tasks. ESIM has a relatively complicated network structure, including a recursive architecture of local inference modeling (MacCartney, 2009; Parikh et al., 2016) and inference composition. The pretrained models we compare to are BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019). We select these models as our baselines because (1) they are open-source and are frequently used as baselines for NLI tasks in related work (Peters et al., 2018; Williams et al., 2018), and (2) their performance is strong on standard leaderboards.5 3 While MRPC is a binary paraphrase classification task rather than an NLI or entailment task, we treat it as a binary entailment task by choosing one of the sentences arbitrarily as the premise and using the other as the hypothesis. 4 MRPC and RTE have no public test set, so we report their performances on the development sets. 5 GLUE leaderboard: https://gluebenchmark. com/leaderboard/; SNLI leaderboard: https:// nlp.stanford.edu/projects/snli/ Training Details Both generative and discriminative models a"
2020.emnlp-main.657,S18-2023,0,0.0768613,"Missing"
2020.emnlp-main.657,D16-1264,0,0.0308844,"he training data shows severe label imbalance and when training labels are randomly corrupted. We additionally use GenNLI to generate hypotheses for given premises and labels. While the generations tend to have low diversity due to high lexical overlap with the premise, they are generally fluent and comport with the given labels, even in the small data setting. 2 2.1 Background and Related Work Generative Classifiers into the generative story. Lewis and Fan (2019) developed generative classifiers for question answering and achieved comparable performance to discriminative models on the SQuAD (Rajpurkar et al., 2016) dataset, and much better performance in challenging experimental settings. In this paper, we develop generative models for natural language inference inspired by models for sequence-to-sequence tasks. We additionally contribute an exploration of several discriminative objectives for fine-tuning our generative classifiers, finding multiple choices to outperform log loss used in prior work. We also compare our generative classifiers with fine-tuning of large-scale pretrained models, and characterize performance under other realistic settings such as imbalanced and noisy datasets. 2.2 While disc"
2020.emnlp-main.657,P06-2101,0,0.0910095,"minative fine-tuning step when training our model, specifically we compare log loss to four other discriminative losses: • Perceptron loss: the loss function underlying the perceptron algorithm (Rosenblatt, 1958) • Hinge loss: the loss function underlying support vector machines (SVMs) and structured SVMs (Wahba et al., 1999; Taskar et al., 2004) • Softmax-margin: which combines log loss with a cost function as in hinge loss (Povey et al., 2008; Gimpel and Smith, 2010) • Bayes risk: the expectation of the cost function with respect to the model’s conditional distribution (Kaiser et al., 2000; Smith and Eisner, 2006) Table 1 shows these discriminative losses.2 Some losses use a cost function, which can be chosen by the practitioner to penalize different errors differently. In our experiments, we define it as cost(y, y 0 ) = 1 for y 6= y 0 and cost(y, y 0 ) = 0 if y = y 0 , where y is the gold label and y 0 is a candidate label. In addition, we introduce a very simple loss that is inspired by these other discriminative losses while performing quite well overall in our experiments. We call it the infinilog loss and define it as 2 Again, the label prior p(y) ends up canceling out because it is uniform over l"
2020.emnlp-main.657,W18-5446,0,0.0415137,"Missing"
2020.emnlp-main.657,N18-1101,0,0.554735,"erimental settings, including small training sets, imbalanced label distributions, and label noise. 1 Introduction Natural language inference (NLI) is the task of identifying the relationship between two fragments of text, called the premise and the hypothesis (Dagan et al., 2005; Dagan et al., 2013). The task was originally defined as binary classification, in which the labels are entailment (the premise implies the hypothesis) or not entailment. Subsequent variations added a third contradiction label. Most models for NLI are trained and evaluated on standard benchmarks (Bowman et al., 2015; Williams et al., 2018; Wang et al., 2018) in a discriminative manner (Conneau et al., 2017; Chen et al., 2017a). These benchmarks typically have relatively clean, balanced, and abundant annotated data, and there ∗ † Equal contribution. Contribution during visiting TTIC. is no distribution shift between the training and test sets. However, when data quality and conditions are not ideal, there is a substantial performance decrease for existing discriminative models, including both simple model architectures and more complex ones. Prior work on document classification and question answering has shown that generative"
2020.emnlp-main.657,D18-1408,1,0.849005,"ssification by introducing discrete latent variables Natural Language Inference Early methods for NLI mainly relied on conventional, feature-based methods trained from smallscale datasets (Dagan et al., 2013; Marelli et al., 2014). The release of larger datasets, such as SNLI, made neural network methods feasible. Such methods can be roughly categorized into two classes: sentence embedding bottleneck methods which first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a natural language inference dataset consists of two natural"
2020.lrec-1.846,P19-1084,0,0.0464383,"Missing"
2020.lrec-1.846,P17-2097,0,0.111928,"he easy and hard sets as it dynamically adjusts the debiasing strategies (i.e. the weight of training instances in Eq 5 and 6). 4. 5. Acknowledgments We would like to thank the anonymous reviewers for their valuable suggestions. This work is supported by the National Science Foundation of China under Grant No. 61751201, No. 61772040, No. 61876004. The corresponding authors of this paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) di"
2020.lrec-1.846,P17-1152,0,0.0770692,"train the encoders, discriminator and classifier in Eq 1 together with a gradient reversal layer (Ganin et al., 2016) as shown in Fig 1. We negate the gradients from the discriminator D (red arrow in Fig 1) to push the hypothesis encoder Eh to the opposite direction while update its parameters. The usage of gradient reversal layer makes it easier to optimize the min-max game in Eq 1 (Xie et al., 2017; 10 It would be more challenging to manipulate the gradients in the non-sentence vector-based models, e.g. models which contain interactions between hypothesis and premise sentence encoders like (Chen et al., 2017a). We leave this to the future work. 11 https://nlp.stanford.edu/projects/snli/ 6856 Chen et al., 2018) than training the two adversarial components alternately like Generative Adversarial Nets (GANs) (Goodfellow et al., 2014). We update the model parameters θ by gradient descending (m is the batch size): new old θD = θD − new old θC = θC − new old θE = θE − h h m 1 X ∇θ [log pD (y i |Eh (xih ))] m i=1 D (2) m 1 X ∇θ [log pC (y i |Eh (xih ), Ep (xip ))] (3) m i=1 C m 1 X ∇θ [log pC (y i |Eh (xih ), Ep (xip ))] m i=1 Eh m (4) γ X ∇θ [log pD (y i |Eh (xih ))] m i=1 Eh | {z } + gradient reverse"
2020.lrec-1.846,Q18-1039,0,0.066773,"et al., 2016) as shown in Fig 1. We negate the gradients from the discriminator D (red arrow in Fig 1) to push the hypothesis encoder Eh to the opposite direction while update its parameters. The usage of gradient reversal layer makes it easier to optimize the min-max game in Eq 1 (Xie et al., 2017; 10 It would be more challenging to manipulate the gradients in the non-sentence vector-based models, e.g. models which contain interactions between hypothesis and premise sentence encoders like (Chen et al., 2017a). We leave this to the future work. 11 https://nlp.stanford.edu/projects/snli/ 6856 Chen et al., 2018) than training the two adversarial components alternately like Generative Adversarial Nets (GANs) (Goodfellow et al., 2014). We update the model parameters θ by gradient descending (m is the batch size): new old θD = θD − new old θC = θC − new old θE = θE − h h m 1 X ∇θ [log pD (y i |Eh (xih ))] m i=1 D (2) m 1 X ∇θ [log pC (y i |Eh (xih ), Ep (xip ))] (3) m i=1 C m 1 X ∇θ [log pC (y i |Eh (xih ), Ep (xip ))] m i=1 Eh m (4) γ X ∇θ [log pD (y i |Eh (xih ))] m i=1 Eh | {z } + gradient reverse 3.2.2. Guidance from Artificial Patterns The artificial patterns turns out to be useful guidances for bo"
2020.lrec-1.846,D19-1418,0,0.0638623,"Missing"
2020.lrec-1.846,D17-1070,0,0.0606994,"ence, we wonder if it is possible to get rid of these biases via debiasing the hypothesis sentence vector. More specifically, we focus on the ‘sentence vector-based models’ 10 category as defined on SNLI’s web page11 . Notably the idea of debiasing NLI via adversarial training has been proposed before (Belinkov et al., 2019; Belinkov et al., 2018). We hereby briefly introduce how we implement our adversarial training and how we incorporate instance reweighting method in this framework. In the following experiments, we use the full training sets without any down-sampling. We use the InferSent (Conneau et al., 2017) (biLSTM with max pooling) model as the benchmark sentence encoder. 3.2.1. Adversarial Debiasing Framework As shown in Fig 1, given the outputs sh = Eh (xh ), sp = Es (xs ) of hypothesis and premise encoders Eh , Ep , we are interested in predicting the NLI label y using a classifier C, pC (y|sh = Eh (xh ), sp = Es (xs )). In addition, we train a hypothesis-only discriminator trying to predict the correct label y solely from the hypothesis sentence representation sh by modeling pD (y|sh = Eh (xh )). We formulate the training process in the adversarial setting by a min-max game. Specifically we"
2020.lrec-1.846,P19-1554,0,0.572229,"true (entailment) or false (contradiction) given the premise, or whether the truth value can not be inferred (neutral). A proper NLI decision should apparently rely on both the premise and the hypothesis. However, some recent studies (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) have shown that it is possible for a trained model to identify the true label by only looking at the hypothesis without observing the premise. The phenomenon is referred to as annotation artifacts (Gururangan et al., 2018), statistical irregularities (Poliak et al., 2018) or partial-input heuristics (Feng et al., 2019). In this paper we use the term hypothesis-only bias (Poliak et al., 2018) to refer to this phenomenon. Such hypothesis-only bias originates from the human annotation process of data collection. In the data collection process of many large-scale NLI datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018), human annotators are required to write new sentences (hypotheses) based on the given premise and a specified label among entailment, contradiction and neutral. Some of the human-elicited hypotheses contain patterns that spuriously correlate to some specific labels. Fo"
2020.lrec-1.846,P18-2103,0,0.0545727,"a under Grant No. 61751201, No. 61772040, No. 61876004. The corresponding authors of this paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) discussed how to use partialinput baseline (hypothesis-only classifier in NLI) in future dataset creation. (Clark et al., 2019) uses an ensemblebased method to mitigate known bias. The InferSent model, which served as an important baseline in this paper, are found to achieve superb performance on SNL"
2020.lrec-1.846,N18-2017,0,0.15606,"ction Natural language inference (NLI) (also known as recognizing textual entailment) is a widely studied task which aims to infer the relationship (e.g., entailment, contradiction, neutral) between two fragments of text, known as premise and hypothesis (Dagan et al., 2006; Dagan et al., 2013). NLI models are usually required to determine whether a hypothesis is true (entailment) or false (contradiction) given the premise, or whether the truth value can not be inferred (neutral). A proper NLI decision should apparently rely on both the premise and the hypothesis. However, some recent studies (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) have shown that it is possible for a trained model to identify the true label by only looking at the hypothesis without observing the premise. The phenomenon is referred to as annotation artifacts (Gururangan et al., 2018), statistical irregularities (Poliak et al., 2018) or partial-input heuristics (Feng et al., 2019). In this paper we use the term hypothesis-only bias (Poliak et al., 2018) to refer to this phenomenon. Such hypothesis-only bias originates from the human annotation process of data collection. In the data collection process of many large-s"
2020.lrec-1.846,D19-6115,0,0.217796,"Missing"
2020.lrec-1.846,N18-1170,0,0.0470107,"orted by the National Science Foundation of China under Grant No. 61751201, No. 61772040, No. 61876004. The corresponding authors of this paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) discussed how to use partialinput baseline (hypothesis-only classifier in NLI) in future dataset creation. (Clark et al., 2019) uses an ensemblebased method to mitigate known bias. The InferSent model, which served as an important baseline in this p"
2020.lrec-1.846,N15-1098,0,0.0960085,"e elaborated in Sec 3.2.2. and Sec 3.2.3. respectively. the performance gap between the easy and hard sets as it dynamically adjusts the debiasing strategies (i.e. the weight of training instances in Eq 5 and 6). 4. 5. Acknowledgments We would like to thank the anonymous reviewers for their valuable suggestions. This work is supported by the National Science Foundation of China under Grant No. 61751201, No. 61772040, No. 61876004. The corresponding authors of this paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed t"
2020.lrec-1.846,D18-1170,1,0.819086,"91.5 90.1 89.9 Several Yes various ... According 54.7 54.4 53.7 53.1 53.1 addition also locals battle dangerous 69.6 68.6 65.7 63.3 63.2 None refused never perfectly Nobody 85.4 80.5 79.0 77.3 77.1 SNLI Table 1: Top 3 artificial patterns sorted by the pattern-label conditional probability p(l|b) (Sec 2.1.). The listed patterns appear at least in 500/200 instances in SNLI/MultiNLI training sets, notably the numbers 500/200 here are chosen only for better visualization. ‘#’ is the placeholder for an arbitrary token. The underlined artificial pattern serves as an example in Sec 2.1.. al., 2018; Luo et al., 2018). The experiments show that the guidance from the derived artificial patterns can be helpful to the success of sentence-level NLI debiasing. 2. Datasets In this section, we identify the artificial patterns from the hypothesis sentences which highly correlate to specific labels in the training sets and then derive hard, easy subsets from the original test sets based on them. 2.1. Artificial Pattern Collection ‘Pattern’ in this work refers to (maybe nonconsecutive) word segments in the hypothesis sentences. We try to identify the ‘artificial patterns’ which spuriously correlate to a specific lab"
2020.lrec-1.846,P19-1334,0,0.0702703,"Missing"
2020.lrec-1.846,K18-1007,0,0.154524,"l Science Foundation of China under Grant No. 61751201, No. 61772040, No. 61876004. The corresponding authors of this paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) discussed how to use partialinput baseline (hypothesis-only classifier in NLI) in future dataset creation. (Clark et al., 2019) uses an ensemblebased method to mitigate known bias. The InferSent model, which served as an important baseline in this paper, are found to achieve s"
2020.lrec-1.846,C18-1198,0,0.146235,"Missing"
2020.lrec-1.846,D16-1244,0,0.23239,"Missing"
2020.lrec-1.846,S18-2023,0,0.0310535,"ference (NLI) (also known as recognizing textual entailment) is a widely studied task which aims to infer the relationship (e.g., entailment, contradiction, neutral) between two fragments of text, known as premise and hypothesis (Dagan et al., 2006; Dagan et al., 2013). NLI models are usually required to determine whether a hypothesis is true (entailment) or false (contradiction) given the premise, or whether the truth value can not be inferred (neutral). A proper NLI decision should apparently rely on both the premise and the hypothesis. However, some recent studies (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) have shown that it is possible for a trained model to identify the true label by only looking at the hypothesis without observing the premise. The phenomenon is referred to as annotation artifacts (Gururangan et al., 2018), statistical irregularities (Poliak et al., 2018) or partial-input heuristics (Feng et al., 2019). In this paper we use the term hypothesis-only bias (Poliak et al., 2018) to refer to this phenomenon. Such hypothesis-only bias originates from the human annotation process of data collection. In the data collection process of many large-scale NLI datasets suc"
2020.lrec-1.846,W17-1609,0,0.0919498,"paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) discussed how to use partialinput baseline (hypothesis-only classifier in NLI) in future dataset creation. (Clark et al., 2019) uses an ensemblebased method to mitigate known bias. The InferSent model, which served as an important baseline in this paper, are found to achieve superb performance on SNLI by wordlevel heuristics (Dasgupta et al., 2018). (MacCartney and Manning, 2009) first re"
2020.lrec-1.846,N18-1179,0,0.0619198,"al inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) discussed how to use partialinput baseline (hypothesis-only classifier in NLI) in future dataset creation. (Clark et al., 2019) uses an ensemblebased method to mitigate known bias. The InferSent model, which served as an important baseline in this paper, are found to achieve superb performance on SNLI by wordlevel heuristics (Dasgupta et al., 2018). (MacCartney and Manning, 2009) first revealed the difficulties of natural language inference model with bag-of-words models. Different from the artificial"
2020.lrec-1.846,D18-1534,0,0.370256,"ke to thank the anonymous reviewers for their valuable suggestions. This work is supported by the National Science Foundation of China under Grant No. 61751201, No. 61772040, No. 61876004. The corresponding authors of this paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) discussed how to use partialinput baseline (hypothesis-only classifier in NLI) in future dataset creation. (Clark et al., 2019) uses an ensemblebased method to mitigate k"
2020.lrec-1.846,L18-1239,0,0.126103,"Missing"
2020.lrec-1.846,N16-1174,0,0.100608,"Missing"
2020.lrec-1.846,D18-1009,0,0.0310084,"Missing"
2020.lrec-1.846,Q17-1036,0,0.0307413,") 82.9 90.4 60.0 InferSent+Guidance 84.1 95.5 61.7 dInferSent 81.6 92.5 59.9 +Guidance 82.2 86.9 63.3 +Guidance+Reweight 80.9 78.2 67.3 (a) InferSent trained on SNLI Model Full Easy Hard InferSent 70.4 92.7 54.4 InferSent+DS(λ=0.8) 69.9 91.4 53.6 InferSent+Guidance 70.1 92.1 54.9 dInferSent 68.8 91.1 54.7 +Guidance 68.0 87.9 55.3 +Guidance+Reweight 66.5 79.4 58.8 (b) InferSent trained on MultiNLI ∆Hard Easy (↓) 38.3 30.4 33.8 32.6 23.6 10.9 works (GANs) (Goodfellow et al., 2014). Several works on learning encoders which are invariant to certain properties of text and image (Chen et al., 2018; Zhang et al., 2017; Xie et al., 2017; Moyer et al., 2018; Jaiswal et al., 2018) in the adversarial settings. ∆Hard Easy (↓) In this study, we show that the hypothesis-only bias in trained NLI models mainly comes from unevenly distributed surface patterns, which could be used to identify hard and easy instances for more convincing re-evaluation on currently overestimated NLI models. The attempts to mitigate the bias are meaningful as such bias not only makes NLI models fragile to adversarial examples. We try to mitigate this bias by removing those artificial patterns in the training sets, with experiments showin"
2020.lrec-1.846,P19-1435,0,0.0944433,"Missing"
2020.lrec-1.846,D15-1075,0,0.189211,"Missing"
2020.lrec-1.846,N18-1101,0,0.117673,"Missing"
2021.naacl-main.437,Q17-1010,0,0.0227941,"und truth deﬁnition are Hyper-parameters: We tune hyper-parameters to achieve the best BLEU score on the validation set. covered by the predicted deﬁnition. The overall We use Adam (Kingma and Ba, 2015) with an ini- metric measures the overall quality of the predicted deﬁnition, referencing the ground-truth deﬁnition. tial learning rate of 10−3 as the optimizer. We set We randomly select 100 entries from the test set, hidden size to 300, batch size to 64 and dropout rate to 0.2. Word embeddings are 300-dimensional, and hire three raters to rate the predicted deﬁnitions pretrained by fastText (Bojanowski et al., 2017). on a scale of 1 to 5, where each entry includes (1) the source word, (2) the ground-truth deﬁnition, We train for up to 50 epochs, and early stop the and (3) the predicted deﬁnition to the raters. We training process once the performance does not show in Table 5 the detailed guideline for raters on improve for 10 consecutive epochs. We run our each point. experiments on a single NVIDIA GeForce GTX 2080Ti GPU with 11 GB memory. The inter-rater kappa (Fleiss and Cohen, 1973) is 0.65 for coverage and 0.66 for overall. We average Baselines: We compare with two reproducible baselines that have a"
2021.naacl-main.437,P18-2043,0,0.0608041,"Missing"
2021.naacl-main.437,N19-1350,0,0.0917766,"ically Fuses different features through a gating mechanism, and generaTes word definitions. Experimental results show that our method is both effective and robust. 1 1 Introduction Deﬁnition Generation (DG) aims at automatically generating an explanatory text for a word. This task is of practical importance to assist dictionary construction, especially in highly productive languages like Chinese (Yang et al., 2020). Most existing methods take the source word as an indecomposable lexico-semantic unit, using features like word embedding (Noraset et al., 2017) and context (Gadetsky et al., 2018; Ishiwatari et al., 2019). Recently, Yang et al. (2020) and Li et al. (2020) achieve improvement by decomposing the word meaning into different semantic components. In decomposing the word meaning, the word formation process is an intuitive and informative way that has not been explored in DG by far. For parataxis languages like Chinese, a word is formed by formation components, i.e., morphemes, and Word 䕾冲 Word Definition 䕾冲䕼共䖃冰ȼ (White flower.) Formation Rule Modifier-Head 䕾冲 䕼䕼☯冰尸ȼ (Vainly spend.) Morphemes: Definitions 䕾1: 䕼共䖃 (white) 冲1: 冰㘴 (flower) Adverb-Verb 䕾2: 䕼䕼☯ (vainly) 冲2: 冰尸 (spend) Figure 1: Word"
2021.naacl-main.437,2020.acl-main.65,0,0.333516,"and generaTes word definitions. Experimental results show that our method is both effective and robust. 1 1 Introduction Deﬁnition Generation (DG) aims at automatically generating an explanatory text for a word. This task is of practical importance to assist dictionary construction, especially in highly productive languages like Chinese (Yang et al., 2020). Most existing methods take the source word as an indecomposable lexico-semantic unit, using features like word embedding (Noraset et al., 2017) and context (Gadetsky et al., 2018; Ishiwatari et al., 2019). Recently, Yang et al. (2020) and Li et al. (2020) achieve improvement by decomposing the word meaning into different semantic components. In decomposing the word meaning, the word formation process is an intuitive and informative way that has not been explored in DG by far. For parataxis languages like Chinese, a word is formed by formation components, i.e., morphemes, and Word 䕾冲 Word Definition 䕾冲䕼共䖃冰ȼ (White flower.) Formation Rule Modifier-Head 䕾冲 䕼䕼☯冰尸ȼ (Vainly spend.) Morphemes: Definitions 䕾1: 䕼共䖃 (white) 冲1: 冰㘴 (flower) Adverb-Verb 䕾2: 䕼䕼☯ (vainly) 冲2: 冰尸 (spend) Figure 1: Word formation process for the polysemous &quot;白花&quot;. With mor"
2021.naacl-main.437,P18-2023,0,0.23046,"Missing"
2021.naacl-main.437,D19-1357,0,0.0366765,"Missing"
2021.naacl-main.437,N19-1097,0,0.152168,"–5531 June 6–11, 2021. ©2021 Association for Computational Linguistics Recent methods attempt to decompose the word meaning by using HowNet sememes (Yang et al., 2020) or modeling latent variables (Li et al., 2020). Semantic Components: To systematically deﬁne words, linguists decompose the word meaning into semantic components (Wierzbicka, 1996). Following this idea, HowNet (Dong and Dong, 2006) uses manually-created sememes to describe the semantic aspects of words. Recent studies also show that leveraging subword information produces better embeddings (Park et al., 2018; Lin and Liu, 2019; Zhu et al., 2019), but these methods lack a clear distinction among different formation rules. 3 Word Formation Process in Chinese It is linguistically motivated to explore the word formation process to better understand words. Instead of combining roots and afﬁxes, Chinese words are formed by characters in a parataxis way (Li et al., 2018). Here, we introduce two formation features and construct a formation-informed dataset. 3.1 Formation components and rules Chinese formation components are morphemes, deﬁned as the smallest meaning-bearing units (Zhu, 1982). Morphemes are unambiguous in representing word mea"
C08-1105,W05-0620,0,0.0428654,"Missing"
C08-1105,J02-3001,0,0.25207,"may cause labeling errors such as constituents outside active region of arguments may be falsely recognized as roles. Introduction Semantic Role Labeling (SRL) has gained the interest of many researchers in the last few years. SRL consists of recognizing arguments involved by predicates of a given sentence and labeling their semantic types. As a well defined task of shallow semantic parsing, SRL has a variety of applications in many kinds of NLP tasks. A variety of approaches has been proposed for the different characteristics of SRL. More recent approaches have involved calibrating features (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; ∗ This work was partial completed while this author was at Toshiba (China) R&D Center. ∗ c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. This paper uses insights from generative linguistics to guide the solution of locality of arguments. In particular, Maximal Projection (MP) which dominates1 active region of arguments according to the projection principle of principle and parameters. Two methods, the anchor group approach and the single anch"
C08-1105,P07-1025,0,0.0332107,"Missing"
C08-1105,W05-0625,0,0.0288869,"Missing"
C08-1105,P04-1043,0,0.0708338,"Missing"
C08-1105,P05-1073,0,0.0397734,"Missing"
C08-1105,W04-3212,0,0.0123727,"D836 structure plus movement. NP-movement principle in principle and parameters indicates that noun phrases only move from A-positions (argument position) which have been assigned roles to A-positions which have not, leaving an NPtrace. On account of θ-theory and government, Apositions are nodes m-commanded 3 by predicates in D-structure. In NP-movement, arguments move to positions which are C-commanded 4 by target predicate and m-commanded by other predicates. Broadly speaking, A-positions are C-commanded by predicates after NP-movement. The key of the well-known pruning algorithm raised in (Xue and Palmer, 2004) is extracting sisters of ancestors as role candidates. Those candidate nodes are all Ccommanders of a predicate. NP-movement can give an explanation why the algorithm works. 4.1.2 Definition of Argument Anchor To capture the characteristics of A-positions, we make definition of A-anchor as following. For every predicate p in the syntax tree T , denote A the set of C-commanders of p: • a left-A-anchor satisfies: 1. left-A-anchor belongs to A; 2. left-A-anchor is a noun phrase (including NNS, NNP, etc.) or simple declarative clause (S); 3. left-A-anchor is on the left hand of p. • a right-A-anc"
C08-1105,D07-1062,0,\N,Missing
C12-2068,C02-1130,0,0.112702,"Missing"
C12-2068,C08-1034,0,0.0166596,"ks whether the context of entity mention Wi contains the word in the relevant classspecific word set. If context words surrounding Wi hit the word in the class-specific feature set of Cj, the binary feature corresponding to Cj is set to 1. 697 3 3.1 Experiments Experimental Settings We test our approach on UKWAC3( M. Baroni et al., 2009), a 2 billion word English corpora constructed from the Web limiting the crawl to the .uk domain which has been PoS-tagged and lemmatized. The input person instances for each class are the same as used by Giuliano (2009) based on the People Ontology defined by Giuliano and Gliozzo (2008). The ontology extracted from WordNet is arranged in a multi-level taxonomy with 21 fine-grained classes, containing 1,657 distinct person instances. The taxonomy has a maximum depth of 4. We extract all entity mentions together with their contexts in the entire corpus. All the contexts in which NEs occur are randomly partitioned into two equally sized subsets. One is used for training and the other for testing, and vice versa. Like other hierarchical classification tasks, the hypernym classes contain all instances of their hyponym classes when constructing the datasets. For example, Mozart is"
C12-2068,W09-1125,0,0.0795897,"amed entity categories defined by the classic Named Entity Classification (NEC) task are coarse grained, typically PERS, LOC, ORG, MISC. The results obtained from coarse grained NEC are insufficient for complex applications such as Information Retrieval, QuestionAnswering or Ontology Population. Consequently, some researchers turn to address the problem of recognizing and categorizing fine-grained NE classes. Fleischman (2001) presents a preliminary study on the subcategorization of location names, and more recent work focuses on the subcategorization of person names (Fleischman et al., 2002; Giuliano, 2009; Asif Ekbal et al., 2010). Fine-grained NEC (FG-NEC) is a more difficult task than classic NEC, due to the increase in the number of classes and the decrease in the semantic differences between classes. The classic NEC can yield a good classification performance using only simple local context features. While for the FG-NEC, just using these features is far from enough to meet the requirements. Take the following sentence for example, “Dennis Rodman, a close friend of Pippen&apos;s who won three NBA Champions with Jordan&apos;s Bulls, was shocked to hear of Pippen&apos;s comments.”, Based on the context inf"
C12-2068,W10-2415,0,0.0688523,"s defined by the classic Named Entity Classification (NEC) task are coarse grained, typically PERS, LOC, ORG, MISC. The results obtained from coarse grained NEC are insufficient for complex applications such as Information Retrieval, QuestionAnswering or Ontology Population. Consequently, some researchers turn to address the problem of recognizing and categorizing fine-grained NE classes. Fleischman (2001) presents a preliminary study on the subcategorization of location names, and more recent work focuses on the subcategorization of person names (Fleischman et al., 2002; Giuliano, 2009; Asif Ekbal et al., 2010). Fine-grained NEC (FG-NEC) is a more difficult task than classic NEC, due to the increase in the number of classes and the decrease in the semantic differences between classes. The classic NEC can yield a good classification performance using only simple local context features. While for the FG-NEC, just using these features is far from enough to meet the requirements. Take the following sentence for example, “Dennis Rodman, a close friend of Pippen&apos;s who won three NBA Champions with Jordan&apos;s Bulls, was shocked to hear of Pippen&apos;s comments.”, Based on the context information “NBA Champions”,"
C12-2068,P11-1053,0,0.0465885,"Missing"
C12-2068,J92-4003,0,0.444087,"e this cluster-based features into our model. Combining these motivations, we present a method exploiting Multi-features for fine-grained classification of NEs in this paper. The only input data for our algorithm is a few manually annotated entities for each class. In addition to adopting the context word features and the word sense disambiguation features proposed by prior work, this paper puts forward three new features: the cluster-based features, the entity-related features and the class-specific features. 1. Cluster-based features are generated by the Brown clustering algorithm (Peter F. Brown et al., 1992) from a large unlabeled corpus. 2. Entity-related features are context features introduced by other related entities. 3. Class-specific features are words extracted for each class. Each word is given a classspecific score denoting its ability to indicate the relevant class. 694 Our work presented here concentrates on the subcategorization of person names, since the previous researches have indicated that the classification of person names which relies on much more contextual information are often more challenging. The person instances are already identified as entities, and only being classifi"
C12-2068,N04-1043,0,0.0179531,"n a window for each entity mention. Only three individual word tokens and their PoS tags before and after the occurrence of the mention will be added into the feature set. In this paper, a context word and its PoS tag are tied together as an ensemble feature. For an entity mention Wi, its context words will be represented as: fcii33 ( wi 3& posi 3) ( wi 3& posi 3) . 2.2 Cluster-based Features Bag-of-words model cannot deal with synonyms. To address this flaw, some work took advantage of the cluster-based features. The preliminary idea of using word clusters as features was presented by Miller et al. (2004), who augmented name tagging training data with hierarchical word clusters generated by the Brown clustering algorithm (Peter F. Brown et al., 1992) from a large unlabeled corpus. Ang Sun et al. (2011) use the Brown algorithm to generate the word clusters as additional features which are applying to improve the performance of the relation extraction system. They use the English portion of the TDT5 corpora as their unlabeled data for inducing word clusters. The result of this word clusters is a binary tree. A particular word can be assigned a binary string by following the path from the root to"
C12-2068,W02-2002,0,0.0969582,"Missing"
C16-1161,S07-1025,0,0.0217758,"a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin, 2007; Chambers and Jurafsky, 2008; Cassidy et al., 2014) have been successful in extracting temporally related events. Temporal reasoning is also explored to solve temporal conflicts in KG (Dylla et al., 2011; Wang et al., 2010). This paper differs from this line of work as we directly use temporal information from KG to perform KG completion. 1722 6 Conclusion and Future Work In this paper, we propose two novel time-aware KG completion models. Time-aware embedding (TAE) model imposes temporal order constraints on the geometric structure of the embedding space and enforces it to be temporally cons"
C16-1161,P14-2082,0,0.0362075,"15; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin, 2007; Chambers and Jurafsky, 2008; Cassidy et al., 2014) have been successful in extracting temporally related events. Temporal reasoning is also explored to solve temporal conflicts in KG (Dylla et al., 2011; Wang et al., 2010). This paper differs from this line of work as we directly use temporal information from KG to perform KG completion. 1722 6 Conclusion and Future Work In this paper, we propose two novel time-aware KG completion models. Time-aware embedding (TAE) model imposes temporal order constraints on the geometric structure of the embedding space and enforces it to be temporally consistent and accurate. Time-aware joint inference with"
C16-1161,P08-1090,0,0.0298628,"Missing"
C16-1161,P07-2044,0,0.0409635,"Missing"
C16-1161,D14-1165,0,0.0722366,"2013): the mean of correct entity ranks (Mean Rank) and the proportion of valid entities ranked in top-10 (Hits@10). As mentioned in (Bordes et al., 2013), the metrics are desirable but flawed when a corrupted triple exists in the KG. As a countermeasure, we may filter out all these corrupted triples which have appeared in KG before ranking. We name the first evaluation set as Raw and the second as Filter. For each test quad (triple), we replace the head/tail entity ei by those entities with compatible types as removing triples with incompatible types during test time leads to better results (Chang et al., 2014; 1720 Wang et al., 2015). Entity type information is easy to obtain for YAGO and Freebase. Then we rank the generated corrupted triples in descending order, according to the plausibility (for baselines and TAE model) or the decision variables (for time-aware ILP model). Then we check whether the original correct triple ranks in top-10. To calculate Hit@10 for ILP model, for each test quad, we add additional P (r ) constraints that at most 10 corrupted are true: i,j xei e1j ≤ 10. Mean Rank is missing for ILP method as we could not rank the binary decision variables. Baseline methods. For compa"
C16-1161,P12-1012,0,0.0169546,"rnal information is employed to improve KG embedding such as combining text (Riedel et al., 2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin, 2007; Chambers and Jurafsky, 2008; Cassidy et al., 2014) have been successful in extracting temporally related events. Temporal reasoning is also explored to solve temporal conflicts in KG (Dylla et al., 2011; Wang et al., 2010). This paper differs from this line of work as we directly use temporal information from KG to perform KG completion. 1722 6 Con"
C16-1161,D15-1038,0,0.0323252,"016) provide a broad overview of machine learning models for KG completion. These models predict new facts in a given knowledge graph using information from existing entities and relations. The most related work from this line of work is KG embedding models (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013). Aside from fact triples, external information is employed to improve KG embedding such as combining text (Riedel et al., 2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin,"
C16-1161,P15-1009,0,0.0161756,"esearch related to our work. Knowledge Graph Completion. Nickel et al. (2016) provide a broad overview of machine learning models for KG completion. These models predict new facts in a given knowledge graph using information from existing entities and relations. The most related work from this line of work is KG embedding models (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013). Aside from fact triples, external information is employed to improve KG embedding such as combining text (Riedel et al., 2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky an"
C16-1161,D15-1082,0,0.194831,"r1T, r2T(tr &lt; tr ) 1 r1T r1 2 r2T (a)TransE (b)TransE-TAE Figure 1: Simple illustration of Temporal Evolving Matrix T in the time-aware embedding (TAE) space. For example, r1 =wasBornIn happened before r2 =diedIn. After projection by T, we get prior relation’s projection r1 T near subsequent relation r2 in the space, i.e.,r1 T ≈ r2 , but r2 T 6= r1 . Here, x+ ∈ ∆ is the observed (i.e., positive) triple, and x− ∈ ∆0 is the negative triple constructed by replacing entities in x+ . γ is the margin separating positive and negative triples and [z]+ = max(0, z). Please refer to (Wang et al., 2014a; Lin et al., 2015b) for TransH, TransR and other models. After we obtain the embeddings, the plausibility of a missing triple can be predicted by using the scoring function. In general, triples with higher plausibility are more likely to be true. 2.3 Time-Aware KG Embedding Model TransE assumes that each relation is time independent and entity/relation representation is only affected by structural patterns in KGs. To better model knowledge evolution, we assume temporal ordered relations are related to each other and evolve in a time dimension. For example, for the same person, there exists a temporal order amo"
C16-1161,W09-2418,0,0.0301052,"o et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin, 2007; Chambers and Jurafsky, 2008; Cassidy et al., 2014) have been successful in extracting temporally related events. Temporal reasoning is also explored to solve temporal conflicts in KG (Dylla et al., 2011; Wang et al., 2010). This paper differs from this line of work as we directly use temporal information from KG to perform KG completion. 1722 6 Conclusion and Future Work In this paper, we propose two novel time-aware KG completion models. Time-aware embedding (TAE) model imposes temporal order constraints on the geometric structure"
C16-1161,N13-1008,0,0.0132003,"08] and a person cannot marry two people at the same time. 5 Related Work There are two lines of research related to our work. Knowledge Graph Completion. Nickel et al. (2016) provide a broad overview of machine learning models for KG completion. These models predict new facts in a given knowledge graph using information from existing entities and relations. The most related work from this line of work is KG embedding models (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013). Aside from fact triples, external information is employed to improve KG embedding such as combining text (Riedel et al., 2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large"
C16-1161,N15-1118,0,0.0239417,"Missing"
C16-1161,D14-1167,0,0.148049,"Projection r1 r2 r2 r1T, r2T(tr &lt; tr ) 1 r1T r1 2 r2T (a)TransE (b)TransE-TAE Figure 1: Simple illustration of Temporal Evolving Matrix T in the time-aware embedding (TAE) space. For example, r1 =wasBornIn happened before r2 =diedIn. After projection by T, we get prior relation’s projection r1 T near subsequent relation r2 in the space, i.e.,r1 T ≈ r2 , but r2 T 6= r1 . Here, x+ ∈ ∆ is the observed (i.e., positive) triple, and x− ∈ ∆0 is the negative triple constructed by replacing entities in x+ . γ is the margin separating positive and negative triples and [z]+ = max(0, z). Please refer to (Wang et al., 2014a; Lin et al., 2015b) for TransH, TransR and other models. After we obtain the embeddings, the plausibility of a missing triple can be predicted by using the scoring function. In general, triples with higher plausibility are more likely to be true. 2.3 Time-Aware KG Embedding Model TransE assumes that each relation is time independent and entity/relation representation is only affected by structural patterns in KGs. To better model knowledge evolution, we assume temporal ordered relations are related to each other and evolve in a time dimension. For example, for the same person, there exists a"
C16-1270,D15-1075,0,0.37321,"epresent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the performance of LSTM-based recurrent neural network. Then, Yin et al. (2015) applied attention mechanic to convolution neural network, Liu et al. (2016a) proposed coupled-LSTM, Vendrov et al. (2015) proposed ordered embedding, Mou et al. (2016) applied Tree-based CNN, Wang and Jiang (2015) proposed matching LSTM, Liu et al. (2016b) applied inner-attention, Cheng et al. (2016) proposed Long Short-Term Memory-Networks to improve the performance. To free the model from traditional parsing process, Bowman et al. (2016) c"
C16-1270,P16-1139,0,0.220145,"(Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the performance of LSTM-based recurrent neural network. Then, Yin et al. (2015) applied attention mechanic to convolution neural network, Liu et al. (2016a) proposed coupled-LSTM, Vendrov et al. (2015) proposed ordered embedding, Mou et al. (2016) applied Tree-based CNN, Wang and Jiang (2015) proposed matching LSTM, Liu et al. (2016b) applied inner-attention, Cheng et al. (2016) proposed Long Short-Term Memory-Networks to improve the performance. To free the model from traditional parsing process, Bowman et al. (2016) combines parsing and interpretation within a single tree sequence hybrid model by integrating tree structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Parikh et al. (2016) uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. 2871 Premise representation ⨁ ~ ~ (a) LSTM Unit (b) rLSTM Unit Figure 1: The inner architecture of the traditional LSTM unit and the re-read LSTM unit. 3 3.1 Model Background The LSTM architecture (Hochreiter and Schmidhuber, 1997) addresses this probl"
C16-1270,D16-1053,0,0.576145,"eep neural network for classification. However, in the sentence encoding process, the premise and the hypothesis cannot affect each other. It is well known that the encoding procedure is just automatically learning useful features. Without the impact between the two sentences, it is difficult for the encoder to extract the sentence-relationship-specific features. Other methods mainly make use of attention mechanism to capture the word-by word alignment information while training (Rockt¨aschel et al., 2015) or just integrate memory network into LSTM to make the model remember more information (Cheng et al., 2016). Among them, only the attention mechanism can make the two sentences contact with each other. However, the word-by-word attention does not represent a better understanding of the sentences. When deciding the entailment relationship between a pair of sentences, what is really matters? Unlike paraphrasing and machine translation, entailment relationship does not force the two sentences have the same meaning. Instead, as long as the premise can cover the meaning of the hypothesis, the entailment stands. Therefore, if the premise entails the hypothesis, that doesn’t really mean that the words in"
C16-1270,E09-1025,0,0.0238895,"ctional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the e"
C16-1270,C08-1043,0,0.0255133,"rage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the atten"
C16-1270,D16-1176,0,0.109637,"Missing"
C16-1270,W07-1407,0,0.0125968,"premise, and use a bidirectional rLSTM to read the hypothesis. The output of the standard BiLSTM is taken as the general input of the bidirectional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check"
C16-1270,P16-2022,0,0.225039,"can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the performance of LSTM-based recurrent neural network. Then, Yin et al. (2015) applied attention mechanic to convolution neural network, Liu et al. (2016a) proposed coupled-LSTM, Vendrov et al. (2015) proposed ordered embedding, Mou et al. (2016) applied Tree-based CNN, Wang and Jiang (2015) proposed matching LSTM, Liu et al. (2016b) applied inner-attention, Cheng et al. (2016) proposed Long Short-Term Memory-Networks to improve the performance. To free the model from traditional parsing process, Bowman et al. (2016) combines parsing and interpretation within a single tree sequence hybrid model by integrating tree structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Parikh et al. (2016) uses attention to decompose the problem into subproblems that can be solved separately, thus making it tr"
C16-1270,D16-1244,0,0.295878,"Missing"
C16-1270,D14-1162,0,0.0887401,"premise and the hypothesis. 4.1 Datasets and Model Configuration We conduct experiments on the Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015). The original data set contains 570,152 sentence pairs, each labeled with one of the following relationships: entailment, contradiction, neutral and −, where − indicates a lack of consensus from the 2874 human annotators. We discard the sentence pairs labeled with − and keep the remaining ones for our experiments. Table 2 summarizes the statistics of the three entailment classes in SNLI. We use 300 dimensional GloVe embeddings (Pennington et al., 2014) to represent words, which is trained on the Wikipedia+Gigaword dataset. The embeddings of unknown tokens are initialized by random vectors. 4.2 Methods for Comparison Although we list all of the approaches designed for recognizing textural entailment task on the SNLI dataset in Table 3, we mainly want to compare our model with the word-by-word attention model by Rockt¨aschel et al. (2015), long short term memory network model by Cheng et al. (2016) and the decomposable attention model by Parikh et al. (2016) since they are either related to our work or achieved the state-of-the-art performanc"
C16-1270,D15-1185,1,0.872603,"ariety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based techniq"
C16-1270,P11-2098,0,0.0237541,", including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the perf"
C16-1270,W11-2402,0,0.0166197,", including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the perf"
C16-1270,U06-1019,0,0.0101027,"utput of the standard BiLSTM is taken as the general input of the bidirectional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer"
C16-1270,W07-1406,0,0.0319259,"eral input of the bidirectional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used"
C16-1270,P06-1051,0,0.0406042,"ard BiLSTM is taken as the general input of the bidirectional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabi"
C16-1309,P12-1056,0,0.0309786,"and has been extensively studied for the decades (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; Fung et al., 2005; He et al., 2007; Sayyadi et al., 2009; Zhao et al., 2012; Sayyadi and Raschid, 2013; Ge et al., 2015). They are based on either document- or keyword-based clustering, which usually suffer from either unawareness of time, high expensive computation cost or deviation of cluster centroids. In contrast, our approach is time-aware, centroid-aware and so efficient that it can be run on a large text stream. In addition, there is much work (Sakaki et al., 2010; Lee et al., 2011; Diao et al., 2012; Aggarwal and Subbian, 2012; Wang et al., 2013; Dong et al., 2015) studying event detection problem in social media. They usually use more or less social media features such as spatio-temporal information, which are not in the same setting with our task. 6 Conclusion and Future Work This paper proposes to use a novel text stream representation – Burst Information Networks to address the retrospective event detection challenge. Based on the BINet, we propose two fast centroid-aware event 3284 detection models that can effectively overcome the limitations of the previous event detection models"
C16-1309,P15-1056,1,0.851895,"(320.27s) 304.56s 716s 9.25s 3591.98s (1350.08s) BINet-ADM 2,562.17s (320.27s) 304.56s 716s 27.3s 3610.03s (1368.13s) Table 6: The running time of 4 parts of our BINet-based event detection approaches. The number in the round bracket is the running time of the model when it is run in 8-way parallel. 5 Related Work Event detection is one of the most popular research topics in recent years and has been extensively studied for the decades (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; Fung et al., 2005; He et al., 2007; Sayyadi et al., 2009; Zhao et al., 2012; Sayyadi and Raschid, 2013; Ge et al., 2015). They are based on either document- or keyword-based clustering, which usually suffer from either unawareness of time, high expensive computation cost or deviation of cluster centroids. In contrast, our approach is time-aware, centroid-aware and so efficient that it can be run on a large text stream. In addition, there is much work (Sakaki et al., 2010; Lee et al., 2011; Diao et al., 2012; Aggarwal and Subbian, 2012; Wang et al., 2013; Dong et al., 2015) studying event detection problem in social media. They usually use more or less social media features such as spatio-temporal information, w"
C16-1309,D16-1075,1,0.770827,"it is likely that the clusters obtained by the methods are not eventcentric, which has an adverse effect on the result, as illustrated in Figure 1. Figure 1: Deviation of cluster centroids: If clusters are not constructed around the centroid of the events (e.g., the dashline cluster is constructed around non-centroids such as people, kill and injure instead of earthquake or bombing), the performance will be adversely affected. To offer a better solution to event detection without the above limitations, we propose to use a novel text stream representation: Burst Information Networks (BINets) (Ge et al., 2016a; Ge et al., 2016b). In contrast to the keyword graph which is based on word co-occurrence, a BINet is constructed based on burst co-occurrence. In a BINet (Fig. 2), a node is a burst of one word, which can be represented by the word with one of its burst periods, and an edge between two nodes indicates how strongly they are related (i.e., how frequently they co-occur). Since the nodes in a BINet contains temporal information (e.g., burst period), a BINet is time-aware in which nodes in a community are both topically and temporally coherent. Hence, we can say each community in a BINet corresp"
C16-1309,P14-5010,0,0.00286061,"event communities: C = [E1 , E2 , ..., Ek ] 3: while kLk > 0 do 4: A ← L[0] (the first element in L) 5: E ← {A} ∪ {A0 |f (A, A0 ) > σA } 6: L←L−E 7: C.append(E) 8: end while 3281 4 Experiments and Evaluation We conduct experiments to evaluate the performance of our approach. We first evaluate our approach on the TDT4 dataset to compare other event detection approaches. Then, we apply our approach on a larger corpus (2009 – 2010 news corpus) to test its scalability and performance. For preprocessing, we remove stopwords and conduct lemmatization and name tagging using Stanford CoreNLP toolkit (Manning et al., 2014) before the construction of a BINet. 4.1 Evaluation on TDT4 The TDT4 collection is a well known dataset for comparing methods for event detection. The English part of the dataset includes approximately 29,000 news documents from news agencies such as CNN and BBC from October 2000 to Janurary 2001 (spanning 4 months), while only 1,884 documents1 are annotated to be related to 71 human identified events (topics). As the setting adopted by previous work (Li et al., 2005; Sayyadi and Raschid, 2013), we use the annotated subset as gold standard for evaluating the performance of our models. As most"
C16-1309,P12-2009,0,0.190351,"based on the BINet representation, which not only solve the centroid deviation problem but also are more efficient than traditional approaches. • We construct and release a dataset for evaluating event detection models on a large text stream during a long time span. 2 2.1 Burst Information Networks Burst Detection A word’s burst refers to a sharp increase of word frequency during a period. It usually indicates key information, important events or trending topics in a text stream as Figure 3 shows and is useful for many applications. In this paper, we detect a word’s burst using the method of Zhao et al. (2012) which is a variant of (Kleinberg, 2003) and models burst detection as a burst state sequence decoding problem where a word w’s burst state st (w) at time t could be 1 or 0 to indicate if the word bursts or not at t. Specially, if a word w bursts at every time epoch during a period, we call this period a burst period of w and w has a burst during this period. In Figure 3, earthquake has 2 burst periods (i.e., Jan 12 - Jan 31, and Feb 27 - Mar 7), which correspond to two famous earthquake events (i.e., 2010 Haiti earthquake and 2010 Chile earthquake). 3277 … government police aid donation Haiti"
chen-etal-2006-study,J90-1003,0,\N,Missing
chen-etal-2006-study,W03-1725,0,\N,Missing
chen-etal-2006-study,W02-1407,0,\N,Missing
chen-etal-2006-study,C02-1125,0,\N,Missing
chen-etal-2006-study,I05-3009,0,\N,Missing
D09-1153,W04-2412,0,0.0160239,"Missing"
D09-1153,P06-2013,0,0.18143,"ce reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese Penn Treebank (CTB). This performance drops to 71.9 when a real parser is used instead1 (Xue, 2008). Comparatively, the best English SRL results reported drops from 91.2 (Pradhan et al., 2008) to 80.56 (Surdeanu et al., 2007). These results suggest that as still in its infancy stage, Chinese full parsing acts as a central bottleneck that severely limits our ability to solve Chinese SRL. On the contrary, Chinese shallow parsing has gained a promising result (Chen et al., 2006); hence it is an alternative choice for Chinese SRL. This paper addresses the Chinese SRL problem on the basis of shallow syntactic information at the level of phrase chunks. We first extend the study on Chinese chunking presented in (Chen et al., 2006) by raising a set of additional features. The new set of features yield improvement over the strong chunking system described in (Chen et al., 2006). On the basis of our shallow parser, we implement lightweight systems which solve SRL as a sequence labeling problem. This is accomplished by casting SRL as the classification of syntactic chunks (e"
D09-1153,I08-2132,0,0.0184496,"antically annotated corpus of Chinese. Their experiments were evaluated only on ten specified verbs with a small collection of Chinese sentences. This work made the first attempt on Chinese SRL and produced promising results. After the CPB was built, (Xue and Palmer, 2005) and (Xue, 2008) have produced more complete and systematic research on Chinese SRL. Ding and Chang (2008) divided SRC into two sub-tasks in sequence. Under the hierarchical architecture, each argument should first be determined whether it is a core argument or an adjunct, and then be classified into fine-grained categories. Chen et al. (2008) introduced an application of transductive SVM in Chinese SRL. Because their experiments took hand-crafted syntactic trees as input, how transductive SVMs perform in Chinese SRL in realistic situations is still unknown. Most existing systems for automatic Chinese SRL make use of a full syntactic parse of the sentence in order to define argument boundaries and 2 Our system is http://code.google.com/p/csrler/ available at to extract relevant information for training classifiers to disambiguate between role labels. On the contrary, in English SRL research, there have been some attempts at relaxin"
D09-1153,D08-1034,0,0.292702,"over the best reported SRL performance in the literature. Additionally, we put forward a rule-based algorithm to automatically acquire Chinese verb formation, which is empirically shown to enhance SRL. 1 Introduction In the last few years, there has been an increasing interest in Semantic Role Labeling (SRL) on several languages, which consists of recognizing arguments involved by predicates of a given sentence and labeling their semantic types. Nearly all previous Chinese SRL research took full syntactic parsing as a necessary pre-processing step, such as (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008). Many features are extracted to encode the complex syntactic information. In English SRL research, there have been some attempts at relaxing the necessity of using full syntactic parses; better understanding of SRL with shallow parsing is achieved by CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, it is still unknown how these methods perform on other languages, such as Chinese. To date, the best SRL performance reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese Penn Treebank (CTB). This perform"
D09-1153,N03-2009,0,0.0190685,"ok hand-crafted syntactic trees as input, how transductive SVMs perform in Chinese SRL in realistic situations is still unknown. Most existing systems for automatic Chinese SRL make use of a full syntactic parse of the sentence in order to define argument boundaries and 2 Our system is http://code.google.com/p/csrler/ available at to extract relevant information for training classifiers to disambiguate between role labels. On the contrary, in English SRL research, there have been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For example, Hacioglu and Ward (2003) considered SRL as a chunking task; Pradhan et al. (2005) introduced a new procedure to incorporate SRL results predicted respectively on full and shallow syntactic parses. Previous work on English suggests that even good labeling performance has been achieved by full parse based SRL systems, partial parse based SRL systems can still enhance their performance. Though better understanding of SRL with shallow parsing on English is achieved by CoNLL-2004 shared task (Carreras and M`arquez, 2004), little is known about how these SRL methods perform on Chinese. 3 Chinese Shallow Parsing There have"
D09-1153,W00-0730,0,0.0392688,"Missing"
D09-1153,N01-1025,0,0.127166,"Missing"
D09-1153,W05-0634,0,0.0259716,"VMs perform in Chinese SRL in realistic situations is still unknown. Most existing systems for automatic Chinese SRL make use of a full syntactic parse of the sentence in order to define argument boundaries and 2 Our system is http://code.google.com/p/csrler/ available at to extract relevant information for training classifiers to disambiguate between role labels. On the contrary, in English SRL research, there have been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For example, Hacioglu and Ward (2003) considered SRL as a chunking task; Pradhan et al. (2005) introduced a new procedure to incorporate SRL results predicted respectively on full and shallow syntactic parses. Previous work on English suggests that even good labeling performance has been achieved by full parse based SRL systems, partial parse based SRL systems can still enhance their performance. Though better understanding of SRL with shallow parsing on English is achieved by CoNLL-2004 shared task (Carreras and M`arquez, 2004), little is known about how these SRL methods perform on Chinese. 3 Chinese Shallow Parsing There have been some research on Chinese shallow parsing, and a vari"
D09-1153,J08-2006,0,0.0246748,"ing the necessity of using full syntactic parses; better understanding of SRL with shallow parsing is achieved by CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, it is still unknown how these methods perform on other languages, such as Chinese. To date, the best SRL performance reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese Penn Treebank (CTB). This performance drops to 71.9 when a real parser is used instead1 (Xue, 2008). Comparatively, the best English SRL results reported drops from 91.2 (Pradhan et al., 2008) to 80.56 (Surdeanu et al., 2007). These results suggest that as still in its infancy stage, Chinese full parsing acts as a central bottleneck that severely limits our ability to solve Chinese SRL. On the contrary, Chinese shallow parsing has gained a promising result (Chen et al., 2006); hence it is an alternative choice for Chinese SRL. This paper addresses the Chinese SRL problem on the basis of shallow syntactic information at the level of phrase chunks. We first extend the study on Chinese chunking presented in (Chen et al., 2006) by raising a set of additional features. The new set of fe"
D09-1153,W95-0107,0,0.0640529,"chunk definitions have been proposed. However, most of these studies did not provide sufficient detail. In our system, we use chunk definition presented in (Chen et al., 2006), which provided a chunk extraction tool. The tool to extract chunks from CTB was developed by modifying the English tool used in CoNLL-2000 shared task, Chunklink3 , and is publicly available at http://www.nlplab.cn/chenwl/chunking.html. The definition of syntactic chunks is illustrated in Line CH in Figure 1. For example, ”保险公司/the insurance company”, consisting of two nouns, is a noun phrase. With IOB2 representation (Ramshaw and Marcus, 1995), the problem of Chinese chunking can be regarded as a sequence labeling task. In this paper, we first implement the chunking method described in (Chen et al., 2006) as a strong baseline. To conveniently illustrate, we denote a word in focus with a fixed window w−2 w−1 ww+1 w+2 , where w is current token. The baseline features includes: • Uni-gram word/POS tag feature: w−2 , w−1 , w, w+1 , w+2 ; • Bi-gram word/POS tag feature: w−2 w−1 , w−1 w, w w+1 , w+1 w+2 ; 3 http://ilk.uvt.nl/team/sabine/chunklink/chunklink 2-22000 for conll.pl 1476 WORD: POS: CH: M1: M2-AI: M2-SRC: 截止 目前 保险 公司 已 为 三峡 工程"
D09-1153,N04-1032,0,0.124339,"achieving significant improvements over the best reported SRL performance in the literature. Additionally, we put forward a rule-based algorithm to automatically acquire Chinese verb formation, which is empirically shown to enhance SRL. 1 Introduction In the last few years, there has been an increasing interest in Semantic Role Labeling (SRL) on several languages, which consists of recognizing arguments involved by predicates of a given sentence and labeling their semantic types. Nearly all previous Chinese SRL research took full syntactic parsing as a necessary pre-processing step, such as (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008). Many features are extracted to encode the complex syntactic information. In English SRL research, there have been some attempts at relaxing the necessity of using full syntactic parses; better understanding of SRL with shallow parsing is achieved by CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, it is still unknown how these methods perform on other languages, such as Chinese. To date, the best SRL performance reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese"
D09-1153,J08-2004,0,0.367894,"mprovements over the best reported SRL performance in the literature. Additionally, we put forward a rule-based algorithm to automatically acquire Chinese verb formation, which is empirically shown to enhance SRL. 1 Introduction In the last few years, there has been an increasing interest in Semantic Role Labeling (SRL) on several languages, which consists of recognizing arguments involved by predicates of a given sentence and labeling their semantic types. Nearly all previous Chinese SRL research took full syntactic parsing as a necessary pre-processing step, such as (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008). Many features are extracted to encode the complex syntactic information. In English SRL research, there have been some attempts at relaxing the necessity of using full syntactic parses; better understanding of SRL with shallow parsing is achieved by CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, it is still unknown how these methods perform on other languages, such as Chinese. To date, the best SRL performance reported on the Chinese Proposition Bank (CPB) corresponds to a F-measure is 92.0, when using the handcrafted parse trees from Chinese Penn Treeb"
D09-1153,N07-1070,0,\N,Missing
D13-1001,D08-1073,0,0.163414,"al analysis, document timestamps are very useful. For instance, temporal information retrieval models take into consideration the document’s creation time for document retrieval and ranking (Kalczynski and Chou, 2005; Berberich et al., 2007) for better dealing with time-sensitive queries; some infor∗ Corresponding author mation retrieval applications such as Google Scholar can list articles published during the time a user specifies for better satisfying users’ needs. In addition, timeline summarization techniques (Hu et al., 2011; Binh Tran et al., 2013) and some event-event ordering models (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009) also rely on the timestamps. Unfortunately, many documents on the web do not have a credible timestamp, as Chambers (2012) reported. Therefore, it is significant to date documents, that is to predict document creation time. One typical method for dating document is based on temporal language models, which were first used for dating by de Jong et al. (2005). They learned language models (unigram) for specific time periods and scored articles with normalized log-likelihood ratio scores. The other typical approach for the task was proposed by Nathanael Chambers (2012). I"
D13-1001,P12-1011,0,0.0784756,"ocument retrieval and ranking (Kalczynski and Chou, 2005; Berberich et al., 2007) for better dealing with time-sensitive queries; some infor∗ Corresponding author mation retrieval applications such as Google Scholar can list articles published during the time a user specifies for better satisfying users’ needs. In addition, timeline summarization techniques (Hu et al., 2011; Binh Tran et al., 2013) and some event-event ordering models (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009) also rely on the timestamps. Unfortunately, many documents on the web do not have a credible timestamp, as Chambers (2012) reported. Therefore, it is significant to date documents, that is to predict document creation time. One typical method for dating document is based on temporal language models, which were first used for dating by de Jong et al. (2005). They learned language models (unigram) for specific time periods and scored articles with normalized log-likelihood ratio scores. The other typical approach for the task was proposed by Nathanael Chambers (2012). In Chambers’s work, discriminative classifiers – maximum entropy (MaxEnt) classifiers were used by incorporating linguistic features and temporal con"
D13-1001,de-marneffe-etal-2006-generating,0,0.00993176,"Missing"
D13-1001,D11-1142,0,0.0108213,"allenges and their corresponding solutions are presented. We first discuss the event extraction and processing involving relative temporal relation mining, event coreference resolution and distinguishing specific extractions from generic ones in Section 3.1. Then, we show the confidence boosting algorithm in detail in Section 3.2. 3.1 Event extraction and processing As mentioned in previous sections, events play a key role in the propagation models. We define an event as a Subject-Predicate-Object (SPO) triple. To extract events from raw text, an open information extraction software - ReVerb (Fader et al., 2011) is used. ReVerb is a program that automatically identifies and extracts relationships from English sentences. It takes raw text as input and outputs SPO triples which are called extractions. However, extractions extracted by ReVerb cannot be used directly for our propagation models for three main reasons. First, the relative temporal relations between documents and the extractions are unavailable. Second, the extractions extracted from different documents do not have any connection even if they refer to the same event. Third, propagations from generic events are very likely to lead to propaga"
D13-1001,P09-1046,0,0.0213263,"mps are very useful. For instance, temporal information retrieval models take into consideration the document’s creation time for document retrieval and ranking (Kalczynski and Chou, 2005; Berberich et al., 2007) for better dealing with time-sensitive queries; some infor∗ Corresponding author mation retrieval applications such as Google Scholar can list articles published during the time a user specifies for better satisfying users’ needs. In addition, timeline summarization techniques (Hu et al., 2011; Binh Tran et al., 2013) and some event-event ordering models (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009) also rely on the timestamps. Unfortunately, many documents on the web do not have a credible timestamp, as Chambers (2012) reported. Therefore, it is significant to date documents, that is to predict document creation time. One typical method for dating document is based on temporal language models, which were first used for dating by de Jong et al. (2005). They learned language models (unigram) for specific time periods and scored articles with normalized log-likelihood ratio scores. The other typical approach for the task was proposed by Nathanael Chambers (2012). In Chambers’s work, discri"
D15-1185,D11-1142,0,0.0773194,"married to a doctor who lives in Austin, the capital of Texas. T1. bemarriedTo(Ayrton Senna,Doctor) T1. Ayrton Senna was married to a doctor T2. livein(Doctor, Austin) T2. [The] doctor lives in Austin T3. beCapitalof(Austin, Texas) T3. Austin [is] the capital of Texas H1. livein(Ayrton Senna, Texas) Hypothesis: Ayrton Senna lives in Texas. Hypothesis: Ayrton Senna lives in Texas. Figure 2: Text Commitments Example Figure 3: Text Predicates Example ments to predicates. For example, the commitments in Figure 2 can be transformed to the predicates (or triples) shown in Figure 3. We use R E VERB (Fader et al., 2011) to extract the triples (predicate + 2 Arguments). To make the inference process in the next section more convenient, we order that all of the arguments should be NPs. Therefore, we check if the arguments in the triples contain or have overlap with any of the NPs, replace it with that NP, and the predicates are successfully extracted. facts. We use AMIE to mine inference rules from YAGO. AMIE1 (Gal´arraga et al., 2013) is a state-ofthe-art inference rule mining system. The motivation of AMIE is that KBs themselves often already contain enough information to derive and add new facts. If, for ex"
D15-1185,H05-1049,0,0.143446,"n et al., 2006). For many natural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T -H pairs. Then the RTE task is reduced to the identification of the commitments from T which are most likel"
D15-1185,C08-1043,0,0.722221,"atural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T -H pairs. Then the RTE task is reduced to the identification of the commitments from T which are most likely to support the"
D15-1185,N06-1006,0,0.0918988,"Missing"
D15-1185,W07-1407,0,0.79166,"(entailed) from the other one (T )(Dagan et al., 2006). For many natural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T -H pairs. Then the RTE task is reduced to the identification of the commitments fro"
D15-1185,P09-1113,0,0.0204461,"in Austin, the capital of Texas, in 1998. H: Ayrton Senna lives in Texas. R T: R1(e11,e12) R2(e21,e22) R3(e31,e32) H: RH(eH1,eH2) Discourse commitment extract YAGO R .. R YAGO H Inference rules R R T R AIME .. R R R1(e11,e12)^R2(e21,e22)=&gt;R3(e31,e32) R4(e41,e42)^R5(e51,e52)=&gt;R6(e61,e62) R7(e71,e72)^R8(e81,e82)=&gt;R9(e91,e92) MLN Construct Markov Logic Network .. Rp(ep1,ep2)^Rq(eq1,eq2)=&gt;Rr(er1,er2) facts P(RH) H is True or false? train Figure 1: The Framework of our RTE system pose to use the predicate-argument structure to represent the extracted discourse commitments. Inspired by the work of (Mintz et al., 2009), we make use of the external knowledge YAGO and borrow the distant supervision technique to mine implicit facts for the extracted predicates. For example, Ayrton Senna was married to a doctor who lives in Austin, the capital of Texas, in 1998. We translate this example into the predicateargument structures such as bemarried(Senna, doctor), livein(doctor, Austin), captial(Austin, Texas). Then through distant supervision, we can get some new facts livein(Senna, Austin), livein(Senna, Texas). To judge the confidence of the new facts, we construct a probabilistic network with all the facts and ad"
D15-1185,P06-2105,0,0.216099,"of the commitments from H. From the work of Hickl (2008), we can see that a deep understanding of text is critical to the RTE performance and discourse commitments can serve a good media to understanding text. However, the limitation of Hickl (2008)’s work is, the extracted discourse commitments are still from the original text and do not explore the implicit meaning latent behind the text. Another kind of deep methods involves first transferring natural language to logic representation and then conducting strict logic inference based on the logic representations (de Salvo Braz et al., 2006; Tatu and Moldovan, 2006; Wotzlaw and Coote, 2013). Through logic inference, some implicit knowledge behind the text can be mined. However, it is not easy to translate the natural language text into formal logic expressions and the translation process inevitably suffer from great information loss. Through analysis above, in our work, we pro1620 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1620–1625, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Align the predicates to the YAGO database T: Ayrton Senna was married to a doctor"
D15-1185,U06-1019,0,0.10598,"ng can be corrected by our framework. For instance, T is “Hughes loved his wife, Gracia, and was absolutely obsessed with his little daughter Elicia.” and H is “Gracia’s daughter is Elicia.” It is not easy for the former baselines to recognize this entailment, but our framework can easily recognize it to be “true”. In this way, our framework has achieved a higher result. 4 Related work Textual Entailment Recognizing (RTE) task has been widely studied by many previous works. Firstly, the method based on similarity and overlap (Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006). This kind of methods can help solve the paraphrase recognition problem, which is a subset of RTE. Another important similarity-based method is tree kernel (Zanzotto and Moschitti, 2006), which rely on the cross-pair similarity between two pairs (T 0 , H 0 ) and (T 00 , H 00 ). Secondly, some approaches extract the knowledge in T -H pair and check if the knowledge in T contains the knowledge in H. Hickl (2008) transformed the T -H pair into discourse commitments, reducing the RTE task to the identification of the commitments from a T which support the inference of the H. Other works map the t"
D15-1185,P06-1051,0,0.467153,"ing predicates and mining inference rules. YAGO2 contains more than 940K facts and about 470K entities. We run the AMIE system on YAGO2 for only one time to get all inference rules (about more than 1.8K in total). For each T -H pair, we only choose a portion of related inference rules to construct MLN. The chosen rules must contain at least one predicate which occurred in the predicates of T -H pair. We only use the MLN to infer when the discourse commitment paraphrasing cannot identify a T -H pair as ”Entailment”, which is a back-off method. We compare our result with 5 baseline systems: (1) Zanzotto and Moschitti (2006)’s simple termoverlap measure, (2) MacCartney et al. (2006)’s semantic graph-mapping approach, (3) Hickl et al. (2006)’s classification-based term alignment approach. (4) Hickl (2008)’s discourse commitment based Alignment, (5) Tatu and Moldovan (2006)’s strict logic based method. The comparison of the 5 baselines and our framework is shown in Table 1. Since we only need to judge “Yes” or “No” for the 1600 examples, the precision is equal to the recall, so that we only report the precision. According to the Table 1, the performance of our framework is higher than Hickl (2008)’s baseline, which"
D15-1186,P06-2013,0,0.0308745,"ho presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3 1629 https://code.google.com/p/word2vec/ word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature engineering. However, by max-pooling operation, the convolution approach only preserved the most evident features in a sentence, thus can"
D15-1186,D08-1034,1,0.921176,"ssign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engineering, which needs domain knowledge and laborious work of feature extraction and selection. Secondly, although sophisticated features are designed, the long-range dependencies in a sentence can hardly be modeled. Thirdly, a specific annotated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different semantic role labels and annotation schema but related latent semantic meaning, can"
D15-1186,J02-3001,0,0.485015,"ing. Experimental results on Chinese Proposition Bank (CPB) show a significant improvement over the state-ofthe-art methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance. 1 Figure 1: A sentence with semantic roles labeled from CPB. Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engine"
D15-1186,P15-1109,0,0.0484622,"mation over very long time spans. Bidirectional RNN (Schuster and Paliwal, 1997) and bidirectional LSTM RNN (Graves et al., 2005) are the extensions of RNN and LSTM RNN with the capability of capturing contextual information from both directions in the sequence. In recent years, RNN has shown the state-of-theart results in many NLP problems such as language modeling (Mikolov et al., 2010) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014). Sundermeyer et al. (2014) also used bidirectional LSTM RNN model to improve strong baselines when modeling translation. More recently, Zhou and Xu (2015) proposed LSTM RNN approach for English Semantic Role Labeling, which shared similar idea with our model. However, the features used and the network architecture were different from ours. Moreover, it is delightful that our work can achieve a rather good result with a relatively simpler model architecture. 5 Conclusion In this paper, we formulate Chinese SRL problem with the framework of bidirectional LSTM RNN model. In our approach, the bidirectional and long-range dependencies in a sentence, which are important for Chinese SRL, can be well modeled. And with the framework of deep neural netwo"
D15-1186,N04-1032,0,0.825387,"n Chinese Proposition Bank (CPB) show a significant improvement over the state-ofthe-art methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance. 1 Figure 1: A sentence with semantic roles labeled from CPB. Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engineering, which needs domain"
D15-1186,D09-1153,1,0.93767,"Table 2, compared to standard pre-training, the influence of heterogenous data is more evident. We can explain this difference via the distinction between these two kinds of methods for performance improvement. The information provided by standard pretraining with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident. 3.1 Experimental Setting To facilitate comparison with previous work, we conduct experiments on the standard benchmark dataset CPB 1.0.1 We follow the same data setting as previous work (Xue, 2008; Sun et al., 2009), which divided the dataset into three parts: 648 files (from chtb 081.fid to chtb 899.fid) are used as the training set. The development set includes 40 files, from chtb 041.fid to chtb 080.fid. The test set includes 72 files, which are chtb 001.fid to chtb 040.fid, and chtb 900.fid to chtb 931.fid. We use another annotated corpus2 with distinct semantic role labels and annotation schema, which is designed by ourselves for other projects, as heterogeneous resource. This labeled dataset has 17,308 annotated sentences, and the semantic roles concerned are like “agent” and “patient”, resulting i"
D15-1186,P10-2031,0,0.609207,"of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engineering, which needs domain knowledge and laborious work of feature extraction and selection. Secondly, although sophisticated features are designed, the long-range dependencies in a sentence can hardly be modeled. Thirdly, a specific annotated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different semantic role labels and annotation schema but related latent semantic meaning, can alleviate this problem. However,"
D15-1186,D14-1003,0,0.0178283,", Hochreiter and Schmidhuber (1997) proposed long-short-term memory (LSTM), which has been shown capable of storing and accessing information over very long time spans. Bidirectional RNN (Schuster and Paliwal, 1997) and bidirectional LSTM RNN (Graves et al., 2005) are the extensions of RNN and LSTM RNN with the capability of capturing contextual information from both directions in the sequence. In recent years, RNN has shown the state-of-theart results in many NLP problems such as language modeling (Mikolov et al., 2010) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014). Sundermeyer et al. (2014) also used bidirectional LSTM RNN model to improve strong baselines when modeling translation. More recently, Zhou and Xu (2015) proposed LSTM RNN approach for English Semantic Role Labeling, which shared similar idea with our model. However, the features used and the network architecture were different from ours. Moreover, it is delightful that our work can achieve a rather good result with a relatively simpler model architecture. 5 Conclusion In this paper, we formulate Chinese SRL problem with the framework of bidirectional LSTM RNN model. In our approach, the bidirectional and long-range d"
D15-1186,W03-1707,0,0.829677,"rt methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance. 1 Figure 1: A sentence with semantic roles labeled from CPB. Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engineering, which needs domain knowledge and laborious work of feature extraction and selection. Secondly, althoug"
D15-1186,J08-2004,0,0.16159,"icate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engineering, which needs domain knowledge and laborious work of feature extraction and selection. Secondly, although sophisticated features are designed, the long-range dependencies in a sentence can hardly be modeled. Thirdly, a specific annotated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different semantic role labels and annotation schema but related latent"
D15-1186,D14-1041,0,0.122523,"un and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3 1629 https://code.google.com/p/word2vec/ word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature engineering. However, by max-pooling operation, the convolution approach only preserved the most evident features in a sentence, thus can only weakly model the dependencies. With our bidirectional LSTM RNN model, this problem can be well all"
D15-1289,P14-1013,0,0.0145563,"epresentations for entities from their binary term vectors with an unsupervised way. The deep neural network (DNN) (Hinton et al., 2006; Bengio et al., 2007) is a multilayer learning model. It is mainly used for learning the high-level abstract representations of original input data. Given the generalization and the abstraction introduced in the representation learning procedure, DNN allows us to better model the interactions among different kinds of input features, and measure the similarity at a more general level. Inspired by the work in (Hinton, 2007; Bengio et al., 2012; He et al., 2013; Cui et al., 2014), we use auto-encoder (Bourlard and Kamp, 1988; Hinton and Zemel, 1994) to learn the representations for classes and properties. The auto-encoder is one of the neural network variants that can automatically discover interesting abstractions to represent an unlabeled dataset. separatively calculating the similarities and aggregating them later with a combination method; 2) the learned representation can not only express the meaning of the original descriptions of an entity but also captures the interactions among different descriptions. 3.1.1 Creating term vector for entity We first generate a"
D15-1289,P13-2006,0,0.176723,"rn the abstract representations for entities from their binary term vectors with an unsupervised way. The deep neural network (DNN) (Hinton et al., 2006; Bengio et al., 2007) is a multilayer learning model. It is mainly used for learning the high-level abstract representations of original input data. Given the generalization and the abstraction introduced in the representation learning procedure, DNN allows us to better model the interactions among different kinds of input features, and measure the similarity at a more general level. Inspired by the work in (Hinton, 2007; Bengio et al., 2012; He et al., 2013; Cui et al., 2014), we use auto-encoder (Bourlard and Kamp, 1988; Hinton and Zemel, 1994) to learn the representations for classes and properties. The auto-encoder is one of the neural network variants that can automatically discover interesting abstractions to represent an unlabeled dataset. separatively calculating the similarities and aggregating them later with a combination method; 2) the learned representation can not only express the meaning of the original descriptions of an entity but also captures the interactions among different descriptions. 3.1.1 Creating term vector for entity W"
D15-1289,W14-2416,0,0.0234937,"Missing"
D15-1289,P14-1090,0,0.0855721,"Missing"
D16-1075,P15-1056,1,0.182234,"challenges for summarizing a text stream. First, a stream summarization model should be able to be aware of redundant information in the stream for avoiding generating redundant content in the summary; second, a stream summarization algorithm should be capable of analyzing text content on the stream level for identifying the most important information in the stream; third, a stream summarization model should be efficient, scalable and able to run in an online fashion because data size of a text stream is usually huge, and it is dynamic and updated every second. The previous approaches (e.g., (Ge et al., 2015b)) tend to cluster similar documents as event detection to avoid redundancy, rank the clusters based on their sizes and topical relevance to the reference summaries, and select one document from each cluster as representative documents. Due to the high time complexity of clustering models, their approaches usually run slowly and are not scalable. 785 To overcome the limitations, we propose Burst Information Networks (BINet) as a novel representation of a text stream. In a BINet (Figure 2), a node is a burst word (including entities) with the time span of one of its burst periods, and an edge"
D16-1075,P15-1155,0,0.0215125,"xt stream due to high complexity in time and space. Other previous work dealing with stream data is mainly focused on topic and event detection (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; He et al., 2007; Sayyadi et al., 2009; Sakaki et al., 2010; Zhao et al., 2012; Ge et al., 2015a), dynamic language and topic modelling (Blei and Lafferty, 2006; Iwata et al., 2010; Wang et al., 2012; Yogatama et al., 2014), incremental (temporal) summarization and timeline generation for one major news event (Allan et al., 2001; Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013; Kedzie et al., 2015; Tran et al., 2015; Yao et al., 2016), a sports match (Takamura et al., 2011) or users on the social network (Li and Cardie, 2014). Different from traditional single and multi792 document summarization (Carbonell and Goldstein, 1998; Lin, 2004; Erkan and Radev, 2004; Conroy et al., 2004; Li et al., 2007; Wan and Yang, 2008; Chen and Chen, 2012; Wan and Zhang, 2014) whose focus is to select important sentences, the focus of stream summarization is to select representative documents referring to important news events. The novel paradigm focuses on the summarization problem in the big data age a"
D16-1075,P13-2099,1,0.841386,"on a real-time text stream due to high complexity in time and space. Other previous work dealing with stream data is mainly focused on topic and event detection (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; He et al., 2007; Sayyadi et al., 2009; Sakaki et al., 2010; Zhao et al., 2012; Ge et al., 2015a), dynamic language and topic modelling (Blei and Lafferty, 2006; Iwata et al., 2010; Wang et al., 2012; Yogatama et al., 2014), incremental (temporal) summarization and timeline generation for one major news event (Allan et al., 2001; Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013; Kedzie et al., 2015; Tran et al., 2015; Yao et al., 2016), a sports match (Takamura et al., 2011) or users on the social network (Li and Cardie, 2014). Different from traditional single and multi792 document summarization (Carbonell and Goldstein, 1998; Lin, 2004; Erkan and Radev, 2004; Conroy et al., 2004; Li et al., 2007; Wan and Yang, 2008; Chen and Chen, 2012; Wan and Zhang, 2014) whose focus is to select important sentences, the focus of stream summarization is to select representative documents referring to important news events. The novel paradigm focuses on the summarization problem"
D16-1075,W04-1013,0,0.022973,"Ge et al. (2015b) used manually edited event chronicles of various topics on the web3 during 2009 3 http://www.mapreport.com; http://www.infoplease.com; 789 as reference summaries for summarizing the news stream during 2010. The information of the reference summaries is summarized in Table 2. In evaluation, they pooled entries in stream sumamries generated by various approaches, annotated each entry based on the reference summary and the manually edited event chronicles on the web, and used precision@K to evaluate the quality of top K event entries in a stream summary instead of using ROUGE (Lin, 2004) because news stream summaries are eventcentric. In this paper, we adopt the same evaluation setting and use the same reference summaries and the annotations with our previous work (Ge et al., 2015b) to evaluate our summaries’ quality. For the event entries that are not in Ge et al. (2015b)’s annotations, we have 3 human judges annotate them according to the previous annotation guideline and consider an entry correct if it is annotated as correct by at least 2 judges. We evaluate our approaches by comparing to Ge et al. (2015b)’s approach and the baselines in their work: • R ANDOM: this baseli"
D16-1075,P14-5010,0,0.00387579,"APW and XIN news stories in English Gigaword (Graff et al., 2003)) as a news stream. We detect burst words using Kleinberg algorithm (Kleinberg, 2003), which models word burst detection as a burst state decoding problem. In total, there are 140,557 documents in the dataset. Topic Disaster Sports Politics Military Comprehensive #Entry 35 19 8 14 85 #Entry in corpus 28 12 5 13 64 Table 2: The number of event entries in the reference summaries. The third column is the number of event entries excluding those events that do not appear in the corpus. We removed stopwords and used Stanford CoreNLP (Manning et al., 2014) to do lemmatization and named tagging, and built BINets on the news stream during 2009 and 2010 separately. On the 2009 news stream, there are 31,888 nodes and 833,313 edges while there are 32,997 nodes and 825,976 edges on the 2010 stream. Ge et al. (2015b) used manually edited event chronicles of various topics on the web3 during 2009 3 http://www.mapreport.com; http://www.infoplease.com; 789 as reference summaries for summarizing the news stream during 2010. The information of the reference summaries is summarized in Table 2. In evaluation, they pooled entries in stream sumamries generated"
D16-1075,Q14-1015,0,0.0274957,"marization challenge. However, they studied the problem on a static timestamped corpus instead of on a dynamic text stream and their proposed pipeline-style approach cannot be applied on a real-time text stream due to high complexity in time and space. Other previous work dealing with stream data is mainly focused on topic and event detection (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; He et al., 2007; Sayyadi et al., 2009; Sakaki et al., 2010; Zhao et al., 2012; Ge et al., 2015a), dynamic language and topic modelling (Blei and Lafferty, 2006; Iwata et al., 2010; Wang et al., 2012; Yogatama et al., 2014), incremental (temporal) summarization and timeline generation for one major news event (Allan et al., 2001; Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013; Kedzie et al., 2015; Tran et al., 2015; Yao et al., 2016), a sports match (Takamura et al., 2011) or users on the social network (Li and Cardie, 2014). Different from traditional single and multi792 document summarization (Carbonell and Goldstein, 1998; Lin, 2004; Erkan and Radev, 2004; Conroy et al., 2004; Li et al., 2007; Wan and Yang, 2008; Chen and Chen, 2012; Wan and Zhang, 2014) whose focus is to select importan"
D16-1075,P12-2009,0,0.164471,"r an entry correct if it is annotated as correct by at least 2 judges. We evaluate our approaches by comparing to Ge et al. (2015b)’s approach and the baselines in their work: • R ANDOM: this baseline randomly selects documents in the dataset as event entries. • N B: this baseline uses Naive Bayes to cluster documents for event detection and ranks the clusters based on the combination score of topical relevance and the event impact (i.e., event cluster size). The earliest documents in the topranked clusters are selected as entries. • B-H AC: similar to N B except that BurstVSM representation (Zhao et al., 2012) is used for event detection using Hierarchical Agglomerative Clustering algorithm. • TA HBM: similar to N B except that the stateof-the-art event detection model (TaHBM) proposed by Ge et al. (2015b) is used for event detection. • Ge et al. (2015b): the state-of-the-art stream summarization approach which used TaHBM to detect events and L2R model to rank events. Note that we did not compare with previous multidocument summarization models because the goal and setting of stream summarization are different from multi-document summarization, as Section 1 https://en.wikipedia.org/wiki/2009 Random"
D16-1075,N16-3015,0,\N,Missing
D16-1212,P06-2013,0,0.553957,") task was first proposed by Gildea and Jurafsky (2002). Previous approaches on Chinese SRL can be classified into two categories: (1) feature-based approaches (2) neural network based approaches. Among feature-based approaches, Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. Xue and Palmer (2003) proposed Chinese Proposition Bank (CPB), which leads to more complete and systematic research on Chinese SRL (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009). Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese SRL. However, most of the aforementioned approaches did not take the compatible ar"
D16-1212,D08-1034,1,0.920352,"Missing"
D16-1212,J02-3001,0,0.822677,"revious works did not explicitly model argument relationships. We use a simple maximum entropy classifier to capture the two categories of argument relationships and test its performance on the Chinese Proposition Bank (CPB). The experiments show that argument relationships is effective in Chinese semantic role labeling task. 1 Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Previous works of Chinese SRL include featurebased approaches and neural network based approaches. Feature-based approaches often extract a However, both of the above two kinds of approaches identify each candidate argument separately without considering the relationship between arguments. We define two categories of argument relationships here: (1) Compatible arguments: if one candidate argument belongs to a given predica"
D16-1212,P16-1116,1,0.675445,"ial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese SRL. However, most of the aforementioned approaches did not take the compatible arguments and incompatible arguments into account. Inspired by Sha et al. (2016), our approach model the two argument relationships explicitly to achieve a better performance on Chinese SRL. 3 Capturing the Relationship Between Arguments We found that there are two typical relationships between candidate arguments: (1) Compatible arguments: if one candidate argument belongs to one 2012 event, then the other is more likely to belong to the same event; (2) incompatible arguments: if one candidate argument belongs to one event, then the other is less likely to belong to the same event. We trained a maximum entropy classifier to predict the relationship between two candidate"
D16-1212,N04-1032,0,0.812915,"citly model argument relationships. We use a simple maximum entropy classifier to capture the two categories of argument relationships and test its performance on the Chinese Proposition Bank (CPB). The experiments show that argument relationships is effective in Chinese semantic role labeling task. 1 Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Previous works of Chinese SRL include featurebased approaches and neural network based approaches. Feature-based approaches often extract a However, both of the above two kinds of approaches identify each candidate argument separately without considering the relationship between arguments. We define two categories of argument relationships here: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is mor"
D16-1212,D09-1153,1,0.914347,"elated Work Semantic Role Labeling (SRL) task was first proposed by Gildea and Jurafsky (2002). Previous approaches on Chinese SRL can be classified into two categories: (1) feature-based approaches (2) neural network based approaches. Among feature-based approaches, Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. Xue and Palmer (2003) proposed Chinese Proposition Bank (CPB), which leads to more complete and systematic research on Chinese SRL (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009). Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese SRL. However, most of the aforementioned a"
D16-1212,P10-2031,0,0.246301,"Relationships for Chinese Semantic Role Labeling Lei Sha, Tingsong Jiang, Sujian Li, Baobao Chang, Zhifang Sui Key Laboratory of Computational Linguistics, Ministry of Education School of Electronics Engineering and Computer Science, Peking University Collaborative Innovation Center for Language Ability, Xuzhou 221009 China shalei, tingsong, lisujian, chbb, szf@pku.edu.cn Abstract large number of handcrafted features from the sentence, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). Neural network based approaches usually take Chinese SRL as sequence labeling task and use bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to solve the problem (Wang et al., 2015). In this paper, we capture the argument relationships for Chinese semantic role labeling task, and improve the task’s performance with the help of argument relationships. We split the relationship between two candidate arguments into two categories: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is more likely to belong to the same pr"
D16-1212,D15-1186,1,0.468647,"neering and Computer Science, Peking University Collaborative Innovation Center for Language Ability, Xuzhou 221009 China shalei, tingsong, lisujian, chbb, szf@pku.edu.cn Abstract large number of handcrafted features from the sentence, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). Neural network based approaches usually take Chinese SRL as sequence labeling task and use bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to solve the problem (Wang et al., 2015). In this paper, we capture the argument relationships for Chinese semantic role labeling task, and improve the task’s performance with the help of argument relationships. We split the relationship between two candidate arguments into two categories: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is more likely to belong to the same predicate; (2) Incompatible arguments: if one candidate argument belongs to a given predicate, then the other is less likely to belong to the same predicate. However, previous works did not explicitly model argument"
D16-1212,W03-1707,0,0.841245,"ure the two categories of argument relationships and test its performance on the Chinese Proposition Bank (CPB). The experiments show that argument relationships is effective in Chinese semantic role labeling task. 1 Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Previous works of Chinese SRL include featurebased approaches and neural network based approaches. Feature-based approaches often extract a However, both of the above two kinds of approaches identify each candidate argument separately without considering the relationship between arguments. We define two categories of argument relationships here: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is more likely to belong to the same predicate; (2) Incompatible arguments: if one candida"
D16-1212,J08-2004,0,0.877101,"when investing entrepreneurs” 2 Related Work Semantic Role Labeling (SRL) task was first proposed by Gildea and Jurafsky (2002). Previous approaches on Chinese SRL can be classified into two categories: (1) feature-based approaches (2) neural network based approaches. Among feature-based approaches, Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. Xue and Palmer (2003) proposed Chinese Proposition Bank (CPB), which leads to more complete and systematic research on Chinese SRL (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009). Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese"
D16-1212,D14-1041,0,0.334716,"assified into two categories: (1) feature-based approaches (2) neural network based approaches. Among feature-based approaches, Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. Xue and Palmer (2003) proposed Chinese Proposition Bank (CPB), which leads to more complete and systematic research on Chinese SRL (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009). Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese SRL. However, most of the aforementioned approaches did not take the compatible arguments and incompatible arguments into account. Inspired by Sha et al. (2016), our approach model the"
D16-1260,S13-2002,0,0.0328848,"Missing"
D16-1260,P14-2082,0,0.0158495,"Missing"
D16-1260,P08-1090,0,0.0168089,"Missing"
D16-1260,P07-2044,0,0.0828666,"Missing"
D16-1260,Q14-1022,0,0.0213226,"Missing"
D16-1260,D14-1165,0,0.0225719,"Missing"
D16-1260,D15-1038,0,0.0453683,"Missing"
D16-1260,P15-1009,0,0.0235187,"Missing"
D16-1260,D15-1082,0,0.00895825,"Missing"
D16-1260,N13-1008,0,0.0236909,"Missing"
D16-1260,D14-1167,0,0.0386669,"Missing"
D17-1020,N03-1020,0,0.594216,"Missing"
D17-1020,P11-1052,0,0.21185,"recall of ability to identify the better summary in a pair, and ROUGE-4 recall which has the highest precision of ability to identify the better summary in a pair (Owczarzak et al., 2012). 5.3 (Conroy et al., 2004) was the participant of the official DUC 2004 evaluation with the best evaluation score. It employs a Hidden Markov Model using topic signature feature and requires a linguistic preprocessing component. CLASSY 11 (Conroy et al., 2011) is the successor of CLASSY 04 and selects the non-redundant sentences using the non-negative matrix factorization algorithm. In the Submodular system (Lin and Bilmes, 2011), multi-document summarization is formulated as a submodular set function maximization problem. DPP (Lin and Bilmes, 2011) combines a sentence saliency model with a global diversity model encouraging non-overlapping information. ICSISumm (Gillick and Favre, 2009) aims at finding the globally optimal summary by formulating the summarization task in Integer Linear Programming. WFS-NMF (Wang et al., 2010) extends the non-negative matrix factorization algorithm and provides a good framework for weighting different terms and documents. GRASSHOPPER, DivRank and GCD are the three graph-based ranking"
D17-1020,W12-2601,0,0.0598294,"the transition matrix. To get the final multi-document summary, we use the same producingSummary function. 5 Task DUC 2003 Task 2 2 http://www-nlpir.nist.gov/projects/duc/intro.html 216 https://tartarus.org/martin/PorterStemmer/ 2012; Hong et al., 2014)3 . We compute ROUGE-2 recall with stemming and stopwords not removed, which provides the best agreement with manual evaluations. We also compute ROUGE-1 recall which has the highest recall of ability to identify the better summary in a pair, and ROUGE-4 recall which has the highest precision of ability to identify the better summary in a pair (Owczarzak et al., 2012). 5.3 (Conroy et al., 2004) was the participant of the official DUC 2004 evaluation with the best evaluation score. It employs a Hidden Markov Model using topic signature feature and requires a linguistic preprocessing component. CLASSY 11 (Conroy et al., 2011) is the successor of CLASSY 04 and selects the non-redundant sentences using the non-negative matrix factorization algorithm. In the Submodular system (Lin and Bilmes, 2011), multi-document summarization is formulated as a submodular set function maximization problem. DPP (Lin and Bilmes, 2011) combines a sentence saliency model with a g"
D17-1020,W09-1802,0,0.0356409,"evaluation with the best evaluation score. It employs a Hidden Markov Model using topic signature feature and requires a linguistic preprocessing component. CLASSY 11 (Conroy et al., 2011) is the successor of CLASSY 04 and selects the non-redundant sentences using the non-negative matrix factorization algorithm. In the Submodular system (Lin and Bilmes, 2011), multi-document summarization is formulated as a submodular set function maximization problem. DPP (Lin and Bilmes, 2011) combines a sentence saliency model with a global diversity model encouraging non-overlapping information. ICSISumm (Gillick and Favre, 2009) aims at finding the globally optimal summary by formulating the summarization task in Integer Linear Programming. WFS-NMF (Wang et al., 2010) extends the non-negative matrix factorization algorithm and provides a good framework for weighting different terms and documents. GRASSHOPPER, DivRank and GCD are the three graph-based ranking models mentioned in Section 2. APRW and AAPRW are our methods. APRW is the method of affinitypreserving random walk described in Section 4.2 and AAPRW is the method of adjustable affinitypreserving random walk described in Section 4.3. Experimental Results In the"
D17-1020,hong-etal-2014-repository,0,0.423113,"x= 215 µ(D−1 W) x + (1 − µ)y T kµ(D−1 W) x + (1 − µ)yk1 (10) ? ? (?) rameter tuning of our method. We preprocess the document data sets by removing stopwords from each sentence and stemming the remaining words using the Porter’s stemmer2 . Also, the sentences containing the said clause (if a said, says, told, tells word and quotation marks appear simultaneously) are filtered out. For evaluation, four reference summaries generated by human judges for each document cluster are provided by DUC as the ground truth. A brief summary over the evaluation datasets is shown in Table 5.1. According to (Hong et al., 2014), we adjust the length limit of summary in DUC 2004 from 665 bytes to 100 words as it provides the same setting for system evaluations. ? ? (? + ?) Figure 4.2: Sentence augmented graphs for summarization in two successive iterations. GA (K): augmented graph in the iteration K. Virtual summary V = {s1 , s3 , s5 }, which is constructed from x in the iteration (K−1) by producingSummary. D = diag([C(s1 ), Cmax , C(s3 ), Cmax , C(s5 )]). GA (K + 1): augmented graph in the iteration (K+1). V = {s1 , s4 , s5 }, which is constructed from x in the iteration K by producingSummary. D = diag([C(s1 ), Cmax"
D17-1020,N07-1013,0,0.0978706,"Missing"
D17-1189,P11-1055,0,0.950309,"ver, the automatic labeling inevitably accompanies with wrong labels because the relations of entity pairs might be missing from KBs or mislabeled. Multi-instances learning (MIL) is proposed by Riedel et al. (2010) to combat the noise. The method divides the training set into multiple bags of entity pairs (shown in Fig 1) and labels the bags with the relations of entity pairs in the KB. Each bag consists of sentences mentioning both head and tail entities. Much effort has been made in reducing the influence of noisy sentences within the bag, including methods based on at-least-one assumption (Hoffmann et al., 2011; Ritter et al., 2013; Zeng et al., 2015) and attention mechanisms over instances (Lin et al., 2016; Ji et al., 2017). However, the sentence level denoise methods can’t fully address the wrong labeling problem largely because they use a hard-label method in which the labels of entity pairs are immutable dur1790 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1790–1795 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing training, no matter whether they are correct or not. As shown in Fig 1, due to the abse"
D17-1189,C14-1220,0,0.187956,", hhn , tn i}. Each bag hhi , ti i contains sentences {x1 , x2 , · · · , xc } which mention both head entity hi and tail entity ti . The representation si of bag hhi , ti i is a weighted combination of related sentence vectors {x1 , x2 , · · · , xc } which are encoded by CNN. Finally, we use soft-label score function to correct wrong labels of bags of entity pairs while computing probabilities for each relation type. 2.1 Sentence Encoder We get the representation of certain sentence xi = {w1 , w2 , · · · , wm } by concatenating word embeddings {w1 , w2 , · · · , wm } and position embeddings (Zeng et al., 2014) {p1 , p2 , · · · , pm }, where wi ∈ Rd , wi ∈ Rdw , pi ∈ Rdp (d = dw + dp ). Convolution layer utilizes a sliding window of size l. We define qi ∈ Rl×d as the concatenation of words within the i-th window. qi = wi−l+1:i (1 ≤ i ≤ m + l − 1) (1) The convolution matrix is denoted by Wc ∈ Rdc ×(l×d) , where dc is the sentence embedding size. The i-th filter of the convolutional layer is computed as: fi = [Wc q + b]i (2) Afterwards, Piecewise max-pooling (Zeng et al., 2015) is used to divide convolutional filter fi into three parts fi1 , fi2 , fi3 by head and tail entities. For example, the sente"
D17-1189,P16-1200,0,0.505909,"rs might be missing from KBs or mislabeled. Multi-instances learning (MIL) is proposed by Riedel et al. (2010) to combat the noise. The method divides the training set into multiple bags of entity pairs (shown in Fig 1) and labels the bags with the relations of entity pairs in the KB. Each bag consists of sentences mentioning both head and tail entities. Much effort has been made in reducing the influence of noisy sentences within the bag, including methods based on at-least-one assumption (Hoffmann et al., 2011; Ritter et al., 2013; Zeng et al., 2015) and attention mechanisms over instances (Lin et al., 2016; Ji et al., 2017). However, the sentence level denoise methods can’t fully address the wrong labeling problem largely because they use a hard-label method in which the labels of entity pairs are immutable dur1790 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1790–1795 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing training, no matter whether they are correct or not. As shown in Fig 1, due to the absence of (Jan Eliasson1 , Sweden) from Nationality relation in the KB, the entity pair is mislabeled"
D17-1189,P09-1113,0,0.946423,"(:) 78(:&/ !@=( ""#(:) P+&:=&/ >? 7,& 89&#41BC:;""#,"" =(9=C(:C2&Q=( &$&G(& ? E? D &B.9$C.89C=(K.&.1$ 7,& 89&#41BC<;""#,"" 9R-J 9669= .89#C&:=C1.89$ 9.8:(KC&2(&:2 B$1#C.89 K1Q:.$,? 0$Q9 T12(.(A9 Figure 1: An example of soft-label correction on Nationality relation. We intend to use syntactic/ semantic information of correctly labeled entity pairs (blue) to correct the false positive and false negative instances (orange) during training. Introduction Relation Extraction (RE) aims to obtain relational facts from plain text. Traditional supervised RE systems suffer from lack of manually labeled data. Mintz et al. (2009) proposes distant supervision, which exploits relational facts in knowledge bases (KBs). Distant supervision automatically generates training examples by aligning entity mentions in plain text with those in KB and labeling entity pairs with their relations in KB. If there’s no relation link between certain entity pair in KB, it will be labeled as negative instance (NA). However, the automatic labeling inevitably accompanies with wrong labels because the relations of entity pairs might be missing from KBs or mislabeled. Multi-instances learning (MIL) is proposed by Riedel et al. (2010) to comba"
D17-1189,Q13-1030,0,0.009397,"ling inevitably accompanies with wrong labels because the relations of entity pairs might be missing from KBs or mislabeled. Multi-instances learning (MIL) is proposed by Riedel et al. (2010) to combat the noise. The method divides the training set into multiple bags of entity pairs (shown in Fig 1) and labels the bags with the relations of entity pairs in the KB. Each bag consists of sentences mentioning both head and tail entities. Much effort has been made in reducing the influence of noisy sentences within the bag, including methods based on at-least-one assumption (Hoffmann et al., 2011; Ritter et al., 2013; Zeng et al., 2015) and attention mechanisms over instances (Lin et al., 2016; Ji et al., 2017). However, the sentence level denoise methods can’t fully address the wrong labeling problem largely because they use a hard-label method in which the labels of entity pairs are immutable dur1790 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1790–1795 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing training, no matter whether they are correct or not. As shown in Fig 1, due to the absence of (Jan Eliasson1"
D17-1189,D12-1042,0,0.872878,"of the DS label. The score of the t-th relation type ot is calculated based on the trained relation matrice M and bias b: exp (Mst + b) ot = P (6) k exp (Msk + b) We use entity-pair level cross-entropy loss function using soft labels as gold labels while training: J(θ) = n X log p(ri |si ; θ) (7) i=1 In the testing stage, we still use the DS label li of certain entity pair hhi , ti i as the gold label: G(θ) = n X i=1 log p(li |si ; θ) (8) Figure 2: Precision/Recall curves of our model and previous state-of-the-art systems. Mintz (Mintz et al., 2009), MultiR (Hoffmann et al., 2011) and MIMLRE (Surdeanu et al., 2012) are feature-based models. ONE (Zeng et al., 2015) and ATT (Lin et al., 2016) are neural network models based on at-least-one assumption and selective attention, respectively. 3 Experiments In this section, we first introduce the dataset and evaluation metrics in our experiments. Then, we demonstrate the parameter settings in our experiments. Besides, we compare the performance of our method with state-of-the-art feature-based and neural network baselines. Case study shows our soft-label corrections are of high accuracy. 3.1 Dataset and Evaluation Metrics We evaluate our model on the benchmark"
D17-1189,P13-2117,0,0.0383401,"Missing"
D17-1189,D15-1203,0,\N,Missing
D18-1170,C14-1151,0,0.462113,"s “games/sports” in the gloss g1 can help to highlight the important words “football” in the context and ignore the words “know each other” which are useless for distinguishing the sense of word “play”. Meanwhile, the context can potentially help to stress on the words “games/sports” of the gloss g1 which is actually the correct sense for the target word. Introduction Word Sense Disambiguation (WSD) is a crucial task and long-standing problem in Natural Language Processing (NLP). Previous researches mainly exploit two kinds of resources. Knowledge-based methods (Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first to directly incorpora"
D18-1170,P15-2003,0,0.0161208,"Language Modeling (Ahn et al., 2016), and LSTMs (Xu et al., 2016; Yang and Mitchell, 2017) show that integrating knowledge and labeled data into a unified system can achieve better performance than other methods which only learn from large scale labeled data. Therefore, it’s a promising and 1 Play in the sentence means behave in a certain way. challenging study to integrate labeled data and lexical knowledge into a unified system. A few recent studies of WSD have exploited several ways to incorporate lexical resources into supervised systems. In the field of traditional feature-based methods (Chen et al., 2015; Rothe and Sch¨utze, 2015), they usually utilize knowledge (to train word sense embeddings) as features of the classifier like the support vector machine (SVM). In the field of neural-based methods, Raganato et al. (2017a) regard lexical resource LEX which is extracted from the WordNet as an auxiliary classification task, and propose a multi-task learning framework for WSD and LEX. Luo et al. (2018) integrate the context and glosses of the target word into a unified framework via a memory network. It encodes the context and glosses of the target word separately, and then models the semantic r"
D18-1170,S01-1001,0,0.642799,"Raganato et al. (2017b) map all the sense annotations in the training and test datasets to WordNet 3.0 via a semi-automatic method. Therefore, We choose WordNet 3.0 as the sense inventory for extracting the gloss. Experiments and Evaluation 4.1 Data SE2 SE3 SE7 SE13 SE15 SemCor Datasets Validation and Evaluation Datasets: We evaluate our model on several English all-words WSD datasets. For a fair comparison, we use the benchmark datasets proposed by Raganato et al. (2017b) which include five standard all-words fine-grained WSD datasets from the Senseval and SemEval competitions: • Senseval-2 (Edmonds and Cotton, 2001, SE2): It consists of 2282 sense annotations, including nouns, verbs, adverbs and adjectives. • Senseval-3 task 1 (Snyder and Palmer, 2004, SE3): It consists of 1850 sense annotations from three different domains (editorial, news story and fiction), including nouns, verbs, adverbs and adjectives. • SemEval-07 task 17 (Pradhan et al., 2007, SE7): It consists of 455 sense annotations of nouns and verbs, which is the smallest among the five datasets. Like Luo et al. (2018) and Raganato et al. (2017a), we choose SE7 as the validation set. • SemEval-13 task 12 (Navigli et al., 2013, SE13): It cons"
D18-1170,P17-1021,0,0.027302,"dNet. All studies listed above show that integrating lexical resources especially gloss into supervised systems of WSD can significantly improve the performance. Therefore, we follow this direction and seek a new way of better integrating gloss knowledge. Instead of building representations for context and gloss separately, we use the inner connection between the gloss and the context to promote the representation of each other. The interaction process can be modeled by a co-attention mechanism which has made great progress in the question answering task (Xiong et al., 2016; Seo et al., 2016; Hao et al., 2017; Lu et al., 2016). We are enlightened by this iterative procedure and introduce it into WSD. We then make some adaptations to the output of the original co-attention model to get the score of each word sense. 3 The Co-Attention Model for WSD In this section, we first give an overview of the CAN: co-attention neural network for WSD (Figure 1). And then, we extend it into a hierarchical architecture HCAN (Figure 2). 3.1 Overview The overall architecture of the proposed nonhierarchical co-attention model is shown in Figure 1. It consists of three parts: 1403 3. The output layer merges the output"
D18-1170,P16-1085,0,0.452294,"of word “play”. Meanwhile, the context can potentially help to stress on the words “games/sports” of the gloss g1 which is actually the correct sense for the target word. Introduction Word Sense Disambiguation (WSD) is a crucial task and long-standing problem in Natural Language Processing (NLP). Previous researches mainly exploit two kinds of resources. Knowledge-based methods (Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first to directly incorporate the gloss knowledge from WordNet into a unified neural network for WSD. This model separately builds the context representation and the gloss representation as distributed vectors and la"
D18-1170,W16-5307,0,0.22808,"Missing"
D18-1170,P18-1230,1,0.685697,"(Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first to directly incorporate the gloss knowledge from WordNet into a unified neural network for WSD. This model separately builds the context representation and the gloss representation as distributed vectors and later calculates their similarity in a memory network. However, we find that the learning of the representations of the context and gloss can contribute to each other. We use an example to illustrate our ideas. Table 1 shows that the red words are more important than the blue words when distinguishing the sense of the target word. In other words, we should pay more attention"
D18-1170,S15-2049,0,0.324327,"and Palmer, 2004, SE3): It consists of 1850 sense annotations from three different domains (editorial, news story and fiction), including nouns, verbs, adverbs and adjectives. • SemEval-07 task 17 (Pradhan et al., 2007, SE7): It consists of 455 sense annotations of nouns and verbs, which is the smallest among the five datasets. Like Luo et al. (2018) and Raganato et al. (2017a), we choose SE7 as the validation set. • SemEval-13 task 12 (Navigli et al., 2013, SE13): It consists of 1644 sense annotations from thirteen documents of various domains. SE13 contains nouns only. • SemEval-15 task 13 (Moro and Navigli, 2015, SE15): It’s the latest WSD dataset, which consists of 1022 sense annotations from three heterogeneous domains. Noun 1066 900 159 1644 531 87002 Verb 517 588 296 0 251 88334 Adj 445 350 0 0 160 31753 Adv 254 12 0 0 80 18947 Table 2: Statistics of the different parts of speech annotations in English all-words WSD train and test datasets. . 4.2 Settings We use the validation set (SE7) to find the optimal hyper parameters of our models: the word embedding size dw , the hidden state size ds of LSTM, the optimizer, etc. However, since there are no adverbs and adjectives in SE7, we randomly sample"
D18-1170,Q14-1019,0,0.708724,"shows that the words “games/sports” in the gloss g1 can help to highlight the important words “football” in the context and ignore the words “know each other” which are useless for distinguishing the sense of word “play”. Meanwhile, the context can potentially help to stress on the words “games/sports” of the gloss g1 which is actually the correct sense for the target word. Introduction Word Sense Disambiguation (WSD) is a crucial task and long-standing problem in Natural Language Processing (NLP). Previous researches mainly exploit two kinds of resources. Knowledge-based methods (Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first"
D18-1170,S13-2040,0,0.303724,"Senseval-2 (Edmonds and Cotton, 2001, SE2): It consists of 2282 sense annotations, including nouns, verbs, adverbs and adjectives. • Senseval-3 task 1 (Snyder and Palmer, 2004, SE3): It consists of 1850 sense annotations from three different domains (editorial, news story and fiction), including nouns, verbs, adverbs and adjectives. • SemEval-07 task 17 (Pradhan et al., 2007, SE7): It consists of 455 sense annotations of nouns and verbs, which is the smallest among the five datasets. Like Luo et al. (2018) and Raganato et al. (2017a), we choose SE7 as the validation set. • SemEval-13 task 12 (Navigli et al., 2013, SE13): It consists of 1644 sense annotations from thirteen documents of various domains. SE13 contains nouns only. • SemEval-15 task 13 (Moro and Navigli, 2015, SE15): It’s the latest WSD dataset, which consists of 1022 sense annotations from three heterogeneous domains. Noun 1066 900 159 1644 531 87002 Verb 517 588 296 0 251 88334 Adj 445 350 0 0 160 31753 Adv 254 12 0 0 80 18947 Table 2: Statistics of the different parts of speech annotations in English all-words WSD train and test datasets. . 4.2 Settings We use the validation set (SE7) to find the optimal hyper parameters of our models:"
D18-1170,S07-1016,0,0.165733,"on several English all-words WSD datasets. For a fair comparison, we use the benchmark datasets proposed by Raganato et al. (2017b) which include five standard all-words fine-grained WSD datasets from the Senseval and SemEval competitions: • Senseval-2 (Edmonds and Cotton, 2001, SE2): It consists of 2282 sense annotations, including nouns, verbs, adverbs and adjectives. • Senseval-3 task 1 (Snyder and Palmer, 2004, SE3): It consists of 1850 sense annotations from three different domains (editorial, news story and fiction), including nouns, verbs, adverbs and adjectives. • SemEval-07 task 17 (Pradhan et al., 2007, SE7): It consists of 455 sense annotations of nouns and verbs, which is the smallest among the five datasets. Like Luo et al. (2018) and Raganato et al. (2017a), we choose SE7 as the validation set. • SemEval-13 task 12 (Navigli et al., 2013, SE13): It consists of 1644 sense annotations from thirteen documents of various domains. SE13 contains nouns only. • SemEval-15 task 13 (Moro and Navigli, 2015, SE15): It’s the latest WSD dataset, which consists of 1022 sense annotations from three heterogeneous domains. Noun 1066 900 159 1644 531 87002 Verb 517 588 296 0 251 88334 Adj 445 350 0 0 160 3"
D18-1170,D17-1120,0,0.485246,"“games/sports” of the gloss g1 which is actually the correct sense for the target word. Introduction Word Sense Disambiguation (WSD) is a crucial task and long-standing problem in Natural Language Processing (NLP). Previous researches mainly exploit two kinds of resources. Knowledge-based methods (Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first to directly incorporate the gloss knowledge from WordNet into a unified neural network for WSD. This model separately builds the context representation and the gloss representation as distributed vectors and later calculates their similarity in a memory network. However, we find that the lea"
D18-1170,E17-1010,0,0.641246,"“games/sports” of the gloss g1 which is actually the correct sense for the target word. Introduction Word Sense Disambiguation (WSD) is a crucial task and long-standing problem in Natural Language Processing (NLP). Previous researches mainly exploit two kinds of resources. Knowledge-based methods (Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first to directly incorporate the gloss knowledge from WordNet into a unified neural network for WSD. This model separately builds the context representation and the gloss representation as distributed vectors and later calculates their similarity in a memory network. However, we find that the lea"
D18-1170,P15-1173,0,0.0774322,"Missing"
D18-1170,W04-0811,0,0.29984,"e, We choose WordNet 3.0 as the sense inventory for extracting the gloss. Experiments and Evaluation 4.1 Data SE2 SE3 SE7 SE13 SE15 SemCor Datasets Validation and Evaluation Datasets: We evaluate our model on several English all-words WSD datasets. For a fair comparison, we use the benchmark datasets proposed by Raganato et al. (2017b) which include five standard all-words fine-grained WSD datasets from the Senseval and SemEval competitions: • Senseval-2 (Edmonds and Cotton, 2001, SE2): It consists of 2282 sense annotations, including nouns, verbs, adverbs and adjectives. • Senseval-3 task 1 (Snyder and Palmer, 2004, SE3): It consists of 1850 sense annotations from three different domains (editorial, news story and fiction), including nouns, verbs, adverbs and adjectives. • SemEval-07 task 17 (Pradhan et al., 2007, SE7): It consists of 455 sense annotations of nouns and verbs, which is the smallest among the five datasets. Like Luo et al. (2018) and Raganato et al. (2017a), we choose SE7 as the validation set. • SemEval-13 task 12 (Navigli et al., 2013, SE13): It consists of 1644 sense annotations from thirteen documents of various domains. SE13 contains nouns only. • SemEval-15 task 13 (Moro and Navigli"
D18-1170,P17-1132,0,0.0278757,"ased methods have shown the effectiveness of textual knowledge such as gloss (Lesk, 1986; Basile et al., 2014) and the structural knowledge (Moro et al., 2014; Agirre et al., 2014) of the lexical databases. However, the prime shortcoming of knowledge-based methods is that they have worse performance than supervised methods, but they have wider coverage for the polysemous words, thanks to the use of large-scale knowledge resources (Navigli, 2009). There are many other tasks such as Chinese Word Segmentation (Zhang et al., 2018), Language Modeling (Ahn et al., 2016), and LSTMs (Xu et al., 2016; Yang and Mitchell, 2017) show that integrating knowledge and labeled data into a unified system can achieve better performance than other methods which only learn from large scale labeled data. Therefore, it’s a promising and 1 Play in the sentence means behave in a certain way. challenging study to integrate labeled data and lexical knowledge into a unified system. A few recent studies of WSD have exploited several ways to incorporate lexical resources into supervised systems. In the field of traditional feature-based methods (Chen et al., 2015; Rothe and Sch¨utze, 2015), they usually utilize knowledge (to train wor"
D18-1170,P10-4014,0,0.785886,"guishing the sense of word “play”. Meanwhile, the context can potentially help to stress on the words “games/sports” of the gloss g1 which is actually the correct sense for the target word. Introduction Word Sense Disambiguation (WSD) is a crucial task and long-standing problem in Natural Language Processing (NLP). Previous researches mainly exploit two kinds of resources. Knowledge-based methods (Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first to directly incorporate the gloss knowledge from WordNet into a unified neural network for WSD. This model separately builds the context representation and the gloss representation as d"
D18-1271,C16-1171,0,0.0159595,"nu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates,"
D18-1271,D14-1092,1,0.890876,"Missing"
D18-1271,D12-1025,1,0.84361,"s languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that our approach is unsuperv"
D18-1271,D14-1061,1,0.825032,"ast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that our approach is unsupervised, effective, in"
D18-1271,N07-2008,0,0.0207658,"0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and"
D18-1271,P98-1069,0,0.239043,"slation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel co"
D18-1271,D16-1075,1,0.854091,"nowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge. Based on the motivations, we employ a promising text stream representation – Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams. With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment. 2496 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2"
D18-1271,C16-1309,1,0.941152,"nowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge. Based on the motivations, we employ a promising text stream representation – Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams. With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment. 2496 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2"
D18-1271,N13-1056,0,0.0349868,"Missing"
D18-1271,W09-3107,1,0.81454,"ed alignment, exploring a paradigm for language knowledge acquisition. • We propose a network decipherment approach for text stream alignment, which can work in both low and rich resource settings and outperform previous approaches. • We release our data (annotations) and systems to guarantee the reproducibility and help future work improve on this task. 2 Burst Information Network A Burst Information Network (BINet) is a graphbased text stream representation and has proven effective for multiple text stream mining tasks (Ge et al., 2016a,b,c). In contrast to many information networks (e.g., (Ji, 2009; Li et al., 2014)), BINets are specially for text streams. They focus on the burst information units which are usually related to important events or trending topics in text streams and illustrate their connections. A BINet is originally defined as G = hV, E, ωi in (Ge et al., 2016a). Each node v ∈ V is a burst element defined as a burst word1 during one of its burst periods hw, Pi where w denotes a word and P denotes one consecutive burst period of w, as Figure 2 shows. Each edge  ∈ E indicates the connection between two burst elements with the weight ω which is defined as the number of doc"
D18-1271,D15-1015,0,0.0135832,"10; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods,"
D18-1271,P06-1103,0,0.0425456,"i and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast"
D18-1271,W02-0902,0,0.0611221,"tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are"
D18-1271,W11-2125,0,0.0196081,"guage knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Ro"
D18-1271,N16-1132,0,0.031503,"Missing"
D18-1271,P08-1088,0,0.0159272,"d (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of t"
D18-1271,D14-1198,1,0.826094,"nt, exploring a paradigm for language knowledge acquisition. • We propose a network decipherment approach for text stream alignment, which can work in both low and rich resource settings and outperform previous approaches. • We release our data (annotations) and systems to guarantee the reproducibility and help future work improve on this task. 2 Burst Information Network A Burst Information Network (BINet) is a graphbased text stream representation and has proven effective for multiple text stream mining tasks (Ge et al., 2016a,b,c). In contrast to many information networks (e.g., (Ji, 2009; Li et al., 2014)), BINets are specially for text streams. They focus on the burst information units which are usually related to important events or trending topics in text streams and illustrate their connections. A BINet is originally defined as G = hV, E, ωi in (Ge et al., 2016a). Each node v ∈ V is a burst element defined as a burst word1 during one of its burst periods hw, Pi where w denotes a word and P denotes one consecutive burst period of w, as Figure 2 shows. Each edge  ∈ E indicates the connection between two burst elements with the weight ω which is defined as the number of documents where these"
D18-1271,W11-2206,1,0.807265,"y for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and"
D18-1271,D11-1006,0,0.0252557,", 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By"
D18-1271,J05-4003,0,0.0814633,"ments with a low score (&lt; 0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016;"
D18-1271,D14-1162,0,0.0810082,"ge. Given that our approach is unsupervised, effective, intuitive, interpretable, and easily implementable, it is promising to use it as a framework for never-ending language knowledge mining from big data, which might benefit NLP applications such as machine translation and cross-lingual information access. For future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach’s effectiveness for more languages and domains (e.g., social stream VS news stream); 2) attempt to use word embedding (e.g., word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications. Acknowledgments We thank the anonymous reviewers for their valuable comments. We also want to thank Xiaoman Pan, Dr. Taylor Cassidy, Dr. Clare R. Voss, Prof. Jiawei Han, Prof. Sujian Li and Prof. Yu Hong for their helpful comments and discussions. This work is supported by NSFC project 61772040 and 61751201. Heng Ji’s work has been suppo"
D18-1271,E09-1091,0,0.0212751,"lignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the"
D18-1271,C10-1124,0,0.0211684,"eans it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining"
D18-1271,P15-2118,0,0.0147857,"s and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narro"
D18-1271,N18-1202,0,0.0218069,"pervised, effective, intuitive, interpretable, and easily implementable, it is promising to use it as a framework for never-ending language knowledge mining from big data, which might benefit NLP applications such as machine translation and cross-lingual information access. For future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach’s effectiveness for more languages and domains (e.g., social stream VS news stream); 2) attempt to use word embedding (e.g., word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications. Acknowledgments We thank the anonymous reviewers for their valuable comments. We also want to thank Xiaoman Pan, Dr. Taylor Cassidy, Dr. Clare R. Voss, Prof. Jiawei Han, Prof. Sujian Li and Prof. Yu Hong for their helpful comments and discussions. This work is supported by NSFC project 61772040 and 61751201. Heng Ji’s work has been supported by the U.S. DARPA AIDA Pro"
D18-1271,P99-1067,0,0.288414,"ing and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. Howev"
D18-1271,N04-1033,0,0.054922,"37 documents. We removed stopwords, conducted lemmatization and name tagging for the English stream, and did word segmentation and name tagging for the Chinese stream using the Stanford CoreNLP toolkit (Manning et al., 2014). 4 Due to the upper bound of Conf (Gc , Ge ), the algorithm must terminate after several iterations. We detected bursts and constructed the BINets5 for the Chinese and English stream based on (Ge et al., 2016a). The constructed Chinese BINet has 7,360 nodes and 33,892 edges while the English one has 8,852 nodes and 85,125 edges. Our seed bi-lingual lexicon is released by (Zens and Ney, 2004), containing 81,990 Chinese word entries, each of which has an English translation. Among the 7,360 nodes in the Chinese BINet, 2,281 nodes need to be deciphered since their words are not in the bi-lingual lexicon. 4.1.2 Evaluation Setting We evaluate our approach in an end-to-end fashion. For a node c in the Chinese BINet, we choose the node e∗ which has the highest score as c’s counterpart in the English BINet: e∗ = arg max Score(c, e) e∈Cand(c) We rank the aligned node pairs by the score and manually evaluate the quality of the top K pairs. A pair hc,ei is annotated as correct if e is a cor"
D18-1271,P11-1002,0,0.0330427,"n (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that ou"
D18-1271,P10-1115,0,0.0274153,"rger than that used in our experiment and they are endlessly updated. 2503 10 The alignments with a low score (&lt; 0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Ki"
D18-1271,W02-2026,0,0.132146,"Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for genera"
D18-1271,P17-1179,0,0.0253021,"Missing"
D18-1271,D17-1207,0,0.0444246,"Missing"
D18-1271,C04-1089,0,0.0807467,"ingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpor"
D18-1271,N10-1063,0,0.0216071,"endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et a"
D18-1271,P06-1010,0,0.229533,"Chinese BINet, its candidate nodes in the English BINet can be derived as: Cand(c) = {e|P(e) ∩ P(c) 6= ∅} where e ∈ Ve , and P(c) and P(e) are the burst periods of c and e respectively. 3.3 Candidate Verification For the candidate list for c (i.e., Cand(c)), we need to verify each node e ∈ Cand(c) and choose the most probable one as c’s counterpart. Formally, we define Score(c, e) as the credibility score of e being the correct counterpart of c and propose the following novel clues for verification. Pronunciation Inspired by previous work on name translation mining (e.g., (Schafer III, 2006; Sproat et al., 2006; Ji, 2009)), for a node e ∈ Cand(c), if its pronunciation is similar to c, then e is likely to be the translation of c. For a Chinese node c and an English node e, we define Sp as its scaled pronunciation score to measure their pronunciation similarity whose range is [0, 1]: 1 Sp ∈ [0, 1] ∝ LD where LD is the normalized (by e’s length) Levenshtein edit distance between c’s pinyin3 string and e’s word string. Translation For a node e ∈ Cand(c), it is possible that e’s word exists or partially exists in the bi-lingual lexicon. We can exploit the translation clue to verify if e is c’s counterpar"
D18-1271,D12-1003,0,0.0225396,"De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across la"
D19-1336,N19-1172,0,0.538622,"t is a normal recurrent neural language model which takes the target pun word as input. CLM (Mou et al., 2015): It is a constrained language model which guarantees that a pre-given word will appear in the generated sequence. CLM+JD (Yu et al., 2018): It is a state-of-theart model for pun generation which extends a constrained language model by jointly decoding conditioned on two word senses. 3.4 Evaluation Metrics Automatic evaluation: We use two metrics to automatically evaluate the creativeness of the generated puns in terms of unusualness and diversity. Following Pauls and Klein (2012) and He et al. (2019)4 , the unusualness is measured by subtracting the log-probability of training sentences from the log-probability of generated pun sentences. Following Yu et al. (2018), the diversity is measured by the ratio of distinct unigrams (Dist-1) and bigrams (Dist-2) in generated sentences. Human evaluation: Three annotators score the randomly sampled 100 outputs of different systems from 1 to 5 in terms of three criteria. Ambiguity evaluates how likely the sentence is a pun. Fluency measures whether the sentence is fluent. Overall is a comprehensive metric. 3390 Model Pun-GAN vs CLM+JD Pun-GAN vs Hum"
D19-1336,W09-2004,0,0.092266,"Missing"
D19-1336,W16-5307,0,0.055248,"Missing"
D19-1336,D18-1170,1,0.925653,"abulary at t-th timestep is calculated as Gθ (xt |x&lt;t ) = f (W h1t + b) + f (W h2t + b) (1) where h1t (h2t ) is the hidden state of t-th step when taking s1 (s2 ) as input, f is the softmax function, and x&lt;t is the preceding t − 1 words. Therefore, the generation probability of the whole sentence x is formulated as Gθ (x|s1 , s2 ) = Y Gθ (xt |x&lt;t ) (2) t To give a warm start to the generator, we pretrain it using the same general training corpus in the original paper. 2.1.2 Discriminator The discriminator is extended from the word sense disambiguation models (K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018a,b). Assuming the pun word w in sentence x has k word senses, we add a new “generated” class. Then, the discriminator is designed to produce a probability distribution over k + 1 classes, which is computed as Dφ (y|x) = softmax(Uw c + b0 ) (3) where c is the context vector from a bi-directional LSTM when taking x as input, Uw is a wordspecific parameter and y is the target label.  Therefore, Dφ y = i|x, i ∈ {1, ..., k} denotes the probability that it belongs to the real i-th word sense, while Dφ (y = k + 1|x) denotes the probability that it is produced by a pun generator. 2.2 Training We fol"
D19-1336,P18-1230,1,0.925353,"abulary at t-th timestep is calculated as Gθ (xt |x&lt;t ) = f (W h1t + b) + f (W h2t + b) (1) where h1t (h2t ) is the hidden state of t-th step when taking s1 (s2 ) as input, f is the softmax function, and x&lt;t is the preceding t − 1 words. Therefore, the generation probability of the whole sentence x is formulated as Gθ (x|s1 , s2 ) = Y Gθ (xt |x&lt;t ) (2) t To give a warm start to the generator, we pretrain it using the same general training corpus in the original paper. 2.1.2 Discriminator The discriminator is extended from the word sense disambiguation models (K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018a,b). Assuming the pun word w in sentence x has k word senses, we add a new “generated” class. Then, the discriminator is designed to produce a probability distribution over k + 1 classes, which is computed as Dφ (y|x) = softmax(Uw c + b0 ) (3) where c is the context vector from a bi-directional LSTM when taking x as input, Uw is a wordspecific parameter and y is the target label.  Therefore, Dφ y = i|x, i ∈ {1, ..., k} denotes the probability that it belongs to the real i-th word sense, while Dφ (y = k + 1|x) denotes the probability that it is produced by a pun generator. 2.2 Training We fol"
D19-1336,P15-1070,0,0.0609047,"Missing"
D19-1336,S17-2005,0,0.244865,"s – English Wikipedia to train Pun-GAN. For generator, we first tag each word in the English Wikipedia corpus with one word sense using an unsupervised WSD tool2 . Then we use the 2,595K tagged corpus to pre-train our generator. For discriminator, we use several types of data for training: 1) SemCor (Luo et al., 2018a,b) which is a manually annotated corpus for WSD, consisting of 226K sense annotations3 (first part in Eq.4); 2) Wikipedia corpus as unlabeled corpus (second part in Eq.4); 3) Generated puns (third part in Eq.4). Evaluation Dataset: We use the pun dataset from SemEval 2017 task7 (Miller et al., 2017) for evaluation. The dataset consists of 1274 humanwritten puns where target pun words are annotated with two word senses. During testing, we extract the word sense pair as the input of our model. 3.2 Experimental Setting The generator is the same as Yu et al. (2018). The discriminator is a single-layer bi-directional LSTM with hidden size 128. We randomly initialize word embeddings with the dimension size of 300. The sample size K is set as 32. Batch size is 32 and learning rate is 0.001. The optimization algorithm is SGD. Before adversarial training, we pre-train the generator for 5 epochs a"
D19-1336,P12-1101,0,0.0181233,"M (Mikolov et al., 2010): It is a normal recurrent neural language model which takes the target pun word as input. CLM (Mou et al., 2015): It is a constrained language model which guarantees that a pre-given word will appear in the generated sequence. CLM+JD (Yu et al., 2018): It is a state-of-theart model for pun generation which extends a constrained language model by jointly decoding conditioned on two word senses. 3.4 Evaluation Metrics Automatic evaluation: We use two metrics to automatically evaluate the creativeness of the generated puns in terms of unusualness and diversity. Following Pauls and Klein (2012) and He et al. (2019)4 , the unusualness is measured by subtracting the log-probability of training sentences from the log-probability of generated pun sentences. Following Yu et al. (2018), the diversity is measured by the ratio of distinct unigrams (Dist-1) and bigrams (Dist-2) in generated sentences. Human evaluation: Three annotators score the randomly sampled 100 outputs of different systems from 1 to 5 in terms of three criteria. Ambiguity evaluates how likely the sentence is a pun. Fluency measures whether the sentence is fluent. Overall is a comprehensive metric. 3390 Model Pun-GAN vs"
D19-1336,P13-2041,0,0.0766619,"Missing"
D19-1336,D17-1120,0,0.046399,"Missing"
D19-1336,P13-2044,0,0.365409,"Missing"
D19-1336,P18-1153,0,0.436043,"in ambiguity and diversity. 2 Model The sketch of the proposed Pun-GAN is depicted in Figure 1. It consists of a pun generator Gθ and a word sense discriminator Dφ . The following sections will elaborate on the architecture of Pun-GAN and its training algorithm. 2.1 Model Structure 2.1.1 Generator Given two senses (s1 , s2 ) of a target word w, the generator Gθ aims to output a sentence x which not only contains the target word w but also express the two corresponding meanings. Considering the simplicity of the model and the ease of training, we adopt the neural constrained language model of Yu et al. (2018) as the generator. Due to space constraints, we strongly recommend that readers refer to the original paper for details. Compared with traditional neural language model, the main difference is that the generated words at each timestep should have the maximum sum of two probabilities which are calculated with s1 and s2 as input, respectively. Formally, the generation probability over the entire vocabulary at t-th timestep is calculated as Gθ (xt |x&lt;t ) = f (W h1t + b) + f (W h2t + b) (1) where h1t (h2t ) is the hidden state of t-th step when taking s1 (s2 ) as input, f is the softmax function,"
I05-4001,O02-2006,0,0.0193125,"experts. Compared with free text, there are more canonical and NLP technologies can be used comparatively easily to extract knowledge from it. Since the knowledge in encyclopedia is more systematic, we can easily construct the basic frame of domain knowledge. So we will use NLP technology and machine learning method to construct the kernel of domain knowledge based on the analysis of the encyclopedia. Then based on the kernel of domain knowledge base, we can extract domain knowledge from other text resources. There exist some researches on extracting knowledge from the encyclopedia [11] [12] [13] [14]. These researches use the encyclopedias as the only source to acquire knowledge. However, with the high-speed improvement in each 2 language processing technologies could be used to extract domain knowledge reliably. domain, there is severe knowledge lag in encyclopedias. So it is inadequate to use encyclopedias as the only source for knowledge acquisition. We need to learn more domain knowledge from other text resource besides encyclopedias. With the surge of Internet, information in it is increasing exponentially. Abundant knowledge lies in this huge Web resource. If we can extract kno"
I05-4001,A94-1014,0,0.0456131,"many experts. Compared with free text, there are more canonical and NLP technologies can be used comparatively easily to extract knowledge from it. Since the knowledge in encyclopedia is more systematic, we can easily construct the basic frame of domain knowledge. So we will use NLP technology and machine learning method to construct the kernel of domain knowledge based on the analysis of the encyclopedia. Then based on the kernel of domain knowledge base, we can extract domain knowledge from other text resources. There exist some researches on extracting knowledge from the encyclopedia [11] [12] [13] [14]. These researches use the encyclopedias as the only source to acquire knowledge. However, with the high-speed improvement in each 2 language processing technologies could be used to extract domain knowledge reliably. domain, there is severe knowledge lag in encyclopedias. So it is inadequate to use encyclopedias as the only source for knowledge acquisition. We need to learn more domain knowledge from other text resource besides encyclopedias. With the surge of Internet, information in it is increasing exponentially. Abundant knowledge lies in this huge Web resource. If we can extrac"
I05-4001,E95-1027,0,\N,Missing
L18-1079,reschke-etal-2014-event,0,0.0969942,"n 2011 Tohoku earthquake and 869 Sanriku earthquake is expressed by the following text in 2011 Tohoku earthquake: infobox generation problem can be divided into three subtasks: event classification, event schema extraction and slot filling. As mentioned before, EventWiki can provide rich information for event classification and event schema extraction. Moreover, it is extremely useful for training a slot filling model for event extraction. Slot and value pairs in the infobox (intra-event information) in EventWiki can be used as weak (distant) supervision for training a slot filling model, as (Reschke et al., 2014) did. For example, for the slot value pair “magnitude: 9.0” in 2011 Tohoku earthquake, we first find out the sentences which “9.0” appears in. The context information of “9.0” can be used as features and “magnitude” is used as the label of “9.0” for training a slot filling model. 2.3.2. Event-event relation extraction and inference As slot filling for event extraction, we can also use interevent information in EventWiki to train an event-event relation extraction model using distant supervision strategy. For an event relation triple, we can find out the sentences that mention both events in th"
L18-1566,P05-1045,0,0.0869125,"et 2 developed by (Riedel et al., 2010)3 . The NYT corpus contains about 1.8 million news articles. 1 2 3 https://https://www.mturk.com/ http://iesl.cs.umass.edu/riedel/ecml/ Apart from the NYT dataset released by (Riedel et al., 2010), Dataset DSTrainSmall DSTrainLarge DSTest HoffmannTest OurTest #sentences 126,184 522,611 172,448 881 2,040 #pairs 67,946 279,786 96,678 565 1,666 #facts 4,700 18,252 1,950 259 547 Table 1: Statistics about the datasets. When constructing the dataset, named entity mentions were first extracted from the text of NYT articles by using Stanford Named Entity Tagger (Finkel et al., 2005). Then, the named entity mentions were linked to the entities in Freebase by using exact string matching. If a sentence mentions two entities that have a relation in Freebase, then a corresponding instance will be generated and labeled as the relation type. Otherwise, an instance with a label NA which indicates that there is no relation between the entity pair, will be generated. Riedel et al. (2010) mainly focus on the relations related to “people”, “business”, “person” and “location”. There are 53 relation labels including the special label NA in the corpus. The Freebase relations were divid"
L18-1566,P11-1055,0,0.189001,"lation. As shown in Figure 1, the second sentence is a false positive instance. (ii) Multiple labels instances. An entity pair may preserve multiple relation types in a KB. For example, (Bill Gates, founderOf, Microsoft) and (Bill Gates, ceoOf, Microsoft) are clearly true. To deal with the two major issues, multi-instance multilabel learning (MIML) was proposed for RE by relaxing the distant supervision assumption and making the at-least-one assumption: if two entities preserve a relation in a KB, at least one sentence that mentions the entity pair expresses the relation (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). The previous work of MIML-based approaches can be mainly categorized into two folds: (i) feature-based approaches (Hoffmann et al., 2011; Surdeanu et al., 2012) and (ii) neural network-based approaches (Zeng et al., 2015; Lin et al., 2016). However, when we carefully examine the experimental settings of the previous MIML-based work (Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016), we find the following issues which may affect the existing conclusions: (i) When the model comparison experiments were conducted by Zeng et al. (2015; Lin et al. (2016), the comp"
L18-1566,P16-1200,0,0.111484,"Missing"
L18-1566,P09-1113,0,0.708093,"Missing"
L18-1566,D12-1042,0,0.553217,"ure 1, the second sentence is a false positive instance. (ii) Multiple labels instances. An entity pair may preserve multiple relation types in a KB. For example, (Bill Gates, founderOf, Microsoft) and (Bill Gates, ceoOf, Microsoft) are clearly true. To deal with the two major issues, multi-instance multilabel learning (MIML) was proposed for RE by relaxing the distant supervision assumption and making the at-least-one assumption: if two entities preserve a relation in a KB, at least one sentence that mentions the entity pair expresses the relation (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). The previous work of MIML-based approaches can be mainly categorized into two folds: (i) feature-based approaches (Hoffmann et al., 2011; Surdeanu et al., 2012) and (ii) neural network-based approaches (Zeng et al., 2015; Lin et al., 2016). However, when we carefully examine the experimental settings of the previous MIML-based work (Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016), we find the following issues which may affect the existing conclusions: (i) When the model comparison experiments were conducted by Zeng et al. (2015; Lin et al. (2016), the compared models were trained"
L18-1566,D15-1203,0,0.270759,"soft) are clearly true. To deal with the two major issues, multi-instance multilabel learning (MIML) was proposed for RE by relaxing the distant supervision assumption and making the at-least-one assumption: if two entities preserve a relation in a KB, at least one sentence that mentions the entity pair expresses the relation (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). The previous work of MIML-based approaches can be mainly categorized into two folds: (i) feature-based approaches (Hoffmann et al., 2011; Surdeanu et al., 2012) and (ii) neural network-based approaches (Zeng et al., 2015; Lin et al., 2016). However, when we carefully examine the experimental settings of the previous MIML-based work (Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016), we find the following issues which may affect the existing conclusions: (i) When the model comparison experiments were conducted by Zeng et al. (2015; Lin et al. (2016), the compared models were trained on the datasets with different size. Particularly, the neural network-based models (Lin et al., 2016) actually used a large training dataset containing 522, 611 sentences, while feature-based models (Hoffmann et al., 2011"
N16-1049,P98-1013,0,0.153293,"es has different slots. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atom"
N16-1049,D13-1178,0,0.129215,"t al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and get good results. 6 Conclusion This paper presented a joint entity-driven model to induct e"
N16-1049,P04-1056,0,0.16463,"rates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jen"
N16-1049,P08-1090,0,0.311784,"traction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and get good results. 6 Conclusion"
N16-1049,P09-1068,0,0.11024,"rlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and get good results. 6 Conclusion This paper presented a joint"
N16-1049,P11-1098,0,0.513984,"vent. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learns syntactic patterns as fillers. However, the pipelined approach suffers from the error propagation problem, which means the errors in the template clustering can lead to more errors in the slot clustering. This paper proposes an entity-driven model which jointly learns templates and slots for event schema induction. The main contribution of this paper are as follows: • To better model the inner connect"
N16-1049,D13-1185,0,0.722137,"of bombing event in MUC-4, it has a bombing template and four main slots Introduction Event schema is a high-level representation of a bunch of similar events. It is very useful for the traditional information extraction (IE)(Sagayam et al., 2012) task. An example of event schema is shown in Table 1. Given the bombing schema, we only need to find proper words to fill the slots when extracting a bombing event. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events,"
N16-1049,P11-1054,0,0.0179381,"ates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chamber"
N16-1049,N13-1104,0,0.728221,"in MUC-4, it has a bombing template and four main slots Introduction Event schema is a high-level representation of a bunch of similar events. It is very useful for the traditional information extraction (IE)(Sagayam et al., 2012) task. An example of event schema is shown in Table 1. Given the bombing schema, we only need to find proper words to fill the slots when extracting a bombing event. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learn"
N16-1049,P03-1028,0,0.0639924,"ts. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can"
N16-1049,J93-3001,0,0.333736,"t templates in the two methods while only 108 entities has different slots. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information"
N16-1049,D11-1142,0,0.015167,"and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary"
N16-1049,P06-2027,0,0.810867,"the slots when extracting a bombing event. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learns syntactic patterns as fillers. However, the pipelined approach suffers from the error propagation problem, which means the errors in the template clustering can lead to more errors in the slot clustering. This paper proposes an entity-driven model which jointly learns templates and slots for event schema induction. The main contribution of this paper are as"
N16-1049,C92-2082,0,0.0452342,"chmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of s"
N16-1049,P10-1029,0,0.0133174,"ent-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captur"
N16-1049,P08-1030,0,0.0579146,"strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal"
N16-1049,W10-0905,0,0.0276645,"animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and"
N16-1049,P15-1019,0,0.389029,"Missing"
N16-1049,D07-1075,0,0.0822568,"and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates w"
N16-1049,D09-1016,0,0.0220445,"nstraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with un"
N16-1049,M92-1008,0,0.221381,"assigned different templates in the two methods while only 108 entities has different slots. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other"
N16-1049,W98-1106,0,0.159823,"t filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432"
N16-1049,P06-2094,0,0.3539,"ng a bombing event. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learns syntactic patterns as fillers. However, the pipelined approach suffers from the error propagation problem, which means the errors in the template clustering can lead to more errors in the slot clustering. This paper proposes an entity-driven model which jointly learns templates and slots for event schema induction. The main contribution of this paper are as follows: • To"
N16-1049,N06-1039,0,0.06114,"Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. Ho"
N16-1049,P03-1029,0,0.0731297,"e full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekin"
N16-1049,H91-1059,0,0.102863,"slot constraint. Generally, we have the following optimization objective: ( ) tr XTT JJ T XT ) ε3 (XT , XS ) = ( T T tr XS JJ XS (8) The whole joint model is shown in Eq 9. The detailed derivation1 is shown in the supplement file. Bombing XT , XS = argmax ε1 (XT ) + ε2 (XS ) + ε3 (XT , XS ) Perpetrator XT ,XS s.t. XT ∈ {0, 1}|E|×|T |XT 1|T |= 1|E| XS ∈ {0, 1}|E|×|S |XS 1|S |= 1|E| Victim The police chief El salvador Students The guerrillas The Peruvian embassy The drag mafia The diplomat Drug traffickers soldiers The Atlacatl battalion (9) 4 Experiment 4.1 Dataset In this paper, we use MUC-4(Sundheim, 1991) as our dataset, which is the same as previous works (Chambers and Jurafsky, 2011; Chambers, 2013). MUC-4 corpus contains 1300 documents in the training set, 200 in development set (TS1, TS2) and 200 in testing set (TS3, TS4) about Latin American news of terrorism events. We ran several times on the 1500 documents (training/dev set) and choose the best |T |and |S |as |T |= 6, |S |= 4. Then we report the performance of test set. For each document, it provides a series of hand-constructed event schemas, which are called gold schemas. With these gold schemas we can evaluate our results. The MUC-4"
N16-1049,C00-2136,0,0.180339,"ore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007;"
N16-1049,N07-4013,0,0.0431629,"garber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011"
P00-1060,H92-1026,0,0.0569682,"Missing"
P00-1060,J93-1002,0,0.0189798,"Missing"
P00-1060,P97-1003,0,0.122012,"evaluation modelling for statistical parsing. The basic idea is that we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction. Our experiment quantitatively analyzes several feature types’ power for syntactic structure prediction and draws a series of interesting conclusions. 1 Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1991] [Magerman, 1992] [Magerman, 1995] [Eisner, 1996]. How to evaluate the different feature types’ effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types’ or feature types combination’s predictive power for syntactic structure. In the following, Section 2 describes the probabilistic evaluation model for sy"
P00-1060,C96-1058,0,0.122575,"at we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction. Our experiment quantitatively analyzes several feature types’ power for syntactic structure prediction and draws a series of interesting conclusions. 1 Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1991] [Magerman, 1992] [Magerman, 1995] [Eisner, 1996]. How to evaluate the different feature types’ effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types’ or feature types combination’s predictive power for syntactic structure. In the following, Section 2 describes the probabilistic evaluation model for syntactic trees; Section 3 proposes an information-theory-based feat"
P00-1060,H92-1025,0,0.0467581,"Missing"
P00-1060,P95-1037,0,0.239856,"basic idea is that we use entropy and conditional entropy to measure whether a feature type grasps some of the information for syntactic structure prediction. Our experiment quantitatively analyzes several feature types’ power for syntactic structure prediction and draws a series of interesting conclusions. 1 Introduction In the field of statistical parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1991] [Magerman, 1992] [Magerman, 1995] [Eisner, 1996]. How to evaluate the different feature types’ effects for syntactic parsing? The paper proposes an information-theory-based feature types analysis model, which uses the measures of predictive information quantity, predictive information gain, predictive information redundancy and predictive information summation to quantitatively analyse the different contextual feature types’ or feature types combination’s predictive power for syntactic structure. In the following, Section 2 describes the probabilistic evaluation model for syntactic trees; Section 3 proposes an information-th"
P00-1060,J93-2004,0,0.0267295,"Missing"
P00-1060,W99-0618,1,\N,Missing
P00-1060,P96-1025,0,\N,Missing
P00-1060,P93-1005,0,\N,Missing
P00-1060,1991.iwpt-1.22,0,\N,Missing
P00-1060,E91-1004,0,\N,Missing
P09-2064,J02-3001,0,0.0202575,"ai , aj , r) · w} ST H (ai , aj , r) = P where ψ is the feature map and w is the parameter vector to learn. Note that the model predicts the rank of ai and aj through calculating ST H (ai , aj , r) rather than ST H (aj , ai , r), where ai precedes aj . In other words, the position information is implicitly encoded in the model rather than explicitly as a feature. The system extracts a number of features to represent various aspects of the syntactic structure of a pair of arguments. All features are listed in Table 1. The Path features are designed as a sequential collection of phrase tags by (Gildea and Jurafsky, 2002). We also use Single Character Category Path, in which each phrase tag is clustered to a category defined by its first character (Pradhan et al., 2005). To characterize the relation between two constituents, we combine features of the two individual arguments as new features (i.e. conjunction features). For example, if the category of the first argument is NP and the category of the second is S, then the conjunction of category feature is NP-S. Prediction Method Assigning different labels to possible rank between two arguments ai and aj , such as labeling ai  aj as ””, identification of them"
P09-2064,W05-0625,0,0.0579635,"Missing"
P09-2064,J08-2002,0,0.0380799,"Missing"
P09-2064,J05-1004,0,0.0404489,"inguistic theories but the lowest in some other theories (Levin and Hovav, 2005). In this paper, the proto-role theory (Dowty, 1991) is taken into account to rank PropBank arguments, partially resolving the two problems above. There are three key points in our solution. First, the rank of Arg0 is the highest. The Agent is almost without exception the highest role in proposed hierarchies. Though PropBank defines semantic roles on a verb by verb basis, for a particular verb, Arg0 is generally the argument exhibiting features of a prototypical Agent while Arg1 is a prototypical Patient or Theme (Palmer et al., 2005). As being the proto-Agent, the rank of Arg0 is higher than other numbered arguments. Second, the rank of the Arg1 is second highest or lowest. Both hierarchy of Arg1 are tested and discussed in section 4. Third, we do not rank other arguments. Two sets of roles closely correspond to numbered arguments: 1) referenced arguments and 2) continuation arguments. To adapt the relation to help these two kinds of arguments, the equivalence relation is divided into several sub-categories. In summary, relations of two arguments ai and aj in this paper include: 1) ai  aj : ai is higher than aj , 2) ai ≺"
P09-2064,J08-2005,0,0.0178431,"ents as new features (i.e. conjunction features). For example, if the category of the first argument is NP and the category of the second is S, then the conjunction of category feature is NP-S. Prediction Method Assigning different labels to possible rank between two arguments ai and aj , such as labeling ai  aj as ””, identification of thematic rank can be formulated as a classification problem. De3 Re-ranking Models for SRC Toutanova et al. (2008) empirically showed that global information is important for SRL and that 254 4 structured solutions outperform local semantic role classifiers. Punyakanok et al. (2008) raised an inference procedure with integer linear programming model, which also showed promising results. Identifying relations among arguments can provide structural information for SRL. Take the sentence ”[Arg0 She] [V addressed] [Arg1 her husband] [ArgM −M N R with her favorite nickname].” for example, if the thematic rank of she and her husband is predicted as that she is higher than her husband, then her husband should not be assigned the highest role. To incorporate the relation information to local classification results, we employ re-ranking approach. Assuming that the local semantic"
P09-2064,A00-2018,0,\N,Missing
P13-2141,P05-1045,0,0.037673,"uent relations in Freebase 2009. The facts of each relation were equally split to two parts for training and testing. Wikipedia 2009 was used as the target corpus, where 800,000 articles were used for training and 400,000 for testing. During the NED phrase, there are 94 unique entity types (they are also relations in Freebase) for the source and destination entities. Note that some entity types contain too few entities and they are discarded. We used 500,000 Wikipedia articles (2,000,000 sentences) for generating training data for the NED component. We used Open NLP POS tagger, Standford NER (Finkel et al., 2005) and MaltParser (Nivre et al., 2006) to label/tag sentences. We employed liblinear (Fan et al., 2008) as classifiers for NED and relation extraction and the solver is L2LR. Figure 2. For a candidate fact without any entity existing in Freebase, we are not able to judge whether it is correct. Thus we only evaluate the candidate facts that at least one entity occurs as the source or destination entity in the test fact set. In Figure 1, we compared our method with two previous methods: MultiR (Hoffmann et al., 2011) and Takamatsu et al. (2012) (Taka). For MultiR, we used the author’s implementati"
P13-2141,P11-1055,0,0.244229,"jing University of Posts and Telecommunications 1 {zhangxingxing,szf}@pku.edu.cn 2 {jiazhan,junyan,zhengc}@microsoft.com 3 junyu.zeng@gmail.com 1 Abstract a candidate entity pair to a relation. Then an existing fact in a KB can be used as a labeled example whose label is the relation name. Then the features of all the sentences (from a given text corpus) containing the entity pair are merged as the feature of the example. Finally a multi-class classifier is trained. However, the accuracy of DS is not satisfying. Some variants have been proposed to improve the performance (Riedel et al., 2010; Hoffmann et al., 2011; Takamatsu et al., 2012). They argue that DS introduces a lot of noise into the training data by merging the features of all the sentences containing the same entity pair, because a sentence containing the entity pair of a relation may not talk about the relation. Riedel et al. (2010) and Hoffmann et al. (2011) introduce hidden variables to indicate whether a sentence is noise and try to infer them from the data. Takamatsu et al. (2012) design a generative model to identify noise patterns. However, as shown in the experiments (Section 4), the above variants do not lead to much improvement in"
P13-2141,P09-1113,0,0.29656,"such as “(Bill Gates, BornIn, Seattle)”. An important task is to enrich such KBs by extracting more facts from text. Specifically, this paper focuses on extracting facts for existing relations. This is different from OpenIE (Banko et al., 2007; Carlson et al., 2010) which needs to discover new relations. Given large amounts of labeled sentences, supervised methods are able to achieve good performance (Zhao and Grishman, 2005; Bunescu and Mooney, 2005). However, it is difficult to handle large scale corpus due to the high cost of labeling. Recently an approach called distant supervision (DS) (Mintz et al., 2009) was proposed, which does not require any labels on the text. It treats the extraction problem as classifying ∗ The contact author. 810 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 810–815, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Second, in DS, the “Other” class is composed of all the candidate entity pairs not existed in the KB, which actually contains many positive facts of non-Other relations because the KB is not complete. Therefore we use a different way to construct negative training examples. pe"
P13-2141,P11-2048,0,0.0123724,"mpact on the accuracy but has not been touched or well handled before. First, each relation has its own schema definition, i.e., the source entity and the destination entity should be of valid types, which is overlooked in DS. Therefore, we propose a component of entity type detection to check it. Second, DS introduces many false negative examples into the training set and we propose a new method to construct negative training examples. Third, we find it is difficult for a single classifier to achieve high accuracy and hence we train multiple classifiers and ensemble them. We also notice that Nguyen and Moschitti (2011a) and Nguyen and Moschitti (2011b) utilize external information such as more facts from Yago and labeled sentences from ACE to improve the Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. However, the accuracy is still not satisfying. In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. We propose an approach to handle these factors. By experimenting on Wikipedia"
P13-2141,I11-1082,0,0.18932,"mpact on the accuracy but has not been touched or well handled before. First, each relation has its own schema definition, i.e., the source entity and the destination entity should be of valid types, which is overlooked in DS. Therefore, we propose a component of entity type detection to check it. Second, DS introduces many false negative examples into the training set and we propose a new method to construct negative training examples. Third, we find it is difficult for a single classifier to achieve high accuracy and hence we train multiple classifiers and ensemble them. We also notice that Nguyen and Moschitti (2011a) and Nguyen and Moschitti (2011b) utilize external information such as more facts from Yago and labeled sentences from ACE to improve the Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. However, the accuracy is still not satisfying. In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. We propose an approach to handle these factors. By experimenting on Wikipedia"
P13-2141,H05-1091,0,0.0316927,"ral knowledge bases (KB) such as Freebase, Yago, etc. They are composed of relational facts often represented in the form of a triplet, (SrcEntity, Relation, DstEntity), such as “(Bill Gates, BornIn, Seattle)”. An important task is to enrich such KBs by extracting more facts from text. Specifically, this paper focuses on extracting facts for existing relations. This is different from OpenIE (Banko et al., 2007; Carlson et al., 2010) which needs to discover new relations. Given large amounts of labeled sentences, supervised methods are able to achieve good performance (Zhao and Grishman, 2005; Bunescu and Mooney, 2005). However, it is difficult to handle large scale corpus due to the high cost of labeling. Recently an approach called distant supervision (DS) (Mintz et al., 2009) was proposed, which does not require any labels on the text. It treats the extraction problem as classifying ∗ The contact author. 810 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 810–815, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Second, in DS, the “Other” class is composed of all the candidate entity pairs not existed in the KB, which actual"
P13-2141,P81-1022,0,0.773384,"Missing"
P13-2141,D11-1135,0,0.0171276,"it violates the 1-to-1/n-to-1 constraint. If r is 1-to-n, the judgement is similar and just simply swap the source and destination entities of the relation. 3.3 Feature Space Partition and Ensemble 1-to-1/n-to-1/1-to-n Relation A 1-to-1 or n-to1 relation is a functional relation: for a relation r, for each valid source entity e1 , there is only one unique destination entity e2 such that (e1 , e2 ) ∈ r. However, in a real KB like Freebase, very few relations meet the exact criterion. Thus we use the The features of DS (Mintz et al., 2009) are very sparse in the corpus. We add some features in (Yao et al., 2011): Trigger Words (the words on the dependency path except stop words) and Entity String (source entity and destination entity). 812 Taka 0.76 0.48 0.82 0.79 Ensemble 0.98 1 1 0.89 1 Table 1: Manual evaluation of top-ranked 50 relation instances for the most frequent 15 relations. 4 P1 + P2 + P1more + P2more 4 0.6 0.4 0.2 We find that without considering the reversed order of entity pairs in a sentence, the precision can be higher, but the recall decreases. For example, for the entity pair ⟨Ventura Pons, Actrius⟩, we only consider sentences with the right order (e.g. Ventura Pons is directed by"
P13-2141,P05-1052,0,0.0787814,"on building large structural knowledge bases (KB) such as Freebase, Yago, etc. They are composed of relational facts often represented in the form of a triplet, (SrcEntity, Relation, DstEntity), such as “(Bill Gates, BornIn, Seattle)”. An important task is to enrich such KBs by extracting more facts from text. Specifically, this paper focuses on extracting facts for existing relations. This is different from OpenIE (Banko et al., 2007; Carlson et al., 2010) which needs to discover new relations. Given large amounts of labeled sentences, supervised methods are able to achieve good performance (Zhao and Grishman, 2005; Bunescu and Mooney, 2005). However, it is difficult to handle large scale corpus due to the high cost of labeling. Recently an approach called distant supervision (DS) (Mintz et al., 2009) was proposed, which does not require any labels on the text. It treats the extraction problem as classifying ∗ The contact author. 810 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 810–815, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Second, in DS, the “Other” class is composed of all the candidate entity pairs not exis"
P13-2141,nivre-etal-2006-maltparser,0,\N,Missing
P13-2141,P12-1076,0,\N,Missing
P15-1056,W06-0901,0,0.0467242,"relevant event chronicle generation work but there are some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is re"
P15-1056,S13-2002,0,0.0329879,"Missing"
P15-1056,P08-1030,1,0.800799,"vent chronicle generation work but there are some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most"
P15-1056,R09-1032,1,0.842039,"er from the indirect description problem since there are many responses (e.g., humanitarian aids) to a disaster. These responses are topically relevant and contain many documents, and where E and E∗ are our chronicle and the manually edited event chronicle respectively. te is e’s 582 time labeled by our method and t∗e is e’s correct time. Note that for multiple entries referring the same event in event chronicles, the earliest entry’s time is used as the event’s time to compute diff. sports 0.800 politics 3.363 disaster 1.042 war 1.610 line for a document (Do et al., 2012), a centroid entity (Ji et al., 2009) or one major event (Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013). In addition, Li and Cardie (2014) generated timelines for users in microblogs. The most related work to ours is Swan and Allan (2000). They used a timeline to show bursty events along the time, which can be seen as an early form of event chronicles. Different from their work, we generate a topically relevant event chronicle based on a reference event chronicle. comprehensive 2.467 Table 3: Difference between an event’s actual time and the time in our chronicles. Time unit is a day. Table 3 shows the per"
P15-1056,D08-1073,0,0.0878802,"Missing"
P15-1056,Q14-1022,0,0.0297756,"Missing"
P15-1056,S13-2012,0,0.0607878,"Missing"
P15-1056,P13-2099,1,0.719355,"n aids) to a disaster. These responses are topically relevant and contain many documents, and where E and E∗ are our chronicle and the manually edited event chronicle respectively. te is e’s 582 time labeled by our method and t∗e is e’s correct time. Note that for multiple entries referring the same event in event chronicles, the earliest entry’s time is used as the event’s time to compute diff. sports 0.800 politics 3.363 disaster 1.042 war 1.610 line for a document (Do et al., 2012), a centroid entity (Ji et al., 2009) or one major event (Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013). In addition, Li and Cardie (2014) generated timelines for users in microblogs. The most related work to ours is Swan and Allan (2000). They used a timeline to show bursty events along the time, which can be seen as an early form of event chronicles. Different from their work, we generate a topically relevant event chronicle based on a reference event chronicle. comprehensive 2.467 Table 3: Difference between an event’s actual time and the time in our chronicles. Time unit is a day. Table 3 shows the performance of our approach in labeling event time. For disaster, sports and war, the accurac"
P15-1056,D12-1092,0,0.0143416,"times called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We thank the anonymou"
P15-1056,N09-2053,1,0.822412,"on work but there are some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focu"
P15-1056,P13-1008,1,0.798212,"2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We thank the anonymous reviewers for their thought-provoki"
P15-1056,C12-1033,0,0.0200147,"c detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We thank the anonymous reviewers for the"
P15-1056,P10-1081,0,0.0232743,"re some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeA"
P15-1056,D12-1062,0,0.0241202,"blems. Disaster event chronicles suffer from the indirect description problem since there are many responses (e.g., humanitarian aids) to a disaster. These responses are topically relevant and contain many documents, and where E and E∗ are our chronicle and the manually edited event chronicle respectively. te is e’s 582 time labeled by our method and t∗e is e’s correct time. Note that for multiple entries referring the same event in event chronicles, the earliest entry’s time is used as the event’s time to compute diff. sports 0.800 politics 3.363 disaster 1.042 war 1.610 line for a document (Do et al., 2012), a centroid entity (Ji et al., 2009) or one major event (Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013). In addition, Li and Cardie (2014) generated timelines for users in microblogs. The most related work to ours is Swan and Allan (2000). They used a timeline to show bursty events along the time, which can be seen as an early form of event chronicles. Different from their work, we generate a topically relevant event chronicle based on a reference event chronicle. comprehensive 2.467 Table 3: Difference between an event’s actual time and the time in our chronicles. Time"
P15-1056,D13-1001,1,0.844429,"in sports chronicles but it is not a good entry in comprehensive chronicles. Compared with comprehensive event chronicles, events in other chronicles tend to describe more details. For example, a sports chronicle may regard each match in the World Cup as an event while comprehensive chronicles consider the World Cup as one event, which requires us to adapt event granularity for different chronicles. Also, we evaluate the time of event entries in these five event chronicles because event’s happening time is not always equal to the timestamp of the document creation time (UzZaman et al., 2012; Ge et al., 2013). We collect existing manually edited 2010 chronicles on the web and use their event time as gold standard. We define a metric to evaluate if the event entry’s time in our chronicle is accurate: P diff = e∈E∩E∗ |(te − t∗e )|/|E ∩ E∗ | not show significant improvement. A possible reason is that a comprehensive event chronicle does not care the topical relevance of a event. In other words, its ranking problem is simpler so that the learning-to-rank does not improve the basic ranking criterion much. Moreover, we analyze the incorrect entries in the chronicles generated by our approaches. In gener"
P15-1056,P14-5010,0,0.00567471,"ifically, we collected disaster, sports, war, politics and comprehensive chronicles during 2009 from mapreport7 , infoplease and Wikipedia8 . To generate chronicles during 2010, we use 2009-2010 APW and Xinhua news in English Gigaword (Graff et al., 2003) and remove documents whose titles and first paragraphs do not include any burst words. We detect burst words using Kleinberg algorithm (Kleinberg, 2003), which is a 2-state finite automaton model and widely used to detect bursts. In total, there are 140,557 documents in the corpus. Preprocessing: We remove stopwords and use Stanford CoreNLP (Manning et al., 2014) to do lemmatization. Parameter setting: For TaHBM, we empirically set α = 0.05, βz = 0.005, βe = 0.0001, γs = 0.05, γx = 0.5, ε = 0.01, the number of topics K = 50, and the number of events E = 5000. We run Gibbs sampler for 2000 iterations with burn-in period of 500 for inference. For event ranking, we set regularization parameter of SVMRank c = 0.1. Chronicle display: We use a heuristic way to generate the description of each event. Since the first paragraph of a news article is usually a good summary of the article and the earliest document in a cluster usually explicitly describes the eve"
P15-1056,P11-1113,0,0.0152544,"ent detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We t"
P15-1056,P05-3021,0,0.114846,"Missing"
P15-1056,P12-2009,0,0.252735,"to the event chronicle) in Section 3.3 are labeled as high rank priority while those without positive documents are labeled as low priority. 4.2 |De | σe : Features We use the following features to train the ranking model, all of which can be provided by TaHBM. • P (s = 1|e): the probability that an event e is topically relevant to the reference chronicle. • P (e|z): the probability reflects an event’s impact given its topic. • σe : the parameter of an event e’s Gaussian distribution. It determines the ‘bandwidth’ 7 8 580 http://www.mapreport.com http://en.wikipedia.org/wiki/2009 5.2 schema (Zhao et al., 2012) to detect events, which is the state-of-the-art event detection method for general domains. Evaluation Methods and Baselines Since there is no existing evaluation metric for the new task, we design a method for evaluation. Although there are manually edited event chronicles on the web, which may serve as references for evaluation, they are often incomplete. For example, the 2010 politics event chronicle on Wikipedia has only two event entries. Hence, we first pool all event entries of existing chronicles on the web and chronicles generated by approaches evaluated in this paper and then have 3"
P15-2110,D14-1082,0,0.0136346,"words. Method 2.1 Local Predictor Conversation-specific features: As mentioned in Section 1, different person roles being the subject or the object of a predicate may have an effect on the tense in a conversation. We analyze the person roles of the subject and the object of the main predicate and encode them as features, which helps our model understand effects of interactions on tense. We develop a Maximum Entropy (MaxEnt) classifier (Zhang, 2004) as the local predictor. Basic features: The unigrams, bigrams and trigrams of a sentence. Dependency parsing features: We use the Stanford parser (Chen and Manning, 2014) to conduct dependency parsing3 on the target sentences and use dependency paths associated with the main predicate of a sentence as well as their dependency types as features. By using the parsing features, 2 3 2.2 Global Predictor As we discussed before, tense ambiguity in a sentence arises from the omissions of sentence components. According to the principle of efficient information transmission (Jaeger and Levy, 2006; http://nlp.cs.rpi.edu/data/chinesetense.zip We use CCProcessed dependencies. 669 Jaeger, 2010) and Gricean Maxims (Grice et al., 1975) in cooperative theory, the omitted elem"
P15-2110,I11-1125,0,0.210627,"l) evidence to enhance the performance. Experimental results demonstrate the power of this hybrid approach, which can serve as a new and promising benchmark. 1 Introduction In natural languages, tense is important to indicate the time at which an action or event takes place. In some languages such as Chinese, verbs do not have explicit morphological or grammatical forms to indicate their tense information. Therefore, automatic tense prediction is important for both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1 The main predicate of a sentence can be considered equal to the root of a dependency parse 668 Proceedings of the 53rd"
P15-2110,xue-zhang-2014-buy,0,0.659972,"tion is important for both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1 The main predicate of a sentence can be considered equal to the root of a dependency parse 668 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 668–673, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a: [如果(if)]你(you)动(touch)我(my)儿子(son)一下(once)，我(I)先(first)废(destroy)了你(you)。 (If you touch my son, I’ll destroy you.) b: 我(I)告诉(tell)你(you)一声，航班(flight)取消(cancel)了。(I’m telling you: the flight"
P15-2110,D08-1074,0,0.0792966,"Missing"
P15-2110,W06-0107,0,0.033577,"atic tense prediction is important for both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1 The main predicate of a sentence can be considered equal to the root of a dependency parse 668 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 668–673, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a: [如果(if)]你(you)动(touch)我(my)儿子(son)一下(once)，我(I)先(first)废(destroy)了你(you)。 (If you touch my son, I’ll destroy you.) b: 我(I)告诉(tell)你(you)一声，航班(flight)取消(cancel)了。(I’m tel"
P15-2110,D14-1204,0,0.557711,"both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1 The main predicate of a sentence can be considered equal to the root of a dependency parse 668 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 668–673, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a: [如果(if)]你(you)动(touch)我(my)儿子(son)一下(once)，我(I)先(first)废(destroy)了你(you)。 (If you touch my son, I’ll destroy you.) b: 我(I)告诉(tell)你(you)一声，航班(flight)取消(cancel)了。(I’m telling you: the flight is canceled.) c:你 (you"
P15-2110,W03-1730,0,0.0363715,"−λ) n−1 X i=1 p p p p p local prediction p p c p p p global prediction p p p p p p sentences Figure 1: Global tense prediction for the conversation 5 in Table 1. 3 Experiments 3.1 Data and Scoring Metric To the best of our knowledge, tense prediction in Chinese conversations has never been studied before and there is no existing benchmark for evaluation. We collected 294 conversations (including 1,857 sentences) from 25 popular Chinese movies, dramas and TV shows. Each conversation contains 2-18 sentences. We manually annotate the main predicate and its tense in each sentence. We use ICTCLAS (Zhang et al., 2003) to do word segmentation as preprocessing. Since tense prediction can be seen as a multiclass classification problem, we use accuracy as the metric to evaluate the performance. We randomly split our dataset into three sets: training set (244 conversations), development set (25 conversations) and test set (25 conversations) for evaluation. In evaluation, we ignore imperative sentences and sentences without predicates. Global tense prediction Inspired by the burst detection algorithm proposed by Kleinberg (2003), we use a 3-state automaton sequence model to globally predict tense based on the ab"
P16-1116,P11-1098,0,0.0369488,"from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one eve"
P16-1116,P15-1017,0,0.13171,"o and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type, which requires a lot of training data. The dynamic multi-pooling convolutional neural network (DMCNN) (Chen et al., 2015) is currently the only widely used deep neural network based approach. DMCNN is mainly used to model contextual features. However, DMCNN still does not consider argument-argument interactions. In summary, most of the above works are either pattern-only or features-only. Moreover, all of these methods consider arguments sepa1225 rately while ignoring the relationship between arguments, which is also important for argument identification. Even the joint method (Li et al., 2013) does not model argument relations directly. We use trigger embedding, sentencelevel embedding, and pattern features tog"
P16-1116,P98-1067,0,0.0819726,", sentence-level embedding, and pattern features together as the our features for balancing. • We proposed a regularization-based method in order to make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and N"
P16-1116,P06-1061,0,0.041959,"ootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et"
P16-1116,P11-1113,0,0.84565,"aining (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate mod"
P16-1116,P11-1114,0,0.19134,"ed and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type,"
P16-1116,E12-1029,0,0.213239,"how that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et"
P16-1116,P08-1030,0,0.738218,"ents. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explore"
P16-1116,W05-0610,0,0.0178924,"f and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into conside"
P16-1116,P13-1008,0,0.390565,"2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type, which requires a lot of training data. The dynamic multi-pooling convolutional neural network (DMCNN) (Chen et al., 2015) is currently the only widely used deep neural network based approach. DMCNN is mainly used to model contextual features. However, DMCNN still does not consider argument-argument interactions. In summary, most of the above works ar"
P16-1116,P10-1081,0,0.591789,"n the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff e"
P16-1116,E12-1030,0,0.0160451,"tern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other met"
P16-1116,P12-1088,0,0.0652043,"and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type, which requires a lot of training data. The dynamic multi-pooling convolutional neural network (DMCNN) (Chen et al., 2015) is currently the only widely used deep neural network based approach. DMCNN is mainly used to model contextual features. However, DMCNN still does not consider argument-argument interactions. In summary, most of the above works are either pattern-onl"
P16-1116,P07-1075,0,0.0284876,"). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based me"
P16-1116,D07-1075,0,0.0301605,"ethods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for exam"
P16-1116,C00-2136,0,0.154785,"ed a regularization-based method in order to make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly super"
P16-1116,P05-1062,0,0.0399324,"3; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (M"
P16-1116,D09-1016,0,0.293776,"re patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012)"
P16-1116,P06-2094,0,0.0388605,"improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 200"
P16-1116,N06-1039,0,0.0146725,"n method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 200"
P16-1116,P05-1047,0,0.0304587,"o make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of e"
P16-1116,P03-1029,0,0.0553078,"d method in order to make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pat"
P16-1116,C98-1064,0,\N,Missing
P17-1189,D16-1212,1,0.768908,"Missing"
P17-1189,N04-1032,0,0.0950101,"Missing"
P17-1189,W13-3820,0,0.176999,"B 1.0 show that our model outperforms state-of-the-art methods. 1 Meanwhile widely solicit opinions, for many times [Rel 修改] [Arg1 *pro*] 。 revise (omitted) . They to system made revise . Figure 1: Sentences from (a) CPB and (b) our heterogeneous dataset. In CPB, each predicate (e.g., 修改) has a specific set of core roles given with numbers (e.g., Arg0). While our dataset uses a different semantic role set, and all roles are nonpredicate-specific. Semantic role labeling (SRL) is one of the fundamental tasks in natural language processing because of its important role in information extraction (Bastianelli et al., 2013), statistical machine translation (Aziz et al., 2016; Xiong et al., 2012), and so on. However, state-of-the-art performance of Chinese SRL is still far from satisfactory. And data sparsity has been a bottleneck which can not be http://www.klcl.pku.edu.cn/ShowNews.aspx?id=156 repeatedly (b) [agent 他们] 对 [patient 系统]进行了[Rel 修改] 。 Introduction 1 the NPC Standing Committee 广泛 征求 意见, [ArgM-ADV 多次] [ArgM-ADV 反复] ignored. For English, the most commonly used benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBan"
P17-1189,P06-2013,0,0.111481,"Missing"
P17-1189,J02-3001,0,0.152943,"Missing"
P17-1189,C16-1120,0,0.317942,"which can not be http://www.klcl.pku.edu.cn/ShowNews.aspx?id=156 repeatedly (b) [agent 他们] 对 [patient 系统]进行了[Rel 修改] 。 Introduction 1 the NPC Standing Committee 广泛 征求 意见, [ArgM-ADV 多次] [ArgM-ADV 反复] ignored. For English, the most commonly used benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 propositions) (Xue, 2008). To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016). The heterogeneous resources introduced by these models include other semantically annotated corpora with annotation schema different to that used in PropBank, and even of a different language. The challenge here lies in the fact that those newly introduced resources are heterogeneous in nature, without sharing the same tagging schema, semantic role set, syntactic tag set and domain. For example, Wang et al. (2015) introduced a heterogeneous dataset, Chinese NetBank, by pretraining word embeddings. Specifically, they learn an LSTM RNN model based on NetBank first, then initi"
P17-1189,D09-1153,1,0.955645,"NLP tasks, like event extraction and relation classification, etc. 2070 CPB only annotate common verbs as predicates. • In terms of semantic roles, CSB has a more fine-grained semantic role set. There are 31 roles defined in five types (as Table. 1 shows). Whereas in CPB, there are totally 23 roles, including core roles and non-core roles. • CSB does not have any pre-defined frames for predicates because all roles are set to be non-predicate-specific. The reason for not defining frames is that frames may lead inconsistencies in labels. For example, according to Chinese verb formation theory (Sun et al., 2009), in CPB, an agent of a verb is often marked as its Arg0, but not all Arg0 are agents. Therefore, roles are defined for predicates with similar syntactic and semantic regularities, rather than single predicate. Two direct benefits of using stand-alone nonpredicate-specific roles are: First, meanings of all semantic roles can be directly inferred from their labels. For instance, roles of things that people are telling (谈 ) or looking (看) are labeled as 内 容/content, because verbs like 谈 and 看 are often followed by an object. Second, we can easily annotate sentences with new predicates without de"
P17-1189,D15-1186,1,0.409335,"s been a bottleneck which can not be http://www.klcl.pku.edu.cn/ShowNews.aspx?id=156 repeatedly (b) [agent 他们] 对 [patient 系统]进行了[Rel 修改] 。 Introduction 1 the NPC Standing Committee 广泛 征求 意见, [ArgM-ADV 多次] [ArgM-ADV 反复] ignored. For English, the most commonly used benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 propositions) (Xue, 2008). To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016). The heterogeneous resources introduced by these models include other semantically annotated corpora with annotation schema different to that used in PropBank, and even of a different language. The challenge here lies in the fact that those newly introduced resources are heterogeneous in nature, without sharing the same tagging schema, semantic role set, syntactic tag set and domain. For example, Wang et al. (2015) introduced a heterogeneous dataset, Chinese NetBank, by pretraining word embeddings. Specifically, they learn an LSTM RNN model based on NetBank"
P17-1189,P12-1095,0,0.0160469,"ely solicit opinions, for many times [Rel 修改] [Arg1 *pro*] 。 revise (omitted) . They to system made revise . Figure 1: Sentences from (a) CPB and (b) our heterogeneous dataset. In CPB, each predicate (e.g., 修改) has a specific set of core roles given with numbers (e.g., Arg0). While our dataset uses a different semantic role set, and all roles are nonpredicate-specific. Semantic role labeling (SRL) is one of the fundamental tasks in natural language processing because of its important role in information extraction (Bastianelli et al., 2013), statistical machine translation (Aziz et al., 2016; Xiong et al., 2012), and so on. However, state-of-the-art performance of Chinese SRL is still far from satisfactory. And data sparsity has been a bottleneck which can not be http://www.klcl.pku.edu.cn/ShowNews.aspx?id=156 repeatedly (b) [agent 他们] 对 [patient 系统]进行了[Rel 修改] 。 Introduction 1 the NPC Standing Committee 广泛 征求 意见, [ArgM-ADV 多次] [ArgM-ADV 反复] ignored. For English, the most commonly used benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 propositions) (Xue, 2008). To mitigate the"
P17-1189,xue-2006-annotating,0,0.0762818,"Missing"
P17-1189,J08-2004,0,0.894447,", 2016; Xiong et al., 2012), and so on. However, state-of-the-art performance of Chinese SRL is still far from satisfactory. And data sparsity has been a bottleneck which can not be http://www.klcl.pku.edu.cn/ShowNews.aspx?id=156 repeatedly (b) [agent 他们] 对 [patient 系统]进行了[Rel 修改] 。 Introduction 1 the NPC Standing Committee 广泛 征求 意见, [ArgM-ADV 多次] [ArgM-ADV 反复] ignored. For English, the most commonly used benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 propositions) (Xue, 2008). To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016). The heterogeneous resources introduced by these models include other semantically annotated corpora with annotation schema different to that used in PropBank, and even of a different language. The challenge here lies in the fact that those newly introduced resources are heterogeneous in nature, without sharing the same tagging schema, semantic role set, syntactic tag set and domain. For example, Wang et al."
P17-1189,W03-1707,0,0.232,"sks in natural language processing because of its important role in information extraction (Bastianelli et al., 2013), statistical machine translation (Aziz et al., 2016; Xiong et al., 2012), and so on. However, state-of-the-art performance of Chinese SRL is still far from satisfactory. And data sparsity has been a bottleneck which can not be http://www.klcl.pku.edu.cn/ShowNews.aspx?id=156 repeatedly (b) [agent 他们] 对 [patient 系统]进行了[Rel 修改] 。 Introduction 1 the NPC Standing Committee 广泛 征求 意见, [ArgM-ADV 多次] [ArgM-ADV 反复] ignored. For English, the most commonly used benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 propositions) (Xue, 2008). To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016). The heterogeneous resources introduced by these models include other semantically annotated corpora with annotation schema different to that used in PropBank, and even of a different language. The challenge here lies in the fact that those newly introduced r"
P17-1189,D14-1041,0,0.0386753,"Missing"
P17-1189,W11-2136,0,\N,Missing
P18-1230,D14-1110,0,0.114658,"Missing"
P18-1230,D17-1167,0,0.0278569,"d semantic labels in WordNet can help to WSD in a multi-task learning framework. As far as we know, there is no study directly integrates glosses or semantic relations of the WordNet into an end-to-end model. In this paper, we focus on how to integrate glosses into a unified neural WSD system. Memory network (Sukhbaatar et al., 2015b; Kumar et al., 2016; Xiong et al., 2016) is initially proposed to solve question answering problems. Recent researches show that memory network obtains the state-of-the-art results in many NLP tasks such as sentiment classification (Li et al., 2017) and analysis (Gui et al., 2017), poetry generation (Zhang et al., 2017), spoken language understanding (Chen et al., 2016), etc. Inspired by the success of memory network used in many NLP tasks, we introduce it into WSD. We make some adaptations to the initial memory network in order to incorporate glosses and capture the inner relationship between the context and glosses. 3 Incorporating Glosses into Neural Word Sense Disambiguation In this section, we first give an overview of the proposed model GAS: a gloss-augmented WSD neural network which integrates the context and the glosses of the target word into a unified framewo"
P18-1230,P16-1085,0,0.188836,"on several English all-words WSD datasets. 1 Introduction Word Sense Disambiguation (WSD) is a fundamental task and long-standing challenge in Natural Language Processing (NLP). There are several lines of research on WSD. Knowledge-based methods focus on exploiting lexical resources to infer the senses of word in the context. Supervised methods usually train multiple classifiers with manual designed features. Although supervised methods can achieve the state-of-the-art performance (Raganato et al., 2017b,a), there are still two major challenges. Firstly, supervised methods (Zhi and Ng, 2010; Iacobacci et al., 2016) usually train a dedicated classifier for each word individually (often called word expert). So it can not easily scale up to all-words WSD task which requires to disambiguate all the polysemous word in texts 1 . Recent neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) solve this problem by building a unified model for all the polysemous words, but they still can’t beat the best word expert system. Secondly, all the neural-based methods always only consider the local context of the target word, ignoring the lexical resources like WordNet (Miller, 1995) which are w"
P18-1230,W16-5307,0,0.165559,"Missing"
P18-1230,C14-1151,0,0.437299,"Recent neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) solve this problem by building a unified model for all the polysemous words, but they still can’t beat the best word expert system. Secondly, all the neural-based methods always only consider the local context of the target word, ignoring the lexical resources like WordNet (Miller, 1995) which are widely used in the knowledge-based methods. The gloss, which extensionally defines a word sense meaning, plays a key role in the well-known Lesk algorithm (Lesk, 1986). Recent studies (Banerjee and Pedersen, 2002; Basile et al., 2014) have shown that enriching gloss information through its semantic relations can greatly improve the accuracy of Lesk algorithm. To this end, our goal is to incorporate the gloss information into a unified neural network for all of the polysemous words. We further consider extending the original gloss through its semantic relations in our framework. As shown in Figure 1, the glosses of hypernyms and hyponyms can enrich the original gloss information as well as help to build better a sense representation. Therefore, we integrate not only the original gloss but also the related glosses of hyperny"
P18-1230,P15-1072,0,0.0598781,"Missing"
P18-1230,P15-2003,0,0.0608941,"Missing"
P18-1230,J81-4005,0,0.734025,"Missing"
P18-1230,Q14-1019,0,0.40617,"external knowledge. The fives blocks list the MFS baseline, two knowledge-based systems, two supervised systems (feature-based), three neuralbased systems and our models, respectively. . • Bi-LSTM+att.+LEX and its variant BiLSTM+att.+LEX+P OS : Raganato et al. (2017a) transfers WSD into a sequence learning task and propose a multi-task learning framework for WSD, POS tagging and coarse-grained semantic labels (LEX). These two models have used the external knowledge, for the LEX is based on lexicographer files in WordNet. gloss information via its semantic relations can help to WSD. • Babelfy: Moro et al. (2014) exploits the semantic network structure from BabelNet and builds a unified graph-based architecture for WSD and Entity Linking. 4.3.2 Supervised Systems The supervised systems mentioned in this paper refers to traditional feature-based systems which train a dedicated classifier for every word individually (word expert). • IMS: Zhi and Ng (2010) selects a linear Support Vector Machine (SVM) as its classifier and makes use of a set of features surrounding the target word within a limited window, such as POS tags, local words and local collocations. • IMS+emb : Iacobacci et al. (2016) selects IM"
P18-1230,D17-1120,0,0.266526,"rich the gloss information. The experimental results show that our model outperforms the state-of-theart systems on several English all-words WSD datasets. 1 Introduction Word Sense Disambiguation (WSD) is a fundamental task and long-standing challenge in Natural Language Processing (NLP). There are several lines of research on WSD. Knowledge-based methods focus on exploiting lexical resources to infer the senses of word in the context. Supervised methods usually train multiple classifiers with manual designed features. Although supervised methods can achieve the state-of-the-art performance (Raganato et al., 2017b,a), there are still two major challenges. Firstly, supervised methods (Zhi and Ng, 2010; Iacobacci et al., 2016) usually train a dedicated classifier for each word individually (often called word expert). So it can not easily scale up to all-words WSD task which requires to disambiguate all the polysemous word in texts 1 . Recent neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) solve this problem by building a unified model for all the polysemous words, but they still can’t beat the best word expert system. Secondly, all the neural-based methods always only con"
P18-1230,E17-1010,0,0.3374,"rich the gloss information. The experimental results show that our model outperforms the state-of-theart systems on several English all-words WSD datasets. 1 Introduction Word Sense Disambiguation (WSD) is a fundamental task and long-standing challenge in Natural Language Processing (NLP). There are several lines of research on WSD. Knowledge-based methods focus on exploiting lexical resources to infer the senses of word in the context. Supervised methods usually train multiple classifiers with manual designed features. Although supervised methods can achieve the state-of-the-art performance (Raganato et al., 2017b,a), there are still two major challenges. Firstly, supervised methods (Zhi and Ng, 2010; Iacobacci et al., 2016) usually train a dedicated classifier for each word individually (often called word expert). So it can not easily scale up to all-words WSD task which requires to disambiguate all the polysemous word in texts 1 . Recent neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) solve this problem by building a unified model for all the polysemous words, but they still can’t beat the best word expert system. Secondly, all the neural-based methods always only con"
P18-1230,P15-1173,0,0.101594,"Missing"
P18-1230,P17-1125,0,0.0318585,"to WSD in a multi-task learning framework. As far as we know, there is no study directly integrates glosses or semantic relations of the WordNet into an end-to-end model. In this paper, we focus on how to integrate glosses into a unified neural WSD system. Memory network (Sukhbaatar et al., 2015b; Kumar et al., 2016; Xiong et al., 2016) is initially proposed to solve question answering problems. Recent researches show that memory network obtains the state-of-the-art results in many NLP tasks such as sentiment classification (Li et al., 2017) and analysis (Gui et al., 2017), poetry generation (Zhang et al., 2017), spoken language understanding (Chen et al., 2016), etc. Inspired by the success of memory network used in many NLP tasks, we introduce it into WSD. We make some adaptations to the initial memory network in order to incorporate glosses and capture the inner relationship between the context and glosses. 3 Incorporating Glosses into Neural Word Sense Disambiguation In this section, we first give an overview of the proposed model GAS: a gloss-augmented WSD neural network which integrates the context and the glosses of the target word into a unified framework. After that, each individual module i"
P18-1230,P10-4014,0,0.319401,"-of-theart systems on several English all-words WSD datasets. 1 Introduction Word Sense Disambiguation (WSD) is a fundamental task and long-standing challenge in Natural Language Processing (NLP). There are several lines of research on WSD. Knowledge-based methods focus on exploiting lexical resources to infer the senses of word in the context. Supervised methods usually train multiple classifiers with manual designed features. Although supervised methods can achieve the state-of-the-art performance (Raganato et al., 2017b,a), there are still two major challenges. Firstly, supervised methods (Zhi and Ng, 2010; Iacobacci et al., 2016) usually train a dedicated classifier for each word individually (often called word expert). So it can not easily scale up to all-words WSD task which requires to disambiguate all the polysemous word in texts 1 . Recent neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) solve this problem by building a unified model for all the polysemous words, but they still can’t beat the best word expert system. Secondly, all the neural-based methods always only consider the local context of the target word, ignoring the lexical resources like WordNet ("
P19-1194,P18-1139,0,0.0593011,"ed on Variational Autoencoder (VAE) to first disentangle the content factor and source sentiment factor, and then combine the content with target sentiment factor. However, the quality of the pseudo-parallel data is not quite satisfactory, which seriously affects the performance of the VAE model. Different from them, we dynamically update the pseudo-parallel data via on-the-fly back-translation (Lample et al., 2018b) during training (Eq. 12). There are some other tasks of NLP also show interest in controlling the fine-grained attribute of text generation. For example, Zhang et al. (2018a) and Ke et al. (2018) propose to control the specificity and diversity in dialogue generation. We borrow ideas from these works but the motivation and proposed models of our work are a far cry from them. The main differences are: (1) Since sentiment is dependent on local context while specificity is independent of local context, there is a series of design in our model to take the local context (or previous generated words) st into consideration (e.g., Eq. 1, Eq. 3). (2) Due to the lack of parallel data, we propose a cycle reinforcement learning algorithm to train the proposed model (Section 2.3). 6 Conclusion In"
P19-1194,D14-1181,0,0.00290278,"ut is also important. Inspired by the Mean Reciprocal Rank metric which is widely used in the Information Retrieval area, we design a Mean Relative Reciprocal Rank (MRRR) metric to measure the relative ranking MRRR = N 1 X 1 N i=1 |rank(vi ) − rank(ˆ vi ) |+ 1 (13) In addition, we also compare our model with the coarse-grained sentiment transfer systems. In order to make the results comparable, we define the generated test samples of all baselines for reproducibility. sentiment intensity larger/smaller than 0.5 as positive/negative results. Then we use a pre-trained binary TextCNN classifier (Kim, 2014) to compute the classification accuracy. 3.4.2 Human Evaluation We also perform human evaluation to assess the quality of generated sentences more accurately. Each item contains the source input, the sampled target sentiment intensity value, and the output of different systems. Then 500 items are distributed to 3 evaluators, who are required to score the generated sentences from 1 to 5 based on the input and target sentiment intensity value in terms of three criteria: content, sentiment, fluency. Content evaluates the content preservation degree. Sentiment refers to how much the output matches"
P19-1194,D18-1549,0,0.0242572,"Missing"
P19-1194,D17-1230,0,0.0454095,"in Figure 2. By means of policy gradient method (Williams, 1992), for each training example, the expected gradient of Eq. 10 can be approximated as: K   1 X (k) ∇θ L(θ) &apos; − r − b ∇θ log pθ (yˆ(k) ) K k=1 (11) where K is the sample size and b is the greedy search decoding baseline that aims to reduce the variance of gradient estimate which is implemented in the same way as Paulus et al. (2017). Nevertheless, RL training strives to optimize a specific metric which may not guarantee the fluency of the generated text (Paulus et al., 2017), and 2016 usually faces the unstable training problems (Li et al., 2017). The most direct way is to expose the sentences which are from the training corpus to the decoder and trained via MLE (also called teacher-forcing). In order to expose the decoder to the original sentence from the training corpus, we borrow ideas from back-translation (Lample et al., 2018a,b). Specifically, the model first generates a sequence yˆ based on the input text x and the target sentiment intensity value vy , and then reconstructs the source input x based on yˆ and the source sentiment intensity value vx . Therefore, the gradient of the cycle reconstruction loss is defined as:   ∇θ"
P19-1194,N18-1169,0,0.0483598,"Missing"
P19-1194,D18-1420,0,0.19687,"ntences whose intensity is from interval [0, 1] with a step of 0.05 to guide the model training (Step 6 in Algorithm 1). 3.2 Experiment Settings We tune hyper-parameters on the validation set. The size of vocabulary is set to 10K. Both the semantic and sentiment embeddings are 300dimensional and are learned from scratch. We 5 implement both encoder and decoder as a 1-layer LSTM with a hidden size of 256, and the former is bidirectional. The batch size is 64. We pre-train our model for 10 epochs with the MLE loss using pseudo-parallel sentences conducted by Jaccard Similarity, which is same as Liao et al. (2018). Harmonic weight β in Eq. 9 is 1 and γ in Eq. 6 is 0.5. The standard deviation σ is set to 0.01 for yielding suitable peaked distributions. The sample size K in Eq. 11 is set to 16. The optimizer is Adam (Kingma and Ba, 2014) with 10−3 initial learning rate for pre-training and 10−5 for cycleRL training. Dropout (Srivastava et al., 2014) is used to avoid overfitting. 3.3 Baselines We compare our proposed method with the following two series of state-of-the-art systems. Fine-grained systems aim to modify an input sentence to satisfy a given sentiment intensity. Liao et al. (2018) construct pse"
P19-1194,D15-1166,0,0.0327,"0, 1]. Intuitively, in order to achieve fine-grained control of sentiment, words whose sentiment intensities are closer to the target sentiment intensity value vy should be assigned a higher probability. Take Figure 2 as an example, at the 5-th time-step, word “good” should be assigned a higher probability than word “bad”, thus the predicted intensity value g(“good”, s4 ) is closer to the target sentiment intensity than g(“bad”, s4 ). To favor words whose sentiment intensity is near vy , we introduce a Gaussian kernel layer which places a Gaussian distribution centered around vy , inspired by Luong et al. (2015) and Zhang et al. (2018a). Specifically, the sentiment probability is formulated as: 2 ! g(Es , st ) − vy 1 s ot = √ exp − (4) 2σ 2 2πσ pst = softmax(ost ) (5) where σ is the standard deviation. To balance both sentiment transformation and content preservation, the final probability distribution pt over the entire vocabulary is defined as a mixture of two probability distributions: pt = γpst + (1 − γ)pct (6) where γ is the hyper-parameter that controls the trade-off between two generation probabilities. 2015 &&apos; Encoder Algorithm 1 The cycle reinforcement learning algorithm for training Seq2Se"
P19-1194,S18-1001,0,0.0359134,"Missing"
P19-1194,E17-1096,0,0.0651517,"Missing"
P19-1194,P02-1040,0,0.107261,"2 2.64 2.54 2.37 2.52 3.84 3.85 2.13 2.14 3.41 2.43 2.84 3.21 Seq2SentiSeq 32.5 10.3 0.13 0.78 35.1 3.62 4.09 4.17 3.96 Human Reference 100.0 100.0 0.07 0.83 31.2 4.51 4.36 4.75 4.54 Table 1: Automatic evaluation and human evaluation in three aspects: Content (BLUE-1, BLUE-2), Sentiment (MAE, MRRR) and Fluency (PPL). Avg shows the average human scores. ↑ denotes larger is better, and vice versa. Bold denotes the best results. review in the test dataset, crowd-workers are required to write five references with sentiment intensity value from V 0 = [0.1, 0.3, 0.5, 0.7, 0.9]. Therefore, the BLEU (Papineni et al., 2002) score between the human reference and the corresponding generated text of the same sentiment intensity can evaluate the content preservation performance. Fluency: To measure the fluency, we calculate the perplexity (PPL) of each generated sequence via a pre-trained bi-directional LSTM language model (Mousa and Schuller, 2017). Sentiment: In order to measure how close the sentiment intensity of outputs to the target intensity values, we define three metrics. Given an input sentence x and a list of target intensity values V = [v1 , v2 , ..., vN ], the corresponding outputs of the model are [yˆ1"
P19-1194,P18-2031,0,0.0321807,"results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation. Our code and data, including outputs of all baselines and our model are available at https://github.com/luofuli/ Fine-grained-Sentiment-Transfer. 1 1 Target Sentiment Text sentiment transfer aims to rephrase the input to satisfy a given sentiment label (value) while preserving its original semantic content. It facilitates various NLP applications, such as automatically converting the attitude of review and fighting against offensive language in social media (dos Santos et al., 2018). Previous work (Shen et al., 2017; Li et al., 2018; Luo et al., 2019) on text sentiment transfer mainly focuses on the coarse-grained level: the reversal of Joint work between WeChat AI and Peking University. 0.1 Horrible food and terrible service! 0.3 Plain food, slow service. 0.5 Food and service need improvement. 0.7 Good food and service. 0.9 Amazing food and perfect service!! Figure 1: An example of the input and output of the fine-grained text sentiment transfer task. The output reviews describe the same content (e.g. food/service) as the input while expressing different sentiment inten"
P19-1194,P18-1080,0,0.0258986,"tly different. In the semantic embedding space, most of the positive words and negative words lie closely. On the contrary, in the sentiment embedding space, positive words are far from negative words. In conclusion, neighbors on semantic embedding space are semantically related, while neighbors on sentiment embedding space express a similar sentiment intensity. Related Work Recently, there is a growing literature on the task of unsupervised sentiment transfer. This task aims to reverse the sentiment polarity of a sentence but keep its content unchanged without parallel data (Fu et al., 2018; Tsvetkov et al., 2018; Li et al., 2018; Xu et al., 2018; Lample et al., 2019). However, there are few researches focus on the fine-grained control of sentiment. Liao et al. (2018) exploits pseudo-parallel data via heuristic rules, thus turns this task to a supervised setting. They then propose a model based on Variational Autoencoder (VAE) to first disentangle the content factor and source sentiment factor, and then combine the content with target sentiment factor. However, the quality of the pseudo-parallel data is not quite satisfactory, which seriously affects the performance of the VAE model. Different from th"
P19-1194,P18-1090,1,0.78267,"e intensity is from interval [0, 1] with a step of 0.05 to guide the model training (Step 6 in Algorithm 1). 3.2 Experiment Settings We tune hyper-parameters on the validation set. The size of vocabulary is set to 10K. Both the semantic and sentiment embeddings are 300dimensional and are learned from scratch. We 5 implement both encoder and decoder as a 1-layer LSTM with a hidden size of 256, and the former is bidirectional. The batch size is 64. We pre-train our model for 10 epochs with the MLE loss using pseudo-parallel sentences conducted by Jaccard Similarity, which is same as Liao et al. (2018). Harmonic weight β in Eq. 9 is 1 and γ in Eq. 6 is 0.5. The standard deviation σ is set to 0.01 for yielding suitable peaked distributions. The sample size K in Eq. 11 is set to 16. The optimizer is Adam (Kingma and Ba, 2014) with 10−3 initial learning rate for pre-training and 10−5 for cycleRL training. Dropout (Srivastava et al., 2014) is used to avoid overfitting. 3.3 Baselines We compare our proposed method with the following two series of state-of-the-art systems. Fine-grained systems aim to modify an input sentence to satisfy a given sentiment intensity. Liao et al. (2018) construct pse"
P19-1194,P18-1102,0,0.162234,"t intensity2 , while keeping the semantic content unchanged. Taking Figure 1 as an example, given the same input and five sentiment intensity values ranging from 0 (most negative) to 1 (most positive), the system generates five different outputs that satisfy the corresponding sentiment intensity in a relative order. There are two main challenges of FTST task. First, it is tough to achieve fine-grained control of the sentiment intensity when generating sentence. Previous work about coarse-grained text sentiment transfer usually uses a separate decoder for each sentiment label (Xu et al., 2018; Zhang et al., 2018b) or embeds each sentiment label into a separate vector (Fu et al., 2018; Li et al., 2018). However, these methods are not feasible for fine-grained text sentiment transfer since the 2 The sentiment intensity is a real-valued score between 0 and 1, following sentiment intensity prediction task in sentiment analysis (Zhang et al., 2017; Mohammad et al., 2018). 2013 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2013–2022 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Input Se Tar Senti 0. 0. 0. 0. 0. t"
P19-1194,W17-5227,0,0.0201437,"challenges of FTST task. First, it is tough to achieve fine-grained control of the sentiment intensity when generating sentence. Previous work about coarse-grained text sentiment transfer usually uses a separate decoder for each sentiment label (Xu et al., 2018; Zhang et al., 2018b) or embeds each sentiment label into a separate vector (Fu et al., 2018; Li et al., 2018). However, these methods are not feasible for fine-grained text sentiment transfer since the 2 The sentiment intensity is a real-valued score between 0 and 1, following sentiment intensity prediction task in sentiment analysis (Zhang et al., 2017; Mohammad et al., 2018). 2013 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2013–2022 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Input Se Tar Senti 0. 0. 0. 0. 0. target sentiment intensity value is a real value, other than discrete labels. Second, parallel data3 is unavailable in practice. In other words, we can only access the corpora which are labeled with fine-grained sentiment ratings or intensity values. Therefore, in the FTST task, we can not train a generative model via ground truth outpu"
P19-1600,D10-1049,0,0.0349147,"xt belongs to the data-to-text generation (Reiter and Dale, 2000). Many previous work (Barzilay and Lapata, 2005, 2006; Liang et al., 2009) treated the task as a pipelined systems, which viewed content selection and surface realization as two separate tasks. Duboue and McKeown (2002) proposed a clustering approach in the biography domain by scoring the semantic relevance of the text and paired knowledge base. In a similar vein, Barzilay and Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al."
P19-1600,H05-1042,0,0.206311,", although our model is able to cover much more comprehensive information than the previous models (Table 2 and 3). Some implicitly expressed (like if a person is retired or not) or rarely covered (like ‘spouse’ or ‘high school’) attributes in the source tables might still be ignored in the descriptions generated by our model. Furthermore, those pieces of information which need some form of inference across Related Work Data-to-text a language generation task to generate text for structured data. Table-to-text belongs to the data-to-text generation (Reiter and Dale, 2000). Many previous work (Barzilay and Lapata, 2005, 2006; Liang et al., 2009) treated the task as a pipelined systems, which viewed content selection and surface realization as two separate tasks. Duboue and McKeown (2002) proposed a clustering approach in the biography domain by scoring the semantic relevance of the text and paired knowledge base. In a similar vein, Barzilay and Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning t"
P19-1600,N06-1046,0,0.14018,"Missing"
P19-1600,E06-1032,0,0.0188045,"er the key points in the infoboxes, we also use information richness (Eq 5) as one of our automatic evaluation. ‘Hit at least 1 word’ for an attribute means that a biography has at least one overlapping word with the words (or their synonyms) in that attribute, which are lemmatized and filtered by a stop-words list like the way we get WB-filter in Sec 4.1. ‘HIT-1 coverage’ for an attribute is the ratio of the instances involving that attribute whose biographies ‘Hit at least 1 word’ in that attribute. Human Evaluation: Since automatic evaluations like BLEU may not be reliable for NLG systems (Callison-Burch et al., 2006; Reiter and Belz, 2009; Reiter, 2018). We use human evaluation which involves the generation fluency, coverage (how much given information in the infobox is mentioned in the related biography) and correctness (how much false or irrelevant information is mentioned in the biography). We firstly sampled 300 generated biographies from the generators for human evaluation. After that, we hired 3 thirdparty crowd-workers who are equipped with sufficient background knowledge to rank the given biographies. We present the generated descriptions to the annotators in a randomized order and ask them to be"
P19-1600,D18-2003,0,0.0145712,"ootball records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work some"
P19-1600,W02-2112,0,0.0655139,"vered (like ‘spouse’ or ‘high school’) attributes in the source tables might still be ignored in the descriptions generated by our model. Furthermore, those pieces of information which need some form of inference across Related Work Data-to-text a language generation task to generate text for structured data. Table-to-text belongs to the data-to-text generation (Reiter and Dale, 2000). Many previous work (Barzilay and Lapata, 2005, 2006; Liang et al., 2009) treated the task as a pipelined systems, which viewed content selection and surface realization as two separate tasks. Duboue and McKeown (2002) proposed a clustering approach in the biography domain by scoring the semantic relevance of the text and paired knowledge base. In a similar vein, Barzilay and Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Onto"
P19-1600,W13-0108,0,0.032971,"and surface realization as two separate tasks. Duboue and McKeown (2002) proposed a clustering approach in the biography domain by scoring the semantic relevance of the text and paired knowledge base. In a similar vein, Barzilay and Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey"
P19-1600,D18-1426,0,0.0208961,"led the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descripti"
P19-1600,E17-1060,0,0.201101,"(2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al. (2016); Tu et al. (2016); Liu et al. (2017a); Gong et al. (2018) in the sense that they emphasis easily ignored (usually less frequent) features or bits of information in the training procedure by smoothing or regularization. The greatest difference between our work and theirs is"
P19-1600,P13-2121,0,0.0111481,"r datasets (with or without force-attention module), respectively. We replace UNK tokens with the most relevant token in the source table according to the attention matrix (Jean et al., 2015). 4.4 Table 2: BLEU and ROUGE scores on the WIKIBIO and WB-filter datasets. The baselines with * are based on our implementation while the others are reported by their authors. Models with † are trained using the RL criterion specified in Sec 3.2.2 while the remaining models are trained using the maximum likelihood estimate (MLE). Baselines KN & Template KN: A template-based KneserNey (KN) language model (Heafield et al., 2013) The extracted template for Table 1 is “name 1 name 2 (born birthdate 1 · · · ”. During inference, the decoder is constrained to emit words from the vocabulary or the special tokens in the tables. Table NLM: Lebret et al. (2016) proposed a neural language model Table NLM taking the attribute information into consideration. Order-planning: Sha et al. (2018) proposed a link matrix to model the order for the attributevalue tuples while generating biographies. Struct-aware: Liu et al. (2018) proposed a structure-aware model using a modified LSTM unit and a specific attention mechanism to incorpora"
P19-1600,P82-1020,0,0.797622,"Missing"
P19-1600,N18-2098,0,0.0182192,"e verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al. (2016); Tu et al."
P19-1600,P15-1001,0,0.0141721,"a, 2014), respectively. We use Xavier initialization (Glorot and Bengio, 2010) for all the parameters in our model. The global constraint of force-attention (Eq 4) is adapted after 4 and 1.5 epochs of training to avoid hurting the primary loss for the WIKIBIO and WB-filter datasets, respectively. Before the richness-oriented reinforced training, the neural generator is pre-trained 8 and 4 epochs for the WIKIBIO and WB-filter datasets (with or without force-attention module), respectively. We replace UNK tokens with the most relevant token in the source table according to the attention matrix (Jean et al., 2015). 4.4 Table 2: BLEU and ROUGE scores on the WIKIBIO and WB-filter datasets. The baselines with * are based on our implementation while the others are reported by their authors. Models with † are trained using the RL criterion specified in Sec 3.2.2 while the remaining models are trained using the maximum likelihood estimate (MLE). Baselines KN & Template KN: A template-based KneserNey (KN) language model (Heafield et al., 2013) The extracted template for Table 1 is “name 1 name 2 (born birthdate 1 · · · ”. During inference, the decoder is constrained to emit words from the vocabulary or the sp"
P19-1600,P18-1154,0,0.0122378,"its of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al"
P19-1600,N18-2101,0,0.101198,"nd Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate"
P19-1600,D16-1032,0,0.0320047,"et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al. (2016); Tu et al. (2016); Liu et al. (2017a); Gong et al. (2018) in the sense that they emphasis easily ignored (usually less frequent) features or bits of information in the training procedure by smoothing or regularization. The greatest difference between our work and theirs is that our method is tailored for covering the key information embedded in the attributes (entries) of the key-value tables rather than single words or labels. Although the deficient score of Tu et al. (2016) in Table 2 has demonstrated that word-level coverage oriented methods may not still be suitable to the structured tabl"
P19-1600,D16-1128,0,0.0981335,"Missing"
P19-1600,C18-1089,0,0.0165885,"ey Attri.: A soccer player who plays as forward Groundless info: A Utah forward in the national team Less Informative: An American forward Table 1: An example for comprehensive generation. Suppose we only have two attribute-value tuples, the underlined content is groundless information not mentioned in source tables. Introduction Generating descriptions for the factual attributevalue tables has attracted widely interests among NLP researchers especially in a neural end-to-end fashion (e.g. Lebret et al. (2016); Liu et al. (2018); Sha et al. (2018); Bao et al. (2018); Puduppully et al. (2018); Li and Wan (2018); Nema et al. (2018)) as shown in Fig 1a. For broader potential applications in this field, we also simulate useroriented generation, whose goal is to provide comprehensive generation for the selected attributes according to particular user interests like Fig 1b. However, we find that previous models might miss key information and generate less informative and groundless content in its generated descriptions towards source tables. For example, in Table 1, the ‘missing key attribute’ case doesn’t mention where the player comes from (birthplace) while the ‘less informative’ one chooses American"
P19-1600,P09-1011,0,0.138946,"over much more comprehensive information than the previous models (Table 2 and 3). Some implicitly expressed (like if a person is retired or not) or rarely covered (like ‘spouse’ or ‘high school’) attributes in the source tables might still be ignored in the descriptions generated by our model. Furthermore, those pieces of information which need some form of inference across Related Work Data-to-text a language generation task to generate text for structured data. Table-to-text belongs to the data-to-text generation (Reiter and Dale, 2000). Many previous work (Barzilay and Lapata, 2005, 2006; Liang et al., 2009) treated the task as a pipelined systems, which viewed content selection and surface realization as two separate tasks. Duboue and McKeown (2002) proposed a clustering approach in the biography domain by scoring the semantic relevance of the text and paired knowledge base. In a similar vein, Barzilay and Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data a"
P19-1600,N19-1263,0,0.0153354,"al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al. (2016); Tu et al. (2016); Liu et al. (2017a); Gong et al. (20"
P19-1600,D17-1189,1,0.842031,"et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al. (2016); Tu et al. (2016); Liu et al"
P19-1600,W16-6624,0,0.0268522,"the text and paired knowledge base. In a similar vein, Barzilay and Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repeti"
P19-1600,N16-1086,0,0.0618734,"ween data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al. (2016); Tu et al. (2016); Liu et al. (2017a); Gong et al. (2018) in the sense that they emphasis easily ignored (usually less frequent) features or bits of information in the train"
P19-1600,N18-1139,0,0.545422,"player who plays as forward Groundless info: A Utah forward in the national team Less Informative: An American forward Table 1: An example for comprehensive generation. Suppose we only have two attribute-value tuples, the underlined content is groundless information not mentioned in source tables. Introduction Generating descriptions for the factual attributevalue tables has attracted widely interests among NLP researchers especially in a neural end-to-end fashion (e.g. Lebret et al. (2016); Liu et al. (2018); Sha et al. (2018); Bao et al. (2018); Puduppully et al. (2018); Li and Wan (2018); Nema et al. (2018)) as shown in Fig 1a. For broader potential applications in this field, we also simulate useroriented generation, whose goal is to provide comprehensive generation for the selected attributes according to particular user interests like Fig 1b. However, we find that previous models might miss key information and generate less informative and groundless content in its generated descriptions towards source tables. For example, in Table 1, the ‘missing key attribute’ case doesn’t mention where the player comes from (birthplace) while the ‘less informative’ one chooses American rather than Utah. Th"
P19-1600,P02-1040,0,0.105184,"Missing"
P19-1600,N18-1137,0,0.347035,"ayer) Comprehensive: A Utah soccer player who plays as forward Missing Key Attri.: A soccer player who plays as forward Groundless info: A Utah forward in the national team Less Informative: An American forward Table 1: An example for comprehensive generation. Suppose we only have two attribute-value tuples, the underlined content is groundless information not mentioned in source tables. Introduction Generating descriptions for the factual attributevalue tables has attracted widely interests among NLP researchers especially in a neural end-to-end fashion (e.g. Lebret et al. (2016); Liu et al. (2018); Sha et al. (2018); Bao et al. (2018); Puduppully et al. (2018); Li and Wan (2018); Nema et al. (2018)) as shown in Fig 1a. For broader potential applications in this field, we also simulate useroriented generation, whose goal is to provide comprehensive generation for the selected attributes according to particular user interests like Fig 1b. However, we find that previous models might miss key information and generate less informative and groundless content in its generated descriptions towards source tables. For example, in Table 1, the ‘missing key attribute’ case doesn’t mention where th"
P19-1600,W18-6532,0,0.0313192,"Missing"
P19-1600,J18-3002,0,0.0261587,"rmation richness (Eq 5) as one of our automatic evaluation. ‘Hit at least 1 word’ for an attribute means that a biography has at least one overlapping word with the words (or their synonyms) in that attribute, which are lemmatized and filtered by a stop-words list like the way we get WB-filter in Sec 4.1. ‘HIT-1 coverage’ for an attribute is the ratio of the instances involving that attribute whose biographies ‘Hit at least 1 word’ in that attribute. Human Evaluation: Since automatic evaluations like BLEU may not be reliable for NLG systems (Callison-Burch et al., 2006; Reiter and Belz, 2009; Reiter, 2018). We use human evaluation which involves the generation fluency, coverage (how much given information in the infobox is mentioned in the related biography) and correctness (how much false or irrelevant information is mentioned in the biography). We firstly sampled 300 generated biographies from the generators for human evaluation. After that, we hired 3 thirdparty crowd-workers who are equipped with sufficient background knowledge to rank the given biographies. We present the generated descriptions to the annotators in a randomized order and ask them to be objective and not to guess which syst"
P19-1600,J09-4008,0,0.0604034,"boxes, we also use information richness (Eq 5) as one of our automatic evaluation. ‘Hit at least 1 word’ for an attribute means that a biography has at least one overlapping word with the words (or their synonyms) in that attribute, which are lemmatized and filtered by a stop-words list like the way we get WB-filter in Sec 4.1. ‘HIT-1 coverage’ for an attribute is the ratio of the instances involving that attribute whose biographies ‘Hit at least 1 word’ in that attribute. Human Evaluation: Since automatic evaluations like BLEU may not be reliable for NLG systems (Callison-Burch et al., 2006; Reiter and Belz, 2009; Reiter, 2018). We use human evaluation which involves the generation fluency, coverage (how much given information in the infobox is mentioned in the related biography) and correctness (how much false or irrelevant information is mentioned in the biography). We firstly sampled 300 generated biographies from the generators for human evaluation. After that, we hired 3 thirdparty crowd-workers who are equipped with sufficient background knowledge to rank the given biographies. We present the generated descriptions to the annotators in a randomized order and ask them to be objective and not to g"
P19-1600,P16-1008,0,0.275522,"nted reinforcement learning to produce accurate, informative and loyal descriptions. 3.1 Force-Attention Module For ‘missing key attributes’ problem (Table 1), we find that the generator usually focuses on particular attributes while the other attributes have relatively low attention values in the entire decoding procedure. So force attention method is proposed to guide the decoder to pay more attention to the previous uncovered attributes with low attention values to avoid potential key attribute missing. Note that FA method focuses on attributelevel coverage rather than word-level coverage (Tu et al., 2016) as our goal is to reduce the ‘missing key attributes’ phenomenons instead of building rigid word-by-word alignment between tables and descriptions. Stepwise Forcing Attention: We define attributeP level attention βtak = avg( xi ∈ak αti ) at the t-th step for attribute ak as the average value of the word-level attention values for the words in that attribute. The word-level coverage is defined as the sum of attention vector before the t-th step i θti = θt−1 + αti (Tu et al., 2016). In the similar way, we define the attribute-level coverage ak γtak = γt−k + βtak as the overall attention for att"
P19-1600,D17-1239,0,0.0943602,"Missing"
P19-1600,D18-1356,0,0.0863405,"Missing"
P19-1603,S18-1032,0,0.0289074,"Missing"
P19-1603,N18-2008,0,0.0522299,"ces a policy gradient learning approach to ensure that the model ends with a specific type of event given in advance. Yao et al. (2018b) uses manually annotated story data to control the ending valence and storyline of story generation. Different from them, our proposed framework can acquire distant sentiment labels without the dependence on the human annotations. Sentimental Text Generation Generating sentimental and emotional texts is a key step towards building intelligent and controllable natural language generation systems. To date several works of dialogue generation (Zhou et al., 2018; Huang et al., 2018; Zhou and Wang, 2018) and text sentiment transfer task (Li et al.; Luo et al., 2019) have studied on generating emotional or sentimental text. They always pre-define a binary sentiment label (positive/negative) or a small limited set of emotions, such as “anger”, “love”. Different from them, controlling the fine-grained sentiment (a numeric value) for story ending generation is not limited to several emotional labels, thus we can not embed each sentiment label into a separate vector as usual. Therefore, we propose to introduce the numeric sentiment value via a Gaussian Kernel Layer. 6 Conclus"
P19-1603,N18-1169,0,0.078025,"control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.1 1 Target Sentiment Story ending generation aims at completing the plot and concluding a story given a story context. Previous works mainly study on how to generate a coherent, reasonable and diversified story ending (Li et al., 2018; Guan et al., 2018; Xu et al., 2018). However, few of them focus on controllable story ending generation, especially ⇤ 0.1 She still lost the game and was very upset. 0.3 She almost won the game, but eventually lost. 0.5 The game ended with a draw. 0.7 She eventually won the game. 0.9 She won the game and was very proud of her team. Figure 1: An example of the input story context and output story endings for this task. All of the story endings are coherent with the story context but express different sentiment intensities. Introduction Equal Contribution. Our code and data can be found at htt"
P19-1603,C18-1088,0,0.0894506,"control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.1 1 Target Sentiment Story ending generation aims at completing the plot and concluding a story given a story context. Previous works mainly study on how to generate a coherent, reasonable and diversified story ending (Li et al., 2018; Guan et al., 2018; Xu et al., 2018). However, few of them focus on controllable story ending generation, especially ⇤ 0.1 She still lost the game and was very upset. 0.3 She almost won the game, but eventually lost. 0.5 The game ended with a draw. 0.7 She eventually won the game. 0.9 She won the game and was very proud of her team. Figure 1: An example of the input story context and output story endings for this task. All of the story endings are coherent with the story context but express different sentiment intensities. Introduction Equal Contribution. Our code and data can be found at htt"
P19-1603,D18-1420,0,0.0224517,"oid(wT (U · WU + bU )) (5) where 2 is the variance, S maps the sentiment embedding into a real value, the target sentiment intensity s is the mean of the Gaussian distribution, WU and bU are trainable parameters. 3.2 Baselines Since there is no direct related work of this task, we design an intuitive pipeline (generate-andmodify) as baseline. It first generates a story ending using a general sequence-to-sequence model with attention (Luong et al., 2015), and then modifies the sentiment of the story ending towards the target sentiment intensity via a fine-grained sentiment modification method (Liao et al., 2018). We call this baseline Seq2Seq + SentiMod. 3.3 Experiment Settings We tune hyper-parameters on the validation set. For the RM and DA sentiment analyzer, we implement the encoder as a 3-layer bidirectional LSTM with a hidden size of 512. We implement the regression module as a MLP with 1 hidden layer of size 32. For domain adaption, we implement a domain discriminator as a MLP with 1 hidden layer of size 32. A Gradient Reversal Layer is added into the domain discriminator. For the sentimental generator, both the semantic and sentiment embeddings are 256 dimensions and randomly initialized. We"
P19-1603,D15-1166,0,0.0213571,"th the target sentiment intensity s. As shown in Figure 3, the probability of generating a target word P is composed of two probabilities: P (yt ) = ↵PR (yt ) + PS (yt ) (1) where PR (yt ) denotes the semantic generation probability, PS (yt ) denotes the sentiment generation probability, ↵ and are trainable coefficients. Specifically, PR (yt ) is defined as follow: PR (yt = w) = wT (WR · hyt + bR ), ht = LSTM(yt 1 , h t 1 , ct ) (2) (3) where w is a one-hot indicator vector of word w, WR and bR are trainable parameters, ht is the t-th hidden state of the LSTM decoder with attention mechanism (Luong et al., 2015). PS (yt ) measures the generation probability of the target word given the target sentiment intensity s. For all words, beyond their semantic embeddings, they also have sentiment embeddings U. The sentiment embeddings of words reflect their sentiment properties. A Gaussian Kernel Layer (Luong et al., 2015; Zhang et al., 2018) is used to encourage words with sentiment intensity near to target sentiment s, and PS (yt ) is defined as follow: ✓ ◆ 1 ( S (Uw) s)2 PS (yt = w) = p exp 2 2 2⇡ (4) S (U, w) Dataset = sigmoid(wT (U · WU + bU )) (5) where 2 is the variance, S maps the sentiment embedding"
P19-1603,N16-1098,0,0.0491088,"timent intensities s. It consists of an encoder and a decoder equipped with a Gaussian Kernel Layer. The encoder is to map the input story context x into a compact vector that can capture its essential context features. Specifically, we use a normal bi-directional LSTM as the encoder. All context words xi are represented by their semantic embeddings E as the input and we use the concatenation of final forward and backward hidden states as the initial hidden state of the decoder. 6021 3 Decoder ?? y?? = ?????? ???? + ?????? ???? Experiment 3.1 ℎ??−1 We choose the widely-used ROCStories corpus (Mostafazadeh et al., 2016) which consists of 100k five-sentence stories. We split the data into a training set with 93,126 stories, a validation set with 5,173 stories and a test set with 5,175 stories. Gaussian Kernel Layer ???? Semantic Embeddings Sentiment Embeddings Target Sentiment Intensity Figure 3: The decoder of the sentimental generator. A Gaussian Kernel Layer is introduced to make use of the target sentiment intensity. The decoder aims to generate a story ending which accords with the target sentiment intensity s. As shown in Figure 3, the probability of generating a target word P is composed of two probabi"
P19-1603,P02-1040,0,0.104057,"story endings in the test set (H-M SentiCons). To evaluate the performance of sentimental generator, for each story context in the test set, we generate five story endings with five target sentiment intensity ranging from [0, 1]. Then we calculate SentiCons of input target sentiment intensities and sentiment intensities of the outputs predicted by the best sentiment analyzer (I-O SentiCons). BLEU: For each story in the test set, we take the context x and the human-annotated sentiment intensity s of the gold story ending y as input. The ˆ Then we calculate the corresponding output is y. BLEU (Papineni et al., 2002) score of y and yˆ as the overall quality of the generated story endings. 3.4.2 Human Evaluation We hire two evaluators who are skilled in English to evaluate the generated story endings. For each story in the test set, we distribute the story context, five target sentiment intensities and corresponding generated story endings to the evaluators. Evaluators are required to score the generated endings from 1 to 5 in terms of three criteria: Coherency, Fluency and Sentiment. Coherency measures whether the endings are coherent with the context. Fluency measures whether the endings are fluent. Sent"
P19-1603,D13-1170,0,0.00606287,"detailed configurations in each module. 2.2 Sentiment Analyzer The sentiment analyzer S aims to predicting the sentiment intensity s of the gold story ending y to construct paired data (x, s; y). As the first attempt to solve the proposed task, we explore three kinds of sentiment analyzers as follows. Rule-based (RB): VADER (Hutto and Gilbert, 2014) is an rule-based unsupervised model for sentiment analysis. We use it to extract the sentiment intensity s of y and then scale s to [0, 1]. Regression Model (RM): We first train a linear regression model R on the Stanford Sentiment Treebank (SST) (Socher et al., 2013) dataset, which is widely-used for sentiment analysis. Then we use R to acquire the sentiment intensity of y. Domain-Adversarial (DA): In the absence of sentiment annotations for the story dataset, domain adaptation can provide an effective solution since there exists some labeled datasets of a similar task but from a different domain. We use adversarial learning (Ganin and Lempitsky, 2015) to extract a domain-independent feature which not only performs well in the SST sentiment regression task but also misleads the domain discriminator. Finally, we use the adapted regression model to acquire"
P19-1603,D18-1462,1,0.934018,"To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.1 1 Target Sentiment Story ending generation aims at completing the plot and concluding a story given a story context. Previous works mainly study on how to generate a coherent, reasonable and diversified story ending (Li et al., 2018; Guan et al., 2018; Xu et al., 2018). However, few of them focus on controllable story ending generation, especially ⇤ 0.1 She still lost the game and was very upset. 0.3 She almost won the game, but eventually lost. 0.5 The game ended with a draw. 0.7 She eventually won the game. 0.9 She won the game and was very proud of her team. Figure 1: An example of the input story context and output story endings for this task. All of the story endings are coherent with the story context but express different sentiment intensities. Introduction Equal Contribution. Our code and data can be found at https://github. com/luofuli/sentimental-"
P19-1603,W18-1505,0,0.0845105,"ll lost the game and was very upset. 0.3 She almost won the game, but eventually lost. 0.5 The game ended with a draw. 0.7 She eventually won the game. 0.9 She won the game and was very proud of her team. Figure 1: An example of the input story context and output story endings for this task. All of the story endings are coherent with the story context but express different sentiment intensities. Introduction Equal Contribution. Our code and data can be found at https://github. com/luofuli/sentimental-story-ending 1 Generated Story Endings controlling the sentiment for story ending generation. Yao et al. (2018b) is the only work on controlling the sentiment for story ending generation. However, their work needs manually label the story dataset with sentiment labels (happy, sad, unknown), which is time-consuming and laborintensive. What’s more, they only focus on coarsegrained sentiment. Different from previous work, we propose the task of controlling the sentiment for story ending generation at a fine-grained level, without any human annotation of story dataset2 . Take Figure 1 as an example, given the same story context, our goal is to generate a story ending that satisfies the given sentiment int"
P19-1603,P18-1102,0,0.0569065,"fically, PR (yt ) is defined as follow: PR (yt = w) = wT (WR · hyt + bR ), ht = LSTM(yt 1 , h t 1 , ct ) (2) (3) where w is a one-hot indicator vector of word w, WR and bR are trainable parameters, ht is the t-th hidden state of the LSTM decoder with attention mechanism (Luong et al., 2015). PS (yt ) measures the generation probability of the target word given the target sentiment intensity s. For all words, beyond their semantic embeddings, they also have sentiment embeddings U. The sentiment embeddings of words reflect their sentiment properties. A Gaussian Kernel Layer (Luong et al., 2015; Zhang et al., 2018) is used to encourage words with sentiment intensity near to target sentiment s, and PS (yt ) is defined as follow: ✓ ◆ 1 ( S (Uw) s)2 PS (yt = w) = p exp 2 2 2⇡ (4) S (U, w) Dataset = sigmoid(wT (U · WU + bU )) (5) where 2 is the variance, S maps the sentiment embedding into a real value, the target sentiment intensity s is the mean of the Gaussian distribution, WU and bU are trainable parameters. 3.2 Baselines Since there is no direct related work of this task, we design an intuitive pipeline (generate-andmodify) as baseline. It first generates a story ending using a general sequence-to-sequ"
P19-1603,P18-1104,0,0.0337842,"t learning approach to ensure that the model ends with a specific type of event given in advance. Yao et al. (2018b) uses manually annotated story data to control the ending valence and storyline of story generation. Different from them, our proposed framework can acquire distant sentiment labels without the dependence on the human annotations. Sentimental Text Generation Generating sentimental and emotional texts is a key step towards building intelligent and controllable natural language generation systems. To date several works of dialogue generation (Zhou et al., 2018; Huang et al., 2018; Zhou and Wang, 2018) and text sentiment transfer task (Li et al.; Luo et al., 2019) have studied on generating emotional or sentimental text. They always pre-define a binary sentiment label (positive/negative) or a small limited set of emotions, such as “anger”, “love”. Different from them, controlling the fine-grained sentiment (a numeric value) for story ending generation is not limited to several emotional labels, thus we can not embed each sentiment label into a separate vector as usual. Therefore, we propose to introduce the numeric sentiment value via a Gaussian Kernel Layer. 6 Conclusion and Future Work In"
W08-2135,J96-1002,0,0.00943365,"S tag of the candidate Lemma and POS of Neighboring words of the candidate Lemma and POS of sibling words of the candidate Length of the constituent headed by the candidate Lemma and POS of the left and right most words of the constituent of the candidate Conjunction of lemma of candidates and predicates; Conjunction of POS of candidates and predicates POS Pattern of all candidates Table 3: Features used to predict syntactic dependency parsing 3.3 Probability Estimation The system defines the conditional probability p(si |si−1 , di ; x) and p(di |si−1 , di−1 ; x) by using the maximum entropy (Berger et al., 1996) framework Denote the tag set of syntactic dependency relations D and the tag set of semantic dependency relations S. Formally, given a feature map φs and a weight vector ws , lists the features used to predict semantic dependency relations, whereas table 3 lists the features used to predict the syntactic dependency relations. The features used for syntactic dependency relation classification are strongly based on previous works (McDonald et al., 2006; Nakagawa, 2007). We just integrate syntactic dependency Relation classification and semantic dependency relation here. If one combines identifi"
W08-2135,W05-0602,0,0.0618226,"Missing"
W08-2135,J02-3001,0,0.0165755,"ered to a category defined by its first character POS Pattern (string of POS tags) of all candidates Single Character POS Pattern of all candidates Table 2: Features used for semantic role labeling 2.4 Semantic Dependency Relation Classification This stage assigns the final argument labels to the argument candidates supplied from the previous stage. A multi-class classifier is trained to classify the types of the arguments supplied by the previous stage. Table 2 lists the features used. It is clear that the general type of features used here is strongly based on previous work on the SRL task (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). Different from CoNLL-2005, the sense of predicates should be labeled as a part of the task. Our system assigns 01 to all predicates. This is a harsh tactic since it do not take the linguistic meaning of the argument-structure into account. 2.5 Semantic Dependency Relation Inference The purpose of inference stage is to incorporate some prior linguistic and structural knowledge, such as ”each predicate takes at most one argument of each type.” We use the inference process intro244 duced by (Punyakanok et al., 2004; Koomen et al., 2005). The process"
W08-2135,W05-0625,0,0.0143518,"RL task (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). Different from CoNLL-2005, the sense of predicates should be labeled as a part of the task. Our system assigns 01 to all predicates. This is a harsh tactic since it do not take the linguistic meaning of the argument-structure into account. 2.5 Semantic Dependency Relation Inference The purpose of inference stage is to incorporate some prior linguistic and structural knowledge, such as ”each predicate takes at most one argument of each type.” We use the inference process intro244 duced by (Punyakanok et al., 2004; Koomen et al., 2005). The process is modeled as an integer Linear Programming Problem (ILP). It takes the predicted probability over each type of the arguments as inputs, and takes the optimal solution that maximizes the linear sum of the probability subject to linguistic constraints as outputs. The constraints are a subset of constraints raised by Koomen et al. (2005) and encoded as following: 1) No overlapping or embedding arguments; 2) No duplicate argument classes for A0-A5; 3) If there is an R-arg argument, then there has to be an arg argument; 4) If there is a C-arg argument, there must be an arg argument;"
W08-2135,H05-1066,0,0.124756,"Missing"
W08-2135,W06-2932,0,0.0225129,"obability Estimation The system defines the conditional probability p(si |si−1 , di ; x) and p(di |si−1 , di−1 ; x) by using the maximum entropy (Berger et al., 1996) framework Denote the tag set of syntactic dependency relations D and the tag set of semantic dependency relations S. Formally, given a feature map φs and a weight vector ws , lists the features used to predict semantic dependency relations, whereas table 3 lists the features used to predict the syntactic dependency relations. The features used for syntactic dependency relation classification are strongly based on previous works (McDonald et al., 2006; Nakagawa, 2007). We just integrate syntactic dependency Relation classification and semantic dependency relation here. If one combines identification and classification of semantic roles as one multi-class classification, the tag set of the second layer can be substituted by the tag set of semantic roles plus a NULL (”not an argument”) label. 3.4 Inference The ”argmax problem” in structured prediction is not tractable in the general case. However, the bilayer graphical model presented in form sections admits efficient search using dynamic programming solution. Searching for the highest proba"
W08-2135,D07-1100,0,0.0296719,"Missing"
W08-2135,C04-1197,0,0.0652386,"Missing"
W08-2135,W04-3212,0,0.162513,"POS Pattern (string of POS tags) of all candidates Single Character POS Pattern of all candidates Table 2: Features used for semantic role labeling 2.4 Semantic Dependency Relation Classification This stage assigns the final argument labels to the argument candidates supplied from the previous stage. A multi-class classifier is trained to classify the types of the arguments supplied by the previous stage. Table 2 lists the features used. It is clear that the general type of features used here is strongly based on previous work on the SRL task (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). Different from CoNLL-2005, the sense of predicates should be labeled as a part of the task. Our system assigns 01 to all predicates. This is a harsh tactic since it do not take the linguistic meaning of the argument-structure into account. 2.5 Semantic Dependency Relation Inference The purpose of inference stage is to incorporate some prior linguistic and structural knowledge, such as ”each predicate takes at most one argument of each type.” We use the inference process intro244 duced by (Punyakanok et al., 2004; Koomen et al., 2005). The process is modeled as an integer Linear Programming P"
W08-2135,W05-0639,0,0.0372408,"Missing"
W08-2135,W08-2121,0,\N,Missing
W08-2135,D07-1096,0,\N,Missing
W12-6307,I05-3017,0,0.0339892,"w experiments of evaluation methods. The final corpora consist of 5000 sentences (or articles, strictly. For simplicity, we refer to the individual article as a sentence, since most of the MicroBlog articles consist of only one sentence.) For evaluation, we adopt the evaluation method used in previous bake-off tasks, and use precision, recall and F-measure to measure the overPreface After years of intensive researches, Chinese word segmentation has achieved a quite high precision. Five prior word segmentation bakeoffs, have been successfully conducted in 2003 (Sproat and Emerson, 2003), 2005 (Emerson, 2005), 2006 (Levow, 2006), 2007 (Jin and Chen, 2007) and 2012 (Zhao and Liu, 2010). These evaluations have established benchmarks for word segmentation with which researchers could evaluate their segmentation system. However, the performance of segmentation is not so satisfying for the MicroBlog corpora. The corpus of a specific domain may have its characteristics in vocabulary, sentence pattern and style. MicroBlog makes no exception. The MicroBlog texts are much similar to oral expression, with a casual style and less deliberation in writing, resulting in a simple and comfortable style: the Micro"
W12-6307,W06-0115,0,0.078264,"ation methods. The final corpora consist of 5000 sentences (or articles, strictly. For simplicity, we refer to the individual article as a sentence, since most of the MicroBlog articles consist of only one sentence.) For evaluation, we adopt the evaluation method used in previous bake-off tasks, and use precision, recall and F-measure to measure the overPreface After years of intensive researches, Chinese word segmentation has achieved a quite high precision. Five prior word segmentation bakeoffs, have been successfully conducted in 2003 (Sproat and Emerson, 2003), 2005 (Emerson, 2005), 2006 (Levow, 2006), 2007 (Jin and Chen, 2007) and 2012 (Zhao and Liu, 2010). These evaluations have established benchmarks for word segmentation with which researchers could evaluate their segmentation system. However, the performance of segmentation is not so satisfying for the MicroBlog corpora. The corpus of a specific domain may have its characteristics in vocabulary, sentence pattern and style. MicroBlog makes no exception. The MicroBlog texts are much similar to oral expression, with a casual style and less deliberation in writing, resulting in a simple and comfortable style: the MicroBlog style. Like oth"
W12-6307,W03-1719,0,\N,Missing
W12-6307,I08-4010,0,\N,Missing
W12-6307,W03-1726,0,\N,Missing
W14-6814,Y98-1020,0,0.196662,"Missing"
W14-6814,W10-4126,0,\N,Missing
W14-6814,W12-6307,1,\N,Missing
W99-0618,P97-1003,0,0.104519,"Missing"
W99-0618,P96-1025,0,\N,Missing
Y09-2011,J96-1002,0,0.00651777,"n all the system improvements, we perform a binominal test of significance at p=0.05, and all significant improvements are marked with a *. From this table, we can see that the most effective features are those so called structural features. 5.3 Classifier Performance Table 5: F-measures for different classifiers. SNoW MaxEnt SVM Syntactic devel. test 96.92 96.60 97.15 96.53 97.44 96.94 Semantic devel. test 82.28 84.19 85.74 86.72 85.46 87.19 We experimented with three popular machine learning algorithms: Support Vector Machine classifier (SVM) (Vapnik, 1998), Maximum Entropy classifier (ME) (Berger et al., 1996), and Sparse Network of Winnows (SNoW) (Roth, 1998). For each algorithm we use the same set of features. In terms of SVM, we used TinySVM3 . All SVM classifiers were realized with default parameters. One-Vs-All strategy is used to solve multi-class classification problem. For ME model, we use maxent4 . For SNoW model, we use UIUC SNoW toolkit5 . In training, SNoW’s default parameters are used with the exception of the separator thickness 1.5, the use of average weight vector, and 5 training cycles. Table 5 shows the classification performance of different classifiers, both on test and developm"
Y09-2011,A00-2031,0,0.0239594,"Missing"
Y09-2011,P05-1022,0,0.0277018,". Some sparse tags cannot be accurately identified, such as Q. The recall, in general, performs worse than precision. This is mainly for that the negative samples (syntactic nodes are not assigned any function label) is much more than the positive sample. 5.5 Using Automatic Parses The results in former experiments are based on the use of hand-crafted parses. In practical use, of course, automatic parses will not be as accurate. To gauge the tagging performance in realistic situation, in this section, we report experiments on function tag labeling with automatic parsing information. Charniak (Charniak and Johnson, 2005) parser, which is ported to Chinese, are used to produce full parses. The parser is re-trained using the same training and development data in function tagging. Table 8 summarizes the parsing performance of Charniak parser. Table 9 shows the parsing performance and the function tag labeling performance (evaluation metric was described in (Blaheta and Charniak, 2000)). 6 Conclusion Function tag assignment has been studied for English and Spanish. In this paper, we address the question of assigning function tags to parsed sentences in Chinese. We describe a Chinese 537 Table 8: Parsing performan"
Y09-2011,P06-2018,0,0.0395342,"Missing"
Y09-2011,N06-1024,0,0.016019,"BJ 43701 47410 EXT 1485 1544 FOC 247 247 PRD 6113 6117 SBJ 38995 65335 TPC 2291 3149 Miscellaneous Label APP 6762 6762 HLN 1501 1501 PN 28559 28581 SHORT* 60 60 TTL 887 891 WH 832 832 Semantic Label BNF 645 645 CND 624 624 DIR 2724 2739 IJ* 28 29 LGS 398 399 LOC 4684 4799 MNR 2424 2618 PRP 1109 1173 TMP 8254 8475 VOC* 20 20 Clause Type IMP* 49 49 Q 931 935 Discrepancy Type ADV 4400 4430 3 Method 3.1 Previous Work There are two main kinds of function assignment methods, which we call parsing method and labeling method. Parsing methods integrate function tag assignment into the parsing process (Gabbard et al., 2006; Merlo and Musillo, 2005), whereas labeling approaches take syntactic parsing as pre-processing and label function tags or NULL tag (which indicates the given constituent does not represent any function tags) to each syntax tree node. (Blaheta, 2004; Jijkoun and de Rijke, 2004; Chrupała and van Genabith, 2006). Gabbard et al. (2006) modify Collins parser’ model 2 to allow it to produce function tags without decreasing the parsing performance. In the original model function tags is deleted after being used to identify and mark arguments. Collins parser use function tags as part of the heuristi"
Y09-2011,P04-1040,0,0.0377766,"Missing"
Y09-2011,D07-1062,0,0.0250633,"4.89 95.06 94.87 95.07 95.03 95.03 95.26 95.23 94.96 *95.33 *95.41 94.94 94.92 94.98 *95.39 *96.87 94.97 95.08 94.89 94.83 94.86 94.73 94.98 95.06 Semantic 75.82 *78.24 77.22 76.49 *81.87 *78.48 *81.10 *76.17 76.98 *78.55 76.84 *81.26 *80.22 77.23 76.55 *78.23 *77.92 *78.09 *80.95 76.82 *78.79 *78.38 75.95 76.92 *77.36 *80.32 *78.02 *79.00 *78.34 *77.69 • Head Trace (htr) The sequential container of the head down upon the phrase. For example, the head word of IP-IPC is “d d/opening”; therefore this feature of IP-TPC is IP↓VP↓VP↓VV. This feature is very similar to etree feature in TAG grammar (Liu and Sarkar, 2007). • C-commander thread of the head C-commander2 thread features, raised by (Sun et al., 2008), are sequential containers of constituents which C-command the head word of the constituent. We design two C-commander threads: 1. all items in the thread are categories of the C-commanders (cct-c); 2. using the word content to occupy the head position (cct-w). For instance, in Figure 1, the noun phrase “dd/Guangxi” and the preposition phrase “d d/opening” are two left c-commanders of the head “dd/opening”, so the cct-w feature for IP-TPC is NP←PP←dd. 2 C-command is a concept in X-bar syntax theory. A"
Y09-2011,H05-1078,0,0.0176235,"5 1544 FOC 247 247 PRD 6113 6117 SBJ 38995 65335 TPC 2291 3149 Miscellaneous Label APP 6762 6762 HLN 1501 1501 PN 28559 28581 SHORT* 60 60 TTL 887 891 WH 832 832 Semantic Label BNF 645 645 CND 624 624 DIR 2724 2739 IJ* 28 29 LGS 398 399 LOC 4684 4799 MNR 2424 2618 PRP 1109 1173 TMP 8254 8475 VOC* 20 20 Clause Type IMP* 49 49 Q 931 935 Discrepancy Type ADV 4400 4430 3 Method 3.1 Previous Work There are two main kinds of function assignment methods, which we call parsing method and labeling method. Parsing methods integrate function tag assignment into the parsing process (Gabbard et al., 2006; Merlo and Musillo, 2005), whereas labeling approaches take syntactic parsing as pre-processing and label function tags or NULL tag (which indicates the given constituent does not represent any function tags) to each syntax tree node. (Blaheta, 2004; Jijkoun and de Rijke, 2004; Chrupała and van Genabith, 2006). Gabbard et al. (2006) modify Collins parser’ model 2 to allow it to produce function tags without decreasing the parsing performance. In the original model function tags is deleted after being used to identify and mark arguments. Collins parser use function tags as part of the heuristics for doing so. A followi"
Y09-2011,N04-1032,0,0.0851794,"hereas the proper noun classifier recognizes that phrase as a PN. Other classifiers, such as HLN predictor, should assign NULL label to this phrase. 4 Features 4.1 Baseline Features Our baseline system uses features introduced by Blaheta (2004): category, cc-category, head, head POS, alt head, alt head POS, category clusters. • Category This is the syntactic category (NP, VP, IP, etc.) of the constituent. • cc-Category If a candidate phrase is comprised of the conjunction of two or more XP, this feature is CCXP. • Head To extract the syntactic head of a phrase, we use head rules described in (Sun and Jurafsky, 2004). This set of head rules are very popular in Chinese parsing research, such as in (Duan et al., 2007; Zhang and Clark, 2008). • Head Word POS The part-of-speech of syntactic head. • Alt Head Word Many kinds of function tags, such as temporals and locatives, occur as prepositional phrases in a sentence, and it is often the case that the head words of those phrase, which are always prepositions, are not very discriminative, for example, “ddd/in the last year”, “ddd/in Beijing”, both share the same head word “d”, but the former is TMP whereas the latter is LOC. Alt Head Word feature is the head o"
Y09-2011,C08-1105,1,0.744974,"7 94.97 95.08 94.89 94.83 94.86 94.73 94.98 95.06 Semantic 75.82 *78.24 77.22 76.49 *81.87 *78.48 *81.10 *76.17 76.98 *78.55 76.84 *81.26 *80.22 77.23 76.55 *78.23 *77.92 *78.09 *80.95 76.82 *78.79 *78.38 75.95 76.92 *77.36 *80.32 *78.02 *79.00 *78.34 *77.69 • Head Trace (htr) The sequential container of the head down upon the phrase. For example, the head word of IP-IPC is “d d/opening”; therefore this feature of IP-TPC is IP↓VP↓VP↓VV. This feature is very similar to etree feature in TAG grammar (Liu and Sarkar, 2007). • C-commander thread of the head C-commander2 thread features, raised by (Sun et al., 2008), are sequential containers of constituents which C-command the head word of the constituent. We design two C-commander threads: 1. all items in the thread are categories of the C-commanders (cct-c); 2. using the word content to occupy the head position (cct-w). For instance, in Figure 1, the noun phrase “dd/Guangxi” and the preposition phrase “d d/opening” are two left c-commanders of the head “dd/opening”, so the cct-w feature for IP-TPC is NP←PP←dd. 2 C-command is a concept in X-bar syntax theory. Assuming α and β are two nodes in a syntax tree: α C-commands β means every parent of α is anc"
Y09-2011,D08-1059,0,0.0500208,"L label to this phrase. 4 Features 4.1 Baseline Features Our baseline system uses features introduced by Blaheta (2004): category, cc-category, head, head POS, alt head, alt head POS, category clusters. • Category This is the syntactic category (NP, VP, IP, etc.) of the constituent. • cc-Category If a candidate phrase is comprised of the conjunction of two or more XP, this feature is CCXP. • Head To extract the syntactic head of a phrase, we use head rules described in (Sun and Jurafsky, 2004). This set of head rules are very popular in Chinese parsing research, such as in (Duan et al., 2007; Zhang and Clark, 2008). • Head Word POS The part-of-speech of syntactic head. • Alt Head Word Many kinds of function tags, such as temporals and locatives, occur as prepositional phrases in a sentence, and it is often the case that the head words of those phrase, which are always prepositions, are not very discriminative, for example, “ddd/in the last year”, “ddd/in Beijing”, both share the same head word “d”, but the former is TMP whereas the latter is LOC. Alt Head Word feature is the head of the object of a prepositional phrase (and undefined for other sorts of constituents), which is designed to capture more in"
