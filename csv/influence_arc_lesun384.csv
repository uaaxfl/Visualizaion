2020.emnlp-main.592,N19-1078,0,0.027429,"… Test Test Wow...[Joker] was great! … at [Cherry Street]… Love [inception] so much. … go to [9th Avenue] … Figure 1: Comparison between regular NER benchmarks and open NER tasks in reality. 2016). Recently, neural network-based supervised models dominate the NER task. By supervised fine-tuning upon large-scale language model pretrained architectures (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), etc.), we have witnessed superior performances on almost all widely-used NER benchmarks, including CoNLL03, ACE2005 and TAC-KBP datasets (Li et al., 2019b; Akbik et al., 2019; Zhai et al., 2019; Li et al., 2019a). Introduction Named entity recognition (NER), or more generally name tagging, aims to identify text spans pertaining to specific entity types. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., *: Corresponding authors. Regular NER Typical Categories Despite the success of recent models, there are specific ad"
2020.emnlp-main.592,W03-0420,0,0.1777,"an et al., 2017; Lin et al., 2018; Xie et al., 2018; Lin et al., 2019b). By contrast, to the best of our knowledge, this is the first work which investigates the essential difference between regular and open NER. By conducting both randomization test (Edgington and Onghena, 2007) and verification experiments, we analyze the impact of name regularity, mention coverage and context pattern sufficiency and shed light on future open NER studies. Related Work Named entity recognition has long been studied and has attracted much attention. Conventional methods (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004) commonly rely on handcraft features to build NER models, which are hard to transfer among different languages, domains and entity types. Recently, deep learning methods, which automatically extract high-level features and perform sequence tagging with neural 6 Conclusion and Future work This paper investigates whether current state-ofthe-art models on regular NER can still work well on open NER. From the perspective of name regularity, mention coverage and context diversity, we conducted both randomization test and verification experiments to evaluate the generalization abilit"
2020.emnlp-main.592,D19-1025,0,0.141013,"chieved significant progress especially under strong pretraining and fine-tuning paradigm (Li et al., 2019b; Akbik et al., 2019; Zhai et al., 2019; Li et al., 2019a; Xia et al., 2019; Lin et al., 2019a). These methods have achieved promising results in almost all popular NER benchmarks considering regular entity types. Several researches have shift attention to name tagging in open scenarios, where entity types may have weaker name regularity and training data are often insufficient. These papers mainly focus on how to exploit weakly-supervised data (T¨ackstr¨om et al., 2013; Ni et al., 2017; Cao et al., 2019; Xue et al., 2019), or devoted to incorporate external resources (Yang et al., 2017; Peng and Dredze, 2016; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018; Lin et al., 2019b). By contrast, to the best of our knowledge, this is the first work which investigates the essential difference between regular and open NER. By conducting both randomization test (Edgington and Onghena, 2007) and verification experiments, we analyze the impact of name regularity, mention coverage and context pattern sufficiency and shed light on future open NER studies. Related Work Named entity recognition has lon"
2020.emnlp-main.592,C02-1025,0,0.0138805,"and Dredze, 2016; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018; Lin et al., 2019b). By contrast, to the best of our knowledge, this is the first work which investigates the essential difference between regular and open NER. By conducting both randomization test (Edgington and Onghena, 2007) and verification experiments, we analyze the impact of name regularity, mention coverage and context pattern sufficiency and shed light on future open NER studies. Related Work Named entity recognition has long been studied and has attracted much attention. Conventional methods (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004) commonly rely on handcraft features to build NER models, which are hard to transfer among different languages, domains and entity types. Recently, deep learning methods, which automatically extract high-level features and perform sequence tagging with neural 6 Conclusion and Future work This paper investigates whether current state-ofthe-art models on regular NER can still work well on open NER. From the perspective of name regularity, mention coverage and context diversity, we conducted both randomization test and verification experiments to evaluate the"
2020.emnlp-main.592,Q16-1026,0,0.0638972,"Missing"
2020.emnlp-main.592,P05-1053,0,0.132229,"pretrained architectures (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), etc.), we have witnessed superior performances on almost all widely-used NER benchmarks, including CoNLL03, ACE2005 and TAC-KBP datasets (Li et al., 2019b; Akbik et al., 2019; Zhai et al., 2019; Li et al., 2019a). Introduction Named entity recognition (NER), or more generally name tagging, aims to identify text spans pertaining to specific entity types. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., *: Corresponding authors. Regular NER Typical Categories Despite the success of recent models, there are specific advantages in current NER benchmarks which significantly facilitate supervised neural networks. First, these benchmarks focus on limited entity types, and most mentions of these types have strong name regularity. For example, nearly all person names follow the “FirstName LastName” or “LastName FirstName” patterns, while location na"
2020.emnlp-main.592,P08-1030,0,0.0630903,"ERT (Devlin et al., 2018), XLNet (Yang et al., 2019), etc.), we have witnessed superior performances on almost all widely-used NER benchmarks, including CoNLL03, ACE2005 and TAC-KBP datasets (Li et al., 2019b; Akbik et al., 2019; Zhai et al., 2019; Li et al., 2019a). Introduction Named entity recognition (NER), or more generally name tagging, aims to identify text spans pertaining to specific entity types. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., *: Corresponding authors. Regular NER Typical Categories Despite the success of recent models, there are specific advantages in current NER benchmarks which significantly facilitate supervised neural networks. First, these benchmarks focus on limited entity types, and most mentions of these types have strong name regularity. For example, nearly all person names follow the “FirstName LastName” or “LastName FirstName” patterns, while location names mostly end with indicator words such as “street” or “road”"
2020.emnlp-main.592,N16-1030,0,0.140127,"Missing"
2020.emnlp-main.592,P13-1008,0,0.0112708,"8), XLNet (Yang et al., 2019), etc.), we have witnessed superior performances on almost all widely-used NER benchmarks, including CoNLL03, ACE2005 and TAC-KBP datasets (Li et al., 2019b; Akbik et al., 2019; Zhai et al., 2019; Li et al., 2019a). Introduction Named entity recognition (NER), or more generally name tagging, aims to identify text spans pertaining to specific entity types. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., *: Corresponding authors. Regular NER Typical Categories Despite the success of recent models, there are specific advantages in current NER benchmarks which significantly facilitate supervised neural networks. First, these benchmarks focus on limited entity types, and most mentions of these types have strong name regularity. For example, nearly all person names follow the “FirstName LastName” or “LastName FirstName” patterns, while location names mostly end with indicator words such as “street” or “road”. Second, the trai"
2020.emnlp-main.592,P19-1129,0,0.023981,"… at [8th Avenue] … Test Test Wow...[Joker] was great! … at [Cherry Street]… Love [inception] so much. … go to [9th Avenue] … Figure 1: Comparison between regular NER benchmarks and open NER tasks in reality. 2016). Recently, neural network-based supervised models dominate the NER task. By supervised fine-tuning upon large-scale language model pretrained architectures (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), etc.), we have witnessed superior performances on almost all widely-used NER benchmarks, including CoNLL03, ACE2005 and TAC-KBP datasets (Li et al., 2019b; Akbik et al., 2019; Zhai et al., 2019; Li et al., 2019a). Introduction Named entity recognition (NER), or more generally name tagging, aims to identify text spans pertaining to specific entity types. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., *: Corresponding authors. Regular NER Typical Categories Despite the success of recent models,"
2020.emnlp-main.592,P19-1511,1,0.906858,"Missing"
2020.emnlp-main.592,D19-1646,1,0.8426,"Missing"
2020.emnlp-main.592,W12-3016,0,0.0840626,"Missing"
2020.emnlp-main.592,P18-1074,0,0.0174384,"019; Li et al., 2019a; Xia et al., 2019; Lin et al., 2019a). These methods have achieved promising results in almost all popular NER benchmarks considering regular entity types. Several researches have shift attention to name tagging in open scenarios, where entity types may have weaker name regularity and training data are often insufficient. These papers mainly focus on how to exploit weakly-supervised data (T¨ackstr¨om et al., 2013; Ni et al., 2017; Cao et al., 2019; Xue et al., 2019), or devoted to incorporate external resources (Yang et al., 2017; Peng and Dredze, 2016; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018; Lin et al., 2019b). By contrast, to the best of our knowledge, this is the first work which investigates the essential difference between regular and open NER. By conducting both randomization test (Edgington and Onghena, 2007) and verification experiments, we analyze the impact of name regularity, mention coverage and context pattern sufficiency and shed light on future open NER studies. Related Work Named entity recognition has long been studied and has attracted much attention. Conventional methods (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 200"
2020.emnlp-main.592,P19-1429,1,0.900061,"Missing"
2020.emnlp-main.592,D18-1019,0,0.0420815,"Missing"
2020.emnlp-main.592,P09-1113,0,0.0720049,"es (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), etc.), we have witnessed superior performances on almost all widely-used NER benchmarks, including CoNLL03, ACE2005 and TAC-KBP datasets (Li et al., 2019b; Akbik et al., 2019; Zhai et al., 2019; Li et al., 2019a). Introduction Named entity recognition (NER), or more generally name tagging, aims to identify text spans pertaining to specific entity types. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., *: Corresponding authors. Regular NER Typical Categories Despite the success of recent models, there are specific advantages in current NER benchmarks which significantly facilitate supervised neural networks. First, these benchmarks focus on limited entity types, and most mentions of these types have strong name regularity. For example, nearly all person names follow the “FirstName LastName” or “LastName FirstName” patterns, while location names mostly end with i"
2020.emnlp-main.592,D18-1124,0,0.0330632,"Missing"
2020.emnlp-main.592,P17-1135,0,0.0176861,"rd, 2019), have achieved significant progress especially under strong pretraining and fine-tuning paradigm (Li et al., 2019b; Akbik et al., 2019; Zhai et al., 2019; Li et al., 2019a; Xia et al., 2019; Lin et al., 2019a). These methods have achieved promising results in almost all popular NER benchmarks considering regular entity types. Several researches have shift attention to name tagging in open scenarios, where entity types may have weaker name regularity and training data are often insufficient. These papers mainly focus on how to exploit weakly-supervised data (T¨ackstr¨om et al., 2013; Ni et al., 2017; Cao et al., 2019; Xue et al., 2019), or devoted to incorporate external resources (Yang et al., 2017; Peng and Dredze, 2016; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018; Lin et al., 2019b). By contrast, to the best of our knowledge, this is the first work which investigates the essential difference between regular and open NER. By conducting both randomization test (Edgington and Onghena, 2007) and verification experiments, we analyze the impact of name regularity, mention coverage and context pattern sufficiency and shed light on future open NER studies. Related Work Named entity r"
2020.emnlp-main.592,P17-1178,0,0.0325496,"19; Zhai et al., 2019; Li et al., 2019a; Xia et al., 2019; Lin et al., 2019a). These methods have achieved promising results in almost all popular NER benchmarks considering regular entity types. Several researches have shift attention to name tagging in open scenarios, where entity types may have weaker name regularity and training data are often insufficient. These papers mainly focus on how to exploit weakly-supervised data (T¨ackstr¨om et al., 2013; Ni et al., 2017; Cao et al., 2019; Xue et al., 2019), or devoted to incorporate external resources (Yang et al., 2017; Peng and Dredze, 2016; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018; Lin et al., 2019b). By contrast, to the best of our knowledge, this is the first work which investigates the essential difference between regular and open NER. By conducting both randomization test (Edgington and Onghena, 2007) and verification experiments, we analyze the impact of name regularity, mention coverage and context pattern sufficiency and shed light on future open NER studies. Related Work Named entity recognition has long been studied and has attracted much attention. Conventional methods (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al.,"
2020.emnlp-main.592,P16-2025,0,0.0251205,"2019b; Akbik et al., 2019; Zhai et al., 2019; Li et al., 2019a; Xia et al., 2019; Lin et al., 2019a). These methods have achieved promising results in almost all popular NER benchmarks considering regular entity types. Several researches have shift attention to name tagging in open scenarios, where entity types may have weaker name regularity and training data are often insufficient. These papers mainly focus on how to exploit weakly-supervised data (T¨ackstr¨om et al., 2013; Ni et al., 2017; Cao et al., 2019; Xue et al., 2019), or devoted to incorporate external resources (Yang et al., 2017; Peng and Dredze, 2016; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018; Lin et al., 2019b). By contrast, to the best of our knowledge, this is the first work which investigates the essential difference between regular and open NER. By conducting both randomization test (Edgington and Onghena, 2007) and verification experiments, we analyze the impact of name regularity, mention coverage and context pattern sufficiency and shed light on future open NER studies. Related Work Named entity recognition has long been studied and has attracted much attention. Conventional methods (Zhou and Su, 2002; Chieu and Ng, 200"
2020.emnlp-main.592,N18-1202,0,0.0341887,"decent training instances to capture Fully-annotated training data is rare Context Pattern Examples Location Movie Train Train starting from [Cherry Street] I watched [avatar]last night …[the matrix] is the best... … at [8th Avenue] … Test Test Wow...[Joker] was great! … at [Cherry Street]… Love [inception] so much. … go to [9th Avenue] … Figure 1: Comparison between regular NER benchmarks and open NER tasks in reality. 2016). Recently, neural network-based supervised models dominate the NER task. By supervised fine-tuning upon large-scale language model pretrained architectures (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019), etc.), we have witnessed superior performances on almost all widely-used NER benchmarks, including CoNLL03, ACE2005 and TAC-KBP datasets (Li et al., 2019b; Akbik et al., 2019; Zhai et al., 2019; Li et al., 2019a). Introduction Named entity recognition (NER), or more generally name tagging, aims to identify text spans pertaining to specific entity types. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extractio"
2020.emnlp-main.592,D16-1264,0,0.0403711,"sed superior performances on almost all widely-used NER benchmarks, including CoNLL03, ACE2005 and TAC-KBP datasets (Li et al., 2019b; Akbik et al., 2019; Zhai et al., 2019; Li et al., 2019a). Introduction Named entity recognition (NER), or more generally name tagging, aims to identify text spans pertaining to specific entity types. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., *: Corresponding authors. Regular NER Typical Categories Despite the success of recent models, there are specific advantages in current NER benchmarks which significantly facilitate supervised neural networks. First, these benchmarks focus on limited entity types, and most mentions of these types have strong name regularity. For example, nearly all person names follow the “FirstName LastName” or “LastName FirstName” patterns, while location names mostly end with indicator words such as “street” or “road”. Second, the training and test data in these benchmarks are sampled from th"
2020.emnlp-main.592,W15-3904,0,0.07668,"Missing"
2020.emnlp-main.592,D18-1034,0,0.015116,"19a; Xia et al., 2019; Lin et al., 2019a). These methods have achieved promising results in almost all popular NER benchmarks considering regular entity types. Several researches have shift attention to name tagging in open scenarios, where entity types may have weaker name regularity and training data are often insufficient. These papers mainly focus on how to exploit weakly-supervised data (T¨ackstr¨om et al., 2013; Ni et al., 2017; Cao et al., 2019; Xue et al., 2019), or devoted to incorporate external resources (Yang et al., 2017; Peng and Dredze, 2016; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018; Lin et al., 2019b). By contrast, to the best of our knowledge, this is the first work which investigates the essential difference between regular and open NER. By conducting both randomization test (Edgington and Onghena, 2007) and verification experiments, we analyze the impact of name regularity, mention coverage and context pattern sufficiency and shed light on future open NER studies. Related Work Named entity recognition has long been studied and has attracted much attention. Conventional methods (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004) commonly rely o"
2020.emnlp-main.592,W04-1221,0,0.101172,"et al., 2018; Xie et al., 2018; Lin et al., 2019b). By contrast, to the best of our knowledge, this is the first work which investigates the essential difference between regular and open NER. By conducting both randomization test (Edgington and Onghena, 2007) and verification experiments, we analyze the impact of name regularity, mention coverage and context pattern sufficiency and shed light on future open NER studies. Related Work Named entity recognition has long been studied and has attracted much attention. Conventional methods (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004) commonly rely on handcraft features to build NER models, which are hard to transfer among different languages, domains and entity types. Recently, deep learning methods, which automatically extract high-level features and perform sequence tagging with neural 6 Conclusion and Future work This paper investigates whether current state-ofthe-art models on regular NER can still work well on open NER. From the perspective of name regularity, mention coverage and context diversity, we conducted both randomization test and verification experiments to evaluate the generalization ability of models. Our"
2020.emnlp-main.592,Q13-1001,0,0.0611509,"Missing"
2020.emnlp-main.592,P02-1060,0,0.226313,"et al., 2017; Peng and Dredze, 2016; Pan et al., 2017; Lin et al., 2018; Xie et al., 2018; Lin et al., 2019b). By contrast, to the best of our knowledge, this is the first work which investigates the essential difference between regular and open NER. By conducting both randomization test (Edgington and Onghena, 2007) and verification experiments, we analyze the impact of name regularity, mention coverage and context pattern sufficiency and shed light on future open NER studies. Related Work Named entity recognition has long been studied and has attracted much attention. Conventional methods (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004) commonly rely on handcraft features to build NER models, which are hard to transfer among different languages, domains and entity types. Recently, deep learning methods, which automatically extract high-level features and perform sequence tagging with neural 6 Conclusion and Future work This paper investigates whether current state-ofthe-art models on regular NER can still work well on open NER. From the perspective of name regularity, mention coverage and context diversity, we conducted both randomization test and verification experime"
2020.emnlp-main.592,W19-5035,0,\N,Missing
2020.findings-emnlp.331,W09-3302,0,0.395133,": this step uses the predicted entity labels by the optimized GBTeacher to fit the GBN. 5 Experiments 5.1 Experimental Setup Datasets: Following Zupon et al. (2019) and Yan et al. (2020), we use CoNLL and OntoNotes datasets. CoNLL is constructed from the CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003), which contains 4 entity types. OntoNotes is a sparse dataset constructed from the OntoNotes datasets (Pradhan et al., 2013) but without numerical categories, which contains 11 entity types. The patterns are n-grams (n ≤ 4). As for the pre-training datasets, we use Wikigold (Balasuriya et al., 2009), GUM (Zeldes, 2017) and half of the DocRED (Yao et al., 2019) for supervised pre-training; we use the remaining half of the DocRED without labels for self-supervised pretraining4 . Baselines. We use the following baselines: 1). LP5 : this is the classical label propagation method, which propagates the seed labels to other entities based on the co-occurrence features. 2). Gupta (Gupta and Manning, 2014): this is a classical bootstrapping system that evaluates patterns and new entities by learning an entity classifier6 . 3). Emboot (Zupon et al., 2019): this method follows Gupta and Manning (20"
2020.findings-emnlp.331,N19-1423,0,0.652484,"ctive bootstrapping (see Figure 2 for an example). To address the sparse supervision problem, this paper proposes a new end-to-end bootstrapping neural network for ESE, called Global Bootstrapping Network (GBN), which can effectively capture the global information of a corpus via an augmented entity-pattern bipartite graph, and learn to leverage both the local and the global semantics for bootstrapping via effective pre-training and finetuning strategies. Our method is motivated by the recent success of the pre-training and fine-tuning strategy in addressing the sparse supervision challenges (Devlin et al., 2019; Hu et al., 2020). Concretely, the Global Bootstrapping Network adopts the encoder-decoder architecture. The encoder is a global-sighted graph neural network, in which each layer aggregates rich information not only between directly linked entities and patterns but also the entities and patterns multi-hop away via augmented links. The decoder is an attentionguided RNN model, which efficiently generates expansion results based on the global-sighted entity representations. Compared with previous methods, GBN can also effectively aggregate the global information rather than only local neighborho"
2020.findings-emnlp.331,N18-1003,0,0.185895,"Missing"
2020.findings-emnlp.331,W14-1611,0,0.473686,"OntoNotes datasets (Pradhan et al., 2013) but without numerical categories, which contains 11 entity types. The patterns are n-grams (n ≤ 4). As for the pre-training datasets, we use Wikigold (Balasuriya et al., 2009), GUM (Zeldes, 2017) and half of the DocRED (Yao et al., 2019) for supervised pre-training; we use the remaining half of the DocRED without labels for self-supervised pretraining4 . Baselines. We use the following baselines: 1). LP5 : this is the classical label propagation method, which propagates the seed labels to other entities based on the co-occurrence features. 2). Gupta (Gupta and Manning, 2014): this is a classical bootstrapping system that evaluates patterns and new entities by learning an entity classifier6 . 3). Emboot (Zupon et al., 2019): this method follows Gupta and Manning (2014), but learns custom word embeddings for entities and patterns, which are used to guide the entity classifier. 4). LTB (Yan et al., 2019): this method performs the lookahead search to capture more information for each entity using the MCTS algorithm. 5). BootstrapNet (Yan et al., 2020): this method uses an end-to-end model to capture information from entity/pattern neighborhoods and expand seeds witho"
2020.findings-emnlp.331,N15-1128,0,0.307138,"(Yan et al., 2020). The pipelined methods (Riloff and Jones, 1999; Collins and Singer, 1999) mainly leverage direct co-occurrence information, which will easily lead to the semantic drifting problem (Curran et al., 2007). To resolve this problem, many pipelined methods are proposed, e.g., mutual exclusive bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008, 2009; Gupta et al., 2018), bootstrapping using negative seeds (Yangarber et al., 2002; Shi et al., 2014), lexical and statistical features (Liao and Grishman, 2010; Gupta and Manning, 2014), word embeddings (Batista et al., 2015; Gupta and Manning, 2015; Zupon et al., 2019), active learning (Berger et al., 2018), lookahead search (Yan et al., 2019), etc. Recently Yan et al. (2020) propose an end-to-end bootstrapping model and show its advantages in information leveraging and flexibility. Besides, there are some other studies that focus on the web-based ESE (Tong and Dean, 2008; Carlson et al., 2010; Chen et al., 2016), which heavily relies on the base search engine. Pre-training and fine-tuning. The early pretrained models on the ImageNet (Russakovsky et al., 2015) show its advantages in many CV tasks (Simonyan and Zisserman, 2014; Johnson e"
2020.findings-emnlp.331,D15-1056,0,0.191073,"d end-to-end paradigm (Yan et al., 2020). The pipelined methods (Riloff and Jones, 1999; Collins and Singer, 1999) mainly leverage direct co-occurrence information, which will easily lead to the semantic drifting problem (Curran et al., 2007). To resolve this problem, many pipelined methods are proposed, e.g., mutual exclusive bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008, 2009; Gupta et al., 2018), bootstrapping using negative seeds (Yangarber et al., 2002; Shi et al., 2014), lexical and statistical features (Liao and Grishman, 2010; Gupta and Manning, 2014), word embeddings (Batista et al., 2015; Gupta and Manning, 2015; Zupon et al., 2019), active learning (Berger et al., 2018), lookahead search (Yan et al., 2019), etc. Recently Yan et al. (2020) propose an end-to-end bootstrapping model and show its advantages in information leveraging and flexibility. Besides, there are some other studies that focus on the web-based ESE (Tong and Dean, 2008; Carlson et al., 2010; Chen et al., 2016), which heavily relies on the base search engine. Pre-training and fine-tuning. The early pretrained models on the ImageNet (Russakovsky et al., 2015) show its advantages in many CV tasks (Simonyan and Z"
2020.findings-emnlp.331,D18-1229,0,0.272766,"99; Collins and Singer, 1999) mainly leverage direct co-occurrence information, which will easily lead to the semantic drifting problem (Curran et al., 2007). To resolve this problem, many pipelined methods are proposed, e.g., mutual exclusive bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008, 2009; Gupta et al., 2018), bootstrapping using negative seeds (Yangarber et al., 2002; Shi et al., 2014), lexical and statistical features (Liao and Grishman, 2010; Gupta and Manning, 2014), word embeddings (Batista et al., 2015; Gupta and Manning, 2015; Zupon et al., 2019), active learning (Berger et al., 2018), lookahead search (Yan et al., 2019), etc. Recently Yan et al. (2020) propose an end-to-end bootstrapping model and show its advantages in information leveraging and flexibility. Besides, there are some other studies that focus on the web-based ESE (Tong and Dean, 2008; Carlson et al., 2010; Chen et al., 2016), which heavily relies on the base search engine. Pre-training and fine-tuning. The early pretrained models on the ImageNet (Russakovsky et al., 2015) show its advantages in many CV tasks (Simonyan and Zisserman, 2014; Johnson et al., 2016; Huang et al., 2017; He et al., 2017). In NLP, t"
2020.findings-emnlp.331,W99-0613,0,0.740791,"ing more global-sighted information, as more layers we used, more global-sighted information can be captured. 6 Related Work Bootstrapping. Bootstrapping is a widely studied technique in IE (Riloff and Shepherd, 1997; Qadir et al., 2015; Gupta et al., 2018), as well as word sense disambiguation (Yoshida et al., 2010), entity translation (Lee and Hwang, 2013), model learning (Whitney and Sarkar, 2012), etc. Currently, bootstrapping methods for ESE can be categorized into two paradigms: pipelined paradigm and end-to-end paradigm (Yan et al., 2020). The pipelined methods (Riloff and Jones, 1999; Collins and Singer, 1999) mainly leverage direct co-occurrence information, which will easily lead to the semantic drifting problem (Curran et al., 2007). To resolve this problem, many pipelined methods are proposed, e.g., mutual exclusive bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008, 2009; Gupta et al., 2018), bootstrapping using negative seeds (Yangarber et al., 2002; Shi et al., 2014), lexical and statistical features (Liao and Grishman, 2010; Gupta and Manning, 2014), word embeddings (Batista et al., 2015; Gupta and Manning, 2015; Zupon et al., 2019), active learning (Berger et al., 2018), lookahe"
2020.findings-emnlp.331,P13-1062,0,0.195528,"onduct experiments with different layer numbers (see Figure 5). From Figure 5, we can see that the performance of the GBN increases with more layers, which also indicates that the performance of bootstrapping methods for ESE can benefit from effectively capturing more global-sighted information, as more layers we used, more global-sighted information can be captured. 6 Related Work Bootstrapping. Bootstrapping is a widely studied technique in IE (Riloff and Shepherd, 1997; Qadir et al., 2015; Gupta et al., 2018), as well as word sense disambiguation (Yoshida et al., 2010), entity translation (Lee and Hwang, 2013), model learning (Whitney and Sarkar, 2012), etc. Currently, bootstrapping methods for ESE can be categorized into two paradigms: pipelined paradigm and end-to-end paradigm (Yan et al., 2020). The pipelined methods (Riloff and Jones, 1999; Collins and Singer, 1999) mainly leverage direct co-occurrence information, which will easily lead to the semantic drifting problem (Curran et al., 2007). To resolve this problem, many pipelined methods are proposed, e.g., mutual exclusive bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008, 2009; Gupta et al., 2018), bootstrapping using negative s"
2020.findings-emnlp.331,C10-1077,0,0.41655,"for ESE can be categorized into two paradigms: pipelined paradigm and end-to-end paradigm (Yan et al., 2020). The pipelined methods (Riloff and Jones, 1999; Collins and Singer, 1999) mainly leverage direct co-occurrence information, which will easily lead to the semantic drifting problem (Curran et al., 2007). To resolve this problem, many pipelined methods are proposed, e.g., mutual exclusive bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008, 2009; Gupta et al., 2018), bootstrapping using negative seeds (Yangarber et al., 2002; Shi et al., 2014), lexical and statistical features (Liao and Grishman, 2010; Gupta and Manning, 2014), word embeddings (Batista et al., 2015; Gupta and Manning, 2015; Zupon et al., 2019), active learning (Berger et al., 2018), lookahead search (Yan et al., 2019), etc. Recently Yan et al. (2020) propose an end-to-end bootstrapping model and show its advantages in information leveraging and flexibility. Besides, there are some other studies that focus on the web-based ESE (Tong and Dean, 2008; Carlson et al., 2010; Chen et al., 2016), which heavily relies on the base search engine. Pre-training and fine-tuning. The early pretrained models on the ImageNet (Russakovsky e"
2020.findings-emnlp.331,U08-1013,0,0.395072,"ense disambiguation (Yoshida et al., 2010), entity translation (Lee and Hwang, 2013), model learning (Whitney and Sarkar, 2012), etc. Currently, bootstrapping methods for ESE can be categorized into two paradigms: pipelined paradigm and end-to-end paradigm (Yan et al., 2020). The pipelined methods (Riloff and Jones, 1999; Collins and Singer, 1999) mainly leverage direct co-occurrence information, which will easily lead to the semantic drifting problem (Curran et al., 2007). To resolve this problem, many pipelined methods are proposed, e.g., mutual exclusive bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008, 2009; Gupta et al., 2018), bootstrapping using negative seeds (Yangarber et al., 2002; Shi et al., 2014), lexical and statistical features (Liao and Grishman, 2010; Gupta and Manning, 2014), word embeddings (Batista et al., 2015; Gupta and Manning, 2015; Zupon et al., 2019), active learning (Berger et al., 2018), lookahead search (Yan et al., 2019), etc. Recently Yan et al. (2020) propose an end-to-end bootstrapping model and show its advantages in information leveraging and flexibility. Besides, there are some other studies that focus on the web-based ESE (Tong and Dean, 2008; Carlson et al"
2020.findings-emnlp.331,P09-1045,0,0.855522,"Missing"
2020.findings-emnlp.331,D14-1162,0,0.118396,"embedding. The distance of two nodes equals to: 1 if they are directly linked; 2 if they are linked by an augmented link. • Link type feature tk : a learnable link type embedding. This paper uses three link types: before, middle and after2 . bedding) using previous expanded entities at each step as the follows: zct = σ(Wz · stc + Uz · ht−1 c ) rct = σ(Wr · stc + Ur · ht−1 c ) t t t ¯ = σ(W · s + r · U · ht−1 ) h c c c c t t t−1 t ¯t h =z h + (1 − z ) h c Node initialization. This paper initializes entity/pattern representations by their average token embeddings using pre-trained GloVe tables (Pennington et al., 2014). There are many other choices for initialization, such as CNN and BERT (Devlin et al., 2019). Based on the flying experiments, this paper adopts the average token embeddings for its simplicity and effectiveness. Compared to previous end-to-end model (Yan et al., 2020), the GBEncoder is different in two aspects: 1). It can leverage more information between entities by introducing distance information and link type features; 2). It has a more globalsighted perceptual field by explicitly modeling augmented links and passing messages through them. 3.2 GBDecoder Using the global-sighted entity emb"
2020.findings-emnlp.331,N18-1202,0,0.0192038,"n the web-based ESE (Tong and Dean, 2008; Carlson et al., 2010; Chen et al., 2016), which heavily relies on the base search engine. Pre-training and fine-tuning. The early pretrained models on the ImageNet (Russakovsky et al., 2015) show its advantages in many CV tasks (Simonyan and Zisserman, 2014; Johnson et al., 2016; Huang et al., 2017; He et al., 2017). In NLP, the pre-training has also been proven its effectiveness on many tasks, including the early word vectors such as word2vec or Glove (Mikolov et al., 2013; Pennington et al., 2014) and recent language model pre-training such as Elmo (Peters et al., 2018), BERT (Devlin et al., 2019) and XLNet (Yang et al., 2020). Recently, Hu et al. (2020) also show the advantages of graph pre-training, which directly inspires our work. 7 Conclusions In this paper, we propose the Global Bootstrapping Network (GBN) and effective “pre-training and fine-tuning” strategies to learn it. Specifically, we design global-sighted GBEncoder to capture both local and global semantics from the corpus and an effective attention-guided GBDecoder to adaptively expand new entities. To learn GBN, we design several pre-training and fine-tuning strategies. Experiments show that t"
2020.findings-emnlp.331,W97-0313,0,0.813705,"tructural information via selfsupervised pre-training strategies. Effect of Encoder layers. To analyze the effect of layer numbers of GBEncoder, we conduct experiments with different layer numbers (see Figure 5). From Figure 5, we can see that the performance of the GBN increases with more layers, which also indicates that the performance of bootstrapping methods for ESE can benefit from effectively capturing more global-sighted information, as more layers we used, more global-sighted information can be captured. 6 Related Work Bootstrapping. Bootstrapping is a widely studied technique in IE (Riloff and Shepherd, 1997; Qadir et al., 2015; Gupta et al., 2018), as well as word sense disambiguation (Yoshida et al., 2010), entity translation (Lee and Hwang, 2013), model learning (Whitney and Sarkar, 2012), etc. Currently, bootstrapping methods for ESE can be categorized into two paradigms: pipelined paradigm and end-to-end paradigm (Yan et al., 2020). The pipelined methods (Riloff and Jones, 1999; Collins and Singer, 1999) mainly leverage direct co-occurrence information, which will easily lead to the semantic drifting problem (Curran et al., 2007). To resolve this problem, many pipelined methods are proposed,"
2020.findings-emnlp.331,C14-1215,1,0.465564,"Missing"
2020.findings-emnlp.331,P12-1065,0,0.591895,"r numbers (see Figure 5). From Figure 5, we can see that the performance of the GBN increases with more layers, which also indicates that the performance of bootstrapping methods for ESE can benefit from effectively capturing more global-sighted information, as more layers we used, more global-sighted information can be captured. 6 Related Work Bootstrapping. Bootstrapping is a widely studied technique in IE (Riloff and Shepherd, 1997; Qadir et al., 2015; Gupta et al., 2018), as well as word sense disambiguation (Yoshida et al., 2010), entity translation (Lee and Hwang, 2013), model learning (Whitney and Sarkar, 2012), etc. Currently, bootstrapping methods for ESE can be categorized into two paradigms: pipelined paradigm and end-to-end paradigm (Yan et al., 2020). The pipelined methods (Riloff and Jones, 1999; Collins and Singer, 1999) mainly leverage direct co-occurrence information, which will easily lead to the semantic drifting problem (Curran et al., 2007). To resolve this problem, many pipelined methods are proposed, e.g., mutual exclusive bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008, 2009; Gupta et al., 2018), bootstrapping using negative seeds (Yangarber et al., 2002; Shi et al., 2"
2020.findings-emnlp.331,2020.findings-emnlp.331,1,0.0530913,"Missing"
2020.findings-emnlp.331,D19-1028,1,0.771548,"Missing"
2020.findings-emnlp.331,C02-1154,0,0.86824,"l learning (Whitney and Sarkar, 2012), etc. Currently, bootstrapping methods for ESE can be categorized into two paradigms: pipelined paradigm and end-to-end paradigm (Yan et al., 2020). The pipelined methods (Riloff and Jones, 1999; Collins and Singer, 1999) mainly leverage direct co-occurrence information, which will easily lead to the semantic drifting problem (Curran et al., 2007). To resolve this problem, many pipelined methods are proposed, e.g., mutual exclusive bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008, 2009; Gupta et al., 2018), bootstrapping using negative seeds (Yangarber et al., 2002; Shi et al., 2014), lexical and statistical features (Liao and Grishman, 2010; Gupta and Manning, 2014), word embeddings (Batista et al., 2015; Gupta and Manning, 2015; Zupon et al., 2019), active learning (Berger et al., 2018), lookahead search (Yan et al., 2019), etc. Recently Yan et al. (2020) propose an end-to-end bootstrapping model and show its advantages in information leveraging and flexibility. Besides, there are some other studies that focus on the web-based ESE (Tong and Dean, 2008; Carlson et al., 2010; Chen et al., 2016), which heavily relies on the base search engine. Pre-traini"
2020.findings-emnlp.331,P19-1074,0,0.0497217,"er to fit the GBN. 5 Experiments 5.1 Experimental Setup Datasets: Following Zupon et al. (2019) and Yan et al. (2020), we use CoNLL and OntoNotes datasets. CoNLL is constructed from the CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003), which contains 4 entity types. OntoNotes is a sparse dataset constructed from the OntoNotes datasets (Pradhan et al., 2013) but without numerical categories, which contains 11 entity types. The patterns are n-grams (n ≤ 4). As for the pre-training datasets, we use Wikigold (Balasuriya et al., 2009), GUM (Zeldes, 2017) and half of the DocRED (Yao et al., 2019) for supervised pre-training; we use the remaining half of the DocRED without labels for self-supervised pretraining4 . Baselines. We use the following baselines: 1). LP5 : this is the classical label propagation method, which propagates the seed labels to other entities based on the co-occurrence features. 2). Gupta (Gupta and Manning, 2014): this is a classical bootstrapping system that evaluates patterns and new entities by learning an entity classifier6 . 3). Emboot (Zupon et al., 2019): this method follows Gupta and Manning (2014), but learns custom word embeddings for entities and patter"
2020.findings-emnlp.331,W19-1504,0,0.226624,"Missing"
2020.findings-emnlp.424,N19-1423,0,0.0520799,"t experiments, this leads to comparable effectiveness but with a shorter training time. Phase one. Using the fine-tuned BERT model, we re-rank a list of documents from an unsupervised ranking model for use in the second phase. As shown in Equation (2), given a query q and a document d, rel (q, d) assigns d a relevance score by modeling the concatenation of the query and the document using the fine-tuned BERT. The ranked list is obtained by ranking the documents with respect to these relevance scores. We refer the reader to prior works describing BERT and ranking with BERT for further details (Devlin et al., 2019; Nogueira and Cho, 2019). rel (q, d) = BERT(q, d) 2.2 (2) Selecting Chunks for Query Expansion In the second phase, the top-kd documents from the first phase are employed as feedback documents and kc chunks of relevant text are extracted from them. This phase is illustrated in Figure 1. In more detail, a sliding window spanning m words is used to decompose each feedback document into overlapping chunks where two neighboring chunks 4719 Figure 1: Chunk selection for query expansion in phase two. are overlapped by up to m/2 words. The i-th chunk is denoted as ci . As expected, these chunks are"
2020.findings-emnlp.424,P16-1035,0,0.0288333,"e pseudo feedback information effectively. Query expansion has long been applied to make use of the pseudo relevance feedback information (Hui et al., 2011) to tackle the vocabulary mismatch problem. Keyword query expansion methods, such as Rocchio’s algorithm (Rocchio, 1971) and the KL query expansion model (Amati, 2003), have been shown to be effective when applied to text retrieval tasks. Moreover, Metzler and Croft (2007) propose to expand beyond unigram keywords by using a Markov random field model. Some query expansion methods use word embeddings to find the relevant terms to the query (Diaz et al., 2016; Zamani and Croft, 2016). Cao et al. (2008) perform query expansion by using classification models to select expansion terms. NPRF (Li et al., 2018) incorporates existing neural ranking models like DRMM (Guo et al., 2016) into an endto-end neural PRF framework. Rather than expanding the query, Nogueira et al. (2019b) propose a document expansion method named Doc2query, which uses a neural machine translation method to generate queries that each document might answer. Doc2query is further improved by docTTTTTquery (Nogueira and Lin, 2019) which replaces the seq2seq transformer with T5 (Raffel"
2020.findings-emnlp.424,D18-1478,1,0.88258,"Missing"
2020.findings-emnlp.424,2020.acl-main.537,0,0.0199013,"ument4 d for training are concatenated and the maximum sequence length is set to 384. We train BERT using crossentropy loss for 2 epochs with a batch size of 32 on a TPU v3. The Adam optimizer (Kingma and Ba, 2015) is used with the learning rate schedule from (Nogueira and Cho, 2019) with an initial learning rate of 1e-6. We conduct a standard fivefold cross-validation. Namely, queries are split into five equal-sized partitions. The query partition on Robust04 follows the settings from (Dai and Callan, 2019). On GOV2, queries are partitioned by the 3.7 Computation of FLOPs Akin to literature (Liu et al., 2020), we report FLOPs (floating point operations) which measures the computational complexity of models. Similar to (Khattab and Zaharia, 2020), we report FLOPs that includes all computations in the three phases of BERT-QE. 4 Results In this section, we report results for the proposed BERT-QE model and compare them to the baseline models. First, in Section 4.1, we use BERT-Large models for all three phases of BERT-QE. In Section 4.2, we evaluate the impact of using smaller BERT models (Table 1) for the second and third phases in order to improve the efficiency of the proposed model. 4.1 Results fo"
2020.findings-emnlp.424,D19-1352,0,0.115784,"y for expansion. All these methods, however, are under-equipped to accurately evaluate the relevance of information pieces used for expansion. This can be caused by the mixing of relevant and non-relevant information in the expansion, like the tokens in RM3 (Lavrenko and Croft, 2001) and the documents in NPRF (Li et al., 2018); or by the facts that the models used for selecting and re-weighting the expansion information are not powerful enough, as they are essentially scalars based on counting. Inspired by the recent advances of pre-trained contextualized models like BERT on the ranking task (Yilmaz et al., 2019; Nogueira et al., 2020), this work attempts to develop query expansion models based on BERT with the goal of more effectively using the relevant information from PRF. In addition, as indicated in previous studies (Qiao et al., 2019; Dai and Callan, 2019), the (pre-)trained BERT-based ranking models have a strong ability to identify highly relevant chunks within documents. This actually provides advantages in choosing text chunks for expansion by providing more flexibility in terms of the granularity for expansions, as compared with using tokens (Lavrenko and Croft, 2001), concepts with one or"
2020.findings-emnlp.69,W12-3010,0,0.865822,"Missing"
2020.findings-emnlp.69,P15-1034,0,0.440267,", ..., hm ) = BiLST M (x1 , x2 , ..., xm ); 3) predict the probability of assigning label yi to a word wi using a fully connected feedforward classifier: P (ˆ yi |S, p, wi ) = sof tmax(W hi + b); 4) finally decode the full label sequence Yˆ using a beamsearch algorithm, e.g., RnnOIE will decode the label sequence [B-ARG1 , B-P, B-ARG2 , IARG2 , I-ARG2 , I-ARG2 , O, O, O, O, O] to extract (Parragon; operates; more than 35 markets). In open IE, all extracted tuples are ranked according to their confidence scores, which is important for downstream tasks, such as QA (Fader et al., 2011) and KBP (Angeli et al., 2015). RnnOIE uses average log probabilities as the confidence of an extracted tuple: Pm logP (ˆ yi |S, p, wi ) ˆ c(S, p, Y ) = i=1 (1) m Given a training corpus, RnnOIE can be supervisedly learned by maximum log-likelihood estiFigure 3: An overview of syntactic patterns as data labelling functions. Two training instances are automatically generated using dependency pattern for predicates “operates” and “has”. mation (MLE): log P (Y|S, p) = m X log P (yi |S, p, wi ) (2) i=1 where Y = (y1 , y2 , ..., ym ) are the gold labels. As discussed above, Y are expensive and labourintensive to obtain and have"
2020.findings-emnlp.69,D15-1075,0,0.042156,"incorrect. For semantic consistency, given an extracted relation and its original sentence, Sem(Yˆ , S) is computed as: Sem(Yˆ , S) = P (positive|Yˆ , S) 785 (6) where P (positive|Yˆ , S) is the semantic similarity between the predicted label sequence Yˆ and its original sentence S. This paper estimates this semantic similarity using a BERT-based classifier, which assigns a similarity score to each sentencetuple pair. Because multiple tuples can be extracted from a single sentence (see Figure 3 for example), we train the classifier using the Stanford Natural Language Inference (SNLI) Corpus (Bowman et al., 2015), so that a high similarity score will be assigned if the original sentence entails the extracted tuple. This semantic consistency can provide useful supervision signals for open IE models. For example, because (Parragon; has; 10 offices) has higher semantic similarity than (has; 10 offices) to sentence “Parragon operates more than 35 markets and has 10 offices.”, the model will be guided to more complete extractions. Semantic-Based Confidence Estimation. In RnnOIE, the confidence score c(S, p, Yˆ ) is estimated only using extraction probabilities. This paper further considers the semantic con"
2020.findings-emnlp.69,P18-2065,0,0.550639,", 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O] or generate a token sequence [&lt;ARG1 &gt;, Parragon, &lt;P &gt;, operates, &lt;ARG2 &gt;, more, than, 35, markets]. The neural open IE systems, unfortunately, rely on the large labelled corpus to achieve good performance, which is often expensive and labourintensive to obtain. Furthermore, open IE needs to extract relations of unlimited types from open domain corpus, which further exacerbates the need for lar"
2020.findings-emnlp.69,N19-1423,0,0.406093,"ency between a tuple and its original sentence. For example, Figure 2 shows the ARG1 “Parragon” and the ARG2 “more than 35 markets” follow the nsubj and dobj dependency structure, respectively. Meanwhile, the extracted tuple (Parragon; operates; more than 35 markets) has a high semantic similarity with its original sentence “Parragon operates more than 35 markets and has 10 offices.”. And we found that the syntactic regularities can be effectively captured using syntactic rules, and the semantic consistency can be effectively modelled using the recent powerful pre-trained models such as BERT (Devlin et al., 2019). Based on the above observations, we propose two learning strategies to exploit syntactic and semantic knowledge for model learning. Figure 1 illustrates the framework of our method. Firstly, syntactic open IE patterns are used as data labelling functions, and a base model is pretrained using the noisy training corpus generated by these labelling functions. Secondly, because the pattern-based labels are often noisy and with limited coverage, we further propose a reinforcement learning algorithm which uses syntactic and semantic-driven reward functions, which can effectively generalize the bas"
2020.findings-emnlp.69,D11-1142,0,0.881246,"is used to generalize the base model to open situations. Open information extraction (Open IE) aims to extract open-domain textual tuples consisting of a predicate and a set of arguments from massive and heterogeneous corpora (Sekine, 2006; Banko et al., 2007). For example, a system will extract a tuple (Parragon; operates; more than 35 markets) from the sentence “Parragon operates more than 35 markets and has 10 offices.”. In contrary to the traditional IE, open IE is completely domain-independent and does not require the predetermined relations. Recently, open IE has gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019"
2020.findings-emnlp.69,D15-1076,0,0.0307563,"RL. By modelling the extraction task as a Markov Decision Process (MDP), we have the following definitions: &lt; S, A, T , R &gt;: Reward Function. The reward function, i.e., the goodness of extracted tuples, is critical in our RL algorithm. This paper estimates the reward R(Yˆ , S) by considering both syntactic constraint and semantic consistency: 2.3 2 The dependency relations are defined https://nlp.stanford.edu/software/ dependencies_manual.pdf in R(Yˆ , S) = Syn(Yˆ ) ∗ Sem(Yˆ , S) (4) where Syn(Yˆ ) is the syntactic constraint score and Sem(Yˆ , S) is the semantic consistency score. Following He et al. (2015); Stanovsky et al. (2018); Jiang et al. (2019), we judge an extracted tuple as correct if and only if it’s predicate and arguments include their corresponding syntactic headwords (Headwords Match). Otherwise, the extracted tuples are judged as incorrect. That is: ( 1, Headwords Match Syn(Yˆ ) = (5) −1, Else where 1 means the predicted label sequence Yˆ is correct and -1 for incorrect. For semantic consistency, given an extracted relation and its original sentence, Sem(Yˆ , S) is computed as: Sem(Yˆ , S) = P (positive|Yˆ , S) 785 (6) where P (positive|Yˆ , S) is the semantic similarity between"
2020.findings-emnlp.69,P19-1523,0,0.652449,"Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O] or generate a token sequence [&lt;ARG1 &gt;, Parragon, &lt;P &gt;, operates, &lt;ARG2 &gt;, more, than, 35, markets]. The neural open IE systems, unfortunately, rely on the large labelled corpus to achieve good performance, which is often expensive and labourintensive to obtain. Furthermore, open IE needs to"
2020.findings-emnlp.69,D12-1048,0,0.898816,"n situations. Open information extraction (Open IE) aims to extract open-domain textual tuples consisting of a predicate and a set of arguments from massive and heterogeneous corpora (Sekine, 2006; Banko et al., 2007). For example, a system will extract a tuple (Parragon; operates; more than 35 markets) from the sentence “Parragon operates more than 35 markets and has 10 offices.”. In contrary to the traditional IE, open IE is completely domain-independent and does not require the predetermined relations. Recently, open IE has gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target"
2020.findings-emnlp.69,D13-1043,0,0.611067,"ic and semantic-driven reward functions, which can effectively generalize the base model to open situations with high accuracy. These two strategies together will ensure the effective learning of open IE models: the data labelling function can pretrain a reasonable initial model so that the RL algorithm can optimize model more effectively; although the pattern-based labels are often noisy and with low coverage, the RL algorithm can generalize the model to open situations with high accuracy. We conducted experiments on three open IE benchmarks: OIE2016 (Stanovsky and Dagan, 2016), WEB and NYT (Mesquita et al., 2013). Experimental results show that the proposed framework significantly outperforms the supervised counterparts, and can even achieve competitive performance with the supervised SoA approach. 1 The main contributions of this paper are: • We propose a syntactic and semantic-driven learning algorithm which can leverage syntactic and semantic knowledge as noisier, higherlevel supervisions and learn neural open IE models without any human-labelled data. • We design two effective learning strategies for exploiting syntactic and semantic knowledge as supervisions: one is to use as data labelling funct"
2020.findings-emnlp.69,P09-1113,0,0.205826,"and many syntactic patterns have been designed for extracting tuples, such as TEXTRUNNER (Banko et al., 2007) and ReVerb (Fader et al., 2011). However, it is difficult to design high coverage syntactic patterns, although many extensions have been proposed, such as WOE (Wu and Weld, 2010), OLLIE (Mausam et al., 2012), ClausIE (Corro and Gemulla, 2013), Standford Open IE (Angeli et al., 2015), PropS (Stanovsky et al., 2016) and OpenIE4 (Mausam, 2016). This paper leverages the power of patterns differently. Inspired by the ideas of data programming (Ratner et al., 2016) and distant supervision (Mintz et al., 2009), we use syntactic patterns 784 • S = {s} are states used to capture the information from the current sentence. Specifically, S are hidden states H obtained by stacked BiLSTM. as data labelling functions, rather than to directly extracting tuples. Concretely, this paper uses dependency patterns from Standford Open IE (Angeli et al., 2015) to design hand-crafted patterns as data labelling functions. As shown in Figure 3, given a sentence and its dependency parse, two training instances are generated: 1) We first identify all its predicates using part of speech (POS) tags. For example, “operates"
2020.findings-emnlp.69,D16-1261,0,0.112935,"en-domain textual tuples consisting of a predicate and a set of arguments from massive and heterogeneous corpora (Sekine, 2006; Banko et al., 2007). For example, a system will extract a tuple (Parragon; operates; more than 35 markets) from the sentence “Parragon operates more than 35 markets and has 10 offices.”. In contrary to the traditional IE, open IE is completely domain-independent and does not require the predetermined relations. Recently, open IE has gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2"
2020.findings-emnlp.69,W16-1307,0,0.0191463,"e (Parragon; operates; more than 35 markets) from the sentence “Parragon operates more than 35 markets and has 10 offices.”. In contrary to the traditional IE, open IE is completely domain-independent and does not require the predetermined relations. Recently, open IE has gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O"
2020.findings-emnlp.69,P18-1046,0,0.0201502,"sers’ expressions or domain heuristics as a generative model. Distant supervision paradigm (Mintz et al., 2009) heuristically generates labelled dataset by aligning facts in KB with sentences in the corpus. The proposed data labelling functions are also motivated by the ideas of data programming and distant supervision. Reinforcement Learning for IE. Reinforcement learning (RL) (Sutton and Barto, 1998) follows the explore and exploit paradigm and is apt for optimizing non-derivative learning objectives in NLP (Wu et al., 2018). Recently, RL has gained much attention in information extraction (Qin et al., 2018b,a; Takanobu et al., 2019). In open IE, Narasimhan et al. (2016) firstly using traditional Q-learning method to extract textual tuples. However, their reward function is chosen to maximize the final extraction accuracy which still relies on human-labelled datasets and can not capture the syntactic and semantic supervisions explicitly. 5 Conclusions This paper proposes an open IE learning approach, which can learn neural models without any humanlabelled data by leveraging syntactic and semantic knowledge as noisier, higher-level supervisions. Specifically, two effective learning strategies are"
2020.findings-emnlp.69,P18-1199,0,0.0143311,"sers’ expressions or domain heuristics as a generative model. Distant supervision paradigm (Mintz et al., 2009) heuristically generates labelled dataset by aligning facts in KB with sentences in the corpus. The proposed data labelling functions are also motivated by the ideas of data programming and distant supervision. Reinforcement Learning for IE. Reinforcement learning (RL) (Sutton and Barto, 1998) follows the explore and exploit paradigm and is apt for optimizing non-derivative learning objectives in NLP (Wu et al., 2018). Recently, RL has gained much attention in information extraction (Qin et al., 2018b,a; Takanobu et al., 2019). In open IE, Narasimhan et al. (2016) firstly using traditional Q-learning method to extract textual tuples. However, their reward function is chosen to maximize the final extraction accuracy which still relies on human-labelled datasets and can not capture the syntactic and semantic supervisions explicitly. 5 Conclusions This paper proposes an open IE learning approach, which can learn neural models without any humanlabelled data by leveraging syntactic and semantic knowledge as noisier, higher-level supervisions. Specifically, two effective learning strategies are"
2020.findings-emnlp.69,D19-1067,0,0.729588,"Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O] or generate a token sequence [&lt;ARG1 &gt;, Parragon, &lt;P &gt;, operates, &lt;ARG2 &gt;, more, than, 35, markets]. The neural open IE systems, unfortunately, rely on the large labelled corpus to achieve good performance, which is often expensive and labourintensive to obtain. Furthermore, open IE needs to extract relations o"
2020.findings-emnlp.69,P06-2094,0,0.0212662,"ta labeling Noisy Training Corpus [Parragon]ARG1 [operates]P [more than 35 markets]ARG2 and has 10 offices. Open IE Model Figure 1: The proposed open IE framework, which consists of two learning strategies: 1) syntactic patterns are used as data labelling functions and a base model is pretrained using the generated labels; 2) a syntactic and semantic-driven RL algorithm is used to generalize the base model to open situations. Open information extraction (Open IE) aims to extract open-domain textual tuples consisting of a predicate and a set of arguments from massive and heterogeneous corpora (Sekine, 2006; Banko et al., 2007). For example, a system will extract a tuple (Parragon; operates; more than 35 markets) from the sentence “Parragon operates more than 35 markets and has 10 offices.”. In contrary to the traditional IE, open IE is completely domain-independent and does not require the predetermined relations. Recently, open IE has gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introducti"
2020.findings-emnlp.69,D16-1252,0,0.0668223,"ent learning algorithm which uses syntactic and semantic-driven reward functions, which can effectively generalize the base model to open situations with high accuracy. These two strategies together will ensure the effective learning of open IE models: the data labelling function can pretrain a reasonable initial model so that the RL algorithm can optimize model more effectively; although the pattern-based labels are often noisy and with low coverage, the RL algorithm can generalize the model to open situations with high accuracy. We conducted experiments on three open IE benchmarks: OIE2016 (Stanovsky and Dagan, 2016), WEB and NYT (Mesquita et al., 2013). Experimental results show that the proposed framework significantly outperforms the supervised counterparts, and can even achieve competitive performance with the supervised SoA approach. 1 The main contributions of this paper are: • We propose a syntactic and semantic-driven learning algorithm which can leverage syntactic and semantic knowledge as noisier, higherlevel supervisions and learn neural open IE models without any human-labelled data. • We design two effective learning strategies for exploiting syntactic and semantic knowledge as supervisions:"
2020.findings-emnlp.69,N18-1081,0,0.201594,"gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O] or generate a token sequence [&lt;ARG1 &gt;, Parragon, &lt;P &gt;, operates, &lt;ARG2 &gt;, more, than, 35, markets]. The neural open IE systems, unfortunately, rely on the large labelled corpus to achieve good performance, which is often expensive and labourintensive to obtain. Furthermor"
2020.findings-emnlp.69,D18-1236,0,0.0952876,"et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O] or generate a token sequence [&lt;ARG1 &gt;, Parragon, &lt;P &gt;, operates, &lt;ARG2 &gt;, more, than, 35, markets]. The neural open IE systems, unfortunately, rely on the large labelled corpus to achieve good performance, which is often expensive and labourintensive to obtain. Furthermore, open IE needs to extract relations of unlimited types from open domain corpus, which further exacerbates the need for large labelled corpus."
2020.findings-emnlp.69,P10-1013,0,0.819567,"ning using Syntactic Pattern-based Data Labelling Functions The first strategy is to use syntactic extraction patterns as data labelling functions, and then the heuristically labelled training corpus will be used to pretrain a neural open IE model. It has long been observed that most relation tuples follow syntactic regularity, and many syntactic patterns have been designed for extracting tuples, such as TEXTRUNNER (Banko et al., 2007) and ReVerb (Fader et al., 2011). However, it is difficult to design high coverage syntactic patterns, although many extensions have been proposed, such as WOE (Wu and Weld, 2010), OLLIE (Mausam et al., 2012), ClausIE (Corro and Gemulla, 2013), Standford Open IE (Angeli et al., 2015), PropS (Stanovsky et al., 2016) and OpenIE4 (Mausam, 2016). This paper leverages the power of patterns differently. Inspired by the ideas of data programming (Ratner et al., 2016) and distant supervision (Mintz et al., 2009), we use syntactic patterns 784 • S = {s} are states used to capture the information from the current sentence. Specifically, S are hidden states H obtained by stacked BiLSTM. as data labelling functions, rather than to directly extracting tuples. Concretely, this paper"
2020.findings-emnlp.69,D18-1397,0,0.0286055,"aradigm (Ratner et al., 2016) creates training datasets by explicitly representing users’ expressions or domain heuristics as a generative model. Distant supervision paradigm (Mintz et al., 2009) heuristically generates labelled dataset by aligning facts in KB with sentences in the corpus. The proposed data labelling functions are also motivated by the ideas of data programming and distant supervision. Reinforcement Learning for IE. Reinforcement learning (RL) (Sutton and Barto, 1998) follows the explore and exploit paradigm and is apt for optimizing non-derivative learning objectives in NLP (Wu et al., 2018). Recently, RL has gained much attention in information extraction (Qin et al., 2018b,a; Takanobu et al., 2019). In open IE, Narasimhan et al. (2016) firstly using traditional Q-learning method to extract textual tuples. However, their reward function is chosen to maximize the final extraction accuracy which still relies on human-labelled datasets and can not capture the syntactic and semantic supervisions explicitly. 5 Conclusions This paper proposes an open IE learning approach, which can learn neural models without any humanlabelled data by leveraging syntactic and semantic knowledge as noi"
2020.findings-emnlp.69,I17-1086,0,0.0139316,"from the sentence “Parragon operates more than 35 markets and has 10 offices.”. In contrary to the traditional IE, open IE is completely domain-independent and does not require the predetermined relations. Recently, open IE has gained much attention (Fader et al., 2011; Akbik and L¨oser, 2012; Mausam et al., 2012; Corro and Gemulla, 2013; Moro and Navigli, 2013; Narasimhan et al., 2016; Corresponding author def dl(x): all verbs are labeled as P … Syntax and Semantic Driven RL Introduction ∗ Parragon operates more than 35 markets and has 10 offices. Pal and Mausam, 2016; Kadry and Dietz, 2017; Yu et al., 2017; Roth et al., 2018) and most of current open IE systems employ end-to-end neural networks, which first encode a sentence using Bi-LSTMs, then extract tuples by sequentially labelling all tokens in the sentence (Stanovsky et al., 2018; Jiang et al., 2019; Roy et al., 2019) or generating the target tuples token-by-token (Zhang et al., 2017; Cui et al., 2018; Sun et al., 2018). For example, to extract (Parragon; operates; more than 35 markets), neural open IE systems will label the sentence as [B-ARG1 , B-P, B-ARG2 , I-ARG2 , IARG2 , I-ARG2 , O, O, O, O, O] or generate a token sequence [&lt;ARG1 &gt;,"
2020.findings-emnlp.69,E17-2011,0,0.0526786,"Missing"
2020.semeval-1.85,N19-1423,0,0.495741,"he statement, we can accumulate the causal knowledge for “post-traumatic stress”, i.e., “a combination of paroxetine and exposure may help cure post-traumatic stress”. To model counterfactual semantics and reason in natural language, SemEval 2020 Subtask 5 provides an English benchmark for two basic problems: detecting counterfactual statements and detecting antecedent and consequence (Yang et al., 2020). We build our evaluation systems that are built on pre-trained transformer-based neural network models, which have shown significant improvements over conventional methods in many NLP fields (Devlin et al., 2019; Liu et al., 2020; Lan et al., 2020). Specifically, in subtask 1, several transformer-based classifiers are designed to detect counterfactual statements. Besides, because counterfactual antecedent expressions are usually expressed using some obvious conditional assumption connectives, such as if and wish. We also equip transformers with additional convolutional neural network to capture the above strong local context information. For subtask 2, we formulate antecedent and consequence extraction as a query-based question answering problem. Specifically, to effectively model context information"
2020.semeval-1.85,D14-1181,0,0.00647642,"regated feature vector r to capture the counterfactual information of the entire statement. We investigate two different aggregation strategies in this section: [CLS] aggregation and convolutional neural network (CNN) aggregation. In [CLS] aggregation, we directly use the representation C of the special symbol [CLS] as the aggregate feature r (Devlin et al., 2019). In counterfactual statements, connectives are often used to express the relation between antecedent and consequence, i.e., “if”, “even if”, and “would”. To capture these local patterns in counterfactual statements, we employ a CNN (Kim, 2014) to aggregate sentence information. Given the token sequence {h1 , ..., hn }, the convolutional filter scans the token sequence and extract the local feature li : li = tanh w·hi:i+h−1 +b. Finally, a max-pooling layer is used to produce the feature r for further counterfactual statement detection: r = max0≤i≤n li . Counterfactual Statement Classifier. After aggregation, the feature vector r will be fed to the counterfactual classifier, which computes a probability of whether it is a counterfactual statement: P (y = 1|x) = σ(wc · r + bc ) (1) where wc is the weight vector, bc is the bias term, a"
2020.semeval-1.85,D14-1162,0,0.0864099,"tion 6. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 658 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 658–663 Barcelona, Spain (Online), December 12, 2020. Classifier Classifier CNN … Pre-trained Transformer Pre-trained Transformer … … [CLS] ?!!! ?!!&quot; ?!#&quot; [CLS] [SEP] ?!!! (a) CLS Aggregation ?!!&quot; ?!#&quot; [SEP] (b) CNN Aggregation Figure 1: The Transformer for Detecting Counterfactual Statements. 2 Background Different from the pre-trained word embedding in NLP (Pennington et al., 2014), pre-trained contextualized models aim to learn encoders to represent words in context for downstream tasks. BERT(Devlin et al., 2019) is a representative large-scale pre-trained transformer, which is trained using mask language modeling (MLM) and next sentence prediction (NSP) task. Whole Word Masking model1 (BERT-WWM) is a simple but effective variant of BERT. In this case, the pre-training stage always mask all of the tokens corresponding to a word instead of a single WordPiece token (sub-token). RoBERTa (Liu et al., 2020) further improves on BERT’s pre-training procedure and achieves subs"
2020.semeval-1.85,N18-1202,0,0.00845177,"presentation, we first convert the raw input text into word-pieces 1 https://github.com/google-research/bert 659 (sub-tokens) {˜ x11 , ..., x ˜n1 , x ˜n2 } in the pre-defined vocabulary. Then, the two special symbols [CLS] and [SEP] will be added to the head and tail of the sentence. Finally, we feed tokenized text ˜x = {[CLS], x ˜11 , ..., x ˜n1 , x ˜n2 , [SEP]} into L-layers pre-trained transformers to obtain the contextualized representation for each sub-tokens. Following (Tenney et al., 2019), we pool token i’s representation hi ∈ Rn×d across all BERT layers P (j) (j) using scalar mixing (Peters et al., 2018): hi = γ L ∈ Rd is the embedding of j=1 αj xi where xi token i from BERT layer j, αj is softmax-normalized weights, and γ is a scalar parameter. We denote the final representation of the special symbol [CLS] as C ∈ Rd . Specifically, we obtain the token-level representation using the representation of the first sub-token in each token. Contextualized Information Aggregation. After obtaining the representation of each word, we produce aggregated feature vector r to capture the counterfactual information of the entire statement. We investigate two different aggregation strategies in this section"
2020.semeval-1.85,2020.semeval-1.40,0,0.0406452,"edent, while the italic term is the consequence: Her post-traumatic stress could have been avoided if a combination of paroxetine and exposure therapy had been prescribed two months earlier. Once understanding the statement, we can accumulate the causal knowledge for “post-traumatic stress”, i.e., “a combination of paroxetine and exposure may help cure post-traumatic stress”. To model counterfactual semantics and reason in natural language, SemEval 2020 Subtask 5 provides an English benchmark for two basic problems: detecting counterfactual statements and detecting antecedent and consequence (Yang et al., 2020). We build our evaluation systems that are built on pre-trained transformer-based neural network models, which have shown significant improvements over conventional methods in many NLP fields (Devlin et al., 2019; Liu et al., 2020; Lan et al., 2020). Specifically, in subtask 1, several transformer-based classifiers are designed to detect counterfactual statements. Besides, because counterfactual antecedent expressions are usually expressed using some obvious conditional assumption connectives, such as if and wish. We also equip transformers with additional convolutional neural network to captu"
2021.acl-long.146,P17-1171,0,0.0730639,"Missing"
2021.acl-long.146,D19-1109,0,0.0287263,"ctly used as reliable knowledge bases. Petroni et al. (2019) propose the LAMA benchmark, which probes knowledge in PLMs using prompt-based retrieval. Jiang et al. (2020a) build a multilingual knowledge probing benchmark based on LAMA. There are many studies focus on probing specific knowledge in PLMs, such as linguistic knowledge (Lin et al., 2019; Tenney et al., 2019; Liu et al., 2019a; Htut et al., 2019; Hewitt and Manning, 2019; Goldberg, 2019; Warstadt et al., 2019), 1861 3 www.wikidata.org semantic knowledge (Tenney et al., 2019; Wallace et al., 2019; Ettinger, 2020) and world knowledge (Davison et al., 2019; Bouraoui et al., 2020; Forbes et al., 2019; Zhou et al., 2019; Roberts et al., 2020; Lin et al., 2020; Tamborrino et al., 2020). Recently, some studies doubt the reliability of PLMs as knowledge base by discovering the the spurious correlation to surface forms (McCoy et al., 2019; Poerner et al., 2020; Shwartz et al., 2020), and their sensitivity to “negation” and “mispriming” (Kassner and Sch¨utze, 2020b). Currently, there are three main paradigms for knowledge extraction from PLMs: prompt-based retrieval (Schick and Sch¨utze, 2021; Li and Liang, 2021), case-based analogy (Schick and Sch¨ut"
2021.acl-long.146,W19-4825,0,0.0495337,"Missing"
2021.acl-long.146,N19-1112,0,0.138507,"rn in &lt;?&gt;. Answer Leakage Context helps if it leaks &lt;?&gt; Figure 1: This paper explores three different kinds of factual knowledge extraction paradigms from MLMs, and reveal the underlying predicting mechanisms behind them. Recently, pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020) have achieved promising performance on many NLP tasks. Apart from utilizing the universal representations from pre-trained models in downstream tasks, some literatures have shown the potential of pretrained masked language models (e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b)) to be factual knowledge bases (Petroni et al., 2019; Bouraoui et al., 2020; Jiang et al., 2020b; Shin et al., 2020; Jiang et al., 2020a; Wang et al., 2020; Kassner and Sch¨utze, 2020a; Kassner et al., 2020). For example, to extract the birthplace of Steve Jobs, we can query MLMs like BERT with “Steve Jobs was born in [MASK]”, where Steve Jobs is the subject ∗ Prompt Bias “was born in” without X predicts &lt;?&gt; Case-based Introduction Corresponding Authors We openly release the source code and data at https: //github.com/c-box/LANKA 1 Mechanism of the fact, “was born in” is a prompt string for"
2021.acl-long.146,2021.ccl-1.108,0,0.0749306,"Missing"
2021.acl-long.146,P19-1334,0,0.0258949,"knowledge in PLMs, such as linguistic knowledge (Lin et al., 2019; Tenney et al., 2019; Liu et al., 2019a; Htut et al., 2019; Hewitt and Manning, 2019; Goldberg, 2019; Warstadt et al., 2019), 1861 3 www.wikidata.org semantic knowledge (Tenney et al., 2019; Wallace et al., 2019; Ettinger, 2020) and world knowledge (Davison et al., 2019; Bouraoui et al., 2020; Forbes et al., 2019; Zhou et al., 2019; Roberts et al., 2020; Lin et al., 2020; Tamborrino et al., 2020). Recently, some studies doubt the reliability of PLMs as knowledge base by discovering the the spurious correlation to surface forms (McCoy et al., 2019; Poerner et al., 2020; Shwartz et al., 2020), and their sensitivity to “negation” and “mispriming” (Kassner and Sch¨utze, 2020b). Currently, there are three main paradigms for knowledge extraction from PLMs: prompt-based retrieval (Schick and Sch¨utze, 2021; Li and Liang, 2021), case-based analogy (Schick and Sch¨utze, 2020a,b), and context-based inference. For promptbased retrieval, current studies focus on seeking better prompts by either mining from corpus (Jiang et al., 2020b) or learning using labeled data (Shin et al., 2020). For case-based analogy, current studies mostly focus on wheth"
2021.acl-long.146,N18-1202,0,0.0338146,"Our findings shed light on the underlying predicting mechanisms of MLMs, and strongly question the previous conclusion that current MLMs can potentially serve as reliable factual knowledge bases1 . 1 X was born in &lt;?&gt;. A was born in B. X was born in &lt;?&gt;. Type Guidance &lt;?&gt; will have the same type as B Context-based X lives in Y. X was born in &lt;?&gt;. Answer Leakage Context helps if it leaks &lt;?&gt; Figure 1: This paper explores three different kinds of factual knowledge extraction paradigms from MLMs, and reveal the underlying predicting mechanisms behind them. Recently, pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020) have achieved promising performance on many NLP tasks. Apart from utilizing the universal representations from pre-trained models in downstream tasks, some literatures have shown the potential of pretrained masked language models (e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b)) to be factual knowledge bases (Petroni et al., 2019; Bouraoui et al., 2020; Jiang et al., 2020b; Shin et al., 2020; Jiang et al., 2020a; Wang et al., 2020; Kassner and Sch¨utze, 2020a; Kassner et al., 2020). For example, to extract the birthplace of Steve Jobs"
2021.acl-long.146,D19-1250,0,0.0356063,"Missing"
2021.acl-long.146,2020.findings-emnlp.71,0,0.0491867,"Missing"
2021.acl-long.146,2020.emnlp-main.346,0,0.467647,"l knowledge extraction paradigms from MLMs, and reveal the underlying predicting mechanisms behind them. Recently, pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020) have achieved promising performance on many NLP tasks. Apart from utilizing the universal representations from pre-trained models in downstream tasks, some literatures have shown the potential of pretrained masked language models (e.g., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019b)) to be factual knowledge bases (Petroni et al., 2019; Bouraoui et al., 2020; Jiang et al., 2020b; Shin et al., 2020; Jiang et al., 2020a; Wang et al., 2020; Kassner and Sch¨utze, 2020a; Kassner et al., 2020). For example, to extract the birthplace of Steve Jobs, we can query MLMs like BERT with “Steve Jobs was born in [MASK]”, where Steve Jobs is the subject ∗ Prompt Bias “was born in” without X predicts &lt;?&gt; Case-based Introduction Corresponding Authors We openly release the source code and data at https: //github.com/c-box/LANKA 1 Mechanism of the fact, “was born in” is a prompt string for the relation “place-of-birth” and [MASK] is a placeholder for the object to predict. Then MLMs are expected to predic"
2021.acl-long.146,2020.emnlp-main.556,0,0.0220365,"ledge (Lin et al., 2019; Tenney et al., 2019; Liu et al., 2019a; Htut et al., 2019; Hewitt and Manning, 2019; Goldberg, 2019; Warstadt et al., 2019), 1861 3 www.wikidata.org semantic knowledge (Tenney et al., 2019; Wallace et al., 2019; Ettinger, 2020) and world knowledge (Davison et al., 2019; Bouraoui et al., 2020; Forbes et al., 2019; Zhou et al., 2019; Roberts et al., 2020; Lin et al., 2020; Tamborrino et al., 2020). Recently, some studies doubt the reliability of PLMs as knowledge base by discovering the the spurious correlation to surface forms (McCoy et al., 2019; Poerner et al., 2020; Shwartz et al., 2020), and their sensitivity to “negation” and “mispriming” (Kassner and Sch¨utze, 2020b). Currently, there are three main paradigms for knowledge extraction from PLMs: prompt-based retrieval (Schick and Sch¨utze, 2021; Li and Liang, 2021), case-based analogy (Schick and Sch¨utze, 2020a,b), and context-based inference. For promptbased retrieval, current studies focus on seeking better prompts by either mining from corpus (Jiang et al., 2020b) or learning using labeled data (Shin et al., 2020). For case-based analogy, current studies mostly focus on whether good cases will lead to good few-shot abil"
2021.acl-long.146,D19-1534,0,0.0191133,"dels (PLMs) raises the question of whether PLMs can be directly used as reliable knowledge bases. Petroni et al. (2019) propose the LAMA benchmark, which probes knowledge in PLMs using prompt-based retrieval. Jiang et al. (2020a) build a multilingual knowledge probing benchmark based on LAMA. There are many studies focus on probing specific knowledge in PLMs, such as linguistic knowledge (Lin et al., 2019; Tenney et al., 2019; Liu et al., 2019a; Htut et al., 2019; Hewitt and Manning, 2019; Goldberg, 2019; Warstadt et al., 2019), 1861 3 www.wikidata.org semantic knowledge (Tenney et al., 2019; Wallace et al., 2019; Ettinger, 2020) and world knowledge (Davison et al., 2019; Bouraoui et al., 2020; Forbes et al., 2019; Zhou et al., 2019; Roberts et al., 2020; Lin et al., 2020; Tamborrino et al., 2020). Recently, some studies doubt the reliability of PLMs as knowledge base by discovering the the spurious correlation to surface forms (McCoy et al., 2019; Poerner et al., 2020; Shwartz et al., 2020), and their sensitivity to “negation” and “mispriming” (Kassner and Sch¨utze, 2020b). Currently, there are three main paradigms for knowledge extraction from PLMs: prompt-based retrieval (Schick and Sch¨utze, 2021;"
2021.acl-long.146,D19-1286,0,0.025347,"benefit many future studies. 2 Related Work The great success of Pre-trained Language Models (PLMs) raises the question of whether PLMs can be directly used as reliable knowledge bases. Petroni et al. (2019) propose the LAMA benchmark, which probes knowledge in PLMs using prompt-based retrieval. Jiang et al. (2020a) build a multilingual knowledge probing benchmark based on LAMA. There are many studies focus on probing specific knowledge in PLMs, such as linguistic knowledge (Lin et al., 2019; Tenney et al., 2019; Liu et al., 2019a; Htut et al., 2019; Hewitt and Manning, 2019; Goldberg, 2019; Warstadt et al., 2019), 1861 3 www.wikidata.org semantic knowledge (Tenney et al., 2019; Wallace et al., 2019; Ettinger, 2020) and world knowledge (Davison et al., 2019; Bouraoui et al., 2020; Forbes et al., 2019; Zhou et al., 2019; Roberts et al., 2020; Lin et al., 2020; Tamborrino et al., 2020). Recently, some studies doubt the reliability of PLMs as knowledge base by discovering the the spurious correlation to surface forms (McCoy et al., 2019; Poerner et al., 2020; Shwartz et al., 2020), and their sensitivity to “negation” and “mispriming” (Kassner and Sch¨utze, 2020b). Currently, there are three main paradigms"
2021.acl-long.146,N19-1421,0,0.022154,"e guidance, and external contexts help knowledge prediction mostly because they contain the correct answer, explicitly or implicitly. These findings strongly question previous conclusions that current MLMs could serve as reliable factual knowledge bases. The findings of this paper can benefit the community in many directions. By explaining the underlying predicting mechanisms of MLMs, we provide reliable explanations for many previous knowledgeintensive techniques. For example, our method can explain why and how incorporating external contexts will help knowledge extraction and CommonsenseQA (Talmor et al., 2019). Our findings also reveal why PLM probing datasets may not be reliable and how the evaluation can be promoted by designing de-biased evaluation datasets. This paper also sheds light on future research directions. For instance, knowing the main benefit of illustrative cases comes from type-guidance, we can enhance many type-centric prediction tasks such as NER (Lample et al., 2016) and factoid QA (Iyyer et al., 2014). Moreover, based on the mechanism of incorporating external contexts, we can better evaluate, seek, and denoise external contexts for different tasks using MLMs. For example, we c"
2021.acl-long.146,2020.acl-main.357,0,0.0368469,"Missing"
2021.acl-long.217,W06-0901,0,0.110087,"Crime • Artifact • Agent • Vehicle • Time • Time Sequence-to-Structure Network Constraint Event Type Transport Controllable Generation Event Type Arrest-Jail Trigger returned Trigger capture Artifact The man Person The man Time Tuesday Agent bounty hunters Destination Los Angeles Origin Mexico Figure 1: The framework of T EXT 2E VENT. Here, T EXT 2E VENT takes raw text as input and generates a Transport event and an Arrest-Jail event. Introduction Event extraction is an essential task for natural language understanding, aiming to transform the text into event records (Doddington et al., 2004; Ahn, 2006). For example, in Figure 1, mapping “The man returned to Los Angeles from Mexico following his capture Tuesday by bounty hunters.” into two event records {Type: Transport, Trigger: returned, Arg1 Role: Artifact, Arg1: The man, Arg2 Role: Destination, Arg2: Los Angeles, ... } and {Type: Arrest-Jail, Trigger: capture, Arg1 Role: Person, Arg1: The man, Arg2 Role: Agent, Arg2: bounty hunters, ... }. Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. First, an event record contains event type, trigger, and arguments, which ∗ Co"
2021.acl-long.217,2020.acl-main.152,0,0.168789,"ity p(yi |y<i , x) at each decoding step i. Unfortunately, this greedy decoding algorithm cannot guarantee the generation of valid event structures. In other words, it could end up with invalid event types, mismatch of argumenttype, and incomplete structure. Furthermore, the greedy decoding algorithm ignores the useful event schema knowledge, which can be used to guide the decoding effectively. For example, we can constrain the model to only generate event type tokens in the type position. To exploit the event schema knowledge, we propose to employ a trie-based constrained decoding algorithm (Chen et al., 2020a; Cao et al., 2021) for event generation. During constrained decoding, the event schema knowledge is injected as the prompt of the decoder and ensures the generation of valid event structures. Concretely, unlike the greedy decoding algorithm that selects the token from the whole target vocabulary V at each step, our trie-based constrained decoding method dynamically chooses and prunes a candidate vocabulary V 0 based on the current generated state. A complete linearized form decoding process can be represented by executing a trie tree search, as shown in Figure 3a. Specifically, each generati"
2021.acl-long.217,P15-1017,0,0.020339,"er, and arguments, which ∗ Corresponding authors. form a table-like structure. And different event types have different structures. For example, in Figure 1, Transport and Arrest-Jail have entirely different structures. Second, an event can be expressed using very different utterances, such as diversified trigger words and heterogeneous syntactic structures. For example, both “the dismission of the man” and “the man departed his job” express the same event record {Type: End-Position, Arg1 Role: PERSON, Arg1: the man}. Currently, most event extraction methods employ the decomposition strategy (Chen et al., 2015; Nguyen and Nguyen, 2019; Wadden et al., 2019; Zhang et al., 2019b; Du and Cardie, 2020; Li et al., 2020; Paolini et al., 2021), i.e., decomposing the prediction of complex event structures into multiple separated subtasks (mostly including entity recognition, trigger detection, argument classifica2795 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2795–2806 August 1–6, 2021. ©2021 Association for Computational Linguistics tion), and then compose the components of differe"
2021.acl-long.217,2020.spnlp-1.9,0,0.0320886,"Missing"
2021.acl-long.217,doddington-etal-2004-automatic,0,0.260541,"tion • Person • Origin • Crime • Artifact • Agent • Vehicle • Time • Time Sequence-to-Structure Network Constraint Event Type Transport Controllable Generation Event Type Arrest-Jail Trigger returned Trigger capture Artifact The man Person The man Time Tuesday Agent bounty hunters Destination Los Angeles Origin Mexico Figure 1: The framework of T EXT 2E VENT. Here, T EXT 2E VENT takes raw text as input and generates a Transport event and an Arrest-Jail event. Introduction Event extraction is an essential task for natural language understanding, aiming to transform the text into event records (Doddington et al., 2004; Ahn, 2006). For example, in Figure 1, mapping “The man returned to Los Angeles from Mexico following his capture Tuesday by bounty hunters.” into two event records {Type: Transport, Trigger: returned, Arg1 Role: Artifact, Arg1: The man, Arg2 Role: Destination, Arg2: Los Angeles, ... } and {Type: Arrest-Jail, Trigger: capture, Arg1 Role: Person, Arg1: The man, Arg2 Role: Agent, Arg2: bounty hunters, ... }. Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. First, an event record contains event type, trigger, and arguments"
2021.acl-long.217,P16-1004,0,0.0133257,"2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has shown its strong structure prediction ability and has been widely used in many NLP tasks, such as machine translation (Kalchbrenner and Blunsom, 2013), semantic parsing (Dong and Lapata, 2016; Song et al., 2020), entity extraction (Strakov´a et al., 2019), relation extraction (Zeng et al., 2018; Zhang et al., 2020b), and aspect term extraction (Ma et al., 2019). Like T EXT 2E VENT in this paper, TANL (Paolini et al., 2021) and GRIT (Du et al., 2021) also employ neural generation models for event extraction, but they focus on sequence generation, rather than structure generation. Different from previous works that extract text span via labeling (Strakov´a et al., 2019) or copy/pointer mechanism (Zeng et al., 2018; Du et al., 2021), T EXT 2E VENT directly generate event schemas and"
2021.acl-long.217,2020.emnlp-main.49,0,0.222984,"ferent event types have different structures. For example, in Figure 1, Transport and Arrest-Jail have entirely different structures. Second, an event can be expressed using very different utterances, such as diversified trigger words and heterogeneous syntactic structures. For example, both “the dismission of the man” and “the man departed his job” express the same event record {Type: End-Position, Arg1 Role: PERSON, Arg1: the man}. Currently, most event extraction methods employ the decomposition strategy (Chen et al., 2015; Nguyen and Nguyen, 2019; Wadden et al., 2019; Zhang et al., 2019b; Du and Cardie, 2020; Li et al., 2020; Paolini et al., 2021), i.e., decomposing the prediction of complex event structures into multiple separated subtasks (mostly including entity recognition, trigger detection, argument classifica2795 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2795–2806 August 1–6, 2021. ©2021 Association for Computational Linguistics tion), and then compose the components of different subtasks for predicting the whole event structure (e.g., pipeline modeling, joint mod"
2021.acl-long.217,2021.eacl-main.52,0,0.0245794,"ds to better decision interactions and information sharing. The neural encoder-decoder generation architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has shown its strong structure prediction ability and has been widely used in many NLP tasks, such as machine translation (Kalchbrenner and Blunsom, 2013), semantic parsing (Dong and Lapata, 2016; Song et al., 2020), entity extraction (Strakov´a et al., 2019), relation extraction (Zeng et al., 2018; Zhang et al., 2020b), and aspect term extraction (Ma et al., 2019). Like T EXT 2E VENT in this paper, TANL (Paolini et al., 2021) and GRIT (Du et al., 2021) also employ neural generation models for event extraction, but they focus on sequence generation, rather than structure generation. Different from previous works that extract text span via labeling (Strakov´a et al., 2019) or copy/pointer mechanism (Zeng et al., 2018; Du et al., 2021), T EXT 2E VENT directly generate event schemas and text spans to form event records via constrained decoding (Cao et al., 2021; Chen et al., 2020a), which allows T EXT 2E VENT to handle various event types and transfer to new types easily. Table 5: Experiment results of variants trained with different-sized trai"
2021.acl-long.217,P11-1113,0,0.0889826,"Missing"
2021.acl-long.217,P18-1048,0,0.0330625,"Missing"
2021.acl-long.217,P16-1025,0,0.0191791,"ream methods usually use different strategies to obtain a complete event structure. These methods can be divided into: 1) pipeline classification (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011, 2018; Huang and Riloff, 2012; Chen et al., 2015; Sha et al., 2016; Lin et al., 2018; Yang et al., 2019; Wang et al., 2019; Ma et al., 2020; Zhang et al., 2020c), 2) multi-task joint models (McClosky et al., 2011; Li et al., 2013, 2014; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2018; Zhang et al., 2019a; Zheng et al., 2019), 3) semantic structure grounding (Huang et al., 2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has shown its strong structure prediction ability and has been widely used in many NLP tasks, such as machine translation (Kalchbrenner and Blunsom, 2013), semantic parsing (Dong"
2021.acl-long.217,P18-1201,0,0.0347051,"Missing"
2021.acl-long.217,P08-1030,0,0.17087,"Missing"
2021.acl-long.217,D13-1176,0,0.0228283,"19), 3) semantic structure grounding (Huang et al., 2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has shown its strong structure prediction ability and has been widely used in many NLP tasks, such as machine translation (Kalchbrenner and Blunsom, 2013), semantic parsing (Dong and Lapata, 2016; Song et al., 2020), entity extraction (Strakov´a et al., 2019), relation extraction (Zeng et al., 2018; Zhang et al., 2020b), and aspect term extraction (Ma et al., 2019). Like T EXT 2E VENT in this paper, TANL (Paolini et al., 2021) and GRIT (Du et al., 2021) also employ neural generation models for event extraction, but they focus on sequence generation, rather than structure generation. Different from previous works that extract text span via labeling (Strakov´a et al., 2019) or copy/pointer mechanism (Zeng et al., 2018; Du et al., 2021), T EXT 2E"
2021.acl-long.217,2020.findings-emnlp.73,0,0.225513,"ve different structures. For example, in Figure 1, Transport and Arrest-Jail have entirely different structures. Second, an event can be expressed using very different utterances, such as diversified trigger words and heterogeneous syntactic structures. For example, both “the dismission of the man” and “the man departed his job” express the same event record {Type: End-Position, Arg1 Role: PERSON, Arg1: the man}. Currently, most event extraction methods employ the decomposition strategy (Chen et al., 2015; Nguyen and Nguyen, 2019; Wadden et al., 2019; Zhang et al., 2019b; Du and Cardie, 2020; Li et al., 2020; Paolini et al., 2021), i.e., decomposing the prediction of complex event structures into multiple separated subtasks (mostly including entity recognition, trigger detection, argument classifica2795 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2795–2806 August 1–6, 2021. ©2021 Association for Computational Linguistics tion), and then compose the components of different subtasks for predicting the whole event structure (e.g., pipeline modeling, joint modeling or joint in"
2021.acl-long.217,D14-1198,0,0.0249817,"Missing"
2021.acl-long.217,P13-1008,0,0.0294107,"directions: event extraction and structure prediction via neural generation model. Event extraction has received widespread attention in recent years, and mainstream methods usually use different strategies to obtain a complete event structure. These methods can be divided into: 1) pipeline classification (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011, 2018; Huang and Riloff, 2012; Chen et al., 2015; Sha et al., 2016; Lin et al., 2018; Yang et al., 2019; Wang et al., 2019; Ma et al., 2020; Zhang et al., 2020c), 2) multi-task joint models (McClosky et al., 2011; Li et al., 2013, 2014; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2018; Zhang et al., 2019a; Zheng et al., 2019), 3) semantic structure grounding (Huang et al., 2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has shown its"
2021.acl-long.217,P10-1081,0,0.119329,"Missing"
2021.acl-long.217,P18-1145,1,0.893805,"Missing"
2021.acl-long.217,2020.acl-main.713,0,0.404679,"We used the same split and preprocessing step as the previous work (Zhang et al., 2019b; Wadden et al., 2019; Du and Cardie, 2020), and we denote it as ACE05-EN. Dataset Split #Sents #Events #Roles ACE05-EN Train Dev Test 17,172 923 832 4,202 450 403 4,859 605 576 ACE05-EN+ Train Dev Test 19,216 901 676 4,419 468 424 6,607 759 689 ERE-EN Train Dev Test 14,736 1,209 1,163 6,208 525 551 8,924 730 822 Table 1: Dataset statistics. In addition to ACE05-EN, we also conducted experiments on two other benchmarks: ACE05-EN+ and ERE-EN, using the same split and preprocessing step in the previous work (Lin et al., 2020). Compared to ACE05-EN, ACE05-EN+ and EREEN further consider pronoun roles and multi-token event triggers. ERE-EN contains 38 event categories and 458 documents. Statistics of all datasets are shown in Table 1. For evaluation, we used the same criteria in previous work (Zhang et al., 2019b; Wadden et al., 2019; Lin et al., 2020). Since T EXT 2E VENT is a text generation model, we reconstructed the offset of predicted trigger mentions by finding the matched utterance in the input sequence one by one. For argument mentions, we found the nearest matched utterance to the predicted trigger mention"
2021.acl-long.217,2020.emnlp-main.128,0,0.0217186,"assification (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011, 2018; Huang and Riloff, 2012; Chen et al., 2015; Sha et al., 2016; Lin et al., 2018; Yang et al., 2019; Wang et al., 2019; Ma et al., 2020; Zhang et al., 2020c), 2) multi-task joint models (McClosky et al., 2011; Li et al., 2013, 2014; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2018; Zhang et al., 2019a; Zheng et al., 2019), 3) semantic structure grounding (Huang et al., 2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has shown its strong structure prediction ability and has been widely used in many NLP tasks, such as machine translation (Kalchbrenner and Blunsom, 2013), semantic parsing (Dong and Lapata, 2016; Song et al., 2020), entity extraction (Strakov´a et al., 2019), relation extraction (Zeng et al., 2018; Zhang et a"
2021.acl-long.217,D18-1156,0,0.0319383,"Missing"
2021.acl-long.217,P19-1344,0,0.0258719,"el all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has shown its strong structure prediction ability and has been widely used in many NLP tasks, such as machine translation (Kalchbrenner and Blunsom, 2013), semantic parsing (Dong and Lapata, 2016; Song et al., 2020), entity extraction (Strakov´a et al., 2019), relation extraction (Zeng et al., 2018; Zhang et al., 2020b), and aspect term extraction (Ma et al., 2019). Like T EXT 2E VENT in this paper, TANL (Paolini et al., 2021) and GRIT (Du et al., 2021) also employ neural generation models for event extraction, but they focus on sequence generation, rather than structure generation. Different from previous works that extract text span via labeling (Strakov´a et al., 2019) or copy/pointer mechanism (Zeng et al., 2018; Du et al., 2021), T EXT 2E VENT directly generate event schemas and text spans to form event records via constrained decoding (Cao et al., 2021; Chen et al., 2020a), which allows T EXT 2E VENT to handle various event types and transfer to n"
2021.acl-long.217,2020.findings-emnlp.318,0,0.0177256,".9 27.3 7.0 44.0 44.7 44.4 8.2 53.3 52.6 52.3 28.9 Our work is a synthesis of two research directions: event extraction and structure prediction via neural generation model. Event extraction has received widespread attention in recent years, and mainstream methods usually use different strategies to obtain a complete event structure. These methods can be divided into: 1) pipeline classification (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011, 2018; Huang and Riloff, 2012; Chen et al., 2015; Sha et al., 2016; Lin et al., 2018; Yang et al., 2019; Wang et al., 2019; Ma et al., 2020; Zhang et al., 2020c), 2) multi-task joint models (McClosky et al., 2011; Li et al., 2013, 2014; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2018; Zhang et al., 2019a; Zheng et al., 2019), 3) semantic structure grounding (Huang et al., 2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-deco"
2021.acl-long.217,P11-1163,0,0.0443934,"nthesis of two research directions: event extraction and structure prediction via neural generation model. Event extraction has received widespread attention in recent years, and mainstream methods usually use different strategies to obtain a complete event structure. These methods can be divided into: 1) pipeline classification (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011, 2018; Huang and Riloff, 2012; Chen et al., 2015; Sha et al., 2016; Lin et al., 2018; Yang et al., 2019; Wang et al., 2019; Ma et al., 2020; Zhang et al., 2020c), 2) multi-task joint models (McClosky et al., 2011; Li et al., 2013, 2014; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2018; Zhang et al., 2019a; Zheng et al., 2019), 3) semantic structure grounding (Huang et al., 2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation architecture (Sutskever et al., 2014; Bahdanau et al., 201"
2021.acl-long.217,N16-1034,0,0.0175715,"iction via neural generation model. Event extraction has received widespread attention in recent years, and mainstream methods usually use different strategies to obtain a complete event structure. These methods can be divided into: 1) pipeline classification (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011, 2018; Huang and Riloff, 2012; Chen et al., 2015; Sha et al., 2016; Lin et al., 2018; Yang et al., 2019; Wang et al., 2019; Ma et al., 2020; Zhang et al., 2020c), 2) multi-task joint models (McClosky et al., 2011; Li et al., 2013, 2014; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2018; Zhang et al., 2019a; Zheng et al., 2019), 3) semantic structure grounding (Huang et al., 2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has shown its strong structure prediction ability and has been wid"
2021.acl-long.217,P16-1116,0,0.0389551,"Missing"
2021.acl-long.217,2020.acl-main.712,0,0.0430364,"l., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has shown its strong structure prediction ability and has been widely used in many NLP tasks, such as machine translation (Kalchbrenner and Blunsom, 2013), semantic parsing (Dong and Lapata, 2016; Song et al., 2020), entity extraction (Strakov´a et al., 2019), relation extraction (Zeng et al., 2018; Zhang et al., 2020b), and aspect term extraction (Ma et al., 2019). Like T EXT 2E VENT in this paper, TANL (Paolini et al., 2021) and GRIT (Du et al., 2021) also employ neural generation models for event extraction, but they focus on sequence generation, rather than structure generation. Different from previous works that extract text span via labeling (Strakov´a et al., 2019) or copy/pointer mechanism (Zeng et al., 2018; Du et al., 2021), T EXT 2E VENT directly generate event schemas and text spans to form e"
2021.acl-long.217,P19-1527,0,0.0563615,"Missing"
2021.acl-long.217,D19-1585,0,0.348685,"hors. form a table-like structure. And different event types have different structures. For example, in Figure 1, Transport and Arrest-Jail have entirely different structures. Second, an event can be expressed using very different utterances, such as diversified trigger words and heterogeneous syntactic structures. For example, both “the dismission of the man” and “the man departed his job” express the same event record {Type: End-Position, Arg1 Role: PERSON, Arg1: the man}. Currently, most event extraction methods employ the decomposition strategy (Chen et al., 2015; Nguyen and Nguyen, 2019; Wadden et al., 2019; Zhang et al., 2019b; Du and Cardie, 2020; Li et al., 2020; Paolini et al., 2021), i.e., decomposing the prediction of complex event structures into multiple separated subtasks (mostly including entity recognition, trigger detection, argument classifica2795 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2795–2806 August 1–6, 2021. ©2021 Association for Computational Linguistics tion), and then compose the components of different subtasks for predicting the whole event str"
2021.acl-long.217,D19-1584,0,0.0146308,"3.7 2.3 0.0 33.6 30.9 27.3 7.0 44.0 44.7 44.4 8.2 53.3 52.6 52.3 28.9 Our work is a synthesis of two research directions: event extraction and structure prediction via neural generation model. Event extraction has received widespread attention in recent years, and mainstream methods usually use different strategies to obtain a complete event structure. These methods can be divided into: 1) pipeline classification (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011, 2018; Huang and Riloff, 2012; Chen et al., 2015; Sha et al., 2016; Lin et al., 2018; Yang et al., 2019; Wang et al., 2019; Ma et al., 2020; Zhang et al., 2020c), 2) multi-task joint models (McClosky et al., 2011; Li et al., 2013, 2014; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2018; Zhang et al., 2019a; Zheng et al., 2019), 3) semantic structure grounding (Huang et al., 2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The ne"
2021.acl-long.217,2020.acl-main.542,0,0.0215998,"nce-to-structure generation models is more challenging: 1) There is an output gap between the event generation model and the text-to-text generation model. Compared with natural word sequences, the linearized event structure contains many non-semantic indicators 2798 such as “(” and “)”, and they don’t follow the syntax constraints of natural language sentences. 2) The non-semantic indicators “(” and “)” appear very frequently but contain little semantic information, which will mislead the learning process. To address the above challenges, we employ a curriculum learning (Bengio et al., 2009; Xu et al., 2020) strategy. Specifically, we first train PLMs using simple event substructure generation tasks so that they would not overfit in non-semantic indicators; then we train the model on the full event structure generation task. Substructure Learning. Because event representations often have complex structures and their token sequences are different from natural language word sequences, it is challenging to train them with the full sequence generation task directly. Therefore, we first train T EXT 2E VENT on simple event substructures. Specifically, we learn our model by starting from generating only"
2021.acl-long.217,N16-1033,0,0.0177047,"action and structure prediction via neural generation model. Event extraction has received widespread attention in recent years, and mainstream methods usually use different strategies to obtain a complete event structure. These methods can be divided into: 1) pipeline classification (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011, 2018; Huang and Riloff, 2012; Chen et al., 2015; Sha et al., 2016; Lin et al., 2018; Yang et al., 2019; Wang et al., 2019; Ma et al., 2020; Zhang et al., 2020c), 2) multi-task joint models (McClosky et al., 2011; Li et al., 2013, 2014; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2018; Zhang et al., 2019a; Zheng et al., 2019), 3) semantic structure grounding (Huang et al., 2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has shown its strong structure prediction abi"
2021.acl-long.217,P19-1522,0,0.0185556,"w/o CD w/o ES 8.6 3.7 2.3 0.0 33.6 30.9 27.3 7.0 44.0 44.7 44.4 8.2 53.3 52.6 52.3 28.9 Our work is a synthesis of two research directions: event extraction and structure prediction via neural generation model. Event extraction has received widespread attention in recent years, and mainstream methods usually use different strategies to obtain a complete event structure. These methods can be divided into: 1) pipeline classification (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011, 2018; Huang and Riloff, 2012; Chen et al., 2015; Sha et al., 2016; Lin et al., 2018; Yang et al., 2019; Wang et al., 2019; Ma et al., 2020; Zhang et al., 2020c), 2) multi-task joint models (McClosky et al., 2011; Li et al., 2013, 2014; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2018; Zhang et al., 2019a; Zheng et al., 2019), 3) semantic structure grounding (Huang et al., 2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and informat"
2021.acl-long.217,P18-1047,0,0.0206569,"al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation architecture (Sutskever et al., 2014; Bahdanau et al., 2015) has shown its strong structure prediction ability and has been widely used in many NLP tasks, such as machine translation (Kalchbrenner and Blunsom, 2013), semantic parsing (Dong and Lapata, 2016; Song et al., 2020), entity extraction (Strakov´a et al., 2019), relation extraction (Zeng et al., 2018; Zhang et al., 2020b), and aspect term extraction (Ma et al., 2019). Like T EXT 2E VENT in this paper, TANL (Paolini et al., 2021) and GRIT (Du et al., 2021) also employ neural generation models for event extraction, but they focus on sequence generation, rather than structure generation. Different from previous works that extract text span via labeling (Strakov´a et al., 2019) or copy/pointer mechanism (Zeng et al., 2018; Du et al., 2021), T EXT 2E VENT directly generate event schemas and text spans to form event records via constrained decoding (Cao et al., 2021; Chen et al., 2020a), which"
2021.acl-long.217,2020.findings-emnlp.23,0,0.0302584,"44.7 44.4 8.2 53.3 52.6 52.3 28.9 Our work is a synthesis of two research directions: event extraction and structure prediction via neural generation model. Event extraction has received widespread attention in recent years, and mainstream methods usually use different strategies to obtain a complete event structure. These methods can be divided into: 1) pipeline classification (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011, 2018; Huang and Riloff, 2012; Chen et al., 2015; Sha et al., 2016; Lin et al., 2018; Yang et al., 2019; Wang et al., 2019; Ma et al., 2020; Zhang et al., 2020c), 2) multi-task joint models (McClosky et al., 2011; Li et al., 2013, 2014; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2018; Zhang et al., 2019a; Zheng et al., 2019), 3) semantic structure grounding (Huang et al., 2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation archi"
2021.acl-long.217,2020.acl-main.667,0,0.0195062,"44.7 44.4 8.2 53.3 52.6 52.3 28.9 Our work is a synthesis of two research directions: event extraction and structure prediction via neural generation model. Event extraction has received widespread attention in recent years, and mainstream methods usually use different strategies to obtain a complete event structure. These methods can be divided into: 1) pipeline classification (Ahn, 2006; Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011, 2018; Huang and Riloff, 2012; Chen et al., 2015; Sha et al., 2016; Lin et al., 2018; Yang et al., 2019; Wang et al., 2019; Ma et al., 2020; Zhang et al., 2020c), 2) multi-task joint models (McClosky et al., 2011; Li et al., 2013, 2014; Yang and Mitchell, 2016; Nguyen et al., 2016; Liu et al., 2018; Zhang et al., 2019a; Zheng et al., 2019), 3) semantic structure grounding (Huang et al., 2016, 2018; Zhang et al., 2020a), and 4) question-answering (Chen et al., 2020b; Du and Cardie, 2020; Li et al., 2020; Liu et al., 2020). Compared with previous methods, we model all subtasks of event extraction in a uniform sequenceto-structure framework, which leads to better decision interactions and information sharing. The neural encoder-decoder generation archi"
2021.acl-long.217,D19-1032,0,0.0293611,"Missing"
2021.acl-long.361,P98-1012,0,0.598099,"Missing"
2021.acl-long.361,P19-1279,0,0.237622,"w that our methods outperform previous state-of-the-art methods and are robust across different datasets1 . 1 Introduction Relation extraction (RE) is the task to extract relation between entity pair in plain text. For example, when given the entity pair (Obama, the United States) in the sentence “Obama was sworn in as the 44th president of the United States”, an RE model should accurately predict the relationship “President of” and extract the corresponding triplet (Obama, President of, the United States) for downstream tasks. Despite the success of many RE models (Zeng et al., 2014; Baldini Soares et al., 2019), most previous RE paradigms rely on the predefined relation types, which are always unavailable in open domain scenario and thereby limits their capability in real applications. ∗ 1 Corresponding authors. Code available at https://github.com/Lfc1993/EI ORE P E ??(?) C P E P C E ??(?) X X X Y Y Y (a) (b) (c) E Entity pair C Context P Relation Prototype X Relation instance C Figure 1: The Structural Causal Model demonstrates the procedure of OpenRE. (a) is the original SCM; (b) Entity intervention that fixes the entity pair and adjusts different contexts; (c) Context intervention that fixes the"
2021.acl-long.361,H05-1091,0,0.23497,"t-sne (van der Maaten and Hinton, 2008) to map each representation to the dimension of 2. For the convenience of comparison, we color each instance with its ground-truth relation label. Since the visualization results of only Hyber or Gcc are marginally different from the full model, so we only choose the full model for visualization. As shown in Figure 3, we can see that each relation is mostly separate from others. However, there still be some instances misclassified due to the overlapping in the representation space. 5 Related Work Current success of supervised relation extraction methods (Bunescu and Mooney, 2005; Qian et al., 2008; Zeng et al., 2014; Zhou et al., 2016; Velikovi et al., 2018) depends heavily on large amount of annotated data. Due to this data bottleneck, some weakly-supervised methods are proposed to learn relation extraction models from distantly labeled datasets (Mintz et al., 2009; Hoffmann et al., 2011; Lin et al., 2016) or few-shot datasets (Han et al., 2018; Baldini Soares et al., 2019; Peng et al., 2020). However, these paradigms still require pre-defined relation types and therefore restricts their application to open scenarios. Open relation extraction, on the other hand, aim"
2021.acl-long.361,N19-1423,0,0.006942,"ing task. Therefore the most critical challenge for OpenRE is how to learn effective representations for relation instances and then cluster them. To this end, Yao et al. (2011) adopts topic model (Blei et al., 2003) to generate latent relation type for unlabelled instances. Later works start to utilize datasets collected using distant supervision for model training. Along this line, Marcheggiani and Titov (2016) utilizes an auto-encoder model and trains the model through self-supervised signals from entity link predictor. Hu et al. (2020) encodes each instance with pretrained language model (Devlin et al., 2019; Baldini Soares et al., 2019) and learn the representation by self-supervised signals from pseudo labels. Unfortunately, current OpenRE models are often unstable and easily collapsed (Simon et al., 2019). For example, OpenRE models frequently cluster all relation instances with context “was born in” into the relation type BORN IN PLACE because they share similar context information. However, “was born in” can also refer to the relation BORN IN TIME. Furthermore, current models also tend to cluster two relation instances with the same entities (i.e., relation instances with the same head and t"
2021.acl-long.361,L18-1544,0,0.0154976,"original prototypical relation instance, mC is the margin. 3.3 Surrogate Loss for Optimizing Causal Effects Based on entity ranking and context contrasting, we approximate the causal effects optimized in Equation (4) with the following ranking and contrastive loss: L(θ; X ) = LE (θ; X ) + LC (θ; X ). Experiments 4.1 Dataset We conduct experiments on two OpenRE datasets – T-REx SPO and T-REx DS, since these datasets are from the same data source but only differ in constructing settings, which is very suitable for evaluating the stability of OpenRE methods. These datasets are both from T-REx3 (Elsahar et al., 2018) – a dataset consists of Wikipedia sentences that are distantly aligned with Wikidata relation triplets; and these aligned sentences are further collected as T-REx SPO and T-REx DS according to whether they have surface-form relations or not. As a result, T-REx SPO contains 763,000 sentences of 615 relations, and T-REx DS contains nearly 12 million sentences of 1189 relations. For both datasets, we 3 https://hadyelsahar.github.io/t-rex/ 4.2 Baseline and Evaluation Metrics Baseline Methods. We compare our model with the following baselines: 1) rel-LDA (Yao et al., 2011), a generative model that"
2021.acl-long.361,D18-1407,0,0.0249557,"Missing"
2021.acl-long.361,2020.acl-main.703,0,0.0132489,") + mE ), (6) where θ is the model parameters, D(Xi , P ) is the distances between representations of generated relation instance Xi and prototypical relation instance P . X is the intervened relation instance set, mE is the margin for entity ranking loss, and n = 3 is the depth of the entity hierarchy. 3.2 Generation-based Context Contrasting for Entity Intervention Different from the context intervention that can easily replace entities, it is more difficult to intervene on entities and modify the context. Fortunately, the rapid progress in pre-trained language model (Radford et al., 2019; Lewis et al., 2020; Raffel et al., 2020) makes the language generation from RDF data2 available (Ribeiro et al., 2020). So in this work, we take a different paradigm named Generation-based Context Contrasting, which directly generates different relation instances from specifically designed relation triplets, and approximately learn to optimize the causal effects of P(Y = 1, P |do(E)) and P(Y = 0, P |do(E)) in Equation (4) via contrastive learning. Specifically, we first sample relation triplets from Wikidata as prototypical relation instance P , and then generates relation triplets with the same entities but di"
2021.acl-long.361,2020.emnlp-main.357,0,0.0235381,"egularizers to guide the learning procedure. But the fundamental cause of the instability is still undiscovered. In this paper, we revisit the procedure of OpenRE from a causal view. By formulating OpenRE using a structural causal model, we identify the cause of the above-mentioned problems, and alleviate the problems by Element Intervention. There are also some recent studies try to introduce causal theory to explain the spurious correlations in neural models (Feng et al., 2018; Gururangan et al., 2018; Tang et al., 2020; Qi et al., 2020; Zeng et al., 2020; Wu et al., 2020; Qin et al., 2020; Fu et al., 2020). However, to the best of our knowledge, this is the first work to revisit OpenRE from the perspective of causality. 6 Conclusions In this paper, we revisit OpenRE from the perspective of causal theory. We find that the strong connections between the generated instance to the prototypical instance through either their entities or their context will result in spurious correlations, which appear in the form of the backdoor paths in the SCM. Then the spurious correlations will mislead OpenRE models. Based on the observations, we propose Element Intervention to block the backdoor paths, which inte"
2021.acl-long.361,P16-1200,0,0.0159875,"own in Figure 3, we can see that each relation is mostly separate from others. However, there still be some instances misclassified due to the overlapping in the representation space. 5 Related Work Current success of supervised relation extraction methods (Bunescu and Mooney, 2005; Qian et al., 2008; Zeng et al., 2014; Zhou et al., 2016; Velikovi et al., 2018) depends heavily on large amount of annotated data. Due to this data bottleneck, some weakly-supervised methods are proposed to learn relation extraction models from distantly labeled datasets (Mintz et al., 2009; Hoffmann et al., 2011; Lin et al., 2016) or few-shot datasets (Han et al., 2018; Baldini Soares et al., 2019; Peng et al., 2020). However, these paradigms still require pre-defined relation types and therefore restricts their application to open scenarios. Open relation extraction, on the other hand, aims to cluster relation instances referring to the same underlying relation without pre-defined relation types. Previous methods for OpenRE can be roughly divided into two categories. The generative method (Yao et al., 2011) formulates OpenRE using a topic model, and the latent relations are generated based on the hand-crafted feature"
2021.acl-long.361,W17-3518,0,0.154872,"lations that extracts only based on the entity pair and is considered as a negative instance to the prototypical relation instance. Then we use the generator to generate texts based on these triplets. Specifically, we first wrap the triplets with special markers “[H], [T],[ R]” corresponds to head entity, tail entity, and relation name. Then we input the concatenated texts for relation instance generation. In our implementation, we 2 https://www.w3.org/TR/WD-rdf-syntax-971002/ use T5 (Raffel et al., 2020; Ribeiro et al., 2020) as the base generator, and pre-train the generator on WebNLG data (Gardent et al., 2017). After sampled these intervened instances, we approximately optimize P(Y, P |do(E)) using the following contrastive loss function: X X LC (θ; X ) = max(D(P, Xp ) Xp ∈P Xn ∈N −D(P, Xn ) + mC , 0), (7) where θ is the model parameters, X is the intervened instance set, P is the positive instance set generated from relation renaming and context expansion, N is the negative instance set generated from relation replacing, P is the original prototypical relation instance, mC is the margin. 3.3 Surrogate Loss for Optimizing Causal Effects Based on entity ranking and context contrasting, we approximat"
2021.acl-long.361,N18-2017,0,0.0634996,"Missing"
2021.acl-long.361,D18-1514,0,0.0199806,"lation is mostly separate from others. However, there still be some instances misclassified due to the overlapping in the representation space. 5 Related Work Current success of supervised relation extraction methods (Bunescu and Mooney, 2005; Qian et al., 2008; Zeng et al., 2014; Zhou et al., 2016; Velikovi et al., 2018) depends heavily on large amount of annotated data. Due to this data bottleneck, some weakly-supervised methods are proposed to learn relation extraction models from distantly labeled datasets (Mintz et al., 2009; Hoffmann et al., 2011; Lin et al., 2016) or few-shot datasets (Han et al., 2018; Baldini Soares et al., 2019; Peng et al., 2020). However, these paradigms still require pre-defined relation types and therefore restricts their application to open scenarios. Open relation extraction, on the other hand, aims to cluster relation instances referring to the same underlying relation without pre-defined relation types. Previous methods for OpenRE can be roughly divided into two categories. The generative method (Yao et al., 2011) formulates OpenRE using a topic model, and the latent relations are generated based on the hand-crafted feature representations of entities and context"
2021.acl-long.361,P11-1055,0,0.0449031,"or visualization. As shown in Figure 3, we can see that each relation is mostly separate from others. However, there still be some instances misclassified due to the overlapping in the representation space. 5 Related Work Current success of supervised relation extraction methods (Bunescu and Mooney, 2005; Qian et al., 2008; Zeng et al., 2014; Zhou et al., 2016; Velikovi et al., 2018) depends heavily on large amount of annotated data. Due to this data bottleneck, some weakly-supervised methods are proposed to learn relation extraction models from distantly labeled datasets (Mintz et al., 2009; Hoffmann et al., 2011; Lin et al., 2016) or few-shot datasets (Han et al., 2018; Baldini Soares et al., 2019; Peng et al., 2020). However, these paradigms still require pre-defined relation types and therefore restricts their application to open scenarios. Open relation extraction, on the other hand, aims to cluster relation instances referring to the same underlying relation without pre-defined relation types. Previous methods for OpenRE can be roughly divided into two categories. The generative method (Yao et al., 2011) formulates OpenRE using a topic model, and the latent relations are generated based on the ha"
2021.acl-long.361,2020.emnlp-main.299,0,0.157262,"hieve this, OpenRE is commonly formulated as a clustering or pair-matching task. Therefore the most critical challenge for OpenRE is how to learn effective representations for relation instances and then cluster them. To this end, Yao et al. (2011) adopts topic model (Blei et al., 2003) to generate latent relation type for unlabelled instances. Later works start to utilize datasets collected using distant supervision for model training. Along this line, Marcheggiani and Titov (2016) utilizes an auto-encoder model and trains the model through self-supervised signals from entity link predictor. Hu et al. (2020) encodes each instance with pretrained language model (Devlin et al., 2019; Baldini Soares et al., 2019) and learn the representation by self-supervised signals from pseudo labels. Unfortunately, current OpenRE models are often unstable and easily collapsed (Simon et al., 2019). For example, OpenRE models frequently cluster all relation instances with context “was born in” into the relation type BORN IN PLACE because they share similar context information. However, “was born in” can also refer to the relation BORN IN TIME. Furthermore, current models also tend to cluster two relation instances"
2021.acl-long.361,Q16-1017,0,0.393373,"relation instance consisting of two entities and their context, OpenRE aims to identify other instances which mention the same relation. To achieve this, OpenRE is commonly formulated as a clustering or pair-matching task. Therefore the most critical challenge for OpenRE is how to learn effective representations for relation instances and then cluster them. To this end, Yao et al. (2011) adopts topic model (Blei et al., 2003) to generate latent relation type for unlabelled instances. Later works start to utilize datasets collected using distant supervision for model training. Along this line, Marcheggiani and Titov (2016) utilizes an auto-encoder model and trains the model through self-supervised signals from entity link predictor. Hu et al. (2020) encodes each instance with pretrained language model (Devlin et al., 2019; Baldini Soares et al., 2019) and learn the representation by self-supervised signals from pseudo labels. Unfortunately, current OpenRE models are often unstable and easily collapsed (Simon et al., 2019). For example, OpenRE models frequently cluster all relation instances with context “was born in” into the relation type BORN IN PLACE because they share similar context information. However, “"
2021.acl-long.361,P09-1113,0,0.0668335,"ose the full model for visualization. As shown in Figure 3, we can see that each relation is mostly separate from others. However, there still be some instances misclassified due to the overlapping in the representation space. 5 Related Work Current success of supervised relation extraction methods (Bunescu and Mooney, 2005; Qian et al., 2008; Zeng et al., 2014; Zhou et al., 2016; Velikovi et al., 2018) depends heavily on large amount of annotated data. Due to this data bottleneck, some weakly-supervised methods are proposed to learn relation extraction models from distantly labeled datasets (Mintz et al., 2009; Hoffmann et al., 2011; Lin et al., 2016) or few-shot datasets (Han et al., 2018; Baldini Soares et al., 2019; Peng et al., 2020). However, these paradigms still require pre-defined relation types and therefore restricts their application to open scenarios. Open relation extraction, on the other hand, aims to cluster relation instances referring to the same underlying relation without pre-defined relation types. Previous methods for OpenRE can be roughly divided into two categories. The generative method (Yao et al., 2011) formulates OpenRE using a topic model, and the latent relations are ge"
2021.acl-long.361,P02-1040,0,0.112617,"on Generated Texts. This experiment studies the effect of different data sources for Hyber module. As shown in Table 2, we can see that Hyber based on T-REx SPO dataset or the generated texts has marginal difference. That means Hyber is robust to the source context. On the other hand, the quality of the generated texts satisfies the demand of this task. Quality of Context Generation(unseen relations). This experiment gives a quantitative analysis of the generator used in our work. We select WebNLG (Gardent et al., 2017) to test the generator, and adopt the widely-used metrics including BLEU (Papineni et al., 2002) and chrF++ (Popovi´c, 2017) for evaluation. As shown in Table 3, we can Figure 3: Visualization of relation representation learned by element intervention. Each relation instance is colored with the ground-truth label. see that our generator is quite effective on seen relation generation. Though the generator suffers a performance drop in unseen relations, the scores are still receptible. Combined with results from other experiments, the generator is sufficient for this task. Visualization of Relation Representations. In this experiment, we visual the representations of the validation instanc"
2021.acl-long.361,2020.emnlp-main.298,0,0.0600738,"Missing"
2021.acl-long.361,W17-4770,0,0.0249673,"Missing"
2021.acl-long.361,C08-1088,0,0.0233447,"Hinton, 2008) to map each representation to the dimension of 2. For the convenience of comparison, we color each instance with its ground-truth relation label. Since the visualization results of only Hyber or Gcc are marginally different from the full model, so we only choose the full model for visualization. As shown in Figure 3, we can see that each relation is mostly separate from others. However, there still be some instances misclassified due to the overlapping in the representation space. 5 Related Work Current success of supervised relation extraction methods (Bunescu and Mooney, 2005; Qian et al., 2008; Zeng et al., 2014; Zhou et al., 2016; Velikovi et al., 2018) depends heavily on large amount of annotated data. Due to this data bottleneck, some weakly-supervised methods are proposed to learn relation extraction models from distantly labeled datasets (Mintz et al., 2009; Hoffmann et al., 2011; Lin et al., 2016) or few-shot datasets (Han et al., 2018; Baldini Soares et al., 2019; Peng et al., 2020). However, these paradigms still require pre-defined relation types and therefore restricts their application to open scenarios. Open relation extraction, on the other hand, aims to cluster relati"
2021.acl-long.361,2020.emnlp-main.58,0,0.0908969,"Missing"
2021.acl-long.361,D07-1043,0,0.347883,"Missing"
2021.acl-long.361,P19-1133,0,0.160112,"t al., 2003) to generate latent relation type for unlabelled instances. Later works start to utilize datasets collected using distant supervision for model training. Along this line, Marcheggiani and Titov (2016) utilizes an auto-encoder model and trains the model through self-supervised signals from entity link predictor. Hu et al. (2020) encodes each instance with pretrained language model (Devlin et al., 2019; Baldini Soares et al., 2019) and learn the representation by self-supervised signals from pseudo labels. Unfortunately, current OpenRE models are often unstable and easily collapsed (Simon et al., 2019). For example, OpenRE models frequently cluster all relation instances with context “was born in” into the relation type BORN IN PLACE because they share similar context information. However, “was born in” can also refer to the relation BORN IN TIME. Furthermore, current models also tend to cluster two relation instances with the same entities (i.e., relation instances with the same head and tail entities) or the same entity types into one relation. This problem can be even more severe if the dataset is generated using distant supervision because it severely relies on prototypical context and"
2021.acl-long.361,D11-1135,0,0.430825,"on that fixes the context and adjusts different entity pairs. Open Relation Extraction (OpenRE), on the other hand, has been proposed to extract relation facts without pre-defined relation types neither annotated data. Given a relation instance consisting of two entities and their context, OpenRE aims to identify other instances which mention the same relation. To achieve this, OpenRE is commonly formulated as a clustering or pair-matching task. Therefore the most critical challenge for OpenRE is how to learn effective representations for relation instances and then cluster them. To this end, Yao et al. (2011) adopts topic model (Blei et al., 2003) to generate latent relation type for unlabelled instances. Later works start to utilize datasets collected using distant supervision for model training. Along this line, Marcheggiani and Titov (2016) utilizes an auto-encoder model and trains the model through self-supervised signals from entity link predictor. Hu et al. (2020) encodes each instance with pretrained language model (Devlin et al., 2019; Baldini Soares et al., 2019) and learn the representation by self-supervised signals from pseudo labels. Unfortunately, current OpenRE models are often unst"
2021.acl-long.361,C14-1220,0,0.191234,"ion extraction datasets show that our methods outperform previous state-of-the-art methods and are robust across different datasets1 . 1 Introduction Relation extraction (RE) is the task to extract relation between entity pair in plain text. For example, when given the entity pair (Obama, the United States) in the sentence “Obama was sworn in as the 44th president of the United States”, an RE model should accurately predict the relationship “President of” and extract the corresponding triplet (Obama, President of, the United States) for downstream tasks. Despite the success of many RE models (Zeng et al., 2014; Baldini Soares et al., 2019), most previous RE paradigms rely on the predefined relation types, which are always unavailable in open domain scenario and thereby limits their capability in real applications. ∗ 1 Corresponding authors. Code available at https://github.com/Lfc1993/EI ORE P E ??(?) C P E P C E ??(?) X X X Y Y Y (a) (b) (c) E Entity pair C Context P Relation Prototype X Relation instance C Figure 1: The Structural Causal Model demonstrates the procedure of OpenRE. (a) is the original SCM; (b) Entity intervention that fixes the entity pair and adjusts different contexts; (c) Conte"
2021.acl-long.361,2020.emnlp-main.590,0,0.0244806,"ffer from the instability, and they also propose two regularizers to guide the learning procedure. But the fundamental cause of the instability is still undiscovered. In this paper, we revisit the procedure of OpenRE from a causal view. By formulating OpenRE using a structural causal model, we identify the cause of the above-mentioned problems, and alleviate the problems by Element Intervention. There are also some recent studies try to introduce causal theory to explain the spurious correlations in neural models (Feng et al., 2018; Gururangan et al., 2018; Tang et al., 2020; Qi et al., 2020; Zeng et al., 2020; Wu et al., 2020; Qin et al., 2020; Fu et al., 2020). However, to the best of our knowledge, this is the first work to revisit OpenRE from the perspective of causality. 6 Conclusions In this paper, we revisit OpenRE from the perspective of causal theory. We find that the strong connections between the generated instance to the prototypical instance through either their entities or their context will result in spurious correlations, which appear in the form of the backdoor paths in the SCM. Then the spurious correlations will mislead OpenRE models. Based on the observations, we propose Element"
2021.acl-long.361,P16-2034,0,0.0196571,"on to the dimension of 2. For the convenience of comparison, we color each instance with its ground-truth relation label. Since the visualization results of only Hyber or Gcc are marginally different from the full model, so we only choose the full model for visualization. As shown in Figure 3, we can see that each relation is mostly separate from others. However, there still be some instances misclassified due to the overlapping in the representation space. 5 Related Work Current success of supervised relation extraction methods (Bunescu and Mooney, 2005; Qian et al., 2008; Zeng et al., 2014; Zhou et al., 2016; Velikovi et al., 2018) depends heavily on large amount of annotated data. Due to this data bottleneck, some weakly-supervised methods are proposed to learn relation extraction models from distantly labeled datasets (Mintz et al., 2009; Hoffmann et al., 2011; Lin et al., 2016) or few-shot datasets (Han et al., 2018; Baldini Soares et al., 2019; Peng et al., 2020). However, these paradigms still require pre-defined relation types and therefore restricts their application to open scenarios. Open relation extraction, on the other hand, aims to cluster relation instances referring to the same und"
2021.acl-long.371,W09-3302,0,0.0293559,"eriments on four standard datasets: (1) CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) is a well known opendomain NER dataset. It consists of 20744 sentences collected from 1393 English news articles and is annotated with four types: PER, ORG, LOC and MISC. (2) Twitter (Godin et al., 2015) is from the WNUT 2016 NER shared task. It consists of 7236 sentences with 10 entity types. (3) Webpage (Ratinov and Roth, 2009) contains 20 webpages, including personal, academic and computer-science conference homepages. It consists of 619 sentences with the four types the same as CoNLL2003. (4) Wikigold (Balasuriya et al., 2009) contains 149 articles from the May 22, 2008 dump of English Wikipedia. It consists of 1969 sentences with the same types of CoNLL2003. Distant Annotation Settings. We use two distant annotation settings: String-Matching and KBMatching (Liang et al., 2020). String-Matching labels dataset by directly matching names in dictionary with sentences. KB-Matching is more complex, which uses a set of hand-crafted rules to DictMatch , which perform NER by directly matching text with names in a dictionary, so no learning is needed. Fully-supervised baselines , including: (i) BiLSTM-CRF (Lample et al., 20"
2021.acl-long.371,D19-1025,0,0.01285,"in dictionary 0.6 Introduction ∗ (a) the application of current NER models. To resolve the data bottleneck, a promising approach is distant supervision based NER (DS-NER). DS-NER automatically generates training data by matching entities in easily-obtained dictionaries with plain texts. Then this distantly-labeled data is used to train NER models, commonly be accompanied by a denoising step. DS-NER significantly reduces the annotation cost for building an effective NER model, and therefore has attracted great attention in recent years (Yang et al., 2018; Shang et al., 2018; Peng et al., 2019; Cao et al., 2019; Liang et al., 2020; Zhang et al., 2021). However, the learning of DS-NER is dictionarybiased, which severely harms the generalization and the robustness of the learned DS-NER models. Specifically, entity dictionaries are often incomplete (missing entities), noisy (containing wrong entities), and ambiguous (a name can be of different entity types, such as Washington). And DS will generate positively-labeled instances from the indictionary names but ignore all other names. Such a biased dataset will inevitably mislead the learned models to overfit in-dictionary names and underfit out-of-dictio"
2021.acl-long.371,W15-4322,0,0.0201053,"Missing"
2021.acl-long.371,P19-1511,1,0.746662,"Missing"
2021.acl-long.371,D19-1646,1,0.817596,"Missing"
2021.acl-long.371,2020.emnlp-main.592,1,0.754634,"Missing"
2021.acl-long.371,P16-1200,0,0.0182455,"ce of DS-NER. 1 (b) 0.6 All mentions 0.5 0.4 0.4 0.2 0.0 Twitter CoNLL Webpage Wikigold 0.3 D1 D2 D3 D4 Figure 1: Dictionary bias in DS-NER happens both at intra-dictionary and inter-dictionary aspects: (a) Averaged likehoods of mentions in/not in the dictionary significantly diverge. (b) Mention likehoods of the models using different dictionaries significantly diverge. Named entity recognition (NER) aims to identify text spans pertaining to specific semantic types, which is a fundamental task of information extraction, and enables various downstream applications such as Relation Extraction (Lin et al., 2016) and Question Answering (Bordes et al., 2015). The past several years have witnessed the remarkable success of supervised NER methods using neural networks (Lample et al., 2016; Ma and Hovy, 2016; Lin et al., 2020), which can automatically extract effective features from data and conduct NER in an end-to-end manner. Unfortunately, supervised methods rely on high-quality labeled data, which is very labor-intensive, and thus severely restricts Corresponding authors In dictionary Not in dictionary 0.6 Introduction ∗ (a) the application of current NER models. To resolve the data bottleneck, a prom"
2021.acl-long.371,2021.ccl-1.108,0,0.0363149,"Missing"
2021.acl-long.371,P16-1101,0,0.0242185,"spects: (a) Averaged likehoods of mentions in/not in the dictionary significantly diverge. (b) Mention likehoods of the models using different dictionaries significantly diverge. Named entity recognition (NER) aims to identify text spans pertaining to specific semantic types, which is a fundamental task of information extraction, and enables various downstream applications such as Relation Extraction (Lin et al., 2016) and Question Answering (Bordes et al., 2015). The past several years have witnessed the remarkable success of supervised NER methods using neural networks (Lample et al., 2016; Ma and Hovy, 2016; Lin et al., 2020), which can automatically extract effective features from data and conduct NER in an end-to-end manner. Unfortunately, supervised methods rely on high-quality labeled data, which is very labor-intensive, and thus severely restricts Corresponding authors In dictionary Not in dictionary 0.6 Introduction ∗ (a) the application of current NER models. To resolve the data bottleneck, a promising approach is distant supervision based NER (DS-NER). DS-NER automatically generates training data by matching entities in easily-obtained dictionaries with plain texts. Then this distantly-l"
2021.acl-long.371,P09-1113,0,0.0396887,"Missing"
2021.acl-long.371,P19-1231,0,0.226178,"In dictionary Not in dictionary 0.6 Introduction ∗ (a) the application of current NER models. To resolve the data bottleneck, a promising approach is distant supervision based NER (DS-NER). DS-NER automatically generates training data by matching entities in easily-obtained dictionaries with plain texts. Then this distantly-labeled data is used to train NER models, commonly be accompanied by a denoising step. DS-NER significantly reduces the annotation cost for building an effective NER model, and therefore has attracted great attention in recent years (Yang et al., 2018; Shang et al., 2018; Peng et al., 2019; Cao et al., 2019; Liang et al., 2020; Zhang et al., 2021). However, the learning of DS-NER is dictionarybiased, which severely harms the generalization and the robustness of the learned DS-NER models. Specifically, entity dictionaries are often incomplete (missing entities), noisy (containing wrong entities), and ambiguous (a name can be of different entity types, such as Washington). And DS will generate positively-labeled instances from the indictionary names but ignore all other names. Such a biased dataset will inevitably mislead the learned models to overfit in-dictionary names and unde"
2021.acl-long.371,C18-1183,0,0.0207284,"everely restricts Corresponding authors In dictionary Not in dictionary 0.6 Introduction ∗ (a) the application of current NER models. To resolve the data bottleneck, a promising approach is distant supervision based NER (DS-NER). DS-NER automatically generates training data by matching entities in easily-obtained dictionaries with plain texts. Then this distantly-labeled data is used to train NER models, commonly be accompanied by a denoising step. DS-NER significantly reduces the annotation cost for building an effective NER model, and therefore has attracted great attention in recent years (Yang et al., 2018; Shang et al., 2018; Peng et al., 2019; Cao et al., 2019; Liang et al., 2020; Zhang et al., 2021). However, the learning of DS-NER is dictionarybiased, which severely harms the generalization and the robustness of the learned DS-NER models. Specifically, entity dictionaries are often incomplete (missing entities), noisy (containing wrong entities), and ambiguous (a name can be of different entity types, such as Washington). And DS will generate positively-labeled instances from the indictionary names but ignore all other names. Such a biased dataset will inevitably mislead the learned models"
2021.acl-long.371,2020.emnlp-main.590,0,0.0178417,"acher model to guide the training of student model. Causal Inference. Causal Inference (Pearl, 2009; Pearl and Mackenzie, 2018) has been widely adopted in psychology, politics and epidemiology for years (MacKinnon et al., 2007; Richiardi et al., 2013; Keele, 2015). It can provide more reliable explanations by removing confounding bias in data, and also provide debiased solutions by learning 4810 causal effect rather than correlation effect. Recently, many causal inference techniques are used in computer vision (Tang et al., 2020; Qi et al., 2020) and natural language process (Wu et al., 2020; Zeng et al., 2020). 6 Conclusions This paper proposes to identify and resolve the dictionary bias in DS-NER via causal intervention. Specifically, we first formulate DS-NER using a structural causal model, then identity the causes of both intra-dictionary and inter-dictionary biases, finally de-bias DS-NER via backdoor adjustment and causal invariance regularizer. Experiments on four datasets and three representative DS-NER models verified the effectiveness and the robustness of our method. Acknowledgements This work is supported by the National Natural Science Foundation of China under Grants no.U1936207, Beij"
2021.acl-long.371,D14-1162,0,0.0837819,"Missing"
2021.acl-long.371,W09-1119,0,0.0986422,"ng strategy: i where λ is a hyper-parameter which controls the relative importance of these two losses and is tuned on the development set. 4 4.1 Experiments Experimental Settings Datasets. We conduct experiments on four standard datasets: (1) CoNLL2003 (Tjong Kim Sang and De Meulder, 2003) is a well known opendomain NER dataset. It consists of 20744 sentences collected from 1393 English news articles and is annotated with four types: PER, ORG, LOC and MISC. (2) Twitter (Godin et al., 2015) is from the WNUT 2016 NER shared task. It consists of 7236 sentences with 10 entity types. (3) Webpage (Ratinov and Roth, 2009) contains 20 webpages, including personal, academic and computer-science conference homepages. It consists of 619 sentences with the four types the same as CoNLL2003. (4) Wikigold (Balasuriya et al., 2009) contains 149 articles from the May 22, 2008 dump of English Wikipedia. It consists of 1969 sentences with the same types of CoNLL2003. Distant Annotation Settings. We use two distant annotation settings: String-Matching and KBMatching (Liang et al., 2020). String-Matching labels dataset by directly matching names in dictionary with sentences. KB-Matching is more complex, which uses a set of"
2021.acl-long.371,D18-1230,0,0.0342814,"orresponding authors In dictionary Not in dictionary 0.6 Introduction ∗ (a) the application of current NER models. To resolve the data bottleneck, a promising approach is distant supervision based NER (DS-NER). DS-NER automatically generates training data by matching entities in easily-obtained dictionaries with plain texts. Then this distantly-labeled data is used to train NER models, commonly be accompanied by a denoising step. DS-NER significantly reduces the annotation cost for building an effective NER model, and therefore has attracted great attention in recent years (Yang et al., 2018; Shang et al., 2018; Peng et al., 2019; Cao et al., 2019; Liang et al., 2020; Zhang et al., 2021). However, the learning of DS-NER is dictionarybiased, which severely harms the generalization and the robustness of the learned DS-NER models. Specifically, entity dictionaries are often incomplete (missing entities), noisy (containing wrong entities), and ambiguous (a name can be of different entity types, such as Washington). And DS will generate positively-labeled instances from the indictionary names but ignore all other names. Such a biased dataset will inevitably mislead the learned models to overfit in-dictio"
2021.acl-long.397,P19-1473,0,0.0128127,"ttings: instead of T5 we use GPT2 or randomly initialized transformers to construct paraphrasing models. Experimental results show that powerful PLMs can improve the performance. Powered by the language generation models to do semantic parsing, our method can benefit from the rapid development of PLMs. 7 Related Work Data Scarcity in Semantic Parsing. Witnessed the labeled data bottleneck problem, many techniques have been proposed to reduce the demand for labeled logical forms. Many weakly supervised learning are proposed (Artzi and Zettlemoyer, 2013; Berant et al., 2013; Reddy et al., 2014; Agrawal et al., 2019; Chen et al., 2020), such as denotation-base learning (Pasupat and Liang, 2016; Goldman et al., 2018), iterative searching (Dasigi et al., 2019). Semi-supervised semantic parsing is also proposed(Chen et al., 2018a). Such as variational auto-encoding (Yin et al., 2018), dual learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semantic parsing dataset from grammar rules and crowdsourcing paraphr"
2021.acl-long.397,Q13-1005,0,0.0426579,"analyze the effect of PLMs, we show the results with different PLM settings: instead of T5 we use GPT2 or randomly initialized transformers to construct paraphrasing models. Experimental results show that powerful PLMs can improve the performance. Powered by the language generation models to do semantic parsing, our method can benefit from the rapid development of PLMs. 7 Related Work Data Scarcity in Semantic Parsing. Witnessed the labeled data bottleneck problem, many techniques have been proposed to reduce the demand for labeled logical forms. Many weakly supervised learning are proposed (Artzi and Zettlemoyer, 2013; Berant et al., 2013; Reddy et al., 2014; Agrawal et al., 2019; Chen et al., 2020), such as denotation-base learning (Pasupat and Liang, 2016; Goldman et al., 2018), iterative searching (Dasigi et al., 2019). Semi-supervised semantic parsing is also proposed(Chen et al., 2018a). Such as variational auto-encoding (Yin et al., 2018), dual learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semant"
2021.acl-long.397,D13-1160,0,0.0621318,"we show the results with different PLM settings: instead of T5 we use GPT2 or randomly initialized transformers to construct paraphrasing models. Experimental results show that powerful PLMs can improve the performance. Powered by the language generation models to do semantic parsing, our method can benefit from the rapid development of PLMs. 7 Related Work Data Scarcity in Semantic Parsing. Witnessed the labeled data bottleneck problem, many techniques have been proposed to reduce the demand for labeled logical forms. Many weakly supervised learning are proposed (Artzi and Zettlemoyer, 2013; Berant et al., 2013; Reddy et al., 2014; Agrawal et al., 2019; Chen et al., 2020), such as denotation-base learning (Pasupat and Liang, 2016; Goldman et al., 2018), iterative searching (Dasigi et al., 2019). Semi-supervised semantic parsing is also proposed(Chen et al., 2018a). Such as variational auto-encoding (Yin et al., 2018), dual learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semantic parsing dataset fr"
2021.acl-long.397,C18-1076,1,0.849047,"020), such as denotation-base learning (Pasupat and Liang, 2016; Goldman et al., 2018), iterative searching (Dasigi et al., 2019). Semi-supervised semantic parsing is also proposed(Chen et al., 2018a). Such as variational auto-encoding (Yin et al., 2018), dual learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase. Guo et al. (2018) produce pseudolabeled data. Jia and Liang (2016) create new “recombinant” training examples with SCFG. The domain transfer techniques are also used to reduce the cost of data collecting for the unseen domain (Su and Yan, 2017; Herzig and Berant, 2018; Lu et al., 2019; Zhong et al., 2020). Goldwasser et al. (2011); Poon and Domingos (2009); Schmitt et al. (2020) leverage external resources or techniques for unsupervised learning. Constrained Decoding. After neural parsers model semantic parsing as a sentence to logical form translation task (Yih et al., 2015; Krishnamurthy et al., 2017; Iyyer"
2021.acl-long.397,P18-1071,1,0.886455,"Missing"
2021.acl-long.397,N19-1273,0,0.0135244,"PLMs can improve the performance. Powered by the language generation models to do semantic parsing, our method can benefit from the rapid development of PLMs. 7 Related Work Data Scarcity in Semantic Parsing. Witnessed the labeled data bottleneck problem, many techniques have been proposed to reduce the demand for labeled logical forms. Many weakly supervised learning are proposed (Artzi and Zettlemoyer, 2013; Berant et al., 2013; Reddy et al., 2014; Agrawal et al., 2019; Chen et al., 2020), such as denotation-base learning (Pasupat and Liang, 2016; Goldman et al., 2018), iterative searching (Dasigi et al., 2019). Semi-supervised semantic parsing is also proposed(Chen et al., 2018a). Such as variational auto-encoding (Yin et al., 2018), dual learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase. Guo et al. (2018) produce pseudolabeled data. Jia and Liang (2016) create new “recombinant” training examples with SCFG. The domain transfer"
2021.acl-long.397,P16-1004,0,0.0219227,"anonical utterance and logical form synchronously. The semantic gap and the structure gap are simultaneously resolved by jointly leveraging paraphrasing and grammar-constrained decoding. Thus, our synchronous decoding employs both the semantic and the structure constraints to solve unsupervised semantic parsing. Semantic parsing aims to translate natural language utterances to their formal meaning representations, such as lambda calculus (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007), FunQL (Kate et al., 2005; Lu et al., 2008), and SQL queries. Currently, most neural semantic parsers (Dong and Lapata, 2016; Chen et al., 2018b; Zhao et al., 2020; Shao et al., 2020) model semantic parsing as a sequence to sequence translation task via encoder-decoder framework. ∗ Structure Constraints Canonical Utterance: Semantic Consistency Introduction Corresponding Author Canonical utterances are pseudo-language representations of logical forms, which have the synchronous structure of logical forms.(Berant and Liang, 2014; Xiao et al., 2016; Su and Yan, 2017; Cao et al., 2020) 1 Grammar Semantic parsing is a challenging task due to the structure gap and the semantic gap between natural language utterances and"
2021.acl-long.397,P14-1133,0,0.466777,"ntations, such as lambda calculus (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007), FunQL (Kate et al., 2005; Lu et al., 2008), and SQL queries. Currently, most neural semantic parsers (Dong and Lapata, 2016; Chen et al., 2018b; Zhao et al., 2020; Shao et al., 2020) model semantic parsing as a sequence to sequence translation task via encoder-decoder framework. ∗ Structure Constraints Canonical Utterance: Semantic Consistency Introduction Corresponding Author Canonical utterances are pseudo-language representations of logical forms, which have the synchronous structure of logical forms.(Berant and Liang, 2014; Xiao et al., 2016; Su and Yan, 2017; Cao et al., 2020) 1 Grammar Semantic parsing is a challenging task due to the structure gap and the semantic gap between natural language utterances and logical forms. For structure gap, because utterances are usually word sequences and logical forms are usually trees/graphs constrained by specific grammars, a semantic parser needs to learn the complex structure transformation rules between them. For semantic gap, because the flexibility of natural languages, the same meaning can be expressed using very different utterances, a semantic parser needs be abl"
2021.acl-long.397,D17-1091,0,0.0228096,"), many constrained decoding algorithms are also proposed, such as type constraint-based illegal token filtering (Krishnamurthy et al., 2017); Lisp interpreter-based method (Liang et al., 2017); type constraints for generating valid actions (Iyyer et al., 2017). Paraphrasing in Semantic Parsing. Paraphrase models have been widely used in semantic parsing. ParaSempre (Berant and Liang, 2014) use paraphrase model to rerank candidate logical forms. Wang et al. (2015) employ SCFG grammar rules to produce MR and canonical utterance pairs, and construct OVERNIGHT dataset by paraphrasing utterances. Dong et al. (2017) use paraphrasing to expand the expressions of query sentences. Compared with these methods, we combine paraphrasing with grammar-constrained decoding, therefore SSD can further reduce the requirement of labeled data and achieve unsupervised semantic parsing. 8 Conclusions We propose an unsupervised semantic parsing method – Synchronous Semantic Decoding, which leverages paraphrasing and grammar-constrained decoding to simultaneously resolve the semantic gap and the structure gap. Specifically, we design 5117 two synchronous semantic decoding algorithms for paraphrasing under grammar constrain"
2021.acl-long.397,P19-1007,0,0.0788007,"ginal paper. Paraphrase Model We obtain the paraphrase model by training T5 and GPT2.0 on WikiAnswer Paraphrase3 , we train 10 epochs with learning rate as 1e-5. Follow Li et al. (2019), we sample 500K pairs of sentences in WikiAnswer corpus as training set and 6K as dev set. We generate adaptive fine-tuning datasets proportional to their labeled datasets, and back-translation(from English 5114 2 3 https://github.com/lingowu/ssd http://knowitall.cs.washington.edu/ paralex Supervised R ECOMBINATION (Jia and Liang, 2016) C ROSS D OMAIN (Su and Yan, 2017) S EQ 2ACTION (Chen et al., 2018b) D UAL (Cao et al., 2019) T WO - STAGE (Cao et al., 2020) SSD (Word-Level) SSD (Grammar-Level) Unsupervised (with nonparallel data) T WO - STAGE (Cao et al., 2020) W MD S AMPLES (Cao et al., 2020) SSD-S AMPLES (Word-Level) SSD-S AMPLES (Grammar-Level) Unsupervised Cross-domain Zero Shot G EN OVERNIGHT S YNTH -S EQ 2S EQ S YNTH PARA -S EQ 2S EQ SSD (Word-Level) SSD (Grammar-Level) Bas. Blo. Cal. Hou. Pub. Rec. Res. Soc. Avg. 85.2 86.2 88.2 87.5 87.2 86.2 86.2 58.1 60.2 61.4 63.7 65.7 64.9 64.9 78.0 79.8 81.5 79.8 80.4 81.7 81.7 71.4 71.4 74.1 73.0 75.7 72.7 72.7 76.4 78.9 80.7 81.4 80.1 82.3 82.3 79.6 84.7 82.9 81.5 86"
2021.acl-long.397,P18-1168,0,0.0133555,"ls. Experimental results show that powerful PLMs can improve the performance. Powered by the language generation models to do semantic parsing, our method can benefit from the rapid development of PLMs. 7 Related Work Data Scarcity in Semantic Parsing. Witnessed the labeled data bottleneck problem, many techniques have been proposed to reduce the demand for labeled logical forms. Many weakly supervised learning are proposed (Artzi and Zettlemoyer, 2013; Berant et al., 2013; Reddy et al., 2014; Agrawal et al., 2019; Chen et al., 2020), such as denotation-base learning (Pasupat and Liang, 2016; Goldman et al., 2018), iterative searching (Dasigi et al., 2019). Semi-supervised semantic parsing is also proposed(Chen et al., 2018a). Such as variational auto-encoding (Yin et al., 2018), dual learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase. Guo et al. (2018) produce pseudolabeled data. Jia and Liang (2016) create new “recombinant” train"
2021.acl-long.397,2020.acl-main.608,0,0.466141,"005; Wong and Mooney, 2007), FunQL (Kate et al., 2005; Lu et al., 2008), and SQL queries. Currently, most neural semantic parsers (Dong and Lapata, 2016; Chen et al., 2018b; Zhao et al., 2020; Shao et al., 2020) model semantic parsing as a sequence to sequence translation task via encoder-decoder framework. ∗ Structure Constraints Canonical Utterance: Semantic Consistency Introduction Corresponding Author Canonical utterances are pseudo-language representations of logical forms, which have the synchronous structure of logical forms.(Berant and Liang, 2014; Xiao et al., 2016; Su and Yan, 2017; Cao et al., 2020) 1 Grammar Semantic parsing is a challenging task due to the structure gap and the semantic gap between natural language utterances and logical forms. For structure gap, because utterances are usually word sequences and logical forms are usually trees/graphs constrained by specific grammars, a semantic parser needs to learn the complex structure transformation rules between them. For semantic gap, because the flexibility of natural languages, the same meaning can be expressed using very different utterances, a semantic parser needs be able to map various expressions to their semantic form. To"
2021.acl-long.397,D18-1188,0,0.0136425,"n et al., 2020), such as denotation-base learning (Pasupat and Liang, 2016; Goldman et al., 2018), iterative searching (Dasigi et al., 2019). Semi-supervised semantic parsing is also proposed(Chen et al., 2018a). Such as variational auto-encoding (Yin et al., 2018), dual learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase. Guo et al. (2018) produce pseudolabeled data. Jia and Liang (2016) create new “recombinant” training examples with SCFG. The domain transfer techniques are also used to reduce the cost of data collecting for the unseen domain (Su and Yan, 2017; Herzig and Berant, 2018; Lu et al., 2019; Zhong et al., 2020). Goldwasser et al. (2011); Poon and Domingos (2009); Schmitt et al. (2020) leverage external resources or techniques for unsupervised learning. Constrained Decoding. After neural parsers model semantic parsing as a sentence to logical form translation task (Yih et al., 2015; Krishnamurthy et al., 2017; Iyyer"
2021.acl-long.397,2020.emnlp-main.118,0,0.0752591,"Missing"
2021.acl-long.397,D18-1190,0,0.083747,"al., 2020): we label each input sentences with the most possible outputs in the nonparallel corpus and use these samples as peusdo training data – we denote this setting as SSD-S AMPLES. Supervised settings Our SSD method can be further enhanced using annotated training instances. Specifically, given the annotated hutterance, logical formi instances, we first transform logical form to its canonical form, then use them to further fine-tune our paraphrase models after unsupervised pre-training. Baselines We compare our method with the following unsupervised baselines: 1) Cross-domain Zero Shot(Herzig and Berant, 2018), which trains on other source domains and then generalizes to target domains in OVERNIGHT and 2) G EN OVERNIGHT(Wang et al., 2015) in which models are trained on synthesized hCU, MRi pairs; 3) We also implement S EQ 2S EQ baseline on the synthesized data as S YNTH -S EQ 2S EQ. 4) S YNTH PARA S EQ 2S EQ is trained on the synthesized data and hCU paraphrase, MRi pairs, the paraphrases are obtained in the same way in Section 4. 6.2 6.2.1 Experimental Results Overall Results The overall results of different baselines and our method are shown in Table 1 and Table 3 (We also demonstrate several cas"
2021.acl-long.397,D19-1394,0,0.078982,"tations and on different domains. Our implementations are public available2 . OVERNIGHT This is a multi-domain dataset, which contains natural language paraphrases paired with lambda DCS logical forms across eight domains. We use the same train/test splits as Wang et al. (2015). G EO(FunQL) This is a semantic parsing benchmark about U.S. geography (Zelle and Mooney, 1996) using the variable-free semantic representation FunQL (Kate et al., 2005). We extend the FunQL grammar to SCFG for this dataset. We follow the standard 600/280 train/test splits. G EO G RANNO This is another version of G EO (Herzig and Berant, 2019), in which lambda DCS logical forms paired with canonical utterances are produced from SCFG. Instead of paraphrasing sentences, crowd workers are required to select the correct canonical utterance from candidate list. We follow the split (train/valid/test 487/59/278) in original paper. Paraphrase Model We obtain the paraphrase model by training T5 and GPT2.0 on WikiAnswer Paraphrase3 , we train 10 epochs with learning rate as 1e-5. Follow Li et al. (2019), we sample 500K pairs of sentences in WikiAnswer corpus as training set and 6K as dev set. We generate adaptive fine-tuning datasets proport"
2021.acl-long.397,P17-1167,0,0.0542132,"Missing"
2021.acl-long.397,P16-1002,0,0.286256,"arg max pparaphrase (cy |x). Instead of directly y∈Y parsing utterance into its logical form, SSD generates its canonical utterance and obtains its logical form based on the one-to-one mapping relation. In following we first introduce the grammar constraints in decoding, and then present two inference algorithms for generating paraphrases under the grammar constraints. 3.1 Grammar Constraints in Decoding Synchronous context-free grammar(SCFG) is employed as our synchronous grammar, which is widely used to convert a meaning representation into an unique canonical utterance (Wang et al., 2015; Jia and Liang, 2016). An SCFG consists of a set of production rules: N → hα, βi, where N is Semantic Constraints Like the type checking in Wang et al. (2015), the constraints of knowledge base schema can be integrated to further refine the grammar. The semantic constraints ensure the generated utterances/logical forms will be semantically valid. 3.2 Decoding 3.2.1 Rule-Level Inference One strategy to generate paraphrase under the grammar constraint is taking the grammar rule as the decoding unit. Grammar-based decoders have been proposed to output sequences of grammar rules instead of words(Yin and Neubig, 2017)."
2021.acl-long.397,D18-1265,0,0.104929,"59.3 66.9 37.1 57.6 58.5 63.2 54.4 52.1 52.4 47.6 51.9 50.3 56.5 45.3 57.4 52.3 62.0 56.3 35.7 29.9 54.5 49.7 56.5 50.7 61.1 53.9 56.6 55.4 53.6 51.6 50.0 48.4 54.0 51.3 47.6 48.4 47.8 44.9 56.9 55.6 53.2 63.3 62.0 62.5 32.4 31.7 31.4 53.8 52.3 50.7 56.5 54.3 51.8 61.1 59.6 55.4 57.2 83.2 56.3 86.9 50.2 76.9 55.7 82.7 58.0 86.9 62.9 88.1 37.7 62.8 54.6 79.8 57.2 70.9 62.5 81.4 54.6 48.6 53.7 45.8 55.7 47.6 56.1 50.3 58.9 55.6 66.4 54.5 32.7 30.3 55.6 48.9 58.3 51.4 62.1 54.3 Table 2: Albation results of our model with different settings on the three datasets. G EO G RANNO Supervised D EP HT (Jie and Lu, 2018) C OPY N ET (Herzig and Berant, 2019) 72.0 One-stage (Cao et al., 2020) 71.9 Two-stage (Cao et al., 2020) 71.6 S EQ 2S EQ (Guo et al., 2020) SSD (Word-Level) 72.9 SSD (Grammar-Level) 72.0 Unsupervised (with nonparallel data) Two-stage (Cao et al., 2020) 63.7 W MD S AMPLES (Cao et al., 2020) 35.3 SSD-S AMPLES (Word-Level) 64.0 SSD-S AMPLES (Grammar-Level) 64.4 Unsupervised S YNTH -S EQ 2S EQ 32.7 S YNTH PARA -S EQ 2S EQ 41.4 SSD (Word-Level) 57.2 SSD (Grammar-Level) 58.5 G EO (FunQL) tains a significant improvement compared with baselines, which also verifies that our method is not limited to s"
2021.acl-long.397,D17-1160,0,0.0820097,"e reduced. For structure gap, previous studies found that constrained decoding can effectively constrain the output structure by injecting grammars of logical forms and facts in 5110 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5110–5121 August 1–6, 2021. ©2021 Association for Computational Linguistics knowledge bases during inference. For example, the grammar-based neural semantic parsers (Xiao et al., 2016; Yin and Neubig, 2017) and the constrained decoding algorithm (Krishnamurthy et al., 2017). For semantic gap, previous studies have found that paraphrasing is an effective technique for resolving the diversity of natural expressions. Using paraphrasing, semantic parsers can handle the different expressions of the same meaning, therefore can reduce the requirement of labeled data. For example, supervised methods (Berant and Liang, 2014; Su and Yan, 2017) use the paraphrasing scores between canonical utterances and sentences to re-rank logical forms; Two-stage (Cao et al., 2020) rewrites utterances to canonical utterances which can be easily parsed. The main drawback of these studies"
2021.acl-long.397,P19-1332,0,0.02312,"QL grammar to SCFG for this dataset. We follow the standard 600/280 train/test splits. G EO G RANNO This is another version of G EO (Herzig and Berant, 2019), in which lambda DCS logical forms paired with canonical utterances are produced from SCFG. Instead of paraphrasing sentences, crowd workers are required to select the correct canonical utterance from candidate list. We follow the split (train/valid/test 487/59/278) in original paper. Paraphrase Model We obtain the paraphrase model by training T5 and GPT2.0 on WikiAnswer Paraphrase3 , we train 10 epochs with learning rate as 1e-5. Follow Li et al. (2019), we sample 500K pairs of sentences in WikiAnswer corpus as training set and 6K as dev set. We generate adaptive fine-tuning datasets proportional to their labeled datasets, and back-translation(from English 5114 2 3 https://github.com/lingowu/ssd http://knowitall.cs.washington.edu/ paralex Supervised R ECOMBINATION (Jia and Liang, 2016) C ROSS D OMAIN (Su and Yan, 2017) S EQ 2ACTION (Chen et al., 2018b) D UAL (Cao et al., 2019) T WO - STAGE (Cao et al., 2020) SSD (Word-Level) SSD (Grammar-Level) Unsupervised (with nonparallel data) T WO - STAGE (Cao et al., 2020) W MD S AMPLES (Cao et al., 20"
2021.acl-long.397,P17-1003,0,0.0152622,"Herzig and Berant, 2018; Lu et al., 2019; Zhong et al., 2020). Goldwasser et al. (2011); Poon and Domingos (2009); Schmitt et al. (2020) leverage external resources or techniques for unsupervised learning. Constrained Decoding. After neural parsers model semantic parsing as a sentence to logical form translation task (Yih et al., 2015; Krishnamurthy et al., 2017; Iyyer et al., 2017; Jie and Lu, 2018; Lindemann et al., 2020), many constrained decoding algorithms are also proposed, such as type constraint-based illegal token filtering (Krishnamurthy et al., 2017); Lisp interpreter-based method (Liang et al., 2017); type constraints for generating valid actions (Iyyer et al., 2017). Paraphrasing in Semantic Parsing. Paraphrase models have been widely used in semantic parsing. ParaSempre (Berant and Liang, 2014) use paraphrase model to rerank candidate logical forms. Wang et al. (2015) employ SCFG grammar rules to produce MR and canonical utterance pairs, and construct OVERNIGHT dataset by paraphrasing utterances. Dong et al. (2017) use paraphrasing to expand the expressions of query sentences. Compared with these methods, we combine paraphrasing with grammar-constrained decoding, therefore SSD can furth"
2021.acl-long.397,2020.emnlp-main.323,0,0.0269183,"and Liang (2016) create new “recombinant” training examples with SCFG. The domain transfer techniques are also used to reduce the cost of data collecting for the unseen domain (Su and Yan, 2017; Herzig and Berant, 2018; Lu et al., 2019; Zhong et al., 2020). Goldwasser et al. (2011); Poon and Domingos (2009); Schmitt et al. (2020) leverage external resources or techniques for unsupervised learning. Constrained Decoding. After neural parsers model semantic parsing as a sentence to logical form translation task (Yih et al., 2015; Krishnamurthy et al., 2017; Iyyer et al., 2017; Jie and Lu, 2018; Lindemann et al., 2020), many constrained decoding algorithms are also proposed, such as type constraint-based illegal token filtering (Krishnamurthy et al., 2017); Lisp interpreter-based method (Liang et al., 2017); type constraints for generating valid actions (Iyyer et al., 2017). Paraphrasing in Semantic Parsing. Paraphrase models have been widely used in semantic parsing. ParaSempre (Berant and Liang, 2014) use paraphrase model to rerank candidate logical forms. Wang et al. (2015) employ SCFG grammar rules to produce MR and canonical utterance pairs, and construct OVERNIGHT dataset by paraphrasing utterances. D"
2021.acl-long.397,D08-1082,0,0.0521072,"om previous staged methods (indicated by gray lines), our method generates canonical utterance and logical form synchronously. The semantic gap and the structure gap are simultaneously resolved by jointly leveraging paraphrasing and grammar-constrained decoding. Thus, our synchronous decoding employs both the semantic and the structure constraints to solve unsupervised semantic parsing. Semantic parsing aims to translate natural language utterances to their formal meaning representations, such as lambda calculus (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007), FunQL (Kate et al., 2005; Lu et al., 2008), and SQL queries. Currently, most neural semantic parsers (Dong and Lapata, 2016; Chen et al., 2018b; Zhao et al., 2020; Shao et al., 2020) model semantic parsing as a sequence to sequence translation task via encoder-decoder framework. ∗ Structure Constraints Canonical Utterance: Semantic Consistency Introduction Corresponding Author Canonical utterances are pseudo-language representations of logical forms, which have the synchronous structure of logical forms.(Berant and Liang, 2014; Xiao et al., 2016; Su and Yan, 2017; Cao et al., 2020) 1 Grammar Semantic parsing is a challenging task due"
2021.acl-long.397,D19-1104,0,0.0180912,"l learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase. Guo et al. (2018) produce pseudolabeled data. Jia and Liang (2016) create new “recombinant” training examples with SCFG. The domain transfer techniques are also used to reduce the cost of data collecting for the unseen domain (Su and Yan, 2017; Herzig and Berant, 2018; Lu et al., 2019; Zhong et al., 2020). Goldwasser et al. (2011); Poon and Domingos (2009); Schmitt et al. (2020) leverage external resources or techniques for unsupervised learning. Constrained Decoding. After neural parsers model semantic parsing as a sentence to logical form translation task (Yih et al., 2015; Krishnamurthy et al., 2017; Iyyer et al., 2017; Jie and Lu, 2018; Lindemann et al., 2020), many constrained decoding algorithms are also proposed, such as type constraint-based illegal token filtering (Krishnamurthy et al., 2017); Lisp interpreter-based method (Liang et al., 2017); type constraints fo"
2021.acl-long.397,P16-1003,0,0.0171586,"nstruct paraphrasing models. Experimental results show that powerful PLMs can improve the performance. Powered by the language generation models to do semantic parsing, our method can benefit from the rapid development of PLMs. 7 Related Work Data Scarcity in Semantic Parsing. Witnessed the labeled data bottleneck problem, many techniques have been proposed to reduce the demand for labeled logical forms. Many weakly supervised learning are proposed (Artzi and Zettlemoyer, 2013; Berant et al., 2013; Reddy et al., 2014; Agrawal et al., 2019; Chen et al., 2020), such as denotation-base learning (Pasupat and Liang, 2016; Goldman et al., 2018), iterative searching (Dasigi et al., 2019). Semi-supervised semantic parsing is also proposed(Chen et al., 2018a). Such as variational auto-encoding (Yin et al., 2018), dual learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase. Guo et al. (2018) produce pseudolabeled data. Jia and Liang (2016) create"
2021.acl-long.397,D09-1001,0,0.0568526,"al information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase. Guo et al. (2018) produce pseudolabeled data. Jia and Liang (2016) create new “recombinant” training examples with SCFG. The domain transfer techniques are also used to reduce the cost of data collecting for the unseen domain (Su and Yan, 2017; Herzig and Berant, 2018; Lu et al., 2019; Zhong et al., 2020). Goldwasser et al. (2011); Poon and Domingos (2009); Schmitt et al. (2020) leverage external resources or techniques for unsupervised learning. Constrained Decoding. After neural parsers model semantic parsing as a sentence to logical form translation task (Yih et al., 2015; Krishnamurthy et al., 2017; Iyyer et al., 2017; Jie and Lu, 2018; Lindemann et al., 2020), many constrained decoding algorithms are also proposed, such as type constraint-based illegal token filtering (Krishnamurthy et al., 2017); Lisp interpreter-based method (Liang et al., 2017); type constraints for generating valid actions (Iyyer et al., 2017). Paraphrasing in Semantic"
2021.acl-long.397,Q14-1030,0,0.0194175,"ith different PLM settings: instead of T5 we use GPT2 or randomly initialized transformers to construct paraphrasing models. Experimental results show that powerful PLMs can improve the performance. Powered by the language generation models to do semantic parsing, our method can benefit from the rapid development of PLMs. 7 Related Work Data Scarcity in Semantic Parsing. Witnessed the labeled data bottleneck problem, many techniques have been proposed to reduce the demand for labeled logical forms. Many weakly supervised learning are proposed (Artzi and Zettlemoyer, 2013; Berant et al., 2013; Reddy et al., 2014; Agrawal et al., 2019; Chen et al., 2020), such as denotation-base learning (Pasupat and Liang, 2016; Goldman et al., 2018), iterative searching (Dasigi et al., 2019). Semi-supervised semantic parsing is also proposed(Chen et al., 2018a). Such as variational auto-encoding (Yin et al., 2018), dual learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semantic parsing dataset from grammar rules and"
2021.acl-long.397,2020.emnlp-main.577,0,0.0547182,"Missing"
2021.acl-long.397,2020.coling-main.289,0,0.027872,"c gap and the structure gap are simultaneously resolved by jointly leveraging paraphrasing and grammar-constrained decoding. Thus, our synchronous decoding employs both the semantic and the structure constraints to solve unsupervised semantic parsing. Semantic parsing aims to translate natural language utterances to their formal meaning representations, such as lambda calculus (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007), FunQL (Kate et al., 2005; Lu et al., 2008), and SQL queries. Currently, most neural semantic parsers (Dong and Lapata, 2016; Chen et al., 2018b; Zhao et al., 2020; Shao et al., 2020) model semantic parsing as a sequence to sequence translation task via encoder-decoder framework. ∗ Structure Constraints Canonical Utterance: Semantic Consistency Introduction Corresponding Author Canonical utterances are pseudo-language representations of logical forms, which have the synchronous structure of logical forms.(Berant and Liang, 2014; Xiao et al., 2016; Su and Yan, 2017; Cao et al., 2020) 1 Grammar Semantic parsing is a challenging task due to the structure gap and the semantic gap between natural language utterances and logical forms. For structure gap, because utterances are u"
2021.acl-long.397,D17-1127,0,0.328483,"yer and Collins, 2005; Wong and Mooney, 2007), FunQL (Kate et al., 2005; Lu et al., 2008), and SQL queries. Currently, most neural semantic parsers (Dong and Lapata, 2016; Chen et al., 2018b; Zhao et al., 2020; Shao et al., 2020) model semantic parsing as a sequence to sequence translation task via encoder-decoder framework. ∗ Structure Constraints Canonical Utterance: Semantic Consistency Introduction Corresponding Author Canonical utterances are pseudo-language representations of logical forms, which have the synchronous structure of logical forms.(Berant and Liang, 2014; Xiao et al., 2016; Su and Yan, 2017; Cao et al., 2020) 1 Grammar Semantic parsing is a challenging task due to the structure gap and the semantic gap between natural language utterances and logical forms. For structure gap, because utterances are usually word sequences and logical forms are usually trees/graphs constrained by specific grammars, a semantic parser needs to learn the complex structure transformation rules between them. For semantic gap, because the flexibility of natural languages, the same meaning can be expressed using very different utterances, a semantic parser needs be able to map various expressions to their"
2021.acl-long.397,P15-1129,0,0.40721,"gical form based on the one-to-one mapping relation. In following we first introduce the grammar constraints in decoding, and then present two inference algorithms for generating paraphrases under the grammar constraints. 3.1 Grammar Constraints in Decoding Synchronous context-free grammar(SCFG) is employed as our synchronous grammar, which is widely used to convert a meaning representation into an unique canonical utterance (Wang et al., 2015; Jia and Liang, 2016). An SCFG consists of a set of production rules: N → hα, βi, where N is Semantic Constraints Like the type checking in Wang et al. (2015), the constraints of knowledge base schema can be integrated to further refine the grammar. The semantic constraints ensure the generated utterances/logical forms will be semantically valid. 3.2 Decoding 3.2.1 Rule-Level Inference One strategy to generate paraphrase under the grammar constraint is taking the grammar rule as the decoding unit. Grammar-based decoders have been proposed to output sequences of grammar rules instead of words(Yin and Neubig, 2017). Like them, our rule-level inference method takes the grammar rule as the decoding unit. Figure 3 (a) shows an example of our rule level"
2021.acl-long.397,P07-1121,0,0.0854641,"y rivers run through state0 Figure 1: Different from previous staged methods (indicated by gray lines), our method generates canonical utterance and logical form synchronously. The semantic gap and the structure gap are simultaneously resolved by jointly leveraging paraphrasing and grammar-constrained decoding. Thus, our synchronous decoding employs both the semantic and the structure constraints to solve unsupervised semantic parsing. Semantic parsing aims to translate natural language utterances to their formal meaning representations, such as lambda calculus (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007), FunQL (Kate et al., 2005; Lu et al., 2008), and SQL queries. Currently, most neural semantic parsers (Dong and Lapata, 2016; Chen et al., 2018b; Zhao et al., 2020; Shao et al., 2020) model semantic parsing as a sequence to sequence translation task via encoder-decoder framework. ∗ Structure Constraints Canonical Utterance: Semantic Consistency Introduction Corresponding Author Canonical utterances are pseudo-language representations of logical forms, which have the synchronous structure of logical forms.(Berant and Liang, 2014; Xiao et al., 2016; Su and Yan, 2017; Cao et al., 2020) 1 Grammar"
2021.acl-long.397,P16-1127,0,0.123827,"calculus (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007), FunQL (Kate et al., 2005; Lu et al., 2008), and SQL queries. Currently, most neural semantic parsers (Dong and Lapata, 2016; Chen et al., 2018b; Zhao et al., 2020; Shao et al., 2020) model semantic parsing as a sequence to sequence translation task via encoder-decoder framework. ∗ Structure Constraints Canonical Utterance: Semantic Consistency Introduction Corresponding Author Canonical utterances are pseudo-language representations of logical forms, which have the synchronous structure of logical forms.(Berant and Liang, 2014; Xiao et al., 2016; Su and Yan, 2017; Cao et al., 2020) 1 Grammar Semantic parsing is a challenging task due to the structure gap and the semantic gap between natural language utterances and logical forms. For structure gap, because utterances are usually word sequences and logical forms are usually trees/graphs constrained by specific grammars, a semantic parser needs to learn the complex structure transformation rules between them. For semantic gap, because the flexibility of natural languages, the same meaning can be expressed using very different utterances, a semantic parser needs be able to map various ex"
2021.acl-long.397,2020.emnlp-main.196,0,0.0370397,"et al., 2019), T5(Raffel et al., 2020) for SSD. However, as described in above, there exists a style bias between natural language sentences and canonical utterances, which hurts the performance of unsupervised semantic par5113 ing. In this section, we describe how to alleviate this bias via adaptive fine-tuning. Given a text generation model, after pretraining it using paraphrase corpus, we fine-tune it using synthesized h sentence, canonical utterance i pairs. Previous studies have shown that the pretraining on synthesized data can significantly improve the performance of semantic parsing (Xu et al., 2020a; Marzoev et al., 2020; Yu et al., 2020; Xu et al., 2020b). Specifically, we design three data synthesis algorithms: 1) CUs We sample CUs from SCFGs, and preserve executable ones. As we do not have the paired sentences, we only fine-tune the language model of the PLMs on CUs. 2) Self Paras We use the trained paraphrase model to get the natural language paraphrases of the sampled canonical utterances to form h sentence, canonical utterance i pairs. 3) External Paras We also use external paraphrase methods such as back translation to get the pairs. 5 Utterance Reranking Adaptive fine-tuning res"
2021.acl-long.397,2020.emnlp-main.31,0,0.0253037,"et al., 2019), T5(Raffel et al., 2020) for SSD. However, as described in above, there exists a style bias between natural language sentences and canonical utterances, which hurts the performance of unsupervised semantic par5113 ing. In this section, we describe how to alleviate this bias via adaptive fine-tuning. Given a text generation model, after pretraining it using paraphrase corpus, we fine-tune it using synthesized h sentence, canonical utterance i pairs. Previous studies have shown that the pretraining on synthesized data can significantly improve the performance of semantic parsing (Xu et al., 2020a; Marzoev et al., 2020; Yu et al., 2020; Xu et al., 2020b). Specifically, we design three data synthesis algorithms: 1) CUs We sample CUs from SCFGs, and preserve executable ones. As we do not have the paired sentences, we only fine-tune the language model of the PLMs on CUs. 2) Self Paras We use the trained paraphrase model to get the natural language paraphrases of the sampled canonical utterances to form h sentence, canonical utterance i pairs. 3) External Paras We also use external paraphrase methods such as back translation to get the pairs. 5 Utterance Reranking Adaptive fine-tuning res"
2021.acl-long.397,P19-1201,0,0.0195169,"blem, many techniques have been proposed to reduce the demand for labeled logical forms. Many weakly supervised learning are proposed (Artzi and Zettlemoyer, 2013; Berant et al., 2013; Reddy et al., 2014; Agrawal et al., 2019; Chen et al., 2020), such as denotation-base learning (Pasupat and Liang, 2016; Goldman et al., 2018), iterative searching (Dasigi et al., 2019). Semi-supervised semantic parsing is also proposed(Chen et al., 2018a). Such as variational auto-encoding (Yin et al., 2018), dual learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase. Guo et al. (2018) produce pseudolabeled data. Jia and Liang (2016) create new “recombinant” training examples with SCFG. The domain transfer techniques are also used to reduce the cost of data collecting for the unseen domain (Su and Yan, 2017; Herzig and Berant, 2018; Lu et al., 2019; Zhong et al., 2020). Goldwasser et al. (2011); Poon and Domingos (2009); Schmitt et al. (2020) leve"
2021.acl-long.397,P15-1128,0,0.0293145,"ules and crowdsourcing paraphrase. Guo et al. (2018) produce pseudolabeled data. Jia and Liang (2016) create new “recombinant” training examples with SCFG. The domain transfer techniques are also used to reduce the cost of data collecting for the unseen domain (Su and Yan, 2017; Herzig and Berant, 2018; Lu et al., 2019; Zhong et al., 2020). Goldwasser et al. (2011); Poon and Domingos (2009); Schmitt et al. (2020) leverage external resources or techniques for unsupervised learning. Constrained Decoding. After neural parsers model semantic parsing as a sentence to logical form translation task (Yih et al., 2015; Krishnamurthy et al., 2017; Iyyer et al., 2017; Jie and Lu, 2018; Lindemann et al., 2020), many constrained decoding algorithms are also proposed, such as type constraint-based illegal token filtering (Krishnamurthy et al., 2017); Lisp interpreter-based method (Liang et al., 2017); type constraints for generating valid actions (Iyyer et al., 2017). Paraphrasing in Semantic Parsing. Paraphrase models have been widely used in semantic parsing. ParaSempre (Berant and Liang, 2014) use paraphrase model to rerank candidate logical forms. Wang et al. (2015) employ SCFG grammar rules to produce MR a"
2021.acl-long.397,P17-1041,0,0.0813498,"aging external resources, therefore the reliance on data can be reduced. For structure gap, previous studies found that constrained decoding can effectively constrain the output structure by injecting grammars of logical forms and facts in 5110 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5110–5121 August 1–6, 2021. ©2021 Association for Computational Linguistics knowledge bases during inference. For example, the grammar-based neural semantic parsers (Xiao et al., 2016; Yin and Neubig, 2017) and the constrained decoding algorithm (Krishnamurthy et al., 2017). For semantic gap, previous studies have found that paraphrasing is an effective technique for resolving the diversity of natural expressions. Using paraphrasing, semantic parsers can handle the different expressions of the same meaning, therefore can reduce the requirement of labeled data. For example, supervised methods (Berant and Liang, 2014; Su and Yan, 2017) use the paraphrasing scores between canonical utterances and sentences to re-rank logical forms; Two-stage (Cao et al., 2020) rewrites utterances to canonical utter"
2021.acl-long.397,P19-1447,0,0.0130138,"n to get the pairs. 5 Utterance Reranking Adaptive fine-tuning resolves the style bias problem by fitting a better paraphrase model. In this section, we propose an utterance reranking algorithm to further alleviate the style bias by reranking and selecting the best canonical form. Given the utterance x and top-N parsing results (yn , cn ), n = 1, 2, ..., N , we rerank all candidates by focusing on semantic similarities between x and cn , so that canonical utterances can be effectively selected. Reranking for semantic parsing has been exploited in many previous studies (Berant and Liang, 2014; Yin and Neubig, 2019). These works employ reranking for canonical utterances selection. Differently, our re-ranker does not need labeled data. Formally, we measure two similarities between x and cn and the final reranking score is calculated by: score(x, c) = log p(c|x) + srec (x, c) + sasso (x, c) (2) Reconstruction Score The reconstruction score measures the coherence and adequacy of the canonical utterances, using the probability of reproducing the original input sentence x from c with the trained paraphrasing model: srec (x, c) = log ppr (x|c) Association Score The association score measures whether x and c co"
2021.acl-long.397,P18-1070,0,0.0176345,"the rapid development of PLMs. 7 Related Work Data Scarcity in Semantic Parsing. Witnessed the labeled data bottleneck problem, many techniques have been proposed to reduce the demand for labeled logical forms. Many weakly supervised learning are proposed (Artzi and Zettlemoyer, 2013; Berant et al., 2013; Reddy et al., 2014; Agrawal et al., 2019; Chen et al., 2020), such as denotation-base learning (Pasupat and Liang, 2016; Goldman et al., 2018), iterative searching (Dasigi et al., 2019). Semi-supervised semantic parsing is also proposed(Chen et al., 2018a). Such as variational auto-encoding (Yin et al., 2018), dual learning framework for semantic parsing (Cao et al., 2019), dual information maximization method (Ye et al., 2019), and backtranslation (Sun et al., 2019). One other strategy is to generate data for semantic parsing, e.g., Wang et al. (2015) construct a semantic parsing dataset from grammar rules and crowdsourcing paraphrase. Guo et al. (2018) produce pseudolabeled data. Jia and Liang (2016) create new “recombinant” training examples with SCFG. The domain transfer techniques are also used to reduce the cost of data collecting for the unseen domain (Su and Yan, 2017; Herzig and Berant, 2"
2021.acl-long.397,2020.acl-main.606,0,0.0184503,"nously. The semantic gap and the structure gap are simultaneously resolved by jointly leveraging paraphrasing and grammar-constrained decoding. Thus, our synchronous decoding employs both the semantic and the structure constraints to solve unsupervised semantic parsing. Semantic parsing aims to translate natural language utterances to their formal meaning representations, such as lambda calculus (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007), FunQL (Kate et al., 2005; Lu et al., 2008), and SQL queries. Currently, most neural semantic parsers (Dong and Lapata, 2016; Chen et al., 2018b; Zhao et al., 2020; Shao et al., 2020) model semantic parsing as a sequence to sequence translation task via encoder-decoder framework. ∗ Structure Constraints Canonical Utterance: Semantic Consistency Introduction Corresponding Author Canonical utterances are pseudo-language representations of logical forms, which have the synchronous structure of logical forms.(Berant and Liang, 2014; Xiao et al., 2016; Su and Yan, 2017; Cao et al., 2020) 1 Grammar Semantic parsing is a challenging task due to the structure gap and the semantic gap between natural language utterances and logical forms. For structure gap, beca"
2021.acl-long.60,P19-1478,0,0.038835,"cant improvement (about 2 to 3 F1 gain). When the size is large (more than 20,000 instances in our experiments), more instances will not result in performance improvements, and the low-quality instances will hurt the performance. 738 PER eats hamburger Reason Reason Hamburger is tasty PER is hungry … Model Pure Knowledge-based Methods Knowledge Hunting (Emami et al., 2018) String Match (Zhang et al., 2020b) Language Model-based Methods LM (Single) (Trinh and Le, 2018) LM (Ensemble) (Trinh and Le, 2018) BERT (w/o finetuning) (Devlin et al., 2019) External Knowledge Enhanced Methods BERT (WscR) Certu et al. (2019) BERT (ASER) Zhang et al. (2020b) BERT (ASER & WscR) Zhang et al. (2020b) BERT (ASER++) BERT (ASER++ & WscR) WSC-schema Style Training Data PER eats hamburger. PER drinks coffee. PER eats hamburger. PER drinks coffee. PER is hungry. PER is sleepy. Hamburger is tasty. Coffee is delicious. Event-Centric Knowledge Graph 97. The fish ate the worm. It was hungry. 98. The fish ate the worm. It was tasty. 97. The fish 98. the worm WSC Questions Predictions Figure 5: An overview of WSC implementation. The blue color means the correct reference while the red color means the wrong one. 4.2 Extrinsic Exp"
2021.acl-long.60,2020.acl-main.478,0,0.0324533,"mplicit relations lack these surface cues. To resolve the implicit discourse relation recognition (IDRR) task, researchers construct high-quality labelled datasets (Prasad et al., 2008) and design elaborate models (Zhang et al., 2016b; Bai and Zhao, 2018; Kishimoto et al., 2020). Associations between Discourse and Narrative. Recent NLP studies have proved that discourse and narratives closely interact with each other, and leveraging discourse knowledge benefits narrative analysis significantly, such as subevents detection (Aldawsari and Finlayson, 2019) and main event relevant identification (Choubey et al., 2020). Motivated by the above observation, this paper leverages the knowledge of discourse by a knowledge projection paradigm. Blessed with the associations at token-, semantic- and coarse category-levels, the discourse corpora and knowledge can be effectively exploited for event relation extraction. 3 Multi-tier Knowledge Projection Network for Event Relation Extraction In this section, we describe how to learn an effective event relation extractor by projecting resourcerich discourse knowledge to the resource-poor narrative task. Specifically, we propose Multi-tier Knowledge Projection Network (M"
2021.acl-long.60,N19-1423,0,0.167836,"ection Network (MKPNet) which can effectively leverage multi-tier discourse knowledge for implicit event relation extraction. Figure 2 shows an overview of MKPNet, which uses token adaptor, semantic adaptor and coarse category adaptor to fully exploit discourse knowledge at different levels. In the following, we first describe the neural architecture of MKPNet and then describe the details of three adaptors. 3.1 Neural Architecture of MKPNet For knowledge projection, we model both event relation extraction (ERE) and discourse relation recognition (DRR) as an instance-pair classification task (Devlin et al., 2019; Kishimoto et al., 2020). For ERE, the input is an event pair such as &lt;E 1 : “PER goes to the restaurant”, E 3 : “PER is so hungry”&gt; and the output is an event relation such as Reason. For DRR, the input is a clause pair such as &lt;D1 : “Tom goes to the restaurant”, D3 :“he is so hungry”&gt; and the output is a discourse relation such as Cause. Specifically, MKPNet extends the SotADRR model — BERT-CLS (Kishimoto et al., 2020) by the VAE-based semantic encoder and the coarse category encoder to model knowledge tier-by-tier (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b). I"
2021.acl-long.60,P15-1166,0,0.0268086,"Zhang et al., 2016a), i.e., µ. During training, we minimize the Kullback-Leibler divergence KL(P ||Q) between the generation model P and the inference model Q. Intuitively, KL divergence connects these two models: KL(q(hz |h[CLS] , hY )||p(hz |h[CLS] )) Full Model Training In this paper, we utilize multi-task learning (Caruana, 1997) to implement the knowledge projection from discourse to narrative. It expects correlative tasks (ERE and DRR) can help each other to learn better by sharing the parameters of three adaptors. Given ERE and DRR training datasets, an alternate optimization approach (Dong et al., 2015) is used to optimizate MKPNet: L(θ) =α(L(θ; Y ) + λKL(P ||Q)) (7) + (1 − α)L(θ; Y c ) To project the semantic-level knowledge, we use the same VAE for both event pair and discourse pair. Therefore, the commonalities of event semantics and discourse semantics can be captured more accurately. 3.4 3.5 (9) where Y can be Y im or Y d according to the different tasks, λ, α are two hyperparameters, KL(P ||Q)) is the KL divergence in the semantic encoder, L(θ; Y ) and L(θ; Y c ) are fine-grained and coarse-grained objectives respectively: Coarse Category Adaptor The token adaptor and the semantic adap"
2021.acl-long.60,D18-1220,0,0.130383,"sizes/qualities, and Figure 4 shows the corresponding results of MKPNet on the development set. We can see that the size is the main factors for performance improvements at the beginning: every 5,000 additional instances can result in a significant improvement (about 2 to 3 F1 gain). When the size is large (more than 20,000 instances in our experiments), more instances will not result in performance improvements, and the low-quality instances will hurt the performance. 738 PER eats hamburger Reason Reason Hamburger is tasty PER is hungry … Model Pure Knowledge-based Methods Knowledge Hunting (Emami et al., 2018) String Match (Zhang et al., 2020b) Language Model-based Methods LM (Single) (Trinh and Le, 2018) LM (Ensemble) (Trinh and Le, 2018) BERT (w/o finetuning) (Devlin et al., 2019) External Knowledge Enhanced Methods BERT (WscR) Certu et al. (2019) BERT (ASER) Zhang et al. (2020b) BERT (ASER & WscR) Zhang et al. (2020b) BERT (ASER++) BERT (ASER++ & WscR) WSC-schema Style Training Data PER eats hamburger. PER drinks coffee. PER eats hamburger. PER drinks coffee. PER is hungry. PER is sleepy. Hamburger is tasty. Coffee is delicious. Event-Centric Knowledge Graph 97. The fish ate the worm. It was hun"
2021.acl-long.60,P19-1471,0,0.0282353,"gory adaptor for label-level knowledge projection. connectives, while implicit relations lack these surface cues. To resolve the implicit discourse relation recognition (IDRR) task, researchers construct high-quality labelled datasets (Prasad et al., 2008) and design elaborate models (Zhang et al., 2016b; Bai and Zhao, 2018; Kishimoto et al., 2020). Associations between Discourse and Narrative. Recent NLP studies have proved that discourse and narratives closely interact with each other, and leveraging discourse knowledge benefits narrative analysis significantly, such as subevents detection (Aldawsari and Finlayson, 2019) and main event relevant identification (Choubey et al., 2020). Motivated by the above observation, this paper leverages the knowledge of discourse by a knowledge projection paradigm. Blessed with the associations at token-, semantic- and coarse category-levels, the discourse corpora and knowledge can be effectively exploited for event relation extraction. 3 Multi-tier Knowledge Projection Network for Event Relation Extraction In this section, we describe how to learn an effective event relation extractor by projecting resourcerich discourse knowledge to the resource-poor narrative task. Speci"
2021.acl-long.60,C18-1048,0,0.158664,"ncoder Event Relation Classifier Coarse Category Encoder Figure 2: An overview of MKPNet, which projects discourse knowledge for event relation extraction: (a) token adaptor for token-level knowledge projection, (b) semantic adaptor for semantic-level knowledge projection, and (c) coarse category adaptor for label-level knowledge projection. connectives, while implicit relations lack these surface cues. To resolve the implicit discourse relation recognition (IDRR) task, researchers construct high-quality labelled datasets (Prasad et al., 2008) and design elaborate models (Zhang et al., 2016b; Bai and Zhao, 2018; Kishimoto et al., 2020). Associations between Discourse and Narrative. Recent NLP studies have proved that discourse and narratives closely interact with each other, and leveraging discourse knowledge benefits narrative analysis significantly, such as subevents detection (Aldawsari and Finlayson, 2019) and main event relevant identification (Choubey et al., 2020). Motivated by the above observation, this paper leverages the knowledge of discourse by a knowledge projection paradigm. Blessed with the associations at token-, semantic- and coarse category-levels, the discourse corpora and knowle"
2021.acl-long.60,P98-1013,0,0.729444,"ves of the explicit event 736 relation instances in ASER core version3 and retaining at most 2200 instances with the highest confidence scores for each category4 . In this way, we obtain 23,181/1400/1400 train/dev/test instances – we denoted it as implicit event relation extraction (IERE) dataset. Implementation. We implement our model based on pytorch-transformers (Wolf et al., 2020). We use BERT-base and set all hyper-parameters using the default settings of the SotADRR model (Kishimoto et al., 2020). Baselines. For ERE, we compare the proposed MKPNet with the following baselines: FrameNet (Baker et al., 1998) ConceptNet (Speer et al., 2017) Event2Mind (Smith et al., 2018) ATOMIC (Sap et al., 2019) Knowlywood (Tandon et al., 2015) ASER (Zhang et al., 2020b) ASER++ (core) ASER++ (high) ASER++ (full) Table 1: Number comparison of event relations in ASER++ and existing event-related resources. Acc Baselines w/o Discourse Knowledge BERT-CLS 53.00 MKPNet w/o KP 53.94 Baselines with Discourse Knowledge BERT-Transfer 54.29 Multi-tier Knowledge Projection MKPNet w/o SA & CA 54.7940.85 MKPNet w/o CA 55.1441.20 MKPNet w/o SA 55.2941.35 MKPNet 55.8641.92 • Baselines w/o Discourse Knowledge are only trained on"
2021.acl-long.60,W16-1701,0,0.028802,"overage of EventKGs is significantly undermined. Besides, because the scale of the existed event relation corpus (Hong et al., 2016) is limited, it is also impractical to build effective event relation classifiers via supervised learning. In this paper, we propose a new paradigm for event relation extraction — knowledge projection. Instead of relying on sparse connectives or building classifiers starting from scratch, we project discourse knowledge to event narratives by exploiting the anthropological linguistic connections between them. Enlightened by Livholts and Tamboukou (2015); Altshuler (2016); Reyes and Wortham (2017), discourses and narratives have significant associations, and their knowledge are shared at different levels: 1) token-level knowledge: discourses and narratives share similar lexical and syntactic structures, 2) semantic-level knowledge: the semantics entailed in discourse pairs and event pairs are analogical, e.g., E 3 -Reason→E 1 and D3 Cause→D1 in Figure 1., and 3) label-level knowledge: heterogeneous event and discourse relations have the same coarse categories, e.g., both the event relation Reason and the discourse relation Cause are included in the coarse-grai"
2021.acl-long.60,N18-1144,0,0.0123695,"om entity-centric ones (Banko et al., 2007; Suchanek et al., 2007; Bollacker et al., 2008; Wu et al., 2012) to event-centric ones. However, the construction of traditional KGs takes domain experts much effort and time, which are often with limited size and cannot effectively resolve realworld applications, e.g., FrameNet (Baker et al., 1998). Recently, many modern and large-scale KGs have been built semi-automatically, which focus on events (Tandon et al., 2015; Rospocher et al., 2016; Gottschalk and Demidova, 2018; Zhang et al., 2020b) and commonsense (Speer et al., 2017; Smith et al., 2018; Huang et al., 2018; Sap et al., 2019). Specifically, Yu et al. (2020) proposes an approach to extract entailment relations between eventualities, e.g., “I eat an apple” entails “I eat fruit”, and release an event entailment graph (EEG). Different from EEG, this paper focuses on implicit event relations which are not extracted due to the absences of the connectives and discontinuity. Knowledge Transfer. Due to the data scarcity problem, many knowledge transfer studies have been proposed, including multi-task learning (Caru739 ana, 1997), transfer learning (Pan and Yang, 2009; Pan et al., 2010), and knowledge dis"
2021.acl-long.60,Q15-1024,0,0.0273238,"Net, and extrinsic experiments on WSC (Levesque et al., 2012) to verify the value of the extracted event relations. (8) 4.1 where Y c ∈ {Temporal, Contingency, Comparison, Expansion}. After that, we use the coarse label embedding network to obtain the corresponding coarse-grained label embedding hY c , which is referred as the coarse category representation. To project that label-level knowledge, we use the same coarse-grained classifier and the same coarse c Intrinsic Experiments Datasets. For discourse relation recognition (DRR), we use PDTB 2.0 (Prasad et al., 2008) with the same splits of Ji and Eisenstein (2015): sections 2-20/0-1/21-22 respectively for train/dev/test. For event relation extraction (ERE), because there is no labelled training corpus, we construct a new dataset by removing the connectives of the explicit event 736 relation instances in ASER core version3 and retaining at most 2200 instances with the highest confidence scores for each category4 . In this way, we obtain 23,181/1400/1400 train/dev/test instances – we denoted it as implicit event relation extraction (IERE) dataset. Implementation. We implement our model based on pytorch-transformers (Wolf et al., 2020). We use BERT-base a"
2021.acl-long.60,2020.lrec-1.145,0,0.353696,"n Classifier Coarse Category Encoder Figure 2: An overview of MKPNet, which projects discourse knowledge for event relation extraction: (a) token adaptor for token-level knowledge projection, (b) semantic adaptor for semantic-level knowledge projection, and (c) coarse category adaptor for label-level knowledge projection. connectives, while implicit relations lack these surface cues. To resolve the implicit discourse relation recognition (IDRR) task, researchers construct high-quality labelled datasets (Prasad et al., 2008) and design elaborate models (Zhang et al., 2016b; Bai and Zhao, 2018; Kishimoto et al., 2020). Associations between Discourse and Narrative. Recent NLP studies have proved that discourse and narratives closely interact with each other, and leveraging discourse knowledge benefits narrative analysis significantly, such as subevents detection (Aldawsari and Finlayson, 2019) and main event relevant identification (Choubey et al., 2020). Motivated by the above observation, this paper leverages the knowledge of discourse by a knowledge projection paradigm. Blessed with the associations at token-, semantic- and coarse category-levels, the discourse corpora and knowledge can be effectively ex"
2021.acl-long.60,2020.emnlp-main.733,0,0.214697,"(Devlin et al., 2019; Kishimoto et al., 2020). For ERE, the input is an event pair such as &lt;E 1 : “PER goes to the restaurant”, E 3 : “PER is so hungry”&gt; and the output is an event relation such as Reason. For DRR, the input is a clause pair such as &lt;D1 : “Tom goes to the restaurant”, D3 :“he is so hungry”&gt; and the output is a discourse relation such as Cause. Specifically, MKPNet extends the SotADRR model — BERT-CLS (Kishimoto et al., 2020) by the VAE-based semantic encoder and the coarse category encoder to model knowledge tier-by-tier (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b). It 1) first utilizes the BERT-based token encoder to encodes an instance pair as a token representation h[CLS] ; 2) then obtains the semantic representation hz via a VAE-based semantic encoder; 3) predicts the coarse-grained label and embeddings it as the coarse category representation hY c ; 4) finally classifies its relation with the guidance of the aggregate instance-pair representation: Y = Classif ierF ine ([h[CLS] ⊕ hz ⊕ hY c ]) (1) where ⊕ means the concatenation operation. In this way, the parameters of MKPNet can be grouped by {θBERT , θSemantic , θCoarse , θF ine }, where θBERT f"
2021.acl-long.60,2020.emnlp-main.242,0,0.153086,"(Devlin et al., 2019; Kishimoto et al., 2020). For ERE, the input is an event pair such as &lt;E 1 : “PER goes to the restaurant”, E 3 : “PER is so hungry”&gt; and the output is an event relation such as Reason. For DRR, the input is a clause pair such as &lt;D1 : “Tom goes to the restaurant”, D3 :“he is so hungry”&gt; and the output is a discourse relation such as Cause. Specifically, MKPNet extends the SotADRR model — BERT-CLS (Kishimoto et al., 2020) by the VAE-based semantic encoder and the coarse category encoder to model knowledge tier-by-tier (Pan et al., 2016; Guo et al., 2019; Kang et al., 2020; Li et al., 2020b). It 1) first utilizes the BERT-based token encoder to encodes an instance pair as a token representation h[CLS] ; 2) then obtains the semantic representation hz via a VAE-based semantic encoder; 3) predicts the coarse-grained label and embeddings it as the coarse category representation hY c ; 4) finally classifies its relation with the guidance of the aggregate instance-pair representation: Y = Classif ierF ine ([h[CLS] ⊕ hz ⊕ hY c ]) (1) where ⊕ means the concatenation operation. In this way, the parameters of MKPNet can be grouped by {θBERT , θSemantic , θCoarse , θF ine }, where θBERT f"
2021.acl-long.60,prasad-etal-2008-penn,0,0.570515,"-based Token Encoder Discourse Relation Classifier VAE-based Semantic Encoder Event Relation Classifier Coarse Category Encoder Figure 2: An overview of MKPNet, which projects discourse knowledge for event relation extraction: (a) token adaptor for token-level knowledge projection, (b) semantic adaptor for semantic-level knowledge projection, and (c) coarse category adaptor for label-level knowledge projection. connectives, while implicit relations lack these surface cues. To resolve the implicit discourse relation recognition (IDRR) task, researchers construct high-quality labelled datasets (Prasad et al., 2008) and design elaborate models (Zhang et al., 2016b; Bai and Zhao, 2018; Kishimoto et al., 2020). Associations between Discourse and Narrative. Recent NLP studies have proved that discourse and narratives closely interact with each other, and leveraging discourse knowledge benefits narrative analysis significantly, such as subevents detection (Aldawsari and Finlayson, 2019) and main event relevant identification (Choubey et al., 2020). Motivated by the above observation, this paper leverages the knowledge of discourse by a knowledge projection paradigm. Blessed with the associations at token-, s"
2021.acl-long.60,P18-1043,0,0.0997096,"rsion3 and retaining at most 2200 instances with the highest confidence scores for each category4 . In this way, we obtain 23,181/1400/1400 train/dev/test instances – we denoted it as implicit event relation extraction (IERE) dataset. Implementation. We implement our model based on pytorch-transformers (Wolf et al., 2020). We use BERT-base and set all hyper-parameters using the default settings of the SotADRR model (Kishimoto et al., 2020). Baselines. For ERE, we compare the proposed MKPNet with the following baselines: FrameNet (Baker et al., 1998) ConceptNet (Speer et al., 2017) Event2Mind (Smith et al., 2018) ATOMIC (Sap et al., 2019) Knowlywood (Tandon et al., 2015) ASER (Zhang et al., 2020b) ASER++ (core) ASER++ (high) ASER++ (full) Table 1: Number comparison of event relations in ASER++ and existing event-related resources. Acc Baselines w/o Discourse Knowledge BERT-CLS 53.00 MKPNet w/o KP 53.94 Baselines with Discourse Knowledge BERT-Transfer 54.29 Multi-tier Knowledge Projection MKPNet w/o SA & CA 54.7940.85 MKPNet w/o CA 55.1441.20 MKPNet w/o SA 55.2941.35 MKPNet 55.8641.92 • Baselines w/o Discourse Knowledge are only trained on IERE training set. We choose the BERT-CLS as the representative"
2021.acl-long.60,D12-1071,0,0.0201188,"ng data in the same way as Zhang et al. (2020b) and fine-tune BERT on it, which we refer to as BERT (ASER++). We compare BERT (ASER++) with these baselines: • Pure Knowledge-based Methods are heuristical rule-based methods, such as Knowledge Hunting (Emami et al., 2018) and String Match (Zhang et al., 2020b). • Language Model-based Methods use language model trained on large-scale corpus and tuned specifically for the WSC task, such as LM (Trinh and Le, 2018). • External Knowledge Enhanced Methods are models based on BERT and trained with the different external knowledge resource, e.g., WscR (Ng, 2012; Certu et al., 2019) We implement our model based on pytorchtransformers (Wolf et al., 2020). BERT-large is used. All hyper-parameters are default settings as Certu et al. (2019). WSC 57.3 56.6 54.5 61.5 61.9 71.4 64.5 72.5 66.2 74.1 Table 5: The overall results of extrinsic experiments. The evaluation metric is accuracy. Extrinsic Results. Table 5 shows the overall results of extrinsic experiments. We can see that: By fine-tuning BERT on our enriched EventKG — ASER++, the WSC performance can be significantly improved. BERT (ASER++) and BERT (ASER++ & WscR) outperform BERT (ASER) and BERT (AS"
2021.acl-long.60,D14-1162,0,0.087151,"e of the aggregate instance-pair representation: Y = Classif ierF ine ([h[CLS] ⊕ hz ⊕ hY c ]) (1) where ⊕ means the concatenation operation. In this way, the parameters of MKPNet can be grouped by {θBERT , θSemantic , θCoarse , θF ine }, where θBERT for BERT-based token encoder, θSemantic for VAE-based semantic encoder, θCoarse for coarse category encoder and θF ine for the final relation classifier layer respectively. 734 3.2 Token Adaptor Q Recent studies have shown that similar tasks usually share similar lexical and syntactic structures and therefore lead to similar token representations (Pennington et al., 2014; Peters et al., 2018). The token adaptor tries to improve the token encoding for ERE by sharing the parameters θBERT of the BERT-based encoders with DRR. In this way, the encoder is more effective due to the more supervision signals and is more general due to the multi-task settings. Specifically, given an event pair &lt;E 1 , E 2 &gt;, we represent it as a sequence: [CLS], e11 , ..., e1|E 1 |, [SEP ], e21 , ..., e2|E 2 |, [SEP ] where[CLS] and [SEP] are special tokens. For each token in the input, its representation is constructed by concatenating the corresponding token, segment and position embe"
2021.acl-long.60,N18-1202,0,0.0146889,"ce-pair representation: Y = Classif ierF ine ([h[CLS] ⊕ hz ⊕ hY c ]) (1) where ⊕ means the concatenation operation. In this way, the parameters of MKPNet can be grouped by {θBERT , θSemantic , θCoarse , θF ine }, where θBERT for BERT-based token encoder, θSemantic for VAE-based semantic encoder, θCoarse for coarse category encoder and θF ine for the final relation classifier layer respectively. 734 3.2 Token Adaptor Q Recent studies have shown that similar tasks usually share similar lexical and syntactic structures and therefore lead to similar token representations (Pennington et al., 2014; Peters et al., 2018). The token adaptor tries to improve the token encoding for ERE by sharing the parameters θBERT of the BERT-based encoders with DRR. In this way, the encoder is more effective due to the more supervision signals and is more general due to the multi-task settings. Specifically, given an event pair &lt;E 1 , E 2 &gt;, we represent it as a sequence: [CLS], e11 , ..., e1|E 1 |, [SEP ], e21 , ..., e2|E 2 |, [SEP ] where[CLS] and [SEP] are special tokens. For each token in the input, its representation is constructed by concatenating the corresponding token, segment and position embeddings. Then, the even"
2021.acl-long.60,S18-1108,0,0.0181975,"1: The knowledge projection paradigm for event relation extraction. The explicit projection directly projects connectives to event relations, e.g., from “because” to Reason. The implicit projection leverages the discourse knowledge to discover implicit event relations without connectives via MKPNet. Introduction Event-centric knowledge graphs (EventKGs) model the narratives of the world by representing events and identifying relations between them, which are critical for machine understanding and can benefit many downstream tasks, such as question answering (Costa et al., 2020), news reading (Vossen, 2018), commonsense knowledge acquisition (Zhang et al., 2020a) and so on. Recently, semi-automatically constructing EventKGs have gained much attention (Tandon et al., 2015; Rospocher et al., 2016; Gottschalk and Demidova, 2018; Zhang et al., 2020b). These methods extract event knowledge from massive raw corpora with or without little human intervention, which makes them scalable solutions to build large-scale ∗ ? Corresponding authors. EventKGs. Commonly, each node in EventKGs represents an event, and each edge represents a predefined relation between an event pair1 . Currently, event relations ar"
2021.acl-long.60,D16-1050,0,0.161158,"VAE-based Semantic Encoder Event Relation Classifier Coarse Category Encoder Figure 2: An overview of MKPNet, which projects discourse knowledge for event relation extraction: (a) token adaptor for token-level knowledge projection, (b) semantic adaptor for semantic-level knowledge projection, and (c) coarse category adaptor for label-level knowledge projection. connectives, while implicit relations lack these surface cues. To resolve the implicit discourse relation recognition (IDRR) task, researchers construct high-quality labelled datasets (Prasad et al., 2008) and design elaborate models (Zhang et al., 2016b; Bai and Zhao, 2018; Kishimoto et al., 2020). Associations between Discourse and Narrative. Recent NLP studies have proved that discourse and narratives closely interact with each other, and leveraging discourse knowledge benefits narrative analysis significantly, such as subevents detection (Aldawsari and Finlayson, 2019) and main event relevant identification (Choubey et al., 2020). Motivated by the above observation, this paper leverages the knowledge of discourse by a knowledge projection paradigm. Blessed with the associations at token-, semantic- and coarse category-levels, the discour"
2021.acl-long.60,D16-1037,0,0.123537,"VAE-based Semantic Encoder Event Relation Classifier Coarse Category Encoder Figure 2: An overview of MKPNet, which projects discourse knowledge for event relation extraction: (a) token adaptor for token-level knowledge projection, (b) semantic adaptor for semantic-level knowledge projection, and (c) coarse category adaptor for label-level knowledge projection. connectives, while implicit relations lack these surface cues. To resolve the implicit discourse relation recognition (IDRR) task, researchers construct high-quality labelled datasets (Prasad et al., 2008) and design elaborate models (Zhang et al., 2016b; Bai and Zhao, 2018; Kishimoto et al., 2020). Associations between Discourse and Narrative. Recent NLP studies have proved that discourse and narratives closely interact with each other, and leveraging discourse knowledge benefits narrative analysis significantly, such as subevents detection (Aldawsari and Finlayson, 2019) and main event relevant identification (Choubey et al., 2020). Motivated by the above observation, this paper leverages the knowledge of discourse by a knowledge projection paradigm. Blessed with the associations at token-, semantic- and coarse category-levels, the discour"
2021.emnlp-main.378,E17-1075,0,0.0511693,"Missing"
2021.emnlp-main.378,2020.acl-main.749,0,0.0219971,"Missing"
2021.emnlp-main.378,P18-1009,0,0.14216,"ve correct labels: where Ws is the weight parameter, Wa is the attribute embedding (i.e., word embedding of attribute words). We use cosine distance to measure similarity and employ ReLU to activate attributes. Then we induce new labels by reasoning over the activated attributes as: (j) scoreVl Learning LA = − |L| X (j) scoreVl ∗ yj (15) j=1 ( 1 yj = −1 , vj ∈ Y , vj ∈ /Y (16) Final Loss. The final loss is a combination of set loss and BAG loss: L = LS + λLA https://pypi.org/project/stanza/ 4615 (17) where λ is the relative weight of these two losses3 . 5 5.1 Model P without label dependency *Choi et al. (2018) 47.1 *ELMo(Onoe and Durrett, 2019) 51.5 BERT(Onoe and Durrett, 2019) 51.6 BERT[in-house] 55.9 with label dependency *LABELGCN (Xiong et al., 2019) 50.3 61.2 LRN w/o IR LRN 54.5 Experiments Settings Datasets We conduct experiments on two standard fine-grained entity typing datasets4 : UltraFine as primary dataset and OntoNotes as complementary dataset. Ultra-Fine contains 6K manuallyannotated examples, 2519 categories, and 5.4 labels per sample on average. Followed Choi et al. (2018) we use the same 2K/2K/2K train/dev/test splits and evaluate using macro precision, recall and F-score. Original"
2021.emnlp-main.378,D15-1103,0,0.0261385,"ning data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can leverage intrinsic dependencies to predict long tail labels; 3) Through the sequenceto-set framework, LRN can consider two kinds of label dependencies simultaneously to jointly reason frequent and long tail labels. mechanisms: deduct"
2021.emnlp-main.378,D19-1643,0,0.0167883,"fore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can leverage intrinsic dependencies to predict long tail labels; 3) Through the sequenceto-set framework, LRN can consider two kinds of label dependencies simultaneously to jointly reason frequent and long tail labels. mechanisms: deductive reasoning for e"
2021.emnlp-main.378,E17-2119,0,0.0371948,"Missing"
2021.emnlp-main.378,P19-1511,1,0.89873,"Missing"
2021.emnlp-main.378,D19-1646,1,0.884217,"Missing"
2021.emnlp-main.378,2020.emnlp-main.592,1,0.893659,"Missing"
2021.emnlp-main.378,D19-1641,0,0.192756,"nt worker musician scientist police terrorism expert scientist ac-vate Ac-vate Func-on ✔ ✗ Only such a potent force, [they] theorize, could collapse some of the war ship … Figure 2: Overview of the process for LRN which contains an encoder, a deductive reasoning-based decoder and an inductive reasoning-based decoder. The figure shows: at step 1, the label person is predicted by deductive reasoning, and the attribute human is activated; at step 3, the label scientist is generated by inductive reasoning. 2019), or requiring latent label representation to reconstruct the co-occurrence structure (Lin and Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Dur"
2021.emnlp-main.378,2020.findings-emnlp.42,0,0.225694,"an auto-regressive network to predict labels et al. (2016a); Xu and Barbosa (2018); Wu et al. based on both the context and previous labels. For (2019); Chen et al. (2020) design new loss function example, given previously-generated label person to exploit label hierarchies. Abhishek et al. (2017) of the mention they, as well as the context they enhance the label representation by sharing paramtheorize, LRN will deduce its new label theorist eters. Shimaoka et al. (2017); Murty et al. (2018); based on the extrinsic dependency between person and theorist derived from data. For intrinsic depen- López and Strube (2020) embed labels into a highdencies, LRN introduces inductive reasoning (i.e., dimension or a new space. And the studies exploit gather generalized information to a conclusion), co-occurrence structures including limiting the label range during label set prediction (Rabinovich and utilizes a bipartite attribute graph to reason labels based on current activated attributes of previ- and Klein, 2017), enriching the label representaous labels. For example, if the attributes {expert, tion by introducing associated labels (Xiong et al., 1 scholar} have been activated, LRN will induce Our source codes a"
2021.emnlp-main.378,C16-1017,0,0.0299401,"easoning, and the attribute human is activated; at step 3, the label scientist is generated by inductive reasoning. 2019), or requiring latent label representation to reconstruct the co-occurrence structure (Lin and Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner"
2021.emnlp-main.378,P18-1010,0,0.018346,"d formulates it us- hierarchy and co-occurrence structures estimated from data to enhance the models. To this end, Ren ing an auto-regressive network to predict labels et al. (2016a); Xu and Barbosa (2018); Wu et al. based on both the context and previous labels. For (2019); Chen et al. (2020) design new loss function example, given previously-generated label person to exploit label hierarchies. Abhishek et al. (2017) of the mention they, as well as the context they enhance the label representation by sharing paramtheorize, LRN will deduce its new label theorist eters. Shimaoka et al. (2017); Murty et al. (2018); based on the extrinsic dependency between person and theorist derived from data. For intrinsic depen- López and Strube (2020) embed labels into a highdencies, LRN introduces inductive reasoning (i.e., dimension or a new space. And the studies exploit gather generalized information to a conclusion), co-occurrence structures including limiting the label range during label set prediction (Rabinovich and utilizes a bipartite attribute graph to reason labels based on current activated attributes of previ- and Klein, 2017), enriching the label representaous labels. For example, if the attributes {"
2021.emnlp-main.378,N19-1087,0,0.0132746,"is generated by inductive reasoning. 2019), or requiring latent label representation to reconstruct the co-occurrence structure (Lin and Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can lever"
2021.emnlp-main.378,N19-1250,0,0.152787,"nd Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can leverage intrinsic dependencies to predict long tail labels; 3) Through the sequenceto-set framework, LRN can consider two kinds of label dep"
2021.emnlp-main.378,D14-1162,0,0.0839453,"Missing"
2021.emnlp-main.378,P17-2052,0,0.0534636,"Missing"
2021.emnlp-main.378,D16-1144,0,0.0219023,"ce structure (Lin and Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can leverage intrinsic dependencies to predict long tail labels; 3) Through the sequenceto-set framework, LRN can consid"
2021.emnlp-main.378,E17-1119,0,0.0222479,"ises) between labels, and formulates it us- hierarchy and co-occurrence structures estimated from data to enhance the models. To this end, Ren ing an auto-regressive network to predict labels et al. (2016a); Xu and Barbosa (2018); Wu et al. based on both the context and previous labels. For (2019); Chen et al. (2020) design new loss function example, given previously-generated label person to exploit label hierarchies. Abhishek et al. (2017) of the mention they, as well as the context they enhance the label representation by sharing paramtheorize, LRN will deduce its new label theorist eters. Shimaoka et al. (2017); Murty et al. (2018); based on the extrinsic dependency between person and theorist derived from data. For intrinsic depen- López and Strube (2020) embed labels into a highdencies, LRN introduces inductive reasoning (i.e., dimension or a new space. And the studies exploit gather generalized information to a conclusion), co-occurrence structures including limiting the label range during label set prediction (Rabinovich and utilizes a bipartite attribute graph to reason labels based on current activated attributes of previ- and Klein, 2017), enriching the label representaous labels. For example"
2021.emnlp-main.378,D18-1121,0,0.0123657,"tation is very efficient (Zupan et al., 1999). In local BAG, we collect attributes in two ways: (1) We mask the entity mention in the sentence, and predict the [MASK] token using masked language model (this paper uses BERT-base-uncased), and the non-stop words whose prediction scores greater than a confidence threshold θc will be used as attributes — we denote them as context attributes; Since PLM usually predicts high-frequency words, the attributes are usually not long-tailed, which facilitates modeling dependencies between head and tail labels. This mask-prediction strategy is also used in Xin et al. (2018), for collecting additional semantic evidence of entity labels. (2) We directly segment the entity mention into words using Stanza2 , and all non-stop words are used as attributes — we denote them as entity attributes. Figure 3 shows several attribute examples. Given attributes, we compute the attribute-label relatedness (i.e. E in G) using the cosine similarity between their GloVe embeddings (Pennington et al., 2014). Reasoning over BAG. At each time step, we activate attributes in BAG by calculating their similarities to the current hidden state of decoder st . For the ith attribute node Va"
2021.emnlp-main.378,N19-1084,0,0.286146,"independently recognize each entity label without considering their dependencies. For this, existing approaches use the predefined label hierarchies (Ren et al., 1 Introduction 2016a; Shimaoka et al., 2017; Abhishek et al., Fine-grained entity typing (FET) aims to classify 2017; Karn et al., 2017; Xu and Barbosa, 2018; Wu entity mentions to a fine-grained semantic label et al., 2019; Chen et al., 2020; Ren, 2020) or label set, e.g., classify “FBI agents"" in “They were ar- co-occurrence statistics from training data (Rabirested by FBI agents."" as {organization, adminis- novich and Klein, 2017; Xiong et al., 2019; Lin tration, force, agent, police}. By providing fine- and Ji, 2019) as external constraints. Unfortunately, grained semantic labels, FET is critical for entity these label structures or statistics are difficult to recognition (Lin et al., 2019a,b, 2020; Zhang et al., obtain when transferring to new scenarios. Sec2021b,a) and can benefit many NLP tasks, such ond, because of the fine-grained and large-scale as relation extraction (Yaghoobzadeh et al., 2017; label set, many long tail labels are only provided Zhang et al., 2019), entity linking (Onoe and Dur- with several or even no training in"
2021.emnlp-main.378,N18-1002,0,0.0145477,"sequentially generate 2 Related Work fine-grained labels in an end-to-end, sequence-toset manner. Figure 1(c) shows several examples. One main challenge for FET is how to exploit complex label dependencies in the large-scale label To capture extrinsic dependencies, LRN introduces set. Previous studies typically use predefined label deductive reasoning (i.e., draw a conclusion based on premises) between labels, and formulates it us- hierarchy and co-occurrence structures estimated from data to enhance the models. To this end, Ren ing an auto-regressive network to predict labels et al. (2016a); Xu and Barbosa (2018); Wu et al. based on both the context and previous labels. For (2019); Chen et al. (2020) design new loss function example, given previously-generated label person to exploit label hierarchies. Abhishek et al. (2017) of the mention they, as well as the context they enhance the label representation by sharing paramtheorize, LRN will deduce its new label theorist eters. Shimaoka et al. (2017); Murty et al. (2018); based on the extrinsic dependency between person and theorist derived from data. For intrinsic depen- López and Strube (2020) embed labels into a highdencies, LRN introduces inductive"
2021.emnlp-main.378,E17-1111,0,0.0372287,"Missing"
2021.emnlp-main.378,C18-1330,0,0.0227817,"nodes Vl , use mt = [ct + g; ut + st ] as input, and calculate and edges E only exist between attributes nodes the probability distribution over label set L: and labels nodes, with the edge weight indicatst = LSTM(st−1 , Wb yt−1 ) (7) ing the attribute-label relatedness. Attributes are represented using natural language words in BAG. ot = Wo mt (8) Figure 2 shows a BAG where Va contains words yt = sof tmax(ot + It ) (9) {scholar, expert, historian, ...}, Vl are all where Wo and Wb are weight parameters and we entity labels in label set L, containing {student, muuse the mask vector It ∈ RL+1 (Yang et al., 2018) sician, scientist, ...} 4614 … the RTC would be forced until [cash] could be raised … object, money, currency, income, resource, financing cash fund, capital, interest, revenue … owner of the technology, receives [royalty payments]. Label object, money, award, payment, gift royalty, payment Entity Attribute fund, award, assistance, support Context Attribute Figure 3: Examples of attributes. BAG Construction. Because there are many labels and many attributes, we dynamically build a local BAG during the decoding for each instance. In this way the BAG is very compact and the computation is very"
2021.emnlp-main.378,D16-1015,0,0.0627348,"Missing"
2021.emnlp-main.378,2020.coling-main.7,0,0.0114082,"ive reasoning. 2019), or requiring latent label representation to reconstruct the co-occurrence structure (Lin and Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by introducing inductive reasoning, LRN can leverage intrinsic depend"
2021.emnlp-main.378,2021.acl-long.371,1,0.831157,"Missing"
2021.emnlp-main.378,P19-1139,0,0.0404571,"Missing"
2021.emnlp-main.378,D18-1231,0,0.014152,"activated; at step 3, the label scientist is generated by inductive reasoning. 2019), or requiring latent label representation to reconstruct the co-occurrence structure (Lin and Ji, 2019). However, these methods require predefined label structures or statistics from training data, and therefore is difficult to be extended to new entity types or domains. The ultra fine-grained label set also leads to data bottleneck and the long tail problem. In recent years, some previous approaches try to tackle this problem by introducing zero/few-shot learning methods (Ma et al., 2016; Huang et al., 2016; Zhou et al., 2018; Yuan and Downey, 2018; Obeidat et al., 2019; Zhang et al., 2020b; Ren et al., 2020), or using data augmentation with denosing strategies (Ren et al., 2016b; Onoe and Durrett, 2019; Zhang et al., 2020a; Ali et al., 2020) or utilizing external knowledge (Corro et al., 2015; Dai et al., 2019) to introduce more external knowledge. In this paper, we propose Label Reasoning Network, which is significantly different from previous methods because 1) by introducing deductive reasoning, LRN can capture extrinsic dependencies between labels in an end-to-end manner without predefined structures; 2) by i"
2021.emnlp-main.637,W13-2322,0,0.0386744,"Missing"
2021.emnlp-main.637,P15-2061,0,0.0557266,"Missing"
2021.emnlp-main.637,P15-1017,0,0.0176047,"er curse is to erase the trigger information in instances and forces the model to focus more on the context. Unfortunately, due to the decisive role of triggers, directly wiping out the trigger information commonly hurts the performance (Lu et al., 2019; Liu et al., 2020b). Some previous approaches try to tackle this problem by introducing more di8078 Event detection (ED) aims to identify and classify event triggers in a sentence, e.g., detecting an Attack event triggered by fire in “They killed by hostile fire in Iraqi”. Recently, supervised ED approaches have achieved promising performance (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Lin et al., 2018, 2019b,a; Du and Cardie, 2020; Liu et al., 2020a; Lu et al., 2021), but when adapting to new event types and domains, a large number of manually annotated event data is required which is expensive. By contrast, fewshot event detection (FSED) aims to build effective event detectors that are able to detect new events from instances (query) with a few labeled instances (support set). Due to their ability to classify novel types, many few-shot algorithms have been used in FSED, e.g., metric-based methods like Prototypical Network ("
2021.emnlp-main.637,W18-2501,0,0.0312047,"Missing"
2021.emnlp-main.637,J02-3001,0,0.13038,"Missing"
2021.emnlp-main.637,P08-1030,0,0.171746,"Missing"
2021.emnlp-main.637,K19-1057,0,0.0240863,"Missing"
2021.emnlp-main.637,2020.nuse-1.5,0,0.144535,"; Nguyen and Grishman, 2015; Nguyen et al., 2016; Lin et al., 2018, 2019b,a; Du and Cardie, 2020; Liu et al., 2020a; Lu et al., 2021), but when adapting to new event types and domains, a large number of manually annotated event data is required which is expensive. By contrast, fewshot event detection (FSED) aims to build effective event detectors that are able to detect new events from instances (query) with a few labeled instances (support set). Due to their ability to classify novel types, many few-shot algorithms have been used in FSED, e.g., metric-based methods like Prototypical Network (Lai et al., 2020; Deng et al., 2020; Cong et al., 2021). Unfortunately, there has long been a “trigger curse” which troubles the learning of event detec∗ Corresponding authors. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8078–8088 c November 7–11, 2021. 2021 Association for Computational Linguistics versified context information like event argument information (Liu et al., 2017, 2019; Ji et al., 2019) and document-level information (Ji and Grishman, 2008; Liao and Grishman, 2010; Duan et al., 2017; Chen et al., 2018). However, rich context information is commo"
2021.emnlp-main.637,P10-1081,0,0.0898249,"Missing"
2021.emnlp-main.637,P18-1145,1,0.89517,"Missing"
2021.emnlp-main.637,P19-1521,1,0.886563,"Missing"
2021.emnlp-main.637,P19-1511,1,0.895938,"Missing"
2021.emnlp-main.637,2021.acl-long.361,1,0.791111,"Missing"
2021.emnlp-main.637,P19-1429,1,0.886663,"Missing"
2021.emnlp-main.637,N16-1034,0,0.0439889,"Missing"
2021.emnlp-main.637,P15-2060,0,0.0603797,"Missing"
2021.emnlp-main.637,2020.emnlp-main.129,0,0.0763779,"relation based on representation learning: 4 The proof is shown in Appendix X g(R(q; θ), q∈Q XX (4) P (s|C, t)P (t|e)R(s; θ)) t∈T s∈S t∈T s∈S   λ LSG (θ) = − Here R is a representation model which inputs s or q and outputs a dense representation. g(·, ·) is a distance metric measuring the similarity between two representations. Such loss function is widely used in many metric-based methods (e.g., Prototypical Networks and Relation Networks). In the Appendix, we prove LSG (θ) is equivalent to L(θ). 4 4.1 Experiments Experimental Settings Datasets.5 We conducted experiments on ACE05, MAVEN (Wang et al., 2020c) and KBP17 datasets. We split train/dev/test sets according to event types and we use event types with more instances for training, the other for dev/test. To conduct 5-shot experiments, we filter event types less than 6 instances. Finally, for ACE05, its train/dev/test set contains 3598/140/149 instances and 20/10/10 types respectively, for MAVEN, those are 34651/1494/1505 instances and 120/45/45 types, for KBP17, those are 15785/768/792 instances and 25/13/13 types. Task Settings. Different from episode evaluation in Lai et al. (2020) and Cong et al. (2021), we employ a more practical even"
2021.emnlp-main.637,2021.acl-long.371,1,0.828759,"Missing"
2021.emnlp-main.762,D15-1056,0,0.0297209,"iloff and Jones, 1999; Gupta and Manning, model from expanding negative entities. 2014; Yan et al., 2020a). During the above process, Currently, most bootstrapping methods define it is core to decide whether the new entities belong expansion boundary using seed-based distance metto the target category (within the expansion bound- rics, i.e., determining whether an entity should ary) or not (outside the expansion boundary) (Shi be expanded by comparing it with seeds. For inet al., 2014; Gupta and Manning, 2014). stance, Riloff and Jones (1999); Gupta and Man* Corresponding author. ning (2014); Batista et al. (2015) define the bound9673 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9673–9682 c November 7–11, 2021. 2021 Association for Computational Linguistics ary using pattern matching statistics or distributional similarities. Unfortunately, these heuristic metrics heavily depend on the selected seeds, making the boundary biased and unreliable (Curran et al., 2007; McIntosh and Curran, 2009). Although some studies extend them with extra constraints (Carlson et al., 2010) or manual participants (Berger et al., 2018), the requirement of expert knowledge mak"
2021.emnlp-main.762,D18-1229,0,0.0198909,"Man* Corresponding author. ning (2014); Batista et al. (2015) define the bound9673 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9673–9682 c November 7–11, 2021. 2021 Association for Computational Linguistics ary using pattern matching statistics or distributional similarities. Unfortunately, these heuristic metrics heavily depend on the selected seeds, making the boundary biased and unreliable (Curran et al., 2007; McIntosh and Curran, 2009). Although some studies extend them with extra constraints (Carlson et al., 2010) or manual participants (Berger et al., 2018), the requirement of expert knowledge makes them ad-hoc and inflexible. Some studies try to learn the distance metrics (Zupon et al., 2019; Yan et al., 2020a), but they still suffer from weak supervision. Furthermore, because the bootstrapping model and the boundary are mostly learned separately, it is hard for these methods to synchronously adjust the boundary when the bootstrapping model updates. entities, and the bootstrapping network can expand new positive entities within the boundaries. (3) By iteratively performing the above adversarial learning process, the bootstrapping network and th"
2021.emnlp-main.762,W14-1611,0,0.236002,"ing the same context pattern–“* is an important should precisely restrict the current bootstrapping city”) (Riloff and Jones, 1999; Gupta and Manning, model from expanding negative entities. 2014; Yan et al., 2020a). During the above process, Currently, most bootstrapping methods define it is core to decide whether the new entities belong expansion boundary using seed-based distance metto the target category (within the expansion bound- rics, i.e., determining whether an entity should ary) or not (outside the expansion boundary) (Shi be expanded by comparing it with seeds. For inet al., 2014; Gupta and Manning, 2014). stance, Riloff and Jones (1999); Gupta and Man* Corresponding author. ning (2014); Batista et al. (2015) define the bound9673 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9673–9682 c November 7–11, 2021. 2021 Association for Computational Linguistics ary using pattern matching statistics or distributional similarities. Unfortunately, these heuristic metrics heavily depend on the selected seeds, making the boundary biased and unreliable (Curran et al., 2007; McIntosh and Curran, 2009). Although some studies extend them with extra constraints (C"
2021.emnlp-main.762,N15-1128,0,0.030077,"Missing"
2021.emnlp-main.762,P13-1062,0,0.0659646,"Missing"
2021.emnlp-main.762,P09-1045,0,0.0367799,"hi be expanded by comparing it with seeds. For inet al., 2014; Gupta and Manning, 2014). stance, Riloff and Jones (1999); Gupta and Man* Corresponding author. ning (2014); Batista et al. (2015) define the bound9673 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9673–9682 c November 7–11, 2021. 2021 Association for Computational Linguistics ary using pattern matching statistics or distributional similarities. Unfortunately, these heuristic metrics heavily depend on the selected seeds, making the boundary biased and unreliable (Curran et al., 2007; McIntosh and Curran, 2009). Although some studies extend them with extra constraints (Carlson et al., 2010) or manual participants (Berger et al., 2018), the requirement of expert knowledge makes them ad-hoc and inflexible. Some studies try to learn the distance metrics (Zupon et al., 2019; Yan et al., 2020a), but they still suffer from weak supervision. Furthermore, because the bootstrapping model and the boundary are mostly learned separately, it is hard for these methods to synchronously adjust the boundary when the bootstrapping model updates. entities, and the bootstrapping network can expand new positive entities"
2021.emnlp-main.762,P18-1046,0,0.0226568,"process, we progressively refine the generator with multiple discriminators by iteratively performing the above local adversarial learning (Global progressive refining). Based on the new goals, we design a Shannon entropy-based learning objective, where the category assignment uncertainty is represented by the Shannon entropy. Formally, at bootstrapping iteration k, we use the following adversarial objective to learn the generator G and the discriminator D: 2.3.1 Pre-training Many previous studies have suggested that pretraining is important for learning convergence in GANs (Li and Ye, 2018; Qin et al., 2018). This paper pre-trains the generator (i.e., the bootstrapping network), and uses the following two kinds of pretraining algorithms: (1) The multi-view learning algorithm (Yan et al., 2020a), where the generator is co-trained with an auxiliary network. (2) Self-supervised and supervised pre-training using external resources (Yan et al., 2020b). Note that, since the external resources are not always accessible, we use the first algorithm as our default setting and set the second one as an alternative. where c is a target category, S c is the corresponding seed set, Gc<k is the set of expanded e"
2021.emnlp-main.762,P02-1006,0,0.52867,"Missing"
2021.emnlp-main.762,D19-1028,1,0.875618,"Missing"
2021.emnlp-main.762,P17-2050,0,0.0605243,"Missing"
2021.emnlp-main.762,C14-1215,1,0.677484,"Missing"
2021.emnlp-main.762,W02-1028,0,0.394695,"stly, it is obviously not enough to define a good boundary using only several positive entities. For example, as shown in Figure 1, when only using several positive entities to learn distance-based boundaries, the boundaries are usually far from optimum, which in turn 1 Introduction influences the quality of following bootstrapping Bootstrapping is a fundamental technique for en- iterations. Therefore, it is critical to enhancing the tity set expansion (ESE). It starts from a few seed boundary learning with more supervision signals or entities (e.g., {London, Beijing, Paris}) prior knowledge (Thelen and Riloff, 2002; Curran and iteratively extracts new entities in the target cat- et al., 2007). Secondly, bootstrapping is a dynamic egory (e.g., {Berlin, Moscow, Tokyo}) to process containing multiple iterations. Therefore, expand the entity set, where new entities are often the boundary needs to be synchronously adjusted evaluated by their context similarities to seeds (e.g., with the bootstrapping model, i.e., a good boundary sharing the same context pattern–“* is an important should precisely restrict the current bootstrapping city”) (Riloff and Jones, 1999; Gupta and Manning, model from expanding negati"
2021.emnlp-main.762,N19-1105,0,0.0430055,"Missing"
2021.emnlp-main.762,N18-1122,0,0.03698,"Missing"
2021.emnlp-main.762,C02-1154,0,0.435846,"Missing"
2021.emnlp-main.762,2020.acl-main.725,0,0.0379063,"Missing"
2021.emnlp-main.762,W19-1504,0,0.0905043,"n Natural Language Processing, pages 9673–9682 c November 7–11, 2021. 2021 Association for Computational Linguistics ary using pattern matching statistics or distributional similarities. Unfortunately, these heuristic metrics heavily depend on the selected seeds, making the boundary biased and unreliable (Curran et al., 2007; McIntosh and Curran, 2009). Although some studies extend them with extra constraints (Carlson et al., 2010) or manual participants (Berger et al., 2018), the requirement of expert knowledge makes them ad-hoc and inflexible. Some studies try to learn the distance metrics (Zupon et al., 2019; Yan et al., 2020a), but they still suffer from weak supervision. Furthermore, because the bootstrapping model and the boundary are mostly learned separately, it is hard for these methods to synchronously adjust the boundary when the bootstrapping model updates. entities, and the bootstrapping network can expand new positive entities within the boundaries. (3) By iteratively performing the above adversarial learning process, the bootstrapping network and the expansion boundaries are progressively refined along bootstrapping iterations. Specifically, we use a discriminator sequence containing"
2021.emnlp-main.762,2020.findings-emnlp.331,1,0.786164,"Missing"
C14-1215,P03-2031,0,0.10831,"Missing"
C14-1215,H05-1071,0,0.0538846,"Missing"
C14-1215,P11-4018,0,0.019383,"on Place: capital name chemical element Person: female first name Person: male first name Person: last name Honorific title Nationality, Religion, Political(adjectival) Category FAC ORG GPE LOC DAT LANG Description Facilities: names of man-made structures Organization: e.g. companies, governmental Place: Geo-political entities Locations other than GPEs Reference to a date or period Any named language Table 3: Target categories Corpus: In our experiments, we used the Google Web 1T corpus (Brants and Franz, 2006) as our expansion corpus. Specifically, we use the open source package LIT-Indexer (Ceylan and Mihalcea, 2011) to support efficient wildcard querying for pattern generation and instance extraction. 2285 Target Expansion Categories: We conduct our experiments on thirteen categories, which are shown in Table 3. Eleven of them are from Curran et al. (2007). Besides the eleven categories, to evaluate how well ESE systems can resolve the semantic drift problem, we use two additional categories (Capital and Chemical Element) which are high likely to drift into other categories. Evaluation Criteria: Following Curran et al (2007), we use precision at top n (P@N) as the performance metrics, i.e., the percentag"
C14-1215,J01-1005,0,0.0392116,"stances of a specific target category from text corpus or Web. For example, given the capital seeds {Rome, Beijing, Paris}, an ESE system should extract all other capitals from Web, such as Ottawa, Moscow and London. ESE system has been used in many applications, e.g., dictionary construction (Cohen and Sarawagi, 2004), word sense disambiguation (Pantel and Lin, 2002), query refinement (Hu et al., 2009), and query suggestion (Cao et al., 2008). Due to the limited supervision provided by ESE (in most cases only 3-5 seeds are given), traditional ESE systems usually employ bootstrapping methods (Cucchiarelli and Velardi, 2001; Etzioni et al., 2005; Pasca, 2007; Riloff and Jones, 1999; Wang and Cohen, 2008). That is, the entity set is iteratively expanded through a pattern generation step and an instance extraction step. Figure 1(a) demonstrates a simple bootstrapping process.  Positive Negative Chicago Rome at the embassy in * Tokyo Milan Beijing * is the capital of London at the embassy in * Tokyo Paris to cities such as * Shanghai * is the capital of Berlin Boston at the hotel in * Milan London Sydney * is the city of Chicago New York * the official web site Nokia * is the city of Rome Beijing Paris at the hote"
C14-1215,P10-1150,0,0.015269,"ve positive seeds from the list of the category’s instances while the initial negative seeds are manually provided. ii) Feedback_Co-Bootstrapping (FB_CB): This is our proposed probabilistic Co-Bootstrapping method with two steps of selecting initial negative seeds: 1) Expand the entity set using only the positive seeds for only first iteration. Return the top ten instances. 2) Select the negative instances in the top ten results of the first iteration as negative seeds. 4.2.1. Overall Performance Several papers have shown that the experimental performance may vary with different seed choices (Kozareva and Hovy, 2010; McIntosh and Curran, 2009; Vvas et al., 2009). Therefore, we input the ESE system with five different positive seed settings for each category. Finally we average the performance on the five settings so that the impact of seed selection can be reduced. P@10 P@20 P@50 P@100 P@200 MAP 0.84 0.74 0.55 0.41 0.34 0.42 POS ME 0.83(0.90)0.79(0.87)0.68(0.78)0.58(0.67)0.51(0.59) 0.95 0.83 0.71 0.57 0.78 Hum_CB 0.97 0.97 0.96 0.90 0.79 0.66 0.85 FB_CB Table 4: The overall experimental results Table 4 shows the overall experimental results. The results in parentheses are the known results of eleven cate"
C14-1215,U08-1013,0,0.606195,"0), etc. 2281 The main drawbacks of the traditional bootstrapping methods are the expansion boundary problem and the semantic drift problem. Currently, two strategies have been exploited to resolve the semantic drift problem. The first is the ranking based approaches (Pantel and Pennacchiotti, 2006; Talukdar et al., 2008), which select highly confident patterns and instances through a ranking algorithm, with the assumption that high-ranked instances will be more likely to be the instances of the target category. The second is the mutual exclusion constraint based methods (Curran et al., 2007; McIntosh and Curran, 2008; Pennacchiotti and Pantel, 2011; Thelen and Riloff, 2002; Yangarber et al., 2002), which expand multiple categories simultaneously and determine the expansion boundary based on the mutually exclusive property of the pre-given categories. 3 3.1 The Co-Bootstrapping Method The Framework of Probabilistic Co-Bootstrapping Given the initial positive seeds and negative seeds, the goal of our method is to extract instances of a specific target semantic category. For demonstration, we will describe our method through the running example shown in Figure 1(b). Specifically, Figure 2 shows the framework"
C14-1215,P09-1045,0,0.437317,"e list of the category’s instances while the initial negative seeds are manually provided. ii) Feedback_Co-Bootstrapping (FB_CB): This is our proposed probabilistic Co-Bootstrapping method with two steps of selecting initial negative seeds: 1) Expand the entity set using only the positive seeds for only first iteration. Return the top ten instances. 2) Select the negative instances in the top ten results of the first iteration as negative seeds. 4.2.1. Overall Performance Several papers have shown that the experimental performance may vary with different seed choices (Kozareva and Hovy, 2010; McIntosh and Curran, 2009; Vvas et al., 2009). Therefore, we input the ESE system with five different positive seed settings for each category. Finally we average the performance on the five settings so that the impact of seed selection can be reduced. P@10 P@20 P@50 P@100 P@200 MAP 0.84 0.74 0.55 0.41 0.34 0.42 POS ME 0.83(0.90)0.79(0.87)0.68(0.78)0.58(0.67)0.51(0.59) 0.95 0.83 0.71 0.57 0.78 Hum_CB 0.97 0.97 0.96 0.90 0.79 0.66 0.85 FB_CB Table 4: The overall experimental results Table 4 shows the overall experimental results. The results in parentheses are the known results of eleven categories (without CAP and ELE"
C14-1215,N04-1041,0,0.0372461,"Missing"
C14-1215,P06-1015,0,0.137591,"able attentions from both research (An et al., 2003; Cafarella et al., 2005; Pantel and Ravichandran, 2004; Pantel et al., 2009; Pasca, 2007; Wang and Cohen, 2008) and industry communities (e.g., Google Sets). Till now, most ESE systems employ bootstrapping methods, such as DIPRE (Brin, 1998), Snowball (Agichtein and Gravano, 2000), etc. 2281 The main drawbacks of the traditional bootstrapping methods are the expansion boundary problem and the semantic drift problem. Currently, two strategies have been exploited to resolve the semantic drift problem. The first is the ranking based approaches (Pantel and Pennacchiotti, 2006; Talukdar et al., 2008), which select highly confident patterns and instances through a ranking algorithm, with the assumption that high-ranked instances will be more likely to be the instances of the target category. The second is the mutual exclusion constraint based methods (Curran et al., 2007; McIntosh and Curran, 2008; Pennacchiotti and Pantel, 2011; Thelen and Riloff, 2002; Yangarber et al., 2002), which expand multiple categories simultaneously and determine the expansion boundary based on the mutually exclusive property of the pre-given categories. 3 3.1 The Co-Bootstrapping Method T"
C14-1215,W11-0319,0,0.0824926,"And all these explanations are reasonable. 2) The semantic drift problem. That is, the expansion category may change gradually when noisy instances/patterns are introduced during the bootstrapping iterations. For example, in Figure 1 (a), the instance Rome will introduce a pattern “* is the city of”, which will introduce many noisy city instances such as Milan and Chicago for the expansion of Capital. And these noisy cities in turn will introduce more city patterns and instances, and finally will lead to a semantic drift from Capital to City. In recent years, some methods (Curran et al, 2007; Pennacchiotti and Pantel, 2011) have exploited mutual exclusion constraint to resolve the semantic drift problem. These methods expand multiple categories simultaneously, and will determine the expansion boundary based on the mutually exclusive property of the pre-given categories. For instance, the exclusive categories Fruit and Company will be jointly expanded and the expansion boundary of {Apple, Banana, Cherry} will be limited by the expansion boundary of {Google, Microsoft, Apple Inc.}. These methods, however, still have the following two drawbacks: 1) These methods require that the expanded categories should be mutual"
C14-1215,D08-1061,0,0.0357429,"ch (An et al., 2003; Cafarella et al., 2005; Pantel and Ravichandran, 2004; Pantel et al., 2009; Pasca, 2007; Wang and Cohen, 2008) and industry communities (e.g., Google Sets). Till now, most ESE systems employ bootstrapping methods, such as DIPRE (Brin, 1998), Snowball (Agichtein and Gravano, 2000), etc. 2281 The main drawbacks of the traditional bootstrapping methods are the expansion boundary problem and the semantic drift problem. Currently, two strategies have been exploited to resolve the semantic drift problem. The first is the ranking based approaches (Pantel and Pennacchiotti, 2006; Talukdar et al., 2008), which select highly confident patterns and instances through a ranking algorithm, with the assumption that high-ranked instances will be more likely to be the instances of the target category. The second is the mutual exclusion constraint based methods (Curran et al., 2007; McIntosh and Curran, 2008; Pennacchiotti and Pantel, 2011; Thelen and Riloff, 2002; Yangarber et al., 2002), which expand multiple categories simultaneously and determine the expansion boundary based on the mutually exclusive property of the pre-given categories. 3 3.1 The Co-Bootstrapping Method The Framework of Probabil"
C14-1215,W02-1028,0,0.424211,"apping methods are the expansion boundary problem and the semantic drift problem. Currently, two strategies have been exploited to resolve the semantic drift problem. The first is the ranking based approaches (Pantel and Pennacchiotti, 2006; Talukdar et al., 2008), which select highly confident patterns and instances through a ranking algorithm, with the assumption that high-ranked instances will be more likely to be the instances of the target category. The second is the mutual exclusion constraint based methods (Curran et al., 2007; McIntosh and Curran, 2008; Pennacchiotti and Pantel, 2011; Thelen and Riloff, 2002; Yangarber et al., 2002), which expand multiple categories simultaneously and determine the expansion boundary based on the mutually exclusive property of the pre-given categories. 3 3.1 The Co-Bootstrapping Method The Framework of Probabilistic Co-Bootstrapping Given the initial positive seeds and negative seeds, the goal of our method is to extract instances of a specific target semantic category. For demonstration, we will describe our method through the running example shown in Figure 1(b). Specifically, Figure 2 shows the framework of our method. The central tasks of our Co-Bootstrapping"
C14-1215,P09-1050,0,0.0549261,"Missing"
C14-1215,D09-1098,0,\N,Missing
C14-1215,C02-1154,0,\N,Missing
C16-1273,P10-1124,0,0.698454,"Missing"
C16-1273,P11-1062,0,0.0193765,"denote it as DIRT. The other uses the BalancedInclusion similarity in (Szpektor and Dagan, 2008), we denote it as BINC. 2) We evaluate a latent topic model based context-sensitive method. We follow the method described in Melamud et al. (2013), a two level model which computes context-sensitive similarity using two predicates’ word-level vectors biased by topic-level context representations. We apply their method on two base word-level similarities, the LIN similarity and the BINC similarity, correspondingly denoted as WT-LIN and WT-BINC. 3) We evaluate the global learning method proposed in Berant et al. (2011), which use ILP solvers to performance global optimization over local classification results—We denote it as ILP. For comparison, we directly use the inference rule resource3 released by Berant et al. (2011), which was also learned from the ReVerb corpus. For our graph-based method, we tune its parameters on the validating dataset, and the final parameters used in our method are as follows: the global restart probability λ=0.1, the weight of the semantic dependent edge α = 4.0, and the context relevance restart weight β=0.7. 2 3 http://u.cs.biu.ac.il/~nlp/resources/downloads/annotation-of-rule"
C16-1273,D07-1017,0,0.0198912,"or inference rule discovery, and most of them are distributional similarity based methods. Based on the distributional hypothesis, traditional methods differ in their feature representations and their similarity measures. For predicate representation, some methods represent predicates using one feature vector, where each feature is a pair of argument instantiations such as X=‘children’-Y=‘skill’(Szpektor et al., 2004; Sekine, 2005; Nakashole et al., 2012; Dutta et al., 2015); some methods represent predicates using two or more feature vectors, one for each argument slot (Lin and Pantel, 2001; Bhagat et al., 2007), e.g., one feature vector for slot X and one for slot Y. To compute the similarity between predicates, many similarity measures have been proposed, such as DIRT Similarity (Lin and Pantel, 2001), Balanced-Inclusion similarity (Szpektor and Dagan, 2008) and Soften Set Inclusion similarity (Nakashole et al., 2012), etc. Hashimoto et al. (2009) proposed a conditional probability based directional similarity measure to acquire verb entailment pairs on a large scale corpus. As discussed in above, the main drawbacks of these methods are that they are context-insensitive and model predicates indepen"
C16-1273,D10-1113,0,0.0352124,"predicate using ontological type signatures (Pantel et al., 2007; Nakashole et al., 2012), e.g., &lt;singer, song> for ‘X sing Y’, based on the assumption that two predicates in a rule must have the same type signature. The shortcomings of the class-based context models are that they need a fine-grained ontology and it is often very challenging to determine the fine-grained types of arguments. The latent topic based model represents the context of a predicate as a vector in a low dimensional space, such as the LSA-based model (Szpektor et al., 2008) and the LDA based model (Ritter et al., 2010; Dinu and Lapata, 2010). Based on the context vector, the similarity between two predicates are computed by combining both the context vector similarity and the feature vector similarity (Szpektor et al., 2008), or by first learning predicate similarity per topic, then combining the per-topic similarities using context vector (Melamud et al., 2013). Currently, most of the context-sensitive methods focus on developing an extra context model, by contrast our method focuses on the learning of context-specific predicate representations, without the need of an extra context model. Recent research has also investigated th"
C16-1273,D11-1142,0,0.301902,"Missing"
C16-1273,D09-1122,0,0.133412,"ntiations such as X=‘children’-Y=‘skill’(Szpektor et al., 2004; Sekine, 2005; Nakashole et al., 2012; Dutta et al., 2015); some methods represent predicates using two or more feature vectors, one for each argument slot (Lin and Pantel, 2001; Bhagat et al., 2007), e.g., one feature vector for slot X and one for slot Y. To compute the similarity between predicates, many similarity measures have been proposed, such as DIRT Similarity (Lin and Pantel, 2001), Balanced-Inclusion similarity (Szpektor and Dagan, 2008) and Soften Set Inclusion similarity (Nakashole et al., 2012), etc. Hashimoto et al. (2009) proposed a conditional probability based directional similarity measure to acquire verb entailment pairs on a large scale corpus. As discussed in above, the main drawbacks of these methods are that they are context-insensitive and model predicates independently. Having observed that the meaning of a predicate is context-sensitive, several recent methods try to model the context of a predicate using class-based model or latent topic model. The class-based models represent the context of a predicate using ontological type signatures (Pantel et al., 2007; Nakashole et al., 2012), e.g., &lt;singer,"
C16-1273,C92-2082,0,0.232639,"elevant information from context-irrelevant information. Experimental results show that our method significantly outperforms traditional inference rule discovery methods. 1 Introduction Inference rule discovery aims to identify entailment relations between predicates, such as ‘X acquire Y à X purchase Y’ and ‘X is author of Y à X write Y’, with each predicate is a textual pattern with (two) variable slots (X and Y in above). Inference rules are important in many fields such as Question Answering (Ravichandran and Hovy, 2002), Textual Entailment (Dagan et al., 2006) and Information Extraction (Hearst, 1992). For example, given the problem “Which company purchases WhatsApp?”, a QA system can extract the answer “Facebook” from the sentence “Facebook acquires WhatsApp for $19 billion” based on the inference rule ‘X acquire Y à X purchase Y’. Given a set of predicates and their instantiations in a large corpus, most traditional methods identify inference rules by computing distributional similarities between predicates, where each predicate is represented as one or more feature vectors of its variable instantiations. For example, given the predicates and their instantiations in Figure 1, we can repr"
C16-1273,P13-1131,0,0.0750252,"pages 2902–2911, Osaka, Japan, December 11-17 2016. X buy Y X purchase Y X acquire Y X learn Y Predicate (Facebook, WhatsApp) (Google, YouTube) (children, skill) Variable Instantiation Figure 1. Some predicates and their variable instantiations These distributional similarity based methods, however, have two main drawbacks: Firstly, these methods are mostly context-insensitive, cannot accurately measure the similarity between two predicates in a specific context. Due to the ambiguity of predicates, a predicate may have different meanings under different contexts (In this paper, as the same as Melamud et al. (2013), the context of a predicate is specified by the predicate’s given arguments). For example, the predicate ‘X acquire Y’ should have different meanings under context (Google, YouTube) and context (children, skill), because it corresponds to two different senses of acquire in these two contexts. Unfortunately, traditional methods mostly use the same representation to represent a predicate in different contexts, therefore may learn invalid inference rules. For example, given two predicates ‘X acquire Y’ and ‘X purchase Y’, traditional context-insensitive methods will return the same similarity be"
C16-1273,D12-1104,0,0.395989,"represented as one or more feature vectors of its variable instantiations. For example, given the predicates and their instantiations in Figure 1, we can represent ‘X acquire Y’ as {X=‘Google’, Y=‘YouTube’, X=‘children’, Y=‘skill’} and measure the similarity between ‘X acquire Y’ and ‘X purchase Y’ based on their common features {X=‘Google’, Y=‘YouTube’}. To achieve the above goal, many similarity measures have been proposed for inference rule discovery, such as DIRT Similarity (Lin and Pantel, 2001), Balanced-Inclusion similarity (Szpektor and Dagan, 2008) and Soft Set Inclusion similarity (Nakashole et al., 2012), etc. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 2902 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2902–2911, Osaka, Japan, December 11-17 2016. X buy Y X purchase Y X acquire Y X learn Y Predicate (Facebook, WhatsApp) (Google, YouTube) (children, skill) Variable Instantiation Figure 1. Some predicates and their variable instantiations These distributional similarity based methods, however, have two main drawbacks: First"
C16-1273,N07-1071,0,0.0334167,"larity (Nakashole et al., 2012), etc. Hashimoto et al. (2009) proposed a conditional probability based directional similarity measure to acquire verb entailment pairs on a large scale corpus. As discussed in above, the main drawbacks of these methods are that they are context-insensitive and model predicates independently. Having observed that the meaning of a predicate is context-sensitive, several recent methods try to model the context of a predicate using class-based model or latent topic model. The class-based models represent the context of a predicate using ontological type signatures (Pantel et al., 2007; Nakashole et al., 2012), e.g., &lt;singer, song> for ‘X sing Y’, based on the assumption that two predicates in a rule must have the same type signature. The shortcomings of the class-based context models are that they need a fine-grained ontology and it is often very challenging to determine the fine-grained types of arguments. The latent topic based model represents the context of a predicate as a vector in a low dimensional space, such as the LSA-based model (Szpektor et al., 2008) and the LDA based model (Ritter et al., 2010; Dinu and Lapata, 2010). Based on the context vector, the similari"
C16-1273,D14-1162,0,0.0807042,"ature f and context c. In this paper, the context of a predicate p is its variable instantiation (X=x, Y=y), such as (X=‘Microsoft’, Y=‘Nokia’) for ‘X acquire Y’. Then we measure the context relevance using the word similarity between feature f and the corresponding argument of context c: CR(f, c) = Sim(f , c ) where fw is the word content of feature f (e.g., people for X=‘people’), fs is the slot signature of feature f (e.g., X for X=‘people’), and cfs is the word in the slot fs of context c. In this paper, the similarity between two words is the cosine similarity between their word vectors (Pennington et al., 2014), using a publicly available pre-trained word vectors1. Finally, the context-sensitive node-dependent restart probability of node i is computed as: λ + β (1 − λ) 1.0 − CR(i, c) if i is a feature λ, = λ if i is a predicate where λ is the global restart probability used for smoothing, β is used to control the impact of context relevance in context-sensitive random walk, which will be empirically tuned. Table 2 shows the learned context-specific representations of ‘X acquire Y’ in different contexts. We can see that our algorithm can effectively learn context-specific representations: the most im"
C16-1273,P10-1044,0,0.0606344,"Missing"
C16-1273,P02-1006,0,0.028796,"algorithm, which can learn context-specific predicate representations by distinguishing context-relevant information from context-irrelevant information. Experimental results show that our method significantly outperforms traditional inference rule discovery methods. 1 Introduction Inference rule discovery aims to identify entailment relations between predicates, such as ‘X acquire Y à X purchase Y’ and ‘X is author of Y à X write Y’, with each predicate is a textual pattern with (two) variable slots (X and Y in above). Inference rules are important in many fields such as Question Answering (Ravichandran and Hovy, 2002), Textual Entailment (Dagan et al., 2006) and Information Extraction (Hearst, 1992). For example, given the problem “Which company purchases WhatsApp?”, a QA system can extract the answer “Facebook” from the sentence “Facebook acquires WhatsApp for $19 billion” based on the inference rule ‘X acquire Y à X purchase Y’. Given a set of predicates and their instantiations in a large corpus, most traditional methods identify inference rules by computing distributional similarities between predicates, where each predicate is represented as one or more feature vectors of its variable instantiations."
C16-1273,I05-5011,0,0.0301543,"the proposed method. Section 4 presents and discusses experimental results. Finally we conclude this paper in Section 5. 2903 2 Related Work Many approaches have been proposed for inference rule discovery, and most of them are distributional similarity based methods. Based on the distributional hypothesis, traditional methods differ in their feature representations and their similarity measures. For predicate representation, some methods represent predicates using one feature vector, where each feature is a pair of argument instantiations such as X=‘children’-Y=‘skill’(Szpektor et al., 2004; Sekine, 2005; Nakashole et al., 2012; Dutta et al., 2015); some methods represent predicates using two or more feature vectors, one for each argument slot (Lin and Pantel, 2001; Bhagat et al., 2007), e.g., one feature vector for slot X and one for slot Y. To compute the similarity between predicates, many similarity measures have been proposed, such as DIRT Similarity (Lin and Pantel, 2001), Balanced-Inclusion similarity (Szpektor and Dagan, 2008) and Soften Set Inclusion similarity (Nakashole et al., 2012), etc. Hashimoto et al. (2009) proposed a conditional probability based directional similarity measu"
C16-1273,C08-1107,0,0.318139,"onal similarities between predicates, where each predicate is represented as one or more feature vectors of its variable instantiations. For example, given the predicates and their instantiations in Figure 1, we can represent ‘X acquire Y’ as {X=‘Google’, Y=‘YouTube’, X=‘children’, Y=‘skill’} and measure the similarity between ‘X acquire Y’ and ‘X purchase Y’ based on their common features {X=‘Google’, Y=‘YouTube’}. To achieve the above goal, many similarity measures have been proposed for inference rule discovery, such as DIRT Similarity (Lin and Pantel, 2001), Balanced-Inclusion similarity (Szpektor and Dagan, 2008) and Soft Set Inclusion similarity (Nakashole et al., 2012), etc. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 2902 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2902–2911, Osaka, Japan, December 11-17 2016. X buy Y X purchase Y X acquire Y X learn Y Predicate (Facebook, WhatsApp) (Google, YouTube) (children, skill) Variable Instantiation Figure 1. Some predicates and their variable instantiations These distributional simila"
C16-1273,P08-1078,0,0.0209503,"latent topic model. The class-based models represent the context of a predicate using ontological type signatures (Pantel et al., 2007; Nakashole et al., 2012), e.g., &lt;singer, song> for ‘X sing Y’, based on the assumption that two predicates in a rule must have the same type signature. The shortcomings of the class-based context models are that they need a fine-grained ontology and it is often very challenging to determine the fine-grained types of arguments. The latent topic based model represents the context of a predicate as a vector in a low dimensional space, such as the LSA-based model (Szpektor et al., 2008) and the LDA based model (Ritter et al., 2010; Dinu and Lapata, 2010). Based on the context vector, the similarity between two predicates are computed by combining both the context vector similarity and the feature vector similarity (Szpektor et al., 2008), or by first learning predicate similarity per topic, then combining the per-topic similarities using context vector (Melamud et al., 2013). Currently, most of the context-sensitive methods focus on developing an extra context model, by contrast our method focuses on the learning of context-specific predicate representations, without the nee"
C16-1273,W04-3206,0,0.339707,"rk. Section 3 describes the proposed method. Section 4 presents and discusses experimental results. Finally we conclude this paper in Section 5. 2903 2 Related Work Many approaches have been proposed for inference rule discovery, and most of them are distributional similarity based methods. Based on the distributional hypothesis, traditional methods differ in their feature representations and their similarity measures. For predicate representation, some methods represent predicates using one feature vector, where each feature is a pair of argument instantiations such as X=‘children’-Y=‘skill’(Szpektor et al., 2004; Sekine, 2005; Nakashole et al., 2012; Dutta et al., 2015); some methods represent predicates using two or more feature vectors, one for each argument slot (Lin and Pantel, 2001; Bhagat et al., 2007), e.g., one feature vector for slot X and one for slot Y. To compute the similarity between predicates, many similarity measures have been proposed, such as DIRT Similarity (Lin and Pantel, 2001), Balanced-Inclusion similarity (Szpektor and Dagan, 2008) and Soften Set Inclusion similarity (Nakashole et al., 2012), etc. Hashimoto et al. (2009) proposed a conditional probability based directional si"
C16-1273,P07-1058,0,0.0645519,"Missing"
C16-1273,P12-2031,0,0.0314329,"Missing"
C18-1076,W14-2401,0,0.0244716,"m. This paraphrase tables is suitable for our needs since it focuses on question paraphrases. The third resource we used is PPDB. Specifically, we use PPDB-2.0 (Pavlick et al., 2015) to calculate the similarity between two phrases by utilizing their scores, which consider many aspects. As we argued before, we only consider single word. So we use the lexical part of PPDB-2.0. Moreover, we pre-process the PPDB dataset by lemming the words. Table 1 shows several nearest neighbor words for the word “currency” from PPDB-2.0. In fact, we can also use lexicon resources like synset from WordNet,7 and Allen (2014) has used the VerbNet for learning a lexicon for broad-coverage semantic parsing. However, we find that the synsets for words are almost covered by the resources mentioned before. So we don’t use these resources here. In order to limit the graph size, we consider the top 10 nearest labeled nodes and top 5 nearest bridge nodes for each unlabeled one; for the each bridge node, we consider the top 5 nearest labeled nodes. Moreover, we also consider edges between labeled nodes and labeled nodes. Finally, the overall similarity score between two given phrases w1 and w2 is computed as follow: sim(w1"
C18-1076,Q13-1005,0,0.0690993,"督学习框架，该框架能够充分利用容易获取的 大量文本语料和词典资源来 进行词典扩充学习。该词典扩充学习方法首先利用大量文本语料和词典资源来学习词语 与词语之间的相似度，并构建用于图传播的图；接着使用图传播算法从少量标注的词汇 中学习新的词汇。本文 在两个公开数据集上进行了实验，实验结果表明：本文系统相比 未使用新词汇的基准系统取得了显著提升，相比当前 最好的系统，也取得了具有竞争力 的结果。 1 Introduction Semantic parsing aims to map natural language sentences into formal meaning representations, e.g., Figure 1 shows an example of semantic parsing. Semantic parsing plays an important role in natural language understanding, and has attracted increasing attention in recent years (Zelle and Mooney, 1996; Wong and Mooney, 2007; Lu et al., 2008; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2014; Li et al., 2015; Chen et al., 2016; Xiao et al., 2016; Jia and Liang, 2016; Reddy et al., 2016; Liang et al., 2017). The performance of semantic parsers critically depends on the quality of lexicon, including accuracy and coverage. Specifically, in order to construct the logical form from a sentence, we first need to learn a lexicon,1 which contains the mappings from natural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example in Figure 1, we can see that lexicon is the foundation of parsing, and lexicon learning plays"
C18-1076,D14-1134,0,0.0185059,"quality of lexicon, including accuracy and coverage. Specifically, in order to construct the logical form from a sentence, we first need to learn a lexicon,1 which contains the mappings from natural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example in Figure 1, we can see that lexicon is the foundation of parsing, and lexicon learning plays an important role in semantic parsing. Traditional semantic parsers are usually domain-specific, which only contains a limited number of logical predicates (Zettlemoyer and Collins, 2005; Kwiatkowksi et al., 2010; Artzi et al., 2014). In this This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 We follow the lexicon style defined in Berant et al. (2013). License details: http:// 892 Proceedings of the 27th International Conference on Computational Linguistics, pages 892–904 Santa Fe, New Mexico, USA, August 20-26, 2018. Type.cityŁPlaceOfBirth.Barack_Obama Grammar PlaceOfBirth.Barack_Obama Grammar Type.City Lexicon Which city was Barack_Obama PlaceOfBirth Lexicon Lexicon barack obama born in ? Figure 1: An example of semantic parsing, which uses lexico"
C18-1076,P14-1091,0,0.0139624,"the parameters in Junto, we set µ1 = 0.55, µ2 = 0.44, µ3 = 0.01, β = 2. Comparing systems: To evaluate our method, we mainly compare our system (Base + lexicon) to the base system (Base) which does not use the learned wide-coverage lexicon, also to system (Base + bridge) which utilize bridge operator to serve as lexicon (Berant et al., 2013). We also compare to several nearly published systems, including semantic parsing based system (Kwiatkowski et al., 2013; Berant and Liang, 2015), information extraction based systems (Yao and Van Durme, 2014; Yao, 2015), machine translation based systems (Bao et al., 2014), embedding based systems (Bordes et al., 2014; Yang et al., 2014), and QA based system (Bast and Haussmann, 2015). 5.1 Experimental Results Table 2 and Table 3 provide the performances of all baselines9 and our method in W EBQUESTIONS and F REE 917. From Table 2 and Table 3, we can see that: 1. Our method achieves competitive performance: Our system outperforms base system (Base) greatly and gets a better performance when comparing to the base system with a bridge operator (Base + bridge). 2. The learned lexicon has wider coverage than the seed one: Our system obtains higher recall than the B"
C18-1076,P14-1133,0,0.140634,"Missing"
C18-1076,Q15-1039,0,0.131802,"n Formula (2), the first part enforces the labels of the seed nodes to keep unchanged. The second part enforces the smoothness, making similar nodes have similar labels. The third part enforces an uniform distribution for the unlabeled nodes. We use the Junto label propagation toolkit8 for label propagation. 6 http://www.answers.com/Q/ https://wordnet.princeton.edu/ 8 https://github.com/parthatalukdar/junto 7 897 4 Semantic Parsing with Extended Lexicon After graph propagation, each unlabeled phrase is labeled with a distribution over the set of predicates. We use SEMPRE (Berant et al., 2013; Berant and Liang, 2015) as our base semantic parser. In order to use the learned lexicon, we add a feature which indicates the final score for each lexical entry. The semantic parser will train on the training data with the learned lexicon as its initial lexicon. Following Berant and Liang (2015), we also use the feature template that conjoins predicates and content lemmas, and this feature template has been proved very helpful in Berant and Liang (2015). 5 Experiments We evaluate our method on two benchmark datasets: W EBQUESTIONS and F REE 917. Dataset: W EBQUESTIONS dataset (Berant et al., 2013) contains 5,810 qu"
C18-1076,D13-1160,0,0.386594,"tural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example in Figure 1, we can see that lexicon is the foundation of parsing, and lexicon learning plays an important role in semantic parsing. Traditional semantic parsers are usually domain-specific, which only contains a limited number of logical predicates (Zettlemoyer and Collins, 2005; Kwiatkowksi et al., 2010; Artzi et al., 2014). In this This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 We follow the lexicon style defined in Berant et al. (2013). License details: http:// 892 Proceedings of the 27th International Conference on Computational Linguistics, pages 892–904 Santa Fe, New Mexico, USA, August 20-26, 2018. Type.cityŁPlaceOfBirth.Barack_Obama Grammar PlaceOfBirth.Barack_Obama Grammar Type.City Lexicon Which city was Barack_Obama PlaceOfBirth Lexicon Lexicon barack obama born in ? Figure 1: An example of semantic parsing, which uses lexicons to map phrases to predicates, and applies grammars to construct the logical form. case, the mappings for each predicate can be learned relatively easily from training corpus. Recently, a grow"
C18-1076,D14-1067,0,0.0216343,"2 = 0.44, µ3 = 0.01, β = 2. Comparing systems: To evaluate our method, we mainly compare our system (Base + lexicon) to the base system (Base) which does not use the learned wide-coverage lexicon, also to system (Base + bridge) which utilize bridge operator to serve as lexicon (Berant et al., 2013). We also compare to several nearly published systems, including semantic parsing based system (Kwiatkowski et al., 2013; Berant and Liang, 2015), information extraction based systems (Yao and Van Durme, 2014; Yao, 2015), machine translation based systems (Bao et al., 2014), embedding based systems (Bordes et al., 2014; Yang et al., 2014), and QA based system (Bast and Haussmann, 2015). 5.1 Experimental Results Table 2 and Table 3 provide the performances of all baselines9 and our method in W EBQUESTIONS and F REE 917. From Table 2 and Table 3, we can see that: 1. Our method achieves competitive performance: Our system outperforms base system (Base) greatly and gets a better performance when comparing to the base system with a bridge operator (Base + bridge). 2. The learned lexicon has wider coverage than the seed one: Our system obtains higher recall than the Base. By utilizing large amount of text corpora"
C18-1076,P13-1042,0,0.383244,"Conference on Computational Linguistics, pages 892–904 Santa Fe, New Mexico, USA, August 20-26, 2018. Type.cityŁPlaceOfBirth.Barack_Obama Grammar PlaceOfBirth.Barack_Obama Grammar Type.City Lexicon Which city was Barack_Obama PlaceOfBirth Lexicon Lexicon barack obama born in ? Figure 1: An example of semantic parsing, which uses lexicons to map phrases to predicates, and applies grammars to construct the logical form. case, the mappings for each predicate can be learned relatively easily from training corpus. Recently, a growing body of research has scaled up semantic parsers to open domain (Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013) , where the number of predicates has increased substantially, making it hard to learn a lexicon with high coverage. To resolve the lexicon coverage problem, there have been several papers on lexicon learning for semantic parsing. Cai and Yates (2013a) learns lexicons by pattern matching. Berant et al. (2013) learns lexicons by aligning Freebase 2 predicates with relations from ClueWeb 3 , and then the alignments are used as lexicons. However, the lexicon coverage of these alignment-based m"
C18-1076,S13-1045,0,0.197027,"Conference on Computational Linguistics, pages 892–904 Santa Fe, New Mexico, USA, August 20-26, 2018. Type.cityŁPlaceOfBirth.Barack_Obama Grammar PlaceOfBirth.Barack_Obama Grammar Type.City Lexicon Which city was Barack_Obama PlaceOfBirth Lexicon Lexicon barack obama born in ? Figure 1: An example of semantic parsing, which uses lexicons to map phrases to predicates, and applies grammars to construct the logical form. case, the mappings for each predicate can be learned relatively easily from training corpus. Recently, a growing body of research has scaled up semantic parsers to open domain (Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013) , where the number of predicates has increased substantially, making it hard to learn a lexicon with high coverage. To resolve the lexicon coverage problem, there have been several papers on lexicon learning for semantic parsing. Cai and Yates (2013a) learns lexicons by pattern matching. Berant et al. (2013) learns lexicons by aligning Freebase 2 predicates with relations from ClueWeb 3 , and then the alignments are used as lexicons. However, the lexicon coverage of these alignment-based m"
C18-1076,P16-1073,1,0.81012,"Missing"
C18-1076,P11-1144,0,0.0347609,"s to get the phrase similarity and phrase co-occurrence. In this way, we can learn more lexicon from little seed lexicon. Krishnamurthy (2016) also learned a lexicon for semantic parsing. However, they aim to extend the predicate side as they think the predicates have limited coverage for new sentences. Our aim is to extend the phrase that can trigger the predicates. Graph-based semi-supervised learning algorithm has been used to resolve the OOV problem in machine translation (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015; Mehdizadeh Seraj et al., 2015). frame semantic parsing (Das and Smith, 2011), sentiment lexicon induction (Hamilton et al., 2016), and morph-syntactic lexicon induction (Faruqui et al., 2016). 3 Graph-based Lexicon Induction Lexicon learning aims to learn the mapping from natural language phrases to predicates in knowledge base. There are three types of lexicons, including entity lexicon (e.g., “city” :: Type.City), unary lexicon (e.g., “barack obama” :: Barack Obama) and binary lexicon (e.g., “born” :: PlaceOfBirth). In most cases entity lexicons are learned using entity linking techniques, therefore we usually only consider unary and binary lexicons in lexicon learn"
C18-1076,P13-1158,0,0.0327371,"cess, especially for tasks related to similarity. Our purpose is to find similar words for the labeled ones, and label 896 neighbor words guilder coin taxa les exchange monetary money scores 3.05 3.03 3.00 2.89 2.85 2.76 2.63 Table 1: Nearest neighbor words for “currency” from PPDB-2.0. them with the same labels. We use the published word vector (Huang et al., 2012) directly, and use cosine distance for similarity. The second resource we used is paraphrase tables. In this part, we want to utilize paraphrase pairs, like “money” and “currency”. We construct these pairs using the Paralex corpus (Fader et al., 2013). Paralex is a large question paraphrases corpus from WikiAnswers,6 and each clique questions were tagged as expressing the same meaning by users. Paraphrase pairs in Paralex are word-aligned using standard machine translation methods. We use the word alignments to construct a word table by applying the consistent word pair heuristic to only unigram. This paraphrase tables is suitable for our needs since it focuses on question paraphrases. The third resource we used is PPDB. Specifically, we use PPDB-2.0 (Pavlick et al., 2015) to calculate the similarity between two phrases by utilizing their"
C18-1076,Q16-1001,0,0.0295267,"exicon. Krishnamurthy (2016) also learned a lexicon for semantic parsing. However, they aim to extend the predicate side as they think the predicates have limited coverage for new sentences. Our aim is to extend the phrase that can trigger the predicates. Graph-based semi-supervised learning algorithm has been used to resolve the OOV problem in machine translation (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015; Mehdizadeh Seraj et al., 2015). frame semantic parsing (Das and Smith, 2011), sentiment lexicon induction (Hamilton et al., 2016), and morph-syntactic lexicon induction (Faruqui et al., 2016). 3 Graph-based Lexicon Induction Lexicon learning aims to learn the mapping from natural language phrases to predicates in knowledge base. There are three types of lexicons, including entity lexicon (e.g., “city” :: Type.City), unary lexicon (e.g., “barack obama” :: Barack Obama) and binary lexicon (e.g., “born” :: PlaceOfBirth). In most cases entity lexicons are learned using entity linking techniques, therefore we usually only consider unary and binary lexicons in lexicon learning. In open-domain semantic parsing, it is hard to learn high-coverage lexicon from annotated data for lexicon lea"
C18-1076,D16-1057,0,0.0275088,"ence. In this way, we can learn more lexicon from little seed lexicon. Krishnamurthy (2016) also learned a lexicon for semantic parsing. However, they aim to extend the predicate side as they think the predicates have limited coverage for new sentences. Our aim is to extend the phrase that can trigger the predicates. Graph-based semi-supervised learning algorithm has been used to resolve the OOV problem in machine translation (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015; Mehdizadeh Seraj et al., 2015). frame semantic parsing (Das and Smith, 2011), sentiment lexicon induction (Hamilton et al., 2016), and morph-syntactic lexicon induction (Faruqui et al., 2016). 3 Graph-based Lexicon Induction Lexicon learning aims to learn the mapping from natural language phrases to predicates in knowledge base. There are three types of lexicons, including entity lexicon (e.g., “city” :: Type.City), unary lexicon (e.g., “barack obama” :: Barack Obama) and binary lexicon (e.g., “born” :: PlaceOfBirth). In most cases entity lexicons are learned using entity linking techniques, therefore we usually only consider unary and binary lexicons in lexicon learning. In open-domain semantic parsing, it is hard to l"
C18-1076,P12-1092,0,0.0454915,"ee resources to compute similarity for graph construction. First, we use distributional representations for phrases to compute the similarity. Recently, a fair amount of research has showed that the word vector is quite useful for natural language process, especially for tasks related to similarity. Our purpose is to find similar words for the labeled ones, and label 896 neighbor words guilder coin taxa les exchange monetary money scores 3.05 3.03 3.00 2.89 2.85 2.76 2.63 Table 1: Nearest neighbor words for “currency” from PPDB-2.0. them with the same labels. We use the published word vector (Huang et al., 2012) directly, and use cosine distance for similarity. The second resource we used is paraphrase tables. In this part, we want to utilize paraphrase pairs, like “money” and “currency”. We construct these pairs using the Paralex corpus (Fader et al., 2013). Paralex is a large question paraphrases corpus from WikiAnswers,6 and each clique questions were tagged as expressing the same meaning by users. Paraphrase pairs in Paralex are word-aligned using standard machine translation methods. We use the word alignments to construct a word table by applying the consistent word pair heuristic to only unigr"
C18-1076,P16-1002,0,0.0288566,"汇。本文 在两个公开数据集上进行了实验，实验结果表明：本文系统相比 未使用新词汇的基准系统取得了显著提升，相比当前 最好的系统，也取得了具有竞争力 的结果。 1 Introduction Semantic parsing aims to map natural language sentences into formal meaning representations, e.g., Figure 1 shows an example of semantic parsing. Semantic parsing plays an important role in natural language understanding, and has attracted increasing attention in recent years (Zelle and Mooney, 1996; Wong and Mooney, 2007; Lu et al., 2008; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2014; Li et al., 2015; Chen et al., 2016; Xiao et al., 2016; Jia and Liang, 2016; Reddy et al., 2016; Liang et al., 2017). The performance of semantic parsers critically depends on the quality of lexicon, including accuracy and coverage. Specifically, in order to construct the logical form from a sentence, we first need to learn a lexicon,1 which contains the mappings from natural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example in Figure 1, we can see that lexicon is the foundation of parsing, and lexicon learning plays an important role in semantic parsing. Traditional semantic parsers are usually domain-specific, which only c"
C18-1076,D12-1069,0,0.165695,"New Mexico, USA, August 20-26, 2018. Type.cityŁPlaceOfBirth.Barack_Obama Grammar PlaceOfBirth.Barack_Obama Grammar Type.City Lexicon Which city was Barack_Obama PlaceOfBirth Lexicon Lexicon barack obama born in ? Figure 1: An example of semantic parsing, which uses lexicons to map phrases to predicates, and applies grammars to construct the logical form. case, the mappings for each predicate can be learned relatively easily from training corpus. Recently, a growing body of research has scaled up semantic parsers to open domain (Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013) , where the number of predicates has increased substantially, making it hard to learn a lexicon with high coverage. To resolve the lexicon coverage problem, there have been several papers on lexicon learning for semantic parsing. Cai and Yates (2013a) learns lexicons by pattern matching. Berant et al. (2013) learns lexicons by aligning Freebase 2 predicates with relations from ClueWeb 3 , and then the alignments are used as lexicons. However, the lexicon coverage of these alignment-based methods highly depends on entity co-occurrences, and they mostly can only learn"
C18-1076,P14-1112,0,0.0138779,"典资源来 进行词典扩充学习。该词典扩充学习方法首先利用大量文本语料和词典资源来学习词语 与词语之间的相似度，并构建用于图传播的图；接着使用图传播算法从少量标注的词汇 中学习新的词汇。本文 在两个公开数据集上进行了实验，实验结果表明：本文系统相比 未使用新词汇的基准系统取得了显著提升，相比当前 最好的系统，也取得了具有竞争力 的结果。 1 Introduction Semantic parsing aims to map natural language sentences into formal meaning representations, e.g., Figure 1 shows an example of semantic parsing. Semantic parsing plays an important role in natural language understanding, and has attracted increasing attention in recent years (Zelle and Mooney, 1996; Wong and Mooney, 2007; Lu et al., 2008; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2014; Li et al., 2015; Chen et al., 2016; Xiao et al., 2016; Jia and Liang, 2016; Reddy et al., 2016; Liang et al., 2017). The performance of semantic parsers critically depends on the quality of lexicon, including accuracy and coverage. Specifically, in order to construct the logical form from a sentence, we first need to learn a lexicon,1 which contains the mappings from natural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example in Figure 1, we can see that lexicon is the foundation of parsing, and lexicon learning plays an important role in semantic par"
C18-1076,N16-1074,0,0.0747749,"pora and lexical resources. 2. We design a graph-based learning algorithm to learn a wide-coverage lexicon from a seed lexicon. 3. We evaluate our approach on two benchmark datasets. Our system outperforms baseline systems significantly, and achieves competitive results with state-of-the-art systems. 2 Related Work Lexicon learning is fundamental for semantic parsing. Traditional semantic parsers usually utilize annotated logical forms to learn the lexicon (Zettlemoyer and Collins, 2005; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Krishnamurthy, 2016). Zettlemoyer and Collins (2005) utilizes the alignment between phrases in sentences and predicates in 894 annotated logical forms, and then assigned a confidence score to each lexical entry. Obviously, these approaches are limited by the annotated data. Recently, many researchers begin to scale up semantic parsers to open domain. Learning high coverage lexicon in open domain requires large amount of annotated data, which is quite expensive even they only use question-answer pairs for supervision. There are several papers that focus on extending the lexicon for open-domain semantic parsing. Ca"
C18-1076,D10-1119,0,0.23069,"critically depends on the quality of lexicon, including accuracy and coverage. Specifically, in order to construct the logical form from a sentence, we first need to learn a lexicon,1 which contains the mappings from natural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example in Figure 1, we can see that lexicon is the foundation of parsing, and lexicon learning plays an important role in semantic parsing. Traditional semantic parsers are usually domain-specific, which only contains a limited number of logical predicates (Zettlemoyer and Collins, 2005; Kwiatkowksi et al., 2010; Artzi et al., 2014). In this This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 We follow the lexicon style defined in Berant et al. (2013). License details: http:// 892 Proceedings of the 27th International Conference on Computational Linguistics, pages 892–904 Santa Fe, New Mexico, USA, August 20-26, 2018. Type.cityŁPlaceOfBirth.Barack_Obama Grammar PlaceOfBirth.Barack_Obama Grammar Type.City Lexicon Which city was Barack_Obama PlaceOfBirth Lexicon Lexicon barack obama born in ? Figure 1: An example of semantic parsi"
C18-1076,D11-1140,0,0.11162,"临词典覆盖度不足的问题。本文提出了一种 基于图的半监督学习框架，该框架能够充分利用容易获取的 大量文本语料和词典资源来 进行词典扩充学习。该词典扩充学习方法首先利用大量文本语料和词典资源来学习词语 与词语之间的相似度，并构建用于图传播的图；接着使用图传播算法从少量标注的词汇 中学习新的词汇。本文 在两个公开数据集上进行了实验，实验结果表明：本文系统相比 未使用新词汇的基准系统取得了显著提升，相比当前 最好的系统，也取得了具有竞争力 的结果。 1 Introduction Semantic parsing aims to map natural language sentences into formal meaning representations, e.g., Figure 1 shows an example of semantic parsing. Semantic parsing plays an important role in natural language understanding, and has attracted increasing attention in recent years (Zelle and Mooney, 1996; Wong and Mooney, 2007; Lu et al., 2008; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2014; Li et al., 2015; Chen et al., 2016; Xiao et al., 2016; Jia and Liang, 2016; Reddy et al., 2016; Liang et al., 2017). The performance of semantic parsers critically depends on the quality of lexicon, including accuracy and coverage. Specifically, in order to construct the logical form from a sentence, we first need to learn a lexicon,1 which contains the mappings from natural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example in Figure 1, we can see that lexicon is the foundation of parsin"
C18-1076,D13-1161,0,0.110205,"18. Type.cityŁPlaceOfBirth.Barack_Obama Grammar PlaceOfBirth.Barack_Obama Grammar Type.City Lexicon Which city was Barack_Obama PlaceOfBirth Lexicon Lexicon barack obama born in ? Figure 1: An example of semantic parsing, which uses lexicons to map phrases to predicates, and applies grammars to construct the logical form. case, the mappings for each predicate can be learned relatively easily from training corpus. Recently, a growing body of research has scaled up semantic parsers to open domain (Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013) , where the number of predicates has increased substantially, making it hard to learn a lexicon with high coverage. To resolve the lexicon coverage problem, there have been several papers on lexicon learning for semantic parsing. Cai and Yates (2013a) learns lexicons by pattern matching. Berant et al. (2013) learns lexicons by aligning Freebase 2 predicates with relations from ClueWeb 3 , and then the alignments are used as lexicons. However, the lexicon coverage of these alignment-based methods highly depends on entity co-occurrences, and they mostly can only learn predicates which indicatin"
C18-1076,D15-1170,0,0.013796,"词典资源来学习词语 与词语之间的相似度，并构建用于图传播的图；接着使用图传播算法从少量标注的词汇 中学习新的词汇。本文 在两个公开数据集上进行了实验，实验结果表明：本文系统相比 未使用新词汇的基准系统取得了显著提升，相比当前 最好的系统，也取得了具有竞争力 的结果。 1 Introduction Semantic parsing aims to map natural language sentences into formal meaning representations, e.g., Figure 1 shows an example of semantic parsing. Semantic parsing plays an important role in natural language understanding, and has attracted increasing attention in recent years (Zelle and Mooney, 1996; Wong and Mooney, 2007; Lu et al., 2008; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2014; Li et al., 2015; Chen et al., 2016; Xiao et al., 2016; Jia and Liang, 2016; Reddy et al., 2016; Liang et al., 2017). The performance of semantic parsers critically depends on the quality of lexicon, including accuracy and coverage. Specifically, in order to construct the logical form from a sentence, we first need to learn a lexicon,1 which contains the mappings from natural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example in Figure 1, we can see that lexicon is the foundation of parsing, and lexicon learning plays an important role in semantic parsing. Traditional"
C18-1076,P11-1060,0,0.0368475,"子-逻辑表达式对来学习词典， 这通常会面临词典覆盖度不足的问题。本文提出了一种 基于图的半监督学习框架，该框架能够充分利用容易获取的 大量文本语料和词典资源来 进行词典扩充学习。该词典扩充学习方法首先利用大量文本语料和词典资源来学习词语 与词语之间的相似度，并构建用于图传播的图；接着使用图传播算法从少量标注的词汇 中学习新的词汇。本文 在两个公开数据集上进行了实验，实验结果表明：本文系统相比 未使用新词汇的基准系统取得了显著提升，相比当前 最好的系统，也取得了具有竞争力 的结果。 1 Introduction Semantic parsing aims to map natural language sentences into formal meaning representations, e.g., Figure 1 shows an example of semantic parsing. Semantic parsing plays an important role in natural language understanding, and has attracted increasing attention in recent years (Zelle and Mooney, 1996; Wong and Mooney, 2007; Lu et al., 2008; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2014; Li et al., 2015; Chen et al., 2016; Xiao et al., 2016; Jia and Liang, 2016; Reddy et al., 2016; Liang et al., 2017). The performance of semantic parsers critically depends on the quality of lexicon, including accuracy and coverage. Specifically, in order to construct the logical form from a sentence, we first need to learn a lexicon,1 which contains the mappings from natural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example in Figure 1, we can see that lexicon i"
C18-1076,P17-1003,0,0.0485245,"基准系统取得了显著提升，相比当前 最好的系统，也取得了具有竞争力 的结果。 1 Introduction Semantic parsing aims to map natural language sentences into formal meaning representations, e.g., Figure 1 shows an example of semantic parsing. Semantic parsing plays an important role in natural language understanding, and has attracted increasing attention in recent years (Zelle and Mooney, 1996; Wong and Mooney, 2007; Lu et al., 2008; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2014; Li et al., 2015; Chen et al., 2016; Xiao et al., 2016; Jia and Liang, 2016; Reddy et al., 2016; Liang et al., 2017). The performance of semantic parsers critically depends on the quality of lexicon, including accuracy and coverage. Specifically, in order to construct the logical form from a sentence, we first need to learn a lexicon,1 which contains the mappings from natural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example in Figure 1, we can see that lexicon is the foundation of parsing, and lexicon learning plays an important role in semantic parsing. Traditional semantic parsers are usually domain-specific, which only contains a limited number of logical predi"
C18-1076,D08-1082,0,0.143701,"度。传统语义解析器利用标注好的句 子-逻辑表达式对来学习词典， 这通常会面临词典覆盖度不足的问题。本文提出了一种 基于图的半监督学习框架，该框架能够充分利用容易获取的 大量文本语料和词典资源来 进行词典扩充学习。该词典扩充学习方法首先利用大量文本语料和词典资源来学习词语 与词语之间的相似度，并构建用于图传播的图；接着使用图传播算法从少量标注的词汇 中学习新的词汇。本文 在两个公开数据集上进行了实验，实验结果表明：本文系统相比 未使用新词汇的基准系统取得了显著提升，相比当前 最好的系统，也取得了具有竞争力 的结果。 1 Introduction Semantic parsing aims to map natural language sentences into formal meaning representations, e.g., Figure 1 shows an example of semantic parsing. Semantic parsing plays an important role in natural language understanding, and has attracted increasing attention in recent years (Zelle and Mooney, 1996; Wong and Mooney, 2007; Lu et al., 2008; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2014; Li et al., 2015; Chen et al., 2016; Xiao et al., 2016; Jia and Liang, 2016; Reddy et al., 2016; Liang et al., 2017). The performance of semantic parsers critically depends on the quality of lexicon, including accuracy and coverage. Specifically, in order to construct the logical form from a sentence, we first need to learn a lexicon,1 which contains the mappings from natural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example in Figure 1, we ca"
C18-1076,D15-1163,0,0.0209374,"o makes use of text corpora and text resources to get the phrase similarity and phrase co-occurrence. In this way, we can learn more lexicon from little seed lexicon. Krishnamurthy (2016) also learned a lexicon for semantic parsing. However, they aim to extend the predicate side as they think the predicates have limited coverage for new sentences. Our aim is to extend the phrase that can trigger the predicates. Graph-based semi-supervised learning algorithm has been used to resolve the OOV problem in machine translation (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015; Mehdizadeh Seraj et al., 2015). frame semantic parsing (Das and Smith, 2011), sentiment lexicon induction (Hamilton et al., 2016), and morph-syntactic lexicon induction (Faruqui et al., 2016). 3 Graph-based Lexicon Induction Lexicon learning aims to learn the mapping from natural language phrases to predicates in knowledge base. There are three types of lexicons, including entity lexicon (e.g., “city” :: Type.City), unary lexicon (e.g., “barack obama” :: Barack Obama) and binary lexicon (e.g., “born” :: PlaceOfBirth). In most cases entity lexicons are learned using entity linking techniques, therefore we usually only consi"
C18-1076,P15-2070,0,0.0335661,"Missing"
C18-1076,P13-1109,0,0.0273427,"utilizes the alignments between knowledge bases and text corpora, but also makes use of text corpora and text resources to get the phrase similarity and phrase co-occurrence. In this way, we can learn more lexicon from little seed lexicon. Krishnamurthy (2016) also learned a lexicon for semantic parsing. However, they aim to extend the predicate side as they think the predicates have limited coverage for new sentences. Our aim is to extend the phrase that can trigger the predicates. Graph-based semi-supervised learning algorithm has been used to resolve the OOV problem in machine translation (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015; Mehdizadeh Seraj et al., 2015). frame semantic parsing (Das and Smith, 2011), sentiment lexicon induction (Hamilton et al., 2016), and morph-syntactic lexicon induction (Faruqui et al., 2016). 3 Graph-based Lexicon Induction Lexicon learning aims to learn the mapping from natural language phrases to predicates in knowledge base. There are three types of lexicons, including entity lexicon (e.g., “city” :: Type.City), unary lexicon (e.g., “barack obama” :: Barack Obama) and binary lexicon (e.g., “born” :: PlaceOfBirth). In most cases entity lexicons are"
C18-1076,P14-1064,0,0.0230904,"s between knowledge bases and text corpora, but also makes use of text corpora and text resources to get the phrase similarity and phrase co-occurrence. In this way, we can learn more lexicon from little seed lexicon. Krishnamurthy (2016) also learned a lexicon for semantic parsing. However, they aim to extend the predicate side as they think the predicates have limited coverage for new sentences. Our aim is to extend the phrase that can trigger the predicates. Graph-based semi-supervised learning algorithm has been used to resolve the OOV problem in machine translation (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015; Mehdizadeh Seraj et al., 2015). frame semantic parsing (Das and Smith, 2011), sentiment lexicon induction (Hamilton et al., 2016), and morph-syntactic lexicon induction (Faruqui et al., 2016). 3 Graph-based Lexicon Induction Lexicon learning aims to learn the mapping from natural language phrases to predicates in knowledge base. There are three types of lexicons, including entity lexicon (e.g., “city” :: Type.City), unary lexicon (e.g., “barack obama” :: Barack Obama) and binary lexicon (e.g., “born” :: PlaceOfBirth). In most cases entity lexicons are learned using entity"
C18-1076,P07-1121,0,0.174138,"语义解析器的性能往往依赖于词典的准确度和覆盖度。传统语义解析器利用标注好的句 子-逻辑表达式对来学习词典， 这通常会面临词典覆盖度不足的问题。本文提出了一种 基于图的半监督学习框架，该框架能够充分利用容易获取的 大量文本语料和词典资源来 进行词典扩充学习。该词典扩充学习方法首先利用大量文本语料和词典资源来学习词语 与词语之间的相似度，并构建用于图传播的图；接着使用图传播算法从少量标注的词汇 中学习新的词汇。本文 在两个公开数据集上进行了实验，实验结果表明：本文系统相比 未使用新词汇的基准系统取得了显著提升，相比当前 最好的系统，也取得了具有竞争力 的结果。 1 Introduction Semantic parsing aims to map natural language sentences into formal meaning representations, e.g., Figure 1 shows an example of semantic parsing. Semantic parsing plays an important role in natural language understanding, and has attracted increasing attention in recent years (Zelle and Mooney, 1996; Wong and Mooney, 2007; Lu et al., 2008; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2014; Li et al., 2015; Chen et al., 2016; Xiao et al., 2016; Jia and Liang, 2016; Reddy et al., 2016; Liang et al., 2017). The performance of semantic parsers critically depends on the quality of lexicon, including accuracy and coverage. Specifically, in order to construct the logical form from a sentence, we first need to learn a lexicon,1 which contains the mappings from natural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example i"
C18-1076,P16-1127,0,0.0796144,"传播算法从少量标注的词汇 中学习新的词汇。本文 在两个公开数据集上进行了实验，实验结果表明：本文系统相比 未使用新词汇的基准系统取得了显著提升，相比当前 最好的系统，也取得了具有竞争力 的结果。 1 Introduction Semantic parsing aims to map natural language sentences into formal meaning representations, e.g., Figure 1 shows an example of semantic parsing. Semantic parsing plays an important role in natural language understanding, and has attracted increasing attention in recent years (Zelle and Mooney, 1996; Wong and Mooney, 2007; Lu et al., 2008; Liang et al., 2011; Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2014; Li et al., 2015; Chen et al., 2016; Xiao et al., 2016; Jia and Liang, 2016; Reddy et al., 2016; Liang et al., 2017). The performance of semantic parsers critically depends on the quality of lexicon, including accuracy and coverage. Specifically, in order to construct the logical form from a sentence, we first need to learn a lexicon,1 which contains the mappings from natural language phrases (e.g., “born”) to logical predicates (e.g., PlaceOfBirth). From the example in Figure 1, we can see that lexicon is the foundation of parsing, and lexicon learning plays an important role in semantic parsing. Traditional semantic parsers are usually domain-s"
C18-1076,D14-1071,0,0.0146983,"β = 2. Comparing systems: To evaluate our method, we mainly compare our system (Base + lexicon) to the base system (Base) which does not use the learned wide-coverage lexicon, also to system (Base + bridge) which utilize bridge operator to serve as lexicon (Berant et al., 2013). We also compare to several nearly published systems, including semantic parsing based system (Kwiatkowski et al., 2013; Berant and Liang, 2015), information extraction based systems (Yao and Van Durme, 2014; Yao, 2015), machine translation based systems (Bao et al., 2014), embedding based systems (Bordes et al., 2014; Yang et al., 2014), and QA based system (Bast and Haussmann, 2015). 5.1 Experimental Results Table 2 and Table 3 provide the performances of all baselines9 and our method in W EBQUESTIONS and F REE 917. From Table 2 and Table 3, we can see that: 1. Our method achieves competitive performance: Our system outperforms base system (Base) greatly and gets a better performance when comparing to the base system with a bridge operator (Base + bridge). 2. The learned lexicon has wider coverage than the seed one: Our system obtains higher recall than the Base. By utilizing large amount of text corpora and lexical resourc"
C18-1076,P14-1090,0,0.0359137,"Missing"
C18-1076,N15-3014,0,0.0142916,"ty computation, we set α = 0.05, β = 0.85. For the parameters in Junto, we set µ1 = 0.55, µ2 = 0.44, µ3 = 0.01, β = 2. Comparing systems: To evaluate our method, we mainly compare our system (Base + lexicon) to the base system (Base) which does not use the learned wide-coverage lexicon, also to system (Base + bridge) which utilize bridge operator to serve as lexicon (Berant et al., 2013). We also compare to several nearly published systems, including semantic parsing based system (Kwiatkowski et al., 2013; Berant and Liang, 2015), information extraction based systems (Yao and Van Durme, 2014; Yao, 2015), machine translation based systems (Bao et al., 2014), embedding based systems (Bordes et al., 2014; Yang et al., 2014), and QA based system (Bast and Haussmann, 2015). 5.1 Experimental Results Table 2 and Table 3 provide the performances of all baselines9 and our method in W EBQUESTIONS and F REE 917. From Table 2 and Table 3, we can see that: 1. Our method achieves competitive performance: Our system outperforms base system (Base) greatly and gets a better performance when comparing to the base system with a bridge operator (Base + bridge). 2. The learned lexicon has wider coverage than the"
C18-1076,P15-1128,0,0.129877,"Missing"
C18-1076,N15-1176,0,0.0236102,"ases and text corpora, but also makes use of text corpora and text resources to get the phrase similarity and phrase co-occurrence. In this way, we can learn more lexicon from little seed lexicon. Krishnamurthy (2016) also learned a lexicon for semantic parsing. However, they aim to extend the predicate side as they think the predicates have limited coverage for new sentences. Our aim is to extend the phrase that can trigger the predicates. Graph-based semi-supervised learning algorithm has been used to resolve the OOV problem in machine translation (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015; Mehdizadeh Seraj et al., 2015). frame semantic parsing (Das and Smith, 2011), sentiment lexicon induction (Hamilton et al., 2016), and morph-syntactic lexicon induction (Faruqui et al., 2016). 3 Graph-based Lexicon Induction Lexicon learning aims to learn the mapping from natural language phrases to predicates in knowledge base. There are three types of lexicons, including entity lexicon (e.g., “city” :: Type.City), unary lexicon (e.g., “barack obama” :: Barack Obama) and binary lexicon (e.g., “born” :: PlaceOfBirth). In most cases entity lexicons are learned using entity linking techniques,"
C18-1240,D15-1177,0,0.0887875,"nd Weston, 2008), word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). The main drawback of these methods is that they represent a word using a single vector, such a uniform representation method cannot accurately represent polysemy. There are also various methods which try to learn multi-prototype word embeddings to solve the polysemy problem, with each vector corresponding to a sense instead of word, such as the Multi-Prototype model (Huang et al., 2012), the MSSG (Neelakantan et al., 2015), the EHModel (Tian et al., 2014), the SenseEmb (Guo et al., 2014) and the SAMS model (Cheng and Kartsaklis, 2015). The main drawback of these models is that they need to induce the senses of a word, which is a very challenging task. Salant and Berant (2017) verified the importance of the contextual information for word representation. Recently, a number of methods are proposed to learn contextual word representations based on neural network (Melamud et al., 2016; Peters et al., 2018) without considering word composition. (Liu et al., 2015) proposed a TWE model which employs the topic assignments to produce topicspecific word embeddings to avoid the challenging task of word sense induction, and generates"
C18-1240,P14-1129,0,0.0723187,"Missing"
C18-1240,C04-1051,0,0.0767564,"Missing"
C18-1240,P17-2070,0,0.0695995,"g task. Salant and Berant (2017) verified the importance of the contextual information for word representation. Recently, a number of methods are proposed to learn contextual word representations based on neural network (Melamud et al., 2016; Peters et al., 2018) without considering word composition. (Liu et al., 2015) proposed a TWE model which employs the topic assignments to produce topicspecific word embeddings to avoid the challenging task of word sense induction, and generates the representation of a larger unit by adding the vectors of topical word embeddings weighted by TFIDF of them. Fadaee et al. (2017) labels a word with topic distribution in both hard and soft ways, and learns topic-specific representations and general representation for a word simultaneously. Shi et al. (2017) introduces a method to enhance the word representation and topic model with each other. By contrast, our proposed model aims at utilizing topic distribution to enhance the word composition models for learning representations of linguistic units of different granularity levels, including word, phrase and sentence. 2.3 Word Composition Due to the context sparsity problem, word composition is a promising technique to l"
C18-1240,P14-1101,0,0.01669,"ee framework that can enhance various kinds of methods. 2 Related Work This section briefly reviews related work, including context representation models, word/phrase embeddings leanings methods and word composition methods. 2.1 Context Representation Model How to represent contextual information is a key topic in natural language processing research area. Currently, a main trend is to represent context via latent variable models (Szpektor and Dagan, 2008), e.g., the Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Using LDA models, a context will be represented as a topic distribution. Frank et al. (2014) used topic models to provide context for a vowel categorisation task in child directed speech. 2835 2.2 Word/Phrase Embeddings Currently, most of the word embeddings methods use either neural network or co-occurrence matrix. Several well-known models include C&W (Collobert and Weston, 2008), word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). The main drawback of these methods is that they represent a word using a single vector, such a uniform representation method cannot accurately represent polysemy. There are also various methods which try to learn multi-prototype word emb"
C18-1240,C14-1048,0,0.027799,"-known models include C&W (Collobert and Weston, 2008), word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). The main drawback of these methods is that they represent a word using a single vector, such a uniform representation method cannot accurately represent polysemy. There are also various methods which try to learn multi-prototype word embeddings to solve the polysemy problem, with each vector corresponding to a sense instead of word, such as the Multi-Prototype model (Huang et al., 2012), the MSSG (Neelakantan et al., 2015), the EHModel (Tian et al., 2014), the SenseEmb (Guo et al., 2014) and the SAMS model (Cheng and Kartsaklis, 2015). The main drawback of these models is that they need to induce the senses of a word, which is a very challenging task. Salant and Berant (2017) verified the importance of the contextual information for word representation. Recently, a number of methods are proposed to learn contextual word representations based on neural network (Melamud et al., 2016; Peters et al., 2018) without considering word composition. (Liu et al., 2015) proposed a TWE model which employs the topic assignments to produce topicspecific word embeddings to avoid the challeng"
C18-1240,D14-1163,0,0.019945,"larity experiments on the ML2010 dataset (Mitchell and Lapata, 2010), which contains 108 pairs of phrases for adjective-noun (AN), verb-object (VO) and compound-noun (NN) respectively. In our experiments, we employ the element-wise addition as base composition model, which turns out to be both robust and effective in many tasks. We compare our system with two baselines: the elementwise addition models correspondingly using the context-unaware word embeddings learned by CBOW and SkipGram. We also compare our system with state-of-the-art results from ML Original (Mitchell and Lapata, 2010), PAS(Hashimoto et al., 2014), PARAGRAM (Wieting et al., 2015b), DeepCCA (Lu et al., 2015) and vecDCS (Tian et al., 2016). The results of the models are taken from the original papers. The overall results are shown in Table 2. AN NN VO ALL ML Original 0.46 0.37 0.45 0.44 PAS 0.46 0.49 0.45 0.47 PARAGRAM 0.50 0.51 0.40 0.47 DeepCCA 0.45 0.45 0.47 0.46 vecDCS 0.41 0.51 0.49 0.47 SkipGram 0.47 0.51 0.39 0.41 CBOW 0.45 0.49 0.40 0.43 Our 0.60 0.54 0.47 0.54 Table 2: The Spearman’s correlation ρ of different methods on ML2010 datasets. From Table 2, we can see that: (1) By incorporating the topic distributions of phrases, our"
C18-1240,P12-1092,0,0.615498,"roposed model attempts to resolve the word sense disambiguation and word composition in a unified framework. Extensive evaluation shows consistent improvements over various strong word representation/composition models at different granularities (including word, phrase and sentence), demonstrating the effectiveness of our proposed method. 1 Introduction Recent development in natural language processing (NLP) has seen a rise of continuous representation learning of linguistic units, which has been successfully applied to various tasks such as contextual word similarity (Iacobacci et al., 2015; Huang et al., 2012), machine translation (Devlin et al., 2014), paraphrase detection (Socher et al., 2011a) and sentiment classification (Tang et al., 2015). The continuous representation of linguistic units bring many benefits. For example, we can easily calculate the semantic relatedness between two words ‘dog’ and ‘cat’ using cosine distance of their continuous representations. Specifically, a representation learning method aims at mapping a linguistic unit with k words S = [w1 , w2 , ..., wk ] into a continuous vector in a low dimensional feature space. Currently, most of the representation learning methods"
C18-1240,P15-1010,0,0.0227061,"g representations. The proposed model attempts to resolve the word sense disambiguation and word composition in a unified framework. Extensive evaluation shows consistent improvements over various strong word representation/composition models at different granularities (including word, phrase and sentence), demonstrating the effectiveness of our proposed method. 1 Introduction Recent development in natural language processing (NLP) has seen a rise of continuous representation learning of linguistic units, which has been successfully applied to various tasks such as contextual word similarity (Iacobacci et al., 2015; Huang et al., 2012), machine translation (Devlin et al., 2014), paraphrase detection (Socher et al., 2011a) and sentiment classification (Tang et al., 2015). The continuous representation of linguistic units bring many benefits. For example, we can easily calculate the semantic relatedness between two words ‘dog’ and ‘cat’ using cosine distance of their continuous representations. Specifically, a representation learning method aims at mapping a linguistic unit with k words S = [w1 , w2 , ..., wk ] into a continuous vector in a low dimensional feature space. Currently, most of the representat"
C18-1240,S15-1002,0,0.324908,"tion method first infers the vectors of all words in S based on embedding matrix. After that, a composition model is utilized to transform the word vectors into a single representation of unit S. For example, to represent the phrase ‘the dog outside’, a composition method will first infer the representations of ‘the’, ‘dog’ and ‘outside’, and then feed those word vectors into a composition model, e.g., element-wise addition, element-wise multiplication (Mikolov et al., 2013), recursive neural network (Socher et al., 2012), gated recurrent neural network, Long-Short Term Memory network (Le and Zuidema, 2015) or convolutional neural network (Xu et al., 2015). There are also approaches that jointly learn the representations of words and larger units, e.g Le and Mikolov (2014). However, the meaning of a large linguistic unit not only depends on its constituent words but also affected by the context around it. For example, the meaning of word ‘going’ in sentence ‘I am going to the bank.’ can be disambiguated based on the local context (the other words in the sentence). But the meaning of word ‘bank’ in this sentence can not be inferred without context outside this sentence. Unfortunately, deriving co"
C18-1240,D15-1161,0,0.0232772,"e and sentence. 2.3 Word Composition Due to the context sparsity problem, word composition is a promising technique to learn representations of linguistic units larger than words. A lot of models have been developed for word composition, e.g., simple models such as element-wise addition/multiplication (Mitchell and Lapata, 2010; Mikolov et al., 2013) and neural network based models such as recurrent neural network (Paulus et al., 2014), gated recurrent neural network, Long-Short Term Memory network (Le and Zuidema, 2015), convolution neural network (Xu et al., 2015) and attention-based model (Ling et al., 2015). These composition models could be used as the base composition model in our model-free framework. Skip-Thought (Kiros et al., 2015) learns word and sentence representations based on sentences around it simultaneously. Doc2vec (Le and Mikolov, 2014) jointly trains the word sentence vectors by predicting the words in it. Recently, (Mao et al., 2017) introduced a topic-aware model to enhance the word composition model based on topic embeddings, which verifies the benefit of topic information for word composition. But despite its apparent success, there remains a major drawback: this method suff"
C18-1240,N15-1028,0,0.0270432,", which contains 108 pairs of phrases for adjective-noun (AN), verb-object (VO) and compound-noun (NN) respectively. In our experiments, we employ the element-wise addition as base composition model, which turns out to be both robust and effective in many tasks. We compare our system with two baselines: the elementwise addition models correspondingly using the context-unaware word embeddings learned by CBOW and SkipGram. We also compare our system with state-of-the-art results from ML Original (Mitchell and Lapata, 2010), PAS(Hashimoto et al., 2014), PARAGRAM (Wieting et al., 2015b), DeepCCA (Lu et al., 2015) and vecDCS (Tian et al., 2016). The results of the models are taken from the original papers. The overall results are shown in Table 2. AN NN VO ALL ML Original 0.46 0.37 0.45 0.44 PAS 0.46 0.49 0.45 0.47 PARAGRAM 0.50 0.51 0.40 0.47 DeepCCA 0.45 0.45 0.47 0.46 vecDCS 0.41 0.51 0.49 0.47 SkipGram 0.47 0.51 0.39 0.41 CBOW 0.45 0.49 0.40 0.43 Our 0.60 0.54 0.47 0.54 Table 2: The Spearman’s correlation ρ of different methods on ML2010 datasets. From Table 2, we can see that: (1) By incorporating the topic distributions of phrases, our contextaware word composition model achieves the best results"
C18-1240,K16-1006,0,0.0640923,"m, with each vector corresponding to a sense instead of word, such as the Multi-Prototype model (Huang et al., 2012), the MSSG (Neelakantan et al., 2015), the EHModel (Tian et al., 2014), the SenseEmb (Guo et al., 2014) and the SAMS model (Cheng and Kartsaklis, 2015). The main drawback of these models is that they need to induce the senses of a word, which is a very challenging task. Salant and Berant (2017) verified the importance of the contextual information for word representation. Recently, a number of methods are proposed to learn contextual word representations based on neural network (Melamud et al., 2016; Peters et al., 2018) without considering word composition. (Liu et al., 2015) proposed a TWE model which employs the topic assignments to produce topicspecific word embeddings to avoid the challenging task of word sense induction, and generates the representation of a larger unit by adding the vectors of topical word embeddings weighted by TFIDF of them. Fadaee et al. (2017) labels a word with topic distribution in both hard and soft ways, and learns topic-specific representations and general representation for a word simultaneously. Shi et al. (2017) introduces a method to enhance the word"
C18-1240,D14-1162,0,0.112373,"a. Currently, a main trend is to represent context via latent variable models (Szpektor and Dagan, 2008), e.g., the Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Using LDA models, a context will be represented as a topic distribution. Frank et al. (2014) used topic models to provide context for a vowel categorisation task in child directed speech. 2835 2.2 Word/Phrase Embeddings Currently, most of the word embeddings methods use either neural network or co-occurrence matrix. Several well-known models include C&W (Collobert and Weston, 2008), word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). The main drawback of these methods is that they represent a word using a single vector, such a uniform representation method cannot accurately represent polysemy. There are also various methods which try to learn multi-prototype word embeddings to solve the polysemy problem, with each vector corresponding to a sense instead of word, such as the Multi-Prototype model (Huang et al., 2012), the MSSG (Neelakantan et al., 2015), the EHModel (Tian et al., 2014), the SenseEmb (Guo et al., 2014) and the SAMS model (Cheng and Kartsaklis, 2015). The main drawback of these models is that they need to i"
C18-1240,N18-1202,0,0.256406,"rresponding to a sense instead of word, such as the Multi-Prototype model (Huang et al., 2012), the MSSG (Neelakantan et al., 2015), the EHModel (Tian et al., 2014), the SenseEmb (Guo et al., 2014) and the SAMS model (Cheng and Kartsaklis, 2015). The main drawback of these models is that they need to induce the senses of a word, which is a very challenging task. Salant and Berant (2017) verified the importance of the contextual information for word representation. Recently, a number of methods are proposed to learn contextual word representations based on neural network (Melamud et al., 2016; Peters et al., 2018) without considering word composition. (Liu et al., 2015) proposed a TWE model which employs the topic assignments to produce topicspecific word embeddings to avoid the challenging task of word sense induction, and generates the representation of a larger unit by adding the vectors of topical word embeddings weighted by TFIDF of them. Fadaee et al. (2017) labels a word with topic distribution in both hard and soft ways, and learns topic-specific representations and general representation for a word simultaneously. Shi et al. (2017) introduces a method to enhance the word representation and top"
C18-1240,D11-1014,0,0.352807,"n a unified framework. Extensive evaluation shows consistent improvements over various strong word representation/composition models at different granularities (including word, phrase and sentence), demonstrating the effectiveness of our proposed method. 1 Introduction Recent development in natural language processing (NLP) has seen a rise of continuous representation learning of linguistic units, which has been successfully applied to various tasks such as contextual word similarity (Iacobacci et al., 2015; Huang et al., 2012), machine translation (Devlin et al., 2014), paraphrase detection (Socher et al., 2011a) and sentiment classification (Tang et al., 2015). The continuous representation of linguistic units bring many benefits. For example, we can easily calculate the semantic relatedness between two words ‘dog’ and ‘cat’ using cosine distance of their continuous representations. Specifically, a representation learning method aims at mapping a linguistic unit with k words S = [w1 , w2 , ..., wk ] into a continuous vector in a low dimensional feature space. Currently, most of the representation learning methods focus on learning word embeddings, mostly based on the distributional hypothesis (Harr"
C18-1240,D12-1110,0,0.0612213,"bine them. Specifically, given a linguistic unit S with [w1 , w2 , ..., wk ] words, a composition method first infers the vectors of all words in S based on embedding matrix. After that, a composition model is utilized to transform the word vectors into a single representation of unit S. For example, to represent the phrase ‘the dog outside’, a composition method will first infer the representations of ‘the’, ‘dog’ and ‘outside’, and then feed those word vectors into a composition model, e.g., element-wise addition, element-wise multiplication (Mikolov et al., 2013), recursive neural network (Socher et al., 2012), gated recurrent neural network, Long-Short Term Memory network (Le and Zuidema, 2015) or convolutional neural network (Xu et al., 2015). There are also approaches that jointly learn the representations of words and larger units, e.g Le and Mikolov (2014). However, the meaning of a large linguistic unit not only depends on its constituent words but also affected by the context around it. For example, the meaning of word ‘going’ in sentence ‘I am going to the bank.’ can be disambiguated based on the local context (the other words in the sentence). But the meaning of word ‘bank’ in this sentenc"
C18-1240,C08-1107,0,0.0140513,"ences in it; (2) This paper verifies that the topic information is benefit for both the accurate word representation and word composition. (3) We proposed a model-free framework that can enhance various kinds of methods. 2 Related Work This section briefly reviews related work, including context representation models, word/phrase embeddings leanings methods and word composition methods. 2.1 Context Representation Model How to represent contextual information is a key topic in natural language processing research area. Currently, a main trend is to represent context via latent variable models (Szpektor and Dagan, 2008), e.g., the Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Using LDA models, a context will be represented as a topic distribution. Frank et al. (2014) used topic models to provide context for a vowel categorisation task in child directed speech. 2835 2.2 Word/Phrase Embeddings Currently, most of the word embeddings methods use either neural network or co-occurrence matrix. Several well-known models include C&W (Collobert and Weston, 2008), word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). The main drawback of these methods is that they represent a word using a singl"
C18-1240,D15-1167,0,0.29463,"nsistent improvements over various strong word representation/composition models at different granularities (including word, phrase and sentence), demonstrating the effectiveness of our proposed method. 1 Introduction Recent development in natural language processing (NLP) has seen a rise of continuous representation learning of linguistic units, which has been successfully applied to various tasks such as contextual word similarity (Iacobacci et al., 2015; Huang et al., 2012), machine translation (Devlin et al., 2014), paraphrase detection (Socher et al., 2011a) and sentiment classification (Tang et al., 2015). The continuous representation of linguistic units bring many benefits. For example, we can easily calculate the semantic relatedness between two words ‘dog’ and ‘cat’ using cosine distance of their continuous representations. Specifically, a representation learning method aims at mapping a linguistic unit with k words S = [w1 , w2 , ..., wk ] into a continuous vector in a low dimensional feature space. Currently, most of the representation learning methods focus on learning word embeddings, mostly based on the distributional hypothesis (Harris, 1954), i.e., words in similar contexts tend to"
C18-1240,C14-1016,0,0.174222,"co-occurrence matrix. Several well-known models include C&W (Collobert and Weston, 2008), word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). The main drawback of these methods is that they represent a word using a single vector, such a uniform representation method cannot accurately represent polysemy. There are also various methods which try to learn multi-prototype word embeddings to solve the polysemy problem, with each vector corresponding to a sense instead of word, such as the Multi-Prototype model (Huang et al., 2012), the MSSG (Neelakantan et al., 2015), the EHModel (Tian et al., 2014), the SenseEmb (Guo et al., 2014) and the SAMS model (Cheng and Kartsaklis, 2015). The main drawback of these models is that they need to induce the senses of a word, which is a very challenging task. Salant and Berant (2017) verified the importance of the contextual information for word representation. Recently, a number of methods are proposed to learn contextual word representations based on neural network (Melamud et al., 2016; Peters et al., 2018) without considering word composition. (Liu et al., 2015) proposed a TWE model which employs the topic assignments to produce topicspecific word"
C18-1240,P16-1121,0,0.0226788,"phrases for adjective-noun (AN), verb-object (VO) and compound-noun (NN) respectively. In our experiments, we employ the element-wise addition as base composition model, which turns out to be both robust and effective in many tasks. We compare our system with two baselines: the elementwise addition models correspondingly using the context-unaware word embeddings learned by CBOW and SkipGram. We also compare our system with state-of-the-art results from ML Original (Mitchell and Lapata, 2010), PAS(Hashimoto et al., 2014), PARAGRAM (Wieting et al., 2015b), DeepCCA (Lu et al., 2015) and vecDCS (Tian et al., 2016). The results of the models are taken from the original papers. The overall results are shown in Table 2. AN NN VO ALL ML Original 0.46 0.37 0.45 0.44 PAS 0.46 0.49 0.45 0.47 PARAGRAM 0.50 0.51 0.40 0.47 DeepCCA 0.45 0.45 0.47 0.46 vecDCS 0.41 0.51 0.49 0.47 SkipGram 0.47 0.51 0.39 0.41 CBOW 0.45 0.49 0.40 0.43 Our 0.60 0.54 0.47 0.54 Table 2: The Spearman’s correlation ρ of different methods on ML2010 datasets. From Table 2, we can see that: (1) By incorporating the topic distributions of phrases, our contextaware word composition model achieves the best results on all three types of phrases:"
C18-1240,Q15-1025,0,0.0221299,"taset (Mitchell and Lapata, 2010), which contains 108 pairs of phrases for adjective-noun (AN), verb-object (VO) and compound-noun (NN) respectively. In our experiments, we employ the element-wise addition as base composition model, which turns out to be both robust and effective in many tasks. We compare our system with two baselines: the elementwise addition models correspondingly using the context-unaware word embeddings learned by CBOW and SkipGram. We also compare our system with state-of-the-art results from ML Original (Mitchell and Lapata, 2010), PAS(Hashimoto et al., 2014), PARAGRAM (Wieting et al., 2015b), DeepCCA (Lu et al., 2015) and vecDCS (Tian et al., 2016). The results of the models are taken from the original papers. The overall results are shown in Table 2. AN NN VO ALL ML Original 0.46 0.37 0.45 0.44 PAS 0.46 0.49 0.45 0.47 PARAGRAM 0.50 0.51 0.40 0.47 DeepCCA 0.45 0.45 0.47 0.46 vecDCS 0.41 0.51 0.49 0.47 SkipGram 0.47 0.51 0.39 0.41 CBOW 0.45 0.49 0.40 0.43 Our 0.60 0.54 0.47 0.54 Table 2: The Spearman’s correlation ρ of different methods on ML2010 datasets. From Table 2, we can see that: (1) By incorporating the topic distributions of phrases, our contextaware word composition mo"
C18-1240,W15-1509,0,0.0986715,"s in S based on embedding matrix. After that, a composition model is utilized to transform the word vectors into a single representation of unit S. For example, to represent the phrase ‘the dog outside’, a composition method will first infer the representations of ‘the’, ‘dog’ and ‘outside’, and then feed those word vectors into a composition model, e.g., element-wise addition, element-wise multiplication (Mikolov et al., 2013), recursive neural network (Socher et al., 2012), gated recurrent neural network, Long-Short Term Memory network (Le and Zuidema, 2015) or convolutional neural network (Xu et al., 2015). There are also approaches that jointly learn the representations of words and larger units, e.g Le and Mikolov (2014). However, the meaning of a large linguistic unit not only depends on its constituent words but also affected by the context around it. For example, the meaning of word ‘going’ in sentence ‘I am going to the bank.’ can be disambiguated based on the local context (the other words in the sentence). But the meaning of word ‘bank’ in this sentence can not be inferred without context outside this sentence. Unfortunately, deriving contextual information outside of a given sentence i"
D12-1010,E06-1002,0,0.147019,"on this problem, rather than other methods such as the EM-Model, which affects the overall performance of our method. 0.88 0.86 0.84 0.82 0.8 0.78 0.76 0.74 0.72 Figure 5. The EL accuracies on TAC 2009 dataset 5 Related Work In this section, we briefly review the related work of EL. Traditionally, the context compatibility based methods link a mention to the entity which has the largest compatibility with it. Cucerzan (2007) modeled the compatibility as the cosine similarity between the vector space representation of mention’s context and of entity’s Wikipedia entry. Mihalcea & Csomai (2007), Bunescu & Pasca (2006), Fader et al. (2009), Gottipati et al.(2011) and Zhang et al.(2011) extended the vector space model with more information such as the entity category and the acronym expansion, etc. Han & Sun (2011) proposed a generative model which computes the compatibility using the evidences from entity’s popularity, name distribution and context word distribution. Kataria et al.(2011) and Sen (2012) used a latent topic model to learn the context model of entities. Zheng et al. (2010), Dredze et al. (2010), Zhang et al. (2010), Zhou et al. (2010) and Ji & Chen(2011) employed the ranking techniques to furt"
D12-1010,J93-2003,0,0.0194747,"Missing"
D12-1010,D07-1074,0,0.819155,"Missing"
D12-1010,C10-1032,0,0.130188,"ace representation of mention’s context and of entity’s Wikipedia entry. Mihalcea & Csomai (2007), Bunescu & Pasca (2006), Fader et al. (2009), Gottipati et al.(2011) and Zhang et al.(2011) extended the vector space model with more information such as the entity category and the acronym expansion, etc. Han & Sun (2011) proposed a generative model which computes the compatibility using the evidences from entity’s popularity, name distribution and context word distribution. Kataria et al.(2011) and Sen (2012) used a latent topic model to learn the context model of entities. Zheng et al. (2010), Dredze et al. (2010), Zhang et al. (2010), Zhou et al. (2010) and Ji & Chen(2011) employed the ranking techniques to further take relations between candidate entities into account. On the other side, the topic coherence based methods link a mention to the entity which are most coherent to the document containing it. Medelyan et al. (2008) measured the topic coherence of an entity to a document as the weighted average of its relatedness to the unambiguous entities in the document. Milne and Witten (2008) extended Medelyan et al. (2008)’s coherence by incorporating commonness and context quality. Bhattacharya and G"
D12-1010,D11-1074,0,0.162754,"Missing"
D12-1010,P11-1095,1,0.588682,"em: one focusing on the effects of mention’s context compatibility and the other dealing with the effects of document’s topic coherence. EL methods based on context 2 www.wikipedia.org www.freebase.com 105 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 105–115, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics compatibility assume that “the referent entity of a mention is reflected by its context”(Mihalcea & Cosomai, 2007; Zhang et al., 2010; Zheng et al., 2010; Han & Sun, 2011; Kataria et al., 2011; Sen 2012). For example, the context compatibility based methods will identify the referent entity of the mention Lion in Figure 1 is the entity Mac OS X Lion, since this entity is more compatible with its context words operating system and release than other candidates such as Lion(big cats) or Lion(band). EL methods based on topic coherence assume that “a mention’s referent entity should be coherent with document’s main topics” (Medelyan et al., 2008; Milne & Witten, 2008; Kulkarni et al., 2009; Han et al., 2011). For example, the topic coherence based methods will lin"
D12-1010,D11-1072,0,0.157282,"Missing"
D12-1010,P11-1138,0,0.1135,"Missing"
D12-1010,C10-1145,0,0.0298092,"ector space representation of mention’s context and of entity’s Wikipedia entry. Mihalcea & Csomai (2007), Bunescu & Pasca (2006), Fader et al. (2009), Gottipati et al.(2011) and Zhang et al.(2011) extended the vector space model with more information such as the entity category and the acronym expansion, etc. Han & Sun (2011) proposed a generative model which computes the compatibility using the evidences from entity’s popularity, name distribution and context word distribution. Kataria et al.(2011) and Sen (2012) used a latent topic model to learn the context model of entities. Zheng et al. (2010), Dredze et al. (2010), Zhang et al. (2010), Zhou et al. (2010) and Ji & Chen(2011) employed the ranking techniques to further take relations between candidate entities into account. On the other side, the topic coherence based methods link a mention to the entity which are most coherent to the document containing it. Medelyan et al. (2008) measured the topic coherence of an entity to a document as the weighted average of its relatedness to the unambiguous entities in the document. Milne and Witten (2008) extended Medelyan et al. (2008)’s coherence by incorporating commonness and context quali"
D12-1010,N10-1072,0,0.165706,"name ambiguity problem: one focusing on the effects of mention’s context compatibility and the other dealing with the effects of document’s topic coherence. EL methods based on context 2 www.wikipedia.org www.freebase.com 105 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 105–115, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics compatibility assume that “the referent entity of a mention is reflected by its context”(Mihalcea & Cosomai, 2007; Zhang et al., 2010; Zheng et al., 2010; Han & Sun, 2011; Kataria et al., 2011; Sen 2012). For example, the context compatibility based methods will identify the referent entity of the mention Lion in Figure 1 is the entity Mac OS X Lion, since this entity is more compatible with its context words operating system and release than other candidates such as Lion(big cats) or Lion(band). EL methods based on topic coherence assume that “a mention’s referent entity should be coherent with document’s main topics” (Medelyan et al., 2008; Milne & Witten, 2008; Kulkarni et al., 2009; Han et al., 2011). For example, the topic coherence based"
D12-1010,C10-1150,0,0.0404059,"of entity’s Wikipedia entry. Mihalcea & Csomai (2007), Bunescu & Pasca (2006), Fader et al. (2009), Gottipati et al.(2011) and Zhang et al.(2011) extended the vector space model with more information such as the entity category and the acronym expansion, etc. Han & Sun (2011) proposed a generative model which computes the compatibility using the evidences from entity’s popularity, name distribution and context word distribution. Kataria et al.(2011) and Sen (2012) used a latent topic model to learn the context model of entities. Zheng et al. (2010), Dredze et al. (2010), Zhang et al. (2010), Zhou et al. (2010) and Ji & Chen(2011) employed the ranking techniques to further take relations between candidate entities into account. On the other side, the topic coherence based methods link a mention to the entity which are most coherent to the document containing it. Medelyan et al. (2008) measured the topic coherence of an entity to a document as the weighted average of its relatedness to the unambiguous entities in the document. Milne and Witten (2008) extended Medelyan et al. (2008)’s coherence by incorporating commonness and context quality. Bhattacharya and Getoor (2006) modeled the topic coherence"
D12-1010,D11-1071,0,0.0564785,"Missing"
D17-1216,N16-1067,0,0.0116146,"tures of event knowledge (Cheung et al., 2013; Chambers, 2013; Bamman et al., 2014; Nguyen et al., 2015). Another direction for learning event-centred knowledge is causality identification (Do et al., 2011; Radinsky et al., 2012; Berant et al., 2014; Hashimoto et al., 2015; Gui et al., 2016), which tried to identify the causality relation in text. For reasoning over these knowledge, Jans et al. (2012) extend introduced skip-grams for collecting statistics. Further improvements include incorporating more information and more complicated models (Radinsky and Horvitz, 2013; Modi and Titov, 2014; Ahrendt and Demberg, 2016). Recent researches tried to solve event prediction problem by transforming it into an language modeling paradigm (Pichotta and Mooney, 2014, 2015, 2016a,b; Rudinger et al., 2015; Hu et al., 2017). The principal difference between previous work and our method is that we not only take various kinds of implicit commonsense knowledge into consideration, but also provide a highly 2039 extensible framework to exploit these kinds of knowledge for commonsense machine comprehension. We also notice the recent progress in RocStories (Mostafazadeh et al., 2017). Rather than inferring a possible ending ge"
D17-1216,P16-1042,0,0.0270677,"focusing on semantic matching between texts (Weston et al., 2014; Kumar et al., 2015; Narasimhan and Barzilay, 2015; Smith et al., 2015; Sukhbaatar et al., 2015; Hill et al., 2015; Wang et al., 2015, 2016a; Cui et al., 2016; Trischler et al., 2016a,b; Kadlec et al., 2016; Kobayashi et al., 2016; Wang and Jiang, 2016b), but ignore the second issues. One notable task is SNLI (Bowman et al., 2015), which considers entailment between two sentences. This task, however, only provides shallow context and thus needs a few kinds of implicit knowledge (Rockt¨aschel et al., 2015; Wang and Jiang, 2016a; Angeli et al., 2016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story understanding needs commonsense knowledge, many researches have been proposed to learn structural event knowledge. Chambers and Jurafsky (2008) first proposed an unsupervised approach to learn partially ordered sets of events from raw text. Many expansions have been introduced later, including unsupervisedly learning narrative schemas and scripts (Chambers and Jurafsky, 2009; Regneri et al., 2011), event schemas and frames (Chambers and Jurafsky, 2011; Balasubramanian et al., 2013; Sha"
D17-1216,baccianella-etal-2010-sentiwordnet,0,0.0179288,") (3) where E1 and E2 are the sets of all entities that link to these two entities in Wikipedia respectively, 2034 1 https://www.wikipedia.org/ and W is the entire Wikipedia. We set the cost of associative inference rule e1 −−−−−−−→ e2 as dist(e1 , e2 ). 2.3 Mining Sentiment Coherent Knowledge Sentiment is one of the central and pervasive aspects of human experience (Ortony et al., 1990). It plays an important role in commonsense stories, i.e., a reasonable hypothesis should be sentimental coherent with its premise document. In this paper, we mine sentiment coherence rules using SentiWordnet (Baccianella et al., 2010), in which each synset of Wordnet is assigned with three sentiment scores: positivity, negativity and objectivity. Concretely, to identify sentimental coherence rule between two element e1 and e2 , we first compute the positivity, negativity and objectivity scores of every element by averaging the scores of all synsets it’s in, then we identify an element to be subjective if its objectivity score is smaller than a threshold, and the distance between its positivity and negativity score is greater than a threshsenti old. Finally, for an inference rule e1 −−−→ e2 , we set its cost to 1 if e1 and"
D17-1216,D13-1178,0,0.0519178,"Missing"
D17-1216,D14-1159,0,0.0116198,"aw text. Many expansions have been introduced later, including unsupervisedly learning narrative schemas and scripts (Chambers and Jurafsky, 2009; Regneri et al., 2011), event schemas and frames (Chambers and Jurafsky, 2011; Balasubramanian et al., 2013; Sha et al., 2016; Huang et al., 2016; Mostafazadeh et al., 2016b), and some generative models to learn latent structures of event knowledge (Cheung et al., 2013; Chambers, 2013; Bamman et al., 2014; Nguyen et al., 2015). Another direction for learning event-centred knowledge is causality identification (Do et al., 2011; Radinsky et al., 2012; Berant et al., 2014; Hashimoto et al., 2015; Gui et al., 2016), which tried to identify the causality relation in text. For reasoning over these knowledge, Jans et al. (2012) extend introduced skip-grams for collecting statistics. Further improvements include incorporating more information and more complicated models (Radinsky and Horvitz, 2013; Modi and Titov, 2014; Ahrendt and Demberg, 2016). Recent researches tried to solve event prediction problem by transforming it into an language modeling paradigm (Pichotta and Mooney, 2014, 2015, 2016a,b; Rudinger et al., 2015; Hu et al., 2017). The principal difference"
D17-1216,D15-1075,0,0.00471654,"o human-like reasoning process. Previous machine comprehension tasks (Richardson et al., 2013; Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016) mainly focus on the first challenge, leading their solutions focusing on semantic matching between texts (Weston et al., 2014; Kumar et al., 2015; Narasimhan and Barzilay, 2015; Smith et al., 2015; Sukhbaatar et al., 2015; Hill et al., 2015; Wang et al., 2015, 2016a; Cui et al., 2016; Trischler et al., 2016a,b; Kadlec et al., 2016; Kobayashi et al., 2016; Wang and Jiang, 2016b), but ignore the second issues. One notable task is SNLI (Bowman et al., 2015), which considers entailment between two sentences. This task, however, only provides shallow context and thus needs a few kinds of implicit knowledge (Rockt¨aschel et al., 2015; Wang and Jiang, 2016a; Angeli et al., 2016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story understanding needs commonsense knowledge, many researches have been proposed to learn structural event knowledge. Chambers and Jurafsky (2008) first proposed an unsupervised approach to learn partially ordered sets of events from raw text. Many expansions have been in"
D17-1216,W17-0908,0,0.0231608,"2017). The principal difference between previous work and our method is that we not only take various kinds of implicit commonsense knowledge into consideration, but also provide a highly 2039 extensible framework to exploit these kinds of knowledge for commonsense machine comprehension. We also notice the recent progress in RocStories (Mostafazadeh et al., 2017). Rather than inferring a possible ending generated from document, recent systems solve this task by discriminatively comparing two candidates. This enables very strong stylistic features being added explicitly (Schwartz et al., 2017; Bugert et al., 2017) or implicitly (Schenk and Chiarcos, 2017), which can select hypothesis without any consideration of given document. Also, some augmentation strategies are introduced to produce more training data (Roemmele and Gordon, 2017; Mihaylov and Frank, 2017; Bugert et al., 2017). These methods are dataset-sensitive and are not the main concentration of our paper. 6 Conclusions and Future Work This paper proposes a commonsense machine comprehension method, which performs effective commonsense reasoning by taking heterogenous knowledge into consideration. Specifically, we mine commonsense knowledge from"
D17-1216,D13-1185,0,0.0145965,"esearches have been proposed to learn structural event knowledge. Chambers and Jurafsky (2008) first proposed an unsupervised approach to learn partially ordered sets of events from raw text. Many expansions have been introduced later, including unsupervisedly learning narrative schemas and scripts (Chambers and Jurafsky, 2009; Regneri et al., 2011), event schemas and frames (Chambers and Jurafsky, 2011; Balasubramanian et al., 2013; Sha et al., 2016; Huang et al., 2016; Mostafazadeh et al., 2016b), and some generative models to learn latent structures of event knowledge (Cheung et al., 2013; Chambers, 2013; Bamman et al., 2014; Nguyen et al., 2015). Another direction for learning event-centred knowledge is causality identification (Do et al., 2011; Radinsky et al., 2012; Berant et al., 2014; Hashimoto et al., 2015; Gui et al., 2016), which tried to identify the causality relation in text. For reasoning over these knowledge, Jans et al. (2012) extend introduced skip-grams for collecting statistics. Further improvements include incorporating more information and more complicated models (Radinsky and Horvitz, 2013; Modi and Titov, 2014; Ahrendt and Demberg, 2016). Recent researches tried to solve"
D17-1216,P09-1068,0,0.012467,"ides shallow context and thus needs a few kinds of implicit knowledge (Rockt¨aschel et al., 2015; Wang and Jiang, 2016a; Angeli et al., 2016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story understanding needs commonsense knowledge, many researches have been proposed to learn structural event knowledge. Chambers and Jurafsky (2008) first proposed an unsupervised approach to learn partially ordered sets of events from raw text. Many expansions have been introduced later, including unsupervisedly learning narrative schemas and scripts (Chambers and Jurafsky, 2009; Regneri et al., 2011), event schemas and frames (Chambers and Jurafsky, 2011; Balasubramanian et al., 2013; Sha et al., 2016; Huang et al., 2016; Mostafazadeh et al., 2016b), and some generative models to learn latent structures of event knowledge (Cheung et al., 2013; Chambers, 2013; Bamman et al., 2014; Nguyen et al., 2015). Another direction for learning event-centred knowledge is causality identification (Do et al., 2011; Radinsky et al., 2012; Berant et al., 2014; Hashimoto et al., 2015; Gui et al., 2016), which tried to identify the causality relation in text. For reasoning over these"
D17-1216,P11-1098,0,0.0154622,"schel et al., 2015; Wang and Jiang, 2016a; Angeli et al., 2016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story understanding needs commonsense knowledge, many researches have been proposed to learn structural event knowledge. Chambers and Jurafsky (2008) first proposed an unsupervised approach to learn partially ordered sets of events from raw text. Many expansions have been introduced later, including unsupervisedly learning narrative schemas and scripts (Chambers and Jurafsky, 2009; Regneri et al., 2011), event schemas and frames (Chambers and Jurafsky, 2011; Balasubramanian et al., 2013; Sha et al., 2016; Huang et al., 2016; Mostafazadeh et al., 2016b), and some generative models to learn latent structures of event knowledge (Cheung et al., 2013; Chambers, 2013; Bamman et al., 2014; Nguyen et al., 2015). Another direction for learning event-centred knowledge is causality identification (Do et al., 2011; Radinsky et al., 2012; Berant et al., 2014; Hashimoto et al., 2015; Gui et al., 2016), which tried to identify the causality relation in text. For reasoning over these knowledge, Jans et al. (2012) extend introduced skip-grams for collecting stat"
D17-1216,P08-1090,0,0.32525,"ight Hypothesis His boss commends him for a job well done. Wrong Hypothesis Ron is immediately fired for insubordination. My son said they would, so we got a dog. We then grabbed a small kitten. Table 1: Examples of RocStories Dataset. In recent years, many methods have been proposed for commonsense machine comprehension. However, these methods mostly either focus on matching explicit information in given texts (Weston et al., 2014; Wang and Jiang, 2016a,b; Wang et al., 2016b; Zhao et al., 2017), or paid attention to one specific kind of commonsense knowledge, such as event temporal relation (Chambers and Jurafsky, 2008; Modi and Titov, 2014; Pichotta and Mooney, 2016b; Hu et al., 2017) and event causality (Do et al., 2011; Radinsky et al., 2012; Hashimoto et al., 2015; Gui et al., 2016). As discussed above, it is obvious that commonsense machine comprehension problem is far from settled by considering only explicit or a single kind of commonsense knowledge. To achieve humanlike comprehension and reasoning, there exist two main challenges: 1) How to mine and represent different kinds of implicit knowledge that commonsense machine comprehension needs. For example, to complete the first example in Table 1, we"
D17-1216,N13-1104,0,0.0121634,"nse knowledge, many researches have been proposed to learn structural event knowledge. Chambers and Jurafsky (2008) first proposed an unsupervised approach to learn partially ordered sets of events from raw text. Many expansions have been introduced later, including unsupervisedly learning narrative schemas and scripts (Chambers and Jurafsky, 2009; Regneri et al., 2011), event schemas and frames (Chambers and Jurafsky, 2011; Balasubramanian et al., 2013; Sha et al., 2016; Huang et al., 2016; Mostafazadeh et al., 2016b), and some generative models to learn latent structures of event knowledge (Cheung et al., 2013; Chambers, 2013; Bamman et al., 2014; Nguyen et al., 2015). Another direction for learning event-centred knowledge is causality identification (Do et al., 2011; Radinsky et al., 2012; Berant et al., 2014; Hashimoto et al., 2015; Gui et al., 2016), which tried to identify the causality relation in text. For reasoning over these knowledge, Jans et al. (2012) extend introduced skip-grams for collecting statistics. Further improvements include incorporating more information and more complicated models (Radinsky and Horvitz, 2013; Modi and Titov, 2014; Ahrendt and Demberg, 2016). Recent researches"
D17-1216,J89-3002,0,0.377907,"Missing"
D17-1216,D11-1027,0,0.133199,"ion. My son said they would, so we got a dog. We then grabbed a small kitten. Table 1: Examples of RocStories Dataset. In recent years, many methods have been proposed for commonsense machine comprehension. However, these methods mostly either focus on matching explicit information in given texts (Weston et al., 2014; Wang and Jiang, 2016a,b; Wang et al., 2016b; Zhao et al., 2017), or paid attention to one specific kind of commonsense knowledge, such as event temporal relation (Chambers and Jurafsky, 2008; Modi and Titov, 2014; Pichotta and Mooney, 2016b; Hu et al., 2017) and event causality (Do et al., 2011; Radinsky et al., 2012; Hashimoto et al., 2015; Gui et al., 2016). As discussed above, it is obvious that commonsense machine comprehension problem is far from settled by considering only explicit or a single kind of commonsense knowledge. To achieve humanlike comprehension and reasoning, there exist two main challenges: 1) How to mine and represent different kinds of implicit knowledge that commonsense machine comprehension needs. For example, to complete the first example in Table 1, we need a system equipped with the event narrative knowledge that “commends X” can be inferred from “X does"
D17-1216,D16-1170,0,0.0247619,"small kitten. Table 1: Examples of RocStories Dataset. In recent years, many methods have been proposed for commonsense machine comprehension. However, these methods mostly either focus on matching explicit information in given texts (Weston et al., 2014; Wang and Jiang, 2016a,b; Wang et al., 2016b; Zhao et al., 2017), or paid attention to one specific kind of commonsense knowledge, such as event temporal relation (Chambers and Jurafsky, 2008; Modi and Titov, 2014; Pichotta and Mooney, 2016b; Hu et al., 2017) and event causality (Do et al., 2011; Radinsky et al., 2012; Hashimoto et al., 2015; Gui et al., 2016). As discussed above, it is obvious that commonsense machine comprehension problem is far from settled by considering only explicit or a single kind of commonsense knowledge. To achieve humanlike comprehension and reasoning, there exist two main challenges: 1) How to mine and represent different kinds of implicit knowledge that commonsense machine comprehension needs. For example, to complete the first example in Table 1, we need a system equipped with the event narrative knowledge that “commends X” can be inferred from “X does a thorough job”, as well as the sentiment coherent knowledge that"
D17-1216,P16-1193,0,0.0177417,"014; Kumar et al., 2015; Narasimhan and Barzilay, 2015; Smith et al., 2015; Sukhbaatar et al., 2015; Hill et al., 2015; Wang et al., 2015, 2016a; Cui et al., 2016; Trischler et al., 2016a,b; Kadlec et al., 2016; Kobayashi et al., 2016; Wang and Jiang, 2016b), but ignore the second issues. One notable task is SNLI (Bowman et al., 2015), which considers entailment between two sentences. This task, however, only provides shallow context and thus needs a few kinds of implicit knowledge (Rockt¨aschel et al., 2015; Wang and Jiang, 2016a; Angeli et al., 2016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story understanding needs commonsense knowledge, many researches have been proposed to learn structural event knowledge. Chambers and Jurafsky (2008) first proposed an unsupervised approach to learn partially ordered sets of events from raw text. Many expansions have been introduced later, including unsupervisedly learning narrative schemas and scripts (Chambers and Jurafsky, 2009; Regneri et al., 2011), event schemas and frames (Chambers and Jurafsky, 2011; Balasubramanian et al., 2013; Sha et al., 2016; Huang et al., 2016; Mostafazadeh et al., 2016b), and"
D17-1216,P16-1025,0,0.0274548,"2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story understanding needs commonsense knowledge, many researches have been proposed to learn structural event knowledge. Chambers and Jurafsky (2008) first proposed an unsupervised approach to learn partially ordered sets of events from raw text. Many expansions have been introduced later, including unsupervisedly learning narrative schemas and scripts (Chambers and Jurafsky, 2009; Regneri et al., 2011), event schemas and frames (Chambers and Jurafsky, 2011; Balasubramanian et al., 2013; Sha et al., 2016; Huang et al., 2016; Mostafazadeh et al., 2016b), and some generative models to learn latent structures of event knowledge (Cheung et al., 2013; Chambers, 2013; Bamman et al., 2014; Nguyen et al., 2015). Another direction for learning event-centred knowledge is causality identification (Do et al., 2011; Radinsky et al., 2012; Berant et al., 2014; Hashimoto et al., 2015; Gui et al., 2016), which tried to identify the causality relation in text. For reasoning over these knowledge, Jans et al. (2012) extend introduced skip-grams for collecting statistics. Further improvements include incorporating more information"
D17-1216,E12-1034,0,0.0550178,"Missing"
D17-1216,P16-1086,0,0.0412179,"two big challenges: 1)Matching explicit information in the given context; 2)Incorporating implicit commonsense knowledge into human-like reasoning process. Previous machine comprehension tasks (Richardson et al., 2013; Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016) mainly focus on the first challenge, leading their solutions focusing on semantic matching between texts (Weston et al., 2014; Kumar et al., 2015; Narasimhan and Barzilay, 2015; Smith et al., 2015; Sukhbaatar et al., 2015; Hill et al., 2015; Wang et al., 2015, 2016a; Cui et al., 2016; Trischler et al., 2016a,b; Kadlec et al., 2016; Kobayashi et al., 2016; Wang and Jiang, 2016b), but ignore the second issues. One notable task is SNLI (Bowman et al., 2015), which considers entailment between two sentences. This task, however, only provides shallow context and thus needs a few kinds of implicit knowledge (Rockt¨aschel et al., 2015; Wang and Jiang, 2016a; Angeli et al., 2016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story understanding needs commonsense knowledge, many researches have been proposed to learn structural event knowledge. Chambers and Jurafsky (2008)"
D17-1216,N16-1099,0,0.0351056,"1)Matching explicit information in the given context; 2)Incorporating implicit commonsense knowledge into human-like reasoning process. Previous machine comprehension tasks (Richardson et al., 2013; Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016) mainly focus on the first challenge, leading their solutions focusing on semantic matching between texts (Weston et al., 2014; Kumar et al., 2015; Narasimhan and Barzilay, 2015; Smith et al., 2015; Sukhbaatar et al., 2015; Hill et al., 2015; Wang et al., 2015, 2016a; Cui et al., 2016; Trischler et al., 2016a,b; Kadlec et al., 2016; Kobayashi et al., 2016; Wang and Jiang, 2016b), but ignore the second issues. One notable task is SNLI (Bowman et al., 2015), which considers entailment between two sentences. This task, however, only provides shallow context and thus needs a few kinds of implicit knowledge (Rockt¨aschel et al., 2015; Wang and Jiang, 2016a; Angeli et al., 2016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story understanding needs commonsense knowledge, many researches have been proposed to learn structural event knowledge. Chambers and Jurafsky (2008) first proposed an unsup"
D17-1216,P14-5010,0,0.0041242,"sists of 1871 commonsense stories, with each story has two candidate story endings. Because stories in the training set of RocStories do not contain wrong hypothesis, and our model has a compact size of parameters, we estimated the parameters of our model using the Validation Set Spring 2016 of RocStories with 1871 commonsense stories. We mined event narrative knowledge from the Training Set Spring 2016 of RocStories, which consists of 45502 commonsense stories. We performed lemmatisation, part of speech annotation, named entity tagging, and dependency parsing using Stanford CoreNLP toolkits (Manning et al., 2014). We used the Jan. 30, 2010 English version of Wikipedia and processed it according to the method described by Hu et al. (2008). Model Training. We used normalized initialization (Glorot and Bengio, 2010) to initialize attention parameters in our model. For calibration parameters, we initialized all wf to 1 and bf to 0. The model parameters were trained using minibatch stochastic gradient descent algorithm. As for hyper-parameters, we set the batch size as 32, the learning rate as 1, the dimension of attention hidden layer K as 32, and the smoothing factor γ as 0.5. Baselines. We compared our"
D17-1216,W17-0913,0,0.0452202,"edge for commonsense machine comprehension. We also notice the recent progress in RocStories (Mostafazadeh et al., 2017). Rather than inferring a possible ending generated from document, recent systems solve this task by discriminatively comparing two candidates. This enables very strong stylistic features being added explicitly (Schwartz et al., 2017; Bugert et al., 2017) or implicitly (Schenk and Chiarcos, 2017), which can select hypothesis without any consideration of given document. Also, some augmentation strategies are introduced to produce more training data (Roemmele and Gordon, 2017; Mihaylov and Frank, 2017; Bugert et al., 2017). These methods are dataset-sensitive and are not the main concentration of our paper. 6 Conclusions and Future Work This paper proposes a commonsense machine comprehension method, which performs effective commonsense reasoning by taking heterogenous knowledge into consideration. Specifically, we mine commonsense knowledge from heterogeneous knowledge sources and simultaneously exploit them by proposing a highly extensible multiknowledge reasoning framework. Experiment results shown that our method surpasses baselines by a large margin. Currently, there are little labeled"
D17-1216,W14-1606,0,0.0353128,"ends him for a job well done. Wrong Hypothesis Ron is immediately fired for insubordination. My son said they would, so we got a dog. We then grabbed a small kitten. Table 1: Examples of RocStories Dataset. In recent years, many methods have been proposed for commonsense machine comprehension. However, these methods mostly either focus on matching explicit information in given texts (Weston et al., 2014; Wang and Jiang, 2016a,b; Wang et al., 2016b; Zhao et al., 2017), or paid attention to one specific kind of commonsense knowledge, such as event temporal relation (Chambers and Jurafsky, 2008; Modi and Titov, 2014; Pichotta and Mooney, 2016b; Hu et al., 2017) and event causality (Do et al., 2011; Radinsky et al., 2012; Hashimoto et al., 2015; Gui et al., 2016). As discussed above, it is obvious that commonsense machine comprehension problem is far from settled by considering only explicit or a single kind of commonsense knowledge. To achieve humanlike comprehension and reasoning, there exist two main challenges: 1) How to mine and represent different kinds of implicit knowledge that commonsense machine comprehension needs. For example, to complete the first example in Table 1, we need a system equipped"
D17-1216,N16-1098,0,0.234024,"fundamental in artificial intelligence, and has long been a key component in natural language understanding and human-like reasoning. For example, to understand the relation between sentences “Mary walked to a restaurant” and “She ordered some foods”, we need commonsense knowledge such as “Mary is a girl”, “restaurant sells food”, etc. The task of understanding natural language with commonsense knowledge is usually referred as commonsense machine comprehension, which has been a hot topic in recent years (Richardson et al., 2013; Weston et al., 2015; Zhang et al., 2016). Recently, RocStories (Mostafazadeh et al., 2016a), a commonsense machine comprehension task, has attached many researchers’ attention due to its significant difference from previous machine comprehension tasks. RocStories focuses on reasoning with implicit commonsense knowledge, rather than matching with explicit information in given contexts. In this task, a system requires choosing a sentence, namely hypothesis, to complete a given commonsense story, called as premise document. Table 1 shows two examples. RocStories proposes a challenging benchmark task for evaluating commonsensebased language understanding. As investigated by Mostafazad"
D17-1216,W16-1007,0,0.162629,"fundamental in artificial intelligence, and has long been a key component in natural language understanding and human-like reasoning. For example, to understand the relation between sentences “Mary walked to a restaurant” and “She ordered some foods”, we need commonsense knowledge such as “Mary is a girl”, “restaurant sells food”, etc. The task of understanding natural language with commonsense knowledge is usually referred as commonsense machine comprehension, which has been a hot topic in recent years (Richardson et al., 2013; Weston et al., 2015; Zhang et al., 2016). Recently, RocStories (Mostafazadeh et al., 2016a), a commonsense machine comprehension task, has attached many researchers’ attention due to its significant difference from previous machine comprehension tasks. RocStories focuses on reasoning with implicit commonsense knowledge, rather than matching with explicit information in given contexts. In this task, a system requires choosing a sentence, namely hypothesis, to complete a given commonsense story, called as premise document. Table 1 shows two examples. RocStories proposes a challenging benchmark task for evaluating commonsensebased language understanding. As investigated by Mostafazad"
D17-1216,W17-0906,0,0.0101082,"ky and Horvitz, 2013; Modi and Titov, 2014; Ahrendt and Demberg, 2016). Recent researches tried to solve event prediction problem by transforming it into an language modeling paradigm (Pichotta and Mooney, 2014, 2015, 2016a,b; Rudinger et al., 2015; Hu et al., 2017). The principal difference between previous work and our method is that we not only take various kinds of implicit commonsense knowledge into consideration, but also provide a highly 2039 extensible framework to exploit these kinds of knowledge for commonsense machine comprehension. We also notice the recent progress in RocStories (Mostafazadeh et al., 2017). Rather than inferring a possible ending generated from document, recent systems solve this task by discriminatively comparing two candidates. This enables very strong stylistic features being added explicitly (Schwartz et al., 2017; Bugert et al., 2017) or implicitly (Schenk and Chiarcos, 2017), which can select hypothesis without any consideration of given document. Also, some augmentation strategies are introduced to produce more training data (Roemmele and Gordon, 2017; Mihaylov and Frank, 2017; Bugert et al., 2017). These methods are dataset-sensitive and are not the main concentration o"
D17-1216,P15-1121,0,0.00636611,"osed negation rules. 5 Related Work Endowing computers with the ability of understanding commonsense story has long a goal of natural language processing. There exist two big challenges: 1)Matching explicit information in the given context; 2)Incorporating implicit commonsense knowledge into human-like reasoning process. Previous machine comprehension tasks (Richardson et al., 2013; Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016) mainly focus on the first challenge, leading their solutions focusing on semantic matching between texts (Weston et al., 2014; Kumar et al., 2015; Narasimhan and Barzilay, 2015; Smith et al., 2015; Sukhbaatar et al., 2015; Hill et al., 2015; Wang et al., 2015, 2016a; Cui et al., 2016; Trischler et al., 2016a,b; Kadlec et al., 2016; Kobayashi et al., 2016; Wang and Jiang, 2016b), but ignore the second issues. One notable task is SNLI (Bowman et al., 2015), which considers entailment between two sentences. This task, however, only provides shallow context and thus needs a few kinds of implicit knowledge (Rockt¨aschel et al., 2015; Wang and Jiang, 2016a; Angeli et al., 2016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizin"
D17-1216,P15-1019,0,0.0898343,"Missing"
D17-1216,D16-1244,0,0.0130624,"Missing"
D17-1216,E14-1024,0,0.036029,"t-centred knowledge is causality identification (Do et al., 2011; Radinsky et al., 2012; Berant et al., 2014; Hashimoto et al., 2015; Gui et al., 2016), which tried to identify the causality relation in text. For reasoning over these knowledge, Jans et al. (2012) extend introduced skip-grams for collecting statistics. Further improvements include incorporating more information and more complicated models (Radinsky and Horvitz, 2013; Modi and Titov, 2014; Ahrendt and Demberg, 2016). Recent researches tried to solve event prediction problem by transforming it into an language modeling paradigm (Pichotta and Mooney, 2014, 2015, 2016a,b; Rudinger et al., 2015; Hu et al., 2017). The principal difference between previous work and our method is that we not only take various kinds of implicit commonsense knowledge into consideration, but also provide a highly 2039 extensible framework to exploit these kinds of knowledge for commonsense machine comprehension. We also notice the recent progress in RocStories (Mostafazadeh et al., 2017). Rather than inferring a possible ending generated from document, recent systems solve this task by discriminatively comparing two candidates. This enables very strong stylistic featu"
D17-1216,W16-6003,0,0.0252311,"Missing"
D17-1216,P16-1027,0,0.0531403,"Missing"
D17-1216,D16-1264,0,0.00361786,"gation rules. Table 8 show the results. We can see that removing negation rules will significantly drop the system performance, which confirm the effectiveness of our proposed negation rules. 5 Related Work Endowing computers with the ability of understanding commonsense story has long a goal of natural language processing. There exist two big challenges: 1)Matching explicit information in the given context; 2)Incorporating implicit commonsense knowledge into human-like reasoning process. Previous machine comprehension tasks (Richardson et al., 2013; Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016) mainly focus on the first challenge, leading their solutions focusing on semantic matching between texts (Weston et al., 2014; Kumar et al., 2015; Narasimhan and Barzilay, 2015; Smith et al., 2015; Sukhbaatar et al., 2015; Hill et al., 2015; Wang et al., 2015, 2016a; Cui et al., 2016; Trischler et al., 2016a,b; Kadlec et al., 2016; Kobayashi et al., 2016; Wang and Jiang, 2016b), but ignore the second issues. One notable task is SNLI (Bowman et al., 2015), which considers entailment between two sentences. This task, however, only provides shallow context and thus needs a few kinds of implicit"
D17-1216,R11-1064,0,0.0176756,"needs a few kinds of implicit knowledge (Rockt¨aschel et al., 2015; Wang and Jiang, 2016a; Angeli et al., 2016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story understanding needs commonsense knowledge, many researches have been proposed to learn structural event knowledge. Chambers and Jurafsky (2008) first proposed an unsupervised approach to learn partially ordered sets of events from raw text. Many expansions have been introduced later, including unsupervisedly learning narrative schemas and scripts (Chambers and Jurafsky, 2009; Regneri et al., 2011), event schemas and frames (Chambers and Jurafsky, 2011; Balasubramanian et al., 2013; Sha et al., 2016; Huang et al., 2016; Mostafazadeh et al., 2016b), and some generative models to learn latent structures of event knowledge (Cheung et al., 2013; Chambers, 2013; Bamman et al., 2014; Nguyen et al., 2015). Another direction for learning event-centred knowledge is causality identification (Do et al., 2011; Radinsky et al., 2012; Berant et al., 2014; Hashimoto et al., 2015; Gui et al., 2016), which tried to identify the causality relation in text. For reasoning over these knowledge, Jans et al."
D17-1216,D13-1020,0,0.0376885,"hod outperforms traditional models significantly. 1 Introduction Commonsense knowledge is fundamental in artificial intelligence, and has long been a key component in natural language understanding and human-like reasoning. For example, to understand the relation between sentences “Mary walked to a restaurant” and “She ordered some foods”, we need commonsense knowledge such as “Mary is a girl”, “restaurant sells food”, etc. The task of understanding natural language with commonsense knowledge is usually referred as commonsense machine comprehension, which has been a hot topic in recent years (Richardson et al., 2013; Weston et al., 2015; Zhang et al., 2016). Recently, RocStories (Mostafazadeh et al., 2016a), a commonsense machine comprehension task, has attached many researchers’ attention due to its significant difference from previous machine comprehension tasks. RocStories focuses on reasoning with implicit commonsense knowledge, rather than matching with explicit information in given contexts. In this task, a system requires choosing a sentence, namely hypothesis, to complete a given commonsense story, called as premise document. Table 1 shows two examples. RocStories proposes a challenging benchmark"
D17-1216,W17-0911,0,0.0454064,"xploit these kinds of knowledge for commonsense machine comprehension. We also notice the recent progress in RocStories (Mostafazadeh et al., 2017). Rather than inferring a possible ending generated from document, recent systems solve this task by discriminatively comparing two candidates. This enables very strong stylistic features being added explicitly (Schwartz et al., 2017; Bugert et al., 2017) or implicitly (Schenk and Chiarcos, 2017), which can select hypothesis without any consideration of given document. Also, some augmentation strategies are introduced to produce more training data (Roemmele and Gordon, 2017; Mihaylov and Frank, 2017; Bugert et al., 2017). These methods are dataset-sensitive and are not the main concentration of our paper. 6 Conclusions and Future Work This paper proposes a commonsense machine comprehension method, which performs effective commonsense reasoning by taking heterogenous knowledge into consideration. Specifically, we mine commonsense knowledge from heterogeneous knowledge sources and simultaneously exploit them by proposing a highly extensible multiknowledge reasoning framework. Experiment results shown that our method surpasses baselines by a large margin. Currently"
D17-1216,D15-1195,0,0.0240287,"Missing"
D17-1216,W17-0910,0,0.0585315,"een previous work and our method is that we not only take various kinds of implicit commonsense knowledge into consideration, but also provide a highly 2039 extensible framework to exploit these kinds of knowledge for commonsense machine comprehension. We also notice the recent progress in RocStories (Mostafazadeh et al., 2017). Rather than inferring a possible ending generated from document, recent systems solve this task by discriminatively comparing two candidates. This enables very strong stylistic features being added explicitly (Schwartz et al., 2017; Bugert et al., 2017) or implicitly (Schenk and Chiarcos, 2017), which can select hypothesis without any consideration of given document. Also, some augmentation strategies are introduced to produce more training data (Roemmele and Gordon, 2017; Mihaylov and Frank, 2017; Bugert et al., 2017). These methods are dataset-sensitive and are not the main concentration of our paper. 6 Conclusions and Future Work This paper proposes a commonsense machine comprehension method, which performs effective commonsense reasoning by taking heterogenous knowledge into consideration. Specifically, we mine commonsense knowledge from heterogeneous knowledge sources and simul"
D17-1216,N16-1049,0,0.0559316,"016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story understanding needs commonsense knowledge, many researches have been proposed to learn structural event knowledge. Chambers and Jurafsky (2008) first proposed an unsupervised approach to learn partially ordered sets of events from raw text. Many expansions have been introduced later, including unsupervisedly learning narrative schemas and scripts (Chambers and Jurafsky, 2009; Regneri et al., 2011), event schemas and frames (Chambers and Jurafsky, 2011; Balasubramanian et al., 2013; Sha et al., 2016; Huang et al., 2016; Mostafazadeh et al., 2016b), and some generative models to learn latent structures of event knowledge (Cheung et al., 2013; Chambers, 2013; Bamman et al., 2014; Nguyen et al., 2015). Another direction for learning event-centred knowledge is causality identification (Do et al., 2011; Radinsky et al., 2012; Berant et al., 2014; Hashimoto et al., 2015; Gui et al., 2016), which tried to identify the causality relation in text. For reasoning over these knowledge, Jans et al. (2012) extend introduced skip-grams for collecting statistics. Further improvements include incorporati"
D17-1216,D15-1197,0,0.0125402,"Work Endowing computers with the ability of understanding commonsense story has long a goal of natural language processing. There exist two big challenges: 1)Matching explicit information in the given context; 2)Incorporating implicit commonsense knowledge into human-like reasoning process. Previous machine comprehension tasks (Richardson et al., 2013; Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016) mainly focus on the first challenge, leading their solutions focusing on semantic matching between texts (Weston et al., 2014; Kumar et al., 2015; Narasimhan and Barzilay, 2015; Smith et al., 2015; Sukhbaatar et al., 2015; Hill et al., 2015; Wang et al., 2015, 2016a; Cui et al., 2016; Trischler et al., 2016a,b; Kadlec et al., 2016; Kobayashi et al., 2016; Wang and Jiang, 2016b), but ignore the second issues. One notable task is SNLI (Bowman et al., 2015), which considers entailment between two sentences. This task, however, only provides shallow context and thus needs a few kinds of implicit knowledge (Rockt¨aschel et al., 2015; Wang and Jiang, 2016a; Angeli et al., 2016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story underst"
D17-1216,P16-1041,0,0.0330806,"Missing"
D17-1216,D16-1013,0,0.0284745,"age processing. There exist two big challenges: 1)Matching explicit information in the given context; 2)Incorporating implicit commonsense knowledge into human-like reasoning process. Previous machine comprehension tasks (Richardson et al., 2013; Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016) mainly focus on the first challenge, leading their solutions focusing on semantic matching between texts (Weston et al., 2014; Kumar et al., 2015; Narasimhan and Barzilay, 2015; Smith et al., 2015; Sukhbaatar et al., 2015; Hill et al., 2015; Wang et al., 2015, 2016a; Cui et al., 2016; Trischler et al., 2016a,b; Kadlec et al., 2016; Kobayashi et al., 2016; Wang and Jiang, 2016b), but ignore the second issues. One notable task is SNLI (Bowman et al., 2015), which considers entailment between two sentences. This task, however, only provides shallow context and thus needs a few kinds of implicit knowledge (Rockt¨aschel et al., 2015; Wang and Jiang, 2016a; Angeli et al., 2016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story understanding needs commonsense knowledge, many researches have been proposed to learn structural event knowledge. Cham"
D17-1216,P15-2115,0,0.00465874,"sense story has long a goal of natural language processing. There exist two big challenges: 1)Matching explicit information in the given context; 2)Incorporating implicit commonsense knowledge into human-like reasoning process. Previous machine comprehension tasks (Richardson et al., 2013; Weston et al., 2015; Hermann et al., 2015; Rajpurkar et al., 2016) mainly focus on the first challenge, leading their solutions focusing on semantic matching between texts (Weston et al., 2014; Kumar et al., 2015; Narasimhan and Barzilay, 2015; Smith et al., 2015; Sukhbaatar et al., 2015; Hill et al., 2015; Wang et al., 2015, 2016a; Cui et al., 2016; Trischler et al., 2016a,b; Kadlec et al., 2016; Kobayashi et al., 2016; Wang and Jiang, 2016b), but ignore the second issues. One notable task is SNLI (Bowman et al., 2015), which considers entailment between two sentences. This task, however, only provides shallow context and thus needs a few kinds of implicit knowledge (Rockt¨aschel et al., 2015; Wang and Jiang, 2016a; Angeli et al., 2016; Wang et al., 2016b; Parikh et al., 2016; Henderson and Popa, 2016; Zhao et al., 2017). Realizing that story understanding needs commonsense knowledge, many researches have been p"
D17-1216,N16-1170,0,0.0598007,"told us that she had just gotten the puppy across the street. My sons begged me to get them one. I told them that if they would care for it, they could have it. Right Hypothesis His boss commends him for a job well done. Wrong Hypothesis Ron is immediately fired for insubordination. My son said they would, so we got a dog. We then grabbed a small kitten. Table 1: Examples of RocStories Dataset. In recent years, many methods have been proposed for commonsense machine comprehension. However, these methods mostly either focus on matching explicit information in given texts (Weston et al., 2014; Wang and Jiang, 2016a,b; Wang et al., 2016b; Zhao et al., 2017), or paid attention to one specific kind of commonsense knowledge, such as event temporal relation (Chambers and Jurafsky, 2008; Modi and Titov, 2014; Pichotta and Mooney, 2016b; Hu et al., 2017) and event causality (Do et al., 2011; Radinsky et al., 2012; Hashimoto et al., 2015; Gui et al., 2016). As discussed above, it is obvious that commonsense machine comprehension problem is far from settled by considering only explicit or a single kind of commonsense knowledge. To achieve humanlike comprehension and reasoning, there exist two main challenges: 1"
D17-1216,C16-1127,0,0.0910405,"t gotten the puppy across the street. My sons begged me to get them one. I told them that if they would care for it, they could have it. Right Hypothesis His boss commends him for a job well done. Wrong Hypothesis Ron is immediately fired for insubordination. My son said they would, so we got a dog. We then grabbed a small kitten. Table 1: Examples of RocStories Dataset. In recent years, many methods have been proposed for commonsense machine comprehension. However, these methods mostly either focus on matching explicit information in given texts (Weston et al., 2014; Wang and Jiang, 2016a,b; Wang et al., 2016b; Zhao et al., 2017), or paid attention to one specific kind of commonsense knowledge, such as event temporal relation (Chambers and Jurafsky, 2008; Modi and Titov, 2014; Pichotta and Mooney, 2016b; Hu et al., 2017) and event causality (Do et al., 2011; Radinsky et al., 2012; Hashimoto et al., 2015; Gui et al., 2016). As discussed above, it is obvious that commonsense machine comprehension problem is far from settled by considering only explicit or a single kind of commonsense knowledge. To achieve humanlike comprehension and reasoning, there exist two main challenges: 1) How to mine and repr"
D18-1478,P16-1035,0,0.293087,"gress in neural information retrieval models (NIRMs) has highlighted promising performance on the ad-hoc search task. State-of-theart NIRMs, such as DRMM (Guo et al., 2016), HiNT (Fan et al., 2018), (Conv)-KNRM (Xiong et al., 2017; Dai et al., 2018), and (Co)PACRR (Hui et al., 2017, 2018), have successfully implemented insights from traditional IR models using neural building blocks. Meanwhile, existing IR research has already demonstrated the effectiveness of incorporating relevance signals from top-ranked documents through pseudo relevance feedback (PRF) models (Buckley and Robertson, 2008; Diaz et al., 2016). PRF models expand the query with terms selected from top-ranked documents, thereby boosting ranking performance by reducing the problem of vocabulary mismatch between the original query and documents (Rocchio, 1971). Existing neural IR models do not have a mechanism for treating expansion terms differently from the original query terms, however, making it non-trivial to combine them with existing PRF approaches. In addition, neural IR models differ in their architectures, making the development of a widely-applicable PRF approach a challenging task. To bridge this gap, we propose a generic n"
D18-1478,D17-1110,1,0.569181,"Missing"
D18-1478,P18-1214,0,0.0310465,"convolutional kernels and pooling operations can be used to successfully identify both unigram and n-gram query matches. PACRR is later improved upon by Co-PACRR, a context-aware variant that takes the local and global context of matching signals into account through the use of three new components (Hui et al., 2018). (Ran et al., 2017) propose a document-based neural relevance model that utilizes complemented medical records to address the mismatch problem in clinical decision support. (Nogueira and Cho, 2017) propose a reinforcement learning approach to reformulating a task-specific query. (Li et al., 2018) propose DAZER, a CNN-based neural model upon interactions between seed words and words in a document for zero-shot document filtering with adversarial learning. (Ai et al., 2018) propose to refine document ranking by learning a deep listwise context model. In summary, most existing neural IR models are based on query-document interaction signals and do not provide a mechanism for incorporating relevance feedback information. This work proposes an approach for incorporating relevance feedback information by embedding neural IR models within a neural pseudo relevance feedback framework, where t"
D18-1478,D17-1061,0,0.0580534,"document. (Hui et al., 2017) propose the PACRR model based on the idea that an appropriate combination of convolutional kernels and pooling operations can be used to successfully identify both unigram and n-gram query matches. PACRR is later improved upon by Co-PACRR, a context-aware variant that takes the local and global context of matching signals into account through the use of three new components (Hui et al., 2018). (Ran et al., 2017) propose a document-based neural relevance model that utilizes complemented medical records to address the mismatch problem in clinical decision support. (Nogueira and Cho, 2017) propose a reinforcement learning approach to reformulating a task-specific query. (Li et al., 2018) propose DAZER, a CNN-based neural model upon interactions between seed words and words in a document for zero-shot document filtering with adversarial learning. (Ai et al., 2018) propose to refine document ranking by learning a deep listwise context model. In summary, most existing neural IR models are based on query-document interaction signals and do not provide a mechanism for incorporating relevance feedback information. This work proposes an approach for incorporating relevance feedback in"
D19-1028,N15-1128,0,0.184649,"011; Tao et al., 2015) and the probability-based method (Shi et al., 2014) are also used to improve the bootstrapping performance. To address the sparse supervision problem, many previous studies score entities by leveraging lexical and statistical features (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Pantel and Pennacchiotti, 2006; Pas¸ca, 2007; Pantel et al., 2009), which, despite the promising effectiveness, could often fail since the sparse statistical features provide little semantic information to evaluate entities. Recently word embedding based methods (Batista et al., 2015; Gupta and Manning, 2015) use fixed word embedding learned on external resources and evaluate entities by their similarity to seeds. Recently, Berger et al. (2018) propose to learn custom embeddings at each bootstrapping iteration, to trade efficiency for effectiveness. 0.69 10 20 50 # patterns 100 200 Figure 4: Performance of our full method using different context patterns in the PMSN. likely due to the case that noises can be included when considering too many patterns. Top patterns selected by different methods. To demonstrate the effectiveness of delayed feedback in our method, we illustrate the top 1 pattern in"
D19-1028,N15-3007,0,0.0207775,"Missing"
D19-1028,P11-1055,0,0.0480765,"lly drifted to other categories. 6 Related Work Entity set expansion (ESE) is a weakly supervised task, which is often given seed entities as supervision and tries to expand new entities related to them. According to the used corpus, there are two types of ESE: limited corpus (Shi et al., 2014; Shen et al., 2017) and large open corpus, e.g. making use of a search engine for the web search (Wang and Cohen, 2007). Weakly supervised methods for information extraction (IE) are often provided insufficient supervision signals, such as knowledge base facts as distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Zeng et al., 2015; Han and Sun, 2016), and light amount of supervision samples in bootstrapping(Riloff and Jones, 1999). As a classical technique, bootstrapping usually exploits pattern (Curran et al., 2007), document (Liao and Grish7 Conclusions In this paper, we propose a deep similarity network-based model combined with the MCTS algorithm to bootstrap Entity Set Expansion. Specifically, we leverage the Monte Carlo Tree Search (MCTS) algorithm to efficiently estimate the delayed feedback of each pattern in the bootstrapping; we propose a Pattern Mover Similarity Network (PMSN) to uniformly"
D19-1028,D15-1056,0,0.209418,"methods (Li et al., 2011; Tao et al., 2015) and the probability-based method (Shi et al., 2014) are also used to improve the bootstrapping performance. To address the sparse supervision problem, many previous studies score entities by leveraging lexical and statistical features (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Pantel and Pennacchiotti, 2006; Pas¸ca, 2007; Pantel et al., 2009), which, despite the promising effectiveness, could often fail since the sparse statistical features provide little semantic information to evaluate entities. Recently word embedding based methods (Batista et al., 2015; Gupta and Manning, 2015) use fixed word embedding learned on external resources and evaluate entities by their similarity to seeds. Recently, Berger et al. (2018) propose to learn custom embeddings at each bootstrapping iteration, to trade efficiency for effectiveness. 0.69 10 20 50 # patterns 100 200 Figure 4: Performance of our full method using different context patterns in the PMSN. likely due to the case that noises can be included when considering too many patterns. Top patterns selected by different methods. To demonstrate the effectiveness of delayed feedback in our method, we illust"
D19-1028,D18-1229,0,0.222623,"the sparse supervision problem, many previous studies score entities by leveraging lexical and statistical features (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Pantel and Pennacchiotti, 2006; Pas¸ca, 2007; Pantel et al., 2009), which, despite the promising effectiveness, could often fail since the sparse statistical features provide little semantic information to evaluate entities. Recently word embedding based methods (Batista et al., 2015; Gupta and Manning, 2015) use fixed word embedding learned on external resources and evaluate entities by their similarity to seeds. Recently, Berger et al. (2018) propose to learn custom embeddings at each bootstrapping iteration, to trade efficiency for effectiveness. 0.69 10 20 50 # patterns 100 200 Figure 4: Performance of our full method using different context patterns in the PMSN. likely due to the case that noises can be included when considering too many patterns. Top patterns selected by different methods. To demonstrate the effectiveness of delayed feedback in our method, we illustrate the top 1 pattern in the first five iterations of three methods in Table 5. From Table 5, we can see that the top patterns by our full method are more related"
D19-1028,C10-1077,0,0.346091,"Missing"
D19-1028,D10-1035,0,0.0297428,"rn Amount 0.9 0.87 0.82 0.8 0.6 0.84 0.75 MAP 0.7 man, 2010) or syntactic and semantic contextual features (He and Grishman, 2015) to extract and classify new instances. Limited to the sparse supervision, previous work estimate patterns mainly based on its direct extraction features, e.g., the matching statistics with known entities (Riloff and Jones, 1999; Agichtein and Gravano, 2000), which often suffers from the semantic drift problem. To avoid semantic drift, most existing approaches exploit extra constraints, such as parallel multiple categories (Thelen and Riloff, 2002; Yangarber, 2003; McIntosh, 2010), coupling constraints (Carlson et al., 2010), and mutual exclusion bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008). Besides, graph-based methods (Li et al., 2011; Tao et al., 2015) and the probability-based method (Shi et al., 2014) are also used to improve the bootstrapping performance. To address the sparse supervision problem, many previous studies score entities by leveraging lexical and statistical features (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Pantel and Pennacchiotti, 2006; Pas¸ca, 2007; Pantel et al., 2009), which, despite the promising effectiveness,"
D19-1028,W14-1611,0,0.0780689,"seed entities, and iteratively extract new entities from corpus by performing the following steps, as demonstrated in Figure 2(a). Pattern generation. Given seed entities and the extracted entities (known entities), a bootstrapping system for ESE firstly generates patterns from the corpus. In this paper, we use lexicon-syntactic surface words around known entities as patterns. Pattern evaluation. This step evaluates generated patterns using sparse supervision and other sources of evidence, e.g., pattern embedding similarity. Many previous studies (Riloff and Jones, 1999; Curran et al., 2007; Gupta and Manning, 2014) use the RlogF function or its variants to evaluate patterns, which usually estimate the instant feedback of each pattern. Entity expansion. This step selects top patterns to match new candidate entities from the corpus. Entity scoring. This step scores candidate entities using sparse supervision, bootstrapping or other external sources of evidence. The top scored entities are then added to the extracted entity set. 3 Enhancing Bootstrapping via Monte Carlo Tree Search In this section, we describe how to enhance the traditional bootstrapping for ESE using the MCTS algorithm for efficient delay"
D19-1028,U08-1013,0,0.638104,"hman, 2015) to extract and classify new instances. Limited to the sparse supervision, previous work estimate patterns mainly based on its direct extraction features, e.g., the matching statistics with known entities (Riloff and Jones, 1999; Agichtein and Gravano, 2000), which often suffers from the semantic drift problem. To avoid semantic drift, most existing approaches exploit extra constraints, such as parallel multiple categories (Thelen and Riloff, 2002; Yangarber, 2003; McIntosh, 2010), coupling constraints (Carlson et al., 2010), and mutual exclusion bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008). Besides, graph-based methods (Li et al., 2011; Tao et al., 2015) and the probability-based method (Shi et al., 2014) are also used to improve the bootstrapping performance. To address the sparse supervision problem, many previous studies score entities by leveraging lexical and statistical features (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Pantel and Pennacchiotti, 2006; Pas¸ca, 2007; Pantel et al., 2009), which, despite the promising effectiveness, could often fail since the sparse statistical features provide little semantic information to evaluate entities. Recently word emb"
D19-1028,P09-1113,0,0.0845101,"and easily semantically drifted to other categories. 6 Related Work Entity set expansion (ESE) is a weakly supervised task, which is often given seed entities as supervision and tries to expand new entities related to them. According to the used corpus, there are two types of ESE: limited corpus (Shi et al., 2014; Shen et al., 2017) and large open corpus, e.g. making use of a search engine for the web search (Wang and Cohen, 2007). Weakly supervised methods for information extraction (IE) are often provided insufficient supervision signals, such as knowledge base facts as distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Zeng et al., 2015; Han and Sun, 2016), and light amount of supervision samples in bootstrapping(Riloff and Jones, 1999). As a classical technique, bootstrapping usually exploits pattern (Curran et al., 2007), document (Liao and Grish7 Conclusions In this paper, we propose a deep similarity network-based model combined with the MCTS algorithm to bootstrap Entity Set Expansion. Specifically, we leverage the Monte Carlo Tree Search (MCTS) algorithm to efficiently estimate the delayed feedback of each pattern in the bootstrapping; we propose a Pattern Mover Similarity Netw"
D19-1028,P05-1047,0,0.0451675,"existing approaches exploit extra constraints, such as parallel multiple categories (Thelen and Riloff, 2002; Yangarber, 2003; McIntosh, 2010), coupling constraints (Carlson et al., 2010), and mutual exclusion bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008). Besides, graph-based methods (Li et al., 2011; Tao et al., 2015) and the probability-based method (Shi et al., 2014) are also used to improve the bootstrapping performance. To address the sparse supervision problem, many previous studies score entities by leveraging lexical and statistical features (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Pantel and Pennacchiotti, 2006; Pas¸ca, 2007; Pantel et al., 2009), which, despite the promising effectiveness, could often fail since the sparse statistical features provide little semantic information to evaluate entities. Recently word embedding based methods (Batista et al., 2015; Gupta and Manning, 2015) use fixed word embedding learned on external resources and evaluate entities by their similarity to seeds. Recently, Berger et al. (2018) propose to learn custom embeddings at each bootstrapping iteration, to trade efficiency for effectiveness. 0.69 10 20 50 # patterns 100 200 Figure 4:"
D19-1028,W12-2402,0,0.0580666,"Missing"
D19-1028,W02-1028,0,0.166846,"e first 5 iterations. MAP vs Context Pattern Amount 0.9 0.87 0.82 0.8 0.6 0.84 0.75 MAP 0.7 man, 2010) or syntactic and semantic contextual features (He and Grishman, 2015) to extract and classify new instances. Limited to the sparse supervision, previous work estimate patterns mainly based on its direct extraction features, e.g., the matching statistics with known entities (Riloff and Jones, 1999; Agichtein and Gravano, 2000), which often suffers from the semantic drift problem. To avoid semantic drift, most existing approaches exploit extra constraints, such as parallel multiple categories (Thelen and Riloff, 2002; Yangarber, 2003; McIntosh, 2010), coupling constraints (Carlson et al., 2010), and mutual exclusion bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008). Besides, graph-based methods (Li et al., 2011; Tao et al., 2015) and the probability-based method (Shi et al., 2014) are also used to improve the bootstrapping performance. To address the sparse supervision problem, many previous studies score entities by leveraging lexical and statistical features (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Pantel and Pennacchiotti, 2006; Pas¸ca, 2007; Pantel et al., 2009), which, des"
D19-1028,P06-1015,0,0.103046,"tra constraints, such as parallel multiple categories (Thelen and Riloff, 2002; Yangarber, 2003; McIntosh, 2010), coupling constraints (Carlson et al., 2010), and mutual exclusion bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008). Besides, graph-based methods (Li et al., 2011; Tao et al., 2015) and the probability-based method (Shi et al., 2014) are also used to improve the bootstrapping performance. To address the sparse supervision problem, many previous studies score entities by leveraging lexical and statistical features (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Pantel and Pennacchiotti, 2006; Pas¸ca, 2007; Pantel et al., 2009), which, despite the promising effectiveness, could often fail since the sparse statistical features provide little semantic information to evaluate entities. Recently word embedding based methods (Batista et al., 2015; Gupta and Manning, 2015) use fixed word embedding learned on external resources and evaluate entities by their similarity to seeds. Recently, Berger et al. (2018) propose to learn custom embeddings at each bootstrapping iteration, to trade efficiency for effectiveness. 0.69 10 20 50 # patterns 100 200 Figure 4: Performance of our full method"
D19-1028,P03-1044,0,0.139549,"vs Context Pattern Amount 0.9 0.87 0.82 0.8 0.6 0.84 0.75 MAP 0.7 man, 2010) or syntactic and semantic contextual features (He and Grishman, 2015) to extract and classify new instances. Limited to the sparse supervision, previous work estimate patterns mainly based on its direct extraction features, e.g., the matching statistics with known entities (Riloff and Jones, 1999; Agichtein and Gravano, 2000), which often suffers from the semantic drift problem. To avoid semantic drift, most existing approaches exploit extra constraints, such as parallel multiple categories (Thelen and Riloff, 2002; Yangarber, 2003; McIntosh, 2010), coupling constraints (Carlson et al., 2010), and mutual exclusion bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008). Besides, graph-based methods (Li et al., 2011; Tao et al., 2015) and the probability-based method (Shi et al., 2014) are also used to improve the bootstrapping performance. To address the sparse supervision problem, many previous studies score entities by leveraging lexical and statistical features (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Pantel and Pennacchiotti, 2006; Pas¸ca, 2007; Pantel et al., 2009), which, despite the promisin"
D19-1028,D14-1162,0,0.0823474,"we set k to a reasonably large value, i.e., 200, to balance between efficiency and effectiveness. 3.3 4.1 In this section, we first describe how to embed patterns; then, we introduce how to obtain the basic distributional pattern embeddings for uniformly representing entities and patterns without adaptation; finally, we introduce the adaptation mechanism combined with the MCTS algorithm. Pattern Embedding. As a basic step of our PMSN model, we first embed a context pattern of an entity as a single embedding vector. Specifically, we use the average word embeddings, from the pre-trained GloVe (Pennington et al., 2014) embeddings, of a pattern surface text as the pattern embeddings. We filter out patterns containing at least two OOV terms. According to our pilot experiments, replacing the average GloVe word embeddings with alternatives such as Convolutional Neural Network and Recurrent Neural Network does not influence performance. Reward Function in MCTS The reward function is critical for efficiently estimating the real delayed feedback of each pattern. Intuitively, a pattern should have higher delayed feedback if it extracts more similar entities and less unrelated entities. Base on this intuition, we de"
D19-1028,C00-2136,0,0.206815,"oid semantic drift, most existing approaches exploit extra constraints, such as parallel multiple categories (Thelen and Riloff, 2002; Yangarber, 2003; McIntosh, 2010), coupling constraints (Carlson et al., 2010), and mutual exclusion bootstrapping (Curran et al., 2007; McIntosh and Curran, 2008). Besides, graph-based methods (Li et al., 2011; Tao et al., 2015) and the probability-based method (Shi et al., 2014) are also used to improve the bootstrapping performance. To address the sparse supervision problem, many previous studies score entities by leveraging lexical and statistical features (Yangarber et al., 2000; Stevenson and Greenwood, 2005; Pantel and Pennacchiotti, 2006; Pas¸ca, 2007; Pantel et al., 2009), which, despite the promising effectiveness, could often fail since the sparse statistical features provide little semantic information to evaluate entities. Recently word embedding based methods (Batista et al., 2015; Gupta and Manning, 2015) use fixed word embedding learned on external resources and evaluate entities by their similarity to seeds. Recently, Berger et al. (2018) propose to learn custom embeddings at each bootstrapping iteration, to trade efficiency for effectiveness. 0.69 10 20"
D19-1028,D15-1203,0,0.0210253,"tegories. 6 Related Work Entity set expansion (ESE) is a weakly supervised task, which is often given seed entities as supervision and tries to expand new entities related to them. According to the used corpus, there are two types of ESE: limited corpus (Shi et al., 2014; Shen et al., 2017) and large open corpus, e.g. making use of a search engine for the web search (Wang and Cohen, 2007). Weakly supervised methods for information extraction (IE) are often provided insufficient supervision signals, such as knowledge base facts as distant supervision (Mintz et al., 2009; Hoffmann et al., 2011; Zeng et al., 2015; Han and Sun, 2016), and light amount of supervision samples in bootstrapping(Riloff and Jones, 1999). As a classical technique, bootstrapping usually exploits pattern (Curran et al., 2007), document (Liao and Grish7 Conclusions In this paper, we propose a deep similarity network-based model combined with the MCTS algorithm to bootstrap Entity Set Expansion. Specifically, we leverage the Monte Carlo Tree Search (MCTS) algorithm to efficiently estimate the delayed feedback of each pattern in the bootstrapping; we propose a Pattern Mover Similarity Network (PMSN) to uniformly embed entities 299"
D19-1028,C14-1215,1,0.672891,"Missing"
D19-1646,W03-0420,0,0.571594,"Missing"
D19-1646,C02-1025,0,0.619048,"Missing"
D19-1646,W05-1303,0,0.0788529,"model. Sohrab and Miwa (2018) extended their method by introducing a new region encoder. Generally, these methods have achieved promising results but heavily rely on fully-annotated data. Gazetteers or dictionaries have long been regarded as a useful and easily-obtainable resource for NER. Previous methods commonly incorporated gazetteers by either using them to as handcraft features (Bender et al., 2003; Tsuruoka and Tsujii, 2003; Ciaramita and Altun, 2005; Minkov et al., 2005; Ritter et al., 2011; Seyler et al., 2018; Yu et al., 2018), or using them to generate data by distant supervision (Cohen, 2005; Ren et al., 2015; Giannakopoulos et al., 2017; Shang et al., 2018). However, the first kind of methods can not fully 6235 leverage the inner mention structure knowledge entailing in gazetteers, while the second approaches will result in remarkable noise. 6 Conclusions This paper first proposes attentive neural networks, an effective region-based model which explicitly models mention-context association. Then we propose to incorporate an auxiliary gazetteer network to enhance ANN. The gazetteer network can effectively learn name knowledge only using easily-available gazetteers, and therefore"
D19-1646,D09-1015,0,0.254869,"Missing"
D19-1646,W17-5224,0,0.0148448,"extended their method by introducing a new region encoder. Generally, these methods have achieved promising results but heavily rely on fully-annotated data. Gazetteers or dictionaries have long been regarded as a useful and easily-obtainable resource for NER. Previous methods commonly incorporated gazetteers by either using them to as handcraft features (Bender et al., 2003; Tsuruoka and Tsujii, 2003; Ciaramita and Altun, 2005; Minkov et al., 2005; Ritter et al., 2011; Seyler et al., 2018; Yu et al., 2018), or using them to generate data by distant supervision (Cohen, 2005; Ren et al., 2015; Giannakopoulos et al., 2017; Shang et al., 2018). However, the first kind of methods can not fully 6235 leverage the inner mention structure knowledge entailing in gazetteers, while the second approaches will result in remarkable noise. 6 Conclusions This paper first proposes attentive neural networks, an effective region-based model which explicitly models mention-context association. Then we propose to incorporate an auxiliary gazetteer network to enhance ANN. The gazetteer network can effectively learn name knowledge only using easily-available gazetteers, and therefore can significantly improve model performance and"
D19-1646,N18-1079,0,0.155297,"tity types. To incorporate such knowledge into ANN, we simply concatenate the representation learned by gazetteer network and the representation learned by the original inner-region encoder. Then this new representation is fed to the following modules of ANN. In this way, name knowledge learned from gazetteers is incorporated to enhance the region encoder, and the requirement of fullyannotated data can be reduced. 4 Experiments 4.1 Experimental Settings Data Preparation. We conducted experiments on ACE2005 named entity recognition task1 . We used the same dataset splits as Wang and Lu (2018); Katiyar and Cardie (2018). For each entity type in ACE2005, we collect a gazetteer from 1 Conventional NER datasets, such as CoNLL2003, removed nested entity mentions and therefore are not suitable for evaluating region-based NER models. LSTM-CRF Neural Transition Segmental Hypergraph Exhaustive ANN GEANN P 73.2 75.2 75.7 81.2 78.9 77.1 R 61.3 65.5 68.3 66.9 69.8 73.3 F1 66.7 70.0 71.8 73.4 74.1 75.2 Table 1: Experiment results on ACE2005 named entity mention recognition. Wikipedia anchor texts, i.e., anchor texts linking to an entity whose type are the same will be included in a gazetteer. The same as previous studie"
D19-1646,N16-1030,0,0.59781,"first work trying to explicitly exploit mention-context association with attention mechanism in region-based NER, as well as the first work which enhances NER model with name knowledge captured from gazetteers using neural networks. 2 Attentive Neural Network for NER This section describes our attentive neural network, which directly classifies over all subsequences of a sentence to recognize whether each subsequence corresponds to an entity mention. Figure 1 (a) shows the architecture of ANN. Given a sentence, ANN first maps all words into word representations {x1 , x2 , ..., xn } following Lample et al. (2016). Then a BiLSTM layer is used to obtain context-aware word representation hA t . After that, for each candidate region sij , we follow Sohrab and Miwa (2018) to use an innerregion encoder to obtain its representation sij , which captures name knowledge considering both its boundary and inside information: sij = MLP([hA i ; j X 1 A hA k ; hj ]), j−i+1 (1) k=i where MLP is a multi-layer perceptron. To explicitly model the association between a region and its context, we design an attentive context encoder, which outputs a contextual vector cij entailing the context knowledge of sij by: cij = i−1"
D19-1646,P19-1511,1,0.834923,"Missing"
D19-1646,D15-1102,0,0.0490893,"ation, such as ELMOs (Peters et al., 2018) and BERT (Devlin et al., 2018), have shown significant progress in many NLP tasks, especially in low-resource cases. To verify the adaptivity of the proposed GEANN, we further introduce BERT into models by replacing the word embedding with BERT representations. Figure 3 shows the results. We can see Related Work Sequential labeling approaches (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004; Lample et al., 2016) are widely used in NER. But this paradigm cannot handle nested mentions without specially designed tagging schema (Lu and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018; Lin et al., 2019). Recently, region-based models provide a natural solution for this issue. Finkel and Manning (2009) first proposed to classify over regions corresponding to parsing tree nodes. Xu et al. (2017) proposed to directly classify over all subsequences with a neural network model. Sohrab and Miwa (2018) extended their method by introducing a new region encoder. Generally, these methods have achieved promising results but heavily rely on fully-annotated data. Gazetteers or dictionaries have long been regarded as a useful and easily-obtai"
D19-1646,H05-1056,0,0.0607659,"regions corresponding to parsing tree nodes. Xu et al. (2017) proposed to directly classify over all subsequences with a neural network model. Sohrab and Miwa (2018) extended their method by introducing a new region encoder. Generally, these methods have achieved promising results but heavily rely on fully-annotated data. Gazetteers or dictionaries have long been regarded as a useful and easily-obtainable resource for NER. Previous methods commonly incorporated gazetteers by either using them to as handcraft features (Bender et al., 2003; Tsuruoka and Tsujii, 2003; Ciaramita and Altun, 2005; Minkov et al., 2005; Ritter et al., 2011; Seyler et al., 2018; Yu et al., 2018), or using them to generate data by distant supervision (Cohen, 2005; Ren et al., 2015; Giannakopoulos et al., 2017; Shang et al., 2018). However, the first kind of methods can not fully 6235 leverage the inner mention structure knowledge entailing in gazetteers, while the second approaches will result in remarkable noise. 6 Conclusions This paper first proposes attentive neural networks, an effective region-based model which explicitly models mention-context association. Then we propose to incorporate an auxiliary gazetteer network t"
D19-1646,N18-1202,0,0.0737391,"Missing"
D19-1646,D11-1141,0,0.201647,"Missing"
D19-1646,W04-1221,0,0.16822,"Missing"
D19-1646,P18-2039,0,0.015429,"es. Xu et al. (2017) proposed to directly classify over all subsequences with a neural network model. Sohrab and Miwa (2018) extended their method by introducing a new region encoder. Generally, these methods have achieved promising results but heavily rely on fully-annotated data. Gazetteers or dictionaries have long been regarded as a useful and easily-obtainable resource for NER. Previous methods commonly incorporated gazetteers by either using them to as handcraft features (Bender et al., 2003; Tsuruoka and Tsujii, 2003; Ciaramita and Altun, 2005; Minkov et al., 2005; Ritter et al., 2011; Seyler et al., 2018; Yu et al., 2018), or using them to generate data by distant supervision (Cohen, 2005; Ren et al., 2015; Giannakopoulos et al., 2017; Shang et al., 2018). However, the first kind of methods can not fully 6235 leverage the inner mention structure knowledge entailing in gazetteers, while the second approaches will result in remarkable noise. 6 Conclusions This paper first proposes attentive neural networks, an effective region-based model which explicitly models mention-context association. Then we propose to incorporate an auxiliary gazetteer network to enhance ANN. The gazetteer network can e"
D19-1646,P17-1114,0,0.122946,"han.jiang}@srcb.ricoh.com 1 Abstract Handcraft Training Data Inner-Region Encoder a Attentive Context Encoder Utterance Encoder Candidate Region Sentence Utterance (a) Attentive Neural Network Easily Available Gazetteer George Washington (b) Gazetteer Network Figure 1: The overall architecture of GEANN. The candidate region is “George Washington”, which literally could be a person or an organization (university). Named entity recognition (NER), aiming to identify text mentions of specific entity types, is a fundamental NLP task. Recently, region-based NER approaches (Finkel and Manning, 2009; Xu et al., 2017; Sohrab and Miwa, 2018) have attracted significant attention, which first encode all candidate regions (commonly all subsequences of a sentence) using a region encoder, then identify whether each subsequence is an entity mention of target types using a classifier. For example, in Figure 1 all subsequences of the sentence, such as “George Washington”, will first be encoded, Xianpei Han is the corresponding author. Utterance Representation Region Representation [ George Washington ] was an leader. Introduction ∗ ORG … … Current region-based NER models only rely on fully-annotated training data"
D19-1646,D18-1345,0,0.02491,"proposed to directly classify over all subsequences with a neural network model. Sohrab and Miwa (2018) extended their method by introducing a new region encoder. Generally, these methods have achieved promising results but heavily rely on fully-annotated data. Gazetteers or dictionaries have long been regarded as a useful and easily-obtainable resource for NER. Previous methods commonly incorporated gazetteers by either using them to as handcraft features (Bender et al., 2003; Tsuruoka and Tsujii, 2003; Ciaramita and Altun, 2005; Minkov et al., 2005; Ritter et al., 2011; Seyler et al., 2018; Yu et al., 2018), or using them to generate data by distant supervision (Cohen, 2005; Ren et al., 2015; Giannakopoulos et al., 2017; Shang et al., 2018). However, the first kind of methods can not fully 6235 leverage the inner mention structure knowledge entailing in gazetteers, while the second approaches will result in remarkable noise. 6 Conclusions This paper first proposes attentive neural networks, an effective region-based model which explicitly models mention-context association. Then we propose to incorporate an auxiliary gazetteer network to enhance ANN. The gazetteer network can effectively learn n"
D19-1646,P02-1060,0,0.394281,"Missing"
D19-1646,D18-1230,0,0.0741387,"oducing a new region encoder. Generally, these methods have achieved promising results but heavily rely on fully-annotated data. Gazetteers or dictionaries have long been regarded as a useful and easily-obtainable resource for NER. Previous methods commonly incorporated gazetteers by either using them to as handcraft features (Bender et al., 2003; Tsuruoka and Tsujii, 2003; Ciaramita and Altun, 2005; Minkov et al., 2005; Ritter et al., 2011; Seyler et al., 2018; Yu et al., 2018), or using them to generate data by distant supervision (Cohen, 2005; Ren et al., 2015; Giannakopoulos et al., 2017; Shang et al., 2018). However, the first kind of methods can not fully 6235 leverage the inner mention structure knowledge entailing in gazetteers, while the second approaches will result in remarkable noise. 6 Conclusions This paper first proposes attentive neural networks, an effective region-based model which explicitly models mention-context association. Then we propose to incorporate an auxiliary gazetteer network to enhance ANN. The gazetteer network can effectively learn name knowledge only using easily-available gazetteers, and therefore can significantly improve model performance and reduce data requirem"
D19-1646,D18-1309,0,0.155252,"Missing"
D19-1646,W03-1306,0,0.132758,"kel and Manning (2009) first proposed to classify over regions corresponding to parsing tree nodes. Xu et al. (2017) proposed to directly classify over all subsequences with a neural network model. Sohrab and Miwa (2018) extended their method by introducing a new region encoder. Generally, these methods have achieved promising results but heavily rely on fully-annotated data. Gazetteers or dictionaries have long been regarded as a useful and easily-obtainable resource for NER. Previous methods commonly incorporated gazetteers by either using them to as handcraft features (Bender et al., 2003; Tsuruoka and Tsujii, 2003; Ciaramita and Altun, 2005; Minkov et al., 2005; Ritter et al., 2011; Seyler et al., 2018; Yu et al., 2018), or using them to generate data by distant supervision (Cohen, 2005; Ren et al., 2015; Giannakopoulos et al., 2017; Shang et al., 2018). However, the first kind of methods can not fully 6235 leverage the inner mention structure knowledge entailing in gazetteers, while the second approaches will result in remarkable noise. 6 Conclusions This paper first proposes attentive neural networks, an effective region-based model which explicitly models mention-context association. Then we propose"
D19-1646,D18-1019,0,0.279394,"ledge of specific entity types. To incorporate such knowledge into ANN, we simply concatenate the representation learned by gazetteer network and the representation learned by the original inner-region encoder. Then this new representation is fed to the following modules of ANN. In this way, name knowledge learned from gazetteers is incorporated to enhance the region encoder, and the requirement of fullyannotated data can be reduced. 4 Experiments 4.1 Experimental Settings Data Preparation. We conducted experiments on ACE2005 named entity recognition task1 . We used the same dataset splits as Wang and Lu (2018); Katiyar and Cardie (2018). For each entity type in ACE2005, we collect a gazetteer from 1 Conventional NER datasets, such as CoNLL2003, removed nested entity mentions and therefore are not suitable for evaluating region-based NER models. LSTM-CRF Neural Transition Segmental Hypergraph Exhaustive ANN GEANN P 73.2 75.2 75.7 81.2 78.9 77.1 R 61.3 65.5 68.3 66.9 69.8 73.3 F1 66.7 70.0 71.8 73.4 74.1 75.2 Table 1: Experiment results on ACE2005 named entity mention recognition. Wikipedia anchor texts, i.e., anchor texts linking to an entity whose type are the same will be included in a gazetteer."
D19-1646,D18-1124,0,0.0868387,"Missing"
D19-3012,D13-1160,0,0.0460888,") to divide an utterance into a sequence of tokens. Then, a string-based entity linking algorithm (Blanco et al., 2015) is utilized to link the tokens with the domain lexicons, which generate the candidate name entities for semantic parsing. Finally, we use Stanford CoreNLP (Manning et al., 2014) to generate the part-of-speech tags (POS) and constituency parse tree for the given sequence of tokens. Related Work Semantic parsing can benefit to many intelligent applications, like intelligent retrieval, personal assistant, etc. There exists a range of semantic parsing toolkits, such as SEMPRE 1 (Berant et al., 2013) and RASA NLU 2 . Unfortunately, most of these toolkits require both linguistic expertise and a large amount of annotated data. CRUISE (Shen et al., 2018) provide an utterance generation system to reduce the human workload of data annotation. However, CRUISE focuses on spoken language understanding. In this paper, we present a platform for building semantic parsing system quickly and easily. 3 EUSP Workflow 3.3 Semantic parser component The semantic parser component consists of two kinds of semantic parsing methods, we will illustrate them in this section. 3.3.1 Grammar-based Semantic Parser T"
D19-3012,P18-1071,1,0.898009,"Missing"
D19-3012,W02-1001,0,0.0796191,"telligent retrieval, recommendation) without training data. If the developers have enough training data, the grammar-based semantic parser can be further improved by optimizing the score of lexicon entries, the weight vector Wparser and Wrank . The https://pinyin.sogou.com/dict/ 69 Token TokenType #Q:QueryValue #N:NormalizedTokenValue #D:DataType #S:Score Figure 3: The format of lexicons. tokens constraints generate construct Figure 4: The framework of Seq2Action. Figure 5: The UI of grammar-based semantic parser. system uses a mini-batch gradient-based discriminant online learning algorithm (Collins, 2002). The training process is as Formula (3). g(S) = F (R) − F (T ) g(S1 , ..., Sk ) = (g(S1 ) + ... + g(Sk ))/k (3) Wt+1 = Wt − α ∗ g(S1 , ..., Sk ) where F (T ) is the annotated result of an utterance S; k is the number of instances in a mini-batch; Wt is the parameters at t-th iteration; α is the learning rate and g is the gradient which is calculated based on SGD. Figure 6: The UI of neural-based semantic parser. could quickly build a domain semantic parser accord to the instructions of the toolkit with the domain dependent lexicon. Furthermore, our platform provides a more easy-to-use way to"
D19-3012,P16-1004,0,0.181435,"exicons to generate meaning representations for a given utterance. The grammar is a set of expert defined rules to compose the semantic units into candidate meaning representations, which is based on the principle of compositionality (Pelletier, 1994). However, to implement grammar-based semantic parsing system the developers have to understand the complex grammar. What’s worse, these parsers require an amount of training dataset that is hard to annotate and only work in a specific domain. Neural semantic parsers convert the utterance directly to meaning representations, like lambda-calculus (Dong and Lapata, 2016) and semantic graph (Chen et al., 2018). One of the major advantages of neural semantic parsing is that the model is trained in an end-to-end way without requiring the developers to understand the complex theory. Unfortunately, neural semantic parser requires a large amount of training data to achieve competitive performance. Thus, it is significantly and crucially desirable to develop a platform for helping developers build semantic parsing systems without requiring complex grammar or costly training data. To address the above challenges, we present an easy-to-use semantic parsing platform (E"
D19-3012,P18-1068,0,0.0381464,"Missing"
D19-3012,P14-5010,0,0.00350739,"(grammar-based and neuralbased). • Cold-Start and Continuous Optimizable: the grammar-based parser only needs domain lexicons, and both of grammar-based and neural-based semantic parsers can be optimized with training data. • Plug and play: the generated semantic parser is an independent module and can be plugged in the original system without much modification. And it could produce various formats of outputs, like lambda-calculus and SparQL (Sirin and Parsia, 2007), etc. 2 3.2 Preprocessor component To extract useful information for semantic parsing, EUSP firstly employs Stanford Tokenizer (Manning et al., 2014) to divide an utterance into a sequence of tokens. Then, a string-based entity linking algorithm (Blanco et al., 2015) is utilized to link the tokens with the domain lexicons, which generate the candidate name entities for semantic parsing. Finally, we use Stanford CoreNLP (Manning et al., 2014) to generate the part-of-speech tags (POS) and constituency parse tree for the given sequence of tokens. Related Work Semantic parsing can benefit to many intelligent applications, like intelligent retrieval, personal assistant, etc. There exists a range of semantic parsing toolkits, such as SEMPRE 1 (B"
D19-3012,P18-4018,0,0.0285829,"the domain lexicons, which generate the candidate name entities for semantic parsing. Finally, we use Stanford CoreNLP (Manning et al., 2014) to generate the part-of-speech tags (POS) and constituency parse tree for the given sequence of tokens. Related Work Semantic parsing can benefit to many intelligent applications, like intelligent retrieval, personal assistant, etc. There exists a range of semantic parsing toolkits, such as SEMPRE 1 (Berant et al., 2013) and RASA NLU 2 . Unfortunately, most of these toolkits require both linguistic expertise and a large amount of annotated data. CRUISE (Shen et al., 2018) provide an utterance generation system to reduce the human workload of data annotation. However, CRUISE focuses on spoken language understanding. In this paper, we present a platform for building semantic parsing system quickly and easily. 3 EUSP Workflow 3.3 Semantic parser component The semantic parser component consists of two kinds of semantic parsing methods, we will illustrate them in this section. 3.3.1 Grammar-based Semantic Parser The framework of grammar-based semantic parser is illustrated in Figure 2, it consists of four modules: lexicon, grammar, scorer and reranker. Lexicon modu"
D19-3012,D07-1071,0,0.0780783,"eq2Action Models Seq2Act Seq2Act (+C1) Seq2Act (+C1+C2) ify its value. 5.1 Neural-based Semantic Parser Results We assess the performance of our method and compare it with previous methods. We conduct experiments on two datasets: GEO and ATIS. GEO contains natural language questions about 494 US geography paired with corresponding Prolog database queries. Following (Zettlemoyer and Collins, 2005), we use the standard 600/280 instance splits for training/test. ATIS contains natural language questions of a flight database, with each question is annotated with a lambda calculus query. Following (Zettlemoyer and Collins, 2007), we use the standard 4473/448 instance splits for training/test. We use 200 hidden units and 100 dimensional word vectors for sentence encoding. And we initialize all parameters by uniformly sampling within the interval [−0.1, 0.1]. We train our model for a total of 30 epochs with an initial learning rate of 0.1, and halve the learning rate every 5 epochs after epoch 15. We replace word vectors for words occurring only once with a universal word vector. We evaluate different systems using the standard accuracy metric, and the accuracies on different datasets are obtained. GEO ATIS 85.0 89.3 8"
I08-2087,N04-1035,0,0.0617475,"Missing"
I08-2087,P03-1011,0,0.0233606,"approach for SMT is promising for its strong ability at combining words. 1 Introduction Statistical Machine Translation (SMT) is attracting more attentions than rule-based and examplebased methods because of the availability of large training corpora and automatic techniques. However, rich language structure is difficult to be integrated in the current SMT framework. Most of the SMT approaches integrating syntactic structures are based on probabilistic tree transducers (treeto-tree model). This leads to a large increase in the model complexity (Yamada and Knight 2001; Yamada and Knight 2002; Gildea 2003; Galley et al. 2004; Knight and Graehl 2005; Liu et al. 2006). However, formally syntax-based methods propose simple but efficient ways to parse and translate sentences (Wu 1997; Chiang 2005). In this paper, we propose a new model of SMT by using structured prediction to perform tree-totree transductions. This model is inspired by Sagae and Lavie (2005), in which a stack-based rep649 Wenbo Li* †Institute of Software Chinese Academy of Sciences Beijing, China, 100080 sunle@iscas.cn resentation of monolingual parsing trees is used. Our contributions lie in the extension of this representation t"
I08-2087,J99-4005,0,0.0680466,"Missing"
I08-2087,koen-2004-pharaoh,0,0.0195058,"solve is a tagging problem similar to a POS tagging (Daumé III et al. 2006). The input sequence x is word pairs and output y is the group of SHIFT and REDUCE operations. For sequence labeling problem, the standard loss function is Hamming distance, which measures the difference between the true output and the predicting one: (1) HL( y, yˆ ) = ∑ δ ( yt , yˆ t ) t where δ is 0 if two variables are equal, and 1 otherwise. 5 Decoder We use a left-to-right beam search decoder to find the best translation given a source sentence. Compared with general phrase-based beam search decoder like Pharaoh (Koehn 2004), this decoder integrates structured information and does not need distortion cost and other costs (e.g. future costs) any more. Therefore, the best translation can be determined by: e* = arg max{ p( f |e) plm (e)ω length (e ) } (2) e where ω is a factor of word length penalty. Similarly, the translation probability p ( f |e) can be further decomposed into: (3) p( f |e) = ∏φ ( f i |ei ) i and φ ( f i |ei ) represents the probability distribution of word pairs. Instead of extracting all possible phrases from word alignments, we consider those translation pairs from the nodes of ITG-like trees o"
I08-2087,W06-3114,0,0.02765,"Missing"
I08-2087,N03-1017,0,0.0148264,"Missing"
I08-2087,P06-1096,0,0.0323536,"Missing"
I08-2087,P06-1077,0,0.0305403,"Missing"
I08-2087,P03-1021,0,0.00548649,"bability p ( f |e) can be further decomposed into: (3) p( f |e) = ∏φ ( f i |ei ) i and φ ( f i |ei ) represents the probability distribution of word pairs. Instead of extracting all possible phrases from word alignments, we consider those translation pairs from the nodes of ITG-like trees only. Like Pharaoh, we calculate their probability as a combination of 5 constituents: phrase translation probability (in both directions), lexical translation probability (in both directions) and phrase penalty (default is set at 2.718). The corresponding weight is trained through minimum error rate method (Och 2003). Parameters of this part can be calculated in advance once tree structures are generated and can be stored as phrase translation table. 5.1 5.2 Recombining and Pruning Different translation options can combine to form the same fragment by beam search decoder. Recombining is therefore needed here to reduce the search space. So, only the one with the lowest cost is kept when several fragments are identical. This recombination is a risk-free operation to improve searching efficiency. Another pruning method used in our system is histogram pruning. Only n-best translations are Core Algorithm Anoth"
I08-2087,J03-1002,0,0.00321509,"t al. 2005) to produce binarized tree alignments. In our method, we predict tree structures from word alignments through several transformations without involving parser and/or tree alignments. 3 f1 f2 e2 f2/e2 (2b) S f2/e2 S,R+ (1c) (1d) f2 * * f1/e1 f1/e2 f2/e1 (2c) f1/e2 S f2/e1 S,R(2d) Figure 2: Two basic representations for tree items Figure 3: “inside-out” transpositions (a) and (b) with two typical complex sequences (c) and (d). In (c) and (d), word correspondence f2-e2 is also extracted as sub-alignments. First, following Koehn et al. (2003), bilingual sentences are trained by GIZA++ (Och and Ney 2003) in two directions (from source to target and target to source). Then, two resulting alignments are recombined to form a whole according to heuristic rules, e.g. grow-diag-final. Second, based on the word alignment matrix, one unique parsing tree can be generated according to ITG constraints where the “left-first” constraint is posed. That is to say, we always make the leaf nodes as the right 650 The two widely known situations that cannot be described by ITGs are called “inside-out” transpositions (Figure 3 a & b). Since they cannot be decomposed in ITGs, we consider them as basic units. In t"
I08-2087,W05-1513,0,0.184403,"he current SMT framework. Most of the SMT approaches integrating syntactic structures are based on probabilistic tree transducers (treeto-tree model). This leads to a large increase in the model complexity (Yamada and Knight 2001; Yamada and Knight 2002; Gildea 2003; Galley et al. 2004; Knight and Graehl 2005; Liu et al. 2006). However, formally syntax-based methods propose simple but efficient ways to parse and translate sentences (Wu 1997; Chiang 2005). In this paper, we propose a new model of SMT by using structured prediction to perform tree-totree transductions. This model is inspired by Sagae and Lavie (2005), in which a stack-based rep649 Wenbo Li* †Institute of Software Chinese Academy of Sciences Beijing, China, 100080 sunle@iscas.cn resentation of monolingual parsing trees is used. Our contributions lie in the extension of this representation to bilingual parsing trees based on ITGs and in the use of a structured prediction method, called SEARN (Daumé III et al. 2007), to predict parsing structures. Furthermore, in order to facilitate the use of structured prediction method, we perform another transformation from ITG-like trees to label sequence with the grouping of stack operations. Then the"
I08-2087,P05-1069,0,0.0202747,"matrix. They can be described using ITG-like trees (c). tion from word alignments to structured parsing trees and then to label sequence are given in section 3. The structured prediction method is described in section 4. In section 5, a beam search decoder with structured information is described. Experiments are given for three European language pairs in section 6 and we conclude our paper with some discussions. f1 e1 e1 2 Related Work Transformation 3.1 Word Alignments and ITG-like Tree e2 f1 * f1 e1 e2 (2a) f1/e1 (1b) f2 e1 f2 * e2 (1a) This method is similar to block-orientation modeling (Tillmann and Zhang 2005) and maximum entropy based phrase reordering model (Xiong et al. 2006), in which local orientations (left/right) of phrase pairs (blocks) are learned via MaxEnt classifiers. However, we assign shift/reduce labeling of ITGs taken from the shift-reduce parsing, and classifier is learned via SEARN. This paper is more elaborated by assigning detailed stackoperations. The use of structured prediction to SMT is also investigated by (Liang et al. 2006; Tillmann and Zhang 2006; Watanabe et al. 2007). In contrast, we use SEARN to estimate one bilingual parsing tree for each sentence pair from its word"
I08-2087,P06-1091,0,0.018206,"s and ITG-like Tree e2 f1 * f1 e1 e2 (2a) f1/e1 (1b) f2 e1 f2 * e2 (1a) This method is similar to block-orientation modeling (Tillmann and Zhang 2005) and maximum entropy based phrase reordering model (Xiong et al. 2006), in which local orientations (left/right) of phrase pairs (blocks) are learned via MaxEnt classifiers. However, we assign shift/reduce labeling of ITGs taken from the shift-reduce parsing, and classifier is learned via SEARN. This paper is more elaborated by assigning detailed stackoperations. The use of structured prediction to SMT is also investigated by (Liang et al. 2006; Tillmann and Zhang 2006; Watanabe et al. 2007). In contrast, we use SEARN to estimate one bilingual parsing tree for each sentence pair from its word correspondences. As a consequence, the generation of target language sentences is assisted by this structured information. Turian et al. (2006) propose a purely discriminative learning method for parsing and translation with tree structured models. The word alignments and English parse tree were fed into the GenPar system (Burbank et al. 2005) to produce binarized tree alignments. In our method, we predict tree structures from word alignments through several transforma"
I08-2087,D07-1080,0,0.0266152,"Missing"
I08-2087,J97-3002,0,0.0335171,"lebased methods because of the availability of large training corpora and automatic techniques. However, rich language structure is difficult to be integrated in the current SMT framework. Most of the SMT approaches integrating syntactic structures are based on probabilistic tree transducers (treeto-tree model). This leads to a large increase in the model complexity (Yamada and Knight 2001; Yamada and Knight 2002; Gildea 2003; Galley et al. 2004; Knight and Graehl 2005; Liu et al. 2006). However, formally syntax-based methods propose simple but efficient ways to parse and translate sentences (Wu 1997; Chiang 2005). In this paper, we propose a new model of SMT by using structured prediction to perform tree-totree transductions. This model is inspired by Sagae and Lavie (2005), in which a stack-based rep649 Wenbo Li* †Institute of Software Chinese Academy of Sciences Beijing, China, 100080 sunle@iscas.cn resentation of monolingual parsing trees is used. Our contributions lie in the extension of this representation to bilingual parsing trees based on ITGs and in the use of a structured prediction method, called SEARN (Daumé III et al. 2007), to predict parsing structures. Furthermore, in ord"
I08-2087,P06-1066,0,0.0298002,"Missing"
I08-2087,P01-1067,0,0.0433401,"Experiments show that the structured prediction approach for SMT is promising for its strong ability at combining words. 1 Introduction Statistical Machine Translation (SMT) is attracting more attentions than rule-based and examplebased methods because of the availability of large training corpora and automatic techniques. However, rich language structure is difficult to be integrated in the current SMT framework. Most of the SMT approaches integrating syntactic structures are based on probabilistic tree transducers (treeto-tree model). This leads to a large increase in the model complexity (Yamada and Knight 2001; Yamada and Knight 2002; Gildea 2003; Galley et al. 2004; Knight and Graehl 2005; Liu et al. 2006). However, formally syntax-based methods propose simple but efficient ways to parse and translate sentences (Wu 1997; Chiang 2005). In this paper, we propose a new model of SMT by using structured prediction to perform tree-totree transductions. This model is inspired by Sagae and Lavie (2005), in which a stack-based rep649 Wenbo Li* †Institute of Software Chinese Academy of Sciences Beijing, China, 100080 sunle@iscas.cn resentation of monolingual parsing trees is used. Our contributions lie in t"
I08-2087,P02-1039,0,0.0251061,"he structured prediction approach for SMT is promising for its strong ability at combining words. 1 Introduction Statistical Machine Translation (SMT) is attracting more attentions than rule-based and examplebased methods because of the availability of large training corpora and automatic techniques. However, rich language structure is difficult to be integrated in the current SMT framework. Most of the SMT approaches integrating syntactic structures are based on probabilistic tree transducers (treeto-tree model). This leads to a large increase in the model complexity (Yamada and Knight 2001; Yamada and Knight 2002; Gildea 2003; Galley et al. 2004; Knight and Graehl 2005; Liu et al. 2006). However, formally syntax-based methods propose simple but efficient ways to parse and translate sentences (Wu 1997; Chiang 2005). In this paper, we propose a new model of SMT by using structured prediction to perform tree-totree transductions. This model is inspired by Sagae and Lavie (2005), in which a stack-based rep649 Wenbo Li* †Institute of Software Chinese Academy of Sciences Beijing, China, 100080 sunle@iscas.cn resentation of monolingual parsing trees is used. Our contributions lie in the extension of this rep"
I08-2087,P05-1033,0,\N,Missing
I08-4020,W06-0132,1,0.549935,"Missing"
I11-1159,W06-3812,0,0.0258234,"ch has a good performance in public evaluation (e.g. Semeval-2007 task 02) (Agirre and Soroa, 2007). The second one is also a vector-based WSI approach which represents the contexts of a target word using bag-of-words vectors and weights each feature (word) based on its TF and IDF. The two vector-based WSI approaches use k-means algorithm to cluster the context vectors of target words and the maximum number of k-means iterations is set to 100. The third one is a graph-based WSI approach. We build a graph according to the approach described in (Agirre et al., 2006). Chinese Whispers algorithm (Biemann, 2006) is used to cluster the graph. The maximum number of Chinese Whispers iterations is set to 100. We also include the “one cluster per word” baseline (1c1w), where all instances of a target word are grouped into a single cluster. In SemEval-2010 task 14, none of the participating systems outperform this baseline in paired F-score (Artiles et al., 2009), which indicates that this baseline is quite strong. According to (Pedersen, 2010), we employ paired F-score as evaluation measure. Let C={Cj|j=1,2,…,n} be a set of clusters generated by a WSI system and S={Gi|i=1,2,…,m} be the set of gold standar"
I11-1159,H05-1097,0,0.0312792,"er we propose a WSI method which can exploit semantic relevance between words by incorporating a word graph into the framework of clustering of context vectors. The method is evaluated on the testing data of the Chinese Word Sense Induction task of the first CIPSSIGHAN Joint Conference on Chinese Language Processing (CLP2010). Experimental results show that our method significantly outperforms the baseline methods. 1 Introduction It has been shown that using word senses instead of surface word forms can improve performance on many natural language processing tasks such as machine translation (Vickrey et al., 2005) and information retrieval (Uzuner et al., 1999; Véronis, 2004). Historically, using word senses usually involved the use of manually compiled resources in which word senses were represented as a fixed list of definitions. However, there seem to be some disadvantages associated with such fixed list of senses paradigm. Firstly, since dictionaries usually contain general definitions, they can not reflect the exact contents of the contexts where target words appear (Véronis, 2004). Secondly, because the “fixed list of senses” paradigm makes the fixed granularity assumption Le Sun Institute of Sof"
I11-1159,E09-1013,0,0.0212167,"rd senses were represented as a fixed list of definitions. However, there seem to be some disadvantages associated with such fixed list of senses paradigm. Firstly, since dictionaries usually contain general definitions, they can not reflect the exact contents of the contexts where target words appear (Véronis, 2004). Secondly, because the “fixed list of senses” paradigm makes the fixed granularity assumption Le Sun Institute of Software, Chinese Academy of Sciences, Beijing, China sunle@iscas.ac.cn of the senses distinction, it may not be suitable in different applications (Kilgarriff, 1997; Brody and Lapata, 2009). To overcome these limitations, some techniques like WSI have been proposed for discovering words senses automatically from unannoteted corpuses. WSI algorithms are usually based on the Distributional Hypothesis which shows that words with similar meanings appear in similar contexts (Harris, 1954). This concept can be leveraged to induce different senses of a target word by clustering the contexts where the target word appears. Much work in WSI is based on the vector space model, in which each context of a target word is represented by a vector of selected features (e.g. the words occurring i"
I11-1159,S10-1081,0,0.0149348,"s set to 100. The third one is a graph-based WSI approach. We build a graph according to the approach described in (Agirre et al., 2006). Chinese Whispers algorithm (Biemann, 2006) is used to cluster the graph. The maximum number of Chinese Whispers iterations is set to 100. We also include the “one cluster per word” baseline (1c1w), where all instances of a target word are grouped into a single cluster. In SemEval-2010 task 14, none of the participating systems outperform this baseline in paired F-score (Artiles et al., 2009), which indicates that this baseline is quite strong. According to (Pedersen, 2010), we employ paired F-score as evaluation measure. Let C={Cj|j=1,2,…,n} be a set of clusters generated by a WSI system and S={Gi|i=1,2,…,m} be the set of gold standard classes. For each cluster Cj, we generate instance pairs, in which is the total number of instances that belong to Cj. Similarly, we generate instance pairs for each gold standard class Gi. Let F(C) is the set of instance pairs generated from any clusters in C and F(S) is the set of instance pairs generated from any gold standard classes in S. Precision and recall are defined in Equation 4 and 5 respectively. 1389 |F (C ) ∩ F ( S"
I11-1159,S07-1002,0,0.0193996,"ctor-based WSI approach, which represents the contexts of a target word using second order co-occurrence vectors (Schűtze, 1998). This approach constructs a word-by-word co-occurrence matrix by identifying bigrams whose number of occurrences is greater than a pre-specified threshold. A row in the matrix is the vector for a context word. Each context is represented by the centroid of all vectors of the words which make up the context. Then these context vectors are clustered to induce senses of target words. This approach has a good performance in public evaluation (e.g. Semeval-2007 task 02) (Agirre and Soroa, 2007). The second one is also a vector-based WSI approach which represents the contexts of a target word using bag-of-words vectors and weights each feature (word) based on its TF and IDF. The two vector-based WSI approaches use k-means algorithm to cluster the context vectors of target words and the maximum number of k-means iterations is set to 100. The third one is a graph-based WSI approach. We build a graph according to the approach described in (Agirre et al., 2006). Chinese Whispers algorithm (Biemann, 2006) is used to cluster the graph. The maximum number of Chinese Whispers iterations is s"
I11-1159,E09-1005,0,0.0251452,", Thailand, November 8 – 13, 2011. 2011 AFNLP P with the entry Pi,j giving the weight of word i in context j. In this paper, we set Pi , j = Figure 1: Three context vectors for the target word bank. In this paper, we propose a WSI method which can exploit semantic relevance between words by incorporating a word graph into the framework of clustering of context vectors. Firstly, we build a graph, where each vertex corresponds to a selected word and edges between vertices are weighted based on the semantic relevance between their associated words. Then we adapt the Personalized PageRank method (Agirre and Soroa, 2009) for incorporating semantic relevance between words into context vectors. The resulting vectors are clustered and each cluster represents an induced sense of the target word. Our method bears some similarity with some graph-based methods of WSI since they all need a graph of words. But in our method the graph is used to incorporate semantic relevance between words into context vectors while in graph-based approaches of WSI it is clustered to induce different senses of a target word. We use two vector-based approaches and one graph-based approach as baselines. Our evaluation under the framework"
I11-1159,J98-1004,0,0.606307,"but distinct words will show no similarity. Figure 1 shows a simple example of three context vectors taken from three contexts of the target word bank, which appears with one sense i.e. sloping land. If we assume that context words are independent, the similarity between context 1 and context 3 will be zero, which means that the senses of the target word bank in the two contexts are different. But, in practice, bank appears with one sense in the two contexts. Some methods have been proposed to use information beyond that which is found in the immediately surrounding context. For example, in (Schűtze, 1998), second order cooccurrence matrix was used to construct rich vectors of word contexts. 1387 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1387–1391, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP P with the entry Pi,j giving the weight of word i in context j. In this paper, we set Pi , j = Figure 1: Three context vectors for the target word bank. In this paper, we propose a WSI method which can exploit semantic relevance between words by incorporating a word graph into the framework of clustering of context vectors. Firstly, we build a"
I11-1159,D09-1056,0,0.0615376,"Missing"
I11-1159,W06-1669,0,\N,Missing
I11-1159,S10-1011,0,\N,Missing
I11-1159,N06-4007,0,\N,Missing
N18-1068,Y14-1039,0,0.0281008,"embeddings with word embeddings using its names and Wikipedia anchors. Zhong et al. (2015) further improved the model of Wang et al. (2014) by aligning entity and text using entity descriptions. Zhang et al. (2015) proposed to model entities with word embeddings of entity names or entity descriptions. Xie et al. (2016) proposed a model to learn the embeddings of a knowledge graph by modelling both knowledge triples and entity descriptions. Xu et al. (2016) learns different representations for entities based on the attention from relation. The textual mentions of relations are also explored by Fan et al. (2014). The universal schema based models (Riedel et al., 2013; Toutanova et al., 2015) enhance knowledge representation by incorporating textual triples, which assume that all the extracted triples express a relationship between the entities and they treat each pattern as a separate relation. The main drawback of these methods is that they assume all the relation mentions will express relationship between entity pairs, which inevitably introduces a lot of noisy information. For example, the sentence “Miami Dolphins in 1966 and the Cincinnati Bengals in 1968” does not express any relation3 Accurate"
N18-1068,D15-1174,0,0.0356831,"g et al. (2015) further improved the model of Wang et al. (2014) by aligning entity and text using entity descriptions. Zhang et al. (2015) proposed to model entities with word embeddings of entity names or entity descriptions. Xie et al. (2016) proposed a model to learn the embeddings of a knowledge graph by modelling both knowledge triples and entity descriptions. Xu et al. (2016) learns different representations for entities based on the attention from relation. The textual mentions of relations are also explored by Fan et al. (2014). The universal schema based models (Riedel et al., 2013; Toutanova et al., 2015) enhance knowledge representation by incorporating textual triples, which assume that all the extracted triples express a relationship between the entities and they treat each pattern as a separate relation. The main drawback of these methods is that they assume all the relation mentions will express relationship between entity pairs, which inevitably introduces a lot of noisy information. For example, the sentence “Miami Dolphins in 1966 and the Cincinnati Bengals in 1968” does not express any relation3 Accurate Text-enhanced Knowledge Graph Representation This section presents our accurate t"
N18-1068,P15-1067,0,0.37677,"n both link prediction and triple classification tasks, and significantly outperforms previous text-enhanced knowledge representation models. 1 Figure 1: A demonstration of our accurate textenhanced model. The meanings of relation parentOf in different triples are distinguished by their entity descriptions, and the relation parentOf emphasizes words which describe its social relationship in entity descriptions. such as link prediction and triple classification (Socher et al., 2013). Recently, translation-based models, including TransE (Bordes et al., 2013), TransH (Wang et al., 2014), TransD (Ji et al., 2015) and TransR (Lin et al., 2015b), have achieved promising results in distributional representation learning of knowledge graph. ComplEx (Trouillon et al., 2016) has achieved the state-of-the-art performance on multiple tasks, such as triple classification and link prediction. Unfortunately, all of these methods only utilize the structure information of knowledge graph, which inevitably suffer from the sparseness and incompleteness of knowledge graph. Even worse, structure information usually cannot distinguish the different meanings of relations and entities in different triples. To address the"
N18-1068,P16-1136,0,0.0187923,"., 2013) and its extensions like TransH (Wang et al., 2014), TransD (Ji et al., 2015), TransR (Lin et al., 2015b). Xiao et al. (2016a) proposed a manifold-based embedding principle to deal with the overstrict geometric form of translation-based assumption. Trouillon et al. (2016) employed complex value embeddings to understand the structural information. In recent years, many methods improve the knowledge representation by exploiting additional information. For example, both the path information and logic rules have been proved to be beneficial for knowledge representation (Lin et al., 2015a; Toutanova et al., 2016; Xiong et al., 2017; Xie et al., 2016; Xu et al., 2016). One other direction to enhance knowledge representation is to utilize entity descriptions of entities and relations. Socher et al. (2013) proposed a neural tensor network model which enhances an entity’s representation using the average of the word embeddings in its name. Wang et al. (2014) proposed a model which combines entity embeddings with word embeddings using its names and Wikipedia anchors. Zhong et al. (2015) further improved the model of Wang et al. (2014) by aligning entity and text using entity descriptions. Zhang et al. (20"
N18-1068,S15-1002,0,0.0214989,"n Hawaii, US” expresses the meanings of /people/person/nationality 748 and tail entity e~∗t ∈ dh with the attention from relation representation. The above two entity description representations are utilized as the attention for learning the triple-sensitive relation mention representation as follows: ~e = e~∗h + e~∗t (4) trix. We use the same pre-trained word embeddings as input for the BiLSTM networks of relation mentions and entity descriptions. BiLSTM Layer. To learn the representation of text mentions, we utilize a BiLSTM (Long Short-Term Memory) (Hochreiter and Schmidhuber, 1997; Le and Zuidema, 2015; Zhou et al., 2016) model to compose the words in a sequence into the distributional representation. Concretely, we employ a two layer Bidirectional LSTM network to generate text representations. The detailed description of LSTM is presented in (Hochreiter and Schmidhuber, 1997). Two different BiLSTM networks are employed to encode relation mentions and entity descriptions respectively. Mutual Attention Layer. Attention based neural networks have recently achieved success in a wide range of tasks, including machine translation, speech recognition and paraphrase detection (Luong et al., 2015;"
N18-1068,D15-1082,0,0.400417,"iple classification tasks, and significantly outperforms previous text-enhanced knowledge representation models. 1 Figure 1: A demonstration of our accurate textenhanced model. The meanings of relation parentOf in different triples are distinguished by their entity descriptions, and the relation parentOf emphasizes words which describe its social relationship in entity descriptions. such as link prediction and triple classification (Socher et al., 2013). Recently, translation-based models, including TransE (Bordes et al., 2013), TransH (Wang et al., 2014), TransD (Ji et al., 2015) and TransR (Lin et al., 2015b), have achieved promising results in distributional representation learning of knowledge graph. ComplEx (Trouillon et al., 2016) has achieved the state-of-the-art performance on multiple tasks, such as triple classification and link prediction. Unfortunately, all of these methods only utilize the structure information of knowledge graph, which inevitably suffer from the sparseness and incompleteness of knowledge graph. Even worse, structure information usually cannot distinguish the different meanings of relations and entities in different triples. To address the above problem, additional in"
N18-1068,D15-1166,0,0.0373633,"Missing"
N18-1068,P16-1219,0,0.0260209,"Missing"
N18-1068,N13-1008,0,0.0370906,"kipedia anchors. Zhong et al. (2015) further improved the model of Wang et al. (2014) by aligning entity and text using entity descriptions. Zhang et al. (2015) proposed to model entities with word embeddings of entity names or entity descriptions. Xie et al. (2016) proposed a model to learn the embeddings of a knowledge graph by modelling both knowledge triples and entity descriptions. Xu et al. (2016) learns different representations for entities based on the attention from relation. The textual mentions of relations are also explored by Fan et al. (2014). The universal schema based models (Riedel et al., 2013; Toutanova et al., 2015) enhance knowledge representation by incorporating textual triples, which assume that all the extracted triples express a relationship between the entities and they treat each pattern as a separate relation. The main drawback of these methods is that they assume all the relation mentions will express relationship between entity pairs, which inevitably introduces a lot of noisy information. For example, the sentence “Miami Dolphins in 1966 and the Cincinnati Bengals in 1968” does not express any relation3 Accurate Text-enhanced Knowledge Graph Representation This sectio"
N18-1068,D17-1060,0,0.0200276,"ons like TransH (Wang et al., 2014), TransD (Ji et al., 2015), TransR (Lin et al., 2015b). Xiao et al. (2016a) proposed a manifold-based embedding principle to deal with the overstrict geometric form of translation-based assumption. Trouillon et al. (2016) employed complex value embeddings to understand the structural information. In recent years, many methods improve the knowledge representation by exploiting additional information. For example, both the path information and logic rules have been proved to be beneficial for knowledge representation (Lin et al., 2015a; Toutanova et al., 2016; Xiong et al., 2017; Xie et al., 2016; Xu et al., 2016). One other direction to enhance knowledge representation is to utilize entity descriptions of entities and relations. Socher et al. (2013) proposed a neural tensor network model which enhances an entity’s representation using the average of the word embeddings in its name. Wang et al. (2014) proposed a model which combines entity embeddings with word embeddings using its names and Wikipedia anchors. Zhong et al. (2015) further improved the model of Wang et al. (2014) by aligning entity and text using entity descriptions. Zhang et al. (2015) proposed to mode"
N18-1068,Q16-1019,0,0.0333695,") model to compose the words in a sequence into the distributional representation. Concretely, we employ a two layer Bidirectional LSTM network to generate text representations. The detailed description of LSTM is presented in (Hochreiter and Schmidhuber, 1997). Two different BiLSTM networks are employed to encode relation mentions and entity descriptions respectively. Mutual Attention Layer. Attention based neural networks have recently achieved success in a wide range of tasks, including machine translation, speech recognition and paraphrase detection (Luong et al., 2015; Yang et al., 2016; Yin et al., 2016; Vaswani et al., 2017). In this paper, we introduce a mutual attention to improve text representations. Given a triple, the goal of our mutual attention mechanism is two-fold. On one hand, our model wants to identify words in relation mention associated with the entity descriptions in the same triple. On the other hand, our model wants to recognize words in entity descriptions which are emphasized by its relation. To achieve the above goal, we first infer the representations of entity descriptions using relation representation as attention: exp(score(h~i , r~0 )) ai (e) = P ~ ~0 i0 exp(score("
N18-1068,D15-1031,0,0.153917,"ty description to handle the ambiguity of relations and entities (Section 3). (ii) We propose a mutual attention mechanism which exploits the textual representations of relation and entity to enhance each other (Section 3.2). (iii) This paper achieves new state-of-the-art performances on triple classification tasks over two most widely used benchmarks (Section 4). are knowledge graph sensitive and require the expert knowledge. Another type of widely used resources is textual information, such as entity descriptions and words co-occurrence with entities (Socher et al., 2013; Wang et al., 2014; Zhong et al., 2015). The main drawback of the above methods is that they represent the same entity/relation in different triples with a unique representation. Unfortunately, by detailed analyzing the triples in knowledge graph, we find two problems of the unique representation: (1) Relations are ambiguous, i.e., the accurate semantic meaning of a relation in a specific triple is related to the entities in the same triple. For example, the relation “parentOf” may refer to two different meanings of (i.e., “father” and “mother”), depending on the entities in triples. (2) Because different relations may concern diff"
N18-1068,P16-2034,0,0.221532,"c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics on related information from text information. For example, the “parentOf” relation will concern more about the social relations and gender attributes of a person, rather than his/her jobs, which are also contained in its descriptions. And such a relation-specific entity description will make an entity has more appropriate, relation-specific representations in different triples. Concretely, we employ BiLSTM model (Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005) with mutual attention mechanism (Zhou et al., 2016) to learn representations for relation mentions and entity descriptions. Specifically, in order to generate triple-specific textual representation of entities and relation, a mutual attention mechanism is proposed to model relation between entity descriptions and relation mention of one triple. Then the learned textual representations are incorporated with previous traditional transitionbased representations, which are, learned from structural information of knowledge graph, directly to obtain enhanced triple specific representations of elements. We evaluate our method on both link prediction"
P11-1095,E06-1002,0,0.312456,"xample, the entity Michael Jeffrey Jordan can be mentioned using more than 10 names, such as Michael Jordan, MJ and Jordan. The name ambiguity problem is related to the fact that a name may refer to different entities in different contexts. For example, the name Bulls can refer to more than 20 entities in Wikipedia, such as the NBA team Chicago Bulls, the football team Belfast Bulls and the cricket team Queensland Bulls. Complicated by the name variation problem and the name ambiguity problem, the entity linking decisions are critically depending on the knowledge of entities (Li et al., 2004; Bunescu & Pasca, 2006; Cucerzan, 2007; Milne & Witten, 2008 and Fader et al., 2009). Based on the previous work, we found that the following three types of entity knowledge can provide critical evidence for the entity linking decisions:  Popularity Knowledge. The popularity knowledge of entities tells us the likelihood of an entity appearing in a document. In entity linking, the entity popularity knowledge can provide a priori information to the possible referent entities of a name mention. For example, without any other information, the popularity knowledge can tell that in a Web page the name “Michael Jordan” w"
P11-1095,J93-2003,0,0.0179507,"e, because the name “MJ” doesn’t refer to the Michael Jeffrey Jordan in Wikipedia, the name model will not be able to identify “MJ” as a name of him, even “MJ” is a popular name of Michael Jeffrey Jordan on Web. To better estimate the distribution P(s|e), this paper proposes a much more generic model, called entity name model, which can capture the variations (including full name, aliases, acronyms and misspellings) of an entity's name using a statistical translation model. Given an entity’s name s, our model assumes that it is a translation of this entity’s full name f using the IBM model 1 (Brown, et al., 1993). Let ∑ be the vocabulary containing all words may be used in the name of entities, the entity name model assumes that a word in ∑ can be translated through the following four ways: 1) It is retained (translated into itself); 2) It is translated into its acronym; 3) It is omitted(translated into the word NULL); 4) It is translated into another word (misspelling or alias). In this way, all name variations of an entity are captured as the possible translations of its full name. To illustrate, Figure 3 shows how the full name “Michael Jeffrey Jordan” can be transalted into its misspelling name “M"
P11-1095,D07-1074,0,0.896914,"ael Jeffrey Jordan can be mentioned using more than 10 names, such as Michael Jordan, MJ and Jordan. The name ambiguity problem is related to the fact that a name may refer to different entities in different contexts. For example, the name Bulls can refer to more than 20 entities in Wikipedia, such as the NBA team Chicago Bulls, the football team Belfast Bulls and the cricket team Queensland Bulls. Complicated by the name variation problem and the name ambiguity problem, the entity linking decisions are critically depending on the knowledge of entities (Li et al., 2004; Bunescu & Pasca, 2006; Cucerzan, 2007; Milne & Witten, 2008 and Fader et al., 2009). Based on the previous work, we found that the following three types of entity knowledge can provide critical evidence for the entity linking decisions:  Popularity Knowledge. The popularity knowledge of entities tells us the likelihood of an entity appearing in a document. In entity linking, the entity popularity knowledge can provide a priori information to the possible referent entities of a name mention. For example, without any other information, the popularity knowledge can tell that in a Web page the name “Michael Jordan” will more likely"
P11-1095,C10-1032,0,0.366225,"as to extract the discriminative features of an entity from its description, then link a name mention to the entity which has the largest context similarity with it. Cucerzan (2007) proposed a Bag of Words based method, which represents each target entity as a vector of terms, then the similarity between a name mention and an entity was computed using the cosine similarity measure. Mihalcea & Csomai (2007), Bunescu & Pasca (2006), Fader et al. (2009) extended the BoW model by incorporating more entity knowledge such as popularity knowledge, entity category knowledge, etc. Zheng et al. (2010), Dredze et al. (2010), Zhang et al. (2010) and Zhou et al. (2010) employed the learning to rank techniques which can further take the relations between candidate entities into account. Because the context 953 Conclusions and Future Work This paper proposes a generative probabilistic model, the entity-mention model, for the entity linking task. The main advantage of our model is it can incorporate multiple types of heterogenous entity knowledge. Furthermore, our model has a statistical foundation, making the entity knowledge extraction approach different from most previous ad hoc approaches. Experimental results sh"
P11-1095,P10-1006,1,0.117674,"hod based on language modeling, called entity context model. In our model, the context of each name mention m is the word window surrounding m, and the window size is set to 50 according to the experiments in (Pedersen et al., 2005). Specifically, the context knowledge of an entity e is encoded in an unigram language model: M e  {Pe (t )} where Pe(t) is the probability of the term t appearing in the context of e. In our model, the term may indicate a word, a named entity (extracted using the Stanford Named Entity Recognizer 4 ) or a Wikipedia concept (extracted using the method described in (Han and Zhao, 2010)). Figure 4 shows two entity context models and the contexts generated using them. Now, given a context c containing n terms t1t2…tn, the entity context model estimates the probability P(c|e) as: P (c |e)  P (t1t2 ...tn |M e )  Pe (t1 ) Pe (t2 )....Pe (tn ) So the main problem is to estimate Pe(t), the probability of a term t appearing in the context of the entity e. Using the annotated name mention data set M, we can get the maximum likelihood estimation of Pe(t) as follows: Pe _ ML (t )  Counte (t )  Counte (t ) t where Counte(t) is the frequency of occurrences of a term t in the context"
P11-1095,C10-1145,0,0.19174,"riminative features of an entity from its description, then link a name mention to the entity which has the largest context similarity with it. Cucerzan (2007) proposed a Bag of Words based method, which represents each target entity as a vector of terms, then the similarity between a name mention and an entity was computed using the cosine similarity measure. Mihalcea & Csomai (2007), Bunescu & Pasca (2006), Fader et al. (2009) extended the BoW model by incorporating more entity knowledge such as popularity knowledge, entity category knowledge, etc. Zheng et al. (2010), Dredze et al. (2010), Zhang et al. (2010) and Zhou et al. (2010) employed the learning to rank techniques which can further take the relations between candidate entities into account. Because the context 953 Conclusions and Future Work This paper proposes a generative probabilistic model, the entity-mention model, for the entity linking task. The main advantage of our model is it can incorporate multiple types of heterogenous entity knowledge. Furthermore, our model has a statistical foundation, making the entity knowledge extraction approach different from most previous ad hoc approaches. Experimental results show that our method ca"
P11-1095,C10-1150,0,0.221997,"n entity from its description, then link a name mention to the entity which has the largest context similarity with it. Cucerzan (2007) proposed a Bag of Words based method, which represents each target entity as a vector of terms, then the similarity between a name mention and an entity was computed using the cosine similarity measure. Mihalcea & Csomai (2007), Bunescu & Pasca (2006), Fader et al. (2009) extended the BoW model by incorporating more entity knowledge such as popularity knowledge, entity category knowledge, etc. Zheng et al. (2010), Dredze et al. (2010), Zhang et al. (2010) and Zhou et al. (2010) employed the learning to rank techniques which can further take the relations between candidate entities into account. Because the context 953 Conclusions and Future Work This paper proposes a generative probabilistic model, the entity-mention model, for the entity linking task. The main advantage of our model is it can incorporate multiple types of heterogenous entity knowledge. Furthermore, our model has a statistical foundation, making the entity knowledge extraction approach different from most previous ad hoc approaches. Experimental results show that our method can achieve competitive p"
P11-1095,N10-1072,0,\N,Missing
P14-2011,C96-1079,0,0.489846,"Missing"
P14-2011,P13-1147,0,0.036598,"ctly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). Table 3. Comparison of different systems on the ACE RDC 2004 corpus 4.2.2 Related Work 6 Conclusions and Future Work This paper proposes a featu"
P14-2011,H05-1091,0,0.274345,"g a more proper sub-tree representation. We believe these two techniques can also be used to further improve the performance of our system. 5 This section briefly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classiﬁcation problem (Kambhatla, 2004; Zhou et al., 2005; Jiang & Zhai, 2007; Chan & Roth, 2010; Chan & Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information"
P14-2011,N07-1015,0,0.633468,"Missing"
P14-2011,P01-1017,0,0.0273864,"Missing"
P14-2011,J93-2004,0,0.0465973,"Missing"
P14-2011,C10-1018,0,0.606505,"Missing"
P14-2011,P04-1043,0,0.117908,"ting its WordNet sense information. Specifically, the first WordNet sense of the terminal word, and all this sense’s hyponym senses will be added as features. For example, WordNet senses {New York#1, city#1, district#1, where £ Instance Feature Features for Relation Extraction This section presents the features we used to enrich the syntactic tree representation. 63 region#1, …} will be added as features to the [NN New York] node in Figure 1. 3.3 In our experiments, we implement the feature-enriched tree kernel by extending the SVMlight (Joachims, 1998) with the proposed tree kernel function (Moschitti, 2004). We apply the one vs. others strategy for multiple classification using SVM. For SVM training, the parameter C is set to 2.4 for all experiments, and the tree kernel parameter λ is tuned to 0.2 for FTK and 0.4 (the optimal parameter setting used in Qian et al.(2008)) for CTK. Context Information Feature The context information of a phrase node is critical for identifying the role and the importance of a sub-tree in the whole relation instance. This paper captures the following context information: 1) Contextual path from sub-tree root to the phrase node. As shown in Zhou et al. (2007), the co"
P14-2011,P11-1056,0,0.00555093,"y capture the syntactic structure similarity between two trees, while ignoring other useful information. To resolve the above problem, the feature-enriched tree kernel (FTK) compute the similarity between two trees as the sum of the similarities between their common sub-trees: 3.1 Relation instances of the same type often share some common characteristics. In this paper, we add the following instance features to the root node of a sub-tree representation: 1) Syntactico-Semantic structure. A feature indicates whether a relation instance has the following four syntactico-semantic structures in (Chan & Roth, 2011) – Premodifiers, Possessive, Preposition, Formulaic and Verbal. 2) Entity-related information of arguments. Features about the entity information of arguments, including: a) #TP1-#TP2: the concat of the major entity types of arguments; b) #ST1#ST2: the concat of the sub entity types of arguments; c) #MT1-#MT2: the concat of the mention types of arguments. 3) Base phrase chunking features. Features about the phrase path between two arguments and the phrases’ head before and after the arguments, which are the same as the phrase chunking features in (Zhou, et al., 2005). where is the similarity b"
P14-2011,E09-1066,0,0.0191984,"al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). Table 3. Comparison of different systems on the ACE RDC 2004 corpus 4.2.2 Related Work 6 Conclusions and Future Work This paper proposes a feature-enriched tree kernel, which can: 1) refine the syntactic tree representation; and 2) better measure the similarity between two trees. For future work, we want to develop a feature weighting algorithm which can accurately measure the r"
P14-2011,D11-1096,0,0.00895967,"problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). Table 3. Comparison of different systems on the ACE RDC 2004 corpus 4.2.2 Related Work 6 Conclusions and Future Work This paper proposes a feature-enriched tree kernel, which can: 1) refine the syntactic tree representation; and 2) better measure the similarity between two trees. For future work, we want to develop a feature weighting algorithm which can accurately measure the relevance of a feature to a relation instance for better RE performance. Comparison with other systems Finally, Table 3 compares the performance of our method with several othe"
P14-2011,N10-1146,0,0.0132831,"estral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). Table 3. Comparison of different systems on the ACE RDC 2004 corpus 4.2.2 Related Work 6 Conclusions and Future Work This paper proposes a feature-enriched tree kernel, which can: 1) refine the syntactic tree representation; and 2) better measure the similarity between two trees. For future work, we want to develop a feature weighting algorithm which can accurately measure the relevance of a feature to a relation instance for better RE performance. Comparison with other systems Finally, Table 3 compares the performance of our method with several other systems. From Table 3, we can see that F"
P14-2011,P12-1028,0,0.0150234,"(2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). Table 3. Comparison of different systems on the ACE RDC 2004 corpus 4.2.2 Related Work 6 Conclusions and Future Work This paper proposes a feature-enriched tree kernel, which can: 1) refine the syntactic tree representation; and 2) better measure the similarity between two trees. For future work, we want to develop a feature weighting algorithm which can accurately measure the relevance of a feature to a relation instance for better RE performance. Comparison with other systems Finally, Table 3 compares the performance of our method with several other systems. From Tabl"
P14-2011,P09-1113,0,0.239766,"Missing"
P14-2011,P04-1054,0,0.356702,"can also be used to further improve the performance of our system. 5 This section briefly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classiﬁcation problem (Kambhatla, 2004; Zhou et al., 2005; Jiang & Zhai, 2007; Chan & Roth, 2010; Chan & Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntacti"
P14-2011,N06-1037,0,0.0504359,"ree kernel, referred as feature-enriched tree kernel (FTK), which can effectively resolve the above problems by enhancing the traditional tree kernel in following ways: 1) We refine the syntactic tree representation by annotating each tree node with a set of discriminant features. These features are utilized to Introduction Relation Extraction (RE) aims to identify a set of predefined relations between pairs of entities in text. In recent years, relation extraction has received considerable research attention. An effective technique is the tree kernel (Zelenko et al., 2003; Zhou et al., 2007; Zhang et al., 2006; Qian et al., 2008), which can exploit syntactic parse tree information for relation extraction. Given a pair of entities in a sentence, the tree kernel-based RE method first represents the relation information between them using a proper sub-tree (e.g., SPT – the sub-tree enclosed by the shortest path linking the two involved entities). For example, the three syntactic tree representations in Figure 1. Then the similarity between two trees are computed using a tree kernel, e.g., the convolution tree kernel proposed by Collins and Duffy (2001). Finally, new relation instances are extracted us"
P14-2011,P06-1104,0,0.0468005,"ree kernel, referred as feature-enriched tree kernel (FTK), which can effectively resolve the above problems by enhancing the traditional tree kernel in following ways: 1) We refine the syntactic tree representation by annotating each tree node with a set of discriminant features. These features are utilized to Introduction Relation Extraction (RE) aims to identify a set of predefined relations between pairs of entities in text. In recent years, relation extraction has received considerable research attention. An effective technique is the tree kernel (Zelenko et al., 2003; Zhou et al., 2007; Zhang et al., 2006; Qian et al., 2008), which can exploit syntactic parse tree information for relation extraction. Given a pair of entities in a sentence, the tree kernel-based RE method first represents the relation information between them using a proper sub-tree (e.g., SPT – the sub-tree enclosed by the shortest path linking the two involved entities). For example, the three syntactic tree representations in Figure 1. Then the similarity between two trees are computed using a tree kernel, e.g., the convolution tree kernel proposed by Collins and Duffy (2001). Finally, new relation instances are extracted us"
P14-2011,P05-1052,0,0.0104431,"fly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classiﬁcation problem (Kambhatla, 2004; Zhou et al., 2005; Jiang & Zhai, 2007; Chan & Roth, 2010; Chan & Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between l"
P14-2011,P05-1053,0,0.241809,"ico-semantic structures in (Chan & Roth, 2011) – Premodifiers, Possessive, Preposition, Formulaic and Verbal. 2) Entity-related information of arguments. Features about the entity information of arguments, including: a) #TP1-#TP2: the concat of the major entity types of arguments; b) #ST1#ST2: the concat of the sub entity types of arguments; c) #MT1-#MT2: the concat of the mention types of arguments. 3) Base phrase chunking features. Features about the phrase path between two arguments and the phrases’ head before and after the arguments, which are the same as the phrase chunking features in (Zhou, et al., 2005). where is the similarity between enumerated sub-trees and , which is computed as: where is the same indicator function as in CTK; is a pair of aligned nodes between and , where and are correspondingly in the same position of tree and ; is the set of all aligned node pairs; is the feature vector similarity between node and , computed as the dot product between their feature vectors and . Notice that, if all nodes are not annotated with features, will be equal to . In this perspective, we can view as a similarity adjusted version of , i.e., only considers whether two nodes are equal, in contras"
P14-2011,D07-1076,0,0.58285,"Missing"
P14-2011,C08-1088,0,\N,Missing
P14-2011,P04-3022,0,\N,Missing
P14-2117,I11-1082,0,0.0409969,"Missing"
P14-2117,P07-1073,0,0.0112304,"ate the semantic consistency of an instance by exploiting the characteristics of its local subspace. 718 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 718–724, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics This paper is organized as follows. Section 2 reviews related work. Section 3 describes the proposed method. Section 4 presents the experiments. Finally Section 5 concludes this paper. 2 Related Work This section briefly reviews the related work. Craven and Kumlien (1999), Wu et al. (2007) and Mintz et al.(2009) were several pioneer work of distant supervision. One main problem of DS assumption is that it often will lead to false positives in training data. To resolve this problem, Bunescu and Mooney (2007), Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one re"
P14-2117,N13-1008,0,0.0866693,"Missing"
P14-2117,D13-1003,0,0.0250573,"Missing"
P14-2117,C92-2082,0,0.130206,"en proposed to discover the underlying patterns of Vi. In this paper, we make a simple and effective assumption that the instances of a single relation type can be represented as the linear combination of other instances of the same relation type. This assumption is well motived in relation extraction, because although there is nearly unlimited ways to express a specific relation, in many cases basic principles of economy of expression and/or conventions of genre will ensure that certain systematic ways will be used to express a specific relation (Wang et al., 2012). For example, as shown in (Hearst, 1992), the IS-A relation is usually expressed using several regular patterns, such as “such NP as {NP ,}* {(or |and)} NP” and “NP {, NP}* {,} or other NP”. Based on the above assumption, we hold many instances for each relation type and directly use these instances to model the subspace of a relation type. Specifically, we represent an instance y of ith type as the linear combination of training instances associated with ith type: y = ®i;1 vi;1 + ®i;2 vi;2 + ::: + +®i;ni vi;ni (1) In this section, we describe our semantic consistency model for relation extraction. We first model the subspaces of al"
P14-2117,D12-1042,0,0.216708,"solve this problem, Bunescu and Mooney (2007), Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can jointly model all instances of an entity pair and all their labels. Several other research issues also have been addressed. Xu et al. (2013), Min et al. (2013) and Zhang et al. (2013) try to resolve the false negative problem raised by the incomplete knowledge base problem. Hoffmann et al. (2010) and Zhang et al. (2010) try to improve the extraction precision by learning a dynamic lexicon. as the features used in (Mintz, 2009) or (Surdeanu et al., 2012), with each feature is TFIDF weighted by taking each instance as an individu"
P14-2117,P10-1030,0,0.0189773,"ral pioneer work of distant supervision. One main problem of DS assumption is that it often will lead to false positives in training data. To resolve this problem, Bunescu and Mooney (2007), Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can jointly model all instances of an entity pair and all their labels. Several other research issues also have been addressed. Xu et al. (2013), Min et al. (2013) and Zhang et al. (2013) try to resolve the false negative problem raised by the incomplete knowledge base problem. Hoffmann et al. (2010) and Zhang et al. (2010) try to improve the extraction precision by learning a dynamic lexic"
P14-2117,P11-1055,0,0.066648,"183,062 training relations and 3,334 testing relations are collected. For tuning and testing, we used the same partition as Surdeanu et al. (2012): 40 queries for development and 160 queries for formal evaluation. In this paper, each instance in KBP is represented as a feature vector using the features as the same as in (Surdeanu et al., 2012). Baselines. We compare our method with four baselines as follows:  Mintz++. This is a traditional DS assumption based model proposed by Mintz et al.(2009).  Hoffmann. This is an at-least-one assumption based multi-instance learning method proposed by Hoffmann et al. (2011).  MIML. This is a multi-instance multi-label model proposed by Surdeanu et al. (2012).  KNN. This is a classical K-NearestNeighbor classifier baseline. Specifically, given an entity pair, we first classify each matching instance using the labels of its 5 (tuned on training corpus) nearest neighbors with cosine similarity, then all matching instances’ classification results are added together. Evaluation. We use the same evaluation settings as Surdeanu et al. (2012). That is, we use the official KBP scorer with two changes: (a) relation mentions are evaluated regardless of their support docu"
P14-2117,P09-1113,0,0.0400778,"nstances, then estimate the semantic consistency by exploiting the characteristics of the local subspace. Experimental results verified the effectiveness of our method. 1 Introduction Relation extraction aims to identify and categorize relations between pairs of entities in text. Due to the time-consuming annotation process, one critical challenge of relation extraction is the lack of training data. To address this limitation, a promising approach is distant supervision (DS), which can automatically gather labeled data by heuristically aligning entities in text with those in a knowledge base (Mintz et al., 2009). The underlying assumption of distant supervision is that every sentence that mentions two entities is likely to express their relation in a knowledge base. Relation Instance Label S1: Jobs was the founder of Apple Founder-of, CEO-of S2: Jobs joins Apple Founder-of, CEO-of Figure 1. Labeled instances by distant supervision, using relations CEO-of(Steve Jobs, Apple Inc.) and Founder-of(Steve Jobs, Apple Inc.) The distant supervision assumption, unfortunately, can often fail and result in a noisy training corpus. For example, in Figure 1 DS assumption will wrongly label S1 as a CEO-of instance"
P14-2117,N13-1095,0,0.015397,". Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can jointly model all instances of an entity pair and all their labels. Several other research issues also have been addressed. Xu et al. (2013), Min et al. (2013) and Zhang et al. (2013) try to resolve the false negative problem raised by the incomplete knowledge base problem. Hoffmann et al. (2010) and Zhang et al. (2010) try to improve the extraction precision by learning a dynamic lexicon. as the features used in (Mintz, 2009) or (Surdeanu et al., 2012), with each feature is TFIDF weighted by taking each instance as an individual document. To model the subspace of ith relation type in the original feature space, a variety of models have been proposed to discover the underlying patterns of Vi. In this paper, we make a simple and effective assumption"
P14-2117,P12-1076,0,0.015931,"method. Section 4 presents the experiments. Finally Section 5 concludes this paper. 2 Related Work This section briefly reviews the related work. Craven and Kumlien (1999), Wu et al. (2007) and Mintz et al.(2009) were several pioneer work of distant supervision. One main problem of DS assumption is that it often will lead to false positives in training data. To resolve this problem, Bunescu and Mooney (2007), Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can jointly model all instances of an entity pair and all their labels. Several other research issues also have been addressed. Xu et al. (2013), Min et al. (2013) and Zha"
P14-2117,D11-1132,0,0.0455455,"Missing"
P14-2117,P13-2117,0,0.0942654,"Missing"
P14-2117,D10-1099,0,0.0143281,"e 23-25 2014. 2014 Association for Computational Linguistics This paper is organized as follows. Section 2 reviews related work. Section 3 describes the proposed method. Section 4 presents the experiments. Finally Section 5 concludes this paper. 2 Related Work This section briefly reviews the related work. Craven and Kumlien (1999), Wu et al. (2007) and Mintz et al.(2009) were several pioneer work of distant supervision. One main problem of DS assumption is that it often will lead to false positives in training data. To resolve this problem, Bunescu and Mooney (2007), Riedel et al. (2010) and Yao et al. (2010) relaxed the DS assumption to the atleast-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Takamatsu et al. (2012) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can"
P14-2117,P13-2141,0,0.0112246,"12) proposed a generative model to eliminate noisy instances. Another research issue of distant supervision is that a pair of entities may participate in more than one relation. To resolve this problem, Hoffmann et al. (2010) proposed a method which can combine a sentence-level model with a corpus-level model to resolve the multi-label problem. Surdeanu et al. (2012) proposed a multi-instance multi-label learning approach which can jointly model all instances of an entity pair and all their labels. Several other research issues also have been addressed. Xu et al. (2013), Min et al. (2013) and Zhang et al. (2013) try to resolve the false negative problem raised by the incomplete knowledge base problem. Hoffmann et al. (2010) and Zhang et al. (2010) try to improve the extraction precision by learning a dynamic lexicon. as the features used in (Mintz, 2009) or (Surdeanu et al., 2012), with each feature is TFIDF weighted by taking each instance as an individual document. To model the subspace of ith relation type in the original feature space, a variety of models have been proposed to discover the underlying patterns of Vi. In this paper, we make a simple and effective assumption that the instances of a"
P15-2056,H93-1012,0,0.54358,"Missing"
P15-2056,H05-1004,0,0.0182836,"ries, we totally collect 2,280 query subtopic candidates. Three annotators manually label these candidates with their subtopics according to the content words of these candidates and their clicked web pages (if there are clicked URLs for the candidate in query log). A candidate belongs to a specific subtopic if at least two annotators agree with it. At last we obtain 1,086 subtopics. We randomly split the original queries into two parts: half used for training and the rest for testing. 3.2 Evaluation Metrics and Baselines To evaluate the performance of our approach, we employ the measures in (Luo, 2005), which are computed as follows, p  ( R , g ( R )) ,  ( R , R ) &apos; i i i &apos; i &apos; i &apos; i r  ( R , g ( R ))  ( R , R )) &apos; i i j &apos; i j j where R’ is the predicted partition and R is the ground-truth partition; π(A, B) is a similarity measure between set A and B, which is Jaccard coefficient in this paper; and g(.) is the optimal mapping between R’ and R. Based on p and r, fmeasure can be calculated as, f  measure  2 p  r pr The higher the f-measure score is, the better performance an approach achieves. We used the following approaches as baselines:  K-means: we perform the standard k-"
P16-1073,Q15-1041,0,0.015549,"tained through traditional method will result in the wrong answer “Rahul Gandhi”, because it cannot identify the vocabulary mismatch between “daughter” and child∧female1 . By contrast, by rewriting “daughter” into “female child”, our method can resolve this vocabulary mismatch. Specifically, we identify two common types of vocabulary mismatch in semantic parsing: 2 Related Work Semantic parsing has attracted considerable research attention in recent years. Generally, semantic parsing methods can be categorized into synchronous context free grammars (SCFG) based methods (Wong and Mooney, 2007; Arthur et al., 2015; Li et al., 2015), syntactic structure based methods (Ge and Mooney, 2009; Reddy et al., 2014; Reddy et al., 2016), combinatory categorical grammars (CCG) based methods (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014; Wang et al., 2014; Artzi et al., 2015), and dependency-based compositional semantics (DCS) based methods (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014; Berant and Liang, 2015; Pasupat and Liang, 2015; Wang et al., 2015). One major challenge of semantic parsing is how to scale to open-domain s"
P16-1073,Q13-1005,0,0.05468,"the compound formula child∧female. 2. N-1 mismatch: a logical constant may correspond to a complicated natural language expression, e.g., the formula population can be expressed using many phrases such as “how many people” and “live in”. 1 In this paper, we may simplify logical forms for readability, e.g., female for gender.female. 767 murthy and Mitchell, 2012; Cai and Yates, 2013a; Berant et al., 2013). Another challenge is how to alleviate the burden of annotation. A possible solution is to employ distant-supervised techniques (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013b; Artzi and Zettlemoyer, 2013), or unsupervised techniques (Poon and Domingos, 2009; Goldwasser et al., 2011; Poon, 2013). There were also several approaches focused on the mismatch problem. Kwiatkowski et al. (2013) addressed the ontology mismatch problem (i.e., two ontologies using different vocabularies) by first parsing a sentence into a domainindependent underspecified logical form, and then using an ontology matching model to transform this underspecified logical form to the target ontology. However, their method is still hard to deal with the 1-N and the N-1 mismatch problems between natural language and target onto"
P16-1073,D15-1198,0,0.050863,"Missing"
P16-1073,P14-1091,0,0.0429553,"for θ1 and θ2 do not maximize reward nor the log-likelihood. However, the reward provides a way to modulate the magnitude of the updates. Specifically, after each update, our model results in making the derivation, which has the highest reward, to get a bigger score. Table 7 presents our learning algorithm. 4.4 Experiments Features As described in Section 4.3, our model uses two kinds of features. One for the semantic parsing module which are simply the same features described in Berant and Liang (2015). One for the 771 System Berant et al., 2013 Yao and Van-Durme, 2014 Berant and Liang, 2014 Bao et al., 2014 Bordes et al., 2014a Yang et al., 2014 Bast and Haussmann, 2015 Yao, 2015 Berant and Liang, 2015 Yih et al., 2015 Our approach all sentences have different structure with their target logical forms, e.g., “Who is keyshia cole dad?” and “What countries have german as the official language?”. System Settings: In our experiments, we use the Freebase Search API for entity lookup. We load Freebase using Virtuoso, and execute logical forms by converting them to SPARQL and querying using Virtuoso. We learn the parameters of our system by making three passes over the training dataset, with the beam s"
P16-1073,P14-1133,0,0.660335,"years. Generally, semantic parsing methods can be categorized into synchronous context free grammars (SCFG) based methods (Wong and Mooney, 2007; Arthur et al., 2015; Li et al., 2015), syntactic structure based methods (Ge and Mooney, 2009; Reddy et al., 2014; Reddy et al., 2016), combinatory categorical grammars (CCG) based methods (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014; Wang et al., 2014; Artzi et al., 2015), and dependency-based compositional semantics (DCS) based methods (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014; Berant and Liang, 2015; Pasupat and Liang, 2015; Wang et al., 2015). One major challenge of semantic parsing is how to scale to open-domain situation like Freebase and Web. A possible solution is to learn lexicons from large amount of web text and a knowledge base using a distant supervised method (Krishna1. 1-N mismatch: a simple word may correspond to a compound formula. For example, the word “daughter” may correspond to the compound formula child∧female. 2. N-1 mismatch: a logical constant may correspond to a complicated natural language expression, e.g., the formula population can be exp"
P16-1073,P13-1158,0,0.165784,"What is the population of $y What is the currency of $y What is the education of $y What is the official language of $y Logical constant population annual-visit currency education officiallanguage To control the size and measure the quality of rewritings using a specific template pair, we also define several features and the similarity between template pairs (See Section 4 for details). To build the paraphrase template pair database, we employ the method described in Fader et al. (2014) to automatically collect paraphrase template pairs. Specifically, we use the WikiAnswers paraphrase corpus (Fader et al., 2013), which contains 23 million question-clusters, and all quesTable 4: Several N-1 mismatch examples. To resolve the N-1 mismatch problem, we propose a template rewriting algorithm, which can rewrite a complicated expression into its simpler form. Specifically, we rewrite sentences based on a set of paraphrase template pairs P = {(ti1 , ti2 )|i = 1, 2, ..., n}, where each template t 769 How many people live in chembakolli? How many people is in chembakolli? How many people live in chembakolli india? How many people live there chembakolli? How many people live there in chembakolli? What is the pop"
P16-1073,Q15-1039,0,0.641209,"ic parsing methods can be categorized into synchronous context free grammars (SCFG) based methods (Wong and Mooney, 2007; Arthur et al., 2015; Li et al., 2015), syntactic structure based methods (Ge and Mooney, 2009; Reddy et al., 2014; Reddy et al., 2016), combinatory categorical grammars (CCG) based methods (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014; Wang et al., 2014; Artzi et al., 2015), and dependency-based compositional semantics (DCS) based methods (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014; Berant and Liang, 2015; Pasupat and Liang, 2015; Wang et al., 2015). One major challenge of semantic parsing is how to scale to open-domain situation like Freebase and Web. A possible solution is to learn lexicons from large amount of web text and a knowledge base using a distant supervised method (Krishna1. 1-N mismatch: a simple word may correspond to a compound formula. For example, the word “daughter” may correspond to the compound formula child∧female. 2. N-1 mismatch: a logical constant may correspond to a complicated natural language expression, e.g., the formula population can be expressed using many phrase"
P16-1073,D13-1160,0,0.442714,"nce rewriting algorithm, which can resolve the 1-N mismatch problem by rewriting a word using its explanation in a dictionary. The other is a template-based sentence rewriting algorithm, which can resolve the N-1 mismatch problem by rewriting complicated expressions using paraphrase template pairs. Given the generated rewritings of a sentence, we propose a ranking function to jointly choose the optimal rewriting and the correct logical form, by taking both the rewriting features and the semantic parsing features into consideration. We conduct experiments on the benchmark WEBQUESTIONS dataset (Berant et al., 2013). Experimental results show that our method can effectively resolve the vocabulary mismatch problem and achieve accurate and robust performance. The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 describes our sentence rewriting method for semantic parsing. Section 4 presents the scoring function which can jointly ranks rewritings and logical forms. Section 5 discusses experimental results. Section 6 concludes this paper. Table 1: Examples of (a) sentences s0 , possible logical form l0 from traditional semantic parser, result r0 for the logical form l0 ;"
P16-1073,D14-1067,0,0.0908651,"not maximize reward nor the log-likelihood. However, the reward provides a way to modulate the magnitude of the updates. Specifically, after each update, our model results in making the derivation, which has the highest reward, to get a bigger score. Table 7 presents our learning algorithm. 4.4 Experiments Features As described in Section 4.3, our model uses two kinds of features. One for the semantic parsing module which are simply the same features described in Berant and Liang (2015). One for the 771 System Berant et al., 2013 Yao and Van-Durme, 2014 Berant and Liang, 2014 Bao et al., 2014 Bordes et al., 2014a Yang et al., 2014 Bast and Haussmann, 2015 Yao, 2015 Berant and Liang, 2015 Yih et al., 2015 Our approach all sentences have different structure with their target logical forms, e.g., “Who is keyshia cole dad?” and “What countries have german as the official language?”. System Settings: In our experiments, we use the Freebase Search API for entity lookup. We load Freebase using Virtuoso, and execute logical forms by converting them to SPARQL and querying using Virtuoso. We learn the parameters of our system by making three passes over the training dataset, with the beam size K = 200, the dic"
P16-1073,P09-1069,0,0.0243802,"andhi”, because it cannot identify the vocabulary mismatch between “daughter” and child∧female1 . By contrast, by rewriting “daughter” into “female child”, our method can resolve this vocabulary mismatch. Specifically, we identify two common types of vocabulary mismatch in semantic parsing: 2 Related Work Semantic parsing has attracted considerable research attention in recent years. Generally, semantic parsing methods can be categorized into synchronous context free grammars (SCFG) based methods (Wong and Mooney, 2007; Arthur et al., 2015; Li et al., 2015), syntactic structure based methods (Ge and Mooney, 2009; Reddy et al., 2014; Reddy et al., 2016), combinatory categorical grammars (CCG) based methods (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014; Wang et al., 2014; Artzi et al., 2015), and dependency-based compositional semantics (DCS) based methods (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014; Berant and Liang, 2015; Pasupat and Liang, 2015; Wang et al., 2015). One major challenge of semantic parsing is how to scale to open-domain situation like Freebase and Web. A possible solution is to learn lexicons f"
P16-1073,P13-1042,0,0.0352332,"is to learn lexicons from large amount of web text and a knowledge base using a distant supervised method (Krishna1. 1-N mismatch: a simple word may correspond to a compound formula. For example, the word “daughter” may correspond to the compound formula child∧female. 2. N-1 mismatch: a logical constant may correspond to a complicated natural language expression, e.g., the formula population can be expressed using many phrases such as “how many people” and “live in”. 1 In this paper, we may simplify logical forms for readability, e.g., female for gender.female. 767 murthy and Mitchell, 2012; Cai and Yates, 2013a; Berant et al., 2013). Another challenge is how to alleviate the burden of annotation. A possible solution is to employ distant-supervised techniques (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013b; Artzi and Zettlemoyer, 2013), or unsupervised techniques (Poon and Domingos, 2009; Goldwasser et al., 2011; Poon, 2013). There were also several approaches focused on the mismatch problem. Kwiatkowski et al. (2013) addressed the ontology mismatch problem (i.e., two ontologies using different vocabularies) by first parsing a sentence into a domainindependent underspecified logical f"
P16-1073,P11-1149,0,0.0189162,"nd to a complicated natural language expression, e.g., the formula population can be expressed using many phrases such as “how many people” and “live in”. 1 In this paper, we may simplify logical forms for readability, e.g., female for gender.female. 767 murthy and Mitchell, 2012; Cai and Yates, 2013a; Berant et al., 2013). Another challenge is how to alleviate the burden of annotation. A possible solution is to employ distant-supervised techniques (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013b; Artzi and Zettlemoyer, 2013), or unsupervised techniques (Poon and Domingos, 2009; Goldwasser et al., 2011; Poon, 2013). There were also several approaches focused on the mismatch problem. Kwiatkowski et al. (2013) addressed the ontology mismatch problem (i.e., two ontologies using different vocabularies) by first parsing a sentence into a domainindependent underspecified logical form, and then using an ontology matching model to transform this underspecified logical form to the target ontology. However, their method is still hard to deal with the 1-N and the N-1 mismatch problems between natural language and target ontologies. Berant and Liang (2014) addressed the structure mismatch problem betwe"
P16-1073,S13-1045,0,0.0468936,"is to learn lexicons from large amount of web text and a knowledge base using a distant supervised method (Krishna1. 1-N mismatch: a simple word may correspond to a compound formula. For example, the word “daughter” may correspond to the compound formula child∧female. 2. N-1 mismatch: a logical constant may correspond to a complicated natural language expression, e.g., the formula population can be expressed using many phrases such as “how many people” and “live in”. 1 In this paper, we may simplify logical forms for readability, e.g., female for gender.female. 767 murthy and Mitchell, 2012; Cai and Yates, 2013a; Berant et al., 2013). Another challenge is how to alleviate the burden of annotation. A possible solution is to employ distant-supervised techniques (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013b; Artzi and Zettlemoyer, 2013), or unsupervised techniques (Poon and Domingos, 2009; Goldwasser et al., 2011; Poon, 2013). There were also several approaches focused on the mismatch problem. Kwiatkowski et al. (2013) addressed the ontology mismatch problem (i.e., two ontologies using different vocabularies) by first parsing a sentence into a domainindependent underspecified logical f"
P16-1073,D14-1116,0,0.0121107,"parsed into the two different logical forms lf1 and lf2 using different ontologies. s1 s2 lf1 lf2 Introduction Semantic parsing is the task of mapping natural language sentences into logical forms which can be executed on a knowledge base (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowksi et al., 2010). Figure 1 shows an example of semantic parsing. Semantic parsing is a fundamental technique of natural language understanding, and has been used in many applications, such as question answering (Liang et al., 2011; He et al., 2014; Zhang et al., 2016) and information extraction (Krishnamurthy and Mitchell, 2012; Choi et al., 2015; Parikh et al., 2015). Semantic parsing, however, is a challenging What is the population of Berlin? How many people live in Berlin? λx.population(Berlin,x) count(λx.person(x)∧live(x,Berlin)) Based on the above observations, one major challenge of semantic parsing is the structural mismatch between a natural language sentence and its target logical form, which are mainly raised by the vocabulary mismatch between natural language and ontologies. Intuitively, if a sentence has the same structure"
P16-1073,P15-1127,0,0.0507564,"Introduction Semantic parsing is the task of mapping natural language sentences into logical forms which can be executed on a knowledge base (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowksi et al., 2010). Figure 1 shows an example of semantic parsing. Semantic parsing is a fundamental technique of natural language understanding, and has been used in many applications, such as question answering (Liang et al., 2011; He et al., 2014; Zhang et al., 2016) and information extraction (Krishnamurthy and Mitchell, 2012; Choi et al., 2015; Parikh et al., 2015). Semantic parsing, however, is a challenging What is the population of Berlin? How many people live in Berlin? λx.population(Berlin,x) count(λx.person(x)∧live(x,Berlin)) Based on the above observations, one major challenge of semantic parsing is the structural mismatch between a natural language sentence and its target logical form, which are mainly raised by the vocabulary mismatch between natural language and ontologies. Intuitively, if a sentence has the same structure with its target logical form, it is easy to get the correct parse, e.g., a semantic parser can easil"
P16-1073,D15-1006,0,0.0529325,"Missing"
P16-1073,W10-2903,0,0.043666,"nd formula. For example, the word “daughter” may correspond to the compound formula child∧female. 2. N-1 mismatch: a logical constant may correspond to a complicated natural language expression, e.g., the formula population can be expressed using many phrases such as “how many people” and “live in”. 1 In this paper, we may simplify logical forms for readability, e.g., female for gender.female. 767 murthy and Mitchell, 2012; Cai and Yates, 2013a; Berant et al., 2013). Another challenge is how to alleviate the burden of annotation. A possible solution is to employ distant-supervised techniques (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013b; Artzi and Zettlemoyer, 2013), or unsupervised techniques (Poon and Domingos, 2009; Goldwasser et al., 2011; Poon, 2013). There were also several approaches focused on the mismatch problem. Kwiatkowski et al. (2013) addressed the ontology mismatch problem (i.e., two ontologies using different vocabularies) by first parsing a sentence into a domainindependent underspecified logical form, and then using an ontology matching model to transform this underspecified logical form to the target ontology. However, their method is still hard to deal with the 1-"
P16-1073,P06-1115,0,0.0129722,"be expressed using different sentences. Furthermore, because logical forms depend on the vocabulary of targetontology, a sentence will be parsed into different logical forms when using different ontologies. For example, in below the two sentences s1 and s2 express the same meaning, and they both can be parsed into the two different logical forms lf1 and lf2 using different ontologies. s1 s2 lf1 lf2 Introduction Semantic parsing is the task of mapping natural language sentences into logical forms which can be executed on a knowledge base (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowksi et al., 2010). Figure 1 shows an example of semantic parsing. Semantic parsing is a fundamental technique of natural language understanding, and has been used in many applications, such as question answering (Liang et al., 2011; He et al., 2014; Zhang et al., 2016) and information extraction (Krishnamurthy and Mitchell, 2012; Choi et al., 2015; Parikh et al., 2015). Semantic parsing, however, is a challenging What is the population of Berlin? How many people live in Berlin? λx.population(Berlin,x) count(λx.person(x)∧live(x,Berlin)) Based on"
P16-1073,P05-1066,0,0.0815997,"nstructing candidate logical forms. Compared with these two methods, we approach the mismatch problem in the parsing stage, which can greatly reduce the difficulty of constructing the correct logical form, through rewriting sentences into the forms which will be structurally consistent with their target logic forms. Sentence rewriting (or paraphrase generation) is the task of generating new sentences that have the same meaning as the original one. Sentence rewriting has been used in many different tasks, e.g., used in statistical machine translation to resolve the word order mismatch problem (Collins et al., 2005; He et al., 2015). To our best knowledge, this paper is the first work to apply sentence rewriting for vocabulary mismatch problem in semantic parsing. 3 Logical Form Word son actress father child∧male actor∧female parent∧male grandaprent parent∧parent brother sibling∧male Wiktionary Explanation male child female actor male parent parent of one’s parent male sibling Table 2: Several examples of words, their logical forms and their explanations in Wiktionary. solving the mismatch problem. Specifically, we solve the 1-N mismatch problem by dictionarybased rewriting and solve the N-1 mismatch pr"
P16-1073,D12-1069,0,0.170193,"Missing"
P16-1073,P14-1112,0,0.126495,"Missing"
P16-1073,P15-1142,0,0.0135902,"e categorized into synchronous context free grammars (SCFG) based methods (Wong and Mooney, 2007; Arthur et al., 2015; Li et al., 2015), syntactic structure based methods (Ge and Mooney, 2009; Reddy et al., 2014; Reddy et al., 2016), combinatory categorical grammars (CCG) based methods (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014; Wang et al., 2014; Artzi et al., 2015), and dependency-based compositional semantics (DCS) based methods (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014; Berant and Liang, 2015; Pasupat and Liang, 2015; Wang et al., 2015). One major challenge of semantic parsing is how to scale to open-domain situation like Freebase and Web. A possible solution is to learn lexicons from large amount of web text and a knowledge base using a distant supervised method (Krishna1. 1-N mismatch: a simple word may correspond to a compound formula. For example, the word “daughter” may correspond to the compound formula child∧female. 2. N-1 mismatch: a logical constant may correspond to a complicated natural language expression, e.g., the formula population can be expressed using many phrases such as “how many peopl"
P16-1073,D09-1001,0,0.0362893,"cal constant may correspond to a complicated natural language expression, e.g., the formula population can be expressed using many phrases such as “how many people” and “live in”. 1 In this paper, we may simplify logical forms for readability, e.g., female for gender.female. 767 murthy and Mitchell, 2012; Cai and Yates, 2013a; Berant et al., 2013). Another challenge is how to alleviate the burden of annotation. A possible solution is to employ distant-supervised techniques (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013b; Artzi and Zettlemoyer, 2013), or unsupervised techniques (Poon and Domingos, 2009; Goldwasser et al., 2011; Poon, 2013). There were also several approaches focused on the mismatch problem. Kwiatkowski et al. (2013) addressed the ontology mismatch problem (i.e., two ontologies using different vocabularies) by first parsing a sentence into a domainindependent underspecified logical form, and then using an ontology matching model to transform this underspecified logical form to the target ontology. However, their method is still hard to deal with the 1-N and the N-1 mismatch problems between natural language and target ontologies. Berant and Liang (2014) addressed the structu"
P16-1073,D10-1119,0,0.308222,"ogical forms depend on the vocabulary of targetontology, a sentence will be parsed into different logical forms when using different ontologies. For example, in below the two sentences s1 and s2 express the same meaning, and they both can be parsed into the two different logical forms lf1 and lf2 using different ontologies. s1 s2 lf1 lf2 Introduction Semantic parsing is the task of mapping natural language sentences into logical forms which can be executed on a knowledge base (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowksi et al., 2010). Figure 1 shows an example of semantic parsing. Semantic parsing is a fundamental technique of natural language understanding, and has been used in many applications, such as question answering (Liang et al., 2011; He et al., 2014; Zhang et al., 2016) and information extraction (Krishnamurthy and Mitchell, 2012; Choi et al., 2015; Parikh et al., 2015). Semantic parsing, however, is a challenging What is the population of Berlin? How many people live in Berlin? λx.population(Berlin,x) count(λx.person(x)∧live(x,Berlin)) Based on the above observations, one major challenge of semantic parsing is"
P16-1073,P13-1092,0,0.035741,"al language expression, e.g., the formula population can be expressed using many phrases such as “how many people” and “live in”. 1 In this paper, we may simplify logical forms for readability, e.g., female for gender.female. 767 murthy and Mitchell, 2012; Cai and Yates, 2013a; Berant et al., 2013). Another challenge is how to alleviate the burden of annotation. A possible solution is to employ distant-supervised techniques (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013b; Artzi and Zettlemoyer, 2013), or unsupervised techniques (Poon and Domingos, 2009; Goldwasser et al., 2011; Poon, 2013). There were also several approaches focused on the mismatch problem. Kwiatkowski et al. (2013) addressed the ontology mismatch problem (i.e., two ontologies using different vocabularies) by first parsing a sentence into a domainindependent underspecified logical form, and then using an ontology matching model to transform this underspecified logical form to the target ontology. However, their method is still hard to deal with the 1-N and the N-1 mismatch problems between natural language and target ontologies. Berant and Liang (2014) addressed the structure mismatch problem between natural la"
P16-1073,D11-1140,0,0.140951,"solve this vocabulary mismatch. Specifically, we identify two common types of vocabulary mismatch in semantic parsing: 2 Related Work Semantic parsing has attracted considerable research attention in recent years. Generally, semantic parsing methods can be categorized into synchronous context free grammars (SCFG) based methods (Wong and Mooney, 2007; Arthur et al., 2015; Li et al., 2015), syntactic structure based methods (Ge and Mooney, 2009; Reddy et al., 2014; Reddy et al., 2016), combinatory categorical grammars (CCG) based methods (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014; Wang et al., 2014; Artzi et al., 2015), and dependency-based compositional semantics (DCS) based methods (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014; Berant and Liang, 2015; Pasupat and Liang, 2015; Wang et al., 2015). One major challenge of semantic parsing is how to scale to open-domain situation like Freebase and Web. A possible solution is to learn lexicons from large amount of web text and a knowledge base using a distant supervised method (Krishna1. 1-N mismatch: a simple word may correspond to a compound formula. For example, the"
P16-1073,Q14-1030,0,0.017677,"nnot identify the vocabulary mismatch between “daughter” and child∧female1 . By contrast, by rewriting “daughter” into “female child”, our method can resolve this vocabulary mismatch. Specifically, we identify two common types of vocabulary mismatch in semantic parsing: 2 Related Work Semantic parsing has attracted considerable research attention in recent years. Generally, semantic parsing methods can be categorized into synchronous context free grammars (SCFG) based methods (Wong and Mooney, 2007; Arthur et al., 2015; Li et al., 2015), syntactic structure based methods (Ge and Mooney, 2009; Reddy et al., 2014; Reddy et al., 2016), combinatory categorical grammars (CCG) based methods (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014; Wang et al., 2014; Artzi et al., 2015), and dependency-based compositional semantics (DCS) based methods (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014; Berant and Liang, 2015; Pasupat and Liang, 2015; Wang et al., 2015). One major challenge of semantic parsing is how to scale to open-domain situation like Freebase and Web. A possible solution is to learn lexicons from large amount of"
P16-1073,D13-1161,0,0.0857147,"phrases such as “how many people” and “live in”. 1 In this paper, we may simplify logical forms for readability, e.g., female for gender.female. 767 murthy and Mitchell, 2012; Cai and Yates, 2013a; Berant et al., 2013). Another challenge is how to alleviate the burden of annotation. A possible solution is to employ distant-supervised techniques (Clarke et al., 2010; Liang et al., 2011; Cai and Yates, 2013b; Artzi and Zettlemoyer, 2013), or unsupervised techniques (Poon and Domingos, 2009; Goldwasser et al., 2011; Poon, 2013). There were also several approaches focused on the mismatch problem. Kwiatkowski et al. (2013) addressed the ontology mismatch problem (i.e., two ontologies using different vocabularies) by first parsing a sentence into a domainindependent underspecified logical form, and then using an ontology matching model to transform this underspecified logical form to the target ontology. However, their method is still hard to deal with the 1-N and the N-1 mismatch problems between natural language and target ontologies. Berant and Liang (2014) addressed the structure mismatch problem between natural language and ontology by generating a set of canonical utterances for each candidate logical form"
P16-1073,D15-1170,0,0.134371,"ional method will result in the wrong answer “Rahul Gandhi”, because it cannot identify the vocabulary mismatch between “daughter” and child∧female1 . By contrast, by rewriting “daughter” into “female child”, our method can resolve this vocabulary mismatch. Specifically, we identify two common types of vocabulary mismatch in semantic parsing: 2 Related Work Semantic parsing has attracted considerable research attention in recent years. Generally, semantic parsing methods can be categorized into synchronous context free grammars (SCFG) based methods (Wong and Mooney, 2007; Arthur et al., 2015; Li et al., 2015), syntactic structure based methods (Ge and Mooney, 2009; Reddy et al., 2014; Reddy et al., 2016), combinatory categorical grammars (CCG) based methods (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014; Wang et al., 2014; Artzi et al., 2015), and dependency-based compositional semantics (DCS) based methods (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014; Berant and Liang, 2015; Pasupat and Liang, 2015; Wang et al., 2015). One major challenge of semantic parsing is how to scale to open-domain situation like Free"
P16-1073,D14-1135,0,0.0598556,"o common types of vocabulary mismatch in semantic parsing: 2 Related Work Semantic parsing has attracted considerable research attention in recent years. Generally, semantic parsing methods can be categorized into synchronous context free grammars (SCFG) based methods (Wong and Mooney, 2007; Arthur et al., 2015; Li et al., 2015), syntactic structure based methods (Ge and Mooney, 2009; Reddy et al., 2014; Reddy et al., 2016), combinatory categorical grammars (CCG) based methods (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014; Wang et al., 2014; Artzi et al., 2015), and dependency-based compositional semantics (DCS) based methods (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014; Berant and Liang, 2015; Pasupat and Liang, 2015; Wang et al., 2015). One major challenge of semantic parsing is how to scale to open-domain situation like Freebase and Web. A possible solution is to learn lexicons from large amount of web text and a knowledge base using a distant supervised method (Krishna1. 1-N mismatch: a simple word may correspond to a compound formula. For example, the word “daughter” may correspond to the compound formul"
P16-1073,P11-1060,0,0.515354,"and they both can be parsed into the two different logical forms lf1 and lf2 using different ontologies. s1 s2 lf1 lf2 Introduction Semantic parsing is the task of mapping natural language sentences into logical forms which can be executed on a knowledge base (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowksi et al., 2010). Figure 1 shows an example of semantic parsing. Semantic parsing is a fundamental technique of natural language understanding, and has been used in many applications, such as question answering (Liang et al., 2011; He et al., 2014; Zhang et al., 2016) and information extraction (Krishnamurthy and Mitchell, 2012; Choi et al., 2015; Parikh et al., 2015). Semantic parsing, however, is a challenging What is the population of Berlin? How many people live in Berlin? λx.population(Berlin,x) count(λx.person(x)∧live(x,Berlin)) Based on the above observations, one major challenge of semantic parsing is the structural mismatch between a natural language sentence and its target logical form, which are mainly raised by the vocabulary mismatch between natural language and ontologies. Intuitively, if a sentence has t"
P16-1073,P15-1129,0,0.0467792,"Missing"
P16-1073,J13-2005,0,0.0282305,"ighest reward out of the K (beam size) root derivations, using local reweighting and history compression. 4.2 Scoring Function To select the best semantic parse, we propose a scoring function which can take both sentence rewriting features and semantic parsing features into consideration. Given a sentence x, a generated rewriting x0 and the derivation d of x0 , we score them using follow function: Base Semantic Parser In this paper, we produce logical forms for each sentence rewritings using an agenda-based semantic parser (Berant and Liang, 2015), which is based on the lambda-DCS proposed by Liang (2013). For parsing, we use the lexicons and the grammars released by Berant et al. (2013), where lexicons are used to trigger unary and binary predicates, and grammars are used to conduct logical forms. The only difference is that we also use the composition rule to make the parser can handle complicated questions involving two binary predicates, e.g., child.obama∧gender.female. score(x, x0 , d) = θ · φ(x, x0 , d) = θ1 · φ(x, x0 ) + θ2 · φ(x0 , d) This scoring function is decomposed into two parts: one for sentence rewriting – θ1 · φ(x, x0 ) and the other for semantic parsing – θ2 · φ(x0 , d). Foll"
P16-1073,P07-1121,0,0.104186,"ferent sentences. Furthermore, because logical forms depend on the vocabulary of targetontology, a sentence will be parsed into different logical forms when using different ontologies. For example, in below the two sentences s1 and s2 express the same meaning, and they both can be parsed into the two different logical forms lf1 and lf2 using different ontologies. s1 s2 lf1 lf2 Introduction Semantic parsing is the task of mapping natural language sentences into logical forms which can be executed on a knowledge base (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowksi et al., 2010). Figure 1 shows an example of semantic parsing. Semantic parsing is a fundamental technique of natural language understanding, and has been used in many applications, such as question answering (Liang et al., 2011; He et al., 2014; Zhang et al., 2016) and information extraction (Krishnamurthy and Mitchell, 2012; Choi et al., 2015; Parikh et al., 2015). Semantic parsing, however, is a challenging What is the population of Berlin? How many people live in Berlin? λx.population(Berlin,x) count(λx.person(x)∧live(x,Berlin)) Based on the above observations"
P16-1073,D08-1082,0,0.0714934,"ermore, because logical forms depend on the vocabulary of targetontology, a sentence will be parsed into different logical forms when using different ontologies. For example, in below the two sentences s1 and s2 express the same meaning, and they both can be parsed into the two different logical forms lf1 and lf2 using different ontologies. s1 s2 lf1 lf2 Introduction Semantic parsing is the task of mapping natural language sentences into logical forms which can be executed on a knowledge base (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowksi et al., 2010). Figure 1 shows an example of semantic parsing. Semantic parsing is a fundamental technique of natural language understanding, and has been used in many applications, such as question answering (Liang et al., 2011; He et al., 2014; Zhang et al., 2016) and information extraction (Krishnamurthy and Mitchell, 2012; Choi et al., 2015; Parikh et al., 2015). Semantic parsing, however, is a challenging What is the population of Berlin? How many people live in Berlin? λx.population(Berlin,x) count(λx.person(x)∧live(x,Berlin)) Based on the above observations, one major chall"
P16-1073,D14-1071,0,0.0476256,"or the log-likelihood. However, the reward provides a way to modulate the magnitude of the updates. Specifically, after each update, our model results in making the derivation, which has the highest reward, to get a bigger score. Table 7 presents our learning algorithm. 4.4 Experiments Features As described in Section 4.3, our model uses two kinds of features. One for the semantic parsing module which are simply the same features described in Berant and Liang (2015). One for the 771 System Berant et al., 2013 Yao and Van-Durme, 2014 Berant and Liang, 2014 Bao et al., 2014 Bordes et al., 2014a Yang et al., 2014 Bast and Haussmann, 2015 Yao, 2015 Berant and Liang, 2015 Yih et al., 2015 Our approach all sentences have different structure with their target logical forms, e.g., “Who is keyshia cole dad?” and “What countries have german as the official language?”. System Settings: In our experiments, we use the Freebase Search API for entity lookup. We load Freebase using Virtuoso, and execute logical forms by converting them to SPARQL and querying using Virtuoso. We learn the parameters of our system by making three passes over the training dataset, with the beam size K = 200, the dictionary rewriting s"
P16-1073,N15-1077,0,0.0124915,"ic parsing is the task of mapping natural language sentences into logical forms which can be executed on a knowledge base (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowksi et al., 2010). Figure 1 shows an example of semantic parsing. Semantic parsing is a fundamental technique of natural language understanding, and has been used in many applications, such as question answering (Liang et al., 2011; He et al., 2014; Zhang et al., 2016) and information extraction (Krishnamurthy and Mitchell, 2012; Choi et al., 2015; Parikh et al., 2015). Semantic parsing, however, is a challenging What is the population of Berlin? How many people live in Berlin? λx.population(Berlin,x) count(λx.person(x)∧live(x,Berlin)) Based on the above observations, one major challenge of semantic parsing is the structural mismatch between a natural language sentence and its target logical form, which are mainly raised by the vocabulary mismatch between natural language and ontologies. Intuitively, if a sentence has the same structure with its target logical form, it is easy to get the correct parse, e.g., a semantic parser can easily parse s1 into lf1 an"
P16-1073,N15-3014,0,0.111636,"provides a way to modulate the magnitude of the updates. Specifically, after each update, our model results in making the derivation, which has the highest reward, to get a bigger score. Table 7 presents our learning algorithm. 4.4 Experiments Features As described in Section 4.3, our model uses two kinds of features. One for the semantic parsing module which are simply the same features described in Berant and Liang (2015). One for the 771 System Berant et al., 2013 Yao and Van-Durme, 2014 Berant and Liang, 2014 Bao et al., 2014 Bordes et al., 2014a Yang et al., 2014 Bast and Haussmann, 2015 Yao, 2015 Berant and Liang, 2015 Yih et al., 2015 Our approach all sentences have different structure with their target logical forms, e.g., “Who is keyshia cole dad?” and “What countries have german as the official language?”. System Settings: In our experiments, we use the Freebase Search API for entity lookup. We load Freebase using Virtuoso, and execute logical forms by converting them to SPARQL and querying using Virtuoso. We learn the parameters of our system by making three passes over the training dataset, with the beam size K = 200, the dictionary rewriting size KD = 100, and the template rewr"
P16-1073,P15-1128,0,0.0477218,"gnitude of the updates. Specifically, after each update, our model results in making the derivation, which has the highest reward, to get a bigger score. Table 7 presents our learning algorithm. 4.4 Experiments Features As described in Section 4.3, our model uses two kinds of features. One for the semantic parsing module which are simply the same features described in Berant and Liang (2015). One for the 771 System Berant et al., 2013 Yao and Van-Durme, 2014 Berant and Liang, 2014 Bao et al., 2014 Bordes et al., 2014a Yang et al., 2014 Bast and Haussmann, 2015 Yao, 2015 Berant and Liang, 2015 Yih et al., 2015 Our approach all sentences have different structure with their target logical forms, e.g., “Who is keyshia cole dad?” and “What countries have german as the official language?”. System Settings: In our experiments, we use the Freebase Search API for entity lookup. We load Freebase using Virtuoso, and execute logical forms by converting them to SPARQL and querying using Virtuoso. We learn the parameters of our system by making three passes over the training dataset, with the beam size K = 200, the dictionary rewriting size KD = 100, and the template rewriting size KT = 100. Baselines: We compa"
P16-1073,D07-1071,0,0.108714,"writing “daughter” into “female child”, our method can resolve this vocabulary mismatch. Specifically, we identify two common types of vocabulary mismatch in semantic parsing: 2 Related Work Semantic parsing has attracted considerable research attention in recent years. Generally, semantic parsing methods can be categorized into synchronous context free grammars (SCFG) based methods (Wong and Mooney, 2007; Arthur et al., 2015; Li et al., 2015), syntactic structure based methods (Ge and Mooney, 2009; Reddy et al., 2014; Reddy et al., 2016), combinatory categorical grammars (CCG) based methods (Zettlemoyer and Collins, 2007; Kwiatkowksi et al., 2010; Kwiatkowski et al., 2011; Krishnamurthy and Mitchell, 2014; Wang et al., 2014; Artzi et al., 2015), and dependency-based compositional semantics (DCS) based methods (Liang et al., 2011; Berant et al., 2013; Berant and Liang, 2014; Berant and Liang, 2015; Pasupat and Liang, 2015; Wang et al., 2015). One major challenge of semantic parsing is how to scale to open-domain situation like Freebase and Web. A possible solution is to learn lexicons from large amount of web text and a knowledge base using a distant supervised method (Krishna1. 1-N mismatch: a simple word may"
P16-1073,P14-1090,0,\N,Missing
P18-1071,D13-1160,0,0.123172,"ng has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017). Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars. The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013), DCS (Liang et al., 2011; Berant et al., 2013), etc. As discussed above, the main drawback of grammar-based methods is that they rely on high-quality lexicons, manually-built grammars, and hand-crafted features. In recent years, one promising direction of semantic parsing is to use semantic graph as representation. Thus semantic parsing is modeled as a semantic graph generation process. Ge and Mooney (2009) build semantic graph by trans773 straints can be exploited for more accurate decoding. We believe this can also be used to enhance previous transition based methods and may also be used in other parsing tasks, e.g., AMR parsing. 6 Hann"
P18-1071,P13-1042,0,0.0406834,"ohn et al., 2016) 5 Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017). Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars. The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013), DCS (Liang et al., 2011; Berant et al., 2013), etc. As discussed above, the main drawback of grammar-based methods is that they rely on high-quality lexicons, manually-built grammars, and hand-crafted features. In recent years, one promising direction of semantic parsing is to use semantic graph as representation. Thus semantic parsing is modeled as a semantic graph generation process. Ge and Mooney (2009) build semantic graph by trans773 straints can be exploited for more accurate decoding. We believe this can also be used to enhance previous transition based methods and may also be used in"
P18-1071,D14-1179,0,0.0423244,"Missing"
P18-1071,D15-1198,0,0.0185982,"ed, the attention model does not take the alignment history into consideration, makes some words are ignored during parsing. For example in the second case in Table 5, “first class” is ignored during the decoding process. This problem can be further solved using explicit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016) 5 Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017). Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars. The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013), DCS (Liang et al., 2011; Berant et al., 2013), etc. As discussed above, the main drawback of grammar-based methods is that they rely on high-quality lexicons, manually-built grammars, and hand-crafted features. In recent years, one promising directi"
P18-1071,Q13-1005,0,0.0500167,"ctures into normal ones. Under-Mapping. As Dong and Lapata (2016) discussed, the attention model does not take the alignment history into consideration, makes some words are ignored during parsing. For example in the second case in Table 5, “first class” is ignored during the decoding process. This problem can be further solved using explicit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016) 5 Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017). Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars. The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013), DCS (Liang et al., 2011; Berant et al., 2013), etc. As discussed above, the main drawback of grammar-based methods is that they rely on high-quality lexicons, manually-built grammars, a"
P18-1071,W10-2903,0,0.0492681,"6; Dong et al., 2017) to transform unseen sentence structures into normal ones. Under-Mapping. As Dong and Lapata (2016) discussed, the attention model does not take the alignment history into consideration, makes some words are ignored during parsing. For example in the second case in Table 5, “first class” is ignored during the decoding process. This problem can be further solved using explicit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016) 5 Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017). Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars. The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013), DCS (Liang et al., 2011; Berant et al., 2013), etc. As discussed above, the main drawback of grammar-based methods is that"
P18-1071,N16-1102,0,0.0175279,"the question words “how many”. For this problem, we can employ sentence rewriting or paraphrasing techniques (Chen et al., 2016; Dong et al., 2017) to transform unseen sentence structures into normal ones. Under-Mapping. As Dong and Lapata (2016) discussed, the attention model does not take the alignment history into consideration, makes some words are ignored during parsing. For example in the second case in Table 5, “first class” is ignored during the decoding process. This problem can be further solved using explicit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016) 5 Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017). Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars. The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 20"
P18-1071,P06-1115,0,0.0694785,"iques (Chen et al., 2016; Dong et al., 2017) to transform unseen sentence structures into normal ones. Under-Mapping. As Dong and Lapata (2016) discussed, the attention model does not take the alignment history into consideration, makes some words are ignored during parsing. For example in the second case in Table 5, “first class” is ignored during the decoding process. This problem can be further solved using explicit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016) 5 Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017). Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars. The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013), DCS (Liang et al., 2011; Berant et al., 2013), etc. As discussed above, the main drawback of grammar-"
P18-1071,P16-1004,0,0.48633,"(Zelle and Mooney, 1996), ATIS (He and Young, 2005) and OVERNIGHT (Wang et al., 2015b). The results show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on G EO and ATIS datasets. The main contributions of this paper are summarized as follows: In recent years, RNN models have achieved success in sequence-to-sequence problems due to its strong ability on both representation learning and prediction, e.g., in machine translation (Cho et al., 2014). A lot of Seq2Seq models have also been employed for semantic parsing (Xiao et al., 2016; Dong and Lapata, 2016; Jia and Liang, 2016), where a sentence is parsed by translating it to linearized logical form using RNN models. There is no need for high-quality lexicons, manually-built grammars, and hand-crafted features. These models are trained end-to-end, and can leverage attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) to learn soft alignments between sentences and logical forms. In this paper, we propose a new neural semantic parsing framework – Sequence-to-Action, which can simultaneously leverage the advantages of semantic graph representation and the strong prediction ability of Seq"
P18-1071,D17-1160,0,0.0916058,"y neural Seq2Seq models, which models semantic parsing as an end-to-end, sentence to logical form machine translation problem. Dong and Lapata (2016), Jia and Liang (2016) and Xiao et al. (2016) transform word sequence to linearized logical forms. One main drawback of these methods is that it is hard to capture and exploit structure and semantic constraints using linearized logical forms. Dong and Lapata (2016) propose a Seq2Tree model to capture the hierarchical structure of logical forms. It has been shown that structure and semantic constraints are effective for enhancing semantic parsing. Krishnamurthy et al. (2017) use type constraints to filter illegal tokens. Liang et al. (2017) adopt a Lisp interpreter with pre-defined functions to produce valid tokens. Iyyer et al. (2017) adopt type constraints to generate valid actions. Inspired by these approaches, we also incorporate both structure and semantic constraints in our neural sequence-to-action model. Transition-based approaches are important in both dependency parsing (Nivre, 2008; Henderson et al., 2013) and AMR parsing (Wang et al., 2015a). In semantic parsing, our method has a tight-coupling with knowledge bases, and conand informal structure, wher"
P18-1071,D17-1091,0,0.0269511,"erate valid actions. Inspired by these approaches, we also incorporate both structure and semantic constraints in our neural sequence-to-action model. Transition-based approaches are important in both dependency parsing (Nivre, 2008; Henderson et al., 2013) and AMR parsing (Wang et al., 2015a). In semantic parsing, our method has a tight-coupling with knowledge bases, and conand informal structure, where entity word “Iowa” and relation word “borders” appear ahead of the question words “how many”. For this problem, we can employ sentence rewriting or paraphrasing techniques (Chen et al., 2016; Dong et al., 2017) to transform unseen sentence structures into normal ones. Under-Mapping. As Dong and Lapata (2016) discussed, the attention model does not take the alignment history into consideration, makes some words are ignored during parsing. For example in the second case in Table 5, “first class” is ignored during the decoding process. This problem can be further solved using explicit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016) 5 Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010;"
P18-1071,D12-1069,0,0.0381314,"to transform unseen sentence structures into normal ones. Under-Mapping. As Dong and Lapata (2016) discussed, the attention model does not take the alignment history into consideration, makes some words are ignored during parsing. For example in the second case in Table 5, “first class” is ignored during the decoding process. This problem can be further solved using explicit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016) 5 Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017). Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars. The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013), DCS (Liang et al., 2011; Berant et al., 2013), etc. As discussed above, the main drawback of grammar-based methods is that they rely on high-quality lexicon"
P18-1071,P09-1069,0,0.443602,"ror analysis. Each example includes the sentence for parsing, with gold parse and predicted parse from our model. forming syntactic tree. Bast and Haussmann (2015) identify the structure of a semantic query using three pre-defined patterns. Reddy et al. (2014, 2016) use Freebase-based semantic graph representation, and convert sentences to semantic graphs using CCG or dependency tree. Yih et al. (2015) generate semantic graphs using a staged heuristic search algorithm. These methods are all based on manually-designed, heuristic generation process, which may suffer from syntactic parse errors (Ge and Mooney, 2009; Reddy et al., 2014, 2016), structure mismatch (Chen et al., 2016), and are hard to deal with complex sentences (Yih et al., 2015). One other direction is to employ neural Seq2Seq models, which models semantic parsing as an end-to-end, sentence to logical form machine translation problem. Dong and Lapata (2016), Jia and Liang (2016) and Xiao et al. (2016) transform word sequence to linearized logical forms. One main drawback of these methods is that it is hard to capture and exploit structure and semantic constraints using linearized logical forms. Dong and Lapata (2016) propose a Seq2Tree mo"
P18-1071,D10-1119,0,0.117353,"Missing"
P18-1071,D13-1161,0,0.0284842,"have a tight-coupling with knowledge bases (Yih et al., 2015), and share many commonalities with syntactic structures (Reddy et al., 2014). Therefore both the structure and semantic constraints from knowledge bases can be easily exploited during parsing (Yih et al., 2015). The main challenge of semantic graph-based parsing is how to effectively construct the semantic graph of a sentence. Currently, semantic graphs Introduction Semantic parsing aims to map natural language sentences to logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2013). For example, the sentence “Which states border Texas?” will be mapped to answer (A, (state (A), next to (A, stateid ( texas )))). A semantic parser needs two functions, one for structure prediction and the other for semantic grounding. Traditional semantic parsers are usually based on compositional grammar, such as CCG (Zettlemoyer and Collins, 2005, 2007), DCS (Liang et al., 2011), etc. These parsers compose structure using manually designed grammars, use lexicons for semantic grounding, and exploit fea766 Proceedings of the 56th Annual Meeting of the Association for Computational Linguisti"
P18-1071,D11-1140,0,0.168631,"lation (Tu et al., 2016; Cohn et al., 2016) 5 Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017). Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars. The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013), DCS (Liang et al., 2011; Berant et al., 2013), etc. As discussed above, the main drawback of grammar-based methods is that they rely on high-quality lexicons, manually-built grammars, and hand-crafted features. In recent years, one promising direction of semantic parsing is to use semantic graph as representation. Thus semantic parsing is modeled as a semantic graph generation process. Ge and Mooney (2009) build semantic graph by trans773 straints can be exploited for more accurate decoding. We believe this can also be used to enhance previous transition based methods a"
P18-1071,P17-1167,0,0.0758544,"Missing"
P18-1071,D15-1170,0,0.0124839,"icit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016) 5 Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017). Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars. The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013), DCS (Liang et al., 2011; Berant et al., 2013), etc. As discussed above, the main drawback of grammar-based methods is that they rely on high-quality lexicons, manually-built grammars, and hand-crafted features. In recent years, one promising direction of semantic parsing is to use semantic graph as representation. Thus semantic parsing is modeled as a semantic graph generation process. Ge and Mooney (2009) build semantic graph by trans773 straints can be exploited for more accurate decoding. We believe this c"
P18-1071,P16-1002,0,0.357349,"6), ATIS (He and Young, 2005) and OVERNIGHT (Wang et al., 2015b). The results show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on G EO and ATIS datasets. The main contributions of this paper are summarized as follows: In recent years, RNN models have achieved success in sequence-to-sequence problems due to its strong ability on both representation learning and prediction, e.g., in machine translation (Cho et al., 2014). A lot of Seq2Seq models have also been employed for semantic parsing (Xiao et al., 2016; Dong and Lapata, 2016; Jia and Liang, 2016), where a sentence is parsed by translating it to linearized logical form using RNN models. There is no need for high-quality lexicons, manually-built grammars, and hand-crafted features. These models are trained end-to-end, and can leverage attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) to learn soft alignments between sentences and logical forms. In this paper, we propose a new neural semantic parsing framework – Sequence-to-Action, which can simultaneously leverage the advantages of semantic graph representation and the strong prediction ability of Seq2Seq models. Specifica"
P18-1071,P17-1003,0,0.142236,"ntence to logical form machine translation problem. Dong and Lapata (2016), Jia and Liang (2016) and Xiao et al. (2016) transform word sequence to linearized logical forms. One main drawback of these methods is that it is hard to capture and exploit structure and semantic constraints using linearized logical forms. Dong and Lapata (2016) propose a Seq2Tree model to capture the hierarchical structure of logical forms. It has been shown that structure and semantic constraints are effective for enhancing semantic parsing. Krishnamurthy et al. (2017) use type constraints to filter illegal tokens. Liang et al. (2017) adopt a Lisp interpreter with pre-defined functions to produce valid tokens. Iyyer et al. (2017) adopt type constraints to generate valid actions. Inspired by these approaches, we also incorporate both structure and semantic constraints in our neural sequence-to-action model. Transition-based approaches are important in both dependency parsing (Nivre, 2008; Henderson et al., 2013) and AMR parsing (Wang et al., 2015a). In semantic parsing, our method has a tight-coupling with knowledge bases, and conand informal structure, where entity word “Iowa” and relation word “borders” appear ahead of th"
P18-1071,P11-1060,0,0.678725,"urrently, semantic graphs Introduction Semantic parsing aims to map natural language sentences to logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2013). For example, the sentence “Which states border Texas?” will be mapped to answer (A, (state (A), next to (A, stateid ( texas )))). A semantic parser needs two functions, one for structure prediction and the other for semantic grounding. Traditional semantic parsers are usually based on compositional grammar, such as CCG (Zettlemoyer and Collins, 2005, 2007), DCS (Liang et al., 2011), etc. These parsers compose structure using manually designed grammars, use lexicons for semantic grounding, and exploit fea766 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 766–777 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics are either constructed by matching with patterns (Bast and Haussmann, 2015), transforming from dependency tree (Reddy et al., 2014, 2016), or via a staged heuristic search algorithm (Yih et al., 2015). These methods are all based on manuallydesigned, heuristic co"
P18-1071,D17-1009,0,0.0475946,"Missing"
P18-1071,D08-1082,0,0.105888,"semantic graphs have a tight-coupling with knowledge bases (Yih et al., 2015), and share many commonalities with syntactic structures (Reddy et al., 2014). Therefore both the structure and semantic constraints from knowledge bases can be easily exploited during parsing (Yih et al., 2015). The main challenge of semantic graph-based parsing is how to effectively construct the semantic graph of a sentence. Currently, semantic graphs Introduction Semantic parsing aims to map natural language sentences to logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2013). For example, the sentence “Which states border Texas?” will be mapped to answer (A, (state (A), next to (A, stateid ( texas )))). A semantic parser needs two functions, one for structure prediction and the other for semantic grounding. Traditional semantic parsers are usually based on compositional grammar, such as CCG (Zettlemoyer and Collins, 2005, 2007), DCS (Liang et al., 2011), etc. These parsers compose structure using manually designed grammars, use lexicons for semantic grounding, and exploit fea766 Proceedings of the 56th Annual Meeting of the Association"
P18-1071,P16-1008,0,0.0169141,"appear ahead of the question words “how many”. For this problem, we can employ sentence rewriting or paraphrasing techniques (Chen et al., 2016; Dong et al., 2017) to transform unseen sentence structures into normal ones. Under-Mapping. As Dong and Lapata (2016) discussed, the attention model does not take the alignment history into consideration, makes some words are ignored during parsing. For example in the second case in Table 5, “first class” is ignored during the decoding process. This problem can be further solved using explicit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016) 5 Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017). Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars. The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 201"
P18-1071,D15-1166,0,0.0251541,"els have achieved success in sequence-to-sequence problems due to its strong ability on both representation learning and prediction, e.g., in machine translation (Cho et al., 2014). A lot of Seq2Seq models have also been employed for semantic parsing (Xiao et al., 2016; Dong and Lapata, 2016; Jia and Liang, 2016), where a sentence is parsed by translating it to linearized logical form using RNN models. There is no need for high-quality lexicons, manually-built grammars, and hand-crafted features. These models are trained end-to-end, and can leverage attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) to learn soft alignments between sentences and logical forms. In this paper, we propose a new neural semantic parsing framework – Sequence-to-Action, which can simultaneously leverage the advantages of semantic graph representation and the strong prediction ability of Seq2Seq models. Specifically, we model semantic parsing as an end-to-end semantic graph generation process. For example in Figure 1, our model will parse the sentence “Which states border Texas” by generating a sequence of actions [add variable:A, add type:state, ...]. To achieve the above goal, we first design an action set whi"
P18-1071,N15-1040,0,0.41166,"n linearized logical forms. We find that the action sequence encoding can better capture structure and semantic information, and is more compact. And the parsing can be enhanced by exploiting structure and semantic constraints. For example, in G EO dataset, the action add edge:next to must subject to the semantic constraint that its arguments must be of type state and state, and the structure constraint that the edge next to must connect two nodes to form a valid graph. We evaluate our approach on three standard datasets: G EO (Zelle and Mooney, 1996), ATIS (He and Young, 2005) and OVERNIGHT (Wang et al., 2015b). The results show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on G EO and ATIS datasets. The main contributions of this paper are summarized as follows: In recent years, RNN models have achieved success in sequence-to-sequence problems due to its strong ability on both representation learning and prediction, e.g., in machine translation (Cho et al., 2014). A lot of Seq2Seq models have also been employed for semantic parsing (Xiao et al., 2016; Dong and Lapata, 2016; Jia and Liang, 2016), where a sentence is parsed by translatin"
P18-1071,J08-4003,0,0.0172109,"capture the hierarchical structure of logical forms. It has been shown that structure and semantic constraints are effective for enhancing semantic parsing. Krishnamurthy et al. (2017) use type constraints to filter illegal tokens. Liang et al. (2017) adopt a Lisp interpreter with pre-defined functions to produce valid tokens. Iyyer et al. (2017) adopt type constraints to generate valid actions. Inspired by these approaches, we also incorporate both structure and semantic constraints in our neural sequence-to-action model. Transition-based approaches are important in both dependency parsing (Nivre, 2008; Henderson et al., 2013) and AMR parsing (Wang et al., 2015a). In semantic parsing, our method has a tight-coupling with knowledge bases, and conand informal structure, where entity word “Iowa” and relation word “borders” appear ahead of the question words “how many”. For this problem, we can employ sentence rewriting or paraphrasing techniques (Chen et al., 2016; Dong et al., 2017) to transform unseen sentence structures into normal ones. Under-Mapping. As Dong and Lapata (2016) discussed, the attention model does not take the alignment history into consideration, makes some words are ignore"
P18-1071,P15-1129,0,0.321291,"Missing"
P18-1071,P13-1092,0,0.043283,"Missing"
P18-1071,P15-1085,0,0.0231068,"apata (2016) discussed, the attention model does not take the alignment history into consideration, makes some words are ignored during parsing. For example in the second case in Table 5, “first class” is ignored during the decoding process. This problem can be further solved using explicit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016) 5 Related Work Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017). Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars. The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013), DCS (Liang et al., 2011; Berant et al., 2013), etc. As discussed above, the main drawback of grammar-based methods is that they rely on high-quality lexicons, manually-built grammars, and hand-crafted features. In recent years, o"
P18-1071,P07-1121,0,0.651055,"red with logical forms, semantic graphs have a tight-coupling with knowledge bases (Yih et al., 2015), and share many commonalities with syntactic structures (Reddy et al., 2014). Therefore both the structure and semantic constraints from knowledge bases can be easily exploited during parsing (Yih et al., 2015). The main challenge of semantic graph-based parsing is how to effectively construct the semantic graph of a sentence. Currently, semantic graphs Introduction Semantic parsing aims to map natural language sentences to logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2013). For example, the sentence “Which states border Texas?” will be mapped to answer (A, (state (A), next to (A, stateid ( texas )))). A semantic parser needs two functions, one for structure prediction and the other for semantic grounding. Traditional semantic parsers are usually based on compositional grammar, such as CCG (Zettlemoyer and Collins, 2005, 2007), DCS (Liang et al., 2011), etc. These parsers compose structure using manually designed grammars, use lexicons for semantic grounding, and exploit fea766 Proceedings of the 56th Annual Meeting of"
P18-1071,P16-1127,0,0.479271,"dard datasets: G EO (Zelle and Mooney, 1996), ATIS (He and Young, 2005) and OVERNIGHT (Wang et al., 2015b). The results show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on G EO and ATIS datasets. The main contributions of this paper are summarized as follows: In recent years, RNN models have achieved success in sequence-to-sequence problems due to its strong ability on both representation learning and prediction, e.g., in machine translation (Cho et al., 2014). A lot of Seq2Seq models have also been employed for semantic parsing (Xiao et al., 2016; Dong and Lapata, 2016; Jia and Liang, 2016), where a sentence is parsed by translating it to linearized logical form using RNN models. There is no need for high-quality lexicons, manually-built grammars, and hand-crafted features. These models are trained end-to-end, and can leverage attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) to learn soft alignments between sentences and logical forms. In this paper, we propose a new neural semantic parsing framework – Sequence-to-Action, which can simultaneously leverage the advantages of semantic graph representation and the strong pr"
P18-1071,P17-1105,0,0.219972,"semantic constraints. Specifically, we incorporate constraints in decoding using a controller. This procedure has two steps: 1) the controller constructs partial semantic graph using the actions generated to current; 2) the controller checks whether a new generated action can meet 4 Experiments In this section, we assess the performance of our method and compare it with previous methods. 770 4.1 Datasets Previous Work Zettlemoyer and Collins (2005) Zettlemoyer and Collins (2007) Kwiatkowksi et al. (2010) Kwiatkowski et al. (2011) Liang et al. (2011)* (+lexicon) Poon (2013) Zhao et al. (2015) Rabinovich et al. (2017) Seq2Seq Models Jia and Liang (2016) Jia and Liang (2016)* (+data) Dong and Lapata (2016): 2Seq Dong and Lapata (2016): 2Tree Our Models Seq2Act Seq2Act (+C1) Seq2Act (+C1+C2) We conduct experiments on three standard datasets: G EO, ATIS and OVERNIGHT. G EO contains natural language questions about US geography paired with corresponding Prolog database queries. Following Zettlemoyer and Collins (2005), we use the standard 600/280 instance splits for training/test. ATIS contains natural language questions of a flight database, with each question is annotated with a lambda calculus query. Follow"
P18-1071,P15-1128,0,0.542219,"st return: A return KB Semantic Graph type A state next_to texas:st Figure 1: Overview of our method, with a demonstration example. tures for candidate logical forms ranking. Unfortunately, it is challenging to design grammars and learn accurate lexicons, especially in wideopen domains. Moreover, it is often hard to design effective features, and its learning process is not end-to-end. To resolve the above problems, two promising lines of work have been proposed: Semantic graph-based methods and Seq2Seq methods. Semantic graph-based methods (Reddy et al., 2014, 2016; Bast and Haussmann, 2015; Yih et al., 2015) represent the meaning of a sentence as a semantic graph (i.e., a sub-graph of a knowledge base, see example in Figure 1) and treat semantic parsing as a semantic graph matching/generation process. Compared with logical forms, semantic graphs have a tight-coupling with knowledge bases (Yih et al., 2015), and share many commonalities with syntactic structures (Reddy et al., 2014). Therefore both the structure and semantic constraints from knowledge bases can be easily exploited during parsing (Yih et al., 2015). The main challenge of semantic graph-based parsing is how to effectively construct"
P18-1071,P16-2033,0,0.0295337,"Missing"
P18-1071,D07-1071,0,0.269124,"next action has a strong correlation with the partial semantic graph generated to current, and illegal actions can be filtered using structure and semantic constraints. Specifically, we incorporate constraints in decoding using a controller. This procedure has two steps: 1) the controller constructs partial semantic graph using the actions generated to current; 2) the controller checks whether a new generated action can meet 4 Experiments In this section, we assess the performance of our method and compare it with previous methods. 770 4.1 Datasets Previous Work Zettlemoyer and Collins (2005) Zettlemoyer and Collins (2007) Kwiatkowksi et al. (2010) Kwiatkowski et al. (2011) Liang et al. (2011)* (+lexicon) Poon (2013) Zhao et al. (2015) Rabinovich et al. (2017) Seq2Seq Models Jia and Liang (2016) Jia and Liang (2016)* (+data) Dong and Lapata (2016): 2Seq Dong and Lapata (2016): 2Tree Our Models Seq2Act Seq2Act (+C1) Seq2Act (+C1+C2) We conduct experiments on three standard datasets: G EO, ATIS and OVERNIGHT. G EO contains natural language questions about US geography paired with corresponding Prolog database queries. Following Zettlemoyer and Collins (2005), we use the standard 600/280 instance splits for traini"
P18-1071,N15-1176,0,0.0240362,"Missing"
P18-1095,P15-1017,0,0.509885,"relations, or entities) in documents, are fundamental and widespread in information extraction (IE). For instance, an event detection (Walker et al., 2006) system may want to detect triggers for “Attack” events, such as “shot” in sentence “He was shot”. In relation detection (Hendrickx et al., 2009), we may want to identify all instances of a specific relation, such as “Jane joined Google” for “Employment” relation. Recently, a number of researches have employed neural network models to solve detection problems, and have achieved significant improvement in many tasks, such as event detection (Chen et al., 2015; Nguyen and Grishman, 2015), relation detection (Zeng et al., 2014; Santos et al., 2015) and named entity recognition (Huang et al., 2015; Chiu and Nichols, 2015; Lample et al., 2016). These methods usually regard detection problems as standard classification tasks, with several positive classes for targets to detect and one negative class for irrelevant (background) instances. For example, an event detection model will identify event triggers in sentence “He was shot” by classifying word “shot” into positive class “Attack”, and classifying all other words into the negative class “NIL”. To op"
P18-1095,P16-2011,0,0.314151,"t/test set and 506/20/167 documents in Chinese train/development/test set respectively. We used Stanford CoreNLP toolkit (Manning et al., 2014) for sentence splitting and word segmentation in Chinese. 4.2 Baselines To verify the effectiveness of our adaptive scaling algorithm, we conducted experiments on two state-of-the-art neural network event detection models. The first one is Dynamic Multipooling Convolutional Neural network (DMCNN) proposed by Chen et al. (2015), a one-layer CNN model with a dynamic multi-pooling operation over convolutional feature maps. The second one is BiLSTM used by Feng et al. (2016) and Yang and Mitchell (2017), where a bidirectional LSTM layer is firstly applied to the input sentence and then word-wise classification is directly conducted on the output of the BiLSTM layer of each word. We compared our method with following baselines upon above-mentioned two models: 1) Vanilla models (Vanilla), which used the original cross-entropy loss function without any additional treatment for class inequality problem. 2) Under-sampling (Sampling), which samples only part of negative instances as the training data. This is the most widely used solution in event detection (Chen et al"
P18-1095,P16-2060,0,0.208094,"ameter or searching procedure. Neural Network based Event Detection. Event detection is a typical task of detection problems. Recently neural network based methods have achieved significant progress in Event Detection. CNNs (Chen et al., 2015; Nguyen and Grishman, 2015) and Bi-LSTMs (Zeng et al., 2016; Yang and Mitchell, 2017) are two effective and widely used models. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) or introducing more complicated architectures to capture larger scale of contexts (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016). 6 Conclusions This paper proposes adaptive scaling algorithm for detection tasks, which can deal with its positive 1040 sparsity problem and directly optimize F-measure by adaptively scaling the influence of negative instances in loss function. Based on the marginal utility theory framework, our method leads to more effective, stable and transferable optimization of neural networks without introducing additional hyper-parameters. Experiments on event detection verified the effectiveness and stability of our adaptive scaling algorithm. The divergence between loss functions and evaluation metr"
P18-1095,W09-2415,0,0.0863123,"Missing"
P18-1095,H05-1087,0,0.137828,"ng instances, rather than Fmeasure on positive classes. Furthermore, due to the positive sparsity problem, training procedure will easily achieve a high accuracy on negative class, but is difficult to converge on positive classes and often leads to a low recall rate. Although simple sampling heuristics can alleviate this problem to some extent, they either suffer from losing inner class information or over-fitting positive instances (He and Garcia, 2009; Fern´andez-Navarro et al., 2011), which often result in instability during the training procedure. Some previous approaches (Joachims, 2005; Jansche, 2005, 2007; Dembczynski et al., 2011; Chinta et al., 2013; Narasimhan et al., 2014; Natarajan et al., 2016) tried to solve this problem by directly optimizing F-measure. Parambath et al. (2014) proved that it is sufficient to solve F-measure optimization problem via cost-sensitive learning, where class-specific cost factors are applied to indicate the importance of different classes to F-measure. However, optimal factors are not known a priori so ε-search needs to be applied, which is too time consuming for the optimization of neural networks. To solve the class inequality problem for sparse detec"
P18-1095,N16-1034,0,0.059447,"ng algorithm is partially inspired by EUM approaches, but is based on the marginal utility framework, which doesn’t introduce any additional hyper-parameter or searching procedure. Neural Network based Event Detection. Event detection is a typical task of detection problems. Recently neural network based methods have achieved significant progress in Event Detection. CNNs (Chen et al., 2015; Nguyen and Grishman, 2015) and Bi-LSTMs (Zeng et al., 2016; Yang and Mitchell, 2017) are two effective and widely used models. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) or introducing more complicated architectures to capture larger scale of contexts (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016). 6 Conclusions This paper proposes adaptive scaling algorithm for detection tasks, which can deal with its positive 1040 sparsity problem and directly optimize F-measure by adaptively scaling the influence of negative instances in loss function. Based on the marginal utility theory framework, our method leads to more effective, stable and transferable optimization of neural networks without introducing additional hyper-parameters. Experiments o"
P18-1095,P15-2060,0,0.332626,"ies) in documents, are fundamental and widespread in information extraction (IE). For instance, an event detection (Walker et al., 2006) system may want to detect triggers for “Attack” events, such as “shot” in sentence “He was shot”. In relation detection (Hendrickx et al., 2009), we may want to identify all instances of a specific relation, such as “Jane joined Google” for “Employment” relation. Recently, a number of researches have employed neural network models to solve detection problems, and have achieved significant improvement in many tasks, such as event detection (Chen et al., 2015; Nguyen and Grishman, 2015), relation detection (Zeng et al., 2014; Santos et al., 2015) and named entity recognition (Huang et al., 2015; Chiu and Nichols, 2015; Lample et al., 2016). These methods usually regard detection problems as standard classification tasks, with several positive classes for targets to detect and one negative class for irrelevant (background) instances. For example, an event detection model will identify event triggers in sentence “He was shot” by classifying word “shot” into positive class “Attack”, and classifying all other words into the negative class “NIL”. To optimize classifiers, cross-en"
P18-1095,P07-1093,0,0.0975887,"Missing"
P18-1095,D16-1085,0,0.0697964,"ce any additional hyper-parameter or searching procedure. Neural Network based Event Detection. Event detection is a typical task of detection problems. Recently neural network based methods have achieved significant progress in Event Detection. CNNs (Chen et al., 2015; Nguyen and Grishman, 2015) and Bi-LSTMs (Zeng et al., 2016; Yang and Mitchell, 2017) are two effective and widely used models. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) or introducing more complicated architectures to capture larger scale of contexts (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016). 6 Conclusions This paper proposes adaptive scaling algorithm for detection tasks, which can deal with its positive 1040 sparsity problem and directly optimize F-measure by adaptively scaling the influence of negative instances in loss function. Based on the marginal utility theory framework, our method leads to more effective, stable and transferable optimization of neural networks without introducing additional hyper-parameters. Experiments on event detection verified the effectiveness and stability of our adaptive scaling algorithm. The divergence between loss functi"
P18-1095,P15-1061,0,0.038868,"Missing"
P18-1095,N16-1030,0,0.0980577,"Missing"
P18-1095,W15-0812,0,0.0292524,"Missing"
P18-1095,P14-5010,0,0.0024899,"r English, we used previously annotated RichERE datasets, including LDC2015E29, LDC2015E68, LDC2016E31 and TAC KBP 2015-2016 Evaluation datasets in LDC2017E02 as the training set. For Chinese, the training set includes LDC2015E105, LDC2015E112, LDC2015E78 and the Chinese part of LDC2017E02. For both Chinese and English, we sampled 20 documents from the evaluation dataset of 2016 year as the development set. Finally, there are 866/20/167 documents in English train/development/test set and 506/20/167 documents in Chinese train/development/test set respectively. We used Stanford CoreNLP toolkit (Manning et al., 2014) for sentence splitting and word segmentation in Chinese. 4.2 Baselines To verify the effectiveness of our adaptive scaling algorithm, we conducted experiments on two state-of-the-art neural network event detection models. The first one is Dynamic Multipooling Convolutional Neural network (DMCNN) proposed by Chen et al. (2015), a one-layer CNN model with a dynamic multi-pooling operation over convolutional feature maps. The second one is BiLSTM used by Feng et al. (2016) and Yang and Mitchell (2017), where a bidirectional LSTM layer is firstly applied to the input sentence and then word-wise c"
P18-1095,P17-1132,0,0.25636,"67 documents in Chinese train/development/test set respectively. We used Stanford CoreNLP toolkit (Manning et al., 2014) for sentence splitting and word segmentation in Chinese. 4.2 Baselines To verify the effectiveness of our adaptive scaling algorithm, we conducted experiments on two state-of-the-art neural network event detection models. The first one is Dynamic Multipooling Convolutional Neural network (DMCNN) proposed by Chen et al. (2015), a one-layer CNN model with a dynamic multi-pooling operation over convolutional feature maps. The second one is BiLSTM used by Feng et al. (2016) and Yang and Mitchell (2017), where a bidirectional LSTM layer is firstly applied to the input sentence and then word-wise classification is directly conducted on the output of the BiLSTM layer of each word. We compared our method with following baselines upon above-mentioned two models: 1) Vanilla models (Vanilla), which used the original cross-entropy loss function without any additional treatment for class inequality problem. 2) Under-sampling (Sampling), which samples only part of negative instances as the training data. This is the most widely used solution in event detection (Chen et al., 2015). 3) Static scaling ("
P18-1095,C14-1220,0,0.080214,"Missing"
P18-1095,D07-1082,0,0.0384431,"ond et al., 2003). Further improvements on this direction involve how to better sampling data with minimum information loss (Carvajal et al., 2004; Estabrooks et al., 2004; Han et al., 2005; Fern´andez-Navarro et al., 2011). Algorithm-level approaches attempt to choose an appropriate inductive bias on models or algorithms to make them more suitable on data imbalance condition, including instance weighting (Ting, 2002; Lin et al., 2017), cost-sensitive learning (Anand et al., 1993; Domingos, 1999; Sun et al., 2007; Krawczyk et al., 2014) and active learning approaches (Ertekin et al., 2007a,b; Zhu and Hovy, 2007). F-Measure Optimization. Previous research on F-measure optimization mainly fell into two paradigms (Nan et al., 2012): 1) Decisiontheoretic approaches (DTA), which first estimate a probability model and find the optimal predictions according to that model (Joachims, 2005; Jansche, 2005, 2007; Dembczynski et al., 2011; BusaFekete et al., 2015; Natarajan et al., 2016). The main drawback of these methods is that they need to estimate the joint probability with exponentially many combinations, thus make them hard to use in practice; 2) Empirical utility maximization (EUM) approaches, which adapt"
P18-1100,D13-1180,1,0.868627,"ing (AES) utilizes natural language processing and machine learning techniques to automatically rate essays written for a target prompt (Dikli, 2006). Currently, the AES systems have been widely used in large-scale English writing tests, e.g. Graduate Record Examination (GRE), to reduce the human efforts in the writing assessments (Attali and Burstein, 2006). Existing AES approaches are promptdependent, where, given a target prompt, rated essays for this particular prompt are required for training (Dikli, 2006; Williamson, 2009; Foltz et al., 1999). While the established models are effective (Chen and He, 2013; Taghipour and Ng, 2016; Alikaniotis et al., 2016; Cummins et al., 2016; Dong et al., 2017), we argue that the models for prompt-independent AES are also desirable to allow for better feasibility and flexibility of AES systems especially when the rated essays for a target prompt are difficult to obtain or even unaccessible. For example, in a writing test within a small class, students are asked to write essays for a target prompt without any rated examples, where the prompt-dependent methods are unlikely to provide effective AES due to the lack of training data. Prompt-independent AES, howeve"
P18-1100,P16-1068,0,0.317348,"Missing"
P18-1100,P16-1075,0,0.544704,"echniques to automatically rate essays written for a target prompt (Dikli, 2006). Currently, the AES systems have been widely used in large-scale English writing tests, e.g. Graduate Record Examination (GRE), to reduce the human efforts in the writing assessments (Attali and Burstein, 2006). Existing AES approaches are promptdependent, where, given a target prompt, rated essays for this particular prompt are required for training (Dikli, 2006; Williamson, 2009; Foltz et al., 1999). While the established models are effective (Chen and He, 2013; Taghipour and Ng, 2016; Alikaniotis et al., 2016; Cummins et al., 2016; Dong et al., 2017), we argue that the models for prompt-independent AES are also desirable to allow for better feasibility and flexibility of AES systems especially when the rated essays for a target prompt are difficult to obtain or even unaccessible. For example, in a writing test within a small class, students are asked to write essays for a target prompt without any rated examples, where the prompt-dependent methods are unlikely to provide effective AES due to the lack of training data. Prompt-independent AES, however, has drawn little attention in the literature, where there only exists"
P18-1100,D16-1115,0,0.378988,"pts. We argue that it is not straightforward, if possible, to apply the established promptdependent AES methods for the mentioned prompt-independent scenario. On one hand, essays for different prompts may differ a lot in the uses of vocabulary, the structure, and the grammatic characteristics; on the other hand, however, established prompt-dependent AES models are designed to learn from these prompt-specific features, including the on/off-topic degree, the tf idf weights of topical terms (Attali and Burstein, 2006; Dikli, 2006), and the n-gram features extracted from word semantic embeddings (Dong and Zhang, 2016; Alikaniotis et al., 2016). Consequently, the prompt-dependent models can hardly learn generalized rules from rated essays for nontarget prompts, and are not suitable for the promptindependent AES. Being aware of this difficulty, to this end, a twostage deep neural network, coined as TDNN, is proposed to tackle the prompt-independent AES problem. In particular, to mitigate the lack of the prompt-dependent labeled data, at the first stage, 1088 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1088–1097 c Melbourne, Australia, July 15"
P18-1100,P13-1045,0,0.0243848,"entation, and is transformed to a 50-dimensional vector through a lookup layer. After that, two bi− LSTM layers are stacked, leading to → e pos . Take Figure 3 for example, given a sentence “Attention please, here is an example.”, it is first converted into a POS sequence using the tagger, namely, VB, VBP, RB, VBZ, DT, NN; thereafter it is further mapped to vector space through one-hot embedding and a lookup layer. - Syntactic embedding aims at encoding an essay in terms of the syntactic relationships among different syntactic components, by encoding an essay recursively. The Stanford Parser (Socher et al., 2013) is employed to label the syntactic structure of words and phrases in sentences, accounting for 59 different types in total. Similar to (Tai et al., 2015), we opt for three stacked bi-LSTM, aiming at encoding individual phrases, sentences, and ultimately the whole essay in sequence. In particular, according to the hierarchical structure from a parsing tree, the phrase-level bi-LSTM first encodes different phrases by consuming syntactic 1090 Figure 2: The model architecture of the proposed hybrid deep learning model. − → embeddings (Sti in Figure 2) from a lookup table of individual syntactic u"
P18-1100,K17-1017,0,0.736344,"ally rate essays written for a target prompt (Dikli, 2006). Currently, the AES systems have been widely used in large-scale English writing tests, e.g. Graduate Record Examination (GRE), to reduce the human efforts in the writing assessments (Attali and Burstein, 2006). Existing AES approaches are promptdependent, where, given a target prompt, rated essays for this particular prompt are required for training (Dikli, 2006; Williamson, 2009; Foltz et al., 1999). While the established models are effective (Chen and He, 2013; Taghipour and Ng, 2016; Alikaniotis et al., 2016; Cummins et al., 2016; Dong et al., 2017), we argue that the models for prompt-independent AES are also desirable to allow for better feasibility and flexibility of AES systems especially when the rated essays for a target prompt are difficult to obtain or even unaccessible. For example, in a writing test within a small class, students are asked to write essays for a target prompt without any rated examples, where the prompt-dependent methods are unlikely to provide effective AES due to the lack of training data. Prompt-independent AES, however, has drawn little attention in the literature, where there only exists unrated essays writ"
P18-1100,P17-1011,0,0.0968198,"r representing sentence structure and an upper layer representing essay structure based on sentence representations, to learn features automatically (Dong and Zhang, 2016). This model is later improved by employing attention layers. Specifically, the model learns text representation with LSTMs which can model the coherence and co-reference among sequences of words and sentences, and uses attention pooling to capture more relevant words and sentences that contribute to the final quality of essays (Dong et al., 2017). Song et al. propose a deep model for identifying discourse modes in an essay (Song et al., 2017). While the literature has shown satisfactory performance of prompt-dependent AES, how to achieve effective essay scoring in a promptindependent setting remains to be explored. Chen & He studied the usefulness of promptindependent text features and achieved a humanmachine rating agreement slightly lower than the use of all text features (Chen and He, 2013) for prompt-dependent essay scoring prediction. A constrained multi-task pairwise preference learning approach was proposed in (Cummins et al., 2016) to combine essays from multiple prompts for training. However, as shown by (Dong and Zhang,"
P18-1100,D16-1193,0,0.730562,"Missing"
P18-1100,P15-1150,0,0.0818049,"Missing"
P18-1100,D15-1278,0,0.0142535,"target prompt. The model architecture is summarized in Figure 2. Intuitively, the model learns the semantic meaning of an essay by encoding it in terms of a sequence of word embeddings, de→ noted as − e sem , hoping to understand what the essay is about; in addition, the part-of-speech information is encoded as a sequence of POS tag→ gings, coined as − e pos ; ultimately, the structural connections between different components in an essay (e.g., terms or phrases) are further captured → via syntactic network, leading to − e synt , where the model learns the organization of the essay. Akin to (Li et al., 2015) and (Zhou and Xu, 2015), biLSTM is employed as a basic component to encode a sequence. Three features are separately captured using the stacked bi-LSTM layers as building blocks to encode different embeddings, whose outputs are subsequently concatenated and fed into several dense layers, generating the ultimate rating. In the following, the architecture of the model is described in details. - Semantic embedding. Akin to the existing works (Alikaniotis et al., 2016; Taghipour and Ng, 2016), semantic word embeddings, namely, the pre-trained 50-dimension GloVe (Pennington et al., 2014), are empl"
P18-1100,D14-1162,0,0.0791499,"e essay. Akin to (Li et al., 2015) and (Zhou and Xu, 2015), biLSTM is employed as a basic component to encode a sequence. Three features are separately captured using the stacked bi-LSTM layers as building blocks to encode different embeddings, whose outputs are subsequently concatenated and fed into several dense layers, generating the ultimate rating. In the following, the architecture of the model is described in details. - Semantic embedding. Akin to the existing works (Alikaniotis et al., 2016; Taghipour and Ng, 2016), semantic word embeddings, namely, the pre-trained 50-dimension GloVe (Pennington et al., 2014), are employed. On top of the word embeddings, two bi-LSTM layers are stacked, namely, the essay layer is constructed on top of the sentence layer, ending up with the semantic representation of the whole essay, which is denoted as → − e sem in Figure 2. - Part-Of-Speech (POS) embeddings for individual terms are first generated by the Stanford Tagger (Toutanova et al., 2003), where 36 different POS tags present. Accordingly, individual words are embedded with 36-dimensional one-hot representation, and is transformed to a 50-dimensional vector through a lookup layer. After that, two bi− LSTM lay"
P18-1100,D15-1049,0,0.325604,"Ks among different prompts). On one hand, this is actually determined by real distribution of ratings for a particular prompt, e.g., how many essays are with an extreme quality for a given prompt in the target data. On the other hand, a fine-grained tuning of the RankSVM (e.g., tuning C+ and C− for positive and negative examClassical regression and classification algorithms are widely used for learning the rating model based on a variety of text features including lexical, syntactic, discourse and semantic features (Larkey, 1998; Rudner, 2002; Attali and Burstein, 2006; Mcnamara et al., 2015; Phandi et al., 2015). There are also approaches that see AES as a preference ranking problem by applying learning to ranking algorithms to learn the rating model. Results show improvement of learning to rank approaches over classical regression and classification algorithms (Chen et al., 2014; Yannakoudakis et al., 2011). In addition, Chen & He propose to incorporate the evaluation metric into the loss function of listwise learning to rank for AES (Chen and He, 2013). Recently, there have been efforts in developing AES approaches based on deep neural networks (DNN), for which feature engineering is not required."
P18-1100,P11-1019,0,0.470602,", and is originally written by students between Grade 7 and Grade 10. As summarized in Table 1, essays from different sets differ in their rating criteria, length, as well as the rating distribution1 . Cross-validation. To fully employ the rated data, a prompt-wise eight-fold cross validation on the ASAP is used for evaluation. In each fold, essays corresponding to a prompt is reserved for testing, and the remaining essays are used as training data. Evaluation metric. The model outputs are first uniformly re-scaled into [0, 10], mirroring the range of ratings in practice. Thereafter, akin to (Yannakoudakis et al., 2011; Chen and He, 2013; Alikaniotis et al., 2016), we report our results primarily based on the quadratic weighted Kappa (QWK), examining the agreement between the predicted ratings and the ground truth. Pearson correlation coefficient (PCC) and Spearman rankorder correlation coefficient (SCC) are also reported. The correlations obtained from individual folds, as well as the average over all eight folds, are reported as the ultimate results. Competing models. Since the promptindependent AES is of interests in this work, the existing AES models are adapted for prompt-independent rating prediction,"
P18-1100,W15-0626,0,0.0403002,"e the literature has shown satisfactory performance of prompt-dependent AES, how to achieve effective essay scoring in a promptindependent setting remains to be explored. Chen & He studied the usefulness of promptindependent text features and achieved a humanmachine rating agreement slightly lower than the use of all text features (Chen and He, 2013) for prompt-dependent essay scoring prediction. A constrained multi-task pairwise preference learning approach was proposed in (Cummins et al., 2016) to combine essays from multiple prompts for training. However, as shown by (Dong and Zhang, 2016; Zesch et al., 2015; Phandi et al., 2015), straightforward applications of existing AES methods for prompt-independent AES lead to a poor performance. lished prompt-dependent AES models on promptindependent AES is not straightforward. Therefore, a two-stage TDNN learning framework was proposed to utilize the prompt-independent features to generate pseudo training data for the target prompt, on which a hybrid deep neural network model is proposed to learn a rating model consuming semantic, part-of-speech, and syntactic signals. Through the experiments on the ASAP dataset, the proposed TDNN model outperforms the b"
P18-1100,P15-1109,0,0.0289752,"del architecture is summarized in Figure 2. Intuitively, the model learns the semantic meaning of an essay by encoding it in terms of a sequence of word embeddings, de→ noted as − e sem , hoping to understand what the essay is about; in addition, the part-of-speech information is encoded as a sequence of POS tag→ gings, coined as − e pos ; ultimately, the structural connections between different components in an essay (e.g., terms or phrases) are further captured → via syntactic network, leading to − e synt , where the model learns the organization of the essay. Akin to (Li et al., 2015) and (Zhou and Xu, 2015), biLSTM is employed as a basic component to encode a sequence. Three features are separately captured using the stacked bi-LSTM layers as building blocks to encode different embeddings, whose outputs are subsequently concatenated and fed into several dense layers, generating the ultimate rating. In the following, the architecture of the model is described in details. - Semantic embedding. Akin to the existing works (Alikaniotis et al., 2016; Taghipour and Ng, 2016), semantic word embeddings, namely, the pre-trained 50-dimension GloVe (Pennington et al., 2014), are employed. On top of the word"
P18-1145,P16-2060,0,0.575128,"tomatic event extraction is a fundamental task of information extraction. Event detection, which aims to identify event triggers of specific types, is a key step of event extraction. For example, from the sentence “Henry was injured, and then passed away soon”, an event detection system should detect an “Injure” event triggered by “injured”, and a “Die” event triggered by “passed away”. Recently, neural network methods, which transform event detection into a word-wise classification paradigm, have achieved significant progress in event detection (Nguyen and Grishman, 2015; Chen et al., 2015b; Ghaeini et al., 2016). For instance, a model will detect events in sentence ”Henry was injured” by successively classifying its three words into NIL, NIL and Injure. By automatically extracting features from raw texts, these methods rely little on prior knowledge and achieved promising results. Unfortunately, word-wise event detection models suffer from the word-trigger mismatch problem, because a number of triggers do not exactly match with a word. Specifically, a trigger can be part of a word or cross multiple words, which is impossible to detect using word-wise models. This problem is more severe in languages w"
P18-1145,P11-1113,0,0.317241,"see that neither character-level or wordlevel representation can achieve competitive results with the NPNs. This verified the necessity of hybrid representation. Besides, we can see that NPN(Char) outperforms other character-level methods in Table 2, which further confirms that our trigger nugget generator is still effective even only using character-level information. 5 Related Work Event detection is an important task in information extraction and has attracted many attentions. Traditional methods (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao et al., 2010; McClosky et al., 2011; Hong et al., 2011; Huang and Riloff, 2012; Li et al., 2013a,b, 2014) rely heavily on hand-craft features, which are hard to transfer among languages and annotation standards. Recently, deep learning methods, which automatically extract high-level features and perform token-level classification with neural networks (Chen et al., 2015b; Nguyen and Grishman, 2015), have achieved significant progress. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) and introducing more complicated architectures to capture larger scale of contexts (Feng et al., 2016; Nguyen and Gr"
P18-1145,P08-1030,0,0.559637,"n on Trigger Classification task on KBP2017Eval. Table 6 shows the experiment results. We can see that neither character-level or wordlevel representation can achieve competitive results with the NPNs. This verified the necessity of hybrid representation. Besides, we can see that NPN(Char) outperforms other character-level methods in Table 2, which further confirms that our trigger nugget generator is still effective even only using character-level information. 5 Related Work Event detection is an important task in information extraction and has attracted many attentions. Traditional methods (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao et al., 2010; McClosky et al., 2011; Hong et al., 2011; Huang and Riloff, 2012; Li et al., 2013a,b, 2014) rely heavily on hand-craft features, which are hard to transfer among languages and annotation standards. Recently, deep learning methods, which automatically extract high-level features and perform token-level classification with neural networks (Chen et al., 2015b; Nguyen and Grishman, 2015), have achieved significant progress. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) and introducing more compl"
P18-1145,C12-1099,0,0.07181,"architectures to capture larger scale of contexts (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016). These methods have achieved promising results in English event detection. Unfortunately, the word-trigger mismatch problem significantly undermines the performance of word-level models in Chinese event detection (Chen and Ji, 2009). To resolve this problem, Chen and Ji (2009) proposed a feature-driven BIO tagging methods at character-level sequences. Qin et al. (2010) introduced a method which can automatically expand candidate Chinese trigger set. While Li et al. (2012) and Li and Zhou (2012) defined manually character compositional patterns for Chinese event triggers. However, their methods rely on hand-crafted features and patterns, which make them difficult to be integrated into recent Deep Learning models. Recent advances have shown that neural networks can effectively capture spatial and positional information from raw inputs (Ren et al., 2015; He et al., 2017; Wang and Jiang, 2017). 1572 This paper designs Nugget Proposal Networks to capture character compositional structure of event triggers, which is more robust and more effective than previous hand-crafted patterns or cha"
P18-1145,C12-1033,0,0.0195235,"this context. After obtaining the scores for each nugget, a softmax layer is applied to normalize the scores: G eOi P (yiG |x; θ) = P N OjG d j=1 e (11) where OiG is the i-the element in OG and θ is the model parameters. 3.2 Event Type Classifier The event type classifier aims to identify whether the given character in the given context will exhibit an event type. Once we detect an event trigger nugget at one character, the hybrid feature fT extracted previously is then feed into a neural network classifier, which further determines the specific type of this trigger. Following previous work (Chen and Ng, 2012), our event type classifier directly classifies nuggets into event subtypes, while ignores the hierarchy between event types. Formally, given the hybrid feature vector fT of input x, a fully-connected layer is applied to compute its scores assigned to each event subtype: OC = WC fT + bC (12) dT where OC ∈ R and dT is the number of event subtypes. Then similar to the trigger nugget generator, a softmax layer is introduced: other character in that nugget into “NIL” (e.g., proposing nugget “É ú”(is injured) at “É” and output “NIL” at “ ”); (ii) overlapped conflict, i.e., proposing two overlapped"
P18-1145,P15-1017,0,0.817604,"ies. Introduction Automatic event extraction is a fundamental task of information extraction. Event detection, which aims to identify event triggers of specific types, is a key step of event extraction. For example, from the sentence “Henry was injured, and then passed away soon”, an event detection system should detect an “Injure” event triggered by “injured”, and a “Die” event triggered by “passed away”. Recently, neural network methods, which transform event detection into a word-wise classification paradigm, have achieved significant progress in event detection (Nguyen and Grishman, 2015; Chen et al., 2015b; Ghaeini et al., 2016). For instance, a model will detect events in sentence ”Henry was injured” by successively classifying its three words into NIL, NIL and Injure. By automatically extracting features from raw texts, these methods rely little on prior knowledge and achieved promising results. Unfortunately, word-wise event detection models suffer from the word-trigger mismatch problem, because a number of triggers do not exactly match with a word. Specifically, a trigger can be part of a word or cross multiple words, which is impossible to detect using word-wise models. This problem is mo"
P18-1145,N09-2053,0,0.45114,"0 46.86 54.50 60.05 43.22 50.27 60.43 51.64 55.69 54.81 46.84 50.51 67.76 45.92 54.74 62.69 42.48 50.64 64.58 50.31 56.56 59.14 46.07 51.80 63.67 51.32 56.83 57.78 46.58 51.57 64.32 53.16 58.21 57.63 47.63 52.15 Table 2: Experiment results on ACE2005 and KBPEval2017. * indicates the result adapted from the original paper. For KBPEval2017, “Trigger Identification” corresponds to the “Span” metric and “Trigger Classification” corresponds to the “Type” metric reported in official evaluation. t Nugget Detection Evaluation (KBPEval2017) datasets. For ACE2005 (LDC2006T06), we used the same setup as Chen and Ji (2009), Feng et al. (2016) and Zeng et al. (2016), in which 569/64/64 documents are used as training/development/test set. For KBPEval2017, we evaluated our model on the 2017 Chinese evaluation dataset(LDC2017E55), using previous RichERE annotated Chinese datasets (LDC2015E78, LDC2015E105, LDC2015E112, and LDC2017E02) as the training set except 20 randomly sampled documents reserved as development set. Finally, there were 506/20/167 documents for training/development/test set. We used Stanford CoreNLP toolkit (Manning et al., 2014) to preprocess all documents for sentence splitting and word segmenta"
P18-1145,P16-2011,0,0.729508,"43.22 50.27 60.43 51.64 55.69 54.81 46.84 50.51 67.76 45.92 54.74 62.69 42.48 50.64 64.58 50.31 56.56 59.14 46.07 51.80 63.67 51.32 56.83 57.78 46.58 51.57 64.32 53.16 58.21 57.63 47.63 52.15 Table 2: Experiment results on ACE2005 and KBPEval2017. * indicates the result adapted from the original paper. For KBPEval2017, “Trigger Identification” corresponds to the “Span” metric and “Trigger Classification” corresponds to the “Type” metric reported in official evaluation. t Nugget Detection Evaluation (KBPEval2017) datasets. For ACE2005 (LDC2006T06), we used the same setup as Chen and Ji (2009), Feng et al. (2016) and Zeng et al. (2016), in which 569/64/64 documents are used as training/development/test set. For KBPEval2017, we evaluated our model on the 2017 Chinese evaluation dataset(LDC2017E55), using previous RichERE annotated Chinese datasets (LDC2015E78, LDC2015E105, LDC2015E112, and LDC2017E02) as the training set except 20 randomly sampled documents reserved as development set. Finally, there were 506/20/167 documents for training/development/test set. We used Stanford CoreNLP toolkit (Manning et al., 2014) to preprocess all documents for sentence splitting and word segmentation. Adadelta updat"
P18-1145,D12-1092,0,0.240062,"ard characters as basic detecting units and are able to 1) directly propose the entire potential trigger nugget at each character by exploiting inner compositional structure of triggers; 2) effectively categorize proposed triggers by learning semantic representation from both characters and words. For example, at character “ú”(injured) in Figure 1 (b), NPNs are not only capable to detect it is part of an Injure event trigger, but also can propose the entire trigger nugget “É ú”(is injured). The main idea behind NPNs is that most Chinese triggers have regular character compositional structure (Li et al., 2012). Concretely, most of Chinese event triggers have one central character which can indicate its event type, e.g. “à”(kill) in “l à”(kill by shooting). Furthermore, characters are composed into a trigger based on regular compositional structures, e.g. “manner + verb” for “là”(kill by shooting), “và”(hack to death), as well as “verb + auxiliary + noun” for “É ú”(is injured) and “E ‹”(beaten). Figure 2 shows the architecture of NPNs. Given a character in sentence, a hybrid representation learning module is first used to learn its semantic representation from both characters and words in the senten"
P18-1145,P13-1145,0,0.0322566,"Missing"
P18-1145,D14-1198,0,0.0921291,"Missing"
P18-1145,P13-1008,0,0.283628,"el representation can achieve competitive results with the NPNs. This verified the necessity of hybrid representation. Besides, we can see that NPN(Char) outperforms other character-level methods in Table 2, which further confirms that our trigger nugget generator is still effective even only using character-level information. 5 Related Work Event detection is an important task in information extraction and has attracted many attentions. Traditional methods (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao et al., 2010; McClosky et al., 2011; Hong et al., 2011; Huang and Riloff, 2012; Li et al., 2013a,b, 2014) rely heavily on hand-craft features, which are hard to transfer among languages and annotation standards. Recently, deep learning methods, which automatically extract high-level features and perform token-level classification with neural networks (Chen et al., 2015b; Nguyen and Grishman, 2015), have achieved significant progress. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) and introducing more complicated architectures to capture larger scale of contexts (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016). Thes"
P18-1145,P10-1081,0,0.643053,"le 6 shows the experiment results. We can see that neither character-level or wordlevel representation can achieve competitive results with the NPNs. This verified the necessity of hybrid representation. Besides, we can see that NPN(Char) outperforms other character-level methods in Table 2, which further confirms that our trigger nugget generator is still effective even only using character-level information. 5 Related Work Event detection is an important task in information extraction and has attracted many attentions. Traditional methods (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao et al., 2010; McClosky et al., 2011; Hong et al., 2011; Huang and Riloff, 2012; Li et al., 2013a,b, 2014) rely heavily on hand-craft features, which are hard to transfer among languages and annotation standards. Recently, deep learning methods, which automatically extract high-level features and perform token-level classification with neural networks (Chen et al., 2015b; Nguyen and Grishman, 2015), have achieved significant progress. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) and introducing more complicated architectures to capture larger scale of"
P18-1145,P14-5010,0,0.00339653,"Eval2017) datasets. For ACE2005 (LDC2006T06), we used the same setup as Chen and Ji (2009), Feng et al. (2016) and Zeng et al. (2016), in which 569/64/64 documents are used as training/development/test set. For KBPEval2017, we evaluated our model on the 2017 Chinese evaluation dataset(LDC2017E55), using previous RichERE annotated Chinese datasets (LDC2015E78, LDC2015E105, LDC2015E112, and LDC2017E02) as the training set except 20 randomly sampled documents reserved as development set. Finally, there were 506/20/167 documents for training/development/test set. We used Stanford CoreNLP toolkit (Manning et al., 2014) to preprocess all documents for sentence splitting and word segmentation. Adadelta update rule (Zeiler, 2012) is applied for optimization. Models are evaluated by micro-averaged Precision(P), Recall(R) and F1-score. For ACE2005, we followed Chen and Ji (2009) to compute the above measures. For KBPEval2017, we used the official evaluation toolkit 2 to obtain these metrics. 4.2 Baselines Three groups of baselines were compared: Character-based NN models. This group of methods solve Chinese Event Detection in a character-level sequential labeling paradigm, which include Convolutional Bi-LSTM mod"
P18-1145,P11-1163,0,0.170962,"riment results. We can see that neither character-level or wordlevel representation can achieve competitive results with the NPNs. This verified the necessity of hybrid representation. Besides, we can see that NPN(Char) outperforms other character-level methods in Table 2, which further confirms that our trigger nugget generator is still effective even only using character-level information. 5 Related Work Event detection is an important task in information extraction and has attracted many attentions. Traditional methods (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao et al., 2010; McClosky et al., 2011; Hong et al., 2011; Huang and Riloff, 2012; Li et al., 2013a,b, 2014) rely heavily on hand-craft features, which are hard to transfer among languages and annotation standards. Recently, deep learning methods, which automatically extract high-level features and perform token-level classification with neural networks (Chen et al., 2015b; Nguyen and Grishman, 2015), have achieved significant progress. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) and introducing more complicated architectures to capture larger scale of contexts (Feng et al.,"
P18-1145,N16-1034,0,0.360441,"ons. Traditional methods (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao et al., 2010; McClosky et al., 2011; Hong et al., 2011; Huang and Riloff, 2012; Li et al., 2013a,b, 2014) rely heavily on hand-craft features, which are hard to transfer among languages and annotation standards. Recently, deep learning methods, which automatically extract high-level features and perform token-level classification with neural networks (Chen et al., 2015b; Nguyen and Grishman, 2015), have achieved significant progress. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) and introducing more complicated architectures to capture larger scale of contexts (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016). These methods have achieved promising results in English event detection. Unfortunately, the word-trigger mismatch problem significantly undermines the performance of word-level models in Chinese event detection (Chen and Ji, 2009). To resolve this problem, Chen and Ji (2009) proposed a feature-driven BIO tagging methods at character-level sequences. Qin et al. (2010) introduced a method which can automatically expand candidate Chinese trigge"
P18-1145,P15-2060,0,0.571714,"igure indicate word boundaries. Introduction Automatic event extraction is a fundamental task of information extraction. Event detection, which aims to identify event triggers of specific types, is a key step of event extraction. For example, from the sentence “Henry was injured, and then passed away soon”, an event detection system should detect an “Injure” event triggered by “injured”, and a “Die” event triggered by “passed away”. Recently, neural network methods, which transform event detection into a word-wise classification paradigm, have achieved significant progress in event detection (Nguyen and Grishman, 2015; Chen et al., 2015b; Ghaeini et al., 2016). For instance, a model will detect events in sentence ”Henry was injured” by successively classifying its three words into NIL, NIL and Injure. By automatically extracting features from raw texts, these methods rely little on prior knowledge and achieved promising results. Unfortunately, word-wise event detection models suffer from the word-trigger mismatch problem, because a number of triggers do not exactly match with a word. Specifically, a trigger can be part of a word or cross multiple words, which is impossible to detect using word-wise models."
P18-1145,D16-1085,0,0.126544,"et al., 2011; Huang and Riloff, 2012; Li et al., 2013a,b, 2014) rely heavily on hand-craft features, which are hard to transfer among languages and annotation standards. Recently, deep learning methods, which automatically extract high-level features and perform token-level classification with neural networks (Chen et al., 2015b; Nguyen and Grishman, 2015), have achieved significant progress. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) and introducing more complicated architectures to capture larger scale of contexts (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016). These methods have achieved promising results in English event detection. Unfortunately, the word-trigger mismatch problem significantly undermines the performance of word-level models in Chinese event detection (Chen and Ji, 2009). To resolve this problem, Chen and Ji (2009) proposed a feature-driven BIO tagging methods at character-level sequences. Qin et al. (2010) introduced a method which can automatically expand candidate Chinese trigger set. While Li et al. (2012) and Li and Zhou (2012) defined manually character compositional patterns for Chinese event triggers"
P18-1145,D09-1016,0,0.153788,"tion task on KBP2017Eval. Table 6 shows the experiment results. We can see that neither character-level or wordlevel representation can achieve competitive results with the NPNs. This verified the necessity of hybrid representation. Besides, we can see that NPN(Char) outperforms other character-level methods in Table 2, which further confirms that our trigger nugget generator is still effective even only using character-level information. 5 Related Work Event detection is an important task in information extraction and has attracted many attentions. Traditional methods (Ji and Grishman, 2008; Patwardhan and Riloff, 2009; Liao et al., 2010; McClosky et al., 2011; Hong et al., 2011; Huang and Riloff, 2012; Li et al., 2013a,b, 2014) rely heavily on hand-craft features, which are hard to transfer among languages and annotation standards. Recently, deep learning methods, which automatically extract high-level features and perform token-level classification with neural networks (Chen et al., 2015b; Nguyen and Grishman, 2015), have achieved significant progress. Some improvements have been made by jointly predicting triggers and arguments (Nguyen et al., 2016) and introducing more complicated architectures to captu"
P18-1145,E99-1023,0,0.0440021,"Missing"
P19-1053,P14-2009,0,0.255066,"Missing"
P19-1053,D14-1181,0,0.0111454,"Missing"
P19-1053,S14-2076,0,0.121985,"o Lu1∗ , Jinsong Su1† , Yubin Ge4 , Linfeng Song5 , Le Sun2 , Jiebo Luo5 1 Xiamen University, Xiamen, China 2 Institute of Software, Chinese Academy of Sciences, Beijing, China 3 University of Chinese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mec"
P19-1053,W04-3250,0,0.0188998,"69 71.70 80.35 71.36 80.59 71.15 80.32 71.01 80.50 72.04 80.96 72.67 81.33 72.90∗∗ 81.53∗ TWITTER Macro-F1 Accuracy — — 66.17 67.71 66.18 67.78 67.20 68.90 67.47 69.17 67.88∗∗ 69.64∗∗ 73.60 74.97 76.82 77.60 76.78 77.54 76.53 77.46 76.58 77.46 77.42 78.08 77.63 78.47 77.72∗∗ 78.61∗ Table 4: Experimental results on various datasets. We directly cited the best experimental results of MN and TNet reported in (Wang et al., 2018; Li et al., 2018). ∗∗ and ∗ means significant at p <0.01 and p <0.05 over the baselines (MN, TNet) on each test set, respectively. Here we conducted 1,000 bootstrap tests (Koehn, 2004) to measure the significance in metric score differences. First, both of our reimplemented MN and TNet are comparable to their original models reported in (Wang et al., 2018; Li et al., 2018). These results show that our reimplemented baselines are competitive. When we replace the CNN of TNet with an attention mechanism, TNet-ATT is slightly inferior to TNet. Moreover, when we perform additional K+1-iteration of training on these models, their performance has not changed significantly, suggesting simply increasing training time is unable to enhance the performance of the neural ASC models. Sec"
P19-1053,P18-1087,0,0.532047,"a-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is laid upon low-freq"
P19-1053,C16-1291,0,0.0132555,"on weights of “cute” in training instances with neural or negative polarity are significantly decreased. Specifically, in these instances, the average weight of “cute” is reduced to 0.07 times of the original. Hence, TNet-ATT(+AS) assigns a smaller weight (0.1090→0.0062) to “cute” and achieves the correct sentiment prediction. 5 Different from these work, our work is in line with the studies of introducing attention supervision to refine the attention mechanism, which have become hot research topics in several NNbased NLP tasks, such as event detection (Liu et al., 2017), machine translation (Liu et al., 2016), and police killing detection (Nguyen and Nguyen, 2018). However, such supervised attention acquisition is labor-intense. Therefore, we mainly commits to automatic mining supervision information for attention mechanisms of neural ASC models. Theoretically, our approach is orthogonal to these models, and we leave the adaptation of our approach into these models as future work. Our work is inspired by two recent models: one is (Wei et al., 2017) proposed to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation probl"
P19-1053,P17-1164,0,0.139556,"sses both TNet and TNet-ATT. These results strongly demonstrate the effectiveness and generality of our approach. 4.3 Case Study Following this trend, researchers have resorted to more sophisticated attention mechanisms to refine neural ASC models. Chen et al., (2017) proposed a multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. An interactive attention network has been designed by Ma et al., (2017) for ASC, where two attention networks were introduced to model the target and context interactively. Liu et al., (2017) proposed to leverage multiple attentions for ASC: one obtained from the left context and the other one acquired from the right context of a given aspect. Very recently, transformation-based model has also been explored for ASC (Li et al., 2018), and the attention mechanism is replaced by CNN. In order to know how our method improves neural ASC models, we deeply analyze attention results of TNet-ATT and TNet-ATT(+AS). It has been found that our proposed approach can solve the above-mentioned two issues well. Table 5 provides two test cases. TNet-ATT incorrectly predicts the sentiment of the fi"
P19-1053,C18-1193,0,0.0198957,"eural or negative polarity are significantly decreased. Specifically, in these instances, the average weight of “cute” is reduced to 0.07 times of the original. Hence, TNet-ATT(+AS) assigns a smaller weight (0.1090→0.0062) to “cute” and achieves the correct sentiment prediction. 5 Different from these work, our work is in line with the studies of introducing attention supervision to refine the attention mechanism, which have become hot research topics in several NNbased NLP tasks, such as event detection (Liu et al., 2017), machine translation (Liu et al., 2016), and police killing detection (Nguyen and Nguyen, 2018). However, such supervised attention acquisition is labor-intense. Therefore, we mainly commits to automatic mining supervision information for attention mechanisms of neural ASC models. Theoretically, our approach is orthogonal to these models, and we leave the adaptation of our approach into these models as future work. Our work is inspired by two recent models: one is (Wei et al., 2017) proposed to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation problems, and the other one is (Xu et al., 2018) where a drop"
P19-1053,D14-1162,0,0.0823334,"Missing"
P19-1053,D17-1047,0,0.480008,"f Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is l"
P19-1053,C16-1311,0,0.569845,"a 3 University of Chinese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus"
P19-1053,D16-1021,0,0.460212,"a 3 University of Chinese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus"
P19-1053,S14-2036,0,0.0370387,"bin Ge4 , Linfeng Song5 , Le Sun2 , Jiebo Luo5 1 Xiamen University, Xiamen, China 2 Institute of Software, Chinese Academy of Sciences, Beijing, China 3 University of Chinese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital rol"
P19-1053,P18-1088,0,0.616588,"na, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is laid upon low-frequency ones. As a res"
P19-1053,D16-1058,0,0.335356,"inese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent w"
P19-1053,K18-1055,0,0.0203569,"hanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is laid upon low-frequency ones. As a result, the performance of attentional neural ASC models is still far from satisfaction. We speculate that this is because there exist widely “apparent patterns” and “inapparent patterns”. Here, “apparent patterns” are interpreted as high-frequency words with strong sentiment polarities and “inapparent patterns” are referred to as low-frequency ones in training data. As mentioned in (Li et al., 2018; Xu et al., 2018; Lin et al., 2017), NNs are easily affected by these two modes: “apparent patterns” tend to be overly learned while “inapparent patterns” often can not be fully learned. Here we use sentences in Table 1 to explain this defect. In the first three training sentences, given the fact that the context word “small” occurs frequently with negative sentiment, the attenIn aspect-level sentiment classification (ASC), it is prevalent to equip dominant neural models with attention mechanisms, for the sake of acquiring the importance of each context word on the given aspect. However, such a mechanism tend"
P19-1053,N16-1174,0,0.159322,"Missing"
P19-1053,E17-2091,0,0.113118,"Missing"
P19-1053,S14-2004,0,\N,Missing
P19-1429,W06-0901,0,0.093195,"ne Hotel. S3: A heavily armed soldier shotAttack the enemy to death. Discrimination S4:That officer was fired from his job. Attack Evaluation Figure 1: Examples of event instances. Identifing ambiguous word fired requires discrimination knowledge and identifing unseen word hacked requires generalization knowledge. Event detection (ED) aims to identify triggers of specific event types. For instance, an ED system will identify fired as an Attack event trigger in the sentence “An American tank fired on the Palestine Hotel.” Event detection plays an important role in Automatic Content Extraction (Ahn, 2006), Information Retrieval (Allan, 2012), and Text Understanding (Chambers and Jurafsky, 2008). Due to the ambiguity and the diversity of natural language expressions (Li et al., 2013; Nguyen and Grishman, 2015), an effective approach should be able to distill both discrimination and generalization knowledge for event detection. Discrimination knowledge aims to distinguish ambiguous triggers in different contexts. As shown in Figure 1, to identify fired in S4 as an EndPosition trigger rather than an Attack trigger, an ED system needs to distill the discrimination knowledge from S1 and S2 that (fi"
P19-1429,P08-1090,0,0.053287,"rimination S4:That officer was fired from his job. Attack Evaluation Figure 1: Examples of event instances. Identifing ambiguous word fired requires discrimination knowledge and identifing unseen word hacked requires generalization knowledge. Event detection (ED) aims to identify triggers of specific event types. For instance, an ED system will identify fired as an Attack event trigger in the sentence “An American tank fired on the Palestine Hotel.” Event detection plays an important role in Automatic Content Extraction (Ahn, 2006), Information Retrieval (Allan, 2012), and Text Understanding (Chambers and Jurafsky, 2008). Due to the ambiguity and the diversity of natural language expressions (Li et al., 2013; Nguyen and Grishman, 2015), an effective approach should be able to distill both discrimination and generalization knowledge for event detection. Discrimination knowledge aims to distinguish ambiguous triggers in different contexts. As shown in Figure 1, to identify fired in S4 as an EndPosition trigger rather than an Attack trigger, an ED system needs to distill the discrimination knowledge from S1 and S2 that (fired, Attack) usually co-occurs Corresponding Author EndPosition S5: A man was hacked to dea"
P19-1429,P17-1038,0,0.114389,"udes lexical feature but includes entity feature. ing using external resources. One strategy is to employ extra knowledge for better representation learning, such as document (Duan et al., 2017; Chen et al., 2018; Liu et al., 2018b), syntactic information (Nguyen and Grishman, 2018; Sha et al., 2018; Orr et al., 2018; Liu et al., 2018c), event arguments (Liu et al., 2017), knowledge bases (Yang and Mitchell, 2017; Lu and Nguyen, 2018) and multi-lingual information (Liu et al., 2018a). The other strategy is generating additional training instances from extra knowledge bases (Liu et al., 2016a; Chen et al., 2017) or news paragraph clusters (Ferguson et al., 2018). Our method does not use any external resources, which could be a good complementary to these methods. Representation Learning via Auxiliary Learning. In recent years, many auxiliary learning techniques have been proposed for better representation learning. Self-supervised learning learns representation by designing auxiliary tasks rather than using manually labeled data. Examples include colorization in vision tasks (Doersch and Zisserman, 2017), language modeling in text tasks (Rei, 2017). Adversarial learning attempts to fool models throug"
P19-1429,P15-1017,0,0.16865,"urs Corresponding Author EndPosition S5: A man was hacked to death by the criminal. Introduction ∗ Generalization with {tank, death, enemy, ...} and (fired, EndPosition) usually co-occurs with {work, fault, job, ...}. Unlike discrimination knowledge, generalization knowledge aims to detect unseen or sparsely labeled triggers, thus needs to be transferred between different trigger words. For example, to identify the unseen word hacked in S5 as an Attack trigger, an ED system needs to distill the generalized Attack pattern “[Trigger] to death” from S3. Currently, most neural network ED methods (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Duan et al., 2017; Yang and Mitchell, 2017) work well on distilling discrimination knowledge, but poorly on distilling generalization knowledge. Table 1 shows the performances of several models on both sparsely (OOV/OOL) and densely (Other) labeled trigger words. These models work well on densely labeled trigger words, i.e., they have a good discrimination ability. But they perform poorly on unseen/sparsely labeled trigger words, i.e., they have a poor generalization ability. This is because these approaches are mostly trigger-centric, thus hard to be general"
P19-1429,D18-1158,0,0.88185,"ing additional dependency features for bidirectional RNN feature extractor, and jointly extracts triggers with its arguments. External Resource based Approaches aim to enhance event detection with external resources, including: SA-ANN-Arg (Liu et al., 2017) which injects event arguments information via supervised attention mechanism; GCN-ED (Nguyen and Grishman, 2018) which exploits syntactic information to capture more accurate context using Graph Convolutional Networks (GCN); GMLATT (Liu et al., 2018a) which exploits the multi-lingual information for more accurate context modeling; HBTNGMA (Chen et al., 2018) which fuses both sentence-level and document-level information, and collectively detects different events in a sentence. For our approach and all baselines, we adopt the pre-trained word embedding using Skip-gram 6 and the open released ELMo models7 . We also report the performance of ELMo as a baseline for demonstrating the performance of universal pretrained representations. All hyper-parameters are tuned on development set. 4371 5 https://github.com/hunterhector/EvmEval https://code.google.com/archive/p/word2vec 7 https://allennlp.org/elmo 6 P R F1 P R Top 3 in TAC 2017 ED Track 3rd in TAC"
P19-1429,I17-1036,0,0.858726,"acked to death by the criminal. Introduction ∗ Generalization with {tank, death, enemy, ...} and (fired, EndPosition) usually co-occurs with {work, fault, job, ...}. Unlike discrimination knowledge, generalization knowledge aims to detect unseen or sparsely labeled triggers, thus needs to be transferred between different trigger words. For example, to identify the unseen word hacked in S5 as an Attack trigger, an ED system needs to distill the generalized Attack pattern “[Trigger] to death” from S3. Currently, most neural network ED methods (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Duan et al., 2017; Yang and Mitchell, 2017) work well on distilling discrimination knowledge, but poorly on distilling generalization knowledge. Table 1 shows the performances of several models on both sparsely (OOV/OOL) and densely (Other) labeled trigger words. These models work well on densely labeled trigger words, i.e., they have a good discrimination ability. But they perform poorly on unseen/sparsely labeled trigger words, i.e., they have a poor generalization ability. This is because these approaches are mostly trigger-centric, thus hard to be generalized well to sparse/unseen words. Furthermore, the l"
P19-1429,D18-1002,0,0.0276993,"ning. In recent years, many auxiliary learning techniques have been proposed for better representation learning. Self-supervised learning learns representation by designing auxiliary tasks rather than using manually labeled data. Examples include colorization in vision tasks (Doersch and Zisserman, 2017), language modeling in text tasks (Rei, 2017). Adversarial learning attempts to fool models through malicious input (Kurakin et al., 2016), it has been broadly used in many scenarios, e.g., domain adaptation (Zeng et al., 2018), knowledge distillation (Qin et al., 2017) and attribute cleaning (Elazar and Goldberg, 2018). Some adversarial-based techniques have been used for event detection. Hong et al. (2018) overcomes spurious features during training via selfregularization. Liu et al. (2019) distills extra knowledge from external NLP resources using a teacher-student network. This paper employs ad4373 versarial ∆-learning algorithm to eliminate lexical information in event representation so that both discrimination and generalization knowledge can be incrementally distilled. 6 Conclusions This paper proposes a new representation learning framework – ∆-learning, which can distill both discrimination and gene"
P19-1429,P16-2011,0,0.0651634,"apture discrimination knowledge, which are complementary to each other. Starting from rw , our method can incrementally distill knowledge in both rd and rg via ∆-learning. By fusing the independent knowledge rd and rg via an effective lexical gate, L in L rd rg rw achieves the best performance on OOV, OOL and Dense instances. 5 Related Work Event Detection. In recent years, neural approaches have achieved significant progress in event detection. Most neural approaches focus on learning effective instance representations (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Lin et al., 2018b). The main drawback of these methods is that they mostly only learn a single and lexical-specific representation, which works well on distilling discrimination knowledge but poorly on generalization knowledge. Some approaches enhance representation learnRepresentations ATT-RNN DMCNN∗ ELMo L rd L rw (w/o ∆) rg L rw (w/o ∆) L rd rg r (w/o ∆) Lw rd L rw rg rw L L rd rg rw OOV 38.7 32.4 31.3 40.0 47.1 40.0 32.3 55.6 57.4 OOL 6.2 9.0 9.0 8.8 11.1 11.4 12.3 23.4 26.7 Sparse 36.7 43.1 47.1 50.0 54.6 52.8 43.1 64.4 55.6 Dense 77.7 77.6 78.0 78.7 78.8 78.8 79.1"
P19-1429,N18-2058,0,0.241264,"e. ing using external resources. One strategy is to employ extra knowledge for better representation learning, such as document (Duan et al., 2017; Chen et al., 2018; Liu et al., 2018b), syntactic information (Nguyen and Grishman, 2018; Sha et al., 2018; Orr et al., 2018; Liu et al., 2018c), event arguments (Liu et al., 2017), knowledge bases (Yang and Mitchell, 2017; Lu and Nguyen, 2018) and multi-lingual information (Liu et al., 2018a). The other strategy is generating additional training instances from extra knowledge bases (Liu et al., 2016a; Chen et al., 2017) or news paragraph clusters (Ferguson et al., 2018). Our method does not use any external resources, which could be a good complementary to these methods. Representation Learning via Auxiliary Learning. In recent years, many auxiliary learning techniques have been proposed for better representation learning. Self-supervised learning learns representation by designing auxiliary tasks rather than using manually labeled data. Examples include colorization in vision tasks (Doersch and Zisserman, 2017), language modeling in text tasks (Rei, 2017). Adversarial learning attempts to fool models through malicious input (Kurakin et al., 2016), it has be"
P19-1429,P16-2060,0,0.044437,"on knowledge, which are complementary to each other. Starting from rw , our method can incrementally distill knowledge in both rd and rg via ∆-learning. By fusing the independent knowledge rd and rg via an effective lexical gate, L in L rd rg rw achieves the best performance on OOV, OOL and Dense instances. 5 Related Work Event Detection. In recent years, neural approaches have achieved significant progress in event detection. Most neural approaches focus on learning effective instance representations (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Lin et al., 2018b). The main drawback of these methods is that they mostly only learn a single and lexical-specific representation, which works well on distilling discrimination knowledge but poorly on generalization knowledge. Some approaches enhance representation learnRepresentations ATT-RNN DMCNN∗ ELMo L rd L rw (w/o ∆) rg L rw (w/o ∆) L rd rg r (w/o ∆) Lw rd L rw rg rw L L rd rg rw OOV 38.7 32.4 31.3 40.0 47.1 40.0 32.3 55.6 57.4 OOL 6.2 9.0 9.0 8.8 11.1 11.4 12.3 23.4 26.7 Sparse 36.7 43.1 47.1 50.0 54.6 52.8 43.1 64.4 55.6 Dense 77.7 77.6 78.0 78.7 78.8 78.8 79.1 78.2 80.0 Table 4: Th"
P19-1429,P18-1048,0,0.0506645,"tion learning. Self-supervised learning learns representation by designing auxiliary tasks rather than using manually labeled data. Examples include colorization in vision tasks (Doersch and Zisserman, 2017), language modeling in text tasks (Rei, 2017). Adversarial learning attempts to fool models through malicious input (Kurakin et al., 2016), it has been broadly used in many scenarios, e.g., domain adaptation (Zeng et al., 2018), knowledge distillation (Qin et al., 2017) and attribute cleaning (Elazar and Goldberg, 2018). Some adversarial-based techniques have been used for event detection. Hong et al. (2018) overcomes spurious features during training via selfregularization. Liu et al. (2019) distills extra knowledge from external NLP resources using a teacher-student network. This paper employs ad4373 versarial ∆-learning algorithm to eliminate lexical information in event representation so that both discrimination and generalization knowledge can be incrementally distilled. 6 Conclusions This paper proposes a new representation learning framework – ∆-learning, which can distill both discrimination and generalization knowledge for event detection. Specifically, two effective ∆-learning algorithm"
P19-1429,P13-1008,0,0.855023,"nces. Identifing ambiguous word fired requires discrimination knowledge and identifing unseen word hacked requires generalization knowledge. Event detection (ED) aims to identify triggers of specific event types. For instance, an ED system will identify fired as an Attack event trigger in the sentence “An American tank fired on the Palestine Hotel.” Event detection plays an important role in Automatic Content Extraction (Ahn, 2006), Information Retrieval (Allan, 2012), and Text Understanding (Chambers and Jurafsky, 2008). Due to the ambiguity and the diversity of natural language expressions (Li et al., 2013; Nguyen and Grishman, 2015), an effective approach should be able to distill both discrimination and generalization knowledge for event detection. Discrimination knowledge aims to distinguish ambiguous triggers in different contexts. As shown in Figure 1, to identify fired in S4 as an EndPosition trigger rather than an Attack trigger, an ED system needs to distill the discrimination knowledge from S1 and S2 that (fired, Attack) usually co-occurs Corresponding Author EndPosition S5: A man was hacked to death by the criminal. Introduction ∗ Generalization with {tank, death, enemy, ...} and (fir"
P19-1429,P10-1081,0,0.590135,"l Θg , we finally fine-tune the full model Θ in Figure 2 by optimizing the event classification loss function: L(Θ) = Levent + λreg · kΘk2 (12) where λreg is the weight coefficient of regularization item and Θ indicates all parameters. L(Θ) can be optimized using mini-batch based stochastic gradient descent algorithms, such as Adadelta (Zeiler, 2012). 4 4.1 Experiments Experimental Settings Dataset. We conduct experiments on two standard English event detection datasets: ACE2005 and KBP2017. ACE2005 (LDC2006T06) contains 599 documents annotated with 33 event types. Following previous studies (Liao and Grishman, 2010; Li et al., 2013; Chen et al., 2015; Liu et al., 2017, 2018a), we use the same 529/30/40 train/dev/test document splits in our experiments. We use ACE2005 as the primary dataset, as the same as previous studies (Nguyen and Grishman, 2018). KBP2017 (LDC2017E55) contains 500 documents with RichERE annotations for TAC KBP 2017 evaluation. For model training, we use previously annotated RichERE datasets, including LDC2015E29, LDC2015E68, LDC2016E31 and TAC KBP 2015-2016 Evaluation datasets. Following previous work (Lin et al., 2018a), we randomly sample 20 documents from the 2016 evaluation datas"
P19-1429,P18-1095,1,0.886639,"Missing"
P19-1429,P18-1145,1,0.911649,"Missing"
P19-1429,D18-1127,0,0.631753,"ch captures generalization knowledge. By decoupling event representations, rd and rg will be independently learned using our ∆-learning algorithm in Section 3. Finally, a gate mechanism is proposed to adaptively fuse the above representations for event detection. Formally, an event detection instance is a pair of trigger candidate and its context, i.e., x = (t, c), where t is a trigger candidate, and c = {c−m , ..., c−1 , c1 , ..., cm } is its context. For example, (fired, “That officer was from his job.”) is an instance for candidate fired. Following previous work (Nguyen and Grishman, 2015; Liu et al., 2018a), given an instance x, we embed each token ti as ti = [pw ; pp ; pe ], where pw is its word embedding, pp is its position embedding, and pe is its entity tag embedding. Therefore t0 is the representation of trigger candidate. In this paper, lexical-specific model Θd and lexical-free model Θg use independent embeddings. 2.1 Lexical-Specific Representation Lexical-specific representation aims to capture discriminative information for distinguishing ambiguous trigger words. For example, we want our representation to capture {officer, job, ...} clues for distinguishing (fired, EndPosition) from"
P19-1429,P16-1201,0,0.192289,"For KBP2017, because TAC KBP2017 allows each team to submit 3 different runs, to make our results comparable with the evaluation results, we select 3 best runs of each system on the development set and report the best test performance among them using the official evaluation toolkit5 , which is referred as Best3 in previous work (Lin et al., 2018a). Baselines. We compare our approach with three types of baselines: Feature based Approaches rely on rich handdesigned features, including: MaxEnt (Li et al., 2013) which employs hand-designed features and uses Max-Entropy Classifier; Combined PSL (Liu et al., 2016b) – the best reported feature-based system which combines global and latent features using Probabilistic Soft Logic framework. Representation Learning based Approaches employ neural networks to automatically extract features for event detection, including: DMCNN (Chen et al., 2015) which uses CNN as sentence feature extractor and concatenates sentence feature and lexical feature for event detection classifier; NC-CNN (Nguyen and Grishman, 2016) which extends traditional CNN by modeling skipgrams for exploiting non-consecutive k-grams; BiRNN (Nguyen et al., 2016) which embeds each token using"
P19-1429,P17-1164,0,0.586761,"Missing"
P19-1429,D18-1156,0,0.528892,"Missing"
P19-1429,D18-1517,0,0.0237223,"ns (ELMo as word representation rw ) on different types of trigger words. For a fair comparison, different from standard DMCNN (Chen et al., 2015) in Table 1 and Table 2, DMCNN∗ excludes lexical feature but includes entity feature. ing using external resources. One strategy is to employ extra knowledge for better representation learning, such as document (Duan et al., 2017; Chen et al., 2018; Liu et al., 2018b), syntactic information (Nguyen and Grishman, 2018; Sha et al., 2018; Orr et al., 2018; Liu et al., 2018c), event arguments (Liu et al., 2017), knowledge bases (Yang and Mitchell, 2017; Lu and Nguyen, 2018) and multi-lingual information (Liu et al., 2018a). The other strategy is generating additional training instances from extra knowledge bases (Liu et al., 2016a; Chen et al., 2017) or news paragraph clusters (Ferguson et al., 2018). Our method does not use any external resources, which could be a good complementary to these methods. Representation Learning via Auxiliary Learning. In recent years, many auxiliary learning techniques have been proposed for better representation learning. Self-supervised learning learns representation by designing auxiliary tasks rather than using manually labeled"
P19-1429,N16-1034,0,0.722648,"-Entropy Classifier; Combined PSL (Liu et al., 2016b) – the best reported feature-based system which combines global and latent features using Probabilistic Soft Logic framework. Representation Learning based Approaches employ neural networks to automatically extract features for event detection, including: DMCNN (Chen et al., 2015) which uses CNN as sentence feature extractor and concatenates sentence feature and lexical feature for event detection classifier; NC-CNN (Nguyen and Grishman, 2016) which extends traditional CNN by modeling skipgrams for exploiting non-consecutive k-grams; BiRNN (Nguyen et al., 2016) which embeds each token using additional dependency features for bidirectional RNN feature extractor, and jointly extracts triggers with its arguments. External Resource based Approaches aim to enhance event detection with external resources, including: SA-ANN-Arg (Liu et al., 2017) which injects event arguments information via supervised attention mechanism; GCN-ED (Nguyen and Grishman, 2018) which exploits syntactic information to capture more accurate context using Graph Convolutional Networks (GCN); GMLATT (Liu et al., 2018a) which exploits the multi-lingual information for more accurate"
P19-1429,P15-2060,0,0.736014,"ambiguous word fired requires discrimination knowledge and identifing unseen word hacked requires generalization knowledge. Event detection (ED) aims to identify triggers of specific event types. For instance, an ED system will identify fired as an Attack event trigger in the sentence “An American tank fired on the Palestine Hotel.” Event detection plays an important role in Automatic Content Extraction (Ahn, 2006), Information Retrieval (Allan, 2012), and Text Understanding (Chambers and Jurafsky, 2008). Due to the ambiguity and the diversity of natural language expressions (Li et al., 2013; Nguyen and Grishman, 2015), an effective approach should be able to distill both discrimination and generalization knowledge for event detection. Discrimination knowledge aims to distinguish ambiguous triggers in different contexts. As shown in Figure 1, to identify fired in S4 as an EndPosition trigger rather than an Attack trigger, an ED system needs to distill the discrimination knowledge from S1 and S2 that (fired, Attack) usually co-occurs Corresponding Author EndPosition S5: A man was hacked to death by the criminal. Introduction ∗ Generalization with {tank, death, enemy, ...} and (fired, EndPosition) usually co-"
P19-1429,D16-1085,0,0.0191034,"ches rely on rich handdesigned features, including: MaxEnt (Li et al., 2013) which employs hand-designed features and uses Max-Entropy Classifier; Combined PSL (Liu et al., 2016b) – the best reported feature-based system which combines global and latent features using Probabilistic Soft Logic framework. Representation Learning based Approaches employ neural networks to automatically extract features for event detection, including: DMCNN (Chen et al., 2015) which uses CNN as sentence feature extractor and concatenates sentence feature and lexical feature for event detection classifier; NC-CNN (Nguyen and Grishman, 2016) which extends traditional CNN by modeling skipgrams for exploiting non-consecutive k-grams; BiRNN (Nguyen et al., 2016) which embeds each token using additional dependency features for bidirectional RNN feature extractor, and jointly extracts triggers with its arguments. External Resource based Approaches aim to enhance event detection with external resources, including: SA-ANN-Arg (Liu et al., 2017) which injects event arguments information via supervised attention mechanism; GCN-ED (Nguyen and Grishman, 2018) which exploits syntactic information to capture more accurate context using Graph"
P19-1429,D18-1041,0,0.0672742,"Missing"
P19-1429,D18-1122,0,0.0286522,"Missing"
P19-1429,N18-1202,0,0.0441247,"Mo OOV 34.3 35.3 31.3 OOL 8.8 9.3 9.0 Other 76.1 75.5 75.7 EndPosition Lexical Gate Fusion Table 1: F1 Scores of previous approaches on different types of triggers (ACE2005), where OOV words are the out-of-vocabulary words in the training corpus, OOL words are the out-of-label words, i.e., an instance whose (word, event type) never occurs in the training corpus but the word is not OOV. DMCNN (Chen et al., 2015) refers to dynamic multi-pooling based CNN; BiLSTM (Duan et al., 2017) refers to bidirectional LSTM based RNN. ELMo refers to the fixed task-independent word representations proposed by Peters et al. (2018). large-scale training data also limits the generalization ability of learned models. Table 1 also shows the performance of using general pre-trained word representation – ELMo (Peters et al., 2018). We can see that, this task-independent lexical-centric representation achieves nearly the same performance to task-specific representations. In this paper, we propose a ∆-representation learning approach, which can incrementally distill both discrimination and generalization knowledge for event detection. ∆-representation learning aims to decouple, learn, and fuse alterable ∆parts for event repres"
P19-1429,P17-1093,0,0.0675323,"Missing"
P19-1429,P17-1194,0,0.0235092,"om extra knowledge bases (Liu et al., 2016a; Chen et al., 2017) or news paragraph clusters (Ferguson et al., 2018). Our method does not use any external resources, which could be a good complementary to these methods. Representation Learning via Auxiliary Learning. In recent years, many auxiliary learning techniques have been proposed for better representation learning. Self-supervised learning learns representation by designing auxiliary tasks rather than using manually labeled data. Examples include colorization in vision tasks (Doersch and Zisserman, 2017), language modeling in text tasks (Rei, 2017). Adversarial learning attempts to fool models through malicious input (Kurakin et al., 2016), it has been broadly used in many scenarios, e.g., domain adaptation (Zeng et al., 2018), knowledge distillation (Qin et al., 2017) and attribute cleaning (Elazar and Goldberg, 2018). Some adversarial-based techniques have been used for event detection. Hong et al. (2018) overcomes spurious features during training via selfregularization. Liu et al. (2019) distills extra knowledge from external NLP resources using a teacher-student network. This paper employs ad4373 versarial ∆-learning algorithm to e"
P19-1429,P17-1132,0,0.130742,"he criminal. Introduction ∗ Generalization with {tank, death, enemy, ...} and (fired, EndPosition) usually co-occurs with {work, fault, job, ...}. Unlike discrimination knowledge, generalization knowledge aims to detect unseen or sparsely labeled triggers, thus needs to be transferred between different trigger words. For example, to identify the unseen word hacked in S5 as an Attack trigger, an ED system needs to distill the generalized Attack pattern “[Trigger] to death” from S3. Currently, most neural network ED methods (Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Duan et al., 2017; Yang and Mitchell, 2017) work well on distilling discrimination knowledge, but poorly on distilling generalization knowledge. Table 1 shows the performances of several models on both sparsely (OOV/OOL) and densely (Other) labeled trigger words. These models work well on densely labeled trigger words, i.e., they have a good discrimination ability. But they perform poorly on unseen/sparsely labeled trigger words, i.e., they have a poor generalization ability. This is because these approaches are mostly trigger-centric, thus hard to be generalized well to sparse/unseen words. Furthermore, the lack of 4366 Proceedings of"
P19-1511,W03-0420,0,0.412345,"ng to both PER and ORG mentions. Introduction Named entity recognition (NER), or more generally entity mention detection1 , aims to identify text spans pertaining to specific entity types such as Person, Organization and Location. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., 2016). Previous approaches (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004; ∗ Corresponding author. In entity mention detection, a mention can be either a named, nominal or pronominal reference of an entity (Katiyar and Cardie, 2018). 1 Lample et al., 2016) commonly regard NER as a sequential labeling task, which generate label sequence for each sentence by assigning one label to each token. These approaches commonly restrict each token belonging to at most one entity mention and, unfortunately, will face a serious problem when recognizing nested entity mentions, where one token may belong to multiple mentions. For example in Figure 1, an Organization"
P19-1511,P15-1017,0,0.0377,"work which attempts to exploit the headdriven phrase structures for nested NER. • We design an objective function, named as Bag Loss. By exploiting the association between words and entity types, Bag Loss can effectively learn ARNs in an end-to-end manner, without using any anchor word annotation. • Head-driven phrase structures are widely spread in natural language. This paper proposes an effective neural network-based solution for exploiting this structure, which can potentially benefit many NLP tasks, such as semantic role labeling (Zhou and Xu, 2015; He et al., 2017) and event extraction (Chen et al., 2015; Lin et al., 2018). 2 Related Work Nested mention detection requires to identify all entity mentions in texts, rather than only outmost mentions in conventional NER. This raises a critical issue to traditional sequential labeling models 5183 because they can only assign one label to each token. To address this issue, mainly two kinds of methods have been proposed. Region-based approaches detect mentions by identifying over subsequences of a sentence respectively, and nested mentions can be detected because they correspond to different subsequences. For this, Finkel and Manning (2009) regarded"
P19-1511,C02-1025,0,0.418971,"and “education” belong to both PER and ORG mentions. Introduction Named entity recognition (NER), or more generally entity mention detection1 , aims to identify text spans pertaining to specific entity types such as Person, Organization and Location. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., 2016). Previous approaches (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004; ∗ Corresponding author. In entity mention detection, a mention can be either a named, nominal or pronominal reference of an entity (Katiyar and Cardie, 2018). 1 Lample et al., 2016) commonly regard NER as a sequential labeling task, which generate label sequence for each sentence by assigning one label to each token. These approaches commonly restrict each token belonging to at most one entity mention and, unfortunately, will face a serious problem when recognizing nested entity mentions, where one token may belong to multiple mentions. For example in Figu"
P19-1511,P18-1009,0,0.0587115,"Missing"
P19-1511,J03-4003,0,0.297437,"Missing"
P19-1511,D09-1015,0,0.800253,"Missing"
P19-1511,P05-1053,0,0.0427731,"how that ARNs achieve the state-of-the-art performance on three standard nested entity mention detection benchmarks. 1 Figure 1: An example of nested entity mentions. Due to the nested structure, “the”,“department”,“of” and “education” belong to both PER and ORG mentions. Introduction Named entity recognition (NER), or more generally entity mention detection1 , aims to identify text spans pertaining to specific entity types such as Person, Organization and Location. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., 2016). Previous approaches (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004; ∗ Corresponding author. In entity mention detection, a mention can be either a named, nominal or pronominal reference of an entity (Katiyar and Cardie, 2018). 1 Lample et al., 2016) commonly regard NER as a sequential labeling task, which generate label sequence for each sentence by assigning one label to each token. These approaches commonly"
P19-1511,P17-1044,0,0.0241188,"st of our knowledge, this is the first work which attempts to exploit the headdriven phrase structures for nested NER. • We design an objective function, named as Bag Loss. By exploiting the association between words and entity types, Bag Loss can effectively learn ARNs in an end-to-end manner, without using any anchor word annotation. • Head-driven phrase structures are widely spread in natural language. This paper proposes an effective neural network-based solution for exploiting this structure, which can potentially benefit many NLP tasks, such as semantic role labeling (Zhou and Xu, 2015; He et al., 2017) and event extraction (Chen et al., 2015; Lin et al., 2018). 2 Related Work Nested mention detection requires to identify all entity mentions in texts, rather than only outmost mentions in conventional NER. This raises a critical issue to traditional sequential labeling models 5183 because they can only assign one label to each token. To address this issue, mainly two kinds of methods have been proposed. Region-based approaches detect mentions by identifying over subsequences of a sentence respectively, and nested mentions can be detected because they correspond to different subsequences. For"
P19-1511,P08-1030,0,0.097306,"ee standard nested entity mention detection benchmarks. 1 Figure 1: An example of nested entity mentions. Due to the nested structure, “the”,“department”,“of” and “education” belong to both PER and ORG mentions. Introduction Named entity recognition (NER), or more generally entity mention detection1 , aims to identify text spans pertaining to specific entity types such as Person, Organization and Location. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., 2016). Previous approaches (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004; ∗ Corresponding author. In entity mention detection, a mention can be either a named, nominal or pronominal reference of an entity (Katiyar and Cardie, 2018). 1 Lample et al., 2016) commonly regard NER as a sequential labeling task, which generate label sequence for each sentence by assigning one label to each token. These approaches commonly restrict each token belonging to at most one entity mention a"
P19-1511,N18-1131,0,0.171212,"nly outmost mentions are used for training. MultiCRF is similar to LSTM-CRF but learns one 3 http://nlp.stanford.edu/data/glove. 6B.zip 4 The hyper-parameter configures are openly released together with our source code at github.com/ sanmusunrise/ARNs. 5 As Wang and Lu (2018) reported, neural network-based baselines significantly outperform all non-neural methods. So we only compared with neural network-based baselines. model for each entity type, and thus is able to recognize nested mentions if they have different types. • Region-based methods, including FOFE (Xu et al., 2017), Cascaded-CRF (Ju et al., 2018) and a transition model (refered as Transition) proposed by Wang et al. (2018). FOFE directly classifies over all sub-sequences of a sentence and thus all potential mentions can be considered. Cascaded-CRF uses several stacked CRF layers to recognize nested mentions at different levels. Transition constructs nested mentions through a sequence of actions. • Hypergraph-based methods, including the LSTM-Hypergraph (LH) model (Katiyar and Cardie, 2018) and the Segmental Hypergraph (SH) by Wang and Lu (2018). LH used an LSTM model to learn features and then decode them into a hypergraph. SH further"
P19-1511,N18-1079,0,0.470002,"ecific entity types such as Person, Organization and Location. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., 2016). Previous approaches (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004; ∗ Corresponding author. In entity mention detection, a mention can be either a named, nominal or pronominal reference of an entity (Katiyar and Cardie, 2018). 1 Lample et al., 2016) commonly regard NER as a sequential labeling task, which generate label sequence for each sentence by assigning one label to each token. These approaches commonly restrict each token belonging to at most one entity mention and, unfortunately, will face a serious problem when recognizing nested entity mentions, where one token may belong to multiple mentions. For example in Figure 1, an Organization entity mention “the department of education” is nested in another Person entity mention “the minister of the department of education”. Nested entity mentions are very common"
P19-1511,N16-1030,0,0.101869,"the next section. 3.1 Anchor Detector An anchor detector is a word-wise classifier, which identifies whether a word is an anchor word of an entity mention of specific types. For the example in Figure 1, the anchor detector should identify that “minister” is an anchor word of a PER mention and “department” is an anchor word of an ORG mention. Formally, given a sentence x1 , x2 , ..., xn , all words are first mapped to a sequence of word representations x1 , x2 , ..., xn where xi is a combination of word embedding, part-of-speech embedding and character-based representation of word xi following Lample et al. (2016). Then we obtain a context-aware representation hA i of each word xi using a bidirectional LSTM layer: −→ −− → A hA i = LSTM(xi , hi−1 ) ←− ←A −− hA i = LSTM(xi , hi+1 ) −→ ←− A A hA i = [hi ; hi ] (1) The learned representation hA i is then fed into a multi-layer perceptron(MLP) classifier, which 5184 computes the scores OiA of the word xi being an anchor word of specific entity types (or NIL if this word is not an anchor word): left mention boundary score Lij and right mention boundary score Rij at word xj by OiA = MLP(hA i ) Rij = tanh(rjT Λ2 hR i + U2 rj + b2 ) (2) where OiA ∈ R|C |and |C"
P19-1511,P13-1008,0,0.0534396,"ty mention detection benchmarks. 1 Figure 1: An example of nested entity mentions. Due to the nested structure, “the”,“department”,“of” and “education” belong to both PER and ORG mentions. Introduction Named entity recognition (NER), or more generally entity mention detection1 , aims to identify text spans pertaining to specific entity types such as Person, Organization and Location. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., 2016). Previous approaches (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004; ∗ Corresponding author. In entity mention detection, a mention can be either a named, nominal or pronominal reference of an entity (Katiyar and Cardie, 2018). 1 Lample et al., 2016) commonly regard NER as a sequential labeling task, which generate label sequence for each sentence by assigning one label to each token. These approaches commonly restrict each token belonging to at most one entity mention and, unfortunately,"
P19-1511,P18-1145,1,0.895696,"Missing"
P19-1511,D15-1102,0,0.367274,"and Sohrab and Miwa (2018) tried to directly classify over all subsequences of a sentence. Besides, Wang et al. (2018) proposed a transition-based method to construct nested mentions via a sequence of specially designed actions. Generally, these approaches are straightforward for nested mention detection, but mostly with high computational cost as they need to classify over almost all sentence subsequences. Schema-based approaches address nested mentions by designing more expressive tagging schemas, rather than changing tagging units. One representative direction is hypergraph-based methods (Lu and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018), where hypergraphbased tags are used to ensure nested mentions can be recovered from word-level tags. Besides, Muis and Lu (2017) developed a gap-based tagging schema to capture nested structures. However, these schemas should be designed very carefully to prevent spurious structures and structural ambiguity (Wang and Lu, 2018). But more expressive, unambiguous schemas will inevitably lead to higher time complexity during both training and decoding. Different from previous methods, this paper proposes a new architecture to address nested mention d"
P19-1511,P14-5010,0,0.007944,"Missing"
P19-1511,P09-1113,0,0.0781255,"the state-of-the-art performance on three standard nested entity mention detection benchmarks. 1 Figure 1: An example of nested entity mentions. Due to the nested structure, “the”,“department”,“of” and “education” belong to both PER and ORG mentions. Introduction Named entity recognition (NER), or more generally entity mention detection1 , aims to identify text spans pertaining to specific entity types such as Person, Organization and Location. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., 2016). Previous approaches (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004; ∗ Corresponding author. In entity mention detection, a mention can be either a named, nominal or pronominal reference of an entity (Katiyar and Cardie, 2018). 1 Lample et al., 2016) commonly regard NER as a sequential labeling task, which generate label sequence for each sentence by assigning one label to each token. These approaches commonly restrict each token"
P19-1511,D17-1276,0,0.211951,"mentions via a sequence of specially designed actions. Generally, these approaches are straightforward for nested mention detection, but mostly with high computational cost as they need to classify over almost all sentence subsequences. Schema-based approaches address nested mentions by designing more expressive tagging schemas, rather than changing tagging units. One representative direction is hypergraph-based methods (Lu and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018), where hypergraphbased tags are used to ensure nested mentions can be recovered from word-level tags. Besides, Muis and Lu (2017) developed a gap-based tagging schema to capture nested structures. However, these schemas should be designed very carefully to prevent spurious structures and structural ambiguity (Wang and Lu, 2018). But more expressive, unambiguous schemas will inevitably lead to higher time complexity during both training and decoding. Different from previous methods, this paper proposes a new architecture to address nested mention detection. Compared with region-based approaches, our ARNs detect mentions by exploiting head-driven phrase structures, rather than exhaustive classifying over subsequences. The"
P19-1511,D14-1162,0,0.0809425,"Missing"
P19-1511,D16-1264,0,0.0307359,"ple of nested entity mentions. Due to the nested structure, “the”,“department”,“of” and “education” belong to both PER and ORG mentions. Introduction Named entity recognition (NER), or more generally entity mention detection1 , aims to identify text spans pertaining to specific entity types such as Person, Organization and Location. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., 2016). Previous approaches (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004; ∗ Corresponding author. In entity mention detection, a mention can be either a named, nominal or pronominal reference of an entity (Katiyar and Cardie, 2018). 1 Lample et al., 2016) commonly regard NER as a sequential labeling task, which generate label sequence for each sentence by assigning one label to each token. These approaches commonly restrict each token belonging to at most one entity mention and, unfortunately, will face a serious problem when recognizing nested entit"
P19-1511,W04-1221,0,0.0999835,"G mentions. Introduction Named entity recognition (NER), or more generally entity mention detection1 , aims to identify text spans pertaining to specific entity types such as Person, Organization and Location. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., 2016). Previous approaches (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004; ∗ Corresponding author. In entity mention detection, a mention can be either a named, nominal or pronominal reference of an entity (Katiyar and Cardie, 2018). 1 Lample et al., 2016) commonly regard NER as a sequential labeling task, which generate label sequence for each sentence by assigning one label to each token. These approaches commonly restrict each token belonging to at most one entity mention and, unfortunately, will face a serious problem when recognizing nested entity mentions, where one token may belong to multiple mentions. For example in Figure 1, an Organization entity mention"
P19-1511,D18-1309,0,0.484428,"Missing"
P19-1511,D12-1042,0,0.0138124,"es. Therefore ARNs can significantly reduce the size of candidate mentions and lead to much lower time complexity. Compared with schema-based approaches, ARNs can naturally address nested mentions since different mentions will have different anchor words. There is no need to design complex tagging schemas, no spurious structures and no structural ambiguity. Furthermore, we also propose Bag Loss, which can train ARNs in an end-to-end manner without any anchor word annotation. The design of Bag Loss is partially inspired by multi-instance learning (MIL) (Zhou and Zhang, 2007; Zhou et al., 2009; Surdeanu et al., 2012), but with a different target. MIL aims to predict a unified label of a bag of instances, while Bag Loss is proposed to train ARNs whose anchor detector is required to predict the label of each instance. Therefore previous MIL methods are not suitable for training ARNs. 3 Anchor-Region Networks for Nested Entity Mention Detection Given a sentence, Anchor-Region Networks detect all entity mentions in a two-step paradigm. First, an anchor detector network identifies anchor words and classifies them into their corresponding entity types. After that, a region recognizer network is applied to recog"
P19-1511,D18-1019,0,0.537263,"classify over all subsequences of a sentence. Besides, Wang et al. (2018) proposed a transition-based method to construct nested mentions via a sequence of specially designed actions. Generally, these approaches are straightforward for nested mention detection, but mostly with high computational cost as they need to classify over almost all sentence subsequences. Schema-based approaches address nested mentions by designing more expressive tagging schemas, rather than changing tagging units. One representative direction is hypergraph-based methods (Lu and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018), where hypergraphbased tags are used to ensure nested mentions can be recovered from word-level tags. Besides, Muis and Lu (2017) developed a gap-based tagging schema to capture nested structures. However, these schemas should be designed very carefully to prevent spurious structures and structural ambiguity (Wang and Lu, 2018). But more expressive, unambiguous schemas will inevitably lead to higher time complexity during both training and decoding. Different from previous methods, this paper proposes a new architecture to address nested mention detection. Compared with region-based approache"
P19-1511,D18-1124,0,0.500733,"a critical issue to traditional sequential labeling models 5183 because they can only assign one label to each token. To address this issue, mainly two kinds of methods have been proposed. Region-based approaches detect mentions by identifying over subsequences of a sentence respectively, and nested mentions can be detected because they correspond to different subsequences. For this, Finkel and Manning (2009) regarded nodes of parsing trees as candidate subsequences. Recently, Xu et al. (2017) and Sohrab and Miwa (2018) tried to directly classify over all subsequences of a sentence. Besides, Wang et al. (2018) proposed a transition-based method to construct nested mentions via a sequence of specially designed actions. Generally, these approaches are straightforward for nested mention detection, but mostly with high computational cost as they need to classify over almost all sentence subsequences. Schema-based approaches address nested mentions by designing more expressive tagging schemas, rather than changing tagging units. One representative direction is hypergraph-based methods (Lu and Roth, 2015; Katiyar and Cardie, 2018; Wang and Lu, 2018), where hypergraphbased tags are used to ensure nested m"
P19-1511,P17-1114,0,0.157746,"requires to identify all entity mentions in texts, rather than only outmost mentions in conventional NER. This raises a critical issue to traditional sequential labeling models 5183 because they can only assign one label to each token. To address this issue, mainly two kinds of methods have been proposed. Region-based approaches detect mentions by identifying over subsequences of a sentence respectively, and nested mentions can be detected because they correspond to different subsequences. For this, Finkel and Manning (2009) regarded nodes of parsing trees as candidate subsequences. Recently, Xu et al. (2017) and Sohrab and Miwa (2018) tried to directly classify over all subsequences of a sentence. Besides, Wang et al. (2018) proposed a transition-based method to construct nested mentions via a sequence of specially designed actions. Generally, these approaches are straightforward for nested mention detection, but mostly with high computational cost as they need to classify over almost all sentence subsequences. Schema-based approaches address nested mentions by designing more expressive tagging schemas, rather than changing tagging units. One representative direction is hypergraph-based methods ("
P19-1511,P02-1060,0,0.492854,",“department”,“of” and “education” belong to both PER and ORG mentions. Introduction Named entity recognition (NER), or more generally entity mention detection1 , aims to identify text spans pertaining to specific entity types such as Person, Organization and Location. NER is a fundamental task of information extraction which enables many downstream NLP applications, such as relation extraction (GuoDong et al., 2005; Mintz et al., 2009), event extraction (Ji and Grishman, 2008; Li et al., 2013) and machine reading comprehension (Rajpurkar et al., 2016; Wang et al., 2016). Previous approaches (Zhou and Su, 2002; Chieu and Ng, 2002; Bender et al., 2003; Settles, 2004; ∗ Corresponding author. In entity mention detection, a mention can be either a named, nominal or pronominal reference of an entity (Katiyar and Cardie, 2018). 1 Lample et al., 2016) commonly regard NER as a sequential labeling task, which generate label sequence for each sentence by assigning one label to each token. These approaches commonly restrict each token belonging to at most one entity mention and, unfortunately, will face a serious problem when recognizing nested entity mentions, where one token may belong to multiple mentions."
P19-1511,P15-1109,0,0.0261332,"nchmarks. To the best of our knowledge, this is the first work which attempts to exploit the headdriven phrase structures for nested NER. • We design an objective function, named as Bag Loss. By exploiting the association between words and entity types, Bag Loss can effectively learn ARNs in an end-to-end manner, without using any anchor word annotation. • Head-driven phrase structures are widely spread in natural language. This paper proposes an effective neural network-based solution for exploiting this structure, which can potentially benefit many NLP tasks, such as semantic role labeling (Zhou and Xu, 2015; He et al., 2017) and event extraction (Chen et al., 2015; Lin et al., 2018). 2 Related Work Nested mention detection requires to identify all entity mentions in texts, rather than only outmost mentions in conventional NER. This raises a critical issue to traditional sequential labeling models 5183 because they can only assign one label to each token. To address this issue, mainly two kinds of methods have been proposed. Region-based approaches detect mentions by identifying over subsequences of a sentence respectively, and nested mentions can be detected because they correspond to different"
P19-1521,P15-1017,0,0.609809,"performances of different models in both English and Chinese event detection. 1 Automatic event extraction is a fundamental task in information extraction. Event detection, aiming to identify trigger words of specific types of events, is a vital step of event extraction. For example, from sentence “Mary was injured, and then she died”, an event detection system is required to detect a Life:Injure event triggered by “injured” and a Life:Die event triggered by “died”. Recently, neural network-based supervised models have achieved promising progress in event detection (Nguyen and Grishman, 2015; Chen et al., 2015; Ghaeini et al., 2016). Commonly, these methods regard event detection as a wordwise classification task with one NIL class for tokens do not trigger any event. Specifically, a neural network automatically extracts high-level features and then feed them into a classifier to categorize words into their corresponding event subCorresponding author. CT 14.4 42.7 7.3 7.7 CR 2.1 4.7 50.0 6.1 MT 1.6 2.6 1.1 28.7 NIL 39.0 40.6 32.3 51.3 CC 1.7 0.9 2.9 3.2 Table 1: Prediction percentage heatmap of triggers with Contact coarse type. Row labels are the golden label and the column labels indicate the pre"
P19-1521,D18-1158,0,0.139863,"twork based methods have achieved promising progress in event detection, especially with CNNs (Chen et al., 2015; Nguyen and Grishman, 2015) and Bi-LSTMs (Zeng et al., 2016; Yang and Mitchell, 2017) based models as automatic feature extractors. Improvements have been made by incorporating arguments knowledge (Nguyen et al., 2016; Liu et al., 2017a; Nguyen and Grishman, 2018; Hong et al., 2018) or capturing larger scale of contexts with more complicated architectures (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016; Lin et al., 2018a,b; Liu et al., 2018a,b; Sha et al., 2018; Chen et al., 2018). Cost-sensitive Learning. Cost-sensitive learning has long been studied in machine learning (Elkan, 2001; Zhou, 2011; Ling and Sheng, 2011). It can be applied both at algorithm-level (Anand et al., 1993; Domingos, 1999; Sun et al., 2007; Krawczyk et al., 2014; Kusner et al., 2014) or datalevel (Ting, 2002; Zadrozny et al., 2003; Mirza et al., 2013), which has achieved great success especially in learning with imbalanced data. 6 Conclusions In this paper, we propose cost-sensitive regularization for neural event detection, which introduces a cost-weighted term of mislabeling likelihood 5281 to"
P19-1521,P16-2011,0,0.0196929,"h clearly demonstrates the effectiveness of our method. 5 Related Work Neural Network based Event Detection. Recently, neural network based methods have achieved promising progress in event detection, especially with CNNs (Chen et al., 2015; Nguyen and Grishman, 2015) and Bi-LSTMs (Zeng et al., 2016; Yang and Mitchell, 2017) based models as automatic feature extractors. Improvements have been made by incorporating arguments knowledge (Nguyen et al., 2016; Liu et al., 2017a; Nguyen and Grishman, 2018; Hong et al., 2018) or capturing larger scale of contexts with more complicated architectures (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016; Lin et al., 2018a,b; Liu et al., 2018a,b; Sha et al., 2018; Chen et al., 2018). Cost-sensitive Learning. Cost-sensitive learning has long been studied in machine learning (Elkan, 2001; Zhou, 2011; Ling and Sheng, 2011). It can be applied both at algorithm-level (Anand et al., 1993; Domingos, 1999; Sun et al., 2007; Krawczyk et al., 2014; Kusner et al., 2014) or datalevel (Ting, 2002; Zadrozny et al., 2003; Mirza et al., 2013), which has achieved great success especially in learning with imbalanced data. 6 Conclusions In this paper, we propose"
P19-1521,P16-2060,0,0.443081,"ferent models in both English and Chinese event detection. 1 Automatic event extraction is a fundamental task in information extraction. Event detection, aiming to identify trigger words of specific types of events, is a vital step of event extraction. For example, from sentence “Mary was injured, and then she died”, an event detection system is required to detect a Life:Injure event triggered by “injured” and a Life:Die event triggered by “died”. Recently, neural network-based supervised models have achieved promising progress in event detection (Nguyen and Grishman, 2015; Chen et al., 2015; Ghaeini et al., 2016). Commonly, these methods regard event detection as a wordwise classification task with one NIL class for tokens do not trigger any event. Specifically, a neural network automatically extracts high-level features and then feed them into a classifier to categorize words into their corresponding event subCorresponding author. CT 14.4 42.7 7.3 7.7 CR 2.1 4.7 50.0 6.1 MT 1.6 2.6 1.1 28.7 NIL 39.0 40.6 32.3 51.3 CC 1.7 0.9 2.9 3.2 Table 1: Prediction percentage heatmap of triggers with Contact coarse type. Row labels are the golden label and the column labels indicate the prediction. BC: Broadcast;"
P19-1521,P18-1048,0,0.0118116,"ly reduces these two kinds of errors without introducing more other types of mislabeling, which clearly demonstrates the effectiveness of our method. 5 Related Work Neural Network based Event Detection. Recently, neural network based methods have achieved promising progress in event detection, especially with CNNs (Chen et al., 2015; Nguyen and Grishman, 2015) and Bi-LSTMs (Zeng et al., 2016; Yang and Mitchell, 2017) based models as automatic feature extractors. Improvements have been made by incorporating arguments knowledge (Nguyen et al., 2016; Liu et al., 2017a; Nguyen and Grishman, 2018; Hong et al., 2018) or capturing larger scale of contexts with more complicated architectures (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016; Lin et al., 2018a,b; Liu et al., 2018a,b; Sha et al., 2018; Chen et al., 2018). Cost-sensitive Learning. Cost-sensitive learning has long been studied in machine learning (Elkan, 2001; Zhou, 2011; Ling and Sheng, 2011). It can be applied both at algorithm-level (Anand et al., 1993; Domingos, 1999; Sun et al., 2007; Krawczyk et al., 2014; Kusner et al., 2014) or datalevel (Ting, 2002; Zadrozny et al., 2003; Mirza et al., 2013), which has achieved great"
P19-1521,P17-1164,0,0.0545434,"ides, costsensitive regularization significantly reduces these two kinds of errors without introducing more other types of mislabeling, which clearly demonstrates the effectiveness of our method. 5 Related Work Neural Network based Event Detection. Recently, neural network based methods have achieved promising progress in event detection, especially with CNNs (Chen et al., 2015; Nguyen and Grishman, 2015) and Bi-LSTMs (Zeng et al., 2016; Yang and Mitchell, 2017) based models as automatic feature extractors. Improvements have been made by incorporating arguments knowledge (Nguyen et al., 2016; Liu et al., 2017a; Nguyen and Grishman, 2018; Hong et al., 2018) or capturing larger scale of contexts with more complicated architectures (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016; Lin et al., 2018a,b; Liu et al., 2018a,b; Sha et al., 2018; Chen et al., 2018). Cost-sensitive Learning. Cost-sensitive learning has long been studied in machine learning (Elkan, 2001; Zhou, 2011; Ling and Sheng, 2011). It can be applied both at algorithm-level (Anand et al., 1993; Domingos, 1999; Sun et al., 2007; Krawczyk et al., 2014; Kusner et al., 2014) or datalevel (Ting, 2002; Zadrozny et al., 2003"
P19-1521,D18-1156,0,0.107028,"Missing"
P19-1521,N16-1034,0,0.0930887,"s our motivation. Besides, costsensitive regularization significantly reduces these two kinds of errors without introducing more other types of mislabeling, which clearly demonstrates the effectiveness of our method. 5 Related Work Neural Network based Event Detection. Recently, neural network based methods have achieved promising progress in event detection, especially with CNNs (Chen et al., 2015; Nguyen and Grishman, 2015) and Bi-LSTMs (Zeng et al., 2016; Yang and Mitchell, 2017) based models as automatic feature extractors. Improvements have been made by incorporating arguments knowledge (Nguyen et al., 2016; Liu et al., 2017a; Nguyen and Grishman, 2018; Hong et al., 2018) or capturing larger scale of contexts with more complicated architectures (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016; Lin et al., 2018a,b; Liu et al., 2018a,b; Sha et al., 2018; Chen et al., 2018). Cost-sensitive Learning. Cost-sensitive learning has long been studied in machine learning (Elkan, 2001; Zhou, 2011; Ling and Sheng, 2011). It can be applied both at algorithm-level (Anand et al., 1993; Domingos, 1999; Sun et al., 2007; Krawczyk et al., 2014; Kusner et al., 2014) or datalevel (Ting, 2002; Zad"
P19-1521,P15-2060,0,0.427227,"significantly improve the performances of different models in both English and Chinese event detection. 1 Automatic event extraction is a fundamental task in information extraction. Event detection, aiming to identify trigger words of specific types of events, is a vital step of event extraction. For example, from sentence “Mary was injured, and then she died”, an event detection system is required to detect a Life:Injure event triggered by “injured” and a Life:Die event triggered by “died”. Recently, neural network-based supervised models have achieved promising progress in event detection (Nguyen and Grishman, 2015; Chen et al., 2015; Ghaeini et al., 2016). Commonly, these methods regard event detection as a wordwise classification task with one NIL class for tokens do not trigger any event. Specifically, a neural network automatically extracts high-level features and then feed them into a classifier to categorize words into their corresponding event subCorresponding author. CT 14.4 42.7 7.3 7.7 CR 2.1 4.7 50.0 6.1 MT 1.6 2.6 1.1 28.7 NIL 39.0 40.6 32.3 51.3 CC 1.7 0.9 2.9 3.2 Table 1: Prediction percentage heatmap of triggers with Contact coarse type. Row labels are the golden label and the column labe"
P19-1521,D16-1085,0,0.019975,"tes the effectiveness of our method. 5 Related Work Neural Network based Event Detection. Recently, neural network based methods have achieved promising progress in event detection, especially with CNNs (Chen et al., 2015; Nguyen and Grishman, 2015) and Bi-LSTMs (Zeng et al., 2016; Yang and Mitchell, 2017) based models as automatic feature extractors. Improvements have been made by incorporating arguments knowledge (Nguyen et al., 2016; Liu et al., 2017a; Nguyen and Grishman, 2018; Hong et al., 2018) or capturing larger scale of contexts with more complicated architectures (Feng et al., 2016; Nguyen and Grishman, 2016; Ghaeini et al., 2016; Lin et al., 2018a,b; Liu et al., 2018a,b; Sha et al., 2018; Chen et al., 2018). Cost-sensitive Learning. Cost-sensitive learning has long been studied in machine learning (Elkan, 2001; Zhou, 2011; Ling and Sheng, 2011). It can be applied both at algorithm-level (Anand et al., 1993; Domingos, 1999; Sun et al., 2007; Krawczyk et al., 2014; Kusner et al., 2014) or datalevel (Ting, 2002; Zadrozny et al., 2003; Mirza et al., 2013), which has achieved great success especially in learning with imbalanced data. 6 Conclusions In this paper, we propose cost-sensitive regularizati"
P19-1521,N18-1202,0,0.072114,"Missing"
P19-1521,P17-1132,0,0.075316,"34.23 46.70 38.71 49.64 44.36 51.89 48.26 52.92 46.98 53.58 49.55 53.71 DMCNN 34.16 47.00 37.63 49.11 42.67 52.28 45.08 52.91 45.73 53.63 46.14 53.88 P Chinese R F1 70.35 68.10 58.34 57.61 53.18 49.19 35.43 35.76 43.40 44.54 49.55 55.83 47.13 46.90 49.77 50.24 51.30 52.30 73.50 69.04 60.27 54.85 55.89 54.91 35.81 38.87 45.50 50.35 50.81 51.93 48.16 49.74 51.85 52.50 53.23 53.38 Table 2: Overall results. CR-POP and CR-INS are our method with population-level and instance-level estimators. All F1 improvements made by CR-POP and CR-INS are statistically significant with p &lt; 0.05. a LSTM model by Yang and Mitchell (2017). Due to page limitation, please refer to original papers for details. 4.2 Baselines1 Following baselines were compared: 1) Cross-entropy Loss (CE), the vanilla loss. 2) Focal Loss (Focal) (Lin et al., 2017), which is an instance-level method that rescales the loss with a factor proportional to the mislabeling probability to enhance the learning on hard instances. 3) Hinge Loss (Hinge), which tries to separate the correct and incorrect predictions with a margin larger than a constant and is widely used in many machine learning tasks. 4) Under-sampling (Sampling), a representative cost-sensitiv"
S16-1098,S15-2046,0,0.021729,"npei,sunle}@nfs.iscas.ac.cn 2 Introduction Semantic Textual Similarity (STS) is the task of measuring the degree of semantic equivalence of a sentence pair (Agirre et al., 2012). STS was first held in SemEval 2012 and has drawn considerable attention in recent years. STS has been widely used in a lot of natural language processing tasks such as information retrieval, machine translation, question answering, text summarization, and so on. Previous methods for this task could be roughly divided into three categories: alignment approaches, vector space approaches and machine learning approaches (Hänig et al., 2015). Alignment approaches align words or phrases in a sentence pair, and then take the quality or coverage of alignments as similarity measure (Sultan et al., 2014). Vector space approaches represent sentences as bag-ofwords vectors and take vector similarity as their similarity measure (Meadow et al., 1992). System Overview To measure the similarity between two sentences, we first extract a series of features, including alignment-based similarity features, vector-based similarity features and sentence constituent similarity features. Then we combine all these features and get an overall similari"
S16-1098,S12-1059,0,0.0444073,"Missing"
S16-1098,S15-2031,0,0.0482721,"Missing"
S16-1098,Q14-1018,0,0.0477068,"Missing"
S16-1098,P14-5010,0,0.00659677,"Missing"
W00-1313,J90-1003,0,\N,Missing
W00-1314,C94-2178,0,0.0549653,"Missing"
W00-1314,J97-2004,0,0.0932165,"Missing"
W00-1314,C96-2129,0,0.0689795,"Missing"
W00-1314,1994.amta-1.3,0,\N,Missing
W00-1314,J93-1004,0,\N,Missing
W00-1314,J93-2003,0,\N,Missing
W00-1314,A94-1006,0,\N,Missing
W00-1314,J93-1006,0,\N,Missing
W02-1211,C96-2129,0,0.0375725,"Missing"
W02-1211,ide-etal-2000-xces,0,0.0772885,"Missing"
W02-1211,P91-1022,0,0.061248,"graph and sentence in another language. This problem has been studied by many researchers and a number of quite encouraging results have been reported. However, almost all bilingual corpora used in research are clear (nearly without sentence omission or insertion) and literal translation bilingual texts. The performance tends to deteriorate significantly when these approaches are applied to noisy complex corpora (with sentence omission or insertion, less literal translation). There are basically three kinds of approaches on sentence alignment: the length-based approach (Gale & Church 1991 and Brown et al. 1991), the lexical approach (key & Roscheisen 1993), and the combination of them (Chen 1993, Wu 1994 and Langlais 1998, etc.). The first published algorithms for aligning sentences in parallel texts are length-based approach proposed by Gale & Church (1991) and Brow et al (1991). Based on the observation that short sentences tend to be translated as short sentences and long sentences as long sentences, they calculate the most likely sentence correspondences as a function of the relative length of the candidates. The basic approach of Brow et al. is similar to Gale and Church, but works by comparing"
W02-1211,C94-2178,0,0.0487049,"Missing"
W02-1211,P93-1002,0,0.029405,"number of quite encouraging results have been reported. However, almost all bilingual corpora used in research are clear (nearly without sentence omission or insertion) and literal translation bilingual texts. The performance tends to deteriorate significantly when these approaches are applied to noisy complex corpora (with sentence omission or insertion, less literal translation). There are basically three kinds of approaches on sentence alignment: the length-based approach (Gale & Church 1991 and Brown et al. 1991), the lexical approach (key & Roscheisen 1993), and the combination of them (Chen 1993, Wu 1994 and Langlais 1998, etc.). The first published algorithms for aligning sentences in parallel texts are length-based approach proposed by Gale & Church (1991) and Brow et al (1991). Based on the observation that short sentences tend to be translated as short sentences and long sentences as long sentences, they calculate the most likely sentence correspondences as a function of the relative length of the candidates. The basic approach of Brow et al. is similar to Gale and Church, but works by comparing sentence length in words rather than characters. While the idea is simple, the models"
W02-1211,P91-1023,0,0.110946,"Missing"
W02-1211,J93-1004,0,\N,Missing
W02-1211,J93-1006,0,\N,Missing
W02-1211,P97-1038,0,\N,Missing
W02-1211,P01-1040,0,\N,Missing
W02-1211,P94-1012,0,\N,Missing
W09-3521,J93-2003,0,\N,Missing
W09-3521,P04-1021,0,\N,Missing
W09-3521,P98-2220,0,\N,Missing
W09-3521,C98-2215,0,\N,Missing
W09-3521,P02-1038,0,\N,Missing
W09-3521,W09-3502,0,\N,Missing
W10-4163,D07-1043,0,0.0152539,"recall, to assess the quality of a clustering solution. Precision indicates the degree of the instances that make up a cluster, which belong to a single class. On the other hand, recall indicates the degree of the instances that make up a class, which belong to a single cluster. But purity and entropy only consider one factor and discard another. So we use FScore measure to assess a clustering solution. For the sake of completeness, we also employ the V-Measure to assess different clustering solutions. V-Measure assesses a cluster solution by considering its homogeneity and its completeness (Rosenberg and Hirschberg, 2007). Homogeneity measures the degree that each cluster contains data points which belong to a single Gold Standard class. And completeness measures the degree that each Gold Standard class contains data points assigned to a single cluster (Rosenberg and Hirschberg, 2007). In general, the larger the V-Measure, the better the clustering performance. More details can be referred to (Rosenberg and Hirschberg, 2007). 4 Results In this section we describe the participant systems and present their results. Since the size of test data may not be large enough to distinguish word senses, participants were"
W10-4163,S07-1002,0,0.0766056,"Missing"
W10-4163,E09-1013,0,0.0547875,"Missing"
W10-4163,W06-1669,0,\N,Missing
W10-4170,W06-3812,0,0.0936882,"Missing"
W10-4170,H05-1097,0,0.087985,"Missing"
W10-4170,S07-1087,0,\N,Missing
W12-6322,S07-1058,0,0.0744275,"Missing"
W12-6322,S07-1024,0,\N,Missing
