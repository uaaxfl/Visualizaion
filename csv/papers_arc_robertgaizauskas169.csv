W16-6631,Don{'}t Mention the Shoe! A Learning to Rank Approach to Content Selection for Image Description Generation,2016,29,0,2,1,25940,josiah wang,Proceedings of the 9th International Natural Language Generation conference,0,"We tackle the sub-task of content selectionn as part of the broader challenge of automaticallyn generating image descriptions.n More specifically, we explore how decisionsn can be made to select what objectn instances should be mentioned in an imagen description, given an image and labelledn bounding boxes. We propose castingn the content selection problem as an learning to rank problem, where object instancesn that are most likely to be mentionedn by humans when describing an imagen are ranked higher than those that aren less likely to be mentioned. Several featuresn are explored: those derived fromn bounding box localisations, from conceptn labels, and from image regions. Objectn instances are then selected based on then ranked list, where we investigate severaln methods for choosing a stopping criterionn as the xe2x80x98cut-offxe2x80x99 point for objects in then ranked list. Our best-performing methodn achieves state-of-the-art performance onn the ImageCLEF2015 sentence generationn challenge."
W16-3605,The {SENSEI} Annotated Corpus: Human Summaries of Reader Comment Conversations in On-line News,2016,15,9,6,1,33318,emma barker,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Researchers are beginning to explore hown to generate summaries of extended argumentativen conversations in social media,n such as those found in reader comments inn on-line news. To date, however, there hasn been little discussion of what these summariesn should be like and a lack of humanauthoredn exemplars, quite likely becausen writing summaries of this kind of interchangen is so difficult. In this paper wen propose one type of reader comment summaryn xe2x80x93 the conversation overview summaryn xe2x80x93 that aims to capture the key argumentativen content of a reader commentn conversation. We describe a method wen have developed to support humans in authoringn conversation overview summariesn and present a publicly available corpus xe2x80x93n the first of its kind xe2x80x93 of news articles plusn comment sets, each multiply annotated,n according to our method, with conversationn overview summaries."
W16-2802,Summarizing Multi-Party Argumentative Conversations in Reader Comment on News,2016,16,7,2,1,33318,emma barker,Proceedings of the Third Workshop on Argument Mining ({A}rg{M}ining2016),0,"Existing approaches to summarizing multi-party argumentative conversations in reader comment are extractive and fail to capture the argumentative nature of these conversations. Work on argument mining proposes schemes for identifying argument elements and relations in text but has not yet addressed how summaries might be generated from a global analysis of a conversation based on these schemes. In this paper we: (1) propose an issue-centred scheme for analysing and graphically representing argument in reader comment discussion in on-line news, and (2) show how summaries capturing the argumentative nature of reader comment can be generated from our graphical representation."
L16-1070,A Document Repository for Social Media and Speech Conversations,2016,4,1,2,0,33317,adam funk,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a successfully implemented document repository REST service for flexible SCRUD (search, crate, read, update, delete) storage of social media conversations, using a GATE/TIPSTER-like document object model and providing a query language for document features. This software is currently being used in the SENSEI research project and will be published as open-source software before the project ends. It is, to the best of our knowledge, the first freely available, general purpose data repository to support large-scale multimodal (i.e., speech or text) conversation analytics."
L16-1489,Cross-validating Image Description Datasets and Evaluation Metrics,2016,25,1,2,1,25940,josiah wang,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The task of automatically generating sentential descriptions of image content has become increasingly popular in recent years, resulting in the development of large-scale image description datasets and the proposal of various metrics for evaluating image description generation systems. However, not much work has been done to analyse and understand both datasets and the metrics. In this paper, we propose using a leave-one-out cross validation (LOOCV) process as a means to analyse multiply annotated, human-authored image description datasets and the various evaluation metrics, i.e. evaluating one image description against other human-authored descriptions of the same image. Such an evaluation process affords various insights into the image description datasets and evaluation metrics, such as the variations of image descriptions within and across datasets and also what the metrics capture. We compute and analyse (i) human upper-bound performance; (ii) ranked correlation between metric pairs across datasets; (iii) lower-bound performance by comparing a set of descriptions describing one image to another sentence not describing that image. Interesting observations are made about the evaluation metrics and image description datasets, and we conclude that such cross-validation methods are extremely useful for assessing and gaining insights into image description datasets and evaluation metrics for image descriptions."
L16-1494,What{'}s the Issue Here?: Task-based Evaluation of Reader Comment Summarization Systems,2016,9,4,8,1,33318,emma barker,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Automatic summarization of reader comments in on-line news is an extremely challenging task and a capability for which there is a clear need. Work to date has focussed on producing extractive summaries using well-known techniques imported from other areas of language processing. But are extractive summaries of comments what users really want? Do they support users in performing the sorts of tasks they are likely to want to perform with reader comments? In this paper we address these questions by doing three things. First, we offer a specification of one possible summary type for reader comment, based on an analysis of reader comment in terms of issues and viewpoints. Second, we define a task-based evaluation framework for reader comment summarization that allows summarization systems to be assessed in terms of how well they support users in a time-limited task of identifying issues and characterising opinion on issues in comments. Third, we describe a pilot evaluation in which we used the task-based evaluation framework to evaluate a prototype reader comment clustering and summarization system, demonstrating the viability of the evaluation framework and illustrating the sorts of insight such an evaluation affords."
W15-4722,"Generating Image Descriptions with Gold Standard Visual Inputs: Motivation, Evaluation and Baselines",2015,31,7,2,1,25940,josiah wang,Proceedings of the 15th {E}uropean Workshop on Natural Language Generation ({ENLG}),0,"In this paper, we present the task of generating image descriptions with gold standard visual detections as input, rather than directly from an image. This allows the Natural Language Generation community to focus on the text generation process, rather than dealing with the noise and complications arising from the visual detection process. We propose a fine-grained evaluation metric specifically for evaluating the content selection capabilities of image description generation systems. To demonstrate the evaluation metric on the task, several baselines are presented using bounding box information and textual information as priors for content selection. The baselines are evaluated using the proposed metric, showing that the fine-grained metric is useful for evaluating the content selection phase of an image description generation system."
W15-2805,Defining Visually Descriptive Language,2015,10,3,1,1,33330,robert gaizauskas,Proceedings of the Fourth Workshop on Vision and Language,0,"In this paper, we introduce the notion of visually descriptive language (VDL) xe2x80x90 intuitively a text segment whose truth can be confirmed by visual sense alone. VDL can be exploited in many vision-based tasks, e.g. image interpretation and story illustration. In contrast to previous work requiring pre-aligned texts and images, we propose a broader definition of VDL that extends to a much larger range of texts without associated images. We also discuss possible VDL annotation tasks and make recommendations for difficult cases. Lastly, we demonstrate the viability of our definition via an annotation exercise across several text genres and analyse inter-annotator agreement. Results show reasonably high levels of agreement between annotators can be reached."
R15-1017,Temporal Relation Classification using a Model of Tense and Aspect,2015,12,2,2,0.601717,642,leon derczynski,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Determining the temporal order of events in a text is difficult. However, it is crucial to the extraction of narratives, plans, and context. We suggest that a simple, established framework of tense and aspect provides a viable model for ordering a subset of events and times in a given text. Using this framework, we investigate extracting features that represent temporal information and integrate these in a machine learning approach. These features improve event-event ordering."
D15-1022,"Combining Geometric, Textual and Visual Features for Predicting Prepositions in Image Descriptions",2015,19,7,6,0,32004,arnau ramisa,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We investigate the role that geometric, textual and visual features play in the task of predicting a preposition that links two visual entities depicted in an image. The task is an important part of the subsequent process of generating image descriptions. We explore the prediction of prepositions for a pair of entities, both in the case when the labels of such entities are known and unknown. In all situations we found clear evidence that all three features contribute to the prediction task."
W14-5406,A Poodle or a Dog? Evaluating Automatic Image Annotation Using Human Descriptions at Different Levels of Granularity,2014,27,7,4,1,25940,josiah wang,Proceedings of the Third Workshop on Vision and Language,0,"Different people may describe the same object in different ways, and at varied levels of granularity (xe2x80x9cpoodlexe2x80x9d, xe2x80x9cdogxe2x80x9d, xe2x80x9cpetxe2x80x9d or xe2x80x9canimalxe2x80x9d?) In this paper, we propose the idea of xe2x80x98granularityawarexe2x80x99 groupings where semantically related concepts are grouped across different levels of granularity to capture the variation in how different people describe the same image content. The idea is demonstrated in the task of automatic image annotation, where these semantic groupings are used to alter the results of image annotation in a manner that affords different insights from its initial, category-independent rankings. The semantic groupings are also incorporated during evaluation against image descriptions written by humans. Our experiments show that semantic groupings result in image annotations that are more informative and flexible than without groupings, although being too flexible may result in image annotations that are less informative."
W14-4802,Assigning Terms to Domains by Document Classification,2014,20,3,1,1,33330,robert gaizauskas,Proceedings of the 4th International Workshop on Computational Terminology (Computerm),0,In this paper we investigate a number of questions relating to the identification of the domain of a term by domain classification of the document in which the term occurs. We propose and evaluate a straightforward method for domain classification of documents in 24 languages that exploits a multilingual thesaurus and Wikipedia. We investigate and provide quantitative results about the extent to which humans agree about the domain classification of documents and terms also the extent to which terms are likely to xe2x80x9cinheritxe2x80x9d the domain of their parent document.
W14-4408,A Hybrid Approach to Multi-document Summarization of Opinions in Reviews,2014,17,23,3,0,33009,giuseppe fabbrizio,Proceedings of the 8th International Natural Language Generation Conference ({INLG}),0,"We present a hybrid method to generate summaries of product and services reviews by combining natural language generation and salient sentence selection techniques. Our system, STARLET-H, receives as input textual reviews with associated rated topics, and produces as output a natural language document summarizing the opinions expressed in the reviews. STARLET-H operates as a hybrid"
P14-2013,Graph Ranking for Collective Named Entity Disambiguation,2014,17,50,2,0,14155,ayman alhelbawy,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Named Entity Disambiguation (NED) refers to the task of mapping different named entity mentions in running text to their correct interpretations in a specific knowledge base (KB). This paper presents a collective disambiguation approach using a graph model. All possible NE candidates are represented as nodes in the graph and associations between different candidates are represented by edges between the nodes. Each node has an initial confidence score, e.g. entity popularity. Page-Rank is used to rank nodes and the final rank is combined with the initial confidence for candidate selection. Experiments on 27,819 NE textual mentions show the effectiveness of using Page-Rank in conjunction with initial confidence: 87% accuracy is achieved, outperforming both baseline and state-of-the-art approaches."
aker-etal-2014-bootstrapping,Bootstrapping Term Extractors for Multiple Languages,2014,13,4,4,1,25116,ahmet aker,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Terminology extraction resources are needed for a wide range of human language technology applications, including knowledge management, information extraction, semantic search, cross-language information retrieval and automatic and assisted translation. We create a low cost method for creating terminology extraction resources for 21 non-English EU languages. Using parallel corpora and a projection method, we create a General POS Tagger for these languages. We also investigate the use of EuroVoc terms and Wikipedia corpus to automatically create term grammar for each language. Our results show that these automatically generated resources can assist term extraction process with similar performance to manually generated resources. All resources resulted in this experiment are freely available for download."
aker-etal-2014-bilingual,Bilingual dictionaries for all {EU} languages,2014,17,7,4,1,25116,ahmet aker,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Bilingual dictionaries can be automatically generated using the GIZA++ tool. However, these dictionaries contain a lot of noise, because of which the quality of outputs of tools relying on the dictionaries are negatively affected. In this work we present three different methods for cleaning noise from automatically generated bilingual dictionaries: LLR, pivot and translation based approach. We have applied these approaches on the GIZA++ dictionaries -- dictionaries covering official EU languages -- in order to remove noise. Our evaluation showed that all methods help to reduce noise. However, the best performance is achieved using the transliteration based approach. We provide all bilingual dictionaries (the original GIZA++ dictionaries and the cleaned ones) free for download. We also provide the cleaning tools and scripts for free download."
C14-1147,Collective Named Entity Disambiguation using Graph Ranking and Clique Partitioning Approaches,2014,27,10,2,0,14155,ayman alhelbawy,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Disambiguating named entities (NE) in running text to their correct interpretations in a specific knowledge base (KB) is an important problem in NLP. This paper presents two collective disambiguation approaches using a graph representation where possible KB candidates for NE textual mentions are represented as nodes and the coherence relations between different NE candidates are represented by edges. Each node has a local confidence score and each edge has a weight. The first approach uses Page-Rank (PR) to rank all nodes and selects a candidate based on PR score combined with local confidence score. The second approach uses an adapted Clique Partitioning technique to find the most weighted clique and expands this clique until all NE textual mentions are disambiguated. Experiments on 27,819 NE textual mentions show the effectiveness of both approaches, outperforming both baseline and state-of-the-art approaches."
W13-0107,Empirical Validation of Reichenbach{'}s Tense Framework,2013,0,1,2,1,642,leon derczynski,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,None
P13-2114,Temporal Signals Help Label Temporal Relations,2013,31,11,2,1,642,leon derczynski,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Automatically determining the temporal order of events and times in a text is difficult, though humans can readily perform this task. Sometimes events and times are related through use of an explicit co-ordination which gives information about the temporal relation: expressions like xe2x80x9cbeforexe2x80x9d and xe2x80x9cas soon asxe2x80x9d. We investigate the rxcbx86 ole that these co-ordinating temporal signals have in determining the type of temporal relations in discourse. Using machine learning, we improve upon prior approaches to the problem, achieving over 80% accuracy at labelling the types of temporal relation between events and times that are related by temporal signals."
llorens-etal-2012-timen,{TIMEN}: An Open Temporal Expression Normalisation Resource,2012,19,46,3,0,37290,hector llorens,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Temporal expressions are words or phrases that describe a point, duration or recurrence in time. Automatically annotating these expressions is a research goal of increasing interest. Recognising them can be achieved with minimally supervised machine learning, but interpreting them accurately (normalisation) is a complex task requiring human knowledge. In this paper, we present TIMEN, a community-driven tool for temporal expression normalisation. TIMEN is derived from current best approaches and is an independent tool, enabling easy integration in existing systems. We argue that temporal expression normalisation can only be effectively performed with a large knowledge base and set of rules. Our solution is a framework and system with which to capture this knowledge for different languages. Using both existing and newly-annotated data, we present results showing competitive performance and invite the IE community to contribute to a knowledge base in order to solve the temporal expression normalisation problem."
paramita-etal-2012-correlation,Correlation between Similarity Measures for Inter-Language Linked {W}ikipedia Articles,2012,12,12,4,1,33315,monica paramita,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Wikipedia articles in different languages have been mined to support various tasks, such as Cross-Language Information Retrieval (CLIR) and Statistical Machine Translation (SMT). Articles on the same topic in different languages are often connected by inter-language links, which can be used to identify similar or comparable content. In this work, we investigate the correlation between similarity measures utilising language-independent and language-dependent features and respective human judgments. A collection of 800 Wikipedia pairs from 8 different language pairs were collected and judged for similarity by two assessors. We report the development of this corpus and inter-assessor agreement between judges across the languages. Results show that similarity measured using language independent features is comparable to using an approach based on translating non-English documents. In both cases the correlation with human judgments is low but also dependent upon the language pair. The results and corpus generated from this work also provide insights into the measurement of cross-language similarity."
aker-etal-2012-light,A light way to collect comparable corpora from the Web,2012,19,21,3,1,25116,ahmet aker,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Statistical Machine Translation (SMT) relies on the availability of rich parallel corpora. However, in the case of under-resourced languages, parallel corpora are not readily available. To overcome this problem previous work has recognized the potential of using comparable corpora as training data. The process of obtaining such data usually involves (1) downloading a separate list of documents for each language, (2) matching the documents between two languages usually by comparing the document contents, and finally (3) extracting useful data for SMT from the matched document pairs. This process requires a large amount of time and resources since a huge volume of documents needs to be downloaded to increase the chances of finding good document pairs. In this work we aim to reduce the amount of time and resources spent for tasks 1 and 2. Instead of obtaining full documents we first obtain just titles along with some meta-data such as time and date of publication. Titles can be obtained through Web Search and RSS News feed collections so that download of the full documents is not needed. We show experimentally that titles can be used to approximate the comparison between documents using full document contents."
skadina-etal-2012-collecting,Collecting and Using Comparable Corpora for Statistical Machine Translation,2012,32,25,10,0,17522,inguna skadicna,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Lack of sufficient parallel data for many languages and domains is currently one of the major obstacles to further advancement of automated translation. The ACCURAT project is addressing this issue by researching methods how to improve machine translation systems by using comparable corpora. In this paper we present tools and techniques developed in the ACCURAT project that allow additional data needed for statistical machine translation to be extracted from comparable corpora. We present methods and tools for acquisition of comparable corpora from the Web and other sources, for evaluation of the comparability of collected corpora, for multi-level alignment of comparable corpora and for extraction of lexical and terminological data for machine translation. Finally, we present initial evaluation results on the utility of collected corpora in domain-adapted machine translation and real-life applications."
barker-gaizauskas-2012-assessing,Assessing the Comparability of News Texts,2012,15,4,2,1,33318,emma barker,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Comparable news texts are frequently proposed as a potential source of alignable subsentential fragments for use in statistical machine translation systems. But can we assess just how potentially useful they will be? In this paper we first discuss a scheme for classifying news text pairs according to the degree of relatedness of the events they report and investigate how robust this classification scheme is via a multi-lingual annotation exercise. We then propose an annotation methodology, similar to that used in summarization evaluation, to allow us to identify and quantify shared content at the subsentential level in news text pairs and report a preliminary exercise to assess this method. We conclude by discussing how this works fits into a broader programme of assessing the potential utility of comparable news texts for extracting paraphrases/translational equivalents for use in language processing applications."
C12-2003,Automatic Bilingual Phrase Extraction from Comparable Corpora,2012,22,14,3,1,25116,ahmet aker,Proceedings of {COLING} 2012: Posters,0,"In this work we present an approach for extracting parallel phrases from comparable news articles to improve statistical machine translation. This is particularly useful for under-resourced languages where parallel corpora are not readily available. Our approach consists of a phrase pair generator that automatically generates candidate parallel phrases and a binary SVM classifier that classifies the candidate phrase pairs as parallel or non-parallel. The phrase pair generator is also used to automatically create training and testing data for the SVM classifier from parallel corpora. We evaluate our approach using English-German, English-Greek and English-Latvian language pairs. The performance of our classifier on the test sets is above 80% precision and 97% accuracy for all language pairs. We also perform an SMT evaluation by measuring the impact of phrases extracted from comparable corpora on SMT quality using BLEU. For all language pairs we obtain significantly better results compared to the baselines."
S10-1075,{USFD}2: Annotating Temporal Expresions and {TLINK}s for {T}emp{E}val-2,2010,12,22,2,1,642,leon derczynski,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"We describe the University of Sheffield system used in the TempEval-2 challenge, USFD2. The challenge requires the automatic identification of temporal entities and relations in text.n n USFD2 identifies and anchors temporal expressions, and also attempts two of the four temporal relation assignment tasks. A rule-based system picks out and anchors temporal expressions, and a maximum entropy classifier assigns temporal link labels, based on features that include descriptions of associated temporal signal words. USFD2 identified temporal expressions successfully, and correctly classified their type in 90% of cases. Determining the relation between an event and time expression in the same sentence was performed at 63% accuracy, the second highest score in this part of the challenge."
P10-1127,Generating Image Descriptions Using Dependency Relational Patterns,2010,25,61,2,1,25116,ahmet aker,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a novel approach to automatic captioning of geo-tagged images by summarizing multiple web-documents that contain information related to an image's location. The summarizer is biased by dependency pattern models towards sentences which contain features typically provided for different scene types such as those of churches, bridges, etc. Our results show that summaries biased by dependency pattern models lead to significantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia baseline summaries. Summaries generated using dependency patterns also lead to more readable summaries than those generated without dependency patterns."
aker-gaizauskas-2010-model,Model Summaries for Location-related Images,2010,16,17,2,1,25116,ahmet aker,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"At present there is no publicly available data set to evaluate the performance of different summarization systems on the task of generating location-related extended image captions. In this paper we describe a corpus of human generated model captions in English and German. We have collected 932 model summaries in English from existing image descriptions and machine translated these summaries into German. We also performed post-editing on the translated German summaries to ensure high quality. Both English and German summaries are evaluated using a readability assessment as in DUC and TAC to assess their quality. Our model summaries performed similar to the ones reported in Dang (2005) and thus are suitable for evaluating automatic summarization systems on the task of generating image descriptions for location related images. In addition, we also investigated whether post-editing of machine-translated model summaries is necessary for automated ROUGE evaluations. We found a high correlation in ROUGE scores between post-edited and non-post-edited model summaries which indicates that the expensive process of post-editing is not necessary."
derczynski-gaizauskas-2010-analysing,Analysing Temporally Annotated Corpora with {CAV}a{T},2010,9,16,2,1,642,leon derczynski,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present CAVaT, a tool that performs Corpus Analysis and Validation for TimeML. CAVaT is an open source, modular checking utility for statistical analysis of features specific to temporally-annotated natural language corpora. It provides reporting, highlights salient links between a variety of general and time-specific linguistic features, and also validates a temporal annotation to ensure that it is logically consistent and sufficiently annotated. Uniquely, CAVaT provides analysis specific to TimeML-annotated temporal information. TimeML is a standard for annotating temporal information in natural language text. In this paper, we present the reporting part of CAVaT, and then its error-checking ability, including the workings of several novel TimeML document verification methods. This is followed by the execution of some example tasks using the tool to show relations between times, events, signals and links. We also demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been detected with CAVaT."
aswani-gaizauskas-2010-developing,Developing Morphological Analysers for {S}outh {A}sian Languages: Experimenting with the {H}indi and {G}ujarati Languages,2010,8,8,2,1,41299,niraj aswani,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"A considerable amount of work has been put into development of stemmers and morphological analysers. The majority of these approaches use hand-crafted suffix-replacement rules but a few try to discover such rules from corpora. While most of the approaches remove or replace suffixes, there are examples of derivational stemmers which are based on prefixes as well. In this paper we present a rule-based morphological analyser. We propose an approach that takes both prefixes as well as suffixes into account. Given a corpus and a dictionary, our method can be used to obtain a set of suffix-replacement rules for deriving an inflected wordÂs root form. We developed an approach for the Hindi language but show that the approach is portable, at least to related languages, by adapting it to the Gujarati language. Given that the entire process of developing such a ruleset is simple and fast, our approach can be used for rapid development of morphological analysers and yet it can obtain competitive results with analysers built relying on human authored rules."
aswani-gaizauskas-2010-english,{E}nglish-{H}indi Transliteration using Multiple Similarity Metrics,2010,12,4,2,1,41299,niraj aswani,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we present an approach to measure the transliteration similarity of English-Hindi word pairs. Our approach has two components. First we propose a bi-directional mapping between one or more characters in the Devanagari script and one or more characters in the Roman script (pronounced as in English). This allows a given Hindi word written in Devanagari to be transliterated into the Roman script and vice-versa. Second, we present an algorithm for computing a similarity measure that is a variant of DiceÂs coefficient measure and the LCSR measure and which also takes into account the constraints needed to match English-Hindi transliterated words. Finally, by evaluating various similarity metrics individually and together under a multiple measure agreement scenario, we show that it is possible to achieve a 0.92 f-measure in identifying English-Hindi word pairs that are transliterations. In order to assess the portability of our approach to other similar languages we adapt our system to the Gujarati language."
catizone-etal-2010-using,Using Dialogue Corpora to Extend Information Extraction Patterns for Natural Language Understanding of Dialogue,2010,8,1,3,0,42896,roberta catizone,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper examines how Natural Language Process (NLP) resources and online dialogue corpora can be used to extend coverage of Information Extraction (IE) templates in a Spoken Dialogue system. IE templates are used as part of a Natural Language Understanding module for identifying meaning in a user utterance. The use of NLP tools in Dialogue systems is a difficult task given 1) spoken dialogue is often not well-formed and 2) there is a serious lack of dialogue data. In spite of that, we have devised a method for extending IE patterns using standard NLP tools and available dialogue corpora found on the web. In this paper, we explain our method which includes using a set of NLP modules developed using GATE (a General Architecture for Text Engineering), as well as a general purpose editing tool that we built to facilitate the IE rule creation process. Lastly, we present directions for future work in this area."
D10-1047,Multi-Document Summarization Using {A}* Search and Discriminative Learning,2010,16,31,3,1,25116,ahmet aker,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"In this paper we address two key challenges for extractive multi-document summarization: the search problem of finding the best scoring summary and the training problem of learning the best model parameters. We propose an A* search algorithm to find the best extractive summary up to a given length, which is both optimal and efficient to run. Further, we propose a discriminative training algorithm which directly maximises the quality of the best summary, rather than assuming a sentence-level decomposition as in earlier work. Our approach leads to significantly better results than earlier techniques across a number of evaluation metrics."
W09-1309,Disambiguation of Biomedical Abbreviations,2009,30,36,4,0,2873,mark stevenson,Proceedings of the {B}io{NLP} 2009 Workshop,0,"Abbreviations are common in biomedical documents and many are ambiguous in the sense that they have several potential expansions. Identifying the correct expansion is necessary for language understanding and important for applications such as document retrieval. Identifying the correct expansion can be viewed as a Word Sense Disambiguation (WSD) problem. A WSD system that uses a variety of knowledge sources, including two types of information specific to the biomedical domain, is also described. This system was tested on a corpus of ambiguous abbreviations, created by automatically identifying the correct expansion in Medline abstracts, and found to identify the correct expansion with up to 99% accuracy."
R09-1002,Summary Generation for Toponym-referenced Images using Object Type Language Models,2009,20,14,2,1,25116,ahmet aker,Proceedings of the International Conference {RANLP}-2009,0,This paper presents a novel approach to automatic captioning of toponym-referenced images. The automatic captioning procedure works by summarizing multiple web-documents that contain information related to an imagexe2x80x99s location. Our summarizer can generate both query-based and language model-biased multidocument summaries. The models are created from large numbers of existing articles pertaining to places of the same xe2x80x9cobject typexe2x80x9d. Evaluation relative to human written captions shows that when language models are used to bias the summarizer the summaries score more highly than the non-biased ones.
W08-1805,A Data Driven Approach to Query Expansion in Question Answering,2008,15,13,3,1,642,leon derczynski,Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering,0,"Automated answering of natural language questions is an interesting and useful problem to solve. Question answering (QA) systems often perform information retrieval at an initial stage. Information retrieval (IR) performance, provided by engines such as Lucene, places a bound on overall system performance. For example, no answer bearing documents are retrieved at low ranks for almost 40% of questions.n n In this paper, answer texts from previous QA evaluations held as part of the Text REtrieval Conferences (TREC) are paired with queries and analysed in an attempt to identify performance-enhancing words. These words are then used to evaluate the performance of a query expansion method.n n Data driven extension words were found to help in over 70% of difficult questions. These words can be used to improve and evaluate query expansion methods. Simple blind relevance feedback (RF) was correctly predicted as unlikely to help overall performance, and an possible explanation is provided for its low value in IR for QA."
W08-1808,Evaluation of Automatically Reformulated Questions in Question Series,2008,5,2,3,0,47726,richard shaw,Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering,0,"Having gold standards allows us to evaluate new methods and approaches against a common benchmark. In this paper we describe a set of gold standard question reformulations and associated reformulation guidelines that we have created to support research into automatic interpretation of questions in TREC question series, where questions may refer anaphorically to the target of the series or to answers to previous questions. We also assess various string comparison metrics for their utility as evaluation measures of the proximity of an automated system's reformulations to the gold standard. Finally we show how we have used this approach to assess the question processing capability of our own QA system and to pinpoint areas for improvement."
W08-1401,Generating Image Captions using Topic Focused Multi-document Summarization,2008,0,0,1,1,33330,robert gaizauskas,Coling 2008: Proceedings of the workshop Multi-source Multilingual Information Extraction and Summarization,0,"In the near future digital cameras will come standardly equipped with GPS and compass and will automatically add global position and direction information to the metadata of every picture taken. Can we use this information, together with information from geographical information systems and the Web more generally, to caption images automatically?"
W08-1407,Evaluating automatically generated user-focused multi-document summaries for geo-referenced images,2008,16,4,2,1,25116,ahmet aker,Coling 2008: Proceedings of the workshop Multi-source Multilingual Information Extraction and Summarization,0,"This paper reports an initial study that aims to assess the viability of a state-of-the-art multi-document summarizer for automatic captioning of geo-referenced images. The automatic captioning procedure requires summarizing multiple web documents that contain information related to images' location. We use SUMMA (Saggion and Gaizauskas, 2005) to generate generic and query-based multi-document summaries and evaluate them using ROUGE evaluation metrics (Lin, 2004) relative to human generated summaries. Results show that, even though query-based summaries perform better than generic ones, they are still not selecting the information that human participants do. In particular, the areas of interest that human summaries display (history, travel information, etc.) are not contained in the query-based summaries. For our future work in automatic image captioning this result suggests that developing the query-based summarizer further and biasing it to account for user-specific requirements will prove worthwhile."
W08-0602,Extracting Clinical Relationships from Patient Narratives,2008,17,37,2,0,16937,angus roberts,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"The Clinical E-Science Framework (CLEF) project has built a system to extract clinically significant information from the textual component of medical records, for clinical research, evidence-based healthcare and genotype-meets-phenotype informatics. One part of this system is the identification of relationships between clinically important entities in the text. Typical approaches to relationship extraction in this domain have used full parses, domain-specific grammars, and large knowledge bases encoding domain knowledge. In other areas of biomedical NLP, statistical machine learning approaches are now routinely applied to relationship extraction. We report on the novel application of these statistical techniques to clinical relationships.n n We describe a supervised machine learning system, trained with a corpus of oncology narratives hand-annotated with clinically important relationships. Various shallow features are extracted from these texts, and used to train statistical classifiers. We compare the suitability of these features for clinical relationship extraction, how extraction varies between inter- and intra-sentential relationships, and examine the amount of training data needed to learn various relationships."
W08-0611,Knowledge Sources for Word Sense Disambiguation of Biomedical Text,2008,20,14,3,0.346047,2873,mark stevenson,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"Like text in other domains, biomedical documents contain a range of terms with more than one possible meaning. These ambiguities form a significant obstacle to the automatic processing of biomedical texts. Previous approaches to resolving this problem have made use of a variety of knowledge sources including linguistic information (from the context in which the ambiguous term is used) and domain-specific resources (such as UMLS). In this paper we compare a range of knowledge sources which have been previously used and introduce a novel one: MeSH terms. The best performance is obtained using linguistic features in combination with MeSH terms. Results from our system outperform published results for previously reported systems on a standard test set (the NLM-WSD corpus)."
demetriou-etal-2008-annalist,{ANNALIST} - {ANN}otation {ALI}gnment and Scoring Tool,2008,6,4,2,1,48283,george demetriou,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper we describe ANNALIST (Annotation, Alignment and Scoring Tool), a scoring system for the evaluation of the output of semantic annotation systems. ANNALIST has been designed as a system that is easily extensible and configurable for different domains, data formats, and evaluation tasks. The system architecture enables data input via the use of plugins and the users can access the systemÂs internal alignment and scoring mechanisms without the need to convert their data to a specified format. Although developed for evaluation tasks that involve the scoring of entity mentions and relations primarily, ANNALISTÂs generic object representation and the availability of a range of criteria for the comparison of annotations enable the system to be tailored to a variety of scoring jobs. The paper reports on results from using ANNALIST in real-world situations in comparison to other scorers which are more established in the literature. ANNALIST has been used extensively for evaluation tasks within the VIKEF (EU FP6) and CLEF (UK MRC) projects."
C08-1102,Acquiring Sense Tagged Examples using Relevance Feedback,2008,24,10,3,0.346047,2873,mark stevenson,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,Supervised approaches to Word Sense Disambiguation (WSD) have been shown to outperform other approaches but are hampered by reliance on labeled training examples (the data acquisition bottleneck). This paper presents a novel approach to the automatic acquisition of labeled examples for WSD which makes use of the Information Retrieval technique of relevance feedback. This semi-supervised method generates additional labeled examples based on existing annotated data. Our approach is applied to a set of ambiguous terms from biomedical journal articles and found to significantly improve the performance of a state-of-the-art WSD system.
S07-1014,{S}em{E}val-2007 Task 15: {T}emp{E}val Temporal Relation Identification,2007,4,212,2,0,3193,marc verhagen,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"The TempEval task proposes a simple way to evaluate automatic extraction of temporal relations. It avoids the pitfalls of evaluating a graph of inter-related labels by defining three sub tasks that allow pairwise evaluation of temporal relations. The task not only allows straightforward evaluation, it also avoids the complexities of full temporal parsing."
S07-1098,{USFD}: Preliminary Exploration of Features and Classifiers for the {T}emp{E}val-2007 Task,2007,5,24,3,0,28213,mark hepple,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"We describe the Sheffield system used in TempEval-2007. Our system takes a machine-learning (ML) based approach, treating temporal relation assignment as a simple classification task and using features easily derived from the TempEval data, i.e. which do not require 'deeper' NLP analysis. We aimed to explore three questions: (1) How well would a 'lite' approach of this kind perform? (2) Which features contribute positively to system performance? (3) Which ML algorithm is better suited for the TempEval tasks? We used the Weka ML workbench to facilitate experimenting with different ML algorithms. The paper describes our system and supplies preliminary answers to the above questions."
saggion-gaizauskas-2006-language,Language Resources for Background Gathering,2006,5,1,2,0,5986,horacio saggion,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We describe the Cubreporter information access system which allows access to news archives through the use of natural language technology. The system includes advanced text search, question answering, summarization, and entity profiling capabilities. It has been designed taking into account the characteristics of the background gathering task."
barker-etal-2006-simulating,Simulating Cub Reporter Dialogues: The collection of naturalistic human-human dialogues for information access to text archives,2006,9,1,4,1,33318,emma barker,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes a dialogue data collection experiment and resulting corpus for dialogues between a senior mobile journalist and a junior cub reporter back at the office. The purpose of the dialogue is for the mobile journalist to collect background information in preparation for an interview or on-the-site coverage of a breaking story. The cub reporter has access to text archives that contain such background information. A unique aspect of these dialogues is that they capture information-seeking behavior for an open-ended task against a large unstructured data source. Initial analyses of the corpus show that the experimental design leads to real-time, mixedinitiative, highly interactive dialogues with many interesting properties."
W05-1527,{SUPPLE}: A Practical Parser for Natural Language Engineering Applications,2005,8,25,1,1,33330,robert gaizauskas,Proceedings of the Ninth International Workshop on Parsing Technology,0,"We describe SUPPLE, a freely-available, open source natural language parsing system, implemented in Prolog, and designed for practical use in language engineering (LE) applications. SUPPLE can be run as a stand-alone application, or as a component within the GATE General Architecture for Text Engineering. SUPPLE is distributed with an example grammar that has been developed over a number of years across several LE projects. This paper describes the key characteristics of the parser and the distributed grammar."
W05-0808,A Hybrid Approach to Align Sentences and Words in {E}nglish-{H}indi Parallel Corpora,2005,11,24,2,1,41299,niraj aswani,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"In this paper we describe an alignment system that aligns English-Hindi texts at the sentence and word level in parallel corpora. We describe a simple sentence length approach to sentence alignment and a hybrid, multi-feature approach to perform word alignment. We use regression techniques in order to learn parameters which characterise the relationship between the lengths of two sentences in parallel text. We use a multi-feature approach with dictionary lookup as a primary technique and other methods such as local word grouping, transliteration similarity (edit-distance) and a nearest aligned neighbours approach to deal with many-to-many word alignment. Our experiments are based on the EMILLE (Enabling Minority Language Engineering) corpus. We obtained 99.09% accuracy for many-to-many sentence alignment and 77% precision and 67.79% recall for many-to-many word alignment."
W05-0819,Aligning Words in {E}nglish-{H}indi Parallel Corpora,2005,2,11,2,1,41299,niraj aswani,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"In this paper, we describe a word alignment algorithm for English-Hindi parallel data. The system was developed to participate in the shared task on word alignment for languages with scarce resources at the ACL 2005 workshop, on Building and using parallel texts: data driven machine translation and beyond. Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours. We trained our system on the training data provided to obtain a list of named entities and cognates and to collect rules for local word grouping in Hindi sentences. The system scored 77.03% precision and 60.68% recall on the shared task unseen test data."
W04-3110,A Large Scale Terminology Resource for Biomedical Text Processing,2004,15,17,2,0,42340,henk harkema,"{HLT}-{NAACL} 2004 Workshop: Linking Biological Literature, Ontologies and Databases",0,"In this paper we discuss the design, implementation, and use of Termino, a large scale terminological resource for text processing. Dealing with terminology is a difficult but unavoidable task for language processing applications, such as Information Extraction in technical domains. Complex, heterogeneous information must be stored about large numbers of terms. At the same time term recognition must be performed in realistic times. Termino attempts to reconcile this tension by maintaining a flexible, extensible relational database for storing terminological information and compiling finite state machines from this database to do term lookup. While Termino has been developed for biomedical applications, its general design allows it to be used for term processing in any domain."
harkema-etal-2004-large,A Large-Scale Resource for Storing and Recognizing Technical Terminology,2004,8,5,2,0,42340,henk harkema,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
mitchell-gaizauskas-2004-labelled,A Labelled Corpus for Prepositional Phrase Attachment,2004,10,0,2,0,52295,brian mitchell,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper describes a labelled corpus intended for training learning algorithms to attach prepositional phrases (PPs). Taken from the PTB2, we believe it is the largest available resource for this purpose, especially as it contains many patterns in which PPs occur ambiguously (nearly all previous research has focused on just one pattern) and we present some results for the five most common patterns. Moreover, the corpus contains some features that, to our knowledge, have not been used before for attaching PPs."
W02-0311,Utilizing text mining results: The Pasta Web System,2002,12,13,2,1,48283,george demetriou,Proceedings of the {ACL}-02 Workshop on Natural Language Processing in the Biomedical Domain,0,"Information Extraction (IE), defined as the activity to extract structured knowledge from unstructured text sources, offers new opportunities for the exploitation of biological information contained in the vast amounts of scientific literature. But while IE technology has received increasing attention in the area of molecular biology, there have not been many examples of IE systems successfully deployed in end-user applications. We describe the development of PASTAWeb, a WWW-based interface to the extraction output of PASTA, an IE system that extracts protein structure information from MEDLINE abstracts. Key characteristics of PASTAWeb are the seamless integration of the PASTA extraction results (templates) with WWW-based technology, the dynamic generation of WWW content from 'static' data and the fusion of information extracted from multiple documents."
P02-1020,Measuring Text Reuse,2002,13,117,2,0.961538,40907,paul clough,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we present results from the METER (MEasuring TExt Reuse) project whose aim is to explore issues pertaining to text reuse and derivation, especially in the context of newspapers using newswire sources. Although the reuse of text by journalists has been studied in linguistics, we are not aware of any investigation using existing computational methods for this particular task. We investigate the classification of newspaper articles according to their degree of dependence upon, or derivation from, a newswire source using a simple 3-level scheme designed by journalists. Three approaches to measuring text similarity are considered: n-gram overlap, Greedy String Tiling, and sentence alignment. Measured against a manually annotated corpus of source and derived news text, we show that a combined classifier with features automatically selected performs best overall for the ternary classification achieving an average F1-measure score of 0.664 across all three categories."
mitchell-gaizauskas-2002-comparison,A Comparison of Machine Learning Algorithms for Prepositional Phrase Attachment,2002,12,8,2,0,52295,brian mitchell,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper presents work which extends previous corpus-based work on training Machine Learning Algorithms to perform Prepositional Phrase attachment. Besides recreating othersxe2x80x99 experiments to see how algorithmsxe2x80x99 performance changes with the number of training examples and using n-fold cross-validation to produce more accurate error rates, we implemented our own vanilla Machine Learning Algorithms as a comparison. We also had people perform exactly the same task as the Machine Learning Algorithms to indicate whether the way forward lies in improving Machine Learning Algorithms or in improving the data sets used to train Machine Learning Algorithms. The results from all these experiments feed into our other work transforming the Penn TreeBank into a more useful resource for training Machine Learning Algorithms to do Prepositional Phrase attachment."
clough-etal-2002-building,Building and annotating a corpus for the study of journalistic text reuse,2002,7,32,2,0.961538,40907,paul clough,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In this paper we present the METER Corpus, a novel resource for the study and analysis of journalistic text reuse. The corpus consists of a set of news stories written by the Press Association (PA), the major UK news agency, and a set of stories about the same news events, as published in various British newspapers. In some cases the newspaper stories are rewritten from the PA source; in other cases they have been independently written by the newspapers' own journalists. We discuss the motivation for creating the corpus, its contents, the annotation of certain attributes for analysis of text reuse and finally the encoding of those annotations into a standardised corpus format: the Text Encoding Initiative (TEI)."
W01-1311,A Pilot Study On Annotating Temporal Relations In Text,2001,2,43,2,1,49133,andrea setzer,Proceedings of the {ACL} 2001 Workshop on Temporal and Spatial Information Processing,0,We describe a pilot study in which a scheme for annotating events and temporal relations between events in text is applied to a small corpus of newswire texts. High levels of agreement between human annotators are shown to be difficult to achieve and we investigate where discrepancies occur and how these might be addressed.
W01-1004,"Using {HLT} for Acquiring, Retrieving and Publishing Knowledge in {AKT}",2001,0,3,6,0,11076,kalina bontcheva,Proceedings of the {ACL} 2001 Workshop on Human Language Technology and Knowledge Management,0,None
H01-1040,Intelligent Access to Text: Integrating Information Extraction Technology into Text Browsers,2001,4,21,1,1,33330,robert gaizauskas,Proceedings of the First International Conference on Human Language Technology Research,0,"In this paper we show how two standard outputs from information extraction (IE) systems -- named entity annotations and scenario templates -- can be used to enhance access to text collections via a standard text browser. We describe how this information is used in a prototype system designed to support information workers' access to a pharmaceutical news archive as part of their industry watch function. We also report results of a preliminary, qualitative user evaluation of the system, which while broadly positive indicates further work needs to be done on the interface to make users aware of the increased potential of IE-enhanced text browsers."
demetriou-gaizauskas-2000-automatically,Automatically Augmenting Terminological Lexicons from Untagged Text,2000,13,6,2,1,48283,george demetriou,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Lexical resources play a crucial role in language technology but lexical acquisition can often be a time-consuming, laborious and costly exercise. In this paper, we describe a method for the automatic acquisition of technical terminology from domain restricted texts without the need for sophisticated natural language processing tools, such as taggers or parsers, or text corpora annotated with labelled cases. The method is based on the idea of using prior or seed knowledge in order to discover co-occurrence patterns for the terms in the texts. A bootstrapping algorithm has been developed that identifies patterns and new terms in an iterative manner. Experiments with scientific journal abstracts in the biology domain indicate an accuracy rate for the extracted terms ranging from 58% to 71%. The new terms have been found useful for improving the coverage of a system used for terminology identification tasks in the biology domain."
setzer-gaizauskas-2000-annotating,Annotating Events and Temporal Information in Newswire Texts,2000,9,59,2,1,49133,andrea setzer,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"If one is concerned with natural language processing applications such as information extraction (IE), which typically involve extracting information about temporally situated scenarios, the ability to accurately position key events in time is of great importance. To date only minimal work has been done in the IE community concerning the extraction of temporal information from text, and the importance, together with the difficulty of the task, suggest that a concerted effort be made to analyse how temporal information is actually conveyed in real texts. To this end we have devised an annotation scheme for annotating those features and relations in texts which enable us to determine the relative order and, if possible, the absolute time, of the events reported in them. Such a scheme could be used to construct an annotated corpus which would yield the benefits normally associated with the construction of such resources: a better understanding of the phenomena of concern, and a resource for the training and evaluation of adaptive algorithms to automatically identify features and relations of interest. We also describe a framework for evaluating the annotation and compute precision and recall for different responses."
A00-1012,Experiments on Sentence Boundary Detection,2000,12,48,2,0.346047,2873,mark stevenson,Sixth Applied Natural Language Processing Conference,0,This paper explores the problem of identifying sentence boundaries in the transcriptions produced by automatic speech recognition systems. An experiment which determines the level of human performance for this task is described as well as a memory-based computational approach to the problem.
A00-1040,Using Corpus-derived Name Lists for Named Entity Recognition,2000,7,38,2,0.346047,2873,mark stevenson,Sixth Applied Natural Language Processing Conference,0,"This paper describes experiments to establish the performance of a named entity recognition system which builds categorized lists of names from manually annotated training data. Names in text are then identified using only these lists. This approach does not perform as well as state-of-the-art named entity recognition systems. However, we then show that by using simple filtering techniques for improving the automatically acquired lists, substantial performance benefits can be achieved, with resulting F-measure scores of 87% on a standard test set. These results provide a baseline against which the contribution of more sophisticated supervised learning techniques for NE recognition should be measured."
W99-0211,Using Coreference Chains for Text Summarization,1999,8,51,3,1,54902,saliha azzam,Coreference and Its Applications,0,"We describe the use of coreference chains for the production of text summaries, using a variety of criteria to select a 'best' chain to represent the main topic of a text. The approach has been implemented within an existing MUC coreference system, which constructs a full discourse model of texts, including information about changes of focus, which can be used in the selection of chains. Some preliminary experiments on the automatic evaluation of summaries are also described, using existing tools to attempt to replicate some of the recent SUMMAC manual evaluations."
P98-1011,Evaluating a Focus-Based Approach to Anaphora Resolution,1998,8,26,3,1,54902,saliha azzam,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"We present an approach to anaphora resolution based on a focusing algorithm, and implemented within an existing MUC (Message Understanding Conference) Information Extraction system, allowing quantitative evaluation against a substantial corpus of annotated real-world texts. Extensions to the basic focusing mechanism can be easily tested, resulting in refinements to the mechanism and resolution rules. Results show that the focusing algorithm is highly sensitive to the quality of syntactic-semantic analyses, when compared to a simpler heuristic-based approach."
P98-1115,Compacting the {P}enn {T}reebank Grammar,1998,5,38,3,0,55364,alexander krotov,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"Treebanks, such as the Penn Treebank (PTB), offer a simple approach to obtaining a broad coverage grammar: one can simply read the grammar off the parse trees in the treebank. While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more treebanked text would be required to obtain a complete grammar, if one exists at some limit. However, we offer an alternative explanation in terms of the underspecification of structures within the treebank. This hypothesis is explored by applying an algorithm to compact the derived grammar by eliminating redundant rules - rules whose right hand sides can be parsed by other rules. The size of the resulting compacted grammar, which is significantly less than that of the full treebank grammar, is shown to approach a limit. However, such a compacted grammar does not yield very good performance figures. A version of the compaction algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated. Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall, but a loss in precision."
O98-4002,Information Extraction: Beyond Document Retrieval,1998,42,180,1,1,33330,robert gaizauskas,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 3, Number 2, August 1998",0,"In this paper we give a synoptic view of the growth of the text processing technology of information extraction (IE) whose function is to extract information about a prexe2x80x90specified set of entities, relations or events from natural language texts and to record this information in structured representations called templates. Here we describe the nature of the IE task, review the history of the area from its origins in AI work in the 1960s and 70s till the present, discuss the techniques being used to carry out the task, describe application areas where IE systems are or are about to be at work, and conclude with a discussion of the challenges facing the area. What emerges is a picture of an exciting new text processing technology with a host of new applications, both on its own and in conjunction with other technologies, such as information retrieval, machine translation and data mining."
C98-1011,Evaluating a Focus-Based Approach to Anaphora Resolution,1998,8,26,3,1,54902,saliha azzam,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"We present an approach to anaphora resolution based on a focusing algorithm, and implemented within an existing MUC (Message Understanding Conference) Information Extraction system, allowing quantitative evaluation against a substantial corpus of annotated real-world texts. Extensions to the basic focusing mechanism can be easily tested, resulting in refinements to the mechanism and resolution rules. Results show that the focusing algorithm is highly sensitive to the quality of syntactic-semantic analyses, when compared to a simpler heuristic-based approach."
C98-1111,Compacting the {P}enn {T}reebank Grammar,1998,5,38,3,0,55364,alexander krotov,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"Treebanks, such as the Penn Treebank (PTB), offer a simple approach to obtaining a broad coverage grammar: one can simply read the grammar off the parse trees in the treebank. While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more treebanked text would be required to obtain a complete grammar, if one exists at some limit. However, we offer an alternative explanation in terms of the underspecification of structures within the treebank. This hypothesis is explored by applying an algorithm to compact the derived grammar by eliminating redundant rules - rules whose right hand sides can be parsed by other rules. The size of the resulting compacted grammar, which is significantly less than that of the full treebank grammar, is shown to approach a limit. However, such a compacted grammar does not yield very good performance figures. A version of the compaction algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated. Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall, but a loss in precision."
W97-1311,Event coreference for information extraction,1997,9,63,2,1,50786,kevin humphreys,"Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts",0,"We propose a general approach for performing event coreference and for constructing complex event representations, such as those required for information extraction tasks. Our approach is based on a representation which allows a tight coupling between world or conceptual modelling and discourse modelling. The representation and the coreference mechanism are fully implemented within the LaSIE information extraction system where the mechanism is used for both object (noun phrase) and event coreference resolution. Indirect evaluation of the approach shows small, but significant benefit, for information extraction tasks."
A97-2017,{GATE} - a General Architecture for Text Engineering,1997,34,126,3,1,41365,hamish cunningham,Fifth Conference on Applied Natural Language Processing: Descriptions of System Demonstrations and Videos,0,"Much progress has been made in the provision of reusable data resources for Natural Language Engineering, such as grammars, lexicons, thesauruscs. Although a number of projects have addressed the provision of reusable algorithmic resources (or 'tools'), takeup of these resources has been relatively slow. This paper describes GATE, a General Architecture for Text Engineering, which is a freely-available system designed to help alleviate the problem."
A97-1035,Software Infrastructure for Natural Language Processing,1997,9,62,3,1,41365,hamish cunningham,Fifth Conference on Applied Natural Language Processing,0,"We classify and review current approaches to software infrastructure for research, development and delivery of NLP systems. The task is motivated by a discussion of current trends in the field of NLP and Language Engineering. We describe a system called GATE (a General Architecture for Text Engineering) that provides a software infrastructure on top of which heterogeneous NLP processing modules may be evaluated and refined individually, or may be combined into larger application systems. GATE aims to support both researchers and developers working on component technologies (e.g. parsing, tagging, morphological analysis) and those working on developing end-user applications (e.g. information extraction, text summarisation, document generation, machine translation, and second language learning). GATE promotes reuse of component technology, permits specialisation and collaboration in large-scale projects, and allows for the comparison and evaluation of alternative technologies. The first release of GATE is now available."
X96-1027,{TIPSTER}-Compatible Projects at {S}heffield,1996,-1,-1,3,1,41365,hamish cunningham,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,None
X96-1059,{NEC} Corporation and {U}niversity of {S}heffield: {``}Description of {NEC}/{S}heffleld System Used For {MET} {J}apanese{''},1996,2,1,4,0,55880,yoshikazu takemoto,"TIPSTER TEXT PROGRAM PHASE II: Proceedings of a Workshop held at Vienna, Virginia, May 6-8, 1996",0,Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing ([1] [2]). It has also been studied in the framework of Japanese information extraction ([3]) in recent years.
C96-2187,{GATE}-a General Architecture for Text Engineering,1996,-1,-1,3,1,41365,hamish cunningham,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,None
C96-1071,Evaluation of an Algorithm for the Recognition and Classification of Proper Names,1996,4,67,2,0,55307,takahiro wakao,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"We describe an information extraction system in which four classes of naming expressions - organisation, person, location and time names - are recognised and classified with nearly 92% combined precision and recall. The system applies a mixture of techniques to perform this task and these are described in detail. We have quantitatively evaluated the system against a blind test set of Wall Street Journal business articles and report results not only for the system as a whole, but for each component technique and for each class of name. These results show that in order to have high recall, the system needs to make use not only of information internal to the naming expression but also information from outside the name. They also show that the contribution of each system component varies from one class of name expression to another."
M93-1026,{S}ussex {U}niversity: Description of the {S}ussex System Used for {MUC}-5,1993,-1,-1,1,1,33330,robert gaizauskas,"Fifth Message Understanding Conference ({MUC}-5): Proceedings of a Conference Held in Baltimore, {M}aryland, August 25-27, 1993",0,None
