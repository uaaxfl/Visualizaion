2021.acl-short.55,An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers,2021,-1,-1,2,0.759851,650,tharindu ranasinghe,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Most studies on word-level Quality Estimation (QE) of machine translation focus on language-specific models. The obvious disadvantages of these approaches are the need for labelled data for each language pair and the high cost required to maintain several language-specific models. To overcome these problems, we explore different approaches to multilingual, word-level QE. We show that multilingual QE models perform on par with the current language-specific models. In the cases of zero-shot and few-shot QE, we demonstrate that it is possible to accurately predict word-level quality for any given new language pair from models trained on other language pairs. Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios."
2020.wmt-1.122,{T}rans{Q}uest at {WMT}2020: Sentence-Level Direct Assessment,2020,-1,-1,2,1,650,tharindu ranasinghe,Proceedings of the Fifth Conference on Machine Translation,0,"This paper presents the team TransQuest{'}s participation in Sentence-Level Direct Assessment shared task in WMT 2020. We introduce a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. The proposed methods achieve state-of-the-art results surpassing the results obtained by OpenKiwi, the baseline used in the shared task. We further fine tune the QE framework by performing ensemble and data augmentation. Our approach is the winning solution in all of the language pairs according to the WMT 2020 official results."
2020.wanlp-1.3,Is it Great or Terrible? Preserving Sentiment in Neural Machine Translation of {A}rabic Reviews,2020,-1,-1,2,0,14109,hadeel saadany,Proceedings of the Fifth Arabic Natural Language Processing Workshop,0,"Since the advent of Neural Machine Translation (NMT) approaches there has been a tremendous improvement in the quality of automatic translation. However, NMT output still lacks accuracy in some low-resource languages and sometimes makes major errors that need extensive postediting. This is particularly noticeable with texts that do not follow common lexico-grammatical standards, such as user generated content (UGC). In this paper we investigate the challenges involved in translating book reviews from Arabic into English, with particular focus on the errors that lead to incorrect translation of sentiment polarity. Our study points to the special characteristics of Arabic UGC, examines the sentiment transfer errors made by Google Translate of Arabic UGC to English, analyzes why the problem occurs, and proposes an error typology specific of the translation of Arabic UGC. Our analysis shows that the output of online translation tools of Arabic UGC can either fail to transfer the sentiment at all by producing a neutral target text, or completely flips the sentiment polarity of the target word or phrase and hence delivers a wrong affect message. We address this problem by fine-tuning an NMT model with respect to sentiment polarity showing that this approach can significantly help with correcting sentiment errors detected in the online translation of Arabic UGC."
2020.semeval-1.94,{RGCL} at {S}em{E}val-2020 Task 6: Neural Approaches to {D}efinition{E}xtraction,2020,-1,-1,3,1,650,tharindu ranasinghe,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"This paper presents the RGCL team submission to SemEval 2020 Task 6: DeftEval, subtasks 1 and 2. The system classifies definitions at the sentence and token levels. It utilises state-of-the-art neural network architectures, which have some task-specific adaptations, including an automatically extended training set. Overall, the approach achieves acceptable evaluation scores, while maintaining flexibility in architecture selection."
2020.rdsm-1.7,Fake or Real? A Study of {A}rabic Satirical Fake News,2020,-1,-1,2,0,14109,hadeel saadany,Proceedings of the 3rd International Workshop on Rumours and Deception in Social Media (RDSM),0,"One very common type of fake news is satire which comes in a form of a news website or an online platform that parodies reputable real news agencies to create a sarcastic version of reality. This type of fake news is often disseminated by individuals on their online platforms as it has a much stronger effect in delivering criticism than through a straightforward message. However, when the satirical text is disseminated via social media without mention of its source, it can be mistaken for real news. This study conducts several exploratory analyses to identify the linguistic properties of Arabic fake news with satirical content. It shows that although it parodies real news, Arabic satirical news has distinguishing features on the lexico-grammatical level. We exploit these features to build a number of machine learning models capable of identifying satirical fake news with an accuracy of up to 98.6{\%}. The study introduces a new dataset (3185 articles) scraped from two Arabic satirical news websites ({`}Al-Hudood{'} and {`}Al-Ahram Al-Mexici{'}) which consists of fake news. The real news dataset consists of 3710 articles collected from three official news sites: the {`}BBC-Arabic{'}, the {`}CNN-Arabic{'} and {`}Al-Jazeera news{'}. Both datasets are concerned with political issues related to the Middle East."
2020.eamt-1.19,Intelligent Translation Memory Matching and Retrieval with Sentence Encoders,2020,18,0,2,1,650,tharindu ranasinghe,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,"Matching and retrieving previously translated segments from the Translation Memory is a key functionality in Translation Memories systems. However this matching and retrieving process is still limited to algorithms based on edit distance which we have identified as a major drawback in Translation Memories systems. In this paper, we introduce sentence encoders to improve matching and retrieving process in Translation Memories systems - an effective and efficient solution to replace edit distance-based algorithms."
2020.coling-main.445,{T}rans{Q}uest: Translation Quality Estimation with Cross-lingual Transformers,2020,-1,-1,2,1,650,tharindu ranasinghe,Proceedings of the 28th International Conference on Computational Linguistics,0,"Recent years have seen big advances in the field of sentence-level quality estimation (QE), largely as a result of using neural-based architectures. However, the majority of these methods work only on the language pair they are trained on and need retraining for new language pairs. This process can prove difficult from a technical point of view and is usually computationally expensive. In this paper we propose a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. Our evaluation shows that the proposed methods achieve state-of-the-art results outperforming current open-source quality estimation frameworks when trained on datasets from WMT. In addition, the framework proves very useful in transfer learning settings, especially when dealing with low-resourced languages, allowing us to obtain very competitive results."
R19-1033,Sentence Simplification for Semantic Role Labelling and Information Extraction,2019,0,0,2,0,1609,richard evans,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"In this paper, we report on the extrinsic evaluation of an automatic sentence simplification method with respect to two NLP tasks: semantic role labelling (SRL) and information extraction (IE). The paper begins with our observation of challenges in the intrinsic evaluation of sentence simplification systems, which motivates the use of extrinsic evaluation of these systems with respect to other NLP tasks. We describe the two NLP systems and the test data used in the extrinsic evaluation, and present arguments and evidence motivating the integration of a sentence simplification step as a means of improving the accuracy of these systems. Our evaluation reveals that their performance is improved by the simplification step: the SRL system is better able to assign semantic roles to the majority of the arguments of verbs and the IE system is better able to identify fillers for all IE template slots."
R19-1106,Toponym Detection in the Bio-Medical Domain: A Hybrid Approach with Deep Learning,2019,0,0,3,1,15125,alistair plum,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"This paper compares how different machine learning classifiers can be used together with simple string matching and named entity recognition to detect locations in texts. We compare five different state-of-the-art machine learning classifiers in order to predict whether a sentence contains a location or not. Following this classification task, we use a string matching algorithm with a gazetteer to identify the exact index of a toponym within the sentence. We evaluate different approaches in terms of machine learning classifiers, text pre-processing and location extraction on the SemEval-2019 Task 12 dataset, compiled for toponym resolution in the bio-medical domain. Finally, we compare the results with our system that was previously submitted to the SemEval-2019 task evaluation."
R19-1115,Enhancing Unsupervised Sentence Similarity Methods with Deep Contextualised Word Representations,2019,0,1,2,1,650,tharindu ranasinghe,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Calculating Semantic Textual Similarity (STS) plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. All modern state of the art STS methods rely on word embeddings one way or another. The recently introduced contextualised word embeddings have proved more effective than standard word embeddings in many natural language processing tasks. This paper evaluates the impact of several contextualised word embeddings on unsupervised STS methods and compares it with the existing supervised/unsupervised STS methods for different datasets in different languages and different domains"
R19-1116,Semantic Textual Similarity with {S}iamese Neural Networks,2019,0,2,2,1,650,tharindu ranasinghe,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Calculating the Semantic Textual Similarity (STS) is an important research area in natural language processing which plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. This paper evaluates Siamese recurrent architectures, a special type of neural networks, which are used here to measure STS. Several variants of the architecture are compared with existing methods"
R19-1155,A Survey of the Perceived Text Adaptation Needs of Adults with Autism,2019,0,0,2,0,12256,victoria yaneva,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"NLP approaches to automatic text adaptation often rely on user-need guidelines which are generic and do not account for the differences between various types of target groups. One such group are adults with high-functioning autism, who are usually able to read long sentences and comprehend difficult words but whose comprehension may be impeded by other linguistic constructions. This is especially challenging for real-world user-generated texts such as product reviews, which cannot be controlled editorially and are thus a particularly good applcation for automatic text adaptation systems. In this paper we present a mixed-methods survey conducted with 24 adult web-users diagnosed with autism and an age-matched control group of 33 neurotypical participants. The aim of the survey was to identify whether the group with autism experienced any barriers when reading online reviews, what these potential barriers were, and what NLP methods would be best suited to improve the accessibility of online reviews for people with autism. The group with autism consistently reported significantly greater difficulties with understanding online product reviews compared to the control group and identified issues related to text length, poor topic organisation, and the use of irony and sarcasm."
W18-6705,Trouble on the Road: Finding Reasons for Commuter Stress from Tweets,2018,-1,-1,3,0,27636,reshmi pillai,Proceedings of the Workshop on Intelligent Interactive Systems and Language Generation (2{IS}{\\&}{NLG}),0,None
W18-6239,What Makes You Stressed? Finding Reasons From Tweets,2018,0,0,3,0,27636,reshmi pillai,"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Detecting stress from social media gives a non-intrusive and inexpensive alternative to traditional tools such as questionnaires or physiological sensors for monitoring mental state of individuals. This paper introduces a novel framework for finding reasons for stress from tweets, analyzing multiple categories for the first time. Three word-vector based methods are evaluated on collections of tweets about politics or airlines and are found to be more accurate than standard machine learning algorithms."
W16-3413,Semantic Textual Similarity in Quality Estimation,2016,18,3,3,1,33757,hanna bechara,Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation,0,None
S15-2017,{M}ini{E}xperts: An {SVM} Approach for Measuring Semantic Textual Similarity,2015,21,5,5,1,33757,hanna bechara,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the system submitted by the University of Wolverhampton and the University of Malaga for SemEval-2015 Task 2: Semantic Textual Similarity. The system uses a Supported Vector Machine approach based on a number of linguistically motivated features. Our system performed satisfactorily for English and obtained a mean 0.7216 Pearson correlation. However, it performed less adequately for Spanish, obtaining only a mean 0.5158."
2015.tc-1.3,The {EXPERT} project: Advancing the state of the art in hybrid translation technologies,2015,8,0,1,1,12551,constantin orasan,Proceedings of Translating and the Computer 37,0,None
2015.eamt-1.6,Can Translation Memories afford not to use paraphrasing ?,2015,16,9,2,0.444444,22441,rohit gupta,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,"This paper investigates to what extent the use of paraphrasing in translation memory (TM) matching and retrieval is useful for human translators. Current translation memories lack semantic knowledge like paraphrasing in matching and retrieval. Due to this, paraphrased segments are often not retrieved. Lack of semantic knowledge also results in inappropriate ranking of the retrieved segments. Gupta and Orxc4x83san (2014) proposed an improved matching algorithm which incorporates paraphrasing. Its automatic evaluation suggested that it could be beneficial to translators. In this paper we perform an extensive human evaluation of the use of paraphrasing in the TM matching and retrieval process. We measure post-editing time, keystrokes, two subjective evaluations, and HTER and HMETEOR to assess the impact on human performance. Our results show that paraphrasing improves TM matching and retrieval, resulting in translation performance increases when translators use paraphrase enhanced TMs."
2014.tc-1.10,Intelligent translation memory matching and retrieval metric exploiting linguistic technology,2014,-1,-1,3,0.444444,22441,rohit gupta,Proceedings of Translating and the Computer 36,0,None
recasens-etal-2012-annotating,Annotating Near-Identity from Coreference Disagreements,2012,24,5,3,0,28963,marta recasens,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We present an extension of the coreference annotation in the English NP4E and the Catalan AnCora-CA corpora with near-identity relations, which are borderline cases of coreference. The annotated subcorpora have 50K tokens each. Near-identity relations, as presented by Recasens et al. (2010; 2011), build upon the idea that identity is a continuum rather than an either/or relation, thus introducing a middle ground category to explain currently problematic cases. The first annotation effort that we describe shows that it is not possible to annotate near-identity explicitly because subjects are not fully aware of it. Therefore, our second annotation effort used an indirect method, and arrived at near-identity annotations by inference from the disagreements between five annotators who had only a two-alternative choice between coreference and non-coreference. The results show that whereas as little as 2-6{\%} of the relations were explicitly annotated as near-identity in the former effort, up to 12-16{\%} of the relations turned out to be near-identical following the indirect method of the latter effort."
temnikova-etal-2012-clcm,{CLCM} - A Linguistic Resource for Effective Simplification of Instructions in the Crisis Management Domain and its Evaluations,2012,25,3,2,0,23303,irina temnikova,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Due to the increasing number of emergency situations which can have substantial consequences, both financially and fatally, the Crisis Management (CM) domain is developing at an exponential speed. The efficient management of emergency situations relies on clear communication between all of the participants in a crisis situation. For these reasons the Text Complexity (TC) of the CM domain needed to be investigated and showed that CM domain texts exhibit high TC levels. This article presents a new linguistic resource in the form of Controlled Language (CL) guidelines for manual text simplification in the CM domain which aims to address high TC in the CM domain and produce clear messages to be used in crisis situations. The effectiveness of the resource has been tested via evaluation from several different perspectives important for the domain. The overall results show that the CLCM simplification has a positive impact on TC, reading comprehension, manual translation and machine translation. Additionally, an investigation of the cognitive difficulty in applying manual simplification operations led to interesting discoveries. This article provides details of the evaluation methods, the conducted experiments, their results and indications about future work."
ou-etal-2008-development,Development and Alignment of a Domain-Specific Ontology for Question Answering,2008,15,47,3,0,48101,shiyan ou,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"With the appearance of Semantic Web technologies, it becomes possible to develop novel, sophisticated question answering systems, where ontologies are usually used as the core knowledge component. In the EU-funded project, QALL-ME, a domain-specific ontology was developed and applied for question answering in the domain of tourism, along with the assistance of two upper ontologies for concept expansion and reasoning. This paper focuses on the development of the QALL-ME ontology in the tourism domain and its alignment with the upper ontologies - WordNet and SUMO. The design of the ontology is presented in the paper, and a semi-automatic alignment procedure is described with some alignment results given as well. Furthermore, the aligned ontology was used to semantically annotate original data obtained from the tourism web sites and natural language questions. The storage schema of the annotated data and the data access method for retrieving answers from the annotated data are also reported in the paper."
cabrio-etal-2008-qall,The {QALL}-{ME} Benchmark: a Multilingual Resource of Annotated Spoken Requests for Question Answering,2008,7,9,6,0,5629,elena cabrio,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents the QALL-ME benchmark, a multilingual resource of annotated spoken requests in the tourism domain, freely available for research purposes. The languages currently involved in the project are Italian, English, Spanish and German. It introduces a semantic annotation scheme for spoken information access requests, specifically derived from Question Answering (QA) research. In addition to pragmatic and semantic annotations, we propose three QA-based annotation levels: the Expected Answer Type, the Expected Answer Quantifier and the Question Topical Target of a request, to fully capture the content of a request and extract the sought-after information. The QALL-ME benchmark is developed under the EU-FP6 QALL-ME project which aims at the realization of a shared and distributed infrastructure for Question Answering (QA) systems on mobile devices (e.g. mobile phones). Questions are formulated by the users in free natural language input, and the system returns the actual sequence of words which constitutes the answer from a collection of information sources (e.g. documents, databases). Within this framework, the benchmark has the twofold purpose of training machine learning based applications for QA, and testing their actual performance with a rapid turnaround in controlled laboratory setting."
C08-3008,Entailment-based Question Answering for Structured Data,2008,4,13,2,0,42996,bogdan sacaleanu,Coling 2008: Companion volume: Demonstrations,0,"This paper describes a Question Answering system which retrieves answers from structured data regarding cinemas and movies. The system represents the first prototype of a multilingual and multimodal QA system for the domain of tourism. Based on specially designed domain ontology and using Textual Entailment as a means for semantic inference, the system can be used in both monolingual and cross-language settings with slight adjustments for new input languages."
postolache-etal-2006-transferring,Transferring Coreference Chains through Word Alignment,2006,9,22,3,0,50208,oana postolache,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper investigates the problem of automatically annotating resources with NP coreference information using a parallel corpus, English-Romanian, in order to transfer, through word alignment, coreference chains from the English part to the Romanian part of the corpus. The results show that we can detect Romanian referential expressions and coreference chains with over 80{\%} F-measure, thus using our method as a preprocessing step followed by manual correction as part of an annotation effort for creating a large Romanian corpus with coreference information is worthwhile."
hasler-etal-2006-nps,{NP}s for Events: Experiments in Coreference Annotation,2006,12,25,2,0,48109,laura hasler,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes a pilot project which developed a methodology for NP and event coreference annotation consisting of detailed annotation schemes and guidelines. In order to develop this, a small sample annotated corpus in the domain of terrorism/security was built. The methodology developed can be used as a basis for large-scale annotation to produce much-needed resources. In contrast to related projects, ours focused almost exclusively on the development of annotation guidelines and schemes, to ensure that future annotations based on this methodology capture the phenomena both reliably and in detail. The project also involved extensive discussions in order to redraft the guidelines, as well as major extensions to PALinkA, our existing annotation tool, to accommodate event as well as NP coreference annotation."
2005.eamt-1.28,Building a {WSD} module within an {MT} system to enable interactive resolution in the user{'}s source language,2005,1,0,1,1,12551,constantin orasan,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"Ambiguous words pose very serious problems to existing machine translation systems. The Translation Checker, a system part of Translution Central addresses this problem by allowing users to disambiguate words in their own language, with little or no knowledge of the target language. In order to achieve this, a multilingual dictionary is being developed using EuroWordNet. Languages are too ambiguous to feasibly present users with all the senses available for a word. To this end, a suite of language processing modules has been developed to reduce the ambiguity of words. The implemented modules and an evaluation of their influence on English, French, German, Italian and Spanish corpora are presented. The results of the evaluation show that the proposed approach dramatically reduces the ambiguity of the language."
tutin-etal-2004-annotation,Annotation of Anaphoric Expressions in an Aligned Bilingual Corpus,2004,7,1,4,0,32037,agnes tutin,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper discusses a French-English corpus annotated and aligned at anaphoric level. It also presents an annotation scheme based on the study of a detailed corpus featuring different types of correspondences and mismatches. The scheme which is adapted from EAGLES recommendations, supports the alignment at anaphoric level and caters for the different kinds of mismatches."
W03-1205,An Evolutionary Approach for Improving the Quality of Automatic Summaries,2003,13,8,1,1,12551,constantin orasan,Proceedings of the {ACL} 2003 Workshop on Multilingual Summarization and Question Answering,0,"Automatic text extraction techniques have proved robust, but very often their summaries are not coherent. In this paper, we propose a new extraction method which uses local coherence as a means to improve the overall quality of automatic summaries. Two algorithms for sentence selection are proposed and evaluated on scientific documents. Evaluation showed that the method ameliorates the quality of summaries, noticeable improvements being obtained for longer summaries produced by an algorithm which selects sentences using an evolutionary algorithm."
E03-1066,{CAST}: A computer-aided summarisation tool,2003,7,18,1,1,12551,constantin orasan,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper we propose computer-aided summarisation (CAS) as an alternative approach to automatic summarisation, and present an ongoing project which aims to develop a CAS system. The need for such an alternative approach is justified by the relatively poor performance of fully automatic methods used in summarisation. Our system combines several summarisation methods, allowing the user of the system to interact with their parameters and output in order to improve the quality of the produced summary."
orasan-krishnamurthy-2002-corpus,A corpus-based investigation of junk emails,2002,6,14,1,1,12551,constantin orasan,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Almost everyone who has an email account receives from time to time unwanted emails. These emails can be jokes from friends or commercial product offers from unknown people. In this paper we focus on these unwanted messages which try to promote a product or service, or to offer some xe2x80x9chotxe2x80x9d business opportunities. These messages are called junk emails. Several methods to filter junk emails were proposed, but none considers the linguistic characteristics of junk emails. In this paper, we investigate the linguistic features of a corpus of junk emails, and try to decide if they constitute a distinct genre. Our corpus of junk emails was build from the messages received by the authors over a period of time. Initially, the corpus consisted of 1563, but after eliminating the duplications automatically we kept only 673 files, totalising just over 373,000 tokens. In order to decide if the junk emails constitute a different genre, a comparison with a corpus of leaflets extracted from BNC and with the whole BNC corpus is carried out. Several characteristics at the lexical and grammatical levels were identified."
orasan-2002-building,Building annotated resources for automatic text summarisation,2002,11,5,1,1,12551,constantin orasan,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Annotated corpora are necessary for automatic summarisation, but given how difficult is to produce them there are only few available. This paper presents an annotation tool which helps the human annotator to select the important units from a text. In addition to the tool, a new annotation scheme is proposed so that phenomena which such as presence of anaphoric expressions and redundancy can be marked. We argue that by annotating these phenomena the results of evaluation can be made more reliable."
W01-0716,Learning to identify animate references,2001,9,18,1,1,12551,constantin orasan,Proceedings of the {ACL} 2001 Workshop on Computational Natural Language Learning ({C}on{LL}),0,"Information about the animacy of nouns is important for a wide range of tasks in NLP. In this paper, we present a method for determining the animacy of English nouns using WordNet and machine learning techniques. Our method firstly categorises the senses from WordNet using an annotated corpus and then uses this information in order to classify nouns for which the sense is not known. Our evaluation results show that the accuracy of the classification of a noun is around 97% and that animate entities are more difficult to identify than inanimate ones."
