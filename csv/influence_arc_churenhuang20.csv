2020.aacl-main.26,J03-2004,0,0.54055,"enon of logical metonymy can be explained in terms of the thematic fit, that is, the degree of compatibility between the verb and one of its arguments (the direct object, in this case). On the one hand, a low thematic fit between an event-selecting verb and an entity-denoting argument triggers the recovery of a covert event, while on the other hand, the recovered event is often the best fitting one, given the information available in the sentence. Research in NLP on logical metonymy initially focused on the problem of covert event retrieval, which was tackled by means of probabilistic models (Lapata and Lascarides, 2003; Shutova, 2009), or by using Distributional Semantic Models (DSMs) that identify the candidate covert event with the one that has the highest thematic fit with the arguments in the sentence (Zarcone et al., 2012). Following the psycholinguistic works by McElree et al. (2001) and Traxler et al. (2002), which reported increased reading times and longer fixations in eye-tracking for the metonymic sentences, Zarcone et al. (2013) proposed a distributional model of the thematic fit between verb and object, and showed that it accurately reproduces the differences between the experimental conditions"
2020.aacl-main.26,Q17-1010,0,0.0114471,"s computed as the similarity score of its corresponding lexical vector ~e with the prototype vector. As we did the probabilistic model, we discard the metonymic verb from this computation. 3 The verb E representing the preferred interpretation of the metonymy is the verb e maximizing the following equation: E = argmaxe P (e)P (o|e)P (s|e) We test two variations of this model, TF-add and TF-prod, which differ for the filler selection update function. Statistics were extracted from Wikipedia 2018, and the vectors were the publiclyavailable Wikipedia embeddings 4 trained with the FastText model (Bojanowski et al., 2017). The verb-filler association score is the Local Mutual Information (Evert, 2008). Similarly, the scores for the subject fillers are defined as: We computed the statistics from a 2018 dump of the English Wikipedia, parsed with the Stanford CoreNLP toolkit (Manning et al., 2014). Dataset MC TR L&L CE Coverage 19/30 (pairs) 21/36 (pairs) 151/174 (items) 195/285 (items) Table 2: Coverage for the probabilistic model. sbj LM I(s, e) = f (e ←−− s)log2 3.3.2 Logical Metonymy as Thematic Fit Distributional models of logical metonymy assume that the event recovery task can be seen as a thematic fit tas"
2020.aacl-main.26,W11-0607,1,0.906684,"Missing"
2020.aacl-main.26,S17-1021,1,0.940651,"nts and their typical participants (see McRae and Matsuki (2009) for an overview) and claims that words act like cues to access event knowledge, incrementally modulating sentence comprehension. The results obtained in a probe recognition experiment by Zarcone et al. (2014), in line with this explanation, suggest that speakers interpret logical metonymies by inferring the most likely event the sentences could refer to, given the contextual cues. Previous research in NLP on logical metonymy has often been influenced by such theoretical explanation (Zarcone and Pad´o, 2011; Zarcone et al., 2012; Chersoni et al., 2017). In our contribution, we propose a general comparison of different classes of computational models for logical metonymy. To begin with, we tested two approaches that have been previously introduced in the literature on the topic: probabilistic and distributional models (Zarcone et al., 2012). We also examined the Structured Distributional Model (SDM) by Chersoni et al. (2019), which represents sentence meaning with a combination of formal structures and distributional embeddings to dynamically integrate knowledge about events and their typical participants, as they are activated by lexical it"
2020.aacl-main.26,2021.ccl-1.108,0,0.0624012,"Missing"
2020.aacl-main.26,P14-5010,0,0.00429667,"Missing"
2020.aacl-main.26,N19-1423,0,0.363105,"ransformer language models into a contrastive study on 1 Notice however that the evidence is not uncontroversial: Delogu et al. (2017) report that coercion costs largely reflect word surprisal, without any specific effect of type shift in the early processing measures. 224 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 224–234 c December 4 - 7, 2020. 2020 Association for Computational Linguistics logical metonymy. Transformers (Vaswani et al., 2017; Devlin et al., 2019) are the dominant class of NLP systems in the last few years, since they are able to generate “dynamic” representations for a target word depending on the sentence context. As the interpretation of logical metonymy is highly sensitive to context, we deem that the contextual representations built by Transformers might be able to integrate the covert event that is missing in the surface form of the sentence. All models are evaluated on their capability of assigning the correct interpretation to a metonymic sentence, that is, recovering the verb that refers to the correct interpretation. This tas"
2020.aacl-main.26,N18-1202,0,0.00848065,"ly-elaborated compositional model. The authors recently introduced a more up-to-date and refined version of their sentence comprehension model (Chersoni et al., 2019), but it has not been tested on the logical metonymy task so far. 2.2 Transformer Models in NLP The traditional approach in Distributional Semantics has been the building of a single, stable vector representation for each word type in the corpus (Turney and Pantel, 2010; Lenci, 2018). Lately, a new generation of embeddings has emerged, in which each occurrence of a word in a specific sentence context gets a unique representation (Peters et al., 2018). The most recent systems typically rely on an LSTM or a Transformer architecture for getting word representations: they are trained on large amounts of textual data and the word vectors are learned as a function of the internal states of the encoder, such that a word in different sentence contexts determines different activation states and is represented by a different vector. Thus, embeddings generated by these new models are said to be contextualized, as opposed to the static vectors generated by the earlier frameworks, and they aim at modeling the specific sense assumed by the word in cont"
2020.aacl-main.26,Q15-1032,0,0.0700545,"Missing"
2020.aacl-main.26,D17-1068,1,0.651403,"Missing"
2020.aacl-main.26,W16-2518,0,0.285284,"Missing"
2020.aacl-main.26,P09-3001,0,0.0413085,"be explained in terms of the thematic fit, that is, the degree of compatibility between the verb and one of its arguments (the direct object, in this case). On the one hand, a low thematic fit between an event-selecting verb and an entity-denoting argument triggers the recovery of a covert event, while on the other hand, the recovered event is often the best fitting one, given the information available in the sentence. Research in NLP on logical metonymy initially focused on the problem of covert event retrieval, which was tackled by means of probabilistic models (Lapata and Lascarides, 2003; Shutova, 2009), or by using Distributional Semantic Models (DSMs) that identify the candidate covert event with the one that has the highest thematic fit with the arguments in the sentence (Zarcone et al., 2012). Following the psycholinguistic works by McElree et al. (2001) and Traxler et al. (2002), which reported increased reading times and longer fixations in eye-tracking for the metonymic sentences, Zarcone et al. (2013) proposed a distributional model of the thematic fit between verb and object, and showed that it accurately reproduces the differences between the experimental conditions in the data fro"
2020.aacl-main.26,W13-0216,1,0.861246,"Missing"
2020.aacl-main.26,W12-1707,0,0.0210505,"Missing"
2020.aacl-main.84,O97-4005,0,0.072584,"s segmented using Stanford Parser (Manning et al., 2014) which fails to identify the word “PO主”, post owner and breaks it into two parts. The same type of error also occurs in other popular segmentation tools. Although Huang et al. (2007) proposed a radical method of word segmentation to meet the challenge, using a concept of classifying a string of character-boundaries into either word-boundaries or non-word-boundaries, their work did not address the cases of code-mixing words, whose word boundaries can also fall on foreigner alphabets. Some other methods mainly rely on unsupervised methods (Chang and Su, 1997) or simple statistical methods based on N-gram frequencies, with indices of collocation and co-occurrence (Chang 833 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 833–842 c December 4 - 7, 2020. 2020 Association for Computational Linguistics and Su, 1997; Chen and Ma, 2002; Dias, 2003). However, these works are mainly designed for new words of pure Chinese characters, which are not applicable to MAWs. In this paper, we address the issue of MAW ident"
2020.aacl-main.84,Y96-1018,1,0.623566,"dying the morpho-lexical status of MAWs (Lun, 2013; Riha and Baker, 2010; Riha, 2010). In the age of Internet and social media, the scale of MAWs, their extraction methods, and resources of MAWs have changed drastically since the last decade. For example, Zheng et al. (2005) extracted a small set of MAWs with manual validation from the corpus of People’s Daily (Year 2002). Jiang and Dang (2007) extracted 93 MAWs (out of 1,053 new domain-specific terms) using a statistical approach with rule-based validation. Recently, Huang and Liu (2017) extracted over 1,157 MAWs from both the Sinica Corpus (Chen et al., 1996) and the Chinese Gigaword Corpus (Huang, 2009) based on manually segmented MAWs in the corpora. Although they have extracted 60,000 tokens with alphabetical letters. However, the list mainly includes pure alphabets those are indeed switching codes of other languages. In our study, these pure code-switching words are excluded according to our definition. Their work has established a taxonomy of distributional patterns of alphabetical letters in MAWs and found that typical MAWs follow Chinese modifier-modified (head) morphological rule and the most frequent and productive pattern is alphabetical"
2020.aacl-main.84,C92-1019,0,0.776232,"y and orthography (e.g. “PK过"", player killed, past tense). Meanwhile, it also demonstrates some properties of the foreigner language (e.g. “维生素ing"", supplementing Vitamin, progressive)), providing a unique lexical resource for studying morphophonological idiosyncrasies of code-mixing words. Second, MAWs serve as an indispensable part of people’s daily vocabulary, especially under the rapid development of social media communication. Yet, being out-liars of the Chinese lexicon, they can cause problems to existing word segmentation/new word extraction tools that are trained on traditional words (Chen and Liu, 1992; Xue and Shen, 2003). Consider the following example: Mandarin Alphabetical Word (MAW) is one indispensable component of Modern Chinese that demonstrates unique code-mixing idiosyncrasies influenced by language exchanges. Yet, this interesting phenomenon has not been properly addressed and is mostly excluded from the Chinese language system. This paper addresses the core problem of MAW identification and proposes to construct a large collection of MAWs from Sina Weibo (SMAW) using an automatic web-based technique which includes rule-based identification, informaticsbased extraction, as well a"
2020.aacl-main.84,C02-1049,0,0.135039,"s, their work did not address the cases of code-mixing words, whose word boundaries can also fall on foreigner alphabets. Some other methods mainly rely on unsupervised methods (Chang and Su, 1997) or simple statistical methods based on N-gram frequencies, with indices of collocation and co-occurrence (Chang 833 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 833–842 c December 4 - 7, 2020. 2020 Association for Computational Linguistics and Su, 1997; Chen and Ma, 2002; Dias, 2003). However, these works are mainly designed for new words of pure Chinese characters, which are not applicable to MAWs. In this paper, we address the issue of MAW identification and present the construction of the Sina MAW lexicon (SMAW) (available at https://github.com/Christainx/SMAW) using a fully automatic information extraction technique. The quality of the MAWs (accurateness and inter-rater agreement) are rated by three experts for system evaluation. Compared to previous resources, this dataset provides an unprecedentedly large, balanced, and structured MAWs as well as a MAW"
2020.aacl-main.84,P14-5010,0,0.0136778,"inese lexical characteristics. It is noteworthy that MAWs shall be taken as a code-mixing phenomenon instead of code-switching as a MAW is still a Chinese word which is not switched into another language. Therefore, in this work, MAWS refer to the combined type which encodes both alphabet(s) and Chinese character(s) in one word, such as “A型”, A-type, “PO主”, post owner, and “γ线”, Gamma Ray. Seg: Golden Seg: PO主 主也不知道链接被吞了 (The post owner didn’t know that the link has been hacked off) PO/主 主/也/不/知道/链接/被/吞/了 PO主 主/也/不/知道/链接/被/吞/了 The sentence in E1 (example 1) is segmented using Stanford Parser (Manning et al., 2014) which fails to identify the word “PO主”, post owner and breaks it into two parts. The same type of error also occurs in other popular segmentation tools. Although Huang et al. (2007) proposed a radical method of word segmentation to meet the challenge, using a concept of classifying a string of character-boundaries into either word-boundaries or non-word-boundaries, their work did not address the cases of code-mixing words, whose word boundaries can also fall on foreigner alphabets. Some other methods mainly rely on unsupervised methods (Chang and Su, 1997) or simple statistical methods based"
2020.aacl-main.84,W03-1806,0,0.256838,"Missing"
2020.aacl-main.84,Y06-1024,1,0.736425,"Missing"
2020.aacl-main.84,P07-2018,1,0.473342,"to another language. Therefore, in this work, MAWS refer to the combined type which encodes both alphabet(s) and Chinese character(s) in one word, such as “A型”, A-type, “PO主”, post owner, and “γ线”, Gamma Ray. Seg: Golden Seg: PO主 主也不知道链接被吞了 (The post owner didn’t know that the link has been hacked off) PO/主 主/也/不/知道/链接/被/吞/了 PO主 主/也/不/知道/链接/被/吞/了 The sentence in E1 (example 1) is segmented using Stanford Parser (Manning et al., 2014) which fails to identify the word “PO主”, post owner and breaks it into two parts. The same type of error also occurs in other popular segmentation tools. Although Huang et al. (2007) proposed a radical method of word segmentation to meet the challenge, using a concept of classifying a string of character-boundaries into either word-boundaries or non-word-boundaries, their work did not address the cases of code-mixing words, whose word boundaries can also fall on foreigner alphabets. Some other methods mainly rely on unsupervised methods (Chang and Su, 1997) or simple statistical methods based on N-gram frequencies, with indices of collocation and co-occurrence (Chang 833 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Lin"
2020.aacl-main.84,W16-2013,0,0.0278025,"e system. This paper addresses the core problem of MAW identification and proposes to construct a large collection of MAWs from Sina Weibo (SMAW) using an automatic web-based technique which includes rule-based identification, informaticsbased extraction, as well as Baidu search engine validation. A collection of 16,207 qualified SMAWs are obtained using this technique along with an annotated corpus of more than 200,000 sentences for linguistic research and applicable inquiries. 1 Introduction E1: Mandarin Alphabetic Words (MAWs), also known as lettered words (Liu, 1994) or code-mixing words (Nguyen and Cornips, 2016), are usually formed by Latin, Greek, Arabic alphabets in combination with Chinese characters, e.g. “X-光/X射 线”, X-ray. Although pure alphabets (e.g. “NBA”) used in Chinese context have also been regarded as MAWs in some previous work (Liu, 1994; Huang and Liu, 2017), they are more like switching-codes that retain the orthography and linguistic behaviors of the original language, instead of showing typical Chinese lexical characteristics. It is noteworthy that MAWs shall be taken as a code-mixing phenomenon instead of code-switching as a MAW is still a Chinese word which is not switched into an"
2020.aacl-main.84,W03-1728,0,0.0204287,".g. “PK过"", player killed, past tense). Meanwhile, it also demonstrates some properties of the foreigner language (e.g. “维生素ing"", supplementing Vitamin, progressive)), providing a unique lexical resource for studying morphophonological idiosyncrasies of code-mixing words. Second, MAWs serve as an indispensable part of people’s daily vocabulary, especially under the rapid development of social media communication. Yet, being out-liars of the Chinese lexicon, they can cause problems to existing word segmentation/new word extraction tools that are trained on traditional words (Chen and Liu, 1992; Xue and Shen, 2003). Consider the following example: Mandarin Alphabetical Word (MAW) is one indispensable component of Modern Chinese that demonstrates unique code-mixing idiosyncrasies influenced by language exchanges. Yet, this interesting phenomenon has not been properly addressed and is mostly excluded from the Chinese language system. This paper addresses the core problem of MAW identification and proposes to construct a large collection of MAWs from Sina Weibo (SMAW) using an automatic web-based technique which includes rule-based identification, informaticsbased extraction, as well as Baidu search engine"
2020.aacl-main.84,W06-0127,0,0.0485724,"Missing"
2020.figlang-1.16,W18-0911,0,0.0890963,"taphors in texts is essential for capturing the authentic meaning of the texts, which can benefit many natural language Related Work Many approaches have been proposed for automatic detection of metaphors, using features of lexical information (Klebanov et al., 2014; Wilks et al., 2013), semantic classes (Klebanov et al., 2016), concreteness (Klebanov et al., 2015), word associations (Xiao et al., 2016), constructions and frames (Hong, 2016) and systems such as traditional machine learning classifiers (Rai et al., 2016), deep neural networks (Do Dinh and Gurevych, 2016) and sequential models (Bizzoni and Ghanimifard, 2018). Despite many advances in the above work, metaphor detection remains a challenging task. The semantic and ontological differences between metaphorical and non-metaphorical expressions are often subtle and their perception may vary from person to person. To tackle such problems, researchers resort to specific domain knowledge (Tsvetkov et al., 2014); lexicons (Mohler et al., 2013; Dodge et al., 2015); supervised methods (Klebanov et al., 2014, 2015, 2016) or using attention-based deep learning models to capture latent patterns (Igam1 http://www.vismet.org/metcor/ documentation/home.html 2 http"
2020.figlang-1.16,Y18-1025,0,0.0277047,"Missing"
2020.figlang-1.16,W14-2302,0,0.456477,"e to interpret/understand complex concepts. On the other hand, as a popular linguistic device, metaphors encode versatile ontological information, which usually involve e.g. domain transfer (Ahrens et al., 2003; Ahrens, 2010; Ahrens and Jiang, 2020), sentiment reverse (Steen et al., 2010) or modality shift (Winter, 2019) etc. Therefore, detecting the metaphors in texts is essential for capturing the authentic meaning of the texts, which can benefit many natural language Related Work Many approaches have been proposed for automatic detection of metaphors, using features of lexical information (Klebanov et al., 2014; Wilks et al., 2013), semantic classes (Klebanov et al., 2016), concreteness (Klebanov et al., 2015), word associations (Xiao et al., 2016), constructions and frames (Hong, 2016) and systems such as traditional machine learning classifiers (Rai et al., 2016), deep neural networks (Do Dinh and Gurevych, 2016) and sequential models (Bizzoni and Ghanimifard, 2018). Despite many advances in the above work, metaphor detection remains a challenging task. The semantic and ontological differences between metaphorical and non-metaphorical expressions are often subtle and their perception may vary from"
2020.figlang-1.16,W15-1402,0,0.320626,"hors encode versatile ontological information, which usually involve e.g. domain transfer (Ahrens et al., 2003; Ahrens, 2010; Ahrens and Jiang, 2020), sentiment reverse (Steen et al., 2010) or modality shift (Winter, 2019) etc. Therefore, detecting the metaphors in texts is essential for capturing the authentic meaning of the texts, which can benefit many natural language Related Work Many approaches have been proposed for automatic detection of metaphors, using features of lexical information (Klebanov et al., 2014; Wilks et al., 2013), semantic classes (Klebanov et al., 2016), concreteness (Klebanov et al., 2015), word associations (Xiao et al., 2016), constructions and frames (Hong, 2016) and systems such as traditional machine learning classifiers (Rai et al., 2016), deep neural networks (Do Dinh and Gurevych, 2016) and sequential models (Bizzoni and Ghanimifard, 2018). Despite many advances in the above work, metaphor detection remains a challenging task. The semantic and ontological differences between metaphorical and non-metaphorical expressions are often subtle and their perception may vary from person to person. To tackle such problems, researchers resort to specific domain knowledge (Tsvetkov"
2020.figlang-1.16,P16-2017,0,0.347753,"as a popular linguistic device, metaphors encode versatile ontological information, which usually involve e.g. domain transfer (Ahrens et al., 2003; Ahrens, 2010; Ahrens and Jiang, 2020), sentiment reverse (Steen et al., 2010) or modality shift (Winter, 2019) etc. Therefore, detecting the metaphors in texts is essential for capturing the authentic meaning of the texts, which can benefit many natural language Related Work Many approaches have been proposed for automatic detection of metaphors, using features of lexical information (Klebanov et al., 2014; Wilks et al., 2013), semantic classes (Klebanov et al., 2016), concreteness (Klebanov et al., 2015), word associations (Xiao et al., 2016), constructions and frames (Hong, 2016) and systems such as traditional machine learning classifiers (Rai et al., 2016), deep neural networks (Do Dinh and Gurevych, 2016) and sequential models (Bizzoni and Ghanimifard, 2018). Despite many advances in the above work, metaphor detection remains a challenging task. The semantic and ontological differences between metaphorical and non-metaphorical expressions are often subtle and their perception may vary from person to person. To tackle such problems, researchers resort"
2020.figlang-1.16,W03-1405,1,0.806458,"and Johnson (1980) in Conceptual Metaphor Theory (CMT), metaphor is not only a property of the language but also a cognitive mechanism that describes our conceptual system. Thus metaphors are devices that transfer the property of one domain to another unrelated or different domain, as in ‘sweet voice’ (use taste to describe sound). Metaphors are prevalent in daily life and play a significant role for people to interpret/understand complex concepts. On the other hand, as a popular linguistic device, metaphors encode versatile ontological information, which usually involve e.g. domain transfer (Ahrens et al., 2003; Ahrens, 2010; Ahrens and Jiang, 2020), sentiment reverse (Steen et al., 2010) or modality shift (Winter, 2019) etc. Therefore, detecting the metaphors in texts is essential for capturing the authentic meaning of the texts, which can benefit many natural language Related Work Many approaches have been proposed for automatic detection of metaphors, using features of lexical information (Klebanov et al., 2014; Wilks et al., 2013), semantic classes (Klebanov et al., 2016), concreteness (Klebanov et al., 2015), word associations (Xiao et al., 2016), constructions and frames (Hong, 2016) and syste"
2020.figlang-1.16,W18-0907,0,0.832024,"653/v1/P17 – EB (embodiment): 2 dimension of nodes representation, including embodiment rating and standard deviation; – EB-diff (embodiment differences): 2 × 5 dimension of node-neighbor pairs (five lexical neighboring words) information. berdiev and Shin, 2018). These methods show different strengths on detecting metaphors, yet each has its respective disadvantages, such as having generalization problems or lack association of their results with the intrinsic properties of metaphors. In addition, the reported performances of metaphor detection so far (around 0.6 F1 in the last shared task) (Leong et al., 2018) are still not promising. This calls for further endeavours in all aspects. In this work, we adopt supervised machine learning algorithms based on four categories of features, which include linguistic norms, ngram-word, -lemma and -pos collocations, word embeddings and cosine similarity between the target nodes and its neighboring words, as well as the strong baselines provided by the organizer of the shared task (Leong et al., 2018; Klebanov et al., 2014, 2015, 2016). Moreover, we use several statistical models and ensemble learning strategies during training and testing so as to test the cro"
2020.figlang-1.16,W13-0904,0,0.0615221,"et al., 2016), constructions and frames (Hong, 2016) and systems such as traditional machine learning classifiers (Rai et al., 2016), deep neural networks (Do Dinh and Gurevych, 2016) and sequential models (Bizzoni and Ghanimifard, 2018). Despite many advances in the above work, metaphor detection remains a challenging task. The semantic and ontological differences between metaphorical and non-metaphorical expressions are often subtle and their perception may vary from person to person. To tackle such problems, researchers resort to specific domain knowledge (Tsvetkov et al., 2014); lexicons (Mohler et al., 2013; Dodge et al., 2015); supervised methods (Klebanov et al., 2014, 2015, 2016) or using attention-based deep learning models to capture latent patterns (Igam1 http://www.vismet.org/metcor/ documentation/home.html 2 https://catalog.ldc.upenn.edu/ LDC2014T06 104 Proceedings of the Second Workshop on Figurative Language Processing, pages 104–109 c July 9, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 – EB (embodiment): 2 dimension of nodes representation, including embodiment rating and standard deviation; – EB-diff (embodiment differences): 2 × 5 dimension o"
2020.figlang-1.16,D14-1162,0,0.086523,"utilise distributional vector representation of word meaning to the nodes based on the distributional hypothesis (Firth, 1957; Lenci, 2018). Two pre-trained Word2Vec models (GoogleNews.300d and Internal-W2V.300d (pre-trained using the VUA and TOFEL corpora)) and the GloVe vectors are used. GoogleNews7 in this work is pre-trained using the continuous bag-of-words architecture for computing vector representations of words (Church, 2017). GloVe8 is an unsupervised learning algorithm for obtaining vector representations for words. We use the 300d vectors pre-trained on Wikipedia 2014+Gigaword 5 (Pennington et al., 2014). • B3: baseline 2 and unigrams, pos tag, topic, concreteness ratings between nodes and up and down words respectively (UL + WordNet + CCDB + U + P + T + CUp + CDown) 4 Three traditional classifiers are used for predicting the metaphoricity of the tokens, including Logistic Regression, Linear SVC and a Random Forest Classifier. The Machine Learning experiments are run through utilities in the SciKit-Learn Laboratory (SKLL) (Pedregosa et al., 2011). 9 For parameter tuning, we use grid search to find optimal parameters for the learners. Finally, we set up the following optimized parameters for t"
2020.figlang-1.16,W16-1103,0,0.126763,"verse (Steen et al., 2010) or modality shift (Winter, 2019) etc. Therefore, detecting the metaphors in texts is essential for capturing the authentic meaning of the texts, which can benefit many natural language Related Work Many approaches have been proposed for automatic detection of metaphors, using features of lexical information (Klebanov et al., 2014; Wilks et al., 2013), semantic classes (Klebanov et al., 2016), concreteness (Klebanov et al., 2015), word associations (Xiao et al., 2016), constructions and frames (Hong, 2016) and systems such as traditional machine learning classifiers (Rai et al., 2016), deep neural networks (Do Dinh and Gurevych, 2016) and sequential models (Bizzoni and Ghanimifard, 2018). Despite many advances in the above work, metaphor detection remains a challenging task. The semantic and ontological differences between metaphorical and non-metaphorical expressions are often subtle and their perception may vary from person to person. To tackle such problems, researchers resort to specific domain knowledge (Tsvetkov et al., 2014); lexicons (Mohler et al., 2013; Dodge et al., 2015); supervised methods (Klebanov et al., 2014, 2015, 2016) or using attention-based deep learn"
2020.figlang-1.16,P14-1024,0,0.66723,"∗ Mingyu Wan1,3 , Kathleen Ahrens2 , Emmanuele Chersoni3 Menghan Jiang3 , Qi Su1 , Rong Xiang4 , Chu-Ren Huang3 1 School of Foreign Languages, Peking University Dept. of English, The Hong Kong Polytechnic University 3 Dept. of Chinese and Bilingual Studies, The Hong Kong Polytechnic University 4 Dept. of Computing, The Hong Kong Polytechnic University ∗ {wanmy , sukia}@pku.edu.cn, {emmanuelechersoni, xiangrong0302}@gmail.com {kathleen.ahrens, menghanengl.jiang, churen.huang}@polyu.edu.hk 2 Abstract processing applications, such as machine translation, dialogue systems and sentiment analysis (Tsvetkov et al., 2014). In this shared task, we aim to detect token-level metaphors from plain texts by focusing on content words (Verbs, Nouns, Adjectives and Adverbs) of two corpora: VUA1 and TOFEL2 . To better understand the intrinsic properties of metaphors and to provide an in-depth analysis to this phenomenon, we propose a linguisticallyenriched model to deal with this task with the use of modality exclusivity and embodiment norms (see details in Section 3). This paper reports a linguistically-enriched method of detecting token-level metaphors for the second shared task on Metaphor Detection. We participate i"
2020.figlang-1.16,W13-0905,0,0.36508,"nd complex concepts. On the other hand, as a popular linguistic device, metaphors encode versatile ontological information, which usually involve e.g. domain transfer (Ahrens et al., 2003; Ahrens, 2010; Ahrens and Jiang, 2020), sentiment reverse (Steen et al., 2010) or modality shift (Winter, 2019) etc. Therefore, detecting the metaphors in texts is essential for capturing the authentic meaning of the texts, which can benefit many natural language Related Work Many approaches have been proposed for automatic detection of metaphors, using features of lexical information (Klebanov et al., 2014; Wilks et al., 2013), semantic classes (Klebanov et al., 2016), concreteness (Klebanov et al., 2015), word associations (Xiao et al., 2016), constructions and frames (Hong, 2016) and systems such as traditional machine learning classifiers (Rai et al., 2016), deep neural networks (Do Dinh and Gurevych, 2016) and sequential models (Bizzoni and Ghanimifard, 2018). Despite many advances in the above work, metaphor detection remains a challenging task. The semantic and ontological differences between metaphorical and non-metaphorical expressions are often subtle and their perception may vary from person to person. To"
2020.lrec-1.14,D16-1171,0,0.0324616,"Missing"
2020.lrec-1.14,D14-1080,0,0.0310631,"urce. This professional annotated lexicon are regarded as a highquality lexicon (Bainbridge et al., 1994) and it the main resource used in this work as the external affective resource. In 2010, a new release of this resource includes a collection of five thousand lexical items 1 (Heise, 2010). 2.2. Deep Neural Networks In recent years, neural network methods have greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (CNN) (Socher et al., 2011), Recursive Neural Network ReNN (Socher et al., 2013), and Recurrent Neural Networks (RNN) (Irsoy and Cardie, 2014). Long-Short Term Memory model (LSTM), well known for text understanding, is introduced by Tang et al. (2015a) who added a gated mechanism to keep long-term memory. Attentionbased neural networks, mostly built from local context, are proposed to highlight semantically important words and sentences (Yang et al., 2016). Other methods build attention models using external knowledge, such as user/product information (Chen et al., 2016) and cognition grounded data (Long et al., 2019). 2.3. Use of Affective Knowledge Previous studies in combining lexicon-based methods and machine learning approach g"
2020.lrec-1.14,C16-1147,0,0.025131,"sifiers and linearly integrates them into one system. Andreevskaia and Bergler (2008), for instance, present an ensemble system of two classifiers with precision-based weighting. This method obtained significant gains in both accuracy and recall over corpus-based classifiers and lexicon-based systems. The second approach incorporates lexicon knowledge into learning algorithms. To name a few, Hutto and Gilbert (2014) design a rule-based approach to indicate sentiment scores. Wilson et al. (2005) and Melville et al. (2009) use a general-purpose sentiment dictionary to improve linear classifier. Jovanoski et al. (2016) also prove that sentiment lexicon can contribute to logistic regression models. In neural network models, a remarkable 1 113 http://www.indiana.edu/∼socpsy/public files/EnglishWords EPAs.xlsx 0.6 red: E blue: P orange: A 0.5 0.4 proportion work on utilizing sentiment lexicons is done by Teng et al. (2016). They treat the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words. Qian et al. (2016) propose to apply linguistic regularization to sentiment classification with three linguistically motivated structured regularizers based on pars"
2020.lrec-1.14,D14-1181,0,0.00506385,"echniques as well as emotion theories to identify sentiment expressions in a natural language context. Typical SA studies analyze subjective documents from the author’s perspective using high-frequency word representations and mapping the text (e.g., sentence or document) to categorical labels, e.g., sentiment polarity, with either a discrete label or a real number in a continuum. Recently, the rising use of neural network models has further elevated the performance of SA without involving laborious feature engineering. Typical neural network models such as Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a) have shown promising results in a variety of sentiment analysis tasks. In spite of this, neural network models still face two main problems. First, neural network approaches lack direct mechanisms to highlight important components in a text. Second, external resources such as linguistic knowledge, cognition grounded data, and affective lexicons, are not fully employed in neural models. To tackle the first problem, cognition-based attention models have been adopted for sentiment classification us"
2020.lrec-1.14,P11-1015,0,0.23939,"Missing"
2020.lrec-1.14,D14-1162,0,0.083828,"Missing"
2020.lrec-1.14,D11-1014,0,0.0879513,"ough, there is no size indication, this EPA based lexicon is commonly used as a three dimensional affective resource. This professional annotated lexicon are regarded as a highquality lexicon (Bainbridge et al., 1994) and it the main resource used in this work as the external affective resource. In 2010, a new release of this resource includes a collection of five thousand lexical items 1 (Heise, 2010). 2.2. Deep Neural Networks In recent years, neural network methods have greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (CNN) (Socher et al., 2011), Recursive Neural Network ReNN (Socher et al., 2013), and Recurrent Neural Networks (RNN) (Irsoy and Cardie, 2014). Long-Short Term Memory model (LSTM), well known for text understanding, is introduced by Tang et al. (2015a) who added a gated mechanism to keep long-term memory. Attentionbased neural networks, mostly built from local context, are proposed to highlight semantically important words and sentences (Yang et al., 2016). Other methods build attention models using external knowledge, such as user/product information (Chen et al., 2016) and cognition grounded data (Long et al., 2019)."
2020.lrec-1.14,D13-1170,0,0.0433099,"to identify sentiment expressions in a natural language context. Typical SA studies analyze subjective documents from the author’s perspective using high-frequency word representations and mapping the text (e.g., sentence or document) to categorical labels, e.g., sentiment polarity, with either a discrete label or a real number in a continuum. Recently, the rising use of neural network models has further elevated the performance of SA without involving laborious feature engineering. Typical neural network models such as Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a) have shown promising results in a variety of sentiment analysis tasks. In spite of this, neural network models still face two main problems. First, neural network approaches lack direct mechanisms to highlight important components in a text. Second, external resources such as linguistic knowledge, cognition grounded data, and affective lexicons, are not fully employed in neural models. To tackle the first problem, cognition-based attention models have been adopted for sentiment classification using text-embedded information such as users, pr"
2020.lrec-1.14,D15-1167,0,0.235271,"age context. Typical SA studies analyze subjective documents from the author’s perspective using high-frequency word representations and mapping the text (e.g., sentence or document) to categorical labels, e.g., sentiment polarity, with either a discrete label or a real number in a continuum. Recently, the rising use of neural network models has further elevated the performance of SA without involving laborious feature engineering. Typical neural network models such as Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a) have shown promising results in a variety of sentiment analysis tasks. In spite of this, neural network models still face two main problems. First, neural network approaches lack direct mechanisms to highlight important components in a text. Second, external resources such as linguistic knowledge, cognition grounded data, and affective lexicons, are not fully employed in neural models. To tackle the first problem, cognition-based attention models have been adopted for sentiment classification using text-embedded information such as users, products, and local context (Tang et al., 2015b; Yan"
2020.lrec-1.14,P15-1098,0,0.122464,"age context. Typical SA studies analyze subjective documents from the author’s perspective using high-frequency word representations and mapping the text (e.g., sentence or document) to categorical labels, e.g., sentiment polarity, with either a discrete label or a real number in a continuum. Recently, the rising use of neural network models has further elevated the performance of SA without involving laborious feature engineering. Typical neural network models such as Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a) have shown promising results in a variety of sentiment analysis tasks. In spite of this, neural network models still face two main problems. First, neural network approaches lack direct mechanisms to highlight important components in a text. Second, external resources such as linguistic knowledge, cognition grounded data, and affective lexicons, are not fully employed in neural models. To tackle the first problem, cognition-based attention models have been adopted for sentiment classification using text-embedded information such as users, products, and local context (Tang et al., 2015b; Yan"
2020.lrec-1.14,D16-1169,0,0.0173378,"d approach incorporates lexicon knowledge into learning algorithms. To name a few, Hutto and Gilbert (2014) design a rule-based approach to indicate sentiment scores. Wilson et al. (2005) and Melville et al. (2009) use a general-purpose sentiment dictionary to improve linear classifier. Jovanoski et al. (2016) also prove that sentiment lexicon can contribute to logistic regression models. In neural network models, a remarkable 1 113 http://www.indiana.edu/∼socpsy/public files/EnglishWords EPAs.xlsx 0.6 red: E blue: P orange: A 0.5 0.4 proportion work on utilizing sentiment lexicons is done by Teng et al. (2016). They treat the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words. Qian et al. (2016) propose to apply linguistic regularization to sentiment classification with three linguistically motivated structured regularizers based on parse trees, topics, and hierarchical word clusters. Zou et al. (2018) adopt a mixed attention mechanism to further highlight the role of sentiment lexicon in the attention layer. Using sentiment polarity in a loss function is one way to employ attention mechanism. However, attention weights are normally obtai"
2020.lrec-1.14,H05-1044,0,0.158412,"on-based methods and machine learning approach generally diverge into two ways. The first approach uses two weighted classifiers and linearly integrates them into one system. Andreevskaia and Bergler (2008), for instance, present an ensemble system of two classifiers with precision-based weighting. This method obtained significant gains in both accuracy and recall over corpus-based classifiers and lexicon-based systems. The second approach incorporates lexicon knowledge into learning algorithms. To name a few, Hutto and Gilbert (2014) design a rule-based approach to indicate sentiment scores. Wilson et al. (2005) and Melville et al. (2009) use a general-purpose sentiment dictionary to improve linear classifier. Jovanoski et al. (2016) also prove that sentiment lexicon can contribute to logistic regression models. In neural network models, a remarkable 1 113 http://www.indiana.edu/∼socpsy/public files/EnglishWords EPAs.xlsx 0.6 red: E blue: P orange: A 0.5 0.4 proportion work on utilizing sentiment lexicons is done by Teng et al. (2016). They treat the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words. Qian et al. (2016) propose to apply lin"
2020.lrec-1.14,N16-1174,0,0.164662,"015a) have shown promising results in a variety of sentiment analysis tasks. In spite of this, neural network models still face two main problems. First, neural network approaches lack direct mechanisms to highlight important components in a text. Second, external resources such as linguistic knowledge, cognition grounded data, and affective lexicons, are not fully employed in neural models. To tackle the first problem, cognition-based attention models have been adopted for sentiment classification using text-embedded information such as users, products, and local context (Tang et al., 2015b; Yang et al., 2016; Chen et al., 2016; Long et al., 2019). For the second problem, Qian et al. (2016) proposed to add linguistic resources to deep learning models for further improvement. Yet, recent method of integration of additional lexical information are limited to matrix manipulation in attention layer due to the incompatibility of such representations with embedding ones, making it quite inefficient. In this paper, we attempt to address this problem by incorporating an affective lexicon as numerical influence values into affective neural network models through the framework of the Affect Control Theory ("
2020.lrec-1.14,C18-1074,0,0.0260178,"to logistic regression models. In neural network models, a remarkable 1 113 http://www.indiana.edu/∼socpsy/public files/EnglishWords EPAs.xlsx 0.6 red: E blue: P orange: A 0.5 0.4 proportion work on utilizing sentiment lexicons is done by Teng et al. (2016). They treat the sentiment score of a sentence as a weighted sum of prior sentiment scores of negation words and sentiment words. Qian et al. (2016) propose to apply linguistic regularization to sentiment classification with three linguistically motivated structured regularizers based on parse trees, topics, and hierarchical word clusters. Zou et al. (2018) adopt a mixed attention mechanism to further highlight the role of sentiment lexicon in the attention layer. Using sentiment polarity in a loss function is one way to employ attention mechanism. However, attention weights are normally obtained using local context information. The computational complexity of reweighing each word by attention requires matrix and softmax manipulation, which slows down the time for training and inference especially with long sentences. 0.3 0.2 0.1 0.0 −4 −2 0 affective value 2 4 Figure 1: Histogram of EPA values 3. Methodology We proposes a novel affection driven"
2020.lrec-1.700,J10-4006,1,0.940389,"e-filler pairs. McRae and Pad´o include, respectively, 1,444 and 414 scores for agents and patients (e.g., doctor-advise and hit-ball), whereas Ferretti includes judgements for 274 instruments and 248 locations (e.g., cut-mower and teach-classroom). The scores range from 1 (atypical) to 7 (very typical) and their distribution per role can be observed in Figures 1, 2 and 3. Experimental Settings 4 5 6 7 McRae dataset 3 3. Ferretti dataset Score tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are"
2020.lrec-1.700,P14-1023,0,0.32911,"e Learning Methods 1. Introduction In recent years, vectors derived from neural network training have quickly replaced the old, count-based Distributional Semantic Models (DSMs) as a de facto standard for word representation in NLP.1 Tools such as Word2Vec (Mikolov et al., 2013a; Mikolov et al., 2013b) have provided the research community with an efficient and scalable method for training vector representations, generally referred to as word embeddings. Moreover, the embeddings have been reported to have an advantage over the old count models also in terms of performance in several NLP tasks (Baroni et al., 2014).2 In this scenario, thematic fit estimation represents an exception. Concretely, the task consists in estimating a typicality score for a filler noun given a verb role (e.g., a system has to predict how plausible a cake is as a patient of the verb to eat). It is generally evaluated by assessing the correlation between collections of human judgements and DSM outputs, and it represents an important benchmark for the capacity of the models of capturing compositional meaning (Lenci, 2018). In a systematic comparison between count-based models and neural embeddings, (Baroni et al., 2014) showed th"
2020.lrec-1.700,Q17-1010,0,0.0525211,"Lenci, 2010) turned out to be particularly influential. Given a verb role, this study made use of a corresponding syntactic relation (e.g., the subject for the agent) to extract its typical fillers. The vectors of the typical fillers were then summed to create distributional representations of the prototypical fillers, and the thematic fit of a noun for a role was finally assessed as the cosine similarity between its filler vector and the role prototype. While word embeddings were taking distributional semantics by storm (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), it is surprising that, after the early studies, such vector representations have not been tested anymore on thematic fit. Moreover, the few works were carried out only with the original Word2Vec embeddings, trained on window-based contexts, and were limited to the Continuous-Bag-of-Words (CBOW) tested on two datasets, in which only the agent and the patient roles are represented. To the best of our knowledge, Skip-Gram vectors have never been tested on thematic fit estimation. Finally, the embeddings trained on syntactic dependencies by (Levy and Goldberg, 2014) proved to be efficient in mod"
2020.lrec-1.700,N19-1423,0,0.0438305,"Missing"
2020.lrec-1.700,N15-1003,0,0.655069,"ectively, 1,444 and 414 scores for agents and patients (e.g., doctor-advise and hit-ball), whereas Ferretti includes judgements for 274 instruments and 248 locations (e.g., cut-mower and teach-classroom). The scores range from 1 (atypical) to 7 (very typical) and their distribution per role can be observed in Figures 1, 2 and 3. Experimental Settings 4 5 6 7 McRae dataset 3 3. Ferretti dataset Score tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved"
2020.lrec-1.700,D17-1138,0,0.0157339,"tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved elements is different: discrete semantic types in the former case, gradient compatibility between arguments and thematic roles in the latter one (Lebani and Lenci, 2018). In the literature using DSMs for modeling thematic fit, the method by (Baroni and Lenci, 2010) turned out to be particularly influential. Given a verb role, this study made use of a corresponding syntactic relation (e.g., the subject for the agent) to extract its typical fillers. The vectors of the typic"
2020.lrec-1.700,K19-1050,0,0.0297538,"Missing"
2020.lrec-1.700,S18-2002,0,0.318261,"tti ones, even without syntactic fillers. Our scores were obtained simply by training the model with standard parameters and with no refined context selection. Thus, we conclude that word embeddings are not always a bad fit for the thematic fit task. Given the growing interest for the psychological plausibility of word embeddings and for their performance on cognitively-motivated benchmarks (Søgaard, 2016; Mandera et al., 2017; Bakarov, 2018; Schwartz and Mitchell, 9 It should be noticed that state-of-the-art neural systems for this task are trained on semantic role labels (Tilk et al., 2016; Hong et al., 2018), and thus they avoid -at least in theory- the problem of dealing with the ambiguity of the prepositions. 5711 2019; Hollenstein et al., 2019), future experiments might add thematic fit estimation to the set of tasks in which they could be tested, by carefully taking into account the impact of factors such as the size of training data and the linguistic information available to the models. Another possible direction of work could aim at adapting the recently-introduced contextualized embeddings (Radford et al., 2018; Peters et al., 2018; Devlin et al., 2019) to the task. 6. Acknowledgements We"
2020.lrec-1.700,E17-2063,0,0.240928,"tors have never been tested on thematic fit estimation. Finally, the embeddings trained on syntactic dependencies by (Levy and Goldberg, 2014) proved to be efficient in modeling the functional similarity between words that tend to have the same function or structural role in a sentence, and thus they seem good candidates to perform well in the task. 4 Here, we present a complete evaluation of the abovementioned models on datasets including agents, patients, instruments and locations. Moreover, given the recent claims that incorporating syntax in DSMs does not lead to significant improvements (Lapesa and Evert, 2017), we pay specific attention to a question not addressed yet in the literature: how essential is syntactic information for building good-quality semantic role prototypes? 1 2 Datasets. We tested our models on three standard datasets derived from (McRae et al., 1998), (Ferretti et al., 2001) and (Pad´o, 2007), containing plausibility judgments for AGENTS specific world knowledge in sentence comprehension, as in the psycholinguistic literature of reference. 4 See also (Turney, 2012) for the distinction between functional similarity and domain similarity. PATIENTS Role Figure 3: Scores distributio"
2020.lrec-1.700,P14-2050,0,0.305943,"t models based on count vectors. Despite the progress made in the recent literature and the introduction of several new architectures and improvements, the studies following the first evaluation attempts have only focused on count models. In the present contribution, we propose a more systematic comparison between embeddings and count-based models on thematic fit estimation. Compared to earlier evaluations, which only tested CBOW vectors on agents and patients datasets, we evaluate both the Word2Vec architectures on a wider variety of roles, as well as the dependency-based word embeddings by (Levy and Goldberg, 2014). Additionally, since the best thematic fit models make use of syntactic information to build ’prototypical’ representations of the verb roles, we test the importance of such information for the model performance. 2. Related Work According to a long tradition of psycholinguistic studies, human semantic memory stores a generalized knowledge about events and their participants (McRae et al., 1998; McRae et al., 2005; Hare et al., 2009). The typicality of the combinations of verbs and arguments has important consequences for sentence processing, as typical combinations require less effort from hu"
2020.lrec-1.700,D17-1257,0,0.0228285,"7 0.248 0.203 0.261 Table 4: Spearman correlations for the Instruments and Locations dataset for all models with all filler sets. This is not surprising: DM is a carefully crafted syntactic DSM, and the addition of lexical syntactic patterns has been hypothesized to have a positive impact in this task (Sayeed et al., 2015). However, the less-refined DEPS model perform similarly to the SG embeddings, which in turn perform always better than the CBOW ones. The performance of LG-DEPS is also close to the SG one: syntactic dependencies, as suggested by some recent contributions in the literature (Li et al., 2017; Lapesa and Evert, 2017), do not improve model performance. As for the fillers, syntax seems instead to play an important role: if we compare models with BOWF fillers with those making use of the ”syntactic” sets, we observe large and significant drops for every model on both datasets, to the point that many correlations on McRae become non-significant. DM fillers are clearly better than the DEPS one, suggesting that syntactic information is more useful to select typical contexts. The results for the other roles (Table 4) instead show a clear advantage of the BOW embeddings over count-based m"
2020.lrec-1.700,D14-1162,0,0.087969,"the method by (Baroni and Lenci, 2010) turned out to be particularly influential. Given a verb role, this study made use of a corresponding syntactic relation (e.g., the subject for the agent) to extract its typical fillers. The vectors of the typical fillers were then summed to create distributional representations of the prototypical fillers, and the thematic fit of a noun for a role was finally assessed as the cosine similarity between its filler vector and the role prototype. While word embeddings were taking distributional semantics by storm (Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017), it is surprising that, after the early studies, such vector representations have not been tested anymore on thematic fit. Moreover, the few works were carried out only with the original Word2Vec embeddings, trained on window-based contexts, and were limited to the Continuous-Bag-of-Words (CBOW) tested on two datasets, in which only the agent and the patient roles are represented. To the best of our knowledge, Skip-Gram vectors have never been tested on thematic fit estimation. Finally, the embeddings trained on syntactic dependencies by (Levy and Goldberg, 2014) pro"
2020.lrec-1.700,N18-1202,0,0.107127,"Missing"
2020.lrec-1.700,D16-1099,1,0.843203,"cant. DM fillers are clearly better than the DEPS one, suggesting that syntactic information is more useful to select typical contexts. The results for the other roles (Table 4) instead show a clear advantage of the BOW embeddings over count-based models. SG embeddings are again the best, followed by the CBOW ones, and LG-DEPS vectors, unexpectedly, are again lagging behind their BOW counterparts. Disappointing results for dependency-based embeddings have also been reported by (Gamallo, 2017), in comparison to syntactic count-based vectors. A possible cause, as suggested by (Asr et al., 2016; Sahlgren and Lenci, 2016), could be found in the fact that embedding models are suboptimal when trained on smaller data sizes, and this could be especially true with sparse dependency contexts. It is also interesting to observe that, with instruments and locations, dropping syntactically-selected fillers does not always cause huge correlation drops for the embedding models, and in some cases it even leads to improvements (cf. the SG and CBOW scores for Locations). Probably, using prepositions to select the fillers turns out to be a rough approximation, and the prototypes are so noisy that there are no gains with respe"
2020.lrec-1.700,D17-1068,1,0.923154,"scores for agents and patients (e.g., doctor-advise and hit-ball), whereas Ferretti includes judgements for 274 instruments and 248 locations (e.g., cut-mower and teach-classroom). The scores range from 1 (atypical) to 7 (very typical) and their distribution per role can be observed in Figures 1, 2 and 3. Experimental Settings 4 5 6 7 McRae dataset 3 3. Ferretti dataset Score tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved elements is different:"
2020.lrec-1.700,W16-2518,0,0.480117,"ce count between a verb v and a filler f and the expected count under independence (the formula is the same for the syntactic method, but adding the third element of the syntactic relation r). For each DSM and filler type, the vectors of the top scoring k fillers for each role were summed to build the prototypes. After testing with k = 10, 20, 30, 40, 50, we observed that the number of fillers did not significantly affect the performance, coherently with the findings of (Greenberg et al., 2015). The reported results have been obtained with k = 20, as in the works by (Baroni et al., 2014) and (Sayeed et al., 2016). The 5 DSMs have been evaluated with all 3 filler types, making 15 different models. Finally, we measured the cosine similarity between roles prototypes and fillers in the datasets, and we computed the Spearman correlation between scores and human judgements. Role Prototypes As in (Baroni and Lenci, 2010), we extract typical fillers for each verb role and sum them to cre5 Ov,f Ev,f LM I(v, f ) = log 4. Results and Discussion The scores for agents and patients datasets and those for instruments and locations follow quite different patterns. In Table 3, DM turns out to be by far the best model"
2020.lrec-1.700,N19-1005,0,0.0434075,"Missing"
2020.lrec-1.700,W16-2521,0,0.012326,"the task, performing similarly to a standard dependency-based model on the Pad´o and McRae datasets (it only lags behind the carefullycrafted DM), and outperforming all count-based competitors on the Ferretti ones, even without syntactic fillers. Our scores were obtained simply by training the model with standard parameters and with no refined context selection. Thus, we conclude that word embeddings are not always a bad fit for the thematic fit task. Given the growing interest for the psychological plausibility of word embeddings and for their performance on cognitively-motivated benchmarks (Søgaard, 2016; Mandera et al., 2017; Bakarov, 2018; Schwartz and Mitchell, 9 It should be noticed that state-of-the-art neural systems for this task are trained on semantic role labels (Tilk et al., 2016; Hong et al., 2018), and thus they avoid -at least in theory- the problem of dealing with the ambiguity of the prepositions. 5711 2019; Hollenstein et al., 2019), future experiments might add thematic fit estimation to the set of tasks in which they could be tested, by carefully taking into account the impact of factors such as the size of training data and the linguistic information available to the model"
2020.lrec-1.700,D16-1017,0,0.374475,"titors on the Ferretti ones, even without syntactic fillers. Our scores were obtained simply by training the model with standard parameters and with no refined context selection. Thus, we conclude that word embeddings are not always a bad fit for the thematic fit task. Given the growing interest for the psychological plausibility of word embeddings and for their performance on cognitively-motivated benchmarks (Søgaard, 2016; Mandera et al., 2017; Bakarov, 2018; Schwartz and Mitchell, 9 It should be noticed that state-of-the-art neural systems for this task are trained on semantic role labels (Tilk et al., 2016; Hong et al., 2018), and thus they avoid -at least in theory- the problem of dealing with the ambiguity of the prepositions. 5711 2019; Hollenstein et al., 2019), future experiments might add thematic fit estimation to the set of tasks in which they could be tested, by carefully taking into account the impact of factors such as the size of training data and the linguistic information available to the models. Another possible direction of work could aim at adapting the recently-introduced contextualized embeddings (Radford et al., 2018; Peters et al., 2018; Devlin et al., 2019) to the task. 6."
2020.lrec-1.700,J13-3006,0,0.0300509,"re tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved elements is different: discrete semantic types in the former case, gradient compatibility between arguments and thematic roles in the latter one (Lebani and Lenci, 2018). In the literature using DSMs for modeling thematic fit, the method by (Baroni and Lenci, 2010) turned out to be particularly influential. Given a verb role, this study made use of a corresponding syntactic relation (e.g., the su"
2020.lrec-1.700,P19-1071,0,0.421821,"teach-classroom). The scores range from 1 (atypical) to 7 (very typical) and their distribution per role can be observed in Figures 1, 2 and 3. Experimental Settings 4 5 6 7 McRae dataset 3 3. Ferretti dataset Score tence processing, several researchers in computational semantics have tried to model this phenomenon, mostly using syntax-based DSMs (Erk et al., 2010; Baroni and Lenci, 2010; Sayeed et al., 2015; Greenberg et al., 2015; Santus et al., 2017). Notice that this research trend developed in parallel with the one aiming at automatically acquiring selectional preferences (Resnik, 1997; Zhang et al., 2019; Zhang et al., 2020), which has mostly been seen as an auxiliary task for improving the performance of systems with different goals, such as semantic role classification (Collobert et al., 2011; Zapirain et al., 2013; Roth and Lapata, 2015) or coreference resolution (Heinzerling et al., 2017). Moreover, although the notions of selectional preference and thematic fit are closely related, the nature of the involved elements is different: discrete semantic types in the former case, gradient compatibility between arguments and thematic roles in the latter one (Lebani and Lenci, 2018). In the lite"
2020.lrec-1.701,W14-2608,0,0.0235638,"e are two main approaches in irony detection: rule-based approaches and machine learning approaches (Joshi et al., 2017). A rule-based approach in irony identification mainly relies on lexicons and syntactic patterns, while a machine learning approach combines different types of features and knowledge bases to detect irony (Maynard and Greenwood, 2014; Khattri et al., 2015). In addition to lexical features, some other types of features can contribute to detecting irony, depending on the text genre. These features included but not limited to sentiment words, punctuation (Carvalho et al., 2009; Buschmeier et al., 2014), emoticon (Buschmeier et al., 2014), emotional scenarios (Reyes and Rosso, 2014), as well as reversals (Li et al., 2019). Meanwhile, Support Vector Machine (SVM) and Logistic Regression (LR) remain the most popular models in classical machine learning approaches for irony detection (Ghosh et al., 2015; Li et al., 2019). Some recent works also tried to tackle irony identification using deep learning methods. Deep learning methods no longer need feature engineering and have shown superior abilities to complex word composition in text (Ghosh and Veale, 2016; Huang et al., 2017). A system based o"
2020.lrec-1.701,N19-1423,0,0.177849,"instances in Ciron are collected from Chinese 5714 microblogs in a grounded and more natural context. The annotation process ensures consistency and quality. The dataset is applied to several popular machine learning-based methods to demonstrate its effectiveness including Naive Bayes (NB), logistic regression (LR), support vector machine (SVM), convolutional neural networks(CNN) (Kim, 2014), long short-term memory networks(LSTM) (Tang et al., 2015), bidirectional LSTM with attention mechanism (BiLSTM-AT) (Zhang et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). The rest of this paper is organized as follows. Section 2 introduces related works on irony theories and illustrates typical examples of ironic sentences in Chinese. Section 3 provides an in-depth description of the data that we extracted from Weibo forums, together with the formalization of the irony features. The performance of the baseline models is evaluated in Section 4, highlighting its validity. Section 5 concludes this paper. 2. Related Works Based on several linguistic studies (Huang et al., 2017), we define ironic text as those expressions showing discrepancy/incongruity between th"
2020.lrec-1.701,W15-2915,0,0.0228667,"y detection has a large potential for various applications in the domain of text mining, especially those that require semantic analysis, such as author profiling, online harassment and hate speech detection, and perhaps, the most well-known task of affective analysis. Compared to other text analysis tasks, irony detection has received limited computational treatment (Barbieri and Saggion, 2014; Ghosh et al., 2019). Affective analysis works started to analyze and summarize linguistic features of irony from a sentiment shifting perspective in order to allow for its computational formalization (Ebert et al., 2015; Long et al., 2019). While theories based on English have provided a relatively comprehensive map of irony, there is still a lack of literature dealing with non-Indo-European languages. Even though irony is a pervasive linguistic phenomenon, some of its features vary in different cultures and in structural properties of a specific language (Xing and Xu, 2015). For example, Karoui and colleagues (2019) suggested capitalized words as a strong hint of irony in English. Yet, this hint does not work for Chinese as there is no capitalization or other obvious lexical variations in surface forms in C"
2020.lrec-1.701,W16-0425,0,0.0299825,"punctuation (Carvalho et al., 2009; Buschmeier et al., 2014), emoticon (Buschmeier et al., 2014), emotional scenarios (Reyes and Rosso, 2014), as well as reversals (Li et al., 2019). Meanwhile, Support Vector Machine (SVM) and Logistic Regression (LR) remain the most popular models in classical machine learning approaches for irony detection (Ghosh et al., 2015; Li et al., 2019). Some recent works also tried to tackle irony identification using deep learning methods. Deep learning methods no longer need feature engineering and have shown superior abilities to complex word composition in text (Ghosh and Veale, 2016; Huang et al., 2017). A system based on Long Short-Term Memory (LSTM) model with a multi-task learning strategy recently performed very well in the irony task (Wu et al., 2018). A large number of studies on irony detection have been conducted for English text. But, attempts on Chinese irony detection are still quite limited (Van Hee et al., 2018). Tang and Chen used a number of linguistic patterns to extract posts from Yahoo Blog as potential irony instances and then manually checked to identify irony instances for Traditional Chinese in Taiwan (2014). The linguistic patterns used in this wor"
2020.lrec-1.701,S15-2080,0,0.0314016,"to detect irony (Maynard and Greenwood, 2014; Khattri et al., 2015). In addition to lexical features, some other types of features can contribute to detecting irony, depending on the text genre. These features included but not limited to sentiment words, punctuation (Carvalho et al., 2009; Buschmeier et al., 2014), emoticon (Buschmeier et al., 2014), emotional scenarios (Reyes and Rosso, 2014), as well as reversals (Li et al., 2019). Meanwhile, Support Vector Machine (SVM) and Logistic Regression (LR) remain the most popular models in classical machine learning approaches for irony detection (Ghosh et al., 2015; Li et al., 2019). Some recent works also tried to tackle irony identification using deep learning methods. Deep learning methods no longer need feature engineering and have shown superior abilities to complex word composition in text (Ghosh and Veale, 2016; Huang et al., 2017). A system based on Long Short-Term Memory (LSTM) model with a multi-task learning strategy recently performed very well in the irony task (Wu et al., 2018). A large number of studies on irony detection have been conducted for English text. But, attempts on Chinese irony detection are still quite limited (Van Hee et al."
2020.lrec-1.701,W15-2905,0,0.0147946,"he contextual meaning of the words poses a serious challenge to text mining tasks such as affective analysis (Reyes and Rosso, 2014). In recent years, irony detection in NLP has become one of the most arduous and attractive research topics. There are two main approaches in irony detection: rule-based approaches and machine learning approaches (Joshi et al., 2017). A rule-based approach in irony identification mainly relies on lexicons and syntactic patterns, while a machine learning approach combines different types of features and knowledge bases to detect irony (Maynard and Greenwood, 2014; Khattri et al., 2015). In addition to lexical features, some other types of features can contribute to detecting irony, depending on the text genre. These features included but not limited to sentiment words, punctuation (Carvalho et al., 2009; Buschmeier et al., 2014), emoticon (Buschmeier et al., 2014), emotional scenarios (Reyes and Rosso, 2014), as well as reversals (Li et al., 2019). Meanwhile, Support Vector Machine (SVM) and Logistic Regression (LR) remain the most popular models in classical machine learning approaches for irony detection (Ghosh et al., 2015; Li et al., 2019). Some recent works also tried"
2020.lrec-1.701,D14-1181,0,0.00965356,"Missing"
2020.lrec-1.701,maynard-greenwood-2014-cares,0,0.0266114,"eversal from the literal to the contextual meaning of the words poses a serious challenge to text mining tasks such as affective analysis (Reyes and Rosso, 2014). In recent years, irony detection in NLP has become one of the most arduous and attractive research topics. There are two main approaches in irony detection: rule-based approaches and machine learning approaches (Joshi et al., 2017). A rule-based approach in irony identification mainly relies on lexicons and syntactic patterns, while a machine learning approach combines different types of features and knowledge bases to detect irony (Maynard and Greenwood, 2014; Khattri et al., 2015). In addition to lexical features, some other types of features can contribute to detecting irony, depending on the text genre. These features included but not limited to sentiment words, punctuation (Carvalho et al., 2009; Buschmeier et al., 2014), emoticon (Buschmeier et al., 2014), emotional scenarios (Reyes and Rosso, 2014), as well as reversals (Li et al., 2019). Meanwhile, Support Vector Machine (SVM) and Logistic Regression (LR) remain the most popular models in classical machine learning approaches for irony detection (Ghosh et al., 2015; Li et al., 2019). Some r"
2020.lrec-1.701,D14-1162,0,0.0980977,"methods and four deep learning methods on Ciron. Traditional methods include: Naive Bayes (NB), Logistic Regression (LR), and Support Vector Machine (SVM). Deep learning methods include: Convolutional Neural Network (CNN) (LeCun et al., 1998), Long Short-term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), Bidirectional Long Short-term Memory network with attention mechanism (BiLSTM-AT) (Zhang et al., 2018) and the context aware Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). For the first three deep learning methods, pre-trained GloVe vectors (Pennington et al., 2014) are used as word embedding features. BERT, with the Chinese pre-trained model, is fine-tuned for Ciron. All models are tuned with the datasets. Detailed settings for the models are provided in Table 4. Accuracy and weighted F1 score are adopted as the metrics of performance. 4.2. Analysis of Result Empirical results are listed in Table 5. In general, traditional methods are outperformed by deep learning methods. NB is the the worst performer as expected. The performance of LR is worse than SVM with a narrow margin. SVM results in a competitive accuracy compared to deep learning methods. Among"
2020.lrec-1.701,C14-1120,0,0.0583209,"no capitalization or other obvious lexical variations in surface forms in Chinese text. Studies on irony detection should take into account the specific ways in which irony is expressed in a given culture and a given language. Otherwise, the capacity of automatic systems in modeling the notion of ”context” will always be limited (Van Hee, 2017). Chinese irony detection in social networks is more challenging because the language of social networks is mostly composed of short statements (Li and Huang, 2019). Few works to date have tried to investigate Chinese irony detection in social networks (Tang and Chen, 2014). However, existing resources are limited to linguistic studies of Chinese irony detection to describe certain lexical patterns as well as syntactic patterns which cannot be readily formulated for machine learning algorithms. The lack of training data for Chinese irony is still a bottleneck for developing computationally-intensive, broad-coverage Chinese irony detection models. To solve this problem, this work aims to first explore the characteristic features of irony of the Chinese language and also to provide a benchmark dataset that can be used for automatic irony detection. In this paper,"
2020.lrec-1.701,P15-1098,0,0.0119301,"e first Chinese resource for Chinese irony detection with such a volume of data and fine-grained annotation. In contrast to many NLP datasets that are crowdsourced, instances in Ciron are collected from Chinese 5714 microblogs in a grounded and more natural context. The annotation process ensures consistency and quality. The dataset is applied to several popular machine learning-based methods to demonstrate its effectiveness including Naive Bayes (NB), logistic regression (LR), support vector machine (SVM), convolutional neural networks(CNN) (Kim, 2014), long short-term memory networks(LSTM) (Tang et al., 2015), bidirectional LSTM with attention mechanism (BiLSTM-AT) (Zhang et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). The rest of this paper is organized as follows. Section 2 introduces related works on irony theories and illustrates typical examples of ironic sentences in Chinese. Section 3 provides an in-depth description of the data that we extracted from Weibo forums, together with the formalization of the irony features. The performance of the baseline models is evaluated in Section 4, highlighting its validity. Section 5 concludes this"
2020.lrec-1.701,S18-1005,0,0.0376319,"Missing"
2020.lrec-1.701,S18-1040,0,0.121314,"ta and fine-grained annotation. In contrast to many NLP datasets that are crowdsourced, instances in Ciron are collected from Chinese 5714 microblogs in a grounded and more natural context. The annotation process ensures consistency and quality. The dataset is applied to several popular machine learning-based methods to demonstrate its effectiveness including Naive Bayes (NB), logistic regression (LR), support vector machine (SVM), convolutional neural networks(CNN) (Kim, 2014), long short-term memory networks(LSTM) (Tang et al., 2015), bidirectional LSTM with attention mechanism (BiLSTM-AT) (Zhang et al., 2018) and Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). The rest of this paper is organized as follows. Section 2 introduces related works on irony theories and illustrates typical examples of ironic sentences in Chinese. Section 3 provides an in-depth description of the data that we extracted from Weibo forums, together with the formalization of the irony features. The performance of the baseline models is evaluated in Section 4, highlighting its validity. Section 5 concludes this paper. 2. Related Works Based on several linguistic studies (Huang et al., 201"
2020.paclic-1.12,Y96-1018,1,0.219074,"rise/organization/experimental unit/improvement” were focused by Government in 1997-2001. 3 The change of relationship between clause length and word length The sentence, as the maximal grammatical unit and minimal statement unit, is considered to be a basic linguistic unit in all languages. Chinese sentences are often defined in terms of characteristics of speech (Huang and Shi 2016; Lu 1993). Chao (1968) and Zhu (1982) defined a sentence as an utterance with pauses and intonation changes at its boundaries. A common approach for identifying sentences in syntactically annotated corpora (e.g., Chen et al., 1996; Chen et al., 2003; Huang and Chen, 2017 for Sinica TreeBank) is to mark all segments between punctuation marks that indicate utterance pauses as sentences. Such punctuation marks include commas, semicolons, colon, periods, exclamation marks, and question marks. Wang and Qin (2014) and Chen (1994) also adopted this operational definition and called such units sentence segments. Wang and Qin (2014) considered the lengths of sentence segments to be relevant to language use in Chinese. Sentences (as defined by Chen et al., 2003; Huang and Chen, 2017) and sentence segments (as defined by Chen, 19"
2020.paclic-1.12,W18-4503,0,0.0414161,") throughout their history. Previous studies about language change focused on sound and sound changes, word and word changes mostly. Lieberman et al. (2007) studied the regularization of English verbs over the last 1200 years and how the rate of regularization depends on the frequency of word usage. Lexicostatistics was used to calculate the evolutionary history of a set of related languages and varieties (Bakker et al. 2009, Barbancon et al. 2013). Baker (2011) focused on words that have changed their frequency and meaning in the study of change in British English over the twentieth century. Degaetano-Ortlieb and Teich (2018) have used relative entropy for detection and analysis of periods of diachronic linguistic change. Campos et al. (2020) set a corpus-driven methodology to quantify automatically diachronic language distance between chronological periods of several languages. The results showed that a diachronic language distance based on perplexity detects the linguistic evolution that had already been explained by the historians of the three languages. There is a long tradition of linguistic research on political discourse. Van Dijk’s (1997) definition of political discourse brings together studies focusing o"
2020.paclic-1.19,P17-4015,0,0.0173741,"2009, Honnibal and Montani 2017). 3.1 Lexical features We select a number ki of speaker’s of each label from conversations i and build a language model with the n-gram frequencies for all turns per category. We then examine the characteristic differences in the use of lexical items using Scaled Fscore. Scaled F-score is a modified metric based on the F-score, the harmonic mean of precision and recall. It addresses issues related to harmonic means dominated by precision, as well as a better representation of low-frequency terms.1 We plot gender and age categories using the Scattertext library (Kessler 2017) to visualize the crosscategorical differences at the n-gram level. Figure 1 and 2 show words and phrases that are more characteristic of each category, while also reporting their frequencies and a list of top characteristic items. We observe that a range of terms reflect (stereo)typicality of gender and age categories in our corpus. For instance, top characteristic terms of male speakers feature the nouns “mate”, “game”, “cards”, “quid” and “football”, while female speaker’s talk more prominently features “baby”, “weekend”, “hair”, “birthday” and “cake” (see Figure 1). More interesting for us"
2020.paclic-1.19,W16-0204,0,0.0455149,"Missing"
2020.paclic-1.36,W03-1405,1,0.507977,"n general, metaphor involves certain concept transfer from one domain (Source) to another (Target), as in ‘sweet voice’ (using taste to describe sound). Lakoff (1980) describes metaphor as a cognitive mechanism (a property of language) reflected by our conceptual system for structuring our understanding of the world. It is a fundamental way to relate our physical and familiar social experiences to a multitude of other subjects and contexts (Lakoff and Johnson, 2008). As a popular linguistic device, metaphors encode versatile ontological information, which usually involve e.g. domain transfer (Ahrens et al., 2003; Ahrens and Jiang, 2020), sentiment reverse (Steen et al., 2010) or modality shift (Winter, 2019) etc. Therefore, detecting the metaphors in texts is essential for capturing the authentic meaning of the texts, which can benefit many natural language processing applications, such as machine translation, dialogue systems and sentiment analysis (Tsvetkov et al., 2014). To better understand the intrinsic properties of metaphors and to provide an in-depth analysis to this phenomenon, we propose a linguisticallyenriched deep learning model extending one published work (WAN et al., 2020) at ACL Figl"
2020.paclic-1.36,2020.figlang-1.28,0,0.0294903,"s, concreteness, word associations, constructions and frames etc. (Hong, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Klebanov et al., 2014; Wilks et al., 2013; Bizzoni and Ghanimifard, 2018; Klebanov et al., 2015). Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016). Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017). To name a few advances, Brooks and Youssef (2020) build up an ensemble of RNN models with Bi-LSTMs and bidirectional attention mechanisms. Chen et al. (2020) employs BERT to obtain the sentence embeddings, and then a linear layer is applied with softmax on each token to make predictions. Maudslay"
2020.paclic-1.36,D18-1060,0,0.0165757,"r detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016). Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017). To name a few advances, Brooks and Youssef (2020) build up an ensemble of RNN models with Bi-LSTMs and bidirectional attention mechanisms. Chen et al. (2020) employs BERT to obtain the sentence embeddings, and then a linear layer is applied with softmax on each token to make predictions. Maudslay et al.(2020) combines the concreteness of a word with its static and contextual embeddings as inputs into a deep Multi-layer Perceptron network for predicting metaphoricity. Gong et al.(2020) used RoBERTa to obtain word embeddings and conca"
2020.paclic-1.36,2020.figlang-1.21,0,0.0540406,"Missing"
2020.paclic-1.36,P16-1018,0,0.0206534,"ng, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Klebanov et al., 2014; Wilks et al., 2013; Bizzoni and Ghanimifard, 2018; Klebanov et al., 2015). Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016). Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017). To name a few advances, Brooks and Youssef (2020) build up an ensemble of RNN models with Bi-LSTMs and bidirectional attention mechanisms. Chen et al. (2020) employs BERT to obtain the sentence embeddings, and then a linear layer is applied with softmax on each token to make predictions. Maudslay et al.(2020) combines the concreteness of a word with its static and"
2020.paclic-1.36,D17-1316,0,0.0215873,"pervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016). Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017). To name a few advances, Brooks and Youssef (2020) build up an ensemble of RNN models with Bi-LSTMs and bidirectional attention mechanisms. Chen et al. (2020) employs BERT to obtain the sentence embeddings, and then a linear layer is applied with softmax on each token to make predictions. Maudslay et al.(2020) combines the concreteness of a word with its static and contextual embeddings as inputs into a deep Multi-layer Perceptron network for predicting metaphoricity. Gong et al.(2020) used RoBERTa to obtain word embeddings and concatenate it with linguistic features (e.g. WordNet, VerbNet) a"
2020.paclic-1.36,W14-2302,0,0.0303564,"; Kathpalia and Carmel, 2011; Klebanov et al., 2008; Semino, 2008; Billow et al., 1997; Bosman, 1987). Over the last decade, automated detection of metaphor has gained increasing research interest among the Natural Language Processing community. Many approaches have been proposed with systems such as traditional machine learning classifiers, deep neural networks and sequential models etc., trained on features of word vectors, n-grams, lexical information, semantic classes, concreteness, word associations, constructions and frames etc. (Hong, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Klebanov et al., 2014; Wilks et al., 2013; Bizzoni and Ghanimifard, 2018; Klebanov et al., 2015). Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016). Recently, deep learning methods have been explored a"
2020.paclic-1.36,W15-1402,0,0.0220215,"et al., 1997; Bosman, 1987). Over the last decade, automated detection of metaphor has gained increasing research interest among the Natural Language Processing community. Many approaches have been proposed with systems such as traditional machine learning classifiers, deep neural networks and sequential models etc., trained on features of word vectors, n-grams, lexical information, semantic classes, concreteness, word associations, constructions and frames etc. (Hong, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Klebanov et al., 2014; Wilks et al., 2013; Bizzoni and Ghanimifard, 2018; Klebanov et al., 2015). Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016). Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 20"
2020.paclic-1.36,P16-2017,0,0.0141913,"sociations, constructions and frames etc. (Hong, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Klebanov et al., 2014; Wilks et al., 2013; Bizzoni and Ghanimifard, 2018; Klebanov et al., 2015). Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016). Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017). To name a few advances, Brooks and Youssef (2020) build up an ensemble of RNN models with Bi-LSTMs and bidirectional attention mechanisms. Chen et al. (2020) employs BERT to obtain the sentence embeddings, and then a linear layer is applied with softmax on each token to make predictions. Maudslay et al.(2020) combines"
2020.paclic-1.36,N18-2014,0,0.0124701,"better understand the intrinsic properties of metaphors and to provide an in-depth analysis to this phenomenon, we propose a linguisticallyenriched deep learning model extending one published work (WAN et al., 2020) at ACL Figlang 2020 workshop by incorporating the modality norms into attention-based BiLSTM. As a continuation of their work, we conduct the current research to further testify the effectiveness of leveraging conceptual norms for metaphor detection. For standard reference, we adopt the dataset of the first and second shared tasks of metaphor detection on verbs of the VUA corpus (Klebanov et al., 2018)1 . Details about the experiment are given in Sections 3-5. 2 Related Work Research on metaphors have been mainly explored in the context of political communication, mental health, teaching, discourse analysis, assessment 1 http://www.vismet.org/metcor/ documentation/home.html of English proficiency, among others (Ahrens and Jiang, 2020; Thibodeau and Boroditsky, 2011; Kathpalia and Carmel, 2011; Klebanov et al., 2008; Semino, 2008; Billow et al., 1997; Bosman, 1987). Over the last decade, automated detection of metaphor has gained increasing research interest among the Natural Language Proces"
2020.paclic-1.36,2020.figlang-1.18,0,0.0469522,"Missing"
2020.paclic-1.36,2020.figlang-1.26,0,0.047693,"Missing"
2020.paclic-1.36,W18-0907,0,0.0211023,"to one architecture with the modality enriched neural networks, as illustrated in Section 4. 3 Data Description 3.1 The VUA Corpus The VU Amsterdam Metaphor Corpus 2 (VUA) (Tekiro˘glu et al., 2015) is used in the experiment for training and testing. The dataset consists of 117 fragments sampled across four genres from the British National Corpus: Academic, News, Conversation, and Fiction. The data is annotated using the MIPVU procedure (Steen, 2010) with a strong inter-annotator agreement (k>0.8). This dataset has been used as the competition corpus for two shared tasks on metaphor detection (Leong et al., 2018; Leong et al., 2020), which is publicly available for standard reference. Information about the size of the sub-genres is given in Table 1. The training and testing texts, sentences, tokens and percentage of metaphors breakdown of the VUA verb track3 is given in Table 2. Text Genres Academic texts Conversation texts Fiction texts News texts TOTAL No. of Tokens 49,561 tokens 48,001 tokens 44,892 tokens 45,116 tokens 187,570 tokens No. of Fragments 16 fragments 24 fragments 12 fragments 63 fragments 115 fragments Table 1: Data components of the VUA corpus 2 http://www.vismet.org/metcor/document"
2020.paclic-1.36,2020.figlang-1.3,0,0.092718,"with the modality enriched neural networks, as illustrated in Section 4. 3 Data Description 3.1 The VUA Corpus The VU Amsterdam Metaphor Corpus 2 (VUA) (Tekiro˘glu et al., 2015) is used in the experiment for training and testing. The dataset consists of 117 fragments sampled across four genres from the British National Corpus: Academic, News, Conversation, and Fiction. The data is annotated using the MIPVU procedure (Steen, 2010) with a strong inter-annotator agreement (k>0.8). This dataset has been used as the competition corpus for two shared tasks on metaphor detection (Leong et al., 2018; Leong et al., 2020), which is publicly available for standard reference. Information about the size of the sub-genres is given in Table 1. The training and testing texts, sentences, tokens and percentage of metaphors breakdown of the VUA verb track3 is given in Table 2. Text Genres Academic texts Conversation texts Fiction texts News texts TOTAL No. of Tokens 49,561 tokens 48,001 tokens 44,892 tokens 45,116 tokens 187,570 tokens No. of Fragments 16 fragments 24 fragments 12 fragments 63 fragments 115 fragments Table 1: Data components of the VUA corpus 2 http://www.vismet.org/metcor/documentation/home.html The p"
2020.paclic-1.36,2020.figlang-1.17,0,0.0323426,"Missing"
2020.paclic-1.36,2020.figlang-1.34,0,0.0438549,"Missing"
2020.paclic-1.36,P19-1378,0,0.011507,"et al., 2015). Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016). Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017). To name a few advances, Brooks and Youssef (2020) build up an ensemble of RNN models with Bi-LSTMs and bidirectional attention mechanisms. Chen et al. (2020) employs BERT to obtain the sentence embeddings, and then a linear layer is applied with softmax on each token to make predictions. Maudslay et al.(2020) combines the concreteness of a word with its static and contextual embeddings as inputs into a deep Multi-layer Perceptron network for predicting metaphoricity. Gong et al.(2020) used RoB"
2020.paclic-1.36,2020.figlang-1.30,0,0.0352506,"Missing"
2020.paclic-1.36,W16-1103,0,0.0176827,"nd Jiang, 2020; Thibodeau and Boroditsky, 2011; Kathpalia and Carmel, 2011; Klebanov et al., 2008; Semino, 2008; Billow et al., 1997; Bosman, 1987). Over the last decade, automated detection of metaphor has gained increasing research interest among the Natural Language Processing community. Many approaches have been proposed with systems such as traditional machine learning classifiers, deep neural networks and sequential models etc., trained on features of word vectors, n-grams, lexical information, semantic classes, concreteness, word associations, constructions and frames etc. (Hong, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Klebanov et al., 2014; Wilks et al., 2013; Bizzoni and Ghanimifard, 2018; Klebanov et al., 2015). Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016). R"
2020.paclic-1.36,D17-1162,0,0.0136985,"ngineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016). Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017). To name a few advances, Brooks and Youssef (2020) build up an ensemble of RNN models with Bi-LSTMs and bidirectional attention mechanisms. Chen et al. (2020) employs BERT to obtain the sentence embeddings, and then a linear layer is applied with softmax on each token to make predictions. Maudslay et al.(2020) combines the concreteness of a word with its static and contextual embeddings as inputs into a deep Multi-layer Perceptron network for predicting metaphoricity. Gong et al.(2020) used RoBERTa to obtain word embeddings and concatenate it with linguistic features"
2020.paclic-1.36,N16-1020,0,0.0151695,"ns and frames etc. (Hong, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Klebanov et al., 2014; Wilks et al., 2013; Bizzoni and Ghanimifard, 2018; Klebanov et al., 2015). Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016). Recently, deep learning methods have been explored and become the main stream technology for metaphor detection (Mao et al., 2019; Dankers et al., 2019; Gao et al., 2018; Wu et al., 2018; Rei et al., 2017; Gutierrez et al., 2017). To name a few advances, Brooks and Youssef (2020) build up an ensemble of RNN models with Bi-LSTMs and bidirectional attention mechanisms. Chen et al. (2020) employs BERT to obtain the sentence embeddings, and then a linear layer is applied with softmax on each token to make predictions. Maudslay et al.(2020) combines the concreteness of a"
2020.paclic-1.36,2020.figlang-1.4,0,0.0360485,"Missing"
2020.paclic-1.36,W15-1404,0,0.056395,"Missing"
2020.paclic-1.36,P14-1024,0,0.0316081,"and familiar social experiences to a multitude of other subjects and contexts (Lakoff and Johnson, 2008). As a popular linguistic device, metaphors encode versatile ontological information, which usually involve e.g. domain transfer (Ahrens et al., 2003; Ahrens and Jiang, 2020), sentiment reverse (Steen et al., 2010) or modality shift (Winter, 2019) etc. Therefore, detecting the metaphors in texts is essential for capturing the authentic meaning of the texts, which can benefit many natural language processing applications, such as machine translation, dialogue systems and sentiment analysis (Tsvetkov et al., 2014). To better understand the intrinsic properties of metaphors and to provide an in-depth analysis to this phenomenon, we propose a linguisticallyenriched deep learning model extending one published work (WAN et al., 2020) at ACL Figlang 2020 workshop by incorporating the modality norms into attention-based BiLSTM. As a continuation of their work, we conduct the current research to further testify the effectiveness of leveraging conceptual norms for metaphor detection. For standard reference, we adopt the dataset of the first and second shared tasks of metaphor detection on verbs of the VUA corp"
2020.paclic-1.36,2020.figlang-1.16,1,0.718588,"transfer (Ahrens et al., 2003; Ahrens and Jiang, 2020), sentiment reverse (Steen et al., 2010) or modality shift (Winter, 2019) etc. Therefore, detecting the metaphors in texts is essential for capturing the authentic meaning of the texts, which can benefit many natural language processing applications, such as machine translation, dialogue systems and sentiment analysis (Tsvetkov et al., 2014). To better understand the intrinsic properties of metaphors and to provide an in-depth analysis to this phenomenon, we propose a linguisticallyenriched deep learning model extending one published work (WAN et al., 2020) at ACL Figlang 2020 workshop by incorporating the modality norms into attention-based BiLSTM. As a continuation of their work, we conduct the current research to further testify the effectiveness of leveraging conceptual norms for metaphor detection. For standard reference, we adopt the dataset of the first and second shared tasks of metaphor detection on verbs of the VUA corpus (Klebanov et al., 2018)1 . Details about the experiment are given in Sections 3-5. 2 Related Work Research on metaphors have been mainly explored in the context of political communication, mental health, teaching, dis"
2020.paclic-1.36,W13-0905,0,0.0318041,"2011; Klebanov et al., 2008; Semino, 2008; Billow et al., 1997; Bosman, 1987). Over the last decade, automated detection of metaphor has gained increasing research interest among the Natural Language Processing community. Many approaches have been proposed with systems such as traditional machine learning classifiers, deep neural networks and sequential models etc., trained on features of word vectors, n-grams, lexical information, semantic classes, concreteness, word associations, constructions and frames etc. (Hong, 2016; Rai et al., 2016; Do Dinh and Gurevych, 2016; Klebanov et al., 2014; Wilks et al., 2013; Bizzoni and Ghanimifard, 2018; Klebanov et al., 2015). Early studies of metaphor detection tend to adopt feature-engineering in a supervised machine learning paradigm, which construct feature vectors based on concreteness and imageability, semantic classification using WordNet, FrameNet, VerbNet, SUMO ontology, property norms and distributional semantic models, syntactic dependency patterns, sensorial and vision-based features (Alnafesah et al., 2020; Klebanov et al., 2016; Shutova et al., 2016; Gutierrez et al., 2016). Recently, deep learning methods have been explored and become the main s"
2020.paclic-1.37,Y04-1030,0,0.0671442,"ale dataset of wine reviews to perform regression predictions on the grade and price of wines. In linguistic studies, wine reviews provide sensory descriptions for the research on language and cognition. Thus, comparative studies based on bilingual wine reviews, such as Chinese and English, underline the issue of how sensory cognition is encoded across different languages. Another possible research application is to build English-Chinese parallel corpus based on wine reviews for domain-specific machine translation or translation studies. Such corpora should follow established guidelines (e.g. Chang, 2004) in order to be sharable. Such a corpus is crucial as terms loaded with rich cultural tradition tend to be considered ‘untranslatable’. In computational term banks, it often leads to oneto-many (cf. Lim, 2018, 2019), many-to-one, or many-to-many mappings, although the mapping relations do not seem to have attracted in-depth research in computational terminology. In this paper, we propose a parallel corpus-driven approach to culturally bound bilingual terms discovery. In particular, we look at EnglishChinese bilingual wine-tasting terminology. Since modern table wine culture and technology are"
2020.paclic-1.38,O07-2003,1,0.59927,"Missing"
2020.paclic-1.52,Y96-1018,1,0.673558,"Missing"
2020.paclic-1.52,W10-2102,1,0.712324,"gies adopted to test the differences between the two epistemic markers “renwei” and “yiwei”. Evidentiality: As a sub-category of epistemic modality, the essence of evidentiality is the way that speakers inherently encode the information sources of a clause (Chafe, 1986). There was no agreement upon the category of evidentiality, as several scholars regarded evidentiality as a grammatical structure (e.g. Willett, 1988; Palmer, 2001), while some others expanded into any evidential expressions referring to speakers’ judgments by linguistic coding of the validity of information (e.g. Chafe, 1986; Su et al., 2010). Following the latter broad sense, studies on evidentiality dealt with the epistemological status of information or evidence at addressers’ disposal (De Haan, 2005; Nuyts, 2006). In speech communication, speakers often use evidential markers to emphasize the explicit source of information, such as “It’s said…”, “People say…”, and “according to…”, in order to improve the credibility of information to a certain degree. According to Rubin et al. (2006), the lexical verb “think” together with “seem&quot; and “sound” were grouped into the moderate degree of certainty as evidential markers. However, we"
2020.paclic-1.65,W13-2322,0,0.0380154,"Missing"
2020.paclic-1.65,P14-1134,0,0.0268151,"encoding the same semantic information as their regular verb counterparts. Since the meaning of LVCs differ from usual predicative structures or the direct aggregation of its semantic components, LVCs, as one of the least explored areas of MWEs in computational linguistics, pose a number of challenges in computational grammar, such as automatic word alignment, annotation and semantic representation. In the model of Abstract Meaning Representation (AMR), it is often assumed that the LVCs and its corresponding regular verb construction (RVC) share the same representation (Banarescu et al. 2012; Flanigan et al. 2014, Bu et al. 2016). AMR is a semantic framework addressing the predicate-argument relation of the whole sentence (to be more fully described in Section 4). However, corpus data suggest that LVCs and RVCs have slightly different semantic meaning. In Urdu, for example, LVs play a central role in the meaning and morphosyntactic choices of the whole construction in (1). Although the LVs par ‘fall’ and ḍaal ‘put’ both occur with ciikh ‘scream’ in Urdu, the LV in (1a), which involves an involuntary action, is preceded by an unmarked nominative subject, whereas the LV ḍaal ‘put’ in (1b), denoting a co"
2020.starsem-1.4,E14-1049,0,0.0431734,"ingual embeddings, vector space models that represent words from multiple languages through some form of mapping from a monolingual to a multilingual space (Conneau et al., 2018; Ruder et al., 2019). A classical study by Mikolov et al. (2013b) learnt a linear projection to transform the space of a source language to the space of a target language by maximizing the similarity between the two spaces. Other approaches apply Canonical Correlation Analysis to simultaneously project words from two languages into a shared embedding space where the correlation between projected vectors are maximized (Faruqui and Dyer, 2014). Other works make use of the max-margin method such that, for embeddings projected from a source language, they maximize the margin between the correct translations and other candidates (Lazaridou et al., 2015; Joulin et al., 2018). For this study, we use the offthe-shelf crosslingual embeddings by Joulin et al. (2018) based on FastText (Bojanowski et al., 2017) 3 Our Proposed Approach In this work, we used regressors trained on crosslingual word embeddings to predict modality ratings in two different scenarios. In the monolingual scenario, we adopt a 5-fold cross-validation to predict the mo"
2020.starsem-1.4,D18-1330,0,0.0986365,"(2013b) learnt a linear projection to transform the space of a source language to the space of a target language by maximizing the similarity between the two spaces. Other approaches apply Canonical Correlation Analysis to simultaneously project words from two languages into a shared embedding space where the correlation between projected vectors are maximized (Faruqui and Dyer, 2014). Other works make use of the max-margin method such that, for embeddings projected from a source language, they maximize the margin between the correct translations and other candidates (Lazaridou et al., 2015; Joulin et al., 2018). For this study, we use the offthe-shelf crosslingual embeddings by Joulin et al. (2018) based on FastText (Bojanowski et al., 2017) 3 Our Proposed Approach In this work, we used regressors trained on crosslingual word embeddings to predict modality ratings in two different scenarios. In the monolingual scenario, we adopt a 5-fold cross-validation to predict the modality norms of an English dataset (Lynott and Connell, 2009, 2013). In the crosslingual scenario, a regressor is trained on a high-resource language, e.g. English, to predict the modality norms of an unseen language. 3.1 Datasets F"
2020.starsem-1.4,Q17-1010,0,0.279266,"nd Chinese norms. Results show that a) crosslingual embeddings perform similarly to or slightly better than monolingual embeddings in a monolingual setting; b) even after training only on English data, the regressor can predict norms in a totally unseen language with moderate-to-high correlations with human judgements. 2 Related Work Although word vectors have been a standard for word representations for almost two decades (Lenci, 2018), they became an essential ingredient for most NLP applications only after the introduction of word embeddings (Mikolov et al., 2013a; Pennington et al., 2014; Bojanowski et al., 2017). Differently from the first generation models using co-occurrence counting and weighting, word embeddings are estimated via neural network training with the objective of maximizing the probability of the contexts of a target word, and they gained popularity due to the availability of efficient and easy-touse tools (Mikolov et al., 2013a). The development of research on crosslingual transfer and the availability of new benchmarks for multilingual NLP has recently led to the introduction of the so-called crosslingual embeddings, vector space models that represent words from multiple languages t"
2020.starsem-1.4,P15-1027,0,0.030858,"study by Mikolov et al. (2013b) learnt a linear projection to transform the space of a source language to the space of a target language by maximizing the similarity between the two spaces. Other approaches apply Canonical Correlation Analysis to simultaneously project words from two languages into a shared embedding space where the correlation between projected vectors are maximized (Faruqui and Dyer, 2014). Other works make use of the max-margin method such that, for embeddings projected from a source language, they maximize the margin between the correct translations and other candidates (Lazaridou et al., 2015; Joulin et al., 2018). For this study, we use the offthe-shelf crosslingual embeddings by Joulin et al. (2018) based on FastText (Bojanowski et al., 2017) 3 Our Proposed Approach In this work, we used regressors trained on crosslingual word embeddings to predict modality ratings in two different scenarios. In the monolingual scenario, we adopt a 5-fold cross-validation to predict the modality norms of an English dataset (Lynott and Connell, 2009, 2013). In the crosslingual scenario, a regressor is trained on a high-resource language, e.g. English, to predict the modality norms of an unseen la"
2020.starsem-1.4,W17-2810,0,0.016163,"thus This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 32 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 32–38 Barcelona, Spain (Online), December 12–13, 2020 and trained on Wikipedia. 1 Despite the success of word embeddings, a common criticism is that they are not grounded in perception, as words are only defined in relation to each other and not to entities and actions in the physical world (Glenberg and Robertson, 2000; Fagarasan et al., 2015; Li and Gauthier, 2017). To address this issue, Fagarasan et al. (2015) used a regression method to map embeddings onto the conceptual properties of the McRae norms (McRae et al., 2005). A similar approach, using feedforward neural networks for predicting properties, was recently described by Li and Summers-Stay (2019). The work by Derby et al. (2019) goes in the opposite direction: instead of predicting norms from embeddings, they combined pretrained vectors and property vectors to inject conceptual knowledge into a new type of word representations. Their Feature2Vec system showed a strong performance in the predic"
2020.starsem-1.4,D19-1595,0,0.0105735,"1 Despite the success of word embeddings, a common criticism is that they are not grounded in perception, as words are only defined in relation to each other and not to entities and actions in the physical world (Glenberg and Robertson, 2000; Fagarasan et al., 2015; Li and Gauthier, 2017). To address this issue, Fagarasan et al. (2015) used a regression method to map embeddings onto the conceptual properties of the McRae norms (McRae et al., 2005). A similar approach, using feedforward neural networks for predicting properties, was recently described by Li and Summers-Stay (2019). The work by Derby et al. (2019) goes in the opposite direction: instead of predicting norms from embeddings, they combined pretrained vectors and property vectors to inject conceptual knowledge into a new type of word representations. Their Feature2Vec system showed a strong performance in the predicting norms of unseen words, compared to previous proposals. Finally, Utsumi (2018, 2020) proposed a similar mapping technique to exploit semantic feature norms by Binder et al. (2016) to analyze the semantic content of word embeddings in terms of neurobiologically-motivated features. Turton et al. (2020) also experimented with e"
2020.starsem-1.4,N19-1423,0,0.0107744,"the research on modality norms. In this first study we used a relatively simple methodology, but several refinements are possible for improving the prediction quality. Two possible directions would be, firstly, to exploit the presence of words that are direct translations from English to the other languages to apply retrofitting techniques (Faruqui et al., 2015; Mrkši´c et al., 2016, 2017; Vuli´c et al., 2018) to the crosslingual space, and secondly, to tackle the task by introducing more advanced neural architectures for the representation of words in context, e.g. multilingual transformers (Devlin et al., 2019; Pires et al., 2019). Table 5: Number and percentage of words that are translations of English items for each dataset. while multimodal concepts are typically associated with lower scores. Strongly multimodal concepts might be more difficult to predict, as their scores for the five modalities are generally closer than in the unimodal concepts. We tested this hypothesis by measuring the Spearman correlation between the word correlations and the modality exclusivity scores from the original dataset, but no strong evidence was found: the models showed no significant correlation for the Italian d"
2020.starsem-1.4,W15-0107,0,0.0288419,"red feature space. It is thus This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 32 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 32–38 Barcelona, Spain (Online), December 12–13, 2020 and trained on Wikipedia. 1 Despite the success of word embeddings, a common criticism is that they are not grounded in perception, as words are only defined in relation to each other and not to entities and actions in the physical world (Glenberg and Robertson, 2000; Fagarasan et al., 2015; Li and Gauthier, 2017). To address this issue, Fagarasan et al. (2015) used a regression method to map embeddings onto the conceptual properties of the McRae norms (McRae et al., 2005). A similar approach, using feedforward neural networks for predicting properties, was recently described by Li and Summers-Stay (2019). The work by Derby et al. (2019) goes in the opposite direction: instead of predicting norms from embeddings, they combined pretrained vectors and property vectors to inject conceptual knowledge into a new type of word representations. Their Feature2Vec system showed a strong p"
2020.starsem-1.4,N15-1184,0,0.0217214,"egressor on a highresource language (e.g. English) and to predict the norms for the low-resource one. In our experiments, we obtained moderate-to-high correlations even in the crosslingual setting. We think this is potentially a very useful application for the research on modality norms. In this first study we used a relatively simple methodology, but several refinements are possible for improving the prediction quality. Two possible directions would be, firstly, to exploit the presence of words that are direct translations from English to the other languages to apply retrofitting techniques (Faruqui et al., 2015; Mrkši´c et al., 2016, 2017; Vuli´c et al., 2018) to the crosslingual space, and secondly, to tackle the task by introducing more advanced neural architectures for the representation of words in context, e.g. multilingual transformers (Devlin et al., 2019; Pires et al., 2019). Table 5: Number and percentage of words that are translations of English items for each dataset. while multimodal concepts are typically associated with lower scores. Strongly multimodal concepts might be more difficult to predict, as their scores for the five modalities are generally closer than in the unimodal concept"
2020.starsem-1.4,S17-2008,0,0.0174212,"n this work, we used regressors trained on crosslingual word embeddings to predict modality ratings in two different scenarios. In the monolingual scenario, we adopt a 5-fold cross-validation to predict the modality norms of an English dataset (Lynott and Connell, 2009, 2013). In the crosslingual scenario, a regressor is trained on a high-resource language, e.g. English, to predict the modality norms of an unseen language. 3.1 Datasets Four modality norms datasets are used in this work: the English norms by Lynott and Connell (2009, 1 We ran experiments also with the Numberbatch embeddings by Speer and Lowry-Duda (2017), which are obtained by retrofitting different types of word embeddings with a subgraph of ConceptNet (Speer et al., 2017). However, while these vectors showed a strong performance in predicting the norms in the monolingual setting, they never achieved significant correlations with human judgements in the crosslingual prediction, and thus we omitted them from the Results section. 33 2013) (1002 words); the Italian norms by Vergallito et al. (2020) (1121 words); the Dutch norms by Speed and Majid (2017) (485 words); the Chinese norms by Chen et al. (2019) (291 words). The latter three datasets"
2020.starsem-1.4,2020.lincr-1.1,0,0.0276807,"-Stay (2019). The work by Derby et al. (2019) goes in the opposite direction: instead of predicting norms from embeddings, they combined pretrained vectors and property vectors to inject conceptual knowledge into a new type of word representations. Their Feature2Vec system showed a strong performance in the predicting norms of unseen words, compared to previous proposals. Finally, Utsumi (2018, 2020) proposed a similar mapping technique to exploit semantic feature norms by Binder et al. (2016) to analyze the semantic content of word embeddings in terms of neurobiologically-motivated features. Turton et al. (2020) also experimented with embeddings based on Binder features, showing that they can achieve performances comparable to Word2Vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) on similarity datasets. possible to train a regressor on a resource-rich language, e.g English, and predict modality ratings for words in an unseen language. We experimented with this crosslingual transfer method on Italian, Dutch and Chinese norms. Results show that a) crosslingual embeddings perform similarly to or slightly better than monolingual embeddings in a monolingual setting; b) even after training o"
2020.starsem-1.4,N16-1018,0,0.0540142,"Missing"
2020.starsem-1.4,Q17-1022,0,0.0292807,"Missing"
2020.starsem-1.4,N18-1048,0,0.0368776,"Missing"
2020.starsem-1.4,2020.figlang-1.16,1,0.5337,"rms with Crosslingual Word Embeddings Emmanuele Chersoni, Rong Xiang, Qin Lu, Chu-Ren Huang The Hong Kong Polytechnic University, 11 Yuk Choi Road, Hong Kong (China) {emmanuelechersoni, xiangrong0302}@gmail.com, csluqin@comp.polyu.edu.hk, churen.huang@polyu.edu.hk Abstract the perceptual strength of lexical items used in their experiments, motivating the publication of datasets in which words are rated according to their association with each of the five senses (see the example in Table 1). Moreover, such norms have also been shown to be useful for other NLP tasks, such as metaphor detection (Wan et al., 2020a,b). Normative studies on modality for English words are relatively common (Lynott and Connell, 2009; Juhasz et al., 2011; Lynott and Connell, 2013; Lynott et al., 2019), and similar norms have also been made available for other languages such as French (Bonin et al., 2015), Serbian (Ðurdevi´ ¯ c et al., 2016), Dutch (Speed and Majid, 2017), Russian (Miklashevsky, 2018), Chinese (Chen et al., 2019) and Italian (Vergallito et al., 2020). But in general, the number of languages for which they are available is still limited, and collecting modality norms is a time-consuming process, especially f"
2020.starsem-1.4,D14-1162,0,0.0833394,"ject conceptual knowledge into a new type of word representations. Their Feature2Vec system showed a strong performance in the predicting norms of unseen words, compared to previous proposals. Finally, Utsumi (2018, 2020) proposed a similar mapping technique to exploit semantic feature norms by Binder et al. (2016) to analyze the semantic content of word embeddings in terms of neurobiologically-motivated features. Turton et al. (2020) also experimented with embeddings based on Binder features, showing that they can achieve performances comparable to Word2Vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) on similarity datasets. possible to train a regressor on a resource-rich language, e.g English, and predict modality ratings for words in an unseen language. We experimented with this crosslingual transfer method on Italian, Dutch and Chinese norms. Results show that a) crosslingual embeddings perform similarly to or slightly better than monolingual embeddings in a monolingual setting; b) even after training only on English data, the regressor can predict norms in a totally unseen language with moderate-to-high correlations with human judgements. 2 Related Work Although word vectors have been"
2020.starsem-1.4,P19-1493,0,0.0223154,"ity norms. In this first study we used a relatively simple methodology, but several refinements are possible for improving the prediction quality. Two possible directions would be, firstly, to exploit the presence of words that are direct translations from English to the other languages to apply retrofitting techniques (Faruqui et al., 2015; Mrkši´c et al., 2016, 2017; Vuli´c et al., 2018) to the crosslingual space, and secondly, to tackle the task by introducing more advanced neural architectures for the representation of words in context, e.g. multilingual transformers (Devlin et al., 2019; Pires et al., 2019). Table 5: Number and percentage of words that are translations of English items for each dataset. while multimodal concepts are typically associated with lower scores. Strongly multimodal concepts might be more difficult to predict, as their scores for the five modalities are generally closer than in the unimodal concepts. We tested this hypothesis by measuring the Spearman correlation between the word correlations and the modality exclusivity scores from the original dataset, but no strong evidence was found: the models showed no significant correlation for the Italian data, while finding po"
2021.econlp-1.5,W19-1909,0,0.0370615,"Missing"
2021.econlp-1.5,W16-5313,1,0.750227,"his suggests that the models take advantage from exposure to financial text, but the tasks do not necessarily require a specialized vocabulary. On the negative side, fluctations in the results confirmed that there is some degree of instability in the fine-tuning of BERT-like models on relatively small datasets (Zhang et al., 2020). In our future work, we plan to investigate also the contextualized embeddings produced by the domain-adapted Transformers. Word embeddings have been used in tasks with important applications in the financial domain, such as the identification of semantic relations (Chersoni et al., 2016; Xiang et al., 2020), which is useful for building domain ontologies (El Maarouf et al., 2021; Mansar et al., 2021; Chersoni and Huang, 2021), and the unsupervised detection of semantic changes in diachronic data, e.g., annual reports of traded companies (Giulianelli et al., 2020; Montariol et al., 2021; Masson and Montariol, 2021). In this perspective, a promising research direction would be to analyze how different domain adaptation strategies affect the quality of the embedding representations. Acknowledgments 5 Conclusions We would like to thank Tobias Daudert, Frank Xing, and Chung-Chi C"
2021.econlp-1.5,W18-3107,0,0.0540179,"Missing"
2021.econlp-1.5,W19-5512,0,0.0409191,"BERT, a Transformer model trained on text from the financial domain. By comparing its performances with the original BERT on a wide variety of financial text processing tasks, we found continual pretraining from the original model to be the more beneficial option. Domain-specific pretraining from scratch, conversely, seems to be less effective. 1 2 Related Work Although financial NLP is a relatively recent field, it already has an active research community, which has regularly introduced new shared tasks and benchmarks in recent years, e.g., sentence boundary detection in financial documents (Azzi et al., 2019; Wan et al., 2019; Au et al., 2021), hypernymy detection (El Maarouf et al., 2021; Mansar et al., 2021), document causality detection (Mariko et al., 2020), document structure extraction (Juge et al., 2019; Bentabet et al., 2020), and document summarization (Zheng et al., 2020). Given the success of Transformer models in general-domain NLP, it is not surprising that they are also a popular choice for many systems competing in financial tasks (Chen et al., 2020). To adapt the original BERT to sentiment analysis in the financial domain, Araci (2019) was the first to propose a FinBERT model by f"
2021.econlp-1.5,N19-1423,0,0.171878,"), demonstrated that FinBERT largely outperformed all the LSTM-based baselines and was slightly better than the original model. The second FinBERT model, introduced by Yang et al. (2020), followed two different training strategies. The first version (FinBERT BaseVocab) was further pretrained from a BERT Base checkpoint on three financial corpora (i.e., the Corporate Reports 10-K & 10-Q from the Securities Introduction The Transformer architectures have taken the field of Natural Language Processing (NLP) by storm, leading to remarkable performance leaps in several tasks (Vaswani et al., 2017; Devlin et al., 2019). The first-generation Transformers were mainly trained on general corpora, such as Wikipedia or Common Crawl. However, considering domain adaptations, many researchers have later injected domain-specific knowledge in such architectures, leading to the publication of Transformers trained on different types of in-domain text, e.g., scientific articles (Beltagy et al., 2019), biomedical text (Lee et al., 2020; Gu et al., 2020), clinical notes (Alsentzer et al., 2019), and patent corpora (Lee and Hsiang, 2020). Since language technologies have seen increasingly frequent use in accounting and fina"
2021.econlp-1.5,D19-1371,0,0.0184547,"ts 10-K & 10-Q from the Securities Introduction The Transformer architectures have taken the field of Natural Language Processing (NLP) by storm, leading to remarkable performance leaps in several tasks (Vaswani et al., 2017; Devlin et al., 2019). The first-generation Transformers were mainly trained on general corpora, such as Wikipedia or Common Crawl. However, considering domain adaptations, many researchers have later injected domain-specific knowledge in such architectures, leading to the publication of Transformers trained on different types of in-domain text, e.g., scientific articles (Beltagy et al., 2019), biomedical text (Lee et al., 2020; Gu et al., 2020), clinical notes (Alsentzer et al., 2019), and patent corpora (Lee and Hsiang, 2020). Since language technologies have seen increasingly frequent use in accounting and finance (Loughran and McDonald, 2016), it is not surprising that several attempts have been made to adapt Transformers to the financial domain (Araci, 2019; Yang et al., 2020; Liu et al., 2020). In this study, we test the FinBERT model by Yang et al. (2020) on a variety of tasks in the field of financial NLP, including sentiment analysis, causality detection, numeral understan"
2021.econlp-1.5,2020.fnp-1.2,0,0.0712605,"Missing"
2021.econlp-1.5,2020.acl-main.365,0,0.0227081,"dels on relatively small datasets (Zhang et al., 2020). In our future work, we plan to investigate also the contextualized embeddings produced by the domain-adapted Transformers. Word embeddings have been used in tasks with important applications in the financial domain, such as the identification of semantic relations (Chersoni et al., 2016; Xiang et al., 2020), which is useful for building domain ontologies (El Maarouf et al., 2021; Mansar et al., 2021; Chersoni and Huang, 2021), and the unsupervised detection of semantic changes in diachronic data, e.g., annual reports of traded companies (Giulianelli et al., 2020; Montariol et al., 2021; Masson and Montariol, 2021). In this perspective, a promising research direction would be to analyze how different domain adaptation strategies affect the quality of the embedding representations. Acknowledgments 5 Conclusions We would like to thank Tobias Daudert, Frank Xing, and Chung-Chi Chen for sharing their datasets with us, and the four anonymous reviewers for their insightful feedback. This research was made possible by the University Postdoc Matching Fund (W16H) and Project of Strategic Importance (ZE2J) at the Hong Kong Polytechnic University. In this paper,"
2021.econlp-1.5,L18-1550,0,0.0151882,". b. In this study, two baseline models were used. One is the BERT Base (Devlin et al., 2019), which consists of a series of stacked Transformer encoders. It was trained using both a masked language modeling objective and a next sentence prediction objective on a concatenation of the Books Corpus (Zhu et al., 2015) and the English version of Wikipedia. The other one is a traditional Support Vector Machine (SVM) baseline (Noble, 2006), where the input representation is the element-wise addition of the word vectors of each word in the sentence. We used the publicly available FastText vectors by Grave et al. (2018). As for the FinBERT models, we used FinBERT BaseVocab (FV w/ BV) and FinBERT FinVocab (FB w/ FV) (Yang et al., 2020). The former was initialized from the original BERT Base (i.e., it also uses the same general-domain vocabulary) and then further pretrained on financial corpora, and the latter was trained afresh on financial corpora for 1M iterations and uses a domain-specific financial vocabulary. Following the methodology by Devlin et al. (2019), all models used a linear layer with sof tmax as a classification layer and the crossentropy loss as a loss function. The texts were directly fed to"
2021.econlp-1.5,W19-6407,0,0.0988783,"Missing"
2021.econlp-1.5,2020.cogalex-1.5,1,0.678358,"odels take advantage from exposure to financial text, but the tasks do not necessarily require a specialized vocabulary. On the negative side, fluctations in the results confirmed that there is some degree of instability in the fine-tuning of BERT-like models on relatively small datasets (Zhang et al., 2020). In our future work, we plan to investigate also the contextualized embeddings produced by the domain-adapted Transformers. Word embeddings have been used in tasks with important applications in the financial domain, such as the identification of semantic relations (Chersoni et al., 2016; Xiang et al., 2020), which is useful for building domain ontologies (El Maarouf et al., 2021; Mansar et al., 2021; Chersoni and Huang, 2021), and the unsupervised detection of semantic changes in diachronic data, e.g., annual reports of traded companies (Giulianelli et al., 2020; Montariol et al., 2021; Masson and Montariol, 2021). In this perspective, a promising research direction would be to analyze how different domain adaptation strategies affect the quality of the embedding representations. Acknowledgments 5 Conclusions We would like to thank Tobias Daudert, Frank Xing, and Chung-Chi Chen for sharing their"
2021.econlp-1.5,2020.coling-main.85,0,0.0488072,"Missing"
2021.econlp-1.5,2021.finnlp-1.5,0,0.0659089,"Missing"
2021.econlp-1.5,2020.fnp-1.21,0,0.0754984,"Missing"
2021.findings-acl.258,N01-1021,0,0.458636,"is way, we were able to analyze the effect of the tense without the interference of the other context words. 4 a. b. 3.2 He had danced in the ballroom. (P ERF ) He was dancing in the ballroom (IM P ERF ) Model and Settings Similarly to Misra et al. (2020), we considered the surprisal score for an argument word (in our case, the location) as a measure of the model’s expectations in the given context; we replaced the location token at the end of each sentence with a [MASK] and we asked BERT to predict its probability. Surprisal was shown to be an efficient predictor of self-paced reading times (Hale, 2001; Levy, 2008; Smith and Levy, 2013) and of the N400 amplitude (Frank et al., 2013), and we expected it to be inversely correlated with typicality: the more typical a location in a given context is, the less surprising it will be. To approximate the results of the original study by Ferretti et al. (2007), BERT’s surprisal scores for the locations would have to show an interaction between aspect and typicality, with the scores being significantly higher for atypical fillers only in the imperfective condition. Moreover, typical fillers 2 More details on the dataset creation are in Appendix A. Sur"
2021.findings-acl.258,2020.acl-main.434,0,0.0180928,"T Y P sentences showing significantly lower Surprisal scores in the T Y P condition than the N ON T Y P ones (see also the boxplots in Figure 1, on the right). The interaction of aspect and typicality, however, was not significant (p > 0.1). Note that the N ON T Y P locations were selected by Ferretti et al. (2007) in order to be plausible, and thus it is interesting that BERT correctly identified the ones of the T Y P sentences as being more typical. However, the verb aspect did not play a role in this, since this ability was not influenced by the aspect condition. As a possible explanation, Klafka and Ettinger (2020) recently showed how the semantic information about the animacy of an argument noun in BERT was spread over the tokens of a sentence, and the same might be true also for the semantic information about the typicality of a location in a given event context. Pairwise comparisons confirmed that T Y P sentences obtained significantly lower scores for both the P ERF and IM P ERF conditions (p < 0.001), while no significant difference between P ERF and IM P ERF in typicality condition was found. We then repeated the experiments using the context mask to block BERT’s attention mechanism for all the wo"
2021.findings-acl.258,Q16-1037,0,0.0335994,"xt of a masked word in a natural language sentence, the model has to predict the word. This conceptually simple yet powerful mechanism has made BERT a very appealing option for NLP researchers working on supervised tasks, and its contextualized representations have taken state-ofthe-art performances to new heights. A number of psycholinguistic-inspired studies designed tests to investigate the actual linguistic abilities of neural network models, including Transformer models. Most of these studies have focused on syntactic phenomena, such as verb-subject agreement and filler-gap dependencies (Linzen et al., 2016; Wilcox et al., 2018; Gulordava et al., 2018; Futrell et al., 2019; Prasad et al., 2019). By contrast, Ettinger (2020) focused on the semantic and pragmatic abilities of the BERT language model by using stimuli from the N400 experiments conducted by Kutas and Hillyard (1984), and showed that the model was strong in associating nouns with their hypernyms, but struggled to handle negations. Close to the spirit of our contribution, Misra et al. (2020) investigated BERT’s predictions in a setting aimed at reproducing human semantic priming; they reported that BERT was indeed sensitive to “priming"
2021.findings-acl.258,2020.coling-main.109,0,0.0773687,"Missing"
2021.findings-acl.258,2020.findings-emnlp.415,0,0.0961082,"cluding Transformer models. Most of these studies have focused on syntactic phenomena, such as verb-subject agreement and filler-gap dependencies (Linzen et al., 2016; Wilcox et al., 2018; Gulordava et al., 2018; Futrell et al., 2019; Prasad et al., 2019). By contrast, Ettinger (2020) focused on the semantic and pragmatic abilities of the BERT language model by using stimuli from the N400 experiments conducted by Kutas and Hillyard (1984), and showed that the model was strong in associating nouns with their hypernyms, but struggled to handle negations. Close to the spirit of our contribution, Misra et al. (2020) investigated BERT’s predictions in a setting aimed at reproducing human semantic priming; they reported that BERT was indeed sensitive to “priming” and predicted a word with higher probability when the context included a related word as opposed to an unrelated one, but this effect decreased in the presence of strongly informative and constraining contexts. Recent work by Metheniti et al. (2020) has explored the capacity of BERT to reproduce the selectional preferences for verbs - which, from our perspective, was equivalent to modeling the thematic fit of typical event participants (Sayeed et"
2021.findings-acl.258,K19-1007,0,0.0269751,"Missing"
2021.findings-acl.258,2020.aacl-main.26,1,0.773031,"idate locations (desert-hole) had very similar plausibility levels. (4) a. b. c. d. The girl was skating/had skated in the rink (T Y P ) / ring (N ON T Y P ). The boy was tobogganing/had tobogganed down the hill (T Y P ) / street (N ON T Y P ). The tourist was browsing/had browsed in the shop (T Y P ) / park (N ON T Y P ). The snake was slithering/had slithered in the desert (T Y P ) / hole (N ON T Y P ). The tendency of masked language models to select a generic and frequent word when faced with the alternative of a more specific and typical filler for the event scenario was also reported by Rambelli et al. (2020) in a logical metonymy interpretation task, e.g., when asked to predict a verb for the masked position in a sentence like The auditor begins [MASK] the taxes, they chose generic verbs like doing instead of more specific ones like auditing, which should be preferred in the given context. 5 Conclusions In this study, we tested whether BERT exhibited aspect-related activation effects for event locations, and whether different degrees of location typicality were identified more easily in sentences in the imperfective aspect. Verb aspect, as shown in previous studies (Ferretti et al., 2007; Madden-"
2021.findings-acl.258,D17-1068,1,0.833058,"ed BERT’s predictions in a setting aimed at reproducing human semantic priming; they reported that BERT was indeed sensitive to “priming” and predicted a word with higher probability when the context included a related word as opposed to an unrelated one, but this effect decreased in the presence of strongly informative and constraining contexts. Recent work by Metheniti et al. (2020) has explored the capacity of BERT to reproduce the selectional preferences for verbs - which, from our perspective, was equivalent to modeling the thematic fit of typical event participants (Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020; Marton and Sayeed, 2021). Metheniti and colleagues reported that the correlation of the predictions with human judgements increased when they applied attention masks to the context words in the sentence and forced the model to focus only on the verbs. Finally, Transformers have been used to model typicality effects in language by Misra et al. (2021), although in a different context; i.e. the influence of typicality on category membership judgements. To the best of our knowledge, the current study is the first to attempt to model argument typicality predictions with BER"
2021.findings-acl.258,W16-2518,0,0.0289446,"al. (2020) investigated BERT’s predictions in a setting aimed at reproducing human semantic priming; they reported that BERT was indeed sensitive to “priming” and predicted a word with higher probability when the context included a related word as opposed to an unrelated one, but this effect decreased in the presence of strongly informative and constraining contexts. Recent work by Metheniti et al. (2020) has explored the capacity of BERT to reproduce the selectional preferences for verbs - which, from our perspective, was equivalent to modeling the thematic fit of typical event participants (Sayeed et al., 2016; Santus et al., 2017; Chersoni et al., 2020; Marton and Sayeed, 2021). Metheniti and colleagues reported that the correlation of the predictions with human judgements increased when they applied attention masks to the context words in the sentence and forced the model to focus only on the verbs. Finally, Transformers have been used to model typicality effects in language by Misra et al. (2021), although in a different context; i.e. the influence of typicality on category membership judgements. To the best of our knowledge, the current study is the first to attempt to model argument typicality"
2021.findings-acl.258,W18-5423,0,0.012118,"n a natural language sentence, the model has to predict the word. This conceptually simple yet powerful mechanism has made BERT a very appealing option for NLP researchers working on supervised tasks, and its contextualized representations have taken state-ofthe-art performances to new heights. A number of psycholinguistic-inspired studies designed tests to investigate the actual linguistic abilities of neural network models, including Transformer models. Most of these studies have focused on syntactic phenomena, such as verb-subject agreement and filler-gap dependencies (Linzen et al., 2016; Wilcox et al., 2018; Gulordava et al., 2018; Futrell et al., 2019; Prasad et al., 2019). By contrast, Ettinger (2020) focused on the semantic and pragmatic abilities of the BERT language model by using stimuli from the N400 experiments conducted by Kutas and Hillyard (1984), and showed that the model was strong in associating nouns with their hypernyms, but struggled to handle negations. Close to the spirit of our contribution, Misra et al. (2020) investigated BERT’s predictions in a setting aimed at reproducing human semantic priming; they reported that BERT was indeed sensitive to “priming” and predicted a wor"
2021.rocling-1.51,P16-2037,1,0.831006,"t, Yuan Ze University 2 School of Information Science and Engineering, Yunnan University 3 Department of Chinese and Bilingual Studies, The Hong Kong Polytechnic University Contact: lcyu@saturn.yzu.edu.tw, wangjin@ynu.edu.cn, peng-bo.peng@polyu.edu.hk, churen.huang@polyu.edu.hk 1 and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016a; Du and Zhang, 2016; Wu et la., 2017) or texts (Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020; Cheng et al, 2021; Wu et al., 2021, Xie et al., 2021). In 2016, we hosted a first dimensional sentiment analysis task for Chinese words (Yu et al., 2016b). In 2017, we extended this task to include both word- and phrase-level dimensional sentiment analysis (Yu et al., 2017). This year, we explore the sentencelevel dimensional sentiment analysis task on educational texts (students’ self-evaluated comments). Structured data such as attendance, homework completion"
2021.rocling-1.51,D19-1343,1,0.83437,"Studies, The Hong Kong Polytechnic University Contact: lcyu@saturn.yzu.edu.tw, wangjin@ynu.edu.cn, peng-bo.peng@polyu.edu.hk, churen.huang@polyu.edu.hk 1 and negative) feelings, and the arousal represents the degree of excitement and calm. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016a; Du and Zhang, 2016; Wu et la., 2017) or texts (Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020; Cheng et al, 2021; Wu et al., 2021, Xie et al., 2021). In 2016, we hosted a first dimensional sentiment analysis task for Chinese words (Yu et al., 2016b). In 2017, we extended this task to include both word- and phrase-level dimensional sentiment analysis (Yu et al., 2017). This year, we explore the sentencelevel dimensional sentiment analysis task on educational texts (students’ self-evaluated comments). Structured data such as attendance, homework completion and in-class participation have been extensively studied to predict students’ learning performance. Unstructured data, such as"
2021.rocling-1.51,W17-5207,0,0.0616408,"Missing"
2021.rocling-1.51,N16-1066,1,0.933056,"tive) feelings, and the arousal represents the degree of excitement and calm. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016a; Du and Zhang, 2016; Wu et la., 2017) or texts (Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020; Cheng et al, 2021; Wu et al., 2021, Xie et al., 2021). In 2016, we hosted a first dimensional sentiment analysis task for Chinese words (Yu et al., 2016b). In 2017, we extended this task to include both word- and phrase-level dimensional sentiment analysis (Yu et al., 2017). This year, we explore the sentencelevel dimensional sentiment analysis task on educational texts (students’ self-evaluated comments). Structured data such as attendance, homework completion and in-class participation have been extensively studied to predict students’ learning performance. Unstructured data, such as selfevaluation comments written by students, is also a useful data resource because it contains rich emotional information that can help illuminate the emotion"
2021.rocling-1.51,I17-4002,1,0.863212,"n, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2013; Wang et al., 2016a; Du and Zhang, 2016; Wu et la., 2017) or texts (Paltoglou et al, 2013; Goel et la., 2017; Zhu et al., 2019; Wang et al., 2019; 2020; Cheng et al, 2021; Wu et al., 2021, Xie et al., 2021). In 2016, we hosted a first dimensional sentiment analysis task for Chinese words (Yu et al., 2016b). In 2017, we extended this task to include both word- and phrase-level dimensional sentiment analysis (Yu et al., 2017). This year, we explore the sentencelevel dimensional sentiment analysis task on educational texts (students’ self-evaluated comments). Structured data such as attendance, homework completion and in-class participation have been extensively studied to predict students’ learning performance. Unstructured data, such as selfevaluation comments written by students, is also a useful data resource because it contains rich emotional information that can help illuminate the emotional states of students (Yu et al., 2018). Dimensional sentiment analysis is an effective technique to recognize the valence"
2021.semeval-1.70,Q17-1010,0,0.0473349,"ery distinct path as a single feature. In total, we generated 267 dependency paths features with this mechanism. Another feature was based on Word Embedding similarity: first, we computed the sum of the embeddings for all the words preceding the target, as a sort of general representation of the sentence context 1 , and then we measured the cosine similarity with the embedding of the target word. If the target was a multiword expression, we summed the embeddings of the words composing it. As word embeddings, we used the publicly available FastText vectors, pre-trained on the Wikipedia corpus (Bojanowski et al., 2017). 2 We added one feature based on the BERT Transformer Model (Devlin et al., 2019) 3 by masking the target word in the original sentence and taking the probability value provided in output by the Softmax. For multiword expressions, we sequentially masked the words composing the target and took the average value. Similarly, we used the GPT-2 Transformer Model (Radford et al., 2019) 4 to obtain a probability score for the full sentence, computed as the product of the probabilities of the single tokens. The total number of extracted features is 300. Finally, we decided to generate polynomial feat"
2021.semeval-1.70,W16-4102,1,0.826572,"meanings. In linguistic typology, for example, complexity is generally studied as a property of the language system as a whole, it is conceived as the number of (morphological, syntactic, semantic etc.) distinctions that a speaker has to master, and it is assessed by comparing different languages (McWhorter, 2001; Parkvall, 2008). On the other hand, in the perspective of psycholinguistics and cognitive science, the notion of complexity can be described as the difficulty encountered by language users while processing concrete linguistic realizations (sentences, utterances etc.) (Blache, 2011; Chersoni et al., 2016, 2017, 2021; Iavarone et al., 2021; Sarti et al., 2021). Finally, in the Computational Linguistics community, the assessment of complexity at the lexical level is often related to readability applications (Shardlow et al., 2020), with the goal of determining if a word in a given text will be difficult to understand for the language users. Such applications are extremely useful for second language learners, for speakers with relatively low literacy and for people with reading disabilities, helping to tailor the difficult level of the texts to the needs of the target users. Task 1 of SemEval 20"
2021.semeval-1.70,S17-1021,1,0.866426,"Missing"
2021.semeval-1.70,N19-1423,0,0.0167303,"tures with this mechanism. Another feature was based on Word Embedding similarity: first, we computed the sum of the embeddings for all the words preceding the target, as a sort of general representation of the sentence context 1 , and then we measured the cosine similarity with the embedding of the target word. If the target was a multiword expression, we summed the embeddings of the words composing it. As word embeddings, we used the publicly available FastText vectors, pre-trained on the Wikipedia corpus (Bojanowski et al., 2017). 2 We added one feature based on the BERT Transformer Model (Devlin et al., 2019) 3 by masking the target word in the original sentence and taking the probability value provided in output by the Softmax. For multiword expressions, we sequentially masked the words composing the target and took the average value. Similarly, we used the GPT-2 Transformer Model (Radford et al., 2019) 4 to obtain a probability score for the full sentence, computed as the product of the probabilities of the single tokens. The total number of extracted features is 300. Finally, we decided to generate polynomial features from our set, in order to exploit potential interactions. We used the Polynom"
2021.semeval-1.70,2021.cmcl-1.23,0,0.0651991,"Missing"
2021.semeval-1.70,2005.mtsummit-papers.11,0,0.0360983,"ressions track. Examples of the instances are shown in Table 1. 4 Evaluation For both the single words and the multiword expressions track, we used the same set of features as input for a regression algorithm. In the multiword expressions track, we computed the value of the features for each of the two words in the target expression and then we took the average. 4.1 Datasets The datasets for the shared task are part of the CompLex corpus, which has been published and described by Shardlow et al. (2020). The annotated sentences were collected using three different corpora: the Europarl corpus (Koehn, 2005), which includes the proceedings of the European Parliament; the CRAFT biomedical corpus (Bada et al., 2012); and the Bible, in the modern version of the World English Bible translation (Christodouloupoulos and Steedman, 2015). The organizers selected targets as either single words (Sub-Task 1) or multiword expressions (SubTask 2), and the datasets include also multiple examples with the same target, as different contexts can determine different complexity values. As for the multiword expressions, they were identified via syntactic patterns, being either adjective-noun or noun-noun phrases. 20"
2021.semeval-1.70,P14-5010,0,0.00258051,"CompLex. In total, we obtained 6 features (4 frequency + 2 length features) for each instance. We also added two Boolean features for Capitalization: the first was equal to 1 if the first letter of the target word was upper case and 0 otherwise; the second one was equal to 1 if all the letters of the target word were upper case and 0 otherwise. The latter feature was added because we noticed that some of the target words in the dataset are acronyms. Apart from the lexical information, Syntactic Features were explored for both single words and 566 multiword expressions. The StanfordNLP tools (Manning et al., 2014) were first used to acquire both the part-of-speech (POS) tags and dependency trees. POS tags of target words were manipulated using one-hot encoding, for a total of 20 POS-based features. On the other hand, directed and path from the target word to the root were extracted as dependency features. We concatenated all dependency tags to the root, using one-hot encoding once again to encode every distinct path as a single feature. In total, we generated 267 dependency paths features with this mechanism. Another feature was based on Word Embedding similarity: first, we computed the sum of the embe"
2021.semeval-1.70,2021.cmcl-1.5,0,0.0626278,"Missing"
2021.semeval-1.70,2020.readi-1.9,0,0.14381,"ker has to master, and it is assessed by comparing different languages (McWhorter, 2001; Parkvall, 2008). On the other hand, in the perspective of psycholinguistics and cognitive science, the notion of complexity can be described as the difficulty encountered by language users while processing concrete linguistic realizations (sentences, utterances etc.) (Blache, 2011; Chersoni et al., 2016, 2017, 2021; Iavarone et al., 2021; Sarti et al., 2021). Finally, in the Computational Linguistics community, the assessment of complexity at the lexical level is often related to readability applications (Shardlow et al., 2020), with the goal of determining if a word in a given text will be difficult to understand for the language users. Such applications are extremely useful for second language learners, for speakers with relatively low literacy and for people with reading disabilities, helping to tailor the difficult level of the texts to the needs of the target users. Task 1 of SemEval 2021 (Shardlow et al., 2021) aims at the development of systems for the estimation of lexical complexity in context, both for single words and for multiword expressions. The organizers provided two datasets with the target words in"
2021.semeval-1.70,D18-1499,0,0.0547684,"Missing"
2021.semeval-1.70,W18-0507,0,0.0594112,"Missing"
2021.semeval-1.70,W17-5910,0,0.0184583,"variable: given a word in context, the word will be judged as complex or not. Of course, this was a simplifying assumption, since there might be many situations where the boundary is not a clear-cut one, and annotators would rather indicate a value in a continuous scale. Moreover, the ”complex” words in the data only needed to be categorized as such by just one 565 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 565–570 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics of the annotators. A further study by Zampieri et al. (2017) analyzed the output of the participating systems, showing that modeling complexity as binary actually hindered their performance. A second iteration of the shared task was organized in 2018 (Yimam et al., 2018), this time features two separate subtasks: the traditional binary classification task, where systems had to predict whether one word was complex or not, and a regression task, where systems had to estimate the probability that an annotator would have considered a given word as complex. Recently, Shardlow et al. (2020) have introduced CompLex, a new gold standard for the estimation of l"
2021.sigdial-1.26,2020.sigdial-1.8,0,0.0220579,"talk should provide a collection of state-of-the-art tools for concordance and collocation analysis of lexical as well as non-lexical elements in talk. • Make the tool available to anyone interested in processing real-world conversational speech data. For the given tasks, the toolkit should lower the technical barrier of access and adoption, making it a useful opensource platform for developers, researchers and designers alike. 2 Overview of the Scikit-talk toolkit The idea of a toolkit for conversational speech processing is inspired by an existing similar tool for online chat conversations (Chang et al., 2020). Scikittalk aims to provide both a unified framework for several existing transcription conventions as well as 253 a range of tools to explore this data type in Python. The Preprocessor module is used to combine several existing datasets via pre-built interfaces for various data formats while preserving as much transcribed information as possible. Apart from corpus building, Scikit-talk also includes tools for data manipulations. It includes string matching tools to explore and identify features of interest. And it includes tools for unsupervised machine learning techniques to explore distrib"
C10-1021,P09-1095,0,0.070286,"Missing"
C10-1021,P08-2045,0,0.0216045,"Missing"
C10-1021,C08-1111,0,0.10928,". Both manually generalized patterns and automatically generalized patterns are designed to extract general cause expressions or specific constructions for emotion causes. Experiments show that our system achieves a performance much higher than a baseline model. 1 Introduction Text-based emotion processing has been a center of attention in the NLP field in the past few years. Most previous researches have focused on detecting the surface information of emotions, especially emotion classes, e.g., “happiness” and “anger” (Mihalcea and Liu 2006, Strapparava and Mihalcea 2008, Abbasi et al, 2008, Tokuhisa et al. 2008). Although most emotion theories recognize the important role of causes in emotion analysis (Descartes, 1649; James, 1884; Plutchik 1980, Wierzbicka 1999), very few studies explore the interactions between emotion and causes. Emotion-cause interaction is the eventive relation which potentially yields the most crucial information in terms of information extraction. For instance, knowing the existence of an emotion is often insufficient to predict future events or decide on the best reaction. However, if the emotion cause is known in addition to the type of emotion, prediction of future events o"
C10-1021,Y09-1011,1,0.821787,"e first attempt to explore the correlation between emotions and causes, and annotate a Chinese emotion cause corpus. The emotion cause corpus focuses on five primary emotions, namely “happiness”, “sadness”, “fear”, “anger”, and “surprise”. The emotions are explicitly expressed by emotion keywords, e.g., gao1xing4 “happy”, shang1xin1 “sad”, etc. The corpus is created as follows. 1. 6,058 entries of Chinese sentences are extracted from the Academia Sinica Balanced Corpus of Mandarin Chinese (Sinica Corpus) with the pattern-match method as well as the list of 91 Chinese primary emotion keywords (Chen et al., 2009). Each entry contains the focus sentence with the emotion keyword “&lt;FocusSentence&gt;” plus the sentence before “&lt;PrefixSentence&gt;” and after “&lt;SuffixSentence&gt;” it. For each entry, the emotion keywords are indexed since more than one emotion may be presented in an entry; 2. Some preprocessing, such as balancing the number of entry among emotions, is done to remove some entries. Finally, 5,629 entries remain; 3. Each emotion keyword is annotated with its corresponding causes if existing. An emotion keyword can sometimes be associ181 3.2 The Analysis of Emotion Causes To have a deeper understanding"
C10-1021,W03-1210,0,0.0123079,"Missing"
C10-1021,W10-0206,1,0.745634,"xtraction, as emotion cause detection is a case of cause detection, some typical patterns used in existing cause detection systems, e.g., “because” and “thus”, can be adopted. In addition, various linguistic cues are examined which potentially indicate emotion causes, such as causative verbs and epistemic markers (Lee at al. 2010a). Then some linguistic patterns of emotion causes are manu179 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 179–187, Beijing, August 2010 ally generalized by examining the linguistic context of the empirical data (Lee et al., 2010b). It is expected that these manually generalized patterns often yield a low-coverage problem. Thus, we extracted features which enable us to automatically capture more emotion-specific constructions. Experiments show that such an integrated system with various linguistic features performs promisingly well. We believe that the present study should provide the foundation for future research on emotion analysis, such as the detection of implicit emotion or cause. The paper is organized as follows. Section 2 discusses the related work on cause-effect detection. Section 3 briefly describes the em"
C10-1021,lee-etal-2010-emotion,1,0.749997,"xtraction, as emotion cause detection is a case of cause detection, some typical patterns used in existing cause detection systems, e.g., “because” and “thus”, can be adopted. In addition, various linguistic cues are examined which potentially indicate emotion causes, such as causative verbs and epistemic markers (Lee at al. 2010a). Then some linguistic patterns of emotion causes are manu179 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 179–187, Beijing, August 2010 ally generalized by examining the linguistic context of the empirical data (Lee et al., 2010b). It is expected that these manually generalized patterns often yield a low-coverage problem. Thus, we extracted features which enable us to automatically capture more emotion-specific constructions. Experiments show that such an integrated system with various linguistic features performs promisingly well. We believe that the present study should provide the foundation for future research on emotion analysis, such as the detection of implicit emotion or cause. The paper is organized as follows. Section 2 discusses the related work on cause-effect detection. Section 3 briefly describes the em"
C10-1021,P02-1047,0,0.0220873,"Missing"
C10-1072,P07-1056,0,0.455354,"Missing"
C10-1072,C04-1200,0,0.209598,"selection method is presented to automatically generate the labeled training data for polarity shifting detection of sentences. The remainder of this paper is organized as follows. Section 2 introduces the related work of sentiment classification. Section 3 presents our approach in details. Experimental results are presented and analyzed in Section 4. Finally, 636 Section 5 draws the conclusion and outlines the future work. 2 Related Work Generally, sentiment classification can be performed at four different levels: word level (Wiebe, 2000), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and document level (Turney, 2002; Pang et al., 2002; Pang and Lee, 2004; Riloff et al., 2006). This paper focuses on document-level sentiment classification. In the literature, there are mainly two kinds of approaches on document-level sentiment classification: term-counting approaches (lexicon-based) and machine learning approaches (corpus-based). Term-counting approaches usually involve deriving a sentiment measure by calculating the total number of negative and positive terms (Turney, 2002; Kim and Hovy, 2004; Kennedy and Inkpen, 2006). Machine learning approaches recas"
C10-1072,P08-2065,1,0.901692,"Missing"
C10-1072,W02-1011,0,0.0411334,"shifting detection of sentences. Then, by using the obtained binary classifier, each document in the original polarity classification training data is split into two partitions, polarity-shifted and polarity-unshifted, which are used to train two base classifiers respectively for further classifier combination. The experimental results across four different domains demonstrate the effectiveness of our approach. 1 Introduction Sentiment classification is a special task of text classification whose objective is to classify a text according to the sentimental polarities of opinions it contains (Pang et al., 2002), e.g., favorable or unfavorable, positive or negative. This task has received considerable interests in the computational linguistic community due to its potential applications. In the literature, machine learning approaches have dominated the research in sentiment classification and achieved the state-of-the-art performance (e.g., Kennedy and Inkpen, 2006; ‡ Natural Language Processing Lab School of Computer Science and Technology Soochow University gdzhou@suda.edu.cn Pang et al., 2002). In a typical machine learning approach, a document (text) is modeled as a bag-of-words, i.e. a set of con"
C10-1072,P02-1053,0,0.02264,"e sentimental orientation of the whole text depends on the sum of the sentimental polarities of content words. Although this assumption is reasonable and has led to initial success, it is linguistically unsound since many function words and constructions can shift the sentimental polarities of a text. For example, in the sentence ‘The chair is not comfortable’, the polarity of the word ‘comfortable’ is positive while the polarity of the whole sentence is reversed because of the negation word ‘not’. Therefore, the overall sentiment of a document is not necessarily the sum of the content parts (Turney, 2002). This phenomenon is one main reason why machine learning approaches fail under some circumstances. As a typical case of polarity shifting, negation has been paid close attention and widely studied in the literature (Na et al., 2004; Wilson et al., 2009; Kennedy and Inkpen, 2006). Generally, there are two steps to incorporate negation information into a system: negation detection and negation classification. For negation detection, some negation trigger words, such as ‘no’, ‘not’, and ‘never’, are usually applied to recognize negation phrases or sentences. As for negation classification, one w"
C10-1072,P09-1027,0,0.204745,"Missing"
C10-1072,J09-3003,0,0.511671,"uctions can shift the sentimental polarities of a text. For example, in the sentence ‘The chair is not comfortable’, the polarity of the word ‘comfortable’ is positive while the polarity of the whole sentence is reversed because of the negation word ‘not’. Therefore, the overall sentiment of a document is not necessarily the sum of the content parts (Turney, 2002). This phenomenon is one main reason why machine learning approaches fail under some circumstances. As a typical case of polarity shifting, negation has been paid close attention and widely studied in the literature (Na et al., 2004; Wilson et al., 2009; Kennedy and Inkpen, 2006). Generally, there are two steps to incorporate negation information into a system: negation detection and negation classification. For negation detection, some negation trigger words, such as ‘no’, ‘not’, and ‘never’, are usually applied to recognize negation phrases or sentences. As for negation classification, one way to import negation information is to directly reverse the polarity of the words which contain negation trigger words as far as term-counting approaches are considered (Kennedy and Inkpen, 2006). An alternative way is to add some negation features (e."
C10-1072,P04-1035,0,\N,Missing
C10-1072,W06-1652,0,\N,Missing
C10-1072,P09-1078,1,\N,Missing
C10-1072,P09-1079,0,\N,Missing
C10-1072,I08-1039,0,\N,Missing
C12-2067,W06-1655,0,0.0242448,"ce labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as using more tags and features (Tang et al., 2009; Zhao et al., 2006), employing word-based tagging without tagging (Zhang and Clark, 2007), employing some joint models that combines a generative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov and semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features (Zhao and Kit, 2011). Although there are various studies CWS individually, there are few studies of active learning on CWS. One related work is about active learning on Japanese word segmentation via Support Vector Machines (SVM) (Sassano, 2002). However, both the two challenging problems mentioned above are unsolved. Specifically, that study annotates the whole sentence as a basic unit, which means much more annotation effort than our model. Furthermore, our corpus scale is much larger than the one in Sassano (2002). This makes SVM impracti"
C12-2067,P07-1007,0,0.0489837,"Missing"
C12-2067,N06-1016,0,0.0567906,"Missing"
C12-2067,I05-3017,0,0.10462,"Missing"
C12-2067,J98-4002,0,0.0700262,"Missing"
C12-2067,P07-2018,1,0.755901,"selected for oracle labelling; Uncertainty-Diversity sampling: In each iteration, all the instances in the unlabeled data U are ranked according to their uncertainty-diversity values and top instances are selected for oracle labeling. 4 4.1 Experimentation Experimental Setting The SIGHAN Bakeoff 2 dataset consists of four different corpora: PKU, MSR, CityU, and AS. But we only report the performance on three of the corpora except AS due to its significant large scale in causing the out-of-memory error. The basic segmenter in the active learning process is trained with a 2-tag labelling model (Huang et al., 2007; Huang and Xue, 2012) and implemented with a public tool for CRF implementation, i.e. CRF++ (Kudo, 2005). For the feature template, we adopt the one by Li and Huang (2009). In all experiments, we use the standard F1 score as our main performance measurement. Besides, the out-of-vocabulary (OOV) recall is used to evaluate the OOV issue. 4.2 Experimental Results In this experiment, we compare the random selection strategy and the two sampling strategies as illustrated in Section 3.3: uncertainty sampling and uncertainty-diversity sampling. To fairly compare the performances of different samplin"
C12-2067,P08-1102,0,0.0582361,"Missing"
C12-2067,Y09-2034,1,0.89236,"sity values and top instances are selected for oracle labeling. 4 4.1 Experimentation Experimental Setting The SIGHAN Bakeoff 2 dataset consists of four different corpora: PKU, MSR, CityU, and AS. But we only report the performance on three of the corpora except AS due to its significant large scale in causing the out-of-memory error. The basic segmenter in the active learning process is trained with a 2-tag labelling model (Huang et al., 2007; Huang and Xue, 2012) and implemented with a public tool for CRF implementation, i.e. CRF++ (Kudo, 2005). For the feature template, we adopt the one by Li and Huang (2009). In all experiments, we use the standard F1 score as our main performance measurement. Besides, the out-of-vocabulary (OOV) recall is used to evaluate the OOV issue. 4.2 Experimental Results In this experiment, we compare the random selection strategy and the two sampling strategies as illustrated in Section 3.3: uncertainty sampling and uncertainty-diversity sampling. To fairly compare the performances of different sampling strategies, we make sure that the number of annotated boundaries in either uncertainty sampling or uncertainty-diversity sampling is the same as random selection. Figure"
C12-2067,D12-1013,1,0.8313,"Missing"
C12-2067,W04-3236,0,0.0922903,"Missing"
C12-2067,P02-1064,0,0.529185,"ive learning on CWS. First, the state-of-the-art methods treat CWS as a sequence labelling task (Jiang et al., 2008; Ng and Low, 2004; Tseng et al., 2005; Zhang et al., 2006), i.e. labelling characters with tags from a pre-defined tag set, representing the position of a character in a word. Different from traditional classification tasks, each character is tagged sequentially according to its corresponding context. Under this circumstance, a character cannot be determined as a single unit to query in active learning. One possible solution is to select one sentence as a unit for annotation, as Sassano (2002) does for Japanese word segmentation. However, such solution is expensive for annotation and since one sentence might contain some words which can be easily segmented correctly by existing models with high confidence, annotating them becomes a waste of time and manual effort. Second, the number of the characters in a CWS corpus is normally extremely huge. For example, among the four corpora in SIGHAN Bakeoff 2 (Emerson, 2005), even the smallest corpus contains more than 1,800,000 characters while others are much larger in the order of tens of millions of characters. Compared to other tasks lik"
C12-2067,D08-1112,0,0.0916365,"ount to avoid duplicate annotation. For example, in the example E-A above, both the words ‘索拉纳 ’ and ‘波 兰 ’ are unknown words for the initial segmenter learned by the initial labelled set L with the boundaries of I A1 , I A 2 , I A9 , I B1 , I B 2 , and I B 9 , among the top uncertain instances. Obviously, some boundaries share the same segmentation information, e.g., I A1 and I B1 . Therefore, labelling both of them is a waste. One straightforward way to handle such duplicate annotation is to compute the similarity between every two instances and then pick those with the highest diversities (Settles and Craven, 2008). This method, however, requires O(N2) in computational complexity where N is the number of all boundaries. When N is huge (e.g. N&gt;1,800,000 in our experiments), the high computational burden is simply unacceptable. Fortunately, we find that the similarity between two boundaries is highly related to their surrounding character N-grams (in particular bigrams) and we can better evaluate the diversity with the help of the surrounding character bigrams. This is done in this paper by recording the frequencies of all surrounding bigrams in a set Scc , where f ci ci +1 ∈ S cc indicates the frequency"
C12-2067,P04-1075,1,0.646546,"Missing"
C12-2067,P98-2206,0,0.0867883,"For the second challenge, we propose 684 a diversity measurement among the instances to avoid duplicate annotation, so as to further reduce the annotation efforts. 2 Related Work Research on CWS has a long history and various methods have been proposed in the literature. Basically, these methods are mainly focus on two categories: unsupervised and supervised. Unsupervised methods aim to build a segmentation system without any lexicon or labelled data. They often start from an empirical definition of a word and then use some statistical measures, e.g. mutual information (Sproat and Shih, 1990; Sun et al., 1998), to learn words from a large unlabelled data resource. Although these unsupervised methods can capture many strong words, their performance is often not high enough for the practical use. Supervised methods, such as HMM tagging (Xue, 2003), character-based classification (Wang et al., 2008) and morpheme-based lexical chunking (Fu et al., 2008), attempt to acquire a model based on a dictionary or a labelled data set. Among them, character-based classification has drawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditiona"
C12-2067,I05-3027,0,0.152497,"ttles and Craven, 2008). Although active learning has been widely employed to many NLP tasks, such as word sense disambiguation (Chan and Ng, 2007; Chen et al., 2006; Fujii et al., 1998), text categorization (Lewis and Gale, 1994; Liere and Tadepalli, 1997; McCallum and Nigam, 1998; Li et al., 2012), and named entity recognition (Shen et al., 2004), there are few studies of active learning on CWS, probably due to the strong challenges inherent in performing active learning on CWS. First, the state-of-the-art methods treat CWS as a sequence labelling task (Jiang et al., 2008; Ng and Low, 2004; Tseng et al., 2005; Zhang et al., 2006), i.e. labelling characters with tags from a pre-defined tag set, representing the position of a character in a word. Different from traditional classification tasks, each character is tagged sequentially according to its corresponding context. Under this circumstance, a character cannot be determined as a single unit to query in active learning. One possible solution is to select one sentence as a unit for annotation, as Sassano (2002) does for Japanese word segmentation. However, such solution is expensive for annotation and since one sentence might contain some words wh"
C12-2067,C10-1132,0,0.014142,"rawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as using more tags and features (Tang et al., 2009; Zhao et al., 2006), employing word-based tagging without tagging (Zhang and Clark, 2007), employing some joint models that combines a generative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov and semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features (Zhao and Kit, 2011). Although there are various studies CWS individually, there are few studies of active learning on CWS. One related work is about active learning on Japanese word segmentation via Support Vector Machines (SVM) (Sassano, 2002). However, both the two challenging problems mentioned above are unsolved. Specifically, that study annotates the whole sentence as a basic unit, which means much more annotation effort than our model. Furthermore, our corpus scale is mu"
C12-2067,O03-4002,0,0.155392,"posed in the literature. Basically, these methods are mainly focus on two categories: unsupervised and supervised. Unsupervised methods aim to build a segmentation system without any lexicon or labelled data. They often start from an empirical definition of a word and then use some statistical measures, e.g. mutual information (Sproat and Shih, 1990; Sun et al., 1998), to learn words from a large unlabelled data resource. Although these unsupervised methods can capture many strong words, their performance is often not high enough for the practical use. Supervised methods, such as HMM tagging (Xue, 2003), character-based classification (Wang et al., 2008) and morpheme-based lexical chunking (Fu et al., 2008), attempt to acquire a model based on a dictionary or a labelled data set. Among them, character-based classification has drawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as"
C12-2067,W03-1730,0,0.0549875,"Missing"
C12-2067,P06-2123,0,0.046661,"Missing"
C12-2067,P07-1106,0,0.0245741,"to acquire a model based on a dictionary or a labelled data set. Among them, character-based classification has drawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as using more tags and features (Tang et al., 2009; Zhao et al., 2006), employing word-based tagging without tagging (Zhang and Clark, 2007), employing some joint models that combines a generative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov and semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features (Zhao and Kit, 2011). Although there are various studies CWS individually, there are few studies of active learning on CWS. One related work is about active learning on Japanese word segmentation via Support Vector Machines (SVM) (Sassano, 2002). However, both the two challenging problems mentioned above are unsolved. Specifically, that study annotates the whole sentence"
C12-2067,I08-4017,0,0.204369,"Missing"
C12-2067,Y06-1012,0,0.0603917,"8) and morpheme-based lexical chunking (Fu et al., 2008), attempt to acquire a model based on a dictionary or a labelled data set. Among them, character-based classification has drawn most attention recently and been further implemented with sequence labelling algorithms (Tseng et al., 2005), e.g., conditional random fields (CRF), which perform well in both invocabulary (IV) recall and out-of-vocabulary (OOV) recall. Based on the character labelling approach, many related studies make efforts to improve the performance by various means, such as using more tags and features (Tang et al., 2009; Zhao et al., 2006), employing word-based tagging without tagging (Zhang and Clark, 2007), employing some joint models that combines a generative model and a discriminative model (Wang et al., 2010; Wang et al., 2011) or Markov and semi-Markov CRF (Andrew, 2006), and integrating unsupervised segmentation features (Zhao and Kit, 2011). Although there are various studies CWS individually, there are few studies of active learning on CWS. One related work is about active learning on Japanese word segmentation via Support Vector Machines (SVM) (Sassano, 2002). However, both the two challenging problems mentioned abov"
C12-2067,C08-1059,0,\N,Missing
C12-2067,I08-4015,0,\N,Missing
C12-2121,W10-0731,0,0.0171921,"much larger than the sampling allowable by traditional experiments, interesting and hitherto unobserved distributional properties of human behaviors may emerge. In addition to this, for Language Technology, the additional motivation is that a web-based crowd can provide data for the construction of large-scale LRs in a faster, cheaper and still reliable way. So far, annotation works conducted by means of crowsourcing techniques have focused on rather simple linguistic tasks, such as the evaluation of automatic translations (Callison-Burch, 2009), word sense disambiguation (Snow et al., 2008; Akkaya et al., 2010, Rumshinsky, 2011), textual entailment (Snow et.al., 2008; Wang and Callison-Burch, 2010), commonsense knowledge (Gordon et al., 2010), text alignment for machine translations (Ambati and Vogel, 2010) and speech transcriptions (Callison-Burch and Dredze, 2010) among others. Such choices are in line with the idea of using the “wisdom of the crowd” as the tasks can be simplified and presented to the workers as a sort of online game such that a large percentage of the population can be expected to perform the task reliably. In this work we explore the untapped strength of crowdsourcing when the"
C12-2121,D09-1020,0,0.0387399,"Missing"
C12-2121,W10-0710,0,0.0125418,"echnology, the additional motivation is that a web-based crowd can provide data for the construction of large-scale LRs in a faster, cheaper and still reliable way. So far, annotation works conducted by means of crowsourcing techniques have focused on rather simple linguistic tasks, such as the evaluation of automatic translations (Callison-Burch, 2009), word sense disambiguation (Snow et al., 2008; Akkaya et al., 2010, Rumshinsky, 2011), textual entailment (Snow et.al., 2008; Wang and Callison-Burch, 2010), commonsense knowledge (Gordon et al., 2010), text alignment for machine translations (Ambati and Vogel, 2010) and speech transcriptions (Callison-Burch and Dredze, 2010) among others. Such choices are in line with the idea of using the “wisdom of the crowd” as the tasks can be simplified and presented to the workers as a sort of online game such that a large percentage of the population can be expected to perform the task reliably. In this work we explore the untapped strength of crowdsourcing when the linguistic task is a complex and challenging one, trying to understand “how far can go the crowd?”. As mentioned, the received wisdom is that when the tasks are complex, crowdsourced data may be too no"
C12-2121,A97-1052,0,0.0164667,"ral Language Processing (NLP) systems are based on supervised learning approaches relying on large amounts of manually annotated training data collected by domain experts. Such annotation process is highly expensive both in terms of money and time. However, the absence of manually annotated Language Resources (LRs) makes supervised NLP systems subject to the so-called knowledge acquisition bottleneck. In recent years, in order to facilitate the development of LRs, two different approaches have been tackled. The first aims at automatically acquiring LRs, such as lexica, from large corpus data (Briscoe and Carroll, 1997; Korhonen et al., 2006, among others). The second investigates the exploitation of the Web 2.0 through the use of crowdsourcing techniques, i.e. by using non-expert annotators recruited on the Web. The crucial motivation of crowdsourcing is that when a simple linguistic task is performed by a population much larger than the sampling allowable by traditional experiments, interesting and hitherto unobserved distributional properties of human behaviors may emerge. In addition to this, for Language Technology, the additional motivation is that a web-based crowd can provide data for the constructi"
C12-2121,D09-1030,0,0.0355223,"ng is that when a simple linguistic task is performed by a population much larger than the sampling allowable by traditional experiments, interesting and hitherto unobserved distributional properties of human behaviors may emerge. In addition to this, for Language Technology, the additional motivation is that a web-based crowd can provide data for the construction of large-scale LRs in a faster, cheaper and still reliable way. So far, annotation works conducted by means of crowsourcing techniques have focused on rather simple linguistic tasks, such as the evaluation of automatic translations (Callison-Burch, 2009), word sense disambiguation (Snow et al., 2008; Akkaya et al., 2010, Rumshinsky, 2011), textual entailment (Snow et.al., 2008; Wang and Callison-Burch, 2010), commonsense knowledge (Gordon et al., 2010), text alignment for machine translations (Ambati and Vogel, 2010) and speech transcriptions (Callison-Burch and Dredze, 2010) among others. Such choices are in line with the idea of using the “wisdom of the crowd” as the tasks can be simplified and presented to the workers as a sort of online game such that a large percentage of the population can be expected to perform the task reliably. In th"
C12-2121,W10-0701,0,0.067738,"based crowd can provide data for the construction of large-scale LRs in a faster, cheaper and still reliable way. So far, annotation works conducted by means of crowsourcing techniques have focused on rather simple linguistic tasks, such as the evaluation of automatic translations (Callison-Burch, 2009), word sense disambiguation (Snow et al., 2008; Akkaya et al., 2010, Rumshinsky, 2011), textual entailment (Snow et.al., 2008; Wang and Callison-Burch, 2010), commonsense knowledge (Gordon et al., 2010), text alignment for machine translations (Ambati and Vogel, 2010) and speech transcriptions (Callison-Burch and Dredze, 2010) among others. Such choices are in line with the idea of using the “wisdom of the crowd” as the tasks can be simplified and presented to the workers as a sort of online game such that a large percentage of the population can be expected to perform the task reliably. In this work we explore the untapped strength of crowdsourcing when the linguistic task is a complex and challenging one, trying to understand “how far can go the crowd?”. As mentioned, the received wisdom is that when the tasks are complex, crowdsourced data may be too noisy to use. However, the noise may come in two ways. One pos"
C12-2121,W10-0724,0,0.0425813,"Missing"
C12-2121,C92-4177,0,0.213301,"Missing"
C12-2121,korhonen-etal-2006-large,0,0.0141478,"P) systems are based on supervised learning approaches relying on large amounts of manually annotated training data collected by domain experts. Such annotation process is highly expensive both in terms of money and time. However, the absence of manually annotated Language Resources (LRs) makes supervised NLP systems subject to the so-called knowledge acquisition bottleneck. In recent years, in order to facilitate the development of LRs, two different approaches have been tackled. The first aims at automatically acquiring LRs, such as lexica, from large corpus data (Briscoe and Carroll, 1997; Korhonen et al., 2006, among others). The second investigates the exploitation of the Web 2.0 through the use of crowdsourcing techniques, i.e. by using non-expert annotators recruited on the Web. The crucial motivation of crowdsourcing is that when a simple linguistic task is performed by a population much larger than the sampling allowable by traditional experiments, interesting and hitherto unobserved distributional properties of human behaviors may emerge. In addition to this, for Language Technology, the additional motivation is that a web-based crowd can provide data for the construction of large-scale LRs i"
C12-2121,J91-4003,0,0.0319951,"Missing"
C12-2121,W11-0409,0,0.0261567,"Missing"
C12-2121,D08-1027,0,0.63543,"med by a population much larger than the sampling allowable by traditional experiments, interesting and hitherto unobserved distributional properties of human behaviors may emerge. In addition to this, for Language Technology, the additional motivation is that a web-based crowd can provide data for the construction of large-scale LRs in a faster, cheaper and still reliable way. So far, annotation works conducted by means of crowsourcing techniques have focused on rather simple linguistic tasks, such as the evaluation of automatic translations (Callison-Burch, 2009), word sense disambiguation (Snow et al., 2008; Akkaya et al., 2010, Rumshinsky, 2011), textual entailment (Snow et.al., 2008; Wang and Callison-Burch, 2010), commonsense knowledge (Gordon et al., 2010), text alignment for machine translations (Ambati and Vogel, 2010) and speech transcriptions (Callison-Burch and Dredze, 2010) among others. Such choices are in line with the idea of using the “wisdom of the crowd” as the tasks can be simplified and presented to the workers as a sort of online game such that a large percentage of the population can be expected to perform the task reliably. In this work we explore the untapped strength of cr"
C12-2121,W10-0725,0,0.0153057,"g and hitherto unobserved distributional properties of human behaviors may emerge. In addition to this, for Language Technology, the additional motivation is that a web-based crowd can provide data for the construction of large-scale LRs in a faster, cheaper and still reliable way. So far, annotation works conducted by means of crowsourcing techniques have focused on rather simple linguistic tasks, such as the evaluation of automatic translations (Callison-Burch, 2009), word sense disambiguation (Snow et al., 2008; Akkaya et al., 2010, Rumshinsky, 2011), textual entailment (Snow et.al., 2008; Wang and Callison-Burch, 2010), commonsense knowledge (Gordon et al., 2010), text alignment for machine translations (Ambati and Vogel, 2010) and speech transcriptions (Callison-Burch and Dredze, 2010) among others. Such choices are in line with the idea of using the “wisdom of the crowd” as the tasks can be simplified and presented to the workers as a sort of online game such that a large percentage of the population can be expected to perform the task reliably. In this work we explore the untapped strength of crowdsourcing when the linguistic task is a complex and challenging one, trying to understand “how far can go the"
C12-2121,zarcone-lenci-2008-computational,0,0.0717693,"Missing"
C12-3066,cavaglia-2002-measuring,0,0.0380962,"(LLR) In order to test the validity of our method, we extract the lexical difference between American English and British English via the Google Books Ngram dataset (Michel et al, 2010), which is the largest such corpus to the best of our knowledge. This enormous database contains millions of digitalized books, which cover about 4 percent (over 5 million volumes) of all the books ever printed. We utilize the American part and the British part during time span of 1830-2009 (180 years, n=180). There have been various approaches to corpus comparison (e.g. Dunning, 1993; Rose and Kilgariff, 1998; Cavaglia, 2002; McInnes, 2004). We compare our result with more common approaches, including chi-square (χ2), as recommended by Kilgarriff (2001), and log-likelihood ratio (LLR) recommended by Rayson and Garside (2000). 529 In Table 1, the top 30 words by each criterion (SMR, χ2 & LLR) are listed. Almost every word in the list by our SMR measure is interpretable in the sense of being American or British except the word cent which demands further explanation. From Table 1 we can see that there are large difference between the ranking of most biased words in AmE and BrE. On the left, almost every word in the"
C12-3066,W00-0901,0,0.0789388,"is the largest such corpus to the best of our knowledge. This enormous database contains millions of digitalized books, which cover about 4 percent (over 5 million volumes) of all the books ever printed. We utilize the American part and the British part during time span of 1830-2009 (180 years, n=180). There have been various approaches to corpus comparison (e.g. Dunning, 1993; Rose and Kilgariff, 1998; Cavaglia, 2002; McInnes, 2004). We compare our result with more common approaches, including chi-square (χ2), as recommended by Kilgarriff (2001), and log-likelihood ratio (LLR) recommended by Rayson and Garside (2000). 529 In Table 1, the top 30 words by each criterion (SMR, χ2 & LLR) are listed. Almost every word in the list by our SMR measure is interpretable in the sense of being American or British except the word cent which demands further explanation. From Table 1 we can see that there are large difference between the ranking of most biased words in AmE and BrE. On the left, almost every word in the list ranked by our method is obviously an American-dominant word or British-dominant word. In the middle, words in the list ranked by chi-square presents a mixture of biased words (e.g. £, labour, centre,"
C12-3066,zhang-etal-2004-distributional,1,0.862443,"Missing"
C90-2010,C88-1060,0,0.0339078,"Missing"
C90-2010,J88-1004,0,0.024294,"Missing"
C90-2010,C86-1045,0,\N,Missing
C90-2010,J88-1001,0,\N,Missing
C92-4194,C92-1019,1,0.820253,", i.e. where the sccnnd key word does not (recur in the leftcontext of the first key word. 6) &lt; kwl > / r &lt; kw2 > : context where a key word (kw2) is 'right-disassociated' from another key word (kwl), i.e. where tile secured key word does not occur in the right-context of the first key word. t ;ommands 1)-3) are helpful tools in studying morphological rules and identifying morphological constmctions for Chinese. Since Chinese writing systems do not include word-breaks, and since no lexicon can ever offer a complete list of words, word segmentatkm is non-trivial in Chinese Language Processing (Chert and Liu 1992). Identifying and util~ing morphological information is therefore essential Ix)th in lexical computing and in natural language processing. PROC.OF COLING-92, NAN-rES,AUG. 23-28, 1992 C o m m a n d 4) is a handy tool to discover cooccurrence restrictions and their semantic consequences. Commands 5) and 6) are used to eliminate ambiguity and to cut down the size of search results. It can be noted that since our tagger is not running yet and since the Chinese running text seldom defines a sentence by a period (a whole paragraph often contains only one period and many commas), the above commands u"
C92-4194,C90-2010,1,\N,Missing
C94-1088,C92-1019,1,0.823636,"mmonly used to resolve categorical or sense ambiguities. Combining both N-gram search and string filtering, fi'equcncy-based word collocatipn is achieved without segmentation. V. Collocation After Segmentation When lexical or phrasal relation is the focus of the study, the above collocation module may sometimes be 541 inadequate. In tiffs case, we will necd to apply the automatic segmentation/tagging program such that we can acquire information involving word pairs as well as grammatical categories. The automatic segmentation proccdurc is an revised version of the program reported in Chen and Liu (1992). The on-line lexicon is the CKIP lexicon of more than 80 thousand cntries (Chen 1994). We did not automatically segment and tag the whole corpus for very good reasons. First, without a correctly tagged corpus, no statistically-based tagger can perform satisfactorily yet. Second, tllcrc is no practical way to recover incorrectly identified words. That is, when the automatic taggcr takes a character fi'om a target word to form an inal~propriate word with a neighboring character; that target word is lost and cannot be identified in this context. Tiros, it will be linguistically more felicitous t"
C94-1088,J93-1001,0,0.0297929,"ntation but is awe to word-based collocational properties can be obtained allow both lexical and sub-lexical information be through an auxiliary modttle of automatic segmentation. automatically extracted. PI(OJ1;CT NOTE: I~ARGI; TEXT COI(I'ORA II. Background: Corpus and Computational Platfonn collocation system has the 1. Introduction To take the full This collocation system is developed on the 20 million charactcr modern Chinese corpus at Academia Collocation has been established as an essential tool in Sinica (Huang and Chen 1992, lquang In Press). This computational linguistics (Church and Mercer 1993). In corpus is composed mostly of newspaper texts. It is addition, various col[ocatiomd programs have been cstimatcd to have proven to bc indispensable in automatic acquisition of"" industrial standard in Taiwan, our collocation system [exical information (e.g. Sinclair 1991, and Bibcr i993). can deal with any corpora encoded by BIG-5 code. The Sincc words arc the natural and undisputed units in program is dcvclopcd undcr a UNiX cnvironmcnt on HP available workstation. text corpora, virtually all the current 14 million words. Following It should, howcver, be portable to any collocationa[ progra"
C94-1088,C92-4194,1,0.91907,"sing sub-lexical information. Furthermore, processing of automatic segmentation but is awe to word-based collocational properties can be obtained allow both lexical and sub-lexical information be through an auxiliary modttle of automatic segmentation. automatically extracted. PI(OJ1;CT NOTE: I~ARGI; TEXT COI(I'ORA II. Background: Corpus and Computational Platfonn collocation system has the 1. Introduction To take the full This collocation system is developed on the 20 million charactcr modern Chinese corpus at Academia Collocation has been established as an essential tool in Sinica (Huang and Chen 1992, lquang In Press). This computational linguistics (Church and Mercer 1993). In corpus is composed mostly of newspaper texts. It is addition, various col[ocatiomd programs have been cstimatcd to have proven to bc indispensable in automatic acquisition of"" industrial standard in Taiwan, our collocation system [exical information (e.g. Sinclair 1991, and Bibcr i993). can deal with any corpora encoded by BIG-5 code. The Sincc words arc the natural and undisputed units in program is dcvclopcd undcr a UNiX cnvironmcnt on HP available workstation. text corpora, virtually all the current 14 million w"
C94-1088,J93-3004,0,\N,Missing
C94-1088,J90-1003,0,\N,Missing
C94-1088,O94-1005,1,\N,Missing
C96-2184,C92-1019,1,0.814526,"entable heuristic guidelines which deal with specific linguistic categories. Data uniformity is achieved by stratification of the standard itself and by defining a standard lexicon as part of the segmentation standard. I. Introduction One important feature of Chinese texts is that they are character-based, not wordbased. Each Chinese character stands for one phonological syllable and in most cases represents a morpheme. The fact that Chinese writing does not mark word boundaries poses the unique question of word segmentation in Chinese computational linguistics (e.g. Sproat and Shih 1990, and Chert and Liu 1992). Since words are the linguistically significant basic elements that are entered in the lexicon and manipulated by grammar rules, no language processing can be done unless words are identified. In theoretical terms, the primacy of the concept of word can be more firmly established if its existence can be empirically supported in a language that does not mark it conventionally in texts (e.g. Bates et al. 1993, Huang et al. 1993). In computational terms, no serious Chinese language processing can be done without segmentation. No efficient sharing of electronic resources or computational tools is"
C96-2184,O92-1003,0,0.105633,"Missing"
C96-2184,J96-3004,0,\N,Missing
C96-2184,J90-1003,0,\N,Missing
C96-2184,Y96-1018,1,\N,Missing
chou-huang-2006-hantology,O03-5003,0,\N,Missing
chou-huang-2006-hantology,huang-etal-2004-sinica,1,\N,Missing
chou-huang-2006-hantology,I05-7002,1,\N,Missing
chung-etal-2008-extracting,P94-1019,0,\N,Missing
chung-etal-2008-extracting,huang-etal-2004-sinica,1,\N,Missing
D16-1205,N09-1003,0,0.141123,"Missing"
D16-1205,J10-4006,1,0.880419,"sts that verbs activate expectations on their typical argument nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010). Experimental subjects seem to exploit a rich event knowledge to activate or inhibit dynamically the representations of the potential arguments. This phenomenon, generally referred to as thematic fit (McRae et al., 1998), supports the idea of a mental lexicon arranged as a web of mutual expectations. Some past works in computational linguistics (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Greenberg et al., 2015) modeled thematic fit estimations by means of dependencybased or of thematic roles-based DSMs. However, these semantic spaces are built similarly to traditional DSMs as they split verb arguments into separate vector dimensions. By using syntactic-semantic links, they encode the relation between an event and each of its participants, but they do not encode directly the relation between participants co-occurring in the same event. Another trend of studies in the NLP community aimed at the introduction of richer contextual features i"
D16-1205,J90-1003,0,0.591713,"ical association measure. 4 4.1 Evaluation Corpus and DSMs We trained our DSMs on the RCV1 corpus, which contains approximately 150 million words (Lewis et al., 2004). The corpus was tagged with the tagger described in Dell’Orletta (2009) and dependencyparsed with DeSR (Attardi et al., 2009). RCV1 was chosen for two reasons: i) to show that our joint context-based representation can deal with data 1969 sparseness even with a training corpus of limited size; ii) to allow a comparison with the results reported by Melamud et al. (2014). All DSMs adopt Positive Pointwise Mutual Information (PPMI; Church and Hanks (1990)) as a context weighting scheme and vary according to three main parameters: i) type of contexts; ii) number of dimensions; iii) application of Singular Value Decomposition (SVD; see Landauer et al. (1998)). For what concerns the first parameter, we developed three types of DSMs: a) traditional bag-ofwords DSMs, where the features are content words co-occurring with the target in a window of width 2; b) dependency-based DSMs, where the features are words in a direct dependency relation with the target; c) joint context-based DSMs, using the joint features described in the previous section. The"
D16-1205,N15-1003,0,0.0186207,"nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010). Experimental subjects seem to exploit a rich event knowledge to activate or inhibit dynamically the representations of the potential arguments. This phenomenon, generally referred to as thematic fit (McRae et al., 1998), supports the idea of a mental lexicon arranged as a web of mutual expectations. Some past works in computational linguistics (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Greenberg et al., 2015) modeled thematic fit estimations by means of dependencybased or of thematic roles-based DSMs. However, these semantic spaces are built similarly to traditional DSMs as they split verb arguments into separate vector dimensions. By using syntactic-semantic links, they encode the relation between an event and each of its participants, but they do not encode directly the relation between participants co-occurring in the same event. Another trend of studies in the NLP community aimed at the introduction of richer contextual features in DSMs, mostly based on word windows. The first example was the"
D16-1205,J15-4004,0,0.051174,"l. (2014) have proposed the Probabilistic Distributional Similarity (PDS), based on the intuition that two words, w1 and w2 , are similar if they are likely to occur in each other’s contexts. PDS assigns a high similarity score when both p(w1 |contexts of w2 ) and p(w2 | contexts of w1 ) are high. We tried to test variations of this measure with our representation, but we were not able to achieve satisfying results. Therefore, we report here only the scores with the cosine. 4.3 Datasets The DSMs are evaluated on two test sets: VerbSim (Yang and Powers, 2006) and the verb subset of SimLex-999 (Hill et al., 2015). The former includes 130 verb pairs, while the latter includes 222 verb pairs. Both datasets are annotated with similarity judgements, so we measured the Spearman correlation between them and the scores assigned by the model. The VerbSim dataset allows for comparison with Melamud et al. (2014), since they also evaluated their model on this test set, achieving a Spearman correlation score of 0.616 and outperforming all the baseline methods. The verb subset of SimLex-999, at the best of our knowledge, has never been used as a benchmark dataset for verb similarity. The SimLex dataset is known fo"
D16-1205,W11-0607,1,0.892111,"expectations on their typical argument nouns and vice versa (McRae et al., 1998; McRae et al., 2005) and nouns do the same with other nouns occurring as co-arguments in the same events (Hare et al., 2009; Bicknell et al., 2010). Experimental subjects seem to exploit a rich event knowledge to activate or inhibit dynamically the representations of the potential arguments. This phenomenon, generally referred to as thematic fit (McRae et al., 1998), supports the idea of a mental lexicon arranged as a web of mutual expectations. Some past works in computational linguistics (Baroni and Lenci, 2010; Lenci, 2011; Sayeed and Demberg, 2014; Greenberg et al., 2015) modeled thematic fit estimations by means of dependencybased or of thematic roles-based DSMs. However, these semantic spaces are built similarly to traditional DSMs as they split verb arguments into separate vector dimensions. By using syntactic-semantic links, they encode the relation between an event and each of its participants, but they do not encode directly the relation between participants co-occurring in the same event. Another trend of studies in the NLP community aimed at the introduction of richer contextual features in DSMs, mostl"
D16-1205,W14-1619,0,0.128219,"measure such proximity, even though other measures have been proposed (Weeds et al., 2004; Santus et al., 2016). Most of DSMs adopt a bag-of-words approach, that is they turn a text span (i.e., a word window or a parsed sentence) into a set of words and they register separately the co-occurrence of each word with a given target. The problem with this approach is that valuable information concerning word interrelations in a context gets lost, because words co-occurring with a target are treated as independent features. This is why works like Ruiz-Casado et al. (2005), Agirre et al. (2009) and Melamud et al. (2014) proposed to introduce richer contexts in distributional spaces, by using entire word windows as features. These richer contexts proved to be helpful to semantically represent verbs, which are characterized by highly context-sensitive meanings, and complex argument structures. In fact, two verbs may share independent words as features despite being very dissimilar from the semantic point of view. For instance kill and heal share the same object nouns in The doctor healed the patient and the The poison killed the patient, but are highly different if we consider their joint dependencies as a sin"
D16-1205,Y16-2021,1,0.794205,"pus. 1 Introduction Distributional Semantic Models (DSMs) rely on the Distributional Hypothesis (Harris, 1954; Sahlgren, 2008), stating that words occurring in similar contexts have similar meanings. On such theoretical grounds, word co-occurrences extracted from corpora are used to build semantic representations in the form of vectors, which have become very popular in the NLP community. Proximity between word vectors is taken as an index of meaning similarity, and vector cosine is generally adopted to measure such proximity, even though other measures have been proposed (Weeds et al., 2004; Santus et al., 2016). Most of DSMs adopt a bag-of-words approach, that is they turn a text span (i.e., a word window or a parsed sentence) into a set of words and they register separately the co-occurrence of each word with a given target. The problem with this approach is that valuable information concerning word interrelations in a context gets lost, because words co-occurring with a target are treated as independent features. This is why works like Ruiz-Casado et al. (2005), Agirre et al. (2009) and Melamud et al. (2014) proposed to introduce richer contexts in distributional spaces, by using entire word windo"
D16-1205,C04-1146,0,0.156904,"Missing"
D16-1205,N15-1050,0,\N,Missing
D17-1048,D14-1080,0,0.028703,"ness of cognition grounded data in building attention models. 2 Related works The basic task in sentiment analysis can be formulated as a classification problem. Class labels can either be binary (positive/negative) or polarity either as intensity by continuous values or as ratings in certain range such as 0 to 5 or 1 to 10, etc.. In recent years, deep learning based methods have greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (Socher et al., 2011), Recursive Neural Network (Socher et al., 2013), and Recurrent Neural Networks (Irsoy and Cardie, 2014). RNN naturally benefits sentiment classification because of its ability to capture sequential information in text. However, standard RNN suffers from the gradient vanishing problem (Bengio et al., 1994) where gradients may grow or decay exponentially over long sequences. To address this problem, Long-Short Term Memory model (LSTM) is introduced by adding a gated mechanism to keep long term memory. Each LSTM layer is generally followed by mean pool463 Eye-Tracking Corpus), and Mishra et al. (Mishra et al., 2016b) are considered high-quality resources (Kennedy, 2003; Cop et al., 2016; Mishra et"
D17-1048,P14-2007,0,0.234147,"Missing"
D17-1048,N13-1088,0,0.22577,"Missing"
D17-1048,D16-1009,0,0.014538,"formance of sentiment analysis without the need for labor intensive feature engineering. 1 462 https://en.wikipedia.org/wiki/Eye-tracking Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 462–471 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing and then feed into the next layer. Experiments in datasets which contain long documents and sentences demonstrate that the LSTM model outperforms the traditional RNN (Tang et al., 2015a,c). Not all words contribute equally to the semantics of a sentence (Hahn and Keller, 2016). Attention based neural networks are proposed to highlight their difference in contribution (Yang et al., 2016). In document level sentiment classification, both sentence level attention and document level attention are proposed. In the sentence level attention layer, an attention mechanism identifies words that are important. Those informative words are aggregated as attention weights to form sentence embedding representation. This method is generally called local context attention method. Similarly, some sentences can also be highlighted to indicate their importance in a document. Apart fro"
D17-1048,P16-2094,0,0.0714979,"Missing"
D17-1048,P11-1015,0,0.0615686,"re other aspects of attentions. Evaluations show the CBA based method outperforms the state-of-the-art local context based attention methods significantly. This brings insight to how cognition grounded data can be brought into NLP tasks. 1 Introduction Sentiment analysis is critical for many applications such as sentimental product recommendation (Dong et al., 2013), public opinion detection (Pang et al., 2008), and human-machine interaction (Clavel and Callejas, 2016), etc.Sentiment analysis has been well-explored (Pang et al., 2002; Vanzo et al., 2014; Tang et al., 2015a; Chen et al., 2016; Maas et al., 2011).Recently, deep learning based methods have further elevated the performance of sentiment analysis without the need for labor intensive feature engineering. 1 462 https://en.wikipedia.org/wiki/Eye-tracking Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 462–471 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing and then feed into the next layer. Experiments in datasets which contain long documents and sentences demonstrate that the LSTM model outperforms the traditional RNN (Tang et al., 2015a,c). Not al"
D17-1048,D16-1171,0,0.438258,"reated equal. Some words are more important than others in conveying the message in a sentence. Similarly, some sentences are more important than others in a document. Although the overall reading time as a cognitive process may reflect the syntax and discourse complexity, reading time of individual words is also an indicator of their semantic importance in text (Roseman, 2001; Demberg and Keller, 2008). Previous attention models are built using information embedded in text including users, products and text in local context for sentiment classification (Tang et al., 2015b; Yang et al., 2016; Chen et al., 2016; Gui et al., 2016). However, attention models using local context based text through distributional similarity lack theoretical foundation to reflect the cognitive basis. But, the key in sentiment analysis lies in its cognitive basis. Thus, we envision that cognition grounded data obtained in text reading should be helpful in building an attention model. In this paper, we propose a novel cognition based attention(CBA) model for sentiment analysis learned from cognition grounded eye-tracking data. Eye-tracking is the process of measuring either the point of gaze or the motion of an eye relativ"
D17-1048,P14-5010,0,0.00351223,"Missing"
D17-1048,P13-2062,0,0.00655233,"iment annotation complexity based on eye-tracking data. Mishra (2014) presents a cognitive study of sentiment detection from the perspective of AI where readers are tested as sentiment readers. Mishra (Mishra et al., 2016b) recently proposes a model in sentiment analysis and sarcasm detection by using eye-tracking data as a feature in addition to text features using Naive-Bayes and SVM classifiers. In other NLP tasks, Joshi (2013) shows that Word-Sense-Disambiguation can make use of simultaneous eye-tracking. Eye-tracking data are also used to measure the difficulty in translation annotation (Mishra et al., 2013). Barrett (2016) finds that gaze patterns during reading are strongly influenced by the role a word plays in terms of syntax, semantic, and discourse. Among different available eye-tracking datasets, the Dundee corpus, GECO (the Ghent We first build a regression model to map syntax, and context features of a word to its reading time based on eye-tracking data. We then apply the model to sentiment analysis text to obtain the estimated reading time of words at the sentence level. The estimated reading time can then be used as the attention weights in its context to build the attention layer in a"
D17-1048,W14-2623,0,0.0545921,"Missing"
D17-1048,K16-1016,0,0.226636,"5). However, they are not suited for other genre of text as userproduct information are not generally available. Attention models can be built not only from local text or user/product information but also from cognitive grounded data, especially eye-tracking data (Rayner, 1998; Allopenna et al., 1998). Joshi (2014) proposes a novel metric called Sentiment Annotation Complexity for measuring sentiment annotation complexity based on eye-tracking data. Mishra (2014) presents a cognitive study of sentiment detection from the perspective of AI where readers are tested as sentiment readers. Mishra (Mishra et al., 2016b) recently proposes a model in sentiment analysis and sarcasm detection by using eye-tracking data as a feature in addition to text features using Naive-Bayes and SVM classifiers. In other NLP tasks, Joshi (2013) shows that Word-Sense-Disambiguation can make use of simultaneous eye-tracking. Eye-tracking data are also used to measure the difficulty in translation annotation (Mishra et al., 2013). Barrett (2016) finds that gaze patterns during reading are strongly influenced by the role a word plays in terms of syntax, semantic, and discourse. Among different available eye-tracking datasets, t"
D17-1048,P14-1146,0,0.0753487,"mplicated deep learning models suffer from serious over-fitting problem. And the result of Deep learning model with word embedding initialization partly supports the fact that the reading time are more depend on the micro level syntax and semantic feature for the word, such as number of letters in word and complexity score of the word instead of the deep level context features. 4.2 Type Num Bool Bool Bool Bool Bool Bool Num Num Num Num Num Num • AvgWordvec — A SVM classifier that takes the average of word embeddings in Word2Vec as document embedding. Here is a list of Group 2 methods: • SSWE (Tang et al., 2014) — A SVM classifier using sentiment specific word embedding. • RNTN+RNN (Socher et al., 2013) — A Recursive Neural Tensor Network(RNTN) to represent sentences and trained using RNN. • Paragraph vector (Le and Mikolov, 2014) — A SVM classifier using document embedding as features. Comparison of different sentiment classification methods • LSTM+LA (Chen et al., 2016) — State-ofthe-art LSTM using local context as attention mechanism in both sentence level and docu5 ment level. https : //en.wikipedia.org/wiki/Coef f iciento fd etermination Because the features used in our model are all text based,"
D17-1048,W02-1011,0,0.0193148,"in documents. Different attention mechanisms can also be incorporated to capture other aspects of attentions. Evaluations show the CBA based method outperforms the state-of-the-art local context based attention methods significantly. This brings insight to how cognition grounded data can be brought into NLP tasks. 1 Introduction Sentiment analysis is critical for many applications such as sentimental product recommendation (Dong et al., 2013), public opinion detection (Pang et al., 2008), and human-machine interaction (Clavel and Callejas, 2016), etc.Sentiment analysis has been well-explored (Pang et al., 2002; Vanzo et al., 2014; Tang et al., 2015a; Chen et al., 2016; Maas et al., 2011).Recently, deep learning based methods have further elevated the performance of sentiment analysis without the need for labor intensive feature engineering. 1 462 https://en.wikipedia.org/wiki/Eye-tracking Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 462–471 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing and then feed into the next layer. Experiments in datasets which contain long documents and sentences demonstrate tha"
D17-1048,P10-1118,0,0.0148968,"ntains eye movement data from English and French newspapers (Kennedy, 2003). Measurements were taken while 10 participants read 20 newspaper articles. GECO is an English-Dutch bilingual corpus with eye-tracking data from 17 participants collected from reading the complete novel The Mysterious Affair at Styles. The corpus has 4,934 sentences, 774,015 tokens, and 9,876 words. The Mishra(Mishra et al., 2016a) dataset contains 994 text snippets with 383 positive and 611 negative examples from newspaper clippings, sampled from seven native speakers. To predict reading time using eye-tracking data, Tomanek et al. (2010) proposes a regression model using linguistic features related to syntax and semantics for calibration. Hahn (2016) proposes a novel approach to model both skipping and reading using unsupervised method which combines neural attention with auto-encoding trained on raw text using reinforcement learning. 3 Our proposed CBA model The basic idea of our method is to add a CBA model into a neural-network based LSTM sentiment classifier. Let D be a collection of documents. A document dk , dk ∈ D, has m number of sentences S1 , S2 , ...Sj , ..., Sm . A sentence Sj is formed by a sequence of words Sj ="
D17-1048,C14-1221,0,0.0609746,"Missing"
D17-1048,D14-1162,0,0.0969541,"Missing"
D17-1048,D16-1172,0,0.244785,"s the best performer. LSTM+LA (2016), which is the state-of-the-art method, uses local attention mechanism to improve performance significantly. Among our CBA based variations, using the GECO dataset gives the best result outperforming LSTM+LA in all three datasets. LSTM+CBAG has significant improvement over LSTM+LA with p values of p &lt; 0.016 on IMDB, p &lt; 0.0019 on Yelp 13, and p &lt; 0.00023 on Yelp 14. LSTM+CBAG has the best result compared to the other two variations because GECO has larger participant size. Its text genre is also closer to the review datasets for sentiment analysis. • CLSTM (Xu et al., 2016) — Cached LSTM to capture the overall semantic information in long text. The two variations include regular CLSTM and bi-directional B-CLSTM. • LSTM+UPA (Chen et al., 2016) — Stateof-the-art LSTM including LA as well as user/product as attention mechanism at both sentence level and document level. Our proposed CBA model has several variations as explained below. • LSTM+CBA — The LSTM classifier using only CBA model at sentence level and document level. Based on the three eye-tracking datasets(GECO, DUNDEE and Mishra’s) for reading time prediction, we label the same model by different training"
D17-1048,N16-1174,0,0.380427,"not all words are created equal. Some words are more important than others in conveying the message in a sentence. Similarly, some sentences are more important than others in a document. Although the overall reading time as a cognitive process may reflect the syntax and discourse complexity, reading time of individual words is also an indicator of their semantic importance in text (Roseman, 2001; Demberg and Keller, 2008). Previous attention models are built using information embedded in text including users, products and text in local context for sentiment classification (Tang et al., 2015b; Yang et al., 2016; Chen et al., 2016; Gui et al., 2016). However, attention models using local context based text through distributional similarity lack theoretical foundation to reflect the cognitive basis. But, the key in sentiment analysis lies in its cognitive basis. Thus, we envision that cognition grounded data obtained in text reading should be helpful in building an attention model. In this paper, we propose a novel cognition based attention(CBA) model for sentiment analysis learned from cognition grounded eye-tracking data. Eye-tracking is the process of measuring either the point of gaze or the motio"
D17-1048,D11-1014,0,0.106007,"ur method outperforms other state-of-the-art methods significantly. This work validates the effectiveness of cognition grounded data in building attention models. 2 Related works The basic task in sentiment analysis can be formulated as a classification problem. Class labels can either be binary (positive/negative) or polarity either as intensity by continuous values or as ratings in certain range such as 0 to 5 or 1 to 10, etc.. In recent years, deep learning based methods have greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (Socher et al., 2011), Recursive Neural Network (Socher et al., 2013), and Recurrent Neural Networks (Irsoy and Cardie, 2014). RNN naturally benefits sentiment classification because of its ability to capture sequential information in text. However, standard RNN suffers from the gradient vanishing problem (Bengio et al., 1994) where gradients may grow or decay exponentially over long sequences. To address this problem, Long-Short Term Memory model (LSTM) is introduced by adding a gated mechanism to keep long term memory. Each LSTM layer is generally followed by mean pool463 Eye-Tracking Corpus), and Mishra et al."
D17-1048,D13-1170,0,0.0222159,"hods significantly. This work validates the effectiveness of cognition grounded data in building attention models. 2 Related works The basic task in sentiment analysis can be formulated as a classification problem. Class labels can either be binary (positive/negative) or polarity either as intensity by continuous values or as ratings in certain range such as 0 to 5 or 1 to 10, etc.. In recent years, deep learning based methods have greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (Socher et al., 2011), Recursive Neural Network (Socher et al., 2013), and Recurrent Neural Networks (Irsoy and Cardie, 2014). RNN naturally benefits sentiment classification because of its ability to capture sequential information in text. However, standard RNN suffers from the gradient vanishing problem (Bengio et al., 1994) where gradients may grow or decay exponentially over long sequences. To address this problem, Long-Short Term Memory model (LSTM) is introduced by adding a gated mechanism to keep long term memory. Each LSTM layer is generally followed by mean pool463 Eye-Tracking Corpus), and Mishra et al. (Mishra et al., 2016b) are considered high-quali"
D17-1048,D15-1167,0,0.465263,"nt analysis because not all words are created equal. Some words are more important than others in conveying the message in a sentence. Similarly, some sentences are more important than others in a document. Although the overall reading time as a cognitive process may reflect the syntax and discourse complexity, reading time of individual words is also an indicator of their semantic importance in text (Roseman, 2001; Demberg and Keller, 2008). Previous attention models are built using information embedded in text including users, products and text in local context for sentiment classification (Tang et al., 2015b; Yang et al., 2016; Chen et al., 2016; Gui et al., 2016). However, attention models using local context based text through distributional similarity lack theoretical foundation to reflect the cognitive basis. But, the key in sentiment analysis lies in its cognitive basis. Thus, we envision that cognition grounded data obtained in text reading should be helpful in building an attention model. In this paper, we propose a novel cognition based attention(CBA) model for sentiment analysis learned from cognition grounded eye-tracking data. Eye-tracking is the process of measuring either the point"
D17-1048,P15-1098,0,0.122497,"nt analysis because not all words are created equal. Some words are more important than others in conveying the message in a sentence. Similarly, some sentences are more important than others in a document. Although the overall reading time as a cognitive process may reflect the syntax and discourse complexity, reading time of individual words is also an indicator of their semantic importance in text (Roseman, 2001; Demberg and Keller, 2008). Previous attention models are built using information embedded in text including users, products and text in local context for sentiment classification (Tang et al., 2015b; Yang et al., 2016; Chen et al., 2016; Gui et al., 2016). However, attention models using local context based text through distributional similarity lack theoretical foundation to reflect the cognitive basis. But, the key in sentiment analysis lies in its cognitive basis. Thus, we envision that cognition grounded data obtained in text reading should be helpful in building an attention model. In this paper, we propose a novel cognition based attention(CBA) model for sentiment analysis learned from cognition grounded eye-tracking data. Eye-tracking is the process of measuring either the point"
huang-etal-2004-sinica,W02-1106,1,\N,Missing
huang-etal-2008-quality,W02-1811,0,\N,Missing
huang-etal-2008-quality,W03-1709,0,\N,Missing
huang-etal-2008-quality,ma-huang-2006-uniform,1,\N,Missing
huang-etal-2008-quality,W03-1730,0,\N,Missing
huang-etal-2008-quality,J03-3001,0,\N,Missing
I05-3007,O98-3004,1,0.821376,"inese VerbNet Project: that ren.wei ᇡࣁ‘to quotation. Yi.wei and jue.de, on the other hand, can only be used in reportage and cannot think’ behaves most like biao.shi ߄ Ң ‘to introduce direct quotation. express, to state’ (salience 0.451), while yi.wei а Distinction between near synonymous pairs ࣁ ‘to take somebody/something as’ is more like can be obtained from Sketch Difference. This jue.de ள ‘to feel, think’ (salience 0.488). The function is verified with results from Tsai et al.’s study on gao.xing ଯᑫ ‘glad’ and kuai.le ז! synonymous relation can be illustrated by (4) and (5). ‘happy’ (Tsai et al., 1998). Gao.xing ‘glad’ 4a. дᇡࣁੇډѦၗԖঁᢀࡐۺख़ाǴ൩ा specific patterns include the negative imperative bie ձ ‘don’t’. It also has a dominant collocation ޕၰӦޑၯᔍೕ߾Ǵௗڙ೭٤చҹǶ ta ren.wei dao hai.wai tou.zi you yi ge guan.nian with the potentiality complement marker de ள hen zhong.yao, jiu shi yao zhi.dao dang.di de (e.g. ta gao.xing de you jiao you tiao Ӵଯᑫள you.xi gui.ze ΞћΞၢ ‘she was so happy that she cried and ‘He believes that for those investing overseas, danced’). In contrast, kuai.le ‘happy’ has the there is a very important principle-one must know specific collocation with holiday nouns s"
I05-3007,xia-etal-2000-developing,0,0.0096387,"n a list of types and distribution of the keyword’s syntactic category. In addition, users can find possible collocations of the keyword from the output of Mutual Information (MI). The most salient grammatical information, such as grammatical functions (subject, object, adjunct etc.) is beyond the scope of the traditional corpus interface tools. Traditional corpora rely on the human users to arrive at these kinds of generalizations. 3. Sketch Engine: A New Corpus-based approach to Grammatical Information Several existing linguistically annotated corpus of Chinese, e.g. Penn Chinese Tree Bank (Xia et al., 2000), Sinica Treebank (Chen et al., 2003), Proposition Bank (Xue and Palmer, 2003, 2005) and Mandarin VerbNet (Wu and Liu, 50 Linguistic Knowledge Net anchored by a lexicon (Huang et al., 2001). A Word Sketch is a one-page list of a Information (MI) to measure the salience of a keyword’s functional distribution and collocation Engine output. MI provides a measure of the in the corpus. The functional distribution degree of association of a given segment with includes: subject, object, prepositional object, others. Pointwise MI, calculated by Equation 3, and modifier. Its collocations are described"
I05-3007,C90-2010,1,\N,Missing
I05-3007,O97-4003,1,\N,Missing
I05-3014,huang-etal-2004-sinica,1,0.782754,"Missing"
I05-4007,W02-1106,1,0.730304,"). Low-density languages, as opposed to high-density languages, usually refer to languages that are not spoken by a large number of people. However, there is neither a direct correspondence between language population and language technology, nor an objective population number that defines density level. In this work, we use the availability of language resources instead to define language density. That is, low-density languages are languages that do not have enough language resources to support fully automated language processing, such as machine translation. In our current line of work, we (Huang et al. 2002) refer to low-density languages as those which do not have enough existing resources for semi-automatic construction of monolingual wordnet. There are two alternative approaches to build parallel wordnets, as shown in Figure 1. The first approach relies on two fully annotated monolingual wordnets with synsets and LSR’s. The second approach requires only one fully annotated WN in addition to LSR-based cross-lingual translation correspondences. Abstract Parallel wordnets built upon correspondences between different languages can play a crucial role in multilingual knowledge processing. Since the"
I05-7003,huang-etal-2004-sinica,1,\N,Missing
I08-1052,C00-1014,0,0.7112,"Missing"
I08-1052,C94-1091,1,0.807876,"Missing"
I13-1096,Y96-1018,1,0.134102,"Information. 4 Precision 0.9429 0.9223 0.8876 Recall 0.8009 0.7898 0.8272 F1 0.8661 0.8509 0.8564 Table 3: Performance of the rule system on time entity extraction. Experiments Pattern month-xun month-day-dp-hour month-day-dp regnalyear-month-day month-day refday-dp year-month-day regnalyear-month refyear-month day-dp-hour dp-hour regnalyear refyear-month-day day-dp refday-dp-hour year-month year-season refday-dp-hour-minute century-periodphase season refday refyear century month dp-hour-minute year decade weekday day hour-minute hour refyear-periodphase We use two different corpora: Sinica (Chen et al., 1996) and TempEval-2 from SemEval-2010 competition (Pustejovsky and Verhagen., 2009). Sinica Corpus contains 10M words and the total number of time entity is 88K as shown in Table 2. The time words are tagged as ‘Nd’. However, there is no entity information. So, when an entity is recognized by our system, we first separate it into elements and then calculate the performance. Durations are labeled as number + classif ier in Sinica, which are not time words. So, we don’t recognize durations in Sinica. For regnal year system, we only include a list of emperors of the Qing dynasty. We don’t deal with f"
I13-1096,pustejovsky-etal-2010-iso,0,0.012964,"lty of Humanities The Hong Kong Polytechnic University The Hong Kong Polytechnic University hongz.xu@gmail.com churenhuang@gmail.com Abstract valid temporal expression is a path from one certain node to another node. The longer the path is, the more confident the recognition will be. CTEMP (Wu et al., 2005) also used linguistic rules for Chinese temporal entity recognition. However, the focus of this work differs from them in that we aims to identify Chinese time entities which could be described with a limited set of rules and can be easily translated into a structured format, such as TIMEX3(Pustejovsky et al., 2010) standard. For this part, the set of rules in this work are more comprehensive than (Wu et al., 2005). However, we don’t include events that are used as time entities, since events intrinsically are not time entities. According to the Generative Lexicon Theory (Pustejovsky, 1995), this is a case of type coercion. In Section 2, we will give a linguistic study on Chinese time entity expressions. In Section 3, we will construct a rule system which is mainly based on our linguistic study. In Section 4, we test rule system on Sinica and TempEval-2 corpora and give a discussion on the experimental r"
I13-1096,I05-1061,0,0.03175,"churenhuang@gmail.com Abstract valid temporal expression is a path from one certain node to another node. The longer the path is, the more confident the recognition will be. CTEMP (Wu et al., 2005) also used linguistic rules for Chinese temporal entity recognition. However, the focus of this work differs from them in that we aims to identify Chinese time entities which could be described with a limited set of rules and can be easily translated into a structured format, such as TIMEX3(Pustejovsky et al., 2010) standard. For this part, the set of rules in this work are more comprehensive than (Wu et al., 2005). However, we don’t include events that are used as time entities, since events intrinsically are not time entities. According to the Generative Lexicon Theory (Pustejovsky, 1995), this is a case of type coercion. In Section 2, we will give a linguistic study on Chinese time entity expressions. In Section 3, we will construct a rule system which is mainly based on our linguistic study. In Section 4, we test rule system on Sinica and TempEval-2 corpora and give a discussion on the experimental result. Section 5 is the conclusion. Chinese time entity is quite complex. In this paper, we give a co"
I13-1096,S10-1010,0,\N,Missing
I17-2043,D15-1167,0,0.0596751,"Missing"
I17-2043,C08-1006,0,0.0298992,"Missing"
I17-2043,P17-2067,0,0.208105,"ltime/2011/03/17/fearingradiation-chinese-rush-to-buy-table-salt/ 2 252 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 252–256, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP dicate the credibility of a piece of news. For example, a conservative might neglect the impact of climate change, while a progressive might exaggerate inequality. On some occasions, it is hard to make fake claims like congressional inquiry. But in other cases, the speaker tend to exaggerate facts like the campaign rally. For the study use profile information, Wang (2017) proposes a hybrid CNN model to detect fake news, which uses speaker profiles as a part of the input data. Wang (2017) also made the first large scale fake news detection benchmark dataset with speaker information such as party affiliation, location of speech, job title, credit history as well as topic. The Long-Short memory network (LSTM), as a neural network model, is proven to work better for long sentences (Tang et al., 2015). Attention models are also proposed to weigh the importance of different words in their context. Current attention models are either based on local semantic attention"
I17-2043,N16-1174,0,0.0150442,"oft-max weight for each label. where W 3 254 http://www.politifact.com/truth-o-meter/ The performance of four baseline models are shown in Table 2 including a simple majority model, the LSTM model without using profile information, the hybrid CNN model proposed by Wang (2017) without profile information(CNNWang), and the hybrid CNN model by Wang with profile information(CNN-WangP). Note that firstly, LSTM without profile does not perform better than CNN-Wang. However, other studies show that when attention model is incorporated, LSTM generally outperforms that of CNN model (Chen et al., 2016; Yang et al., 2016) which will be shown later. Secondly, CNNWangP, which uses speaker profiles has the best performance. For word representation, we train the skip-gram word embedding (Mikolov et al., 2013) on each dataset separately to initialize the word vectors. All embedding sizes on the model are set to N = 200, a commonly used size. In speaker profiles, there are four basic attributes: party affiliation(Pa), location of speech(La), job title of speaker(Ti), and credit history(Ch) which counts the inaccurate statements for speaker in past speeches. Note that credit history is not a commonly available data."
I17-2043,D16-1171,0,0.0136344,"models ~ l is the soft-max weight for each label. where W 3 254 http://www.politifact.com/truth-o-meter/ The performance of four baseline models are shown in Table 2 including a simple majority model, the LSTM model without using profile information, the hybrid CNN model proposed by Wang (2017) without profile information(CNNWang), and the hybrid CNN model by Wang with profile information(CNN-WangP). Note that firstly, LSTM without profile does not perform better than CNN-Wang. However, other studies show that when attention model is incorporated, LSTM generally outperforms that of CNN model (Chen et al., 2016; Yang et al., 2016) which will be shown later. Secondly, CNNWangP, which uses speaker profiles has the best performance. For word representation, we train the skip-gram word embedding (Mikolov et al., 2013) on each dataset separately to initialize the word vectors. All embedding sizes on the model are set to N = 200, a commonly used size. In speaker profiles, there are four basic attributes: party affiliation(Pa), location of speech(La), job title of speaker(Ti), and credit history(Ch) which counts the inaccurate statements for speaker in past speeches. Note that credit history is not a commo"
I17-2043,I13-1039,0,0.182189,"Missing"
K17-1006,2014.lilt-9.5,0,0.0215713,"nize metaphoric phrases. Zhou et al. (2011) use the Maximum Entrophy model to detect the metaphoric reading of verb phrases based on collocation with noun phrases, and point out that there is no mature syntactic and semantic tool for metaphor analysis in Chinese. Our study will close the gap by building a model of metaphor detection based on syntactic conditions. Regarding metaphor detection, most papers emphasize on distinguishing metaphoric senses from literal senses in a polysemy network. Disambiguation of senses has been handled by DSMs based on the availability of contextual information (Baroni et al., 2014; Boleda et al., 2012; Erk and Padó, 2010; Kartsaklis and Sadrzadeh 2013). When more contextual information is incorporated, disambiguation would be more successful. It should be noted that the senses of one form have different degrees of transparency to be traced in semantics. The senses of a form which can be chained together via overlapping semantics, as in the case of polysemy (cut a new window in the wall vs. the ball broke a window), are more likely to be traced. On the contrary, when the senses of a linguistic form are discrete as in the case of homonymy (e.g. piano keys vs. key point),"
K17-1006,W06-3506,0,0.0492086,"d Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in both manually-tagged linguistic resources (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Broadwell et al., 2013) and corpus-based approach (Birke and Sarker, 2007; Shutova et al., 2013; Neuman et al., 2013; Hovy et al., 2013). Metaphor detection in Chinese is at the incipient stage. Fu et al., (2016) uses hierarchical clustering https://en.wikipedia.org/wiki/Baidu_Baike 37 for Chinese noun phrases according to their contextual information to recognize metaphoric phrases. Zhou et al. (2011) use the Maximum Entrophy model to detect the metaphoric reading of verb phrases based on collocation with noun phrases, and point out that there is no mature synt"
K17-1006,P14-1023,0,0.00966636,"nize metaphoric phrases. Zhou et al. (2011) use the Maximum Entrophy model to detect the metaphoric reading of verb phrases based on collocation with noun phrases, and point out that there is no mature syntactic and semantic tool for metaphor analysis in Chinese. Our study will close the gap by building a model of metaphor detection based on syntactic conditions. Regarding metaphor detection, most papers emphasize on distinguishing metaphoric senses from literal senses in a polysemy network. Disambiguation of senses has been handled by DSMs based on the availability of contextual information (Baroni et al., 2014; Boleda et al., 2012; Erk and Padó, 2010; Kartsaklis and Sadrzadeh 2013). When more contextual information is incorporated, disambiguation would be more successful. It should be noted that the senses of one form have different degrees of transparency to be traced in semantics. The senses of a form which can be chained together via overlapping semantics, as in the case of polysemy (cut a new window in the wall vs. the ball broke a window), are more likely to be traced. On the contrary, when the senses of a linguistic form are discrete as in the case of homonymy (e.g. piano keys vs. key point),"
K17-1006,P16-1018,0,0.061043,"Missing"
K17-1006,W13-0908,0,0.0157053,"ormation by radicals increases both the precision and the recall in metaphor detection. Although this approach is especially effective for Chinese because of the information embedded in radicals, broader implications include the possibility of leveraging eventive information from different sources in other languages. 2 Related Work The task of metaphor detection has been handled in a wide variety of approaches including clustering models (Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010), semantic similarity graphs (Sporleder and Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in both manually-tagged linguistic resources (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Broadwell et"
K17-1006,W13-0907,0,0.0665575,"rimarily rely on contextual information. This study provides a novel approach to detect and classify metaphors by analyzing eventive information. Concepts can be classified into a wide array of event types according to ontology, the organization of knowledge (Huang et al., 2007). Eventive information thus can be applied to the classification of metaphors, which concern mappings of conceptual structures from a source domain to a target domain. The classification of metaphoric and literal senses has been approached by different methods such as vector-space models with distributional statistics (Hovy et al., 2013; Tsvetkov et al., 2014) and compositional distributional semantic models (CDSMs) (Kartsaklis and Sadrzadeh, 2013a). Most of the studies regarding metaphoric detection have been done in English, while the task in Chinese is at the incipient stage. The relevant studies such as clustering models and similarity computation in context (Fu et al., 2016; Wang, 2010) mainly focus on the metaphoric sense of each individual noun or adjectival phrase because the analyses are highly dependent on contextual information. However, metaphoric senses of verbs are less touched because it is difficult to define"
K17-1006,D12-1112,0,0.0149991,"es. Zhou et al. (2011) use the Maximum Entrophy model to detect the metaphoric reading of verb phrases based on collocation with noun phrases, and point out that there is no mature syntactic and semantic tool for metaphor analysis in Chinese. Our study will close the gap by building a model of metaphor detection based on syntactic conditions. Regarding metaphor detection, most papers emphasize on distinguishing metaphoric senses from literal senses in a polysemy network. Disambiguation of senses has been handled by DSMs based on the availability of contextual information (Baroni et al., 2014; Boleda et al., 2012; Erk and Padó, 2010; Kartsaklis and Sadrzadeh 2013). When more contextual information is incorporated, disambiguation would be more successful. It should be noted that the senses of one form have different degrees of transparency to be traced in semantics. The senses of a form which can be chained together via overlapping semantics, as in the case of polysemy (cut a new window in the wall vs. the ball broke a window), are more likely to be traced. On the contrary, when the senses of a linguistic form are discrete as in the case of homonymy (e.g. piano keys vs. key point), they may be problema"
K17-1006,chou-huang-2006-hantology,1,0.791552,"Missing"
K17-1006,P10-2017,0,0.0116245,") use the Maximum Entrophy model to detect the metaphoric reading of verb phrases based on collocation with noun phrases, and point out that there is no mature syntactic and semantic tool for metaphor analysis in Chinese. Our study will close the gap by building a model of metaphor detection based on syntactic conditions. Regarding metaphor detection, most papers emphasize on distinguishing metaphoric senses from literal senses in a polysemy network. Disambiguation of senses has been handled by DSMs based on the availability of contextual information (Baroni et al., 2014; Boleda et al., 2012; Erk and Padó, 2010; Kartsaklis and Sadrzadeh 2013). When more contextual information is incorporated, disambiguation would be more successful. It should be noted that the senses of one form have different degrees of transparency to be traced in semantics. The senses of a form which can be chained together via overlapping semantics, as in the case of polysemy (cut a new window in the wall vs. the ball broke a window), are more likely to be traced. On the contrary, when the senses of a linguistic form are discrete as in the case of homonymy (e.g. piano keys vs. key point), they may be problematic to DSM (Baroni e"
K17-1006,D13-1166,0,0.0368647,"Missing"
K17-1006,W13-3513,0,0.0224189,"Missing"
K17-1006,P14-1024,0,0.19485,"ntextual information. This study provides a novel approach to detect and classify metaphors by analyzing eventive information. Concepts can be classified into a wide array of event types according to ontology, the organization of knowledge (Huang et al., 2007). Eventive information thus can be applied to the classification of metaphors, which concern mappings of conceptual structures from a source domain to a target domain. The classification of metaphoric and literal senses has been approached by different methods such as vector-space models with distributional statistics (Hovy et al., 2013; Tsvetkov et al., 2014) and compositional distributional semantic models (CDSMs) (Kartsaklis and Sadrzadeh, 2013a). Most of the studies regarding metaphoric detection have been done in English, while the task in Chinese is at the incipient stage. The relevant studies such as clustering models and similarity computation in context (Fu et al., 2016; Wang, 2010) mainly focus on the metaphoric sense of each individual noun or adjectival phrase because the analyses are highly dependent on contextual information. However, metaphoric senses of verbs are less touched because it is difficult to define regularities of their c"
K17-1006,P10-1071,0,0.057361,"Missing"
K17-1006,C10-1113,0,0.0622662,"Missing"
K17-1006,E09-1086,0,0.024469,"cting metaphors. The approach of leveraging event type information by radicals increases both the precision and the recall in metaphor detection. Although this approach is especially effective for Chinese because of the information embedded in radicals, broader implications include the possibility of leveraging eventive information from different sources in other languages. 2 Related Work The task of metaphor detection has been handled in a wide variety of approaches including clustering models (Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010), semantic similarity graphs (Sporleder and Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in both manually-tagged linguistic resources (Gedigian et"
K17-1006,Q15-1016,0,0.0524555,"Missing"
K17-1006,W13-0909,0,0.02709,"Missing"
K17-1006,P10-1116,0,0.0126172,"ng event type information by radicals increases both the precision and the recall in metaphor detection. Although this approach is especially effective for Chinese because of the information embedded in radicals, broader implications include the possibility of leveraging eventive information from different sources in other languages. 2 Related Work The task of metaphor detection has been handled in a wide variety of approaches including clustering models (Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010), semantic similarity graphs (Sporleder and Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in both manually-tagged linguistic resources (Gedigian et al., 2006; Krishnakumaran and Zh"
K17-1006,W13-0906,0,0.0184013,"Work The task of metaphor detection has been handled in a wide variety of approaches including clustering models (Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010), semantic similarity graphs (Sporleder and Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in both manually-tagged linguistic resources (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Broadwell et al., 2013) and corpus-based approach (Birke and Sarker, 2007; Shutova et al., 2013; Neuman et al., 2013; Hovy et al., 2013). Metaphor detection in Chinese is at the incipient stage. Fu et al., (2016) uses hierarchical clustering https://en.wikipedia.org/wiki/Baidu_Baike 37 for Chinese noun phrases according to their contextual i"
K17-1006,N10-1039,0,0.0144066,"ove the effectiveness of eventive information in detecting metaphors. The approach of leveraging event type information by radicals increases both the precision and the recall in metaphor detection. Although this approach is especially effective for Chinese because of the information embedded in radicals, broader implications include the possibility of leveraging eventive information from different sources in other languages. 2 Related Work The task of metaphor detection has been handled in a wide variety of approaches including clustering models (Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010), semantic similarity graphs (Sporleder and Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in b"
K17-1006,W13-0904,0,0.0160985,"ent sources in other languages. 2 Related Work The task of metaphor detection has been handled in a wide variety of approaches including clustering models (Birke and Sarkar, 2006; Shutova et al., 2010; Li and Sporleder, 2010), semantic similarity graphs (Sporleder and Li, 2009), topic modeling (Li et al., 2010; Heintz et al., 2013), and compositional distributional semantic models (CDSMs) (Gutiérrez et al. 2016). Feature-based classification, in particular, attracts most attention since a wide array of contextual information is included (Sporleder and Li, 2009; Dunn., 2013; Hovy et al., 2011; Mohler et al., 2013; Neuman et al., 2013; Tsvetkov et al., 2013; Tsvetkov et al., 2014). Since the studies regarding metaphor identification have primarily focused on English, there are more available datasets in English in both manually-tagged linguistic resources (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Broadwell et al., 2013) and corpus-based approach (Birke and Sarker, 2007; Shutova et al., 2013; Neuman et al., 2013; Hovy et al., 2013). Metaphor detection in Chinese is at the incipient stage. Fu et al., (2016) uses hierarchical clustering https://en.wikipedia.org/wiki/Baidu_Baike 37 for Chinese"
K17-1006,W07-0103,0,\N,Missing
K17-1006,E06-1042,0,\N,Missing
L16-1360,J04-1002,0,0.14253,"Missing"
L16-1722,W11-2501,1,0.38469,"Missing"
L16-1722,E12-1004,0,0.22534,"Missing"
L16-1722,W09-0215,0,0.0557322,"Missing"
L16-1722,C92-2082,0,0.209128,"Missing"
L16-1722,P13-2078,0,0.00781986,"Missing"
L16-1722,N15-1098,0,0.476188,"Missing"
L16-1722,E14-1054,0,0.0390832,"Missing"
L16-1722,C14-1097,0,0.715404,"Missing"
L16-1722,E14-4008,1,0.754471,"close hypernym, which are therefore attributionally similar (Weeds et al., 2014). The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The fe"
L16-1722,Y14-1018,1,0.121991,"ntiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution (Santus et al., 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011). The ablation test has show"
L16-1722,W15-4208,1,0.746309,"s et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly extracted from EVALution (Santus et al., 2015) 1 , Lenci/Benotto (Benotto, 2015) and BLESS (Baroni and Lenci, 2011). The ablation test has shown that four out of thirteen feat"
L16-1722,C08-1107,0,0.029533,"Missing"
L16-1722,W03-1011,0,0.807271,"ting hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have claimed that these methods may learn whether a term y is a prototypical hypernym, regardless of its actual relation with a term x. In this paper we further investigate and revise ROOT13 (Santus et al., 2016b), a supervised method based on a Random Forest algorithm and thirteen corpus-based features. The feature contribution is evaluated with an ablation test, using a 10-fold cross validation on 9,600 pairs randomly"
L16-1722,C14-1212,0,0.148115,"ymy in fact represents a key organization principle of semantic memory (Murphy, 2002), the backbone of taxonomies and ontologies, and one of the crucial semantic relations supporting lexical entailment (Geffet and Dagan, 2005). Co-hyponymy (or coordination) is instead the relation held by words sharing a close hypernym, which are therefore attributionally similar (Weeds et al., 2014). The ability of discriminating hypernymy, co-hyponymy and random words has potentially infinite applications, including Automatic Thesauri Creation, Paraphrasing, Textual Entailment, Sentiment Analysis and so on (Weeds et al., 2014; Tungthamthiti et al. 2015). For this reason, in the last decades, numerous methods, datasets and shared tasks have been proposed to improve computers’ ability in such discrimination, generally achieving promising results (Santus et al., 2016b; Roller et al., 2014, Weeds et al., 2014; Santus et al. 2014a; Rimmel, 2014; Lenci and Benotto, 2012; Kotlerman et al., 2010; Geffet and Dagan, 2005; Weeds and Weir, 2003). Both supervised and unsupervised approaches have been investigated. The former have been shown to outperform the latter in Weeds et al. (2014), even though Levy et al. (2015) have cl"
L16-1722,Y15-1021,1,0.87055,"Missing"
L16-1723,W11-2501,1,0.863897,"Missing"
L16-1723,J90-1003,0,0.489356,"ty measures play a fundamental role in tasks such as Information Retrieval (IR), Text Classification (TC), Text Summarization (TS), Question Answering (QA), Sentiment Analysis (SA), and so on (Terra and Clarke, 2003; Tungthamthiti et al., 2015). They can be either knowledge-based or corpus-based (Gomaa and Fahmy, 2013). The former rely on lexicons or semantic networks, such as WordNet (Fellbaum, 1998), measuring the distance between the nodes in the network. The latter, instead, compute the similarity between words relying on statistical information about their distributions in large corpora (Church and Hanks, 1990). Knowledge based approaches generally exploit hand-crafted resources. While being hand-crafted ensures high quality, it also entails arbitrariness and high development and update costs. This is the main reason why these resources are known for their limited coverage (Santus et al., 2015b). Such limitation has often prompted researchers to pursue hybrid approaches (Turney, 2001). A key assumption of corpus-based approaches is that similarity between words can be measured by looking at words co-occurrences. In particular, following the Distributional Hypothesis (Harris, 1954; Firth, 1957), thes"
L16-1723,C92-2082,0,0.177797,"approaches generally exploit the Distributional Hypothesis, according to which words that occur in similar contexts also have similar meanings (Harris, 1954). Although these approaches extract statistics from large corpora, they vary in the way they define what has to be considered context (i.e. lexical context, syntactic context, documents, etc.), how the association with such context is measured (e.g. frequency of co-occurrence, association measures like Pointwise Mutual Information, etc.), and how the association with the contexts is used to identify the similarity (Terra and Clarke, 2003; Hearst, 1992; Santus et al., 2014a; Santus et al., 2014b; Santus et al., 2016a). A common way to represent word meaning in NLP is by using vectors to encode the association between the target words and their contexts. The resulting vector space is generally referred as Vector Space Model (VSM) or, more specifically, as Distributional Semantic Model (DSM). In such vector space, word similarity can be calculated by using the Vector Cosine, which measures the angle between the vectors (Turney and Pantel, 2010). Other measures – such as Manhattan Distance, Dice’s Coefficient, Euclidean Distance, Jaccard Simil"
L16-1723,Q15-1016,0,0.605479,"ctor Cosine is generally considered to be the optimal choice (Bullinaria and Levy 2007). Another common way to represent word meaning is using word embeddings, which are vector-space word representations that are implicitly learned by the input-layer weights of neural networks. These models have shown a strong ability to capture synonymy and analogies (such as in the famous “King - Man + Woman = Queen” example, where Mikolov et al. (2013) subtract the vector of “Man” from the one of “King”, and then add the vector of “Woman”, obtaining a very similar vector to the one of “Queen”), even though Levy et al. (2015) have claimed that traditional count-based DSMs can achieve the same results if their hyperparameters are properly optimized. A well-known problem with the distributional approaches is that they rely on a very loose definition of similarity. In fact, vectors have as nearest neighbours not only synonyms, but also hypernyms, co-hyponyms, antonyms, as well as a wide range of other semantically related items (Santus et al., 2015). For this reason, several datasets have been proposed by the NLP community to test distributional similarity measures. Among the most common ones, there are the English a"
L16-1723,J07-2002,0,0.0420041,"Missing"
L16-1723,2003.mtsummit-papers.42,0,0.263036,"Missing"
L16-1723,W15-4208,1,0.935386,"Missing"
L16-1723,E14-4008,1,0.92777,"Missing"
L16-1723,N03-1032,0,0.173981,"Missing"
L16-1723,Y15-1021,1,0.830439,"Missing"
L16-1723,C08-1114,0,0.186322,"Missing"
L16-1726,N09-1003,0,0.0438364,"among different relationships between words (Santus et al., 2014c). In recent years, DSMs have been adopted in several semantic tasks, including the identification of semantic relatedness and similarity. The former task is concerned with whether two words are related or not, independently from their paradigmatic similarity. The identification of semantic similarity, instead, is a more specific task and it consists in identifying words that are paradigmatically related (Sun, 2014; Murphy, 2003). DSMs have successfully addressed these two tasks by using vector cosine as a measure of similarity (Agirre et al., 2009). Unfortunately, however, DSMs are not yet able to discriminate the several semantic relations that exist between words (Santus et al., 2014c). For example, a word such as ⿓ (long, dragon) can be in relation with several other words: 1. Antonymy: ⿓ (long, dragon) vs. 鳳 1 (feng, phoenix) 2. Hypernymy: ⿓ (long, dragon) vs. (shuizhongshengwu, aquatic life) ⽔中⽣物 3. Nearsynonymy 2 : ⿓ (long, dragon) vs 兔 (tu, rabbit) 1 In Chinese culture, dragon and phoenix roughly mean male and female, or king and queen. 2 This term, defined in Chinese WordNet, implies relatedness 4. Synonymy: 不 能 (buneng, cannot)"
L16-1726,Y96-1018,1,0.667444,"pecially pointed out that there is currently no dataset detailing part-whole relations that can be used for the evaluation of their methodology for Mandarin. The current paper addresses the necessity and importance of the construction of a practical dataset for the training and evaluation of DSMs. 3. Construction of Dataset The word pairs from which EVALution-MAN was constructed came from Chinese WordNet (CWN: (Huang et al., 2010)). CWN is a knowledge system modeled on the original Princeton WordNet. The system’s word 4584 sense examples and lexical semantic relations came from Sinica Corpus (Chen et al., 1996). We extracted pairs holding the following relations: synonymy (i.e. words belonging to the same CWN synset); antonymy (i.e. words that have the opposite synset in CWN); hyponymy (i.e. one word’s CWN synset is the subordinate of another’s); meronymy (i.e. one word’s CWN synset is a part, member or substance of another’s); nearsynonymy (i.e. words sharing in relatedness rather than within the same synset). Table 1: Numbers of Checking results 3.1. Data Collection In order to include only prototypical pairs in EVALutionMAN, we filtered the CWN pairs and then further assessed them through manual"
L16-1726,J06-1005,0,0.0543596,"e to) 不可以 5. Meronymy: 墻 (jianzhuwu, wall) vs. (jianzhuwu, building) 建築物 A DSM might be able to identify these words as similar/related, but it will struggle to to discriminate the paradigmatic relations they hold. While this is indeed still a challenging task, the NLP community has adopted several approaches (Jia et al., 2014). The new approaches can be classified according to the need of annotated resources to achieve their goal of identifying multiple relations between given word pairs: supervised, which is a method of discrimination that relies on large scale datasets to train the models (Girju et al., 2006; Van Hage et al., 2006); semi-supervised, which uses a small dataset that acts as a seed to extract more relation instances and patterns iteratively (Pantel and Pennacchiotti, 2006); and unsupervised which does not require any manually annotated dataset to train the model (Jia et al., 2014; Santus et al., 2014a; Santus et al., 2014b; Santus et al., 2014c). While a dataset is necessary for training the former two approaches, the third approach will also need a dataset for testing. Because of this necessity, several banchmarks have been constructued for English. To meet the demand, several benc"
L16-1726,W97-0802,0,0.603536,"Missing"
L16-1726,J15-4004,0,0.0395602,"ALution-MAN, a new resource for training and evaluating Mandarin DSMs. EVALutionMAN is a traditional Chinese version of the English EVALution 1.0 (Santus et al., 2015), as it was built following a very similar methodology. instead of similarity. 4583 2. Related Work 2.1. Datasets for DSMs For the training and evaluation of DSMs, several datasets have been widely used. While general-purpose datasets are still being made use of, a recent trend in the creation of benchmarks for the training of DSMs has been their construction from data derived from specific tasks performed by human participants (Hill et al., 2015). The general purpose resources used for the training and evaluation of DSMs have followed the model laid out by WordNet (Fellbaum, 1998). For Mandarin, Chinese, there is Chinese WordNet (CWN: (Huang et al., 2010)) and How-Net (Liu and Li, 2002). CWN consists of more than 50,000 word relation pairs covering the relationships of antonymy, synonymy, hyponymy/hypernymy, meronymy/holonymy, paranymy, nearsynonymy and variant. Paranymy in this context refers to co-hyponymy, i.e., lexical items that share hypernym pairs, while variant refers to pairs that are identical in meaning and use but differ i"
L16-1726,Y06-1024,1,0.74854,"ecific” while 城市 (chengshi, city) can be tagged as ”General”; 3. Abstract/Concrete: 玫瑰 (meigui, rose) can be tagged as ”Concrete” while 概念 (gainian, concept) can be tagged as ”Abstract”; 4. Event/Action/Time/Space/Object/Animal /Plant/Food/Color/People/Attribute: such as 轉變成 (zhuanbiancheng, transform into) can be tagged as ”Action”. Only relata that showed agreement between at least two of the taggers were treated as positive results, leaving the total 4585 set of semantically tagged relata at 373. Meanwhile, frequency information was calculated in a combined corpus of Chen et al. (1996) and Hong and Huang (2006). 4. Conclusion and Future Work EVALution-MAN is a dataset of Mandarin word relation pairs in Traditional Chinese for training and evaluation of DSMs or other applications. It has been manually rated according to relation pairs, and tagged for semantic type by native Mandarin speakers. It is freely available online at ”https://github.com/LHongchao/EVALution_MAN”. Future work will focus on extending the number of manually tagged relation pairs through extracting existing pairs through other ontology resources and/or through the use of behavioral methods that elicit semantic types of given words"
L16-1726,huang-etal-2004-sinica,1,0.610431,"was constructed from Taiwanese Mandarin language sources. Thus, variances in terminology such as, 北市 (beishi, North city) vs. 台北市 (taibei shi, Taipei city) did not fit the background of our raters, whom were all from Mainland China. Relata Synonymy Antonymy Hyponymy Meronymy Nearsynonymy Total 61 34 185 19 61 360 114 50 247 32 51 494 3.3. Semantic Tagging As a further step in describing the dataset, we identified the semantic type information of the positive pairs. For the 376 relata, their total frequency and PoS distribution were calculated in a combined corpus that included Sinica Corpus (Huang et al., 2004) and Chinese GigaWord (Huang et al., 2010). An additional three Ph.D. students were then asked to tag these relata according to their semantic types: 1. Basic/Subordinate/Superordinate: 研討會 (yantaohui, seminar) can be tagged as ”Basic” while 會議 (huiyi, meeting) can be tagged as ”Superodinate”; The next step in constructing the dataset involved the rating of relatedness between word pairs. We divided the 494 word relation pairs extracted from CWN into two groups equalling 250 and 244 word relation pairs. Each word relation pair was placed in carrier sentences that represented a specific relatio"
L16-1726,O02-2003,0,0.530534,"4583 2. Related Work 2.1. Datasets for DSMs For the training and evaluation of DSMs, several datasets have been widely used. While general-purpose datasets are still being made use of, a recent trend in the creation of benchmarks for the training of DSMs has been their construction from data derived from specific tasks performed by human participants (Hill et al., 2015). The general purpose resources used for the training and evaluation of DSMs have followed the model laid out by WordNet (Fellbaum, 1998). For Mandarin, Chinese, there is Chinese WordNet (CWN: (Huang et al., 2010)) and How-Net (Liu and Li, 2002). CWN consists of more than 50,000 word relation pairs covering the relationships of antonymy, synonymy, hyponymy/hypernymy, meronymy/holonymy, paranymy, nearsynonymy and variant. Paranymy in this context refers to co-hyponymy, i.e., lexical items that share hypernym pairs, while variant refers to pairs that are identical in meaning and use but differ in orthography (Huang et al., 2010). Nearsynonymy here refers to two words are related instead of similar. Because CWN was constructed using WordNet, it carries all its limitations (e.g. arbitrariness). This recently has been criticized, as it do"
L16-1726,D08-1103,0,0.0368024,"mation such as agent, event, patient, location, etc. While structurally distinct from WordNet, the relation pairs that can be extracted from its graph structure also have not been evaluated against human judgment. A well-known benchmark for the evaluation of DSM is the set of eighty multiple choice synonym questions from the Test of English as a Foreign Language (TOEFL). This dataset was introduced for the first time by (Landauer and Dumais, 1997) and it allowed for the comparision of computer performance (and other computational models) against that of the performance of college applicants. (Mohammad et al., 2008) used a similar paradigm for their dataset, built from 162 questions from the Graduate Record Examination (GRE) that targeted antonymy. Both datasets address only one semantic relation. Their sizes and focus on single relations make them inappropriate for an extensive evaluation of DSMs. BLESS (Baroni and Lenci Evaluation of Semantic Spaces) was the first English dataset especially designed for the evaluation of multiple semantic word relations. It features 200 basic target concepts instantiated by 26,554 relata. It contains five different word relations: co-hypernymy, hypernymy, meronymy, att"
L16-1726,P06-1015,0,0.0178279,"iminate the paradigmatic relations they hold. While this is indeed still a challenging task, the NLP community has adopted several approaches (Jia et al., 2014). The new approaches can be classified according to the need of annotated resources to achieve their goal of identifying multiple relations between given word pairs: supervised, which is a method of discrimination that relies on large scale datasets to train the models (Girju et al., 2006; Van Hage et al., 2006); semi-supervised, which uses a small dataset that acts as a seed to extract more relation instances and patterns iteratively (Pantel and Pennacchiotti, 2006); and unsupervised which does not require any manually annotated dataset to train the model (Jia et al., 2014; Santus et al., 2014a; Santus et al., 2014b; Santus et al., 2014c). While a dataset is necessary for training the former two approaches, the third approach will also need a dataset for testing. Because of this necessity, several banchmarks have been constructued for English. To meet the demand, several benchmarks have been constructed for English TOEFL (Landauer and Dumais, 1997), BLESS (Baroni and Lenci, 2011), LENCI/Benotto (Benotto, 2015) and EVALution 1.0 (Santus et al., 2015). How"
L16-1726,E14-4008,1,0.931977,"importance for tasks such as word sense disambiguation, lexical replacement, dictionary construction, and entailment understanding, to name a few (Sun, 2014). DSMs are founded on the assumption that the lexical similarity between words depends on their distributed context. According to the Distributional Hypothesis, if compared concepts or words have similar distributional features (hence, context), they are likely to have higher semantic similarity (Harris, 1954). One major shortcoming of current DSMs is that they cannot be used for discrimination among different relationships between words (Santus et al., 2014c). In recent years, DSMs have been adopted in several semantic tasks, including the identification of semantic relatedness and similarity. The former task is concerned with whether two words are related or not, independently from their paradigmatic similarity. The identification of semantic similarity, instead, is a more specific task and it consists in identifying words that are paradigmatically related (Sun, 2014; Murphy, 2003). DSMs have successfully addressed these two tasks by using vector cosine as a measure of similarity (Agirre et al., 2009). Unfortunately, however, DSMs are not yet a"
L16-1726,Y14-1018,1,0.929371,"importance for tasks such as word sense disambiguation, lexical replacement, dictionary construction, and entailment understanding, to name a few (Sun, 2014). DSMs are founded on the assumption that the lexical similarity between words depends on their distributed context. According to the Distributional Hypothesis, if compared concepts or words have similar distributional features (hence, context), they are likely to have higher semantic similarity (Harris, 1954). One major shortcoming of current DSMs is that they cannot be used for discrimination among different relationships between words (Santus et al., 2014c). In recent years, DSMs have been adopted in several semantic tasks, including the identification of semantic relatedness and similarity. The former task is concerned with whether two words are related or not, independently from their paradigmatic similarity. The identification of semantic similarity, instead, is a more specific task and it consists in identifying words that are paradigmatically related (Sun, 2014; Murphy, 2003). DSMs have successfully addressed these two tasks by using vector cosine as a measure of similarity (Agirre et al., 2009). Unfortunately, however, DSMs are not yet a"
L16-1726,W15-4208,1,0.896217,"DSMs. BLESS (Baroni and Lenci Evaluation of Semantic Spaces) was the first English dataset especially designed for the evaluation of multiple semantic word relations. It features 200 basic target concepts instantiated by 26,554 relata. It contains five different word relations: co-hypernymy, hypernymy, meronymy, attribute, and event. The structure of the dataset is in the form of “concept-relation-word”tuples. For each concept, BLESS also features semantically unrelated words. The shortcoming of BLESS, however, is that the dataset didn’t take synonymy and antonymy into consideration. Benotto (2015) constructed an English dataset targeting hypernymy, synonymy, and antonymy through elicitation experiments following the method introduced by (Paradis et al., 2009). Target words were firstly selected from sources such as WordNet and GermaNet (Hamp et al., 1997). An elicitation experiment was then conducted through Amazon Mechanical Turk to ask every participant to produce antonyms, synonyms and hyponyms of each word. In this way, 8,910 word relation pairs were collected. A DSMs-oriented resource that overcomes the shortcomings of the above datasets for English is EVALution 1.0 (Santus et al."
L16-1726,W11-2501,0,0.0645677,"Missing"
L18-1394,W00-1205,1,0.397416,"et al., 2017). One of the long-term plans of the project is to extend the set of (mostly European) languages to Asian languages, including Chinese Mandarin. Chinese linguistic tradition is different from the European which presents a challenge when trying to apply the guidelines created under the European project to Chinese. In this paper we make a pilot study of one particular type of verbal MWEs - LVCs and see how the existing resources for Chinese can be adjusted to the PARSEME annotation schema. We consider two possible corpora for annotation: the Sinica Balanced Corpus of Modern Chinese(Huang et al., 2000) and Universal Dependencies (Nivre et al., 2017). Finally, we have chosen Chinese Universal Dependencies treebank1 (wiki data) because it features syntactic annotation in dependency-based format required by the shared task.Unlike other languages such as Czech (Bejˇcek et al., 2017), there is no resources we can use to generate MWEs automatically for Chinese. But we don’t do the annotation from scratch as well. We use a list of Chinese light verbs to pre-process the data, and then manual work is performed to 1 http://universaldependencies.org/zh/ overview/introduction.html assign the correspond"
L18-1394,W14-5810,1,0.915303,"unique distribution different from regular verbs with higher semantic content and selectional restrictions. (Butt, 2010) examines Chinese light verbs in the paper, but deals with directional complements and aspectual markers only, without mentioning any of the more typical usage of light verbs in Chinese as dealt with in literature on LVC in Chinese (e.g., (Zhu, 1985)). Moreover, the Propbank (Xue and Palmer, 2005) also treats light verbs as verb with zero argument. However, the issues is, these previous work cannot be directly transferred to the PARSEME framework, hence we further refer to (Lin et al., 2014). In (Lin et al., 2014), the authors specifically dealt with annotation of light verbs in Chinese, as well as automatic classification of different Chinese light verbs, therefore is the directly relevant resource we can make reference to. 4. Adjusting PARSEME guidelines to the database of Light Verbs 4.2. The annotation guidelines of the PARSEME shared task2 define light verb constructions with several key characteristics. The first one is that LVCs are formed by a verb and a (single or compound) noun, which either directly depends on verb (and possibly contains a case marker or a postposition"
L18-1394,W17-1704,0,0.0294746,"and then manually assigning the corresponding nouns or correcting false positives. Keywords: verbal multiword expressions, light verb constructions, Chinese, corpus linguistics 1. Introduction Multiword Expressions (MWEs) present a challenge in a bunch of areas of Natural Language Processing like Machine Translation or Information Extraction. They are idiosyncratic in their nature - the meaning of the whole can not be derived from the meaning of its parts. The exact definition of MWEs varies across different linguistic theories and can not be necessarily universal for all languages. PARSEME (Savary et al., 2017) is a European project which aims at processing multiword expressions from different perspectives and for various languages. Universal guidelines were created to annotate verbal multiword expressions distinguishing several subtypes unique for a language: idioms, LVCs, verb-particle constructions, inherently reflexive verbs and others (miscellaneous category). Corpora in 18 languages have been annotated which result in a multilingual resource (Savary et al., 2017). One of the long-term plans of the project is to extend the set of (mostly European) languages to Asian languages, including Chinese"
lee-etal-2010-emotion,W09-3001,1,\N,Missing
lee-etal-2010-emotion,C08-1111,0,\N,Missing
lee-etal-2010-emotion,S07-1072,0,\N,Missing
lee-etal-2014-annotating,W09-3001,1,\N,Missing
lee-etal-2014-annotating,S07-1000,0,\N,Missing
lee-etal-2014-annotating,C08-1111,0,\N,Missing
ma-huang-2006-uniform,W02-1811,0,\N,Missing
ma-huang-2006-uniform,C02-1145,0,\N,Missing
ma-huang-2006-uniform,W00-1205,1,\N,Missing
ma-huang-2006-uniform,O92-1003,0,\N,Missing
O00-2001,Y99-1005,1,0.780744,"Missing"
O00-2004,J93-2005,0,\N,Missing
O03-1006,W03-1405,1,\N,Missing
O05-1018,H92-1116,0,0.145491,"Missing"
O05-5001,W98-1120,0,\N,Missing
O05-5001,O99-4001,0,\N,Missing
O05-5001,Y03-1022,0,\N,Missing
O05-5001,J94-4001,0,\N,Missing
O05-5001,M95-1012,0,\N,Missing
O05-5001,A94-1007,0,\N,Missing
O05-5001,W03-1402,0,\N,Missing
O05-5001,W03-1509,0,\N,Missing
O05-5001,C02-1012,0,\N,Missing
O05-5001,J04-2002,0,\N,Missing
O05-5001,Y03-1014,0,\N,Missing
O05-5001,W03-1405,1,\N,Missing
O05-5001,W03-1403,0,\N,Missing
O05-5001,P05-1015,0,\N,Missing
O05-5001,P92-1003,0,\N,Missing
O05-5001,O03-1006,1,\N,Missing
O05-5001,O00-2002,1,\N,Missing
O05-5001,huang-etal-2004-sinica,1,\N,Missing
O05-5001,Y95-1011,1,\N,Missing
O05-5001,A97-1029,0,\N,Missing
O05-5001,O98-3003,1,\N,Missing
O05-5012,W03-1405,1,0.882823,"Missing"
O05-5012,W03-1402,0,0.0406774,"Missing"
O05-5012,alonge-castelli-2002-way,0,0.0549808,"Missing"
O05-5012,alonge-lonneker-2004-metaphors,0,0.0250753,"Missing"
O05-5012,O03-1006,1,0.900504,"Missing"
O05-5012,Y03-1014,1,0.90297,"Missing"
O05-5012,huang-etal-2004-sinica,1,0.873342,"Missing"
O05-5012,W03-1403,0,0.0343296,"Missing"
O05-5012,J04-2002,0,0.0817759,"Missing"
O06-1002,O03-1010,0,0.055697,"Missing"
O06-1002,Y03-1016,0,0.0450925,"Missing"
O06-1002,O04-2005,0,0.0615608,"Missing"
O06-1003,W02-1106,1,0.899142,"Missing"
O06-1003,I05-4007,1,0.801,"Missing"
O06-1003,W06-3909,0,0.0924504,"Missing"
O07-2003,huang-etal-2004-sinica,1,0.844707,"Missing"
O07-2006,Y96-1018,1,0.796691,"Missing"
O07-2006,ma-huang-2006-uniform,1,0.806511,"Missing"
O07-2006,H93-1052,0,0.26181,"Missing"
O08-1009,C92-1019,0,0.116062,"Missing"
O08-1009,P04-1059,0,0.0746096,"Missing"
O08-1009,Y06-1001,0,0.0305597,"Missing"
O08-1009,O99-2001,0,0.0297197,"Missing"
O08-1009,W06-0115,0,0.165904,"Missing"
O08-1009,O03-4002,0,0.27067,"Missing"
O08-1009,P07-2018,1,0.896796,"hinese word segmentation tool must adapt to textual variations with minimal training input and yet robust enough to yield reliable segmentation result for all variants. Various lexicon-driven approaches to Chinese segmentation, e.g. [1,16], achieve high f-scores yet require massive training for any variation. Text-driven approach, e.g. [12], can be easily adapted for domain and genre changes yet has difficulty matching the high f-scores of the lexicon-driven approaches. In this paper, we refine and implement an innovative text-driven word boundary decision (WBD) segmentation model proposed in [15]. The WBD model treats word segmentation simply and efficiently as a binary decision on whether to realize the natural textual break between two adjacent characters as a word boundary. The WBD model allows simple and quick training data preparation converting characters as contextual vectors for learning the word boundary decision. Machine learning experiments with four different classifiers show that training with 1,000 vectors and 1 million vectors achieve comparable and reliable results. In addition, when applied to SigHAN Bakeoff 3 competition data, the WBD model produces OOV recall rates"
O08-1009,W06-0127,0,0.0459218,"Missing"
O08-1009,I08-4017,0,0.0263458,"Missing"
O08-1009,J96-3004,0,\N,Missing
O08-2008,O04-1030,1,0.840861,"Missing"
O08-2008,huang-etal-2004-sinica,1,0.898065,"Missing"
O08-2008,W03-1726,1,0.856717,"Missing"
O08-2008,W03-1705,1,0.849533,"Missing"
O97-4003,C92-1019,1,0.799397,"Missing"
O97-4003,Y96-1018,1,0.914189,"Missing"
O97-4003,O92-1003,0,0.0681692,"Missing"
O97-4003,P89-1010,0,0.0545131,"Missing"
O97-4003,P94-1010,0,0.0461372,"Missing"
O98-3004,C96-1055,0,0.0585671,"Missing"
O98-3004,J93-2005,0,0.323339,"Missing"
P06-2050,I05-7002,1,0.891992,"Missing"
P06-2106,francopoulo-etal-2006-lexical,1,0.861995,"Missing"
P06-2106,W03-1905,1,0.81081,"ess ordinal, pronoun, for instance. The classifier phrase is syntactically generated according to a specific pattern. Here are some usages of classifiers and their syntactic patterns. gual conditions and perform operations on lexical entries. Originally, in order to meet expectations placed upon lexicons as critical resources for content processing in the Semantic Web, the MILE syntactic and semantic lexical objects have been formalized in RDF(S), thus providing a web-based means to implement the MILE architecture and allowing for encoding individual lexical entries as instances of the model (Ide et al., 2003; Bertagna et al., 2004b). In the framework of our project, by situating our work in the context of W3C standards and relying on standardized technologies underlying this community, the original RDF schema for ISLE lexical entries has been made compliant to OWL. The whole data model has been formalized in OWL by using Prot´eg´e 3.2 beta and has been extended to cover the morphological component as well (see Figure 2). Prot´eg´e 3.2 beta has been also used as a tool to instantiate the lexical entries of our sample monolingual lexicons, thus ensuring adherence to the model, encoding coherence an"
P06-2106,bel-etal-2000-simple,1,0.877753,"s: to increase the competitive edge of Asian countries, to bring Asian countries to closer to their western counterparts, and to bring more cohesion among Asian countries. To achieve this goal, we have launched a two year project to create a common standard for Asian language resources. The project is comprised of the following four research items. There is a long history of creating a standard for western language resources. The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EAGLES1 , PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Calzolari et al., 2003) and LIRICS2 . These continuous efforts has been crystallized as activities in ISO-TC37/SC4 which aims to make an international standard for language resources. 2 (4) Evaluation through application classification Figure 1: Relations among research items 1 Introduction 1 (2) Sample lexicons (1) building a description framework of lexical entries (2) building sample lexicons (3) building an upper-layer ontology (4) evaluating the proposed framework through an application Figure 1 illustrates the relations among these research items. Our main aim is the researc"
P06-2106,C94-1091,1,0.485541,"ical operations, which are special lexical entities allowing the user to define multilin3 MILE is based on the experience derived from existing computational lexicons (e.g. LE-PAROLE, SIMPLE, EuroWordNet, etc.). 828 “CL” stands for a classifier. They always follow cardinal numbers in Japanese. Note that different classifiers are used for different nouns. In the above examples, classifier “hiki” is used to count noun “inu (dog)”, while “satsu” for “hon (book)”. The classifier is determined based on the semantic type of the noun. In the Thai language, classifiers are used in various situations (Sornlertlamvanich et al., 1994). The classifier plays an important role in construction with noun to express ordinal, pronoun, for instance. The classifier phrase is syntactically generated according to a specific pattern. Here are some usages of classifiers and their syntactic patterns. gual conditions and perform operations on lexical entries. Originally, in order to meet expectations placed upon lexicons as critical resources for content processing in the Semantic Web, the MILE syntactic and semantic lexical objects have been formalized in RDF(S), thus providing a web-based means to implement the MILE architecture and al"
P06-2106,zhang-etal-2004-distributional,1,0.766907,"the set of postpositions as values of FunctionType instead of conventional function types such as “subj” and “obj”. It might be an user defined data category or language dependent data category. Furthermore, it is preferable to prepare the mapping between Japanese postpositions and conventional function types. This is interesting because it seems more a terminological difference, but the model can be applied also to Japanese. 4 Building sample lexicons 4.1 Swadesh list and basic lexicon The issue involved in defining a basic lexicon for a given language is more complicated than one may think (Zhang et al., 2004). The naive approach of simply taking the most frequent words in a language is flawed in many ways. First, all frequency counts are corpus-based and hence inherit the bias of corpus sampling. For instance, since it is easier to sample written formal texts, words used predominantly in informal contexts are usually underrepresented. Second, frequency of content words is topic-dependent and may vary from corpus to corpus. Last, and most crucially, frequency of a word does not correlate to its conceptual necessity, 4.2 Aligning multilingual lexical entries Since our goal is to build a multilingual"
P06-2106,bertagna-etal-2004-content,1,0.887142,"e morphological, syntactic and semantic layers. Moreover, an intermediate module allows to define mechanisms of linkage and mapping between the syntactic and semantic layers. Within each layer, a basic linguistic information unit is identified; basic units are separated but still interlinked each other across the different layers. Within each of the MLM layers, different types of lexical object are distinguished : fits with as many Asian languages as possible, and contributing to the ISO-TC37/SC4 activities. As a starting point, we employ an existing description framework, the MILE framework (Bertagna et al., 2004a), to describe several lexical entries of several Asian languages. Through building sample lexicons (research item (2)), we will find problems of the existing framework, and extend it so as to fit with Asian languages. In this extension, we need to be careful in keeping consistency with the existing framework. We start with Chinese, Japanese and Thai as target Asian languages and plan to expand the coverage of languages. The research items (2) and (3) also comprise the similar feedback loop. Through building sample lexicons, we refine an upper-layer ontology. An application built in the resea"
P06-2106,Y06-1043,1,\N,Missing
P07-2018,P04-1059,0,0.0824876,"Missing"
P07-2018,O99-2001,0,0.0950061,"Missing"
P07-2018,W03-1719,0,0.081678,"Missing"
P07-2018,O03-4002,0,\N,Missing
P07-2018,C92-1019,0,\N,Missing
P07-2018,W03-1726,0,\N,Missing
P07-2039,ma-huang-2006-uniform,1,0.831199,"are those between PRC and Taiwan Mandarin, we will start with known contrasting pairs of these two language variants and mine potential variant pairs from their collocates. These potential variant pairs are then checked for their phonological similarity to determine whether they are true variants or not. In order to effectively select collocates from specific grammatical constructions, the Chinese Word Sketch3 is adopted. In particular, we use the Word Sketch dif2 To facilitate processing, the complete CGC was segmented and POS tagged using the Academia Sinica segmentation and tagging system (Ma and Huang, 2006). 3 http://wordsketch.ling.sinica.edu.tw 154 ference (WSDiff) function to pick the grammatical contexts as well as contrasting pairs. It is important to bear in mind that Chinese texts are composed of Chinese characters, hence it is impossible to compare a transliterated NE with the alphabetical form in its original language. The following characteristics of a transliterated NE’s in CGC are exploited to allow discovery of transliteration variations without referring to original NE. • frequent co-occurrence of named entities within certain syntagmatic relations – named entities frequently co-oc"
P07-2039,W03-1508,0,0.0496861,"Missing"
P07-2039,P98-2220,0,0.0377422,"teration concentrate on the identification of either the transliterated term or the original term, given knowledge of the other (e.g. (Virga and Khudanpur, 1 For instance, we found at least 14 transliteration variants for Lewinsky,such as 呂茵斯基，呂文絲基，呂茵斯，陸文斯基，陸茵斯 基， 柳思基，陸雯絲姬，陸文斯基，呂茵斯基，露文斯基，李文斯基，露溫 斯基，蘿恩斯 基，李雯斯基 and so on. 153 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 153–156, c Prague, June 2007. 2007 Association for Computational Linguistics 2003)). These studies are typically either rule-based or statistics-based, and specific to a language pair with a fixed direction (e.g. (Wan and Verspoor, 1998; Jiang et al., 2007)). To the best of our knowledge, ours is the first attempt to discover transliterated NE’s without assuming prior knowledge of the entities. In particular, we propose that transliteration variants can be discovered by extracting and comparing terms from similar linguistic context based on CGC and WSE tools. This proposal has great potential of increasing robustness of future NER work by enabling discovery of new and unknown transliterated NE’s. Our study shows that resolution of transliterated NE variations can be fully automated. This will have strong and positive implica"
P07-2039,C98-2215,0,\N,Missing
P09-1078,P07-1056,0,0.0295975,"e carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents 1 . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset2 (Pang and Lee, 2004) and one dataset from product reviews of domain DVD3 (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories. Classification Algorithm: Many classification algorithms are available for text classification, such as Naïve Bayes, Maximum Entropy, k-NN, and SVM. Among these methods, SVM is shown to perform better than other methods (Yang and Pedersen, 1997; Pang et al., 1 2 3 http://people.csail.mit.edu/~jrennie/20Newsgroups/ http://www.cs.cornell.edu/People/pabo/movie-review-data/ http://www.seas.upenn.edu/~mdredze/dataset"
P09-1078,P06-2079,0,0.0544922,"Missing"
P09-1078,W02-1011,0,0.0203942,"Missing"
P09-1078,P04-1035,0,0.00597525,"Experimental Studies 4.1 Experimental Setup Data Set: The experiments are carried out on both topic-based and sentiment text classification datasets. In topic-based text classification, we use two popular data sets: one subset of Reuters-21578 referred to as R2 and the 20 Newsgroup dataset referred to as 20NG. In detail, R2 consist of about 2,000 2-category documents from standard corpus of Reuters-21578. And 20NG is a collection of approximately 20,000 20-category documents 1 . In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset2 (Pang and Lee, 2004) and one dataset from product reviews of domain DVD3 (Blitzer et al., 2007). Both of them are 2-category tasks and each consists of 2,000 reviews. In our experiments, the document numbers of all data sets are (nearly) equally distributed cross all categories. Classification Algorithm: Many classification algorithms are available for text classification, such as Naïve Bayes, Maximum Entropy, k-NN, and SVM. Among these methods, SVM is shown to perform better than other methods (Yang and Pedersen, 1997; Pang et al., 1 2 3 http://people.csail.mit.edu/~jrennie/20Newsgroups/ http://www.cs.cornell.ed"
P09-1078,W06-1652,0,0.262065,"Missing"
P10-1043,P07-1056,0,0.914534,"rmally domain-independent and serves as highly relevant clues to sentiment classification. The latter type of statement called impersonal view, e.g. ‘it is too small’, contains Y ’s “objective” (i.e. or at least criteria-based) evaluation of the target object. This kind of information tends to contain much domain-specific classification knowledge. Although such information is sometimes not as explicit as personal views in classifying the sentiment of a text, speaker’s sentiment is usually implied by the evaluation result. It is well-known that sentiment classification is very domain-specific (Blitzer et al., 2007), so it is critical to eliminate its dependence on a large-scale labeled data for its wide applications. Since the unlabeled data is ample and easy to collect, a successful semi-supervised sentiment classification system would significantly minimize the involvement of labor and time. Therefore, given the two different views mentioned above, one promising application is to adopt them in co-training algorithms, which has been proven to be an effective semi-supervised learning strategy of incorporating unlabeled data to further improve the classification performance (Zhu, 2005). In addition, we w"
P10-1043,P09-1027,0,0.158629,"7) present a domain adaptation approach for sentiment classification. Semi-supervised methods combine unlabeled data with labeled training data (often small-scaled) to improve the models. Compared to the supervised and unsupervised methods, semi-supervised methods for sentiment classification are relatively new and have much less related studies. Dasgupta and Ng (2009) integrate various methods in semi-supervised sentiment classification including spectral clustering, active learning, transductive learning, and ensemble learning. They achieve a very impressive improvement across five domains. Wan (2009) applies a co-training method to semi-supervised learning with labeled English corpus and unlabeled Chinese corpus for Chinese sentiment classification. 3 Unsupervised Mining of Personal and Impersonal Views As mentioned in Section 1, the objective of sentiment classification is to classify a specific binary relation: X ’s evaluation on Y, where X is an object set including different kinds of persons and Y is another object set including the target objects to be evaluated. First of all, we focus on an analysis on sentences in product reviews regarding the two views: personal and impersonal vie"
P10-1043,J09-3003,0,0.0225,"Section 3 presents our unsupervised approach for mining personal and impersonal views. Section 4 and Section 5 propose our supervised and semi-supervised methods on sentiment classification respectively. Experimental results are presented and analyzed in Section 6. Section 7 discusses on the differences between personal/impersonal and subjective/objective. Finally, Section 8 draws our conclusions and outlines the future work. 2 Related Work Recently, a variety of studies have been reported on sentiment classification at different levels: word level (Esuli and Sebastiani, 2005), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and document level (Turney, 2002; Pang et al., 2002). This paper focuses on the document-level sentiment classification. Generally, document-level sentiment classification methods can be categorized into three types: unsupervised, supervised, and semi-supervised. Unsupervised methods involve deriving a sentiment classifier without any labeled documents. Most of previous work use a set of labeled sentiment words called seed words to perform unsupervised classification. Turney (2002) determines the sentiment orientation of a document by ca"
P10-1043,C08-1135,0,0.0450621,"on methods can be categorized into three types: unsupervised, supervised, and semi-supervised. Unsupervised methods involve deriving a sentiment classifier without any labeled documents. Most of previous work use a set of labeled sentiment words called seed words to perform unsupervised classification. Turney (2002) determines the sentiment orientation of a document by calculating point-wise mutual information between the words in the document and the seed words of ‘excellent’ and ‘poor’. Kennedy and Inkpen (2006) use a term-counting method with a set of seed words to determine the sentiment. Zagibalov and Carroll (2008) first propose a seed word selection approach and then apply the same term-counting method for Chinese sentiment classifications. These unsupervised approaches are believed to be domain-independent for sentiment classification. Supervised methods consider sentiment classification as a standard classification problem in which labeled data in a domain are used to train a domain-specific classifier. Pang et al. (2002) are the first to apply supervised machine learning methods to sentiment classification. Subsequently, many other studies make efforts to improve the performance of machine learning-"
P10-1043,C04-1200,0,0.118119,"pproach for mining personal and impersonal views. Section 4 and Section 5 propose our supervised and semi-supervised methods on sentiment classification respectively. Experimental results are presented and analyzed in Section 6. Section 7 discusses on the differences between personal/impersonal and subjective/objective. Finally, Section 8 draws our conclusions and outlines the future work. 2 Related Work Recently, a variety of studies have been reported on sentiment classification at different levels: word level (Esuli and Sebastiani, 2005), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and document level (Turney, 2002; Pang et al., 2002). This paper focuses on the document-level sentiment classification. Generally, document-level sentiment classification methods can be categorized into three types: unsupervised, supervised, and semi-supervised. Unsupervised methods involve deriving a sentiment classifier without any labeled documents. Most of previous work use a set of labeled sentiment words called seed words to perform unsupervised classification. Turney (2002) determines the sentiment orientation of a document by calculating point-wise mutual informat"
P10-1043,P07-1055,0,0.0367316,"nt classification. Supervised methods consider sentiment classification as a standard classification problem in which labeled data in a domain are used to train a domain-specific classifier. Pang et al. (2002) are the first to apply supervised machine learning methods to sentiment classification. Subsequently, many other studies make efforts to improve the performance of machine learning-based classifiers by various means, such as using subjectivity summarization (Pang and Lee, 2004), seeking new superior textual features (Riloff et al., 2006), and employing document subcomponent information (McDonald et al., 2007). As far as the challenge of domain-dependency is concerned, Blitzer et al. (2007) present a domain adaptation approach for sentiment classification. Semi-supervised methods combine unlabeled data with labeled training data (often small-scaled) to improve the models. Compared to the supervised and unsupervised methods, semi-supervised methods for sentiment classification are relatively new and have much less related studies. Dasgupta and Ng (2009) integrate various methods in semi-supervised sentiment classification including spectral clustering, active learning, transductive learning, and ens"
P10-1043,W02-1011,0,0.0239994,"statements towards a target object for evaluation. To obtain them, an unsupervised mining approach is proposed. On this basis, an ensemble method and a co-training algorithm are explored to employ the two views in supervised and semi-supervised sentiment classification respectively. Experimental results across eight domains demonstrate the effectiveness of our proposed approach. 1 Introduction As a special task of text classification, sentiment classification aims to classify a text according to the expressed sentimental polarities of opinions such as ‘thumb up’ or ‘thumb down’ on the movies (Pang et al., 2002). This task has recently received considerable interests in the Natural Language Processing (NLP) community due to its wide applications. In general, the objective of sentiment classification can be represented as a kind of binary relation R, defined as an ordered triple (X, Y, G), where X is an object set including different kinds of people (e.g. writers, reviewers, or users), Y is another object set including the target objects (e.g. products, events, or even some people), and G is a subset of the Cartesian product X × Y . The concerned relation in sentiment classification is X ’s evaluation"
P10-1043,W06-1652,0,0.105249,"supervised approaches are believed to be domain-independent for sentiment classification. Supervised methods consider sentiment classification as a standard classification problem in which labeled data in a domain are used to train a domain-specific classifier. Pang et al. (2002) are the first to apply supervised machine learning methods to sentiment classification. Subsequently, many other studies make efforts to improve the performance of machine learning-based classifiers by various means, such as using subjectivity summarization (Pang and Lee, 2004), seeking new superior textual features (Riloff et al., 2006), and employing document subcomponent information (McDonald et al., 2007). As far as the challenge of domain-dependency is concerned, Blitzer et al. (2007) present a domain adaptation approach for sentiment classification. Semi-supervised methods combine unlabeled data with labeled training data (often small-scaled) to improve the models. Compared to the supervised and unsupervised methods, semi-supervised methods for sentiment classification are relatively new and have much less related studies. Dasgupta and Ng (2009) integrate various methods in semi-supervised sentiment classification inclu"
P10-1043,P02-1053,0,0.0110004,"and Section 5 propose our supervised and semi-supervised methods on sentiment classification respectively. Experimental results are presented and analyzed in Section 6. Section 7 discusses on the differences between personal/impersonal and subjective/objective. Finally, Section 8 draws our conclusions and outlines the future work. 2 Related Work Recently, a variety of studies have been reported on sentiment classification at different levels: word level (Esuli and Sebastiani, 2005), phrase level (Wilson et al., 2009), sentence level (Kim and Hovy, 2004; Liu et al., 2005), and document level (Turney, 2002; Pang et al., 2002). This paper focuses on the document-level sentiment classification. Generally, document-level sentiment classification methods can be categorized into three types: unsupervised, supervised, and semi-supervised. Unsupervised methods involve deriving a sentiment classifier without any labeled documents. Most of previous work use a set of labeled sentiment words called seed words to perform unsupervised classification. Turney (2002) determines the sentiment orientation of a document by calculating point-wise mutual information between the words in the document and the seed wo"
P10-1043,P04-1035,0,\N,Missing
P10-1043,P09-1079,0,\N,Missing
P13-2091,I08-1041,0,0.0192107,"ve and negative) with a high probability and does not apply to fine-grained emotion categories (e.g., happy, angry, and sad). This motivates our joint modeling in terms of the coarse-grained emotion categories. Specifically, we consider the news text and the comment text as two different views of expressing either the news reader’s or comment writer’s emotions. Given the two views, a co-training algorithm is proposed to perform semi-supervised emotion classification so that the information in the unlabeled data can be exploited to improve the classification performance. ing (Alm et al., 2005; Aman and Szpakowicz, 2008; Chen et al., 2010; Purver and Battersby, 2012; Moshfeghi et al., 2011), and so far, we have not seen any studies on semi-supervised learning on fine-grained emotion classification. 2 3 2.1 Related Work Comment Writer’s Emotion Classification Comment writer’s emotion classification has been a hot research topic in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment cl"
P13-2091,C10-1021,1,0.605042,". I still can not forget last year. (2) My father-in-law got to experience this quake... what a suffering. Comment Writer’s emotion: sad Comment Reader’s emotion: Unknown Introduction Emotion classification aims to predict the emotion categories (e.g., happy, angry, or sad) of a given text (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). With the rapid growth of computer mediated communication applications, such as social websites and miro-blogs, the research on emotion classification has been attracting more and more attentions recently from the natural language processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012). In general, a single text may possess two kinds of emotions, writer’s emotion and reader’s emotion, where the former concerns the emotion expressed by the writer when writing the text and the latter concerns the emotion expressed by a reader after reading the text. For example, consider two short texts drawn from a news and corresponding comments, as shown in Figure 1. On * * Corresponding author Figure 1: An example of writer’s and reader’s emotions on a news and its comments Accordingly, emotion classification can be grouped into two categories: reader’s emotio"
P13-2091,W02-1011,0,0.0151406,"the two views, a co-training algorithm is proposed to perform semi-supervised emotion classification so that the information in the unlabeled data can be exploited to improve the classification performance. ing (Alm et al., 2005; Aman and Szpakowicz, 2008; Chen et al., 2010; Purver and Battersby, 2012; Moshfeghi et al., 2011), and so far, we have not seen any studies on semi-supervised learning on fine-grained emotion classification. 2 3 2.1 Related Work Comment Writer’s Emotion Classification Comment writer’s emotion classification has been a hot research topic in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et"
P13-2091,E12-1049,0,0.120126,"orget last year. (2) My father-in-law got to experience this quake... what a suffering. Comment Writer’s emotion: sad Comment Reader’s emotion: Unknown Introduction Emotion classification aims to predict the emotion categories (e.g., happy, angry, or sad) of a given text (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). With the rapid growth of computer mediated communication applications, such as social websites and miro-blogs, the research on emotion classification has been attracting more and more attentions recently from the natural language processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012). In general, a single text may possess two kinds of emotions, writer’s emotion and reader’s emotion, where the former concerns the emotion expressed by the writer when writing the text and the latter concerns the emotion expressed by a reader after reading the text. For example, consider two short texts drawn from a news and corresponding comments, as shown in Figure 1. On * * Corresponding author Figure 1: An example of writer’s and reader’s emotions on a news and its comments Accordingly, emotion classification can be grouped into two categories: reader’s emotion and writer’s emotion classi"
P13-2091,D09-1150,0,0.697347,"emotion while the emotion of a reader after reading the comments is not clear (Some may feel sorry but others might feel careless). News: Today's Japan earthquake could be 2011 quake aftershock. …… News Writer’s emotion: None News Reader’s emotion: sad, worried Comments: (1) I hope everything is ok, so sad. I still can not forget last year. (2) My father-in-law got to experience this quake... what a suffering. Comment Writer’s emotion: sad Comment Reader’s emotion: Unknown Introduction Emotion classification aims to predict the emotion categories (e.g., happy, angry, or sad) of a given text (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). With the rapid growth of computer mediated communication applications, such as social websites and miro-blogs, the research on emotion classification has been attracting more and more attentions recently from the natural language processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012). In general, a single text may possess two kinds of emotions, writer’s emotion and reader’s emotion, where the former concerns the emotion expressed by the writer when writing the text and the latter concerns the emotion expressed by a reader after reading the te"
P13-2091,P09-2038,0,0.311983,"motion of a reader after reading the comments is not clear (Some may feel sorry but others might feel careless). News: Today's Japan earthquake could be 2011 quake aftershock. …… News Writer’s emotion: None News Reader’s emotion: sad, worried Comments: (1) I hope everything is ok, so sad. I still can not forget last year. (2) My father-in-law got to experience this quake... what a suffering. Comment Writer’s emotion: sad Comment Reader’s emotion: Unknown Introduction Emotion classification aims to predict the emotion categories (e.g., happy, angry, or sad) of a given text (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). With the rapid growth of computer mediated communication applications, such as social websites and miro-blogs, the research on emotion classification has been attracting more and more attentions recently from the natural language processing (NLP) community (Chen et al., 2010; Purver and Battersby, 2012). In general, a single text may possess two kinds of emotions, writer’s emotion and reader’s emotion, where the former concerns the emotion expressed by the writer when writing the text and the latter concerns the emotion expressed by a reader after reading the text. For example, consider two"
P13-2091,W06-1652,0,0.014076,"in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion cl"
P13-2091,P09-1079,0,0.0353899,"st decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion classification has been e"
P13-2091,P02-1053,0,0.00981,"-training algorithm is proposed to perform semi-supervised emotion classification so that the information in the unlabeled data can be exploited to improve the classification performance. ing (Alm et al., 2005; Aman and Szpakowicz, 2008; Chen et al., 2010; Purver and Battersby, 2012; Moshfeghi et al., 2011), and so far, we have not seen any studies on semi-supervised learning on fine-grained emotion classification. 2 3 2.1 Related Work Comment Writer’s Emotion Classification Comment writer’s emotion classification has been a hot research topic in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li"
P13-2091,P10-1043,1,0.556285,"2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion classification has been extensively studie"
P13-2091,E12-1031,0,0.100839,"slike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion classification has been extensively studied, there are only a few studies on news reader’s emotion classification from the NLP and related communities. Lin et al. (2007) first describe the task of reader’s emotion classification on the news articles and then employ some standard machine learning approaches to train a classifier for determining the reader’s em"
P13-2091,J09-3003,0,0.0290799,"to perform semi-supervised emotion classification so that the information in the unlabeled data can be exploited to improve the classification performance. ing (Alm et al., 2005; Aman and Szpakowicz, 2008; Chen et al., 2010; Purver and Battersby, 2012; Moshfeghi et al., 2011), and so far, we have not seen any studies on semi-supervised learning on fine-grained emotion classification. 2 3 2.1 Related Work Comment Writer’s Emotion Classification Comment writer’s emotion classification has been a hot research topic in NLP during the last decade (Pang et al., 2002; Turney, 2002; Alm et al., 2005; Wilson et al., 2009) and previous studies can be mainly grouped into two categories: coarse-grained and fine-grained emotion classification. Coarse-grained emotion classification, also called sentiment classification, concerns only two emotion categories, such as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grai"
P13-2091,C10-1136,0,0.440288,"uch as like or dislike and positive or negative (Pang and Lee, 2008; Liu, 2012). This kind of emotion classification has attracted much attention since the pioneer work by Pang et al. (2002) in the NLP community due to its wide applications (Cui et al., 2006; Riloff et al., 2006; Dasgupta and Ng, 2009; Li et al., 2010; Li et al., 2011). In comparison, fine-grained emotion classification aims to classify a text into multiple emotion categories, such as happy, angry, and sad. One main group of related studies on this task is about emotion resource construction, such as emotion lexicon building (Xu et al., 2010; Volkova et al., 2012) and sentence-level or document-level corpus construction (Quan and Ren, 2009; Das and Bandyopadhyay, 2009). Besides, all the related studies focus on supervised learn2.2 News Reader’s Emotion Classification While comment writer’s emotion classification has been extensively studied, there are only a few studies on news reader’s emotion classification from the NLP and related communities. Lin et al. (2007) first describe the task of reader’s emotion classification on the news articles and then employ some standard machine learning approaches to train a classifier for dete"
P13-2091,H05-1073,0,\N,Missing
S15-2113,W14-2608,0,0.145372,"ifying sarcasm which goes far behind the surface of the text and takes into account features on four levels: signatures, degree of unexpectedness, style, and emotional scenarios. They have demonstrated that these features do not help the identification in isolation. However, they do if they are combined in a complex framework. Barbieri and Saggion (2014) focused their approach on the use of lexical and semantic features, such as the frequency of the words in different reference corpora, the length of the words, and the number of related synsets in WordNet (Miller and Fellbaum, 1998). Finally, Buschmeier et al. (2014) assessed the impact of features used in previous studies, and they provide an important baseline for irony detection in English. Many datasets for the study of irony and sarcasm in Twitter are nowadays available. Thanks to the use of hashtags, it is easier to collect data with specific characteristics in Twitter. Reyes et al. (2013), for example, created a corpus of 40.000 tweets with four categories: Irony, Education, Humour, and Politics. Among the other resources, it is worth mentioning the sarcastic Amazon product reviews collected by Filatova (2012) and the Italian examples collected and"
S15-2113,W10-2914,0,0.0735455,"relying on the presence of emoticons, onomatopoeic expressions, and heavy punctuation in the text surface. Hao and Veale (2010) have investigated similes of the form “x as y” in a large corpus, proposing a method to automatically discriminate ironic from non-ironic similes. Tsur et al. (2010) proposed a semi-supervised approach for the automatic recognition of sarcasm in Amazon product reviews, exploiting some features that were specific to Amazon. Their method employed two modules: a semi-supervised acquisition of sarcastic patterns and a classifier. This method was then applied to tweets by Davidov et al. (2010), achieving even better results. Gonz´alez-Ib´an˜ ez et al. (2011) constructed a corpus of sarcastic tweets and used it to compare judgements made by humans and machine learning algorithms, concluding that none of them performed well. 674 More recently, Reyes et al. (2013) defined a complex model for identifying sarcasm which goes far behind the surface of the text and takes into account features on four levels: signatures, degree of unexpectedness, style, and emotional scenarios. They have demonstrated that these features do not help the identification in isolation. However, they do if they a"
S15-2113,filatova-2012-irony,0,0.0226777,"and Fellbaum, 1998). Finally, Buschmeier et al. (2014) assessed the impact of features used in previous studies, and they provide an important baseline for irony detection in English. Many datasets for the study of irony and sarcasm in Twitter are nowadays available. Thanks to the use of hashtags, it is easier to collect data with specific characteristics in Twitter. Reyes et al. (2013), for example, created a corpus of 40.000 tweets with four categories: Irony, Education, Humour, and Politics. Among the other resources, it is worth mentioning the sarcastic Amazon product reviews collected by Filatova (2012) and the Italian examples collected and annotated by Gianti et al. (2012), later used in Bosco et al. (2013). 3 Methodology 3.1 Data Pre-processing Considering the unregulated and arbitrary nature of the texts we are working with, we use some heuristic rules to pre-process them. These rules help us get more reliable syntactic structures when calling the syntactic parser. Twitter users often use repeated vowels (e.g. “loooove”) or capitalization (e.g. “LOVE”) to emphasize certain sentiments or emotions. The normalization consists of removing the repeated vowels (e.g. from “loooove” to “love”) a"
S15-2113,S15-2080,0,0.0253939,"d slang (“slng”) is not a self-evident task. The reason why none of these studies has proved to be the representative method that could widely be adopted and applied by other researchers is that they have not yet reached optimal results. Thus, the devising of a computational model able to accurately detect polarity is very much on-going. 673 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 673–678, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics This paper describes the model we developed for Task 11 of SemEval-2015 (Ghosh et al., 2015), which is concerned with the Sentiment Analysis of Figurative Language in Twitter. Our model came first in the SemEval-2015 task for irony and third in the overall ranking, showing that the features we proposed produce more reliable results in sentiment analysis of ironic tweets. 2 Related Work Irony is defined by Quintilian in the first century CE as “saying the opposite of what you mean” (Quintilian, 1922). It violates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashi"
S15-2113,P11-2102,0,0.158153,"f polarity shifting in sentiment analysis still requires much research. For example, Li, et.al. (2010), explores the polarity shifters in English which significantly improve the performance of sentiment analysis. Besides, figurative uses of Currently, there is no method that can guarantee the unequivocal recognition of irony or sarcasm. Training a computer to perform such a highly pragmatic task does indeed pose a challenge to computational linguists. A good number of studies have been recently devoted to finding a solution to the problem. Most of them have focused on tweets (Gonz´alez-Ib´an˜ ez et al., 2011; Reyes et al., 2013; Liebrecht et al., 2013; Riloff et al., 2013; Barbieri et al., 2014; Vanzo et al., 2014). Identifying figurative language in short messages (generally consisting of no more than 140 characters) that do not make use of conventional language, but employ “little space-consuming” elements, such as emoticons (“:D”), abbreviations (“abbr.”) and slang (“slng”) is not a self-evident task. The reason why none of these studies has proved to be the representative method that could widely be adopted and applied by other researchers is that they have not yet reached optimal results. Th"
S15-2113,P03-1054,0,0.0163421,"orm “?!”. We also compiled an emoticon dictionary based on training data and internet resources. Another step that we considered relevant at this point is the maximal matching segmentation. The segmentation is, in fact, often lost in tweets, as white spaces and punctuation are not always used in their customary format (e.g. “yeahright”). In order to get rid of this problem, we tried to segment all the out of vocabulary tokens through a maximal matching algorithm according to an English dictionary (e.g. the token “yeahright” would be segmented as “yeah right”). Finally, we use Stanford parser (Klein and Manning, 2003) to get the POS tags and dependency structures of the normalized tweets. 3.2 Feature Set After the pre-processing, we then extract features of the following kinds. UniToken Token uni-grams are the basic features in our approach. The normalized forms of the emphasized tokens are put in a special bag with tags describing their emphasis types {duplicate vowel, capitalized, heavy punctuation, emoticon} BiToken Bi-grams of the normalized tokens are also used as features. DepTokenPair The “parent-child” pairs based on dependency structures are also used as features. PolarityWin In order to identify"
S15-2113,W07-0101,0,0.121857,"Missing"
S15-2113,C10-1072,1,0.84487,"Missing"
S15-2113,W13-1605,0,0.0768567,"Missing"
S15-2113,D13-1066,0,0.102715,"Missing"
S15-2113,Y14-1047,0,0.237799,"r Task 11 of SemEval-2015 (Ghosh et al., 2015), which is concerned with the Sentiment Analysis of Figurative Language in Twitter. Our model came first in the SemEval-2015 task for irony and third in the overall ranking, showing that the features we proposed produce more reliable results in sentiment analysis of ironic tweets. 2 Related Work Irony is defined by Quintilian in the first century CE as “saying the opposite of what you mean” (Quintilian, 1922). It violates the expectations of the listener by flouting the maxim of quality (Grice, 1975; Stringfellow Jr, 1994; Gibbs and Colston, 2007; Tungthamthiti et al., 2014). In the same fashion, sarcasm is generally understood as the use of irony “to mock or convey contempt” (Stevenson, 2010). While irony and sarcasm are well studied in linguistics and psychology, their automatic identification through Natural Language Processing methods is a relatively novel task (Pang and Lee, 2008). Not to mention that irony and sarcasm pose a difficult problem in Sentiment Analysis of micro blogging and social media (Barbieri et al., 2014). Up to this date, several approaches have been proposed to automatically identify irony and sarcasm in tweets and comments. Carvalho et a"
S15-2113,C14-1221,0,0.0609998,"xplores the polarity shifters in English which significantly improve the performance of sentiment analysis. Besides, figurative uses of Currently, there is no method that can guarantee the unequivocal recognition of irony or sarcasm. Training a computer to perform such a highly pragmatic task does indeed pose a challenge to computational linguists. A good number of studies have been recently devoted to finding a solution to the problem. Most of them have focused on tweets (Gonz´alez-Ib´an˜ ez et al., 2011; Reyes et al., 2013; Liebrecht et al., 2013; Riloff et al., 2013; Barbieri et al., 2014; Vanzo et al., 2014). Identifying figurative language in short messages (generally consisting of no more than 140 characters) that do not make use of conventional language, but employ “little space-consuming” elements, such as emoticons (“:D”), abbreviations (“abbr.”) and slang (“slng”) is not a self-evident task. The reason why none of these studies has proved to be the representative method that could widely be adopted and applied by other researchers is that they have not yet reached optimal results. Thus, the devising of a computational model able to accurately detect polarity is very much on-going. 673 Proce"
S15-2113,baccianella-etal-2010-sentiwordnet,0,\N,Missing
S15-2113,E14-3007,0,\N,Missing
tokunaga-etal-2008-adapting,bel-etal-2000-simple,1,\N,Missing
tokunaga-etal-2008-adapting,P06-2106,1,\N,Missing
tokunaga-etal-2008-adapting,I08-1052,1,\N,Missing
tokunaga-etal-2008-adapting,francopoulo-etal-2006-lexical,1,\N,Missing
vossen-etal-2008-kyoto,W02-1304,1,\N,Missing
vossen-etal-2008-kyoto,W01-0703,1,\N,Missing
vossen-etal-2008-kyoto,magnini-cavaglia-2000-integrating,0,\N,Missing
vossen-etal-2008-kyoto,atserias-etal-2004-towards,1,\N,Missing
vossen-etal-2008-kyoto,soria-etal-2006-moving,1,\N,Missing
vossen-etal-2008-kyoto,chou-huang-2006-hantology,1,\N,Missing
vossen-etal-2008-kyoto,W06-1003,1,\N,Missing
W00-1205,Y96-1018,1,0.934457,"Missing"
W00-1205,C92-1019,1,0.846302,"Missing"
W00-1205,C96-2184,1,0.877286,"Missing"
W00-1205,O96-2006,1,\N,Missing
W00-1205,J93-2004,0,\N,Missing
W00-1205,O97-4003,1,\N,Missing
W00-1205,O94-1005,1,\N,Missing
W00-1205,O99-4004,1,\N,Missing
W02-1102,magnini-cavaglia-2000-integrating,0,0.0941427,"Missing"
W02-1102,W02-1106,1,0.860137,"ly extracted by its corresponding synset, there is simply no ambiguity in assigning the domain tags. And if there is more than one lexical item in that synset, all will share the same domain tag.           Geometry Algebra Metaphor Formal Abbreviate … Lexical Sources Latin Greece Spanish French American … Please note that by induction and actual examples from the lexical organization in WordNet, it is found that a hyponym is very likely to belong to the same domain as its hypernym. Similar results are also found for wordnet based cross-lingual inference of lexical semantic relations [4]. For instance, under the term &apos;mathematics,&apos; all the hyponyms below are related to this field of study. To make us of this lexical semantic phenomenon, we make a table of all the domain terms and map them to their unique WordNet sense identification number. Later we use the tree expansion method (discussed in more detail in Section 2.4) to trace down all the hyponyms. For example, by using this method, the hyponyms of Linguistics are all labeled as &apos;linguistics&apos; and so forth.     … Natural Science Physics Nuclear Chemistry Biology Palaeontology Botany Animal Fish Bird … Applied Science Me"
W02-1805,Y96-1018,1,0.809969,"be harder to predict and hence more ambiguous. In terms of the number of possible categories, more frequent words are more likely to have larger number of categories. Since we have just showed in last session that larger  ô õüúü ø A G »ø ô õüúü  Diagram 2. Frequency and ambiguity ø Ambiguity Æ Å Ä Ð ÃÁÈÄÆÈø Ã Ã ÈÃÃÃ ÄÃÃÃÃ ÄÈÃÃÃ  øö ÅÃÃÃÃ ÅÈÃÃÃ ÆÃÃÃÃ ÀÅØÀÃÈ ÆÈÃÃÃ Ù ø . Diagram 2 plots the degree of ambiguity of each across frequency ranges. As suggested by the ambiguous word in terms of its frequency in the two competing tendencies discussed above, our Sinica Corpus (Chen et al. 1996). Not only does exhaustive study actually shows that there is no the distribution of the degree of ambiguity vary correlation between degree of ambiguity and widely, the medium tendency line (thick black frequency. This generalization can be shown line in the diagram) varies barely perceptibly with even more clarity in Diagram 3. ø Æ  ô   õüú ü Å Ä ø  ô õüú ü   Diagram 3. Degree of Ambiguity vs. Frequency Ranking Ã Ã ÅÃÃ ÇÃÃ ÉÃÃ ËÃÃ ÄÃÃÃ ÄÅÃÃ Frequency Ranking In Diagram 3, entropy value of each word form is plotted against its frequency ranking. When word forms share the same fr"
W02-1805,O99-2001,0,0.0700975,"Missing"
W06-1003,huang-etal-2004-sinica,1,0.835312,"“curvatura, svolta, curva” (C). Therefore the procedure will propose a new candidate meronymy relation between the two Italian WordNet synsets (D). Figure 6. Schema of Wordnet Synsets Returned by WN Web Services. The scores returned by the method “GetWeightedSynsetsByIli” are used by our module to calculate the reliability rating for each new proposed relation. 3.3 A Case Study: Cross-fertilization between Italian and Chinese Wordnets. We explore this idea with a case-study involving the ItalianWordNet (Roventini et al., 2003) and the Academia Sinica Bilingual Ontological Wordnet (Sinica BOW, Huang et al., 2004). The BOW integrates three resources: WordNet, English-Chinese Translation Equivalents Database (ECTED), and SUMO (Suggested Upper Merged Ontology). With the integration of these three key resources, Sinica BOW functions both as an English-Chinese bilingual wordnet and a bilingual lexical access to SUMO. Sinica Bow currently has two bilingual versions, corresponding to WordNet 1.6. and 1.7. Based on these bootstrapped versions, a Chinese Wordnet (CWN, Huang et al. 2005) is under construction with handcrafted senses and lexical semantic relations. For the current experiment, we have used the ve"
W06-1003,kemps-snijders-etal-2006-lexus,0,0.0289533,"n guage away from where the language is spoken. Lastly, the vast range of diversity of languages also makes it impossible to have one single universal centralized resource, or even a centralized repository of resources. Although the paradigm of distributed and interoperable lexical resources has largely been discussed and invoked, very little has been made in comparison for the development of new methods and techniques for its practical realization. Some initial steps are made to design frameworks enabling inter-lexica access, search, integration and operability. An example is the Lexus tool (Kemps-Snijders et al., 2006), based on the Lexical Markup Framework (Romary et al., 2006), that goes in the direction of managing the exchange of data among large-scale lexical resources. A similar tool, but more tailored to the collaborative creation of lexicons for endangered language, is SHAWEL (Gulrajani and Harrison, 2002). However, the general impression is that little has been made towards the development of new methods and techniques for attaining a concrete interoperability among lexical resources. Admittedly, this is a long-term scenario requiring the contribution of many different actors and initiatives (among"
W06-1003,O05-5001,1,\N,Missing
W08-1907,H94-1111,0,0.0781824,"Missing"
W08-1907,O97-3005,0,0.0197996,"Missing"
W08-1907,I05-7002,1,0.916682,"with WordNet. Therefore, character-based and word-based ontologies are integrated to provide resources from character to word for Chinese language processing. 2.1. Hanzi and kanji: Shared Orthography of Two Typologically Different Languages Chinese and Japanese are two typologically different languages sharing the same orthography since they both use Chinese characters in written text. What makes this sharing of orthography unique among languages in the world is that Chinese characters (kanji in Japanese and hanzi in Chinese) explicitly encode information of semantic classification (Xyu 121, Chou and Huang 2005). This partially explains the process of Japanese adopting Chinese orthography even though the two languages are not related. The adaptation is supposed to be based on meaning and not on cognates sharing some linguistic forms. However, this meaning-based view of kanji/hanzi orthography faces a great challenge given the fact that Japanese and Chinese form-meaning pair do not have strict one-to-one mapping. There are meanings instantiated with different forms, as well as same forms representing different meanings. The character 湯 is one of most famous faux amis. It stands for ‘hot soup’ in Chine"
W08-1907,I05-1059,0,\N,Missing
W08-1907,I08-1052,1,\N,Missing
W08-1907,chou-huang-2006-hantology,1,\N,Missing
W08-1907,1995.mtsummit-1.17,0,\N,Missing
W09-3001,S07-1072,0,0.137074,"(i.e. assign emotion tags to emotional content.) Emotion computing often requires a large and high-quality annotated data, however, there is a lack of this kind of corpus. This is not only because of the enormous human involvement, but also because of the unavailability of emotion annotation scheme, which is robust and versatile for both emotion annotation and emotion computing. Tokuhisa et al. (2008) is the only work that explores the issue of emotion detection while most of the previous studies concentrate on the emotion classification given a known emotion context (Mihalcea and Liu, 2006; Kozareva et al., 2007.) Even for emotion classification, some issues remain unresolved, such as the complicated relationships among different emotion types, emotion type selection, and so on. Thus, it is still far from solving the emotion problem if emotion annotation is just considered as emotion-tag assignment. In this paper, we first explore the relationships among different emotion types with the support of a proposed emotion taxonomy, which combines some psychological theories and linguistic semantics. Based on the emotion taxonomy, a robust and versatile emotion annotation scheme is designed and used in both"
W09-3001,C08-1111,0,0.481275,"notation can be divided into two subtasks: emotion detection (i.e. differentiate emotional content from neutral content), which is a very important task for affective information extraction, and emotion classification (i.e. assign emotion tags to emotional content.) Emotion computing often requires a large and high-quality annotated data, however, there is a lack of this kind of corpus. This is not only because of the enormous human involvement, but also because of the unavailability of emotion annotation scheme, which is robust and versatile for both emotion annotation and emotion computing. Tokuhisa et al. (2008) is the only work that explores the issue of emotion detection while most of the previous studies concentrate on the emotion classification given a known emotion context (Mihalcea and Liu, 2006; Kozareva et al., 2007.) Even for emotion classification, some issues remain unresolved, such as the complicated relationships among different emotion types, emotion type selection, and so on. Thus, it is still far from solving the emotion problem if emotion annotation is just considered as emotion-tag assignment. In this paper, we first explore the relationships among different emotion types with the s"
W09-3303,baroni-etal-2008-cleaneval,0,0.0117928,"stencies from the French EuroWordnet. Later, Sagot and Fišer (2008) explained how they needed to recourse to PWN, BalkaNet (Tufis, 2000) and other resources (notably Wikipedia) to build WOLF, a free French WordNet that is promising but still a very preliminary resource. Some languages are straight-off purely under-resourced. The Web as Corpus initiative arose (Kilgarriff and Grefenstette, 2003) as an attempt to design tools and methodologies to use the web for overcoming data sparseness (Keller and Lapata, 2002). Nevertheless, this initiative raised non-trivial technical problems described in Baroni et al. (2008). Moreover, the web is not structured enough to easily and massively extract semantic relations. In this context, Wiktionary could appear to be a paradisiac playground for creating various lexiNote: The experiments of this paper are based on Wiktionary’s dumps downloaded in year 2008. Differences may be observed with the current versions available online. 1 Kuo Tzu-Yi Graduate Institute of Linguistics NTU, Taiwan tzuyikuo@ntu.edu.tw Introduction Reliable and comprehensive lexical resources constitute a crucial prerequisite for various NLP tasks. However their building cost keeps them rare. In"
W09-3303,zesch-etal-2008-extracting,0,0.0625772,"more languages only makes this observation more acute. In spite of various initiatives, costs make resource development extremely slow or/and result in non freely accessible resources. Collaborative resources might bring an attractive solution 19 Proceedings of the 2009 Workshop on the People’s Web Meets NLP, ACL-IJCNLP 2009, pages 19–27, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP cal resources. We describe below the Wiktionary resource and we explain the restrictions and problems we are facing when trying to exploit it. This description may complete few earlier ones, for example Zesch et al. (2008a). 2.1 2.2.2 Layouts In the following paragraph, we outline wiktionary’s general structure. We only consider words in the wiktionary’s own language. An entry consists of a graphical form and a corresponding article that is divided into the following, possibly embedded, sections: • etymology sections separate homonyms when relevant; • among an etymology section, different parts of speech may occur; • definitions and examples belong to a part of speech section and may be subdivided into subsenses; • translations, synonyms/antonyms and hypernyms/hyponyms are linked to a given part of speech, wit"
W09-3303,W08-1912,1,0.934088,"er, V is 4 http://it.wiktionary.org/w/index.php? title=cardinale&oldid=758205 5 http://en.wiktionary.org/wiki/WT:ELE 6 http://meta.wikimedia.org/wiki/List_ of_Wiktionaries 21 a set of words and E is defined by a relation R R E 7−→ E : (w1 , w2 ) ∈ E if and only if w1 → w2 . Most of lexical networks, as networks extracted from real world, are small worlds (SW) networks. Comparing structural characteristics of wiktionary-based lexical networks to some standard resource should be done according to wellknown properties of SW networks (Watts and Strogatz, 1998; Barabasi et al., 2000; Newman, 2003; Gaume et al., 2008). These properties are: • Edge sparsity: SW are sparse in edges m = O(n) or m = O(n log(n)) • Short paths: in SW, the average path length (L)7 is short. Generally there is at least one short path between any two nodes. • High clustering: in SW, the clustering coefficient (C) that expresses the probability that two distinct nodes adjacent to a given third one are adjacent, is an order of magnitude higher than for Erdos-Renyi (random) graphs: CSW  Crandom ; this indicates that the graph is locally dense, although it is globally sparse. • Heavy-tailed degree distribution: the distribution of the"
W09-3303,W02-1030,0,0.0283747,"ample, we consider French language resources. Jacquin et al. (2002) highlighted the limitations and inconsistencies from the French EuroWordnet. Later, Sagot and Fišer (2008) explained how they needed to recourse to PWN, BalkaNet (Tufis, 2000) and other resources (notably Wikipedia) to build WOLF, a free French WordNet that is promising but still a very preliminary resource. Some languages are straight-off purely under-resourced. The Web as Corpus initiative arose (Kilgarriff and Grefenstette, 2003) as an attempt to design tools and methodologies to use the web for overcoming data sparseness (Keller and Lapata, 2002). Nevertheless, this initiative raised non-trivial technical problems described in Baroni et al. (2008). Moreover, the web is not structured enough to easily and massively extract semantic relations. In this context, Wiktionary could appear to be a paradisiac playground for creating various lexiNote: The experiments of this paper are based on Wiktionary’s dumps downloaded in year 2008. Differences may be observed with the current versions available online. 1 Kuo Tzu-Yi Graduate Institute of Linguistics NTU, Taiwan tzuyikuo@ntu.edu.tw Introduction Reliable and comprehensive lexical resources co"
W09-3303,J03-3001,0,0.0277846,"of lexical resources, be it due to the low-quality or non-existence of such resources, or to copyrightsrelated problems. As an example, we consider French language resources. Jacquin et al. (2002) highlighted the limitations and inconsistencies from the French EuroWordnet. Later, Sagot and Fišer (2008) explained how they needed to recourse to PWN, BalkaNet (Tufis, 2000) and other resources (notably Wikipedia) to build WOLF, a free French WordNet that is promising but still a very preliminary resource. Some languages are straight-off purely under-resourced. The Web as Corpus initiative arose (Kilgarriff and Grefenstette, 2003) as an attempt to design tools and methodologies to use the web for overcoming data sparseness (Keller and Lapata, 2002). Nevertheless, this initiative raised non-trivial technical problems described in Baroni et al. (2008). Moreover, the web is not structured enough to easily and massively extract semantic relations. In this context, Wiktionary could appear to be a paradisiac playground for creating various lexiNote: The experiments of this paper are based on Wiktionary’s dumps downloaded in year 2008. Differences may be observed with the current versions available online. 1 Kuo Tzu-Yi Gradua"
W09-3418,W06-1001,0,0.116172,"ies a precise expression of sense relations (Huang et al., 2008). In recent years, WordNet-like resources have become one of the most reliable and essential resources for linguistic studies for all languages (Magnini and Cavaglia, 2000; Soria et al. 2009; Strapparava and Valitutti, 2004). Lexical Markup Framework (LMF, ISO24613) is the ISO standard which provides a common standardized framework for the construction of natural language processing lexicons (Francopoulo et al., 2009). One important purpose of LMF is to define a standard for lexicons which covers multilingual lexical information (Francopoulo et al., 2006b). In this study, we describe the design and implementation of the Wordnet-LMF (Soria et al. 2009) to represent lexical semantics in Chinese WordNet. The rest of this paper is organized as follows: Section 2 introduces Chinese WordNet and Lexical Markup Framework. Section 3 describes how we represent Chinese WordNet in the Lexical Markup Framework (CWN-LMF). Section 4 presents an example on Chinese word sense distinction using CWN-LMF format. Quantitative analysis of compiled CWN-LMF is presented in Section 5. We also describe the application scenario using CWN-LMF for information interoperab"
W09-3418,magnini-cavaglia-2000-integrating,0,0.458758,"integrates WordNet, English-Chinese Translation Equiva1 Wordnet, available online at http://wordnetweb.princeton.edu/perl/webwn 2 Global WordNet Association (GWA), available online at http://www.globalwordnet.org/ lents Database (ECTED) and SUMO for crosslanguage linguistic studies. As a follow-up, Chinese WordNet (CWN) has been built as a robust lexical knowledge system which also embodies a precise expression of sense relations (Huang et al., 2008). In recent years, WordNet-like resources have become one of the most reliable and essential resources for linguistic studies for all languages (Magnini and Cavaglia, 2000; Soria et al. 2009; Strapparava and Valitutti, 2004). Lexical Markup Framework (LMF, ISO24613) is the ISO standard which provides a common standardized framework for the construction of natural language processing lexicons (Francopoulo et al., 2009). One important purpose of LMF is to define a standard for lexicons which covers multilingual lexical information (Francopoulo et al., 2006b). In this study, we describe the design and implementation of the Wordnet-LMF (Soria et al. 2009) to represent lexical semantics in Chinese WordNet. The rest of this paper is organized as follows: Section 2 in"
W09-3418,huang-etal-2004-sinica,1,0.944371,"MF used to represent lexical semantics in Chinese WordNet. The compiled CWN-LMF will be released to the community for linguistic researches. 1 Introduction Princeton WordNet 1 is an English lexical database that groups nouns, verbs, adjectives and adverbs into sets of cognitive synonyms, which are named as synsets (Fellbaum, 1998; Miller, 1995). The Global WordNet Association (GWA) 2 built on the results of Princeton WordNet and Euro WordNet (Vossen, 2004) is a free and public association that provides a platform to share and connect all languages in the world. For Mandarin Chinese in Taiwan, Huang et al. (2004) constructed the Academia Sinica Bilingual Ontological Wordnet (Sinica BOW) which integrates WordNet, English-Chinese Translation Equiva1 Wordnet, available online at http://wordnetweb.princeton.edu/perl/webwn 2 Global WordNet Association (GWA), available online at http://www.globalwordnet.org/ lents Database (ECTED) and SUMO for crosslanguage linguistic studies. As a follow-up, Chinese WordNet (CWN) has been built as a robust lexical knowledge system which also embodies a precise expression of sense relations (Huang et al., 2008). In recent years, WordNet-like resources have become one of the"
W09-3418,I05-3014,1,0.746685,"analyzed according to the guidelines of Chinese word sense distinctions (CKIP, 2003; Huang et al. 2003) which contain information including Partof-Speech, sense definition, example sentences, corresponding English synset(s) from Princeton WordNet, lexical semantic relations and so on. Unlike Princeton WordNet, CWN has not been constructed mainly on the synsets and semantic relations. Rather it focuses to provide precise expression for the Chinese sense division and the semantic relations needs to be based on the linguistic theories, especially lexical semantics (Huang et al., 2008). Moreover, Huang et al. (2005) designed and implemented the Sinica Sense Management System (SSMS) to store and manage word sense data generated in the analysis stage. SSMS is meaning-driven. Each sense of a lemma is identified specifically using a unique identifier and given a separate entry. There are 8,646 lemmas / 25,961 senses until December 2008 have been analyzed and stored in SSMS. Figure 1 shows the result of sense distinction for 足跡 zu-ji ‘footprint’ as an example in Chinese WordNet. Huang et al. (2004) proposed Domain LexicoTaxonomy (DLT) as a domain taxonomy populated with lexical entries. By using DLT with Chin"
W09-3418,strapparava-valitutti-2004-wordnet,0,0.058237,"n Equiva1 Wordnet, available online at http://wordnetweb.princeton.edu/perl/webwn 2 Global WordNet Association (GWA), available online at http://www.globalwordnet.org/ lents Database (ECTED) and SUMO for crosslanguage linguistic studies. As a follow-up, Chinese WordNet (CWN) has been built as a robust lexical knowledge system which also embodies a precise expression of sense relations (Huang et al., 2008). In recent years, WordNet-like resources have become one of the most reliable and essential resources for linguistic studies for all languages (Magnini and Cavaglia, 2000; Soria et al. 2009; Strapparava and Valitutti, 2004). Lexical Markup Framework (LMF, ISO24613) is the ISO standard which provides a common standardized framework for the construction of natural language processing lexicons (Francopoulo et al., 2009). One important purpose of LMF is to define a standard for lexicons which covers multilingual lexical information (Francopoulo et al., 2006b). In this study, we describe the design and implementation of the Wordnet-LMF (Soria et al. 2009) to represent lexical semantics in Chinese WordNet. The rest of this paper is organized as follows: Section 2 introduces Chinese WordNet and Lexical Markup Framework"
W09-3418,P06-2106,1,0.884198,"Missing"
W09-3418,vossen-etal-2008-kyoto,1,0.832458,"guages. A unified framework is needed for information exchange. LMF is hence adopted as the framework at lexical semantic level in this project. The WordNet in these languages are compiled with designed WordNet-LMF format. CWN-LMF will also be involved and benefit for cross-language interpretabilities in semantic search field. 7 6 Application Scenarios The EU-7 project, KYOTO (Knowledge Yielding Ontologies for Transition-based Organization), wants to make knowledge sharable between communities of people, culture, language and computers, by assigning meaning to text and giving text to meaning (Vossen et al., 2008a; 2008b). The goal of KYOTO is a system that allows people in communities to define the meaning of their words and terms in a shared Wiki platform so that it becomes anchored across languages and cultures but also so that a computer can use this knowledge to detect knowledge and facts in text. KYOTO is a generic system offering knowledge transition and information across different target groups, transgressing linguistic, cultural and geographic boundaries. Initially developed for the environmental domain, KYOTO will be usable in any knowledge domain for mining, organizing, and distributing in"
W09-3418,O05-5001,1,\N,Missing
W09-3421,francopoulo-etal-2006-lexical,1,0.857972,"Missing"
W09-3421,bel-etal-2000-simple,1,0.763962,"the advantages of corpusbased approaches is that the techniques used are less language specific than classical rulebased approaches where a human analyses the behaviour of target languages and constructs rules manually. This naturally led the way for international resource standardisation, and indeed there is a long standing precedent in the West for it. The Human Language Technology (HLT) society in Europe has been particularly zealous in this regard, propelling the creation of resource interoperability through a series of initiatives, namely EAGLES (Sanfilippo et al., 1999), PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Ide et al., 2003), and LIRICS1 . These 1 • Based on existing description frameworks, each research member tries to describe several lexical entries and find problems with them. • Through periodical meetings, we exchange information about problems found and generalise them to propose solutions. • Through an implementation of an application system, we verify the effectiveness of the proposed framework. Below we summarise our significant contribution to an International Standard (ISO24613; Lexical Markup Framework: LMF). 1st year After considering many characteristics of Asian langua"
W09-3421,W03-1905,1,0.826021,"pproaches is that the techniques used are less language specific than classical rulebased approaches where a human analyses the behaviour of target languages and constructs rules manually. This naturally led the way for international resource standardisation, and indeed there is a long standing precedent in the West for it. The Human Language Technology (HLT) society in Europe has been particularly zealous in this regard, propelling the creation of resource interoperability through a series of initiatives, namely EAGLES (Sanfilippo et al., 1999), PAROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Ide et al., 2003), and LIRICS1 . These 1 • Based on existing description frameworks, each research member tries to describe several lexical entries and find problems with them. • Through periodical meetings, we exchange information about problems found and generalise them to propose solutions. • Through an implementation of an application system, we verify the effectiveness of the proposed framework. Below we summarise our significant contribution to an International Standard (ISO24613; Lexical Markup Framework: LMF). 1st year After considering many characteristics of Asian languages, we elucidated the shortco"
W09-3421,P06-2106,1,0.822025,"Missing"
W09-3421,tokunaga-etal-2008-adapting,1,0.730656,"Missing"
W09-3421,W06-1001,1,\N,Missing
W10-0206,H05-1073,0,0.0297807,"Missing"
W10-0206,W09-3001,1,0.74252,"surprised by this exquisite performance.” (5) Ni2ao4-Leo de-POSS hua4-word hen3-very ling4-make kai3luo4lin2-Caroline shang1xin1-sad. “Caroline was very saddened by Leo’s words.” (6) Dui4yu2-for wei4lai2-future, lao3shi2shuo1-frankly wo31.SG hen3-very hai4pa4-scared. “Frankly, I am very scared about the future.” The causes in (1) and (2) are propositional causes, which indicate the actual events involved in causing the emotions. The ones in (3) and (4) are nominalized causes, whereas (5) and (6) involve nominal causes 3.2 Based on the list of 91 Chinese primary emotion keywords identified in Chen et al. (2009), we extract 6,058 instances of sentences by keyword matching from the Sinica Corpus 1 , which is a tagged balanced corpus of Mandarin Chinese containing a total of ten million words. Each instance contains the focus sentence with the emotion keyword “&lt;FocusSentence>” plus the sentence before “&lt;PrefixSentence>” and after “&lt;SuffixSentence>” it. The extracted instances include all primary emotion keywords occurring in the Sinica Corpus except for the emotion class happiness, as the keywords of happiness exceptionally outnumber other emotion classes. In order to balance the number of each emotion"
W10-0206,S07-1094,0,0.0189096,"Missing"
W10-0206,S07-1072,0,0.027591,"Missing"
W10-0206,lee-etal-2010-emotion,1,0.612111,"ions of external events and in turn trigger reactions. The ability to detect implicit invoking causes as well as predict actual reactions will add rich dimensions to emotion analysis and lead to further research on event computing. 3 Emotion Cause Corpus This section briefly describes how the emotion cause corpus is constructed. We first explain what 46 an emotion cause is and discuss how emotion cause is linguistically expressed in Chinese. We then describe the corpus data and the annotation scheme. For more detailed discussion on the construction of the emotion cause corpus, please refer to Lee et al. (2010). 3.1 Cause Events Following Talmy (2000), the cause of an emotion should be an event itself. In this work, it is called a cause event. By cause event, we do not necessarily mean the actual trigger of the emotion or what leads to the emotion. Rather, it refers to the immediate cause of the emotion, which can be the actual trigger event or the perception of the trigger event. Adapting TimeML annotation scheme (Saurí et al. 2004), events refer to situations that happen or occur. In this study, cause events specifically refer to the explicitly expressed arguments or events that are highly linked"
W10-0206,C08-1111,0,0.0547333,"nt. Experiments show that our system achieves a promising performance for cause occurrence detection as well as cause event detection. The current study should lay the ground for future research on the inferences of implicit information and the discovery of new information based on cause-event relation. 1 Introduction Text-based emotion processing has attracted plenty of attention in NLP. Most research has focused on the emotion detection and classification by identifying the emotion types, for instances happiness and sadness, for a given sentence or document (Alm 2005, Mihalcea and Liu 2006, Tokuhisa et al. 2008). However, on top of this surface level information, deeper level information regarding emotions, such as the experiencer, cause, and result of an emotion, needs to be extracted and analyzed for real world applications (Alm 2009). Chu-Ren Huang†‡ ‡ Institute of Linguistics Academia Sinica In this paper, we aim at mining one of the crucial deep level types of information, i.e. emotion cause, which provides useful information for applications ranging from economic forecasting, public opinion mining, to product design. Emotion cause detection is a new research area in emotion processing. In emoti"
W10-2102,P08-1105,0,\N,Missing
W10-2102,N09-2040,0,\N,Missing
W10-2102,P02-1054,0,\N,Missing
W10-4102,Y09-1011,1,0.804785,"ad, they choose to rely on the intuition of annotators (Ren’s Blog Emotion Corpus, RBEC, Quan and Ren, 2009) or authors (Mishne’s blog emotion corpus, Mishne, 2005). Therefore, one of the crucial drawbacks of emotion corpora is the problem of poor quality. In this paper, we explore emotion annotation from a different perspective. We concentrate on explicit emotions, and utilize their contextual information for emotion recognition. In terms of emotion representation, textual emotion corpora are basically annotated using either the enumerative representation or the compositional representation (Chen et al., 2009). The enumerative representation assigns an emotion a unique label, such as pride and jealousy. The compositional representation represents an emotion through a vector with a small set of fixed basic emotions with associated strength. For instance, pride is decomposed into “happiness + fear” according to Turner (2000). With regard to emotion recognition technologies, there are two kinds of classification models. One is based on an independent view (Mishne, 2005; Mihalcea and Liu, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Strapparava and Mihalcea, 2008), and the other is a depende"
W10-4102,C10-1021,1,0.757943,"nt view takes into account complicated emotion expressions, such as emotion interaction and emotion co-occurrences, and thus requires more complicated models. Abbasi et al. (2008) adopt an ensemble classifier to detect the cooccurrences of different emotions; Keshtkar and Inkpen (2009) use iteratively single-label classifiers in the top-down order of a given emotion hierarchy. In this paper, we examine emotion recognition as a multi-label problem and investigate several multi-label classification approaches. 2.2 2009), and the other is for emotion cause detection, which is manually annotated (Chen et al. 2010). Emotion Cause Detection Although most emotion theories recognize the important role of causes in emotion analysis (Descartes, 1649; James, 1884; Plutchik, 1962; Wierzbicka 1996), yet very few studies in NLP explore the event composition and causal relation of emotions. As a pilot study, the current study proposes an emotion cause detection system. Emotion cause detection can be considered as a kind of causal relation detection between two events. In other words, emotion is envisioned as an event type which triggers another event, i.e. cause event. We attempt to examine emotion cause relation"
W10-4102,W10-0217,0,0.0606646,"Missing"
W10-4102,W03-1210,0,0.0268849,"important role of causes in emotion analysis (Descartes, 1649; James, 1884; Plutchik, 1962; Wierzbicka 1996), yet very few studies in NLP explore the event composition and causal relation of emotions. As a pilot study, the current study proposes an emotion cause detection system. Emotion cause detection can be considered as a kind of causal relation detection between two events. In other words, emotion is envisioned as an event type which triggers another event, i.e. cause event. We attempt to examine emotion cause relations for open domains. However, not much work (Marcu and Echihabi, 2002; Girju, 2003; Chang and Choi, 2006) has been done on this kind of general causal relation for open domains. Most existing causal relation detection systems contain two steps: 1) cause candidate identification; 2) causal relation detection. However, Step 1) is often oversimplified in real systems. For example, the cause-effect pairs are limited to two noun phrases (Chang and Choi, 2005; Girju, 2003), or two clauses connected with selected conjunction words (Marcu and Echihabi, 2002). Moreover, the task of Step 2) often is considered as a binary classification problem, i.e. “causal” vs. “noncausal”. With re"
W10-4102,W10-0202,0,0.0475986,"Missing"
W10-4102,P02-1047,0,0.0136598,"ion theories recognize the important role of causes in emotion analysis (Descartes, 1649; James, 1884; Plutchik, 1962; Wierzbicka 1996), yet very few studies in NLP explore the event composition and causal relation of emotions. As a pilot study, the current study proposes an emotion cause detection system. Emotion cause detection can be considered as a kind of causal relation detection between two events. In other words, emotion is envisioned as an event type which triggers another event, i.e. cause event. We attempt to examine emotion cause relations for open domains. However, not much work (Marcu and Echihabi, 2002; Girju, 2003; Chang and Choi, 2006) has been done on this kind of general causal relation for open domains. Most existing causal relation detection systems contain two steps: 1) cause candidate identification; 2) causal relation detection. However, Step 1) is often oversimplified in real systems. For example, the cause-effect pairs are limited to two noun phrases (Chang and Choi, 2005; Girju, 2003), or two clauses connected with selected conjunction words (Marcu and Echihabi, 2002). Moreover, the task of Step 2) often is considered as a binary classification problem, i.e. “causal” vs. “noncau"
W10-4102,W02-1011,0,0.0116803,"Missing"
W10-4102,W10-0209,0,0.022508,"Missing"
W10-4102,P09-1095,0,0.0127337,"For example, the cause-effect pairs are limited to two noun phrases (Chang and Choi, 2005; Girju, 2003), or two clauses connected with selected conjunction words (Marcu and Echihabi, 2002). Moreover, the task of Step 2) often is considered as a binary classification problem, i.e. “causal” vs. “noncausal”. With regard to feature extraction, there are two kinds of information extracted to identify the causal relation in Step 2). One is constructions expressing a cause-effect relation (Chang and Choi, 2005; Girju, 2003), and the other is semantic information in a text (Marcu and Echihabi, 2002; Persing and Ng, 2009), such as word pair probability. Undoubtedly, the two kinds of information often interact with each other in a real cause detection system. 3 Emotion (EASC) Annotated Sinica Corpus EASC is an emotion annotated corpus comprising two kinds of sentences: emotional-sentence corpus and neutral-sentence corpus. It involves two components: one for emotion recognition, which is created with an unsupervised method (Chen et al. 3.1 The Corpus for Emotion Recognition With the help of a set of rules and a collection of high quality emotion keywords, a pattern-based approach is used to extract emotional se"
W10-4102,D09-1150,0,0.0226354,"cerning emotion remain unresolved, such as emotion definition, emotion representation, and emotion classification technologies. For the emotion definition, emotion has been well-known for its abstract and uncertain definition which hinders emotion processing as a whole. Ortony et al., (1987) conducted an empirical study for a structure of affective lexicon based on the ~500 words used in previous emotion studies. However, most of the emotion corpora in NLP try to avoid the emotion definition problem. Instead, they choose to rely on the intuition of annotators (Ren’s Blog Emotion Corpus, RBEC, Quan and Ren, 2009) or authors (Mishne’s blog emotion corpus, Mishne, 2005). Therefore, one of the crucial drawbacks of emotion corpora is the problem of poor quality. In this paper, we explore emotion annotation from a different perspective. We concentrate on explicit emotions, and utilize their contextual information for emotion recognition. In terms of emotion representation, textual emotion corpora are basically annotated using either the enumerative representation or the compositional representation (Chen et al., 2009). The enumerative representation assigns an emotion a unique label, such as pride and jeal"
W10-4102,C08-1111,0,0.0234916,"enumerative representation or the compositional representation (Chen et al., 2009). The enumerative representation assigns an emotion a unique label, such as pride and jealousy. The compositional representation represents an emotion through a vector with a small set of fixed basic emotions with associated strength. For instance, pride is decomposed into “happiness + fear” according to Turner (2000). With regard to emotion recognition technologies, there are two kinds of classification models. One is based on an independent view (Mishne, 2005; Mihalcea and Liu, 2006; Aman and Szpakowicz, 2007; Tokuhisa et al., 2008; Strapparava and Mihalcea, 2008), and the other is a dependent view (Abbasi et al, 2008; Keshtkar and Inkpen, 2009). The independent view treats emotions separately, and often chooses a single-label classification approach to identify emotions. In contrast, the dependent view takes into account complicated emotion expressions, such as emotion interaction and emotion co-occurrences, and thus requires more complicated models. Abbasi et al. (2008) adopt an ensemble classifier to detect the cooccurrences of different emotions; Keshtkar and Inkpen (2009) use iteratively single-label classifiers in"
W10-4152,S07-1024,1,\N,Missing
W10-4152,P98-1012,0,\N,Missing
W10-4152,C98-1012,0,\N,Missing
W10-4152,S07-1012,0,\N,Missing
W14-4715,W10-0701,0,0.0747281,"ntal paradigm does not allow manipulation of a large number of subjects simultaneously. In this paper, we explore the possibility of conducting lexical access related experiments through crowdsourcing. With the crowdsourcing experiments, we intend to ask specific question about the share strategy of determination of lexical units, as well as determination of semantic transparencies, two issues that would have direct implications of how individuals access their mental lexicon. Many scholars discuss applying crowdsourcing method to language resource construction recent years (Snow et al., 2008; Callison-Burch and Dredze, 2010; Munro et al., 2010; Gurevych and Zesch, 2013). Crowdsourcing has been proved to be an efficient tool to build lexical resources, for example, Wiktionary, whose goal is to become the free online dictionary for all the words in all languages; Biemann (2013) presents another example which creates the Turk Bootstrap Word Sense Inventory for 397 frequent nouns from scratch using Amazon Mechanical Turk. And there is more and more literature focusing on conducting experiments on crowdsourcing platforms (Schnoebelen and Kuperman, 2010; Paolacci et al., 2010; Berinsky et al., 2011; Rand, 2012; Mason"
W14-4715,W10-0719,0,0.314462,"pulation of a large number of subjects simultaneously. In this paper, we explore the possibility of conducting lexical access related experiments through crowdsourcing. With the crowdsourcing experiments, we intend to ask specific question about the share strategy of determination of lexical units, as well as determination of semantic transparencies, two issues that would have direct implications of how individuals access their mental lexicon. Many scholars discuss applying crowdsourcing method to language resource construction recent years (Snow et al., 2008; Callison-Burch and Dredze, 2010; Munro et al., 2010; Gurevych and Zesch, 2013). Crowdsourcing has been proved to be an efficient tool to build lexical resources, for example, Wiktionary, whose goal is to become the free online dictionary for all the words in all languages; Biemann (2013) presents another example which creates the Turk Bootstrap Word Sense Inventory for 397 frequent nouns from scratch using Amazon Mechanical Turk. And there is more and more literature focusing on conducting experiments on crowdsourcing platforms (Schnoebelen and Kuperman, 2010; Paolacci et al., 2010; Berinsky et al., 2011; Rand, 2012; Mason and Suri, 2012; Crum"
W14-4715,D08-1027,0,0.267321,"Missing"
W14-5301,W10-1810,0,0.0289607,"hinese. Huang et al. (2013), the only such study based on comparable corpora so far, suggested that the subtlety of the underlining grammatical variations of these two dialectal variants at early stage of divergence may have contributed to the challenge as well as scarcity of previous studies. 1.2 Light Verbs in Light Verb Variations The study of English light verb constructions (LVCs) (e.g., take a look, make an offer) has been an important topic in linguistics (Jespersen, 1965; Butt and Geuder, 2003; among others) as well as in Computational Linguistics (Tu and Dan, 2011; Nagy et al., 2013; Hwang et al., 2010; among others). Identification of LVCs is a fundamental crucial task for Natural Language Processing (NLP) applications, such as information retrieval and machine translation. For example, Tu and Dan (2011) proposed a supervised learning system to automatically identify English LVCs by training with groups of contextual or statistical features. Nagy et al. (2013) introduced a system that enables the full coverage identification of English LVCs in running context by using a machine leaning approach. However, little work has been done to identify Chinese LVCs, especially between different varia"
W14-5301,W14-5810,1,0.910564,"piao proceed cast-ticket ‘to cast votes’, where the complement is in the V(erb)-O(bject) form, usually can only be found in Taiwan Mandarin. Hence, Chinese LVCs are challenging for both linguistic studies and computational applications in two aspects: (a) to identify collocation constraints of the different light verbs in order to automatically classify and predict their uses in context, and (b) to identify the collocation constraints of the same light verb in order to differentiate and predict the two Chinese variants based on the use of such light verbs. The first issue has been explored in Lin et al. (2014): by analyzing Mainland and Taiwan Mandarin data extracted from comparable corpora with statistical and machine learning approaches, the authors find the five light verbs 從事 congshi, 搞 gao, 加以 jiayi, 進行 jinxing, and 做 zuo can be reliably differentiated from each other in each variety. But to the best of our knowledge, there has been no previous computational study on modeling the light verb variations, or other syntactic variations of Chinese dialects or variants of the same dialect. Therefore, this paper builds on the study of Lin et al. (2014) and will adopt a comparable corpus driven approa"
W14-5301,I13-1038,0,0.169754,"inland and Taiwan Chinese. Huang et al. (2013), the only such study based on comparable corpora so far, suggested that the subtlety of the underlining grammatical variations of these two dialectal variants at early stage of divergence may have contributed to the challenge as well as scarcity of previous studies. 1.2 Light Verbs in Light Verb Variations The study of English light verb constructions (LVCs) (e.g., take a look, make an offer) has been an important topic in linguistics (Jespersen, 1965; Butt and Geuder, 2003; among others) as well as in Computational Linguistics (Tu and Dan, 2011; Nagy et al., 2013; Hwang et al., 2010; among others). Identification of LVCs is a fundamental crucial task for Natural Language Processing (NLP) applications, such as information retrieval and machine translation. For example, Tu and Dan (2011) proposed a supervised learning system to automatically identify English LVCs by training with groups of contextual or statistical features. Nagy et al. (2013) introduced a system that enables the full coverage identification of English LVCs in running context by using a machine leaning approach. However, little work has been done to identify Chinese LVCs, especially bet"
W14-5301,W11-0807,0,0.146593,"Missing"
W14-5810,W14-5301,1,0.855344,"tences where jiayi takes complements denoting accomplishment events, e.g. gaizheng ‘to correct’ and jiejue ‘to solve’. However, jiayi in Taiwan corpus mainly takes complements denoting activity events, and thus almost all instances of Taiwan jiayi are mixed with those of the other light verbs. Meanwhile, our results show a tendency that all other light verbs (jinxing, congshi, zuo, and gao) mostly take activity complements but fewer accomplishment complements in both Taiwan and Mainland corpora. More discussion on the light verb variations between Mainland and Taiwan Mandarin can be found in (Huang et al., 2014). jinxing gao zuo jiayi congshi 0 2 2 0 68 0 Mainland 1 2 3 32 110 23 33 116 41 36 80 14 0 161 0 67 66 21 4 37 11 81 0 46 0 30 120 19 0 90 Taiwan 1 2 3 10 77 20 23 30 0 4 47 5 0 1 6 20 68 0 4 64 31 132 196 22 Table 7: Clustering results on Mainland and Taiwan corpora. 4 4.1 Applications and Implications Implications for Future Studies In the study above, we were able to annotate a corpus with all the types of significant context and, based on this annotated corpus, we were able to use statistic model to differentiate the use of different light verbs in different contexts. Such a module of gene"
W14-5810,I13-1038,0,0.120906,"ur results show that a language resource based methodology optimally incorporating linguistic information can resolve challenges posed by light verbs in NLP. 1 Introduction Identification of Light Verb Construction (LVC) plays an important role and poses a special challenge in many Natural Language Processing (NLP) applications, e.g. information retrieval and machine translation. In addition to addressing issues related to LVC as a contributing factor to errors for various applications, a few computational linguistics studies have targeted LVC in English specifically (e.g., Tu and Roth, 2011; Nagy et al., 2013). To the best of our knowledge, however, there has been no computational linguistic study dealing with LVCs in Chinese specifically. It is important to know that, due to their lack of semantic content, light verbs can behave rather idiosyncratically in each language. Chinese LVC, in particular, has the characteristic that allows many different light verbs to share similar usage and be interchangeable in some context. We should also note that light verbs in Chinese can take both verbs, deverabal nouns, and eventive nouns, while the morphological status of these categories are typically unmarked"
W14-5810,W11-0807,0,0.110596,"nnotated corpora. Our results show that a language resource based methodology optimally incorporating linguistic information can resolve challenges posed by light verbs in NLP. 1 Introduction Identification of Light Verb Construction (LVC) plays an important role and poses a special challenge in many Natural Language Processing (NLP) applications, e.g. information retrieval and machine translation. In addition to addressing issues related to LVC as a contributing factor to errors for various applications, a few computational linguistics studies have targeted LVC in English specifically (e.g., Tu and Roth, 2011; Nagy et al., 2013). To the best of our knowledge, however, there has been no computational linguistic study dealing with LVCs in Chinese specifically. It is important to know that, due to their lack of semantic content, light verbs can behave rather idiosyncratically in each language. Chinese LVC, in particular, has the characteristic that allows many different light verbs to share similar usage and be interchangeable in some context. We should also note that light verbs in Chinese can take both verbs, deverabal nouns, and eventive nouns, while the morphological status of these categories ar"
W14-5818,W10-0701,0,0.0723145,"ts to support their own studies. But this kind of datasets are usually relatively small and restrictive, so cannot be used widely, for example, (徐彩华 and 李镗, 2001; Myers et al., 2004; 干 红梅, 2008; Mok, 2009), etc. Some datasets, although large enough and can be used in other studies, are not publicly accessible, for example, (王春茂 and 彭聃龄, 1999; 高兵 and 高峰强, 2005), etc. A large and publicly accessible semantic transparency dataset of Chinese compounds is still a gap in Chinese language resources. Crowdsourcing, as an emerging method of data collection and resource construction (Snow et al., 2008; Callison-Burch and Dredze, 2010; Munro et al., 2010; Schnoebelen and Kuperman, 2010; Gurevych and Zesch, 2013; Wang et al., 2013) and an emerging method of behavioral experiment (Paolacci et al., 2010; This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 147 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 147–156, Coling 2014, Dublin, Ireland, August 24 2014. Berinsky et al., 2011; Mason and Suri, 2012; Rand, 2012; Crump"
W14-5818,Y96-1018,1,0.785665,"monosyllabic nouns, N; b) the set of monosyllabic adjectives, A; and c) the set of monosyllabic verbs, V. (2) Extract the words of the structure NN, AN, or VN 1 from the “Lexicon of Common Words in Contemporary Chinese” (现代汉语常用词表). In this step, NN means both morphemes of the word appear in the set N; AN means the first morpheme appears in the set A and the second appears in the set N; VN means the first morpheme appears in the set V and the second appears in the set N. After this step, we get “word list 1”. (3) Extract the words which have mid-range frequencies 2 from the Sinica Corpus 4.0 (Chen et al., 1996). These words are represented in traditional Chinese characters. We convert them into simplified Chinese characters and only reserve the words which also appear in “word list 1”. After this step, we get “word list 2”. (4) Manually verify “word list 2” to generate the final list. Things need to be verified include the following aspects. (a) Because in “word list 2” word structures are judged automatically, there are many errors, so we have to verify the correctness of the word structure judgments. (b) We have to make sure that the morphemes of each word are free morphemes. (c) We also need to d"
W14-5818,W10-0719,0,0.320223,"But this kind of datasets are usually relatively small and restrictive, so cannot be used widely, for example, (徐彩华 and 李镗, 2001; Myers et al., 2004; 干 红梅, 2008; Mok, 2009), etc. Some datasets, although large enough and can be used in other studies, are not publicly accessible, for example, (王春茂 and 彭聃龄, 1999; 高兵 and 高峰强, 2005), etc. A large and publicly accessible semantic transparency dataset of Chinese compounds is still a gap in Chinese language resources. Crowdsourcing, as an emerging method of data collection and resource construction (Snow et al., 2008; Callison-Burch and Dredze, 2010; Munro et al., 2010; Schnoebelen and Kuperman, 2010; Gurevych and Zesch, 2013; Wang et al., 2013) and an emerging method of behavioral experiment (Paolacci et al., 2010; This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 147 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 147–156, Coling 2014, Dublin, Ireland, August 24 2014. Berinsky et al., 2011; Mason and Suri, 2012; Rand, 2012; Crump et al., 2013), is a"
W14-5818,D08-1027,0,0.200686,"Missing"
W14-5819,J12-2006,0,0.0297169,"sandwiches describes a dynamic modality about the subject’s ability of eating. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 157 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 157–166, Coling 2014, Dublin, Ireland, August 24 2014. However, no eating event has actually happened. Modality has been considered in modeling speaker’s opinions (Benamara et al., 2012), machine translation (Baker et al., 2012), etc. Sauri et al. (2006; 2012) proposed a framework for modeling modalities. However, their definition of modality is a little different from that used by linguists. The main motivation of their work is to predict the factuality of a proposition. As a result, all factors that may affect the factuality of propositions are regarded as modalities. In our framework, we will adopt the definition in linguistic studies that modality expresses a speaker’s belief or attitude on an embedded proposition (Palmer, 2001). Factuality is determined by many factors other than modalities. However, we don’t wa"
W14-5819,W12-3802,0,0.0248415,"ed propositions. For example, he can eat two sandwiches describes a dynamic modality about the subject’s ability of eating. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 157 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 157–166, Coling 2014, Dublin, Ireland, August 24 2014. However, no eating event has actually happened. Modality has been considered in modeling speaker’s opinions (Benamara et al., 2012), machine translation (Baker et al., 2012), etc. Sauri et al. (2006; 2012) proposed a framework for modeling modalities. However, their definition of modality is a little different from that used by linguists. The main motivation of their work is to predict the factuality of a proposition. As a result, all factors that may affect the factuality of propositions are regarded as modalities. In our framework, we will adopt the definition in linguistic studies that modality expresses a speaker’s belief or attitude on an embedded proposition (Palmer, 2001). Factuality is determined by many factors o"
W14-5819,W09-2307,0,0.0225532,"d detective novels any more”. Event Modality SpeechAct MacroAvg Accuracy Prec 0.709 0.395 0.430 0.511 f1 Rec 0.939 0.124 0.130 0.398 0.679 F1 0.807 0.189 0.199 0.399 Prec 0.853 0.731 0.829 0.804 +f2 Rec 0.969 0.473 0.664 0.702 0.836 F1 0.908 0.574 0.737 0.740 Prec 0.833 0.744 0.845 0.807 +f3 Rec 0.974 0.431 0.609 0.671 0.824 F1 0.898 0.545 0.707 0.717 Table 8: Coarse level classification result. 4.2 Experimental Result To give a real performance, the annotated syntactic and semantic information are not used. Instead, we use the Stanford word segmenter (Tseng et al., 2005) and Stanford parser (Chang et al., 2009) to get the syntactic structure of the sentences. All the experiment are results of 5-fold cross validation with a SVM classifier implemented in LibSVM (Chang and Lin, 2011). The result of the coarse level classification for modality, event and speech act is shown in Table 8. We can see that the overall performance is reasonable. The F-Measure for modality is not as good as the others. This is due to the fact that the modal markers and operators are quite critical for identifying modalities, which may be sparse in our corpus. We suggest that maintaining a comprehensive dictionary of modal oper"
W14-5819,C90-2010,1,0.264737,"chy is shown in Figure 1. Some traditional notions are kept in use e.g. accomplishment and achievement. However, they now refer to event types rather than verb classes. 3 Annotating a Chinese Corpus 3.1 Data Selection For annotation, we choose Sinica Treebank 3.0 (Huang et al., 2000), which contains more than 60,000 trees. Sinica Treebank is a subset of Sinica Corpus (Chen et al., 1996), which is a balanced corpus that contains different genres of materials, including news, novels and some transcripts of spoken Chinese. Sinica Treebank is annotated based on the Information-based Case Grammar (Chen and Huang, 1990). The annotated syntactic and semantic information is kept for further studies, e.g. feature evaluation and selection. For annotation, we only select the sentences that are labeled as S and end with punctuation of period ‘。’, exclamation ‘！’, semicolon ‘；’ and question mark ‘？’. After removing duplicate sentences, we get 5612 sentences Table 3 shows the detailed information of the raw corpus. There are 45728 tokens from 11681 types in the corpus. For the heads of the sentences, there are 2127 different verbs. 161 Figure 1: Sentence type hierarchy. Sentences 5612 Different Verbs 2127 Different"
W14-5819,Y96-1018,1,0.125061,"for one week” “he ran for an hour” “he broke the cup” “He putted a spin on the dice” “he wrote a letter” “he started up the computer” Table 2: Complex event types that are composed by more than one primitives. The overall hierarchy is shown in Figure 1. Some traditional notions are kept in use e.g. accomplishment and achievement. However, they now refer to event types rather than verb classes. 3 Annotating a Chinese Corpus 3.1 Data Selection For annotation, we choose Sinica Treebank 3.0 (Huang et al., 2000), which contains more than 60,000 trees. Sinica Treebank is a subset of Sinica Corpus (Chen et al., 1996), which is a balanced corpus that contains different genres of materials, including news, novels and some transcripts of spoken Chinese. Sinica Treebank is annotated based on the Information-based Case Grammar (Chen and Huang, 1990). The annotated syntactic and semantic information is kept for further studies, e.g. feature evaluation and selection. For annotation, we only select the sentences that are labeled as S and end with punctuation of period ‘。’, exclamation ‘！’, semicolon ‘；’ and question mark ‘？’. After removing duplicate sentences, we get 5612 sentences Table 3 shows the detailed inf"
W14-5819,W00-1205,1,0.483956,"ge4 shai3zi0 ta1 xie3 le0 yi1 feng1 xin4 ta1 ba3 dian4nao3 qi3dong4 le0 “he knocked the door” “he was ill for one week” “he ran for an hour” “he broke the cup” “He putted a spin on the dice” “he wrote a letter” “he started up the computer” Table 2: Complex event types that are composed by more than one primitives. The overall hierarchy is shown in Figure 1. Some traditional notions are kept in use e.g. accomplishment and achievement. However, they now refer to event types rather than verb classes. 3 Annotating a Chinese Corpus 3.1 Data Selection For annotation, we choose Sinica Treebank 3.0 (Huang et al., 2000), which contains more than 60,000 trees. Sinica Treebank is a subset of Sinica Corpus (Chen et al., 1996), which is a balanced corpus that contains different genres of materials, including news, novels and some transcripts of spoken Chinese. Sinica Treebank is annotated based on the Information-based Case Grammar (Chen and Huang, 1990). The annotated syntactic and semantic information is kept for further studies, e.g. feature evaluation and selection. For annotation, we only select the sentences that are labeled as S and end with punctuation of period ‘。’, exclamation ‘！’, semicolon ‘；’ and qu"
W14-5819,P07-1113,0,0.0225619,"onsidering that each of the finer-grained event types has different semantic entailments. To differentiate them is useful for deep semantic processing and will thus benefit NLP applications such as question answering and machine translation, etc. We also provide experiments to show that the different types of sentences are differentiable with a promising performance. 1 Introduction Event classification is a fundamental task for NLP applications, such as question answering and machine translation, which need deep understanding of the text. Previous work (Siegel, 1999; Siegel and McKeown, 2000; Palmer et al., 2007; Zarcone and Lenci, 2008; Cao et al., 2006; Zhu et al., 2000) aims to classify events into four categories, namely state, activity, accomplishment and achievement, i.e. Vendler’s framework adopted from linguistic studies (Vendler, 1967; Smith, 1991). High performance was reported on the classification, however based on the assumption that all sentences describe an event, which is not case in real text. Modalities and speech acts are not considered and no finer-grained classification is proposed. The aim for aspectual classification for a specific language is to build verb classes. In such fra"
W14-5819,J12-2002,0,0.074357,"Missing"
W14-5819,J00-4004,0,0.301514,"iner-grained categories, considering that each of the finer-grained event types has different semantic entailments. To differentiate them is useful for deep semantic processing and will thus benefit NLP applications such as question answering and machine translation, etc. We also provide experiments to show that the different types of sentences are differentiable with a promising performance. 1 Introduction Event classification is a fundamental task for NLP applications, such as question answering and machine translation, which need deep understanding of the text. Previous work (Siegel, 1999; Siegel and McKeown, 2000; Palmer et al., 2007; Zarcone and Lenci, 2008; Cao et al., 2006; Zhu et al., 2000) aims to classify events into four categories, namely state, activity, accomplishment and achievement, i.e. Vendler’s framework adopted from linguistic studies (Vendler, 1967; Smith, 1991). High performance was reported on the classification, however based on the assumption that all sentences describe an event, which is not case in real text. Modalities and speech acts are not considered and no finer-grained classification is proposed. The aim for aspectual classification for a specific language is to build verb"
W14-5819,P99-1015,0,0.300916,"ther provide finer-grained categories, considering that each of the finer-grained event types has different semantic entailments. To differentiate them is useful for deep semantic processing and will thus benefit NLP applications such as question answering and machine translation, etc. We also provide experiments to show that the different types of sentences are differentiable with a promising performance. 1 Introduction Event classification is a fundamental task for NLP applications, such as question answering and machine translation, which need deep understanding of the text. Previous work (Siegel, 1999; Siegel and McKeown, 2000; Palmer et al., 2007; Zarcone and Lenci, 2008; Cao et al., 2006; Zhu et al., 2000) aims to classify events into four categories, namely state, activity, accomplishment and achievement, i.e. Vendler’s framework adopted from linguistic studies (Vendler, 1967; Smith, 1991). High performance was reported on the classification, however based on the assumption that all sentences describe an event, which is not case in real text. Modalities and speech acts are not considered and no finer-grained classification is proposed. The aim for aspectual classification for a specific"
W14-5819,I05-3027,0,0.0281304,"ao3shuo1 (novel) le0 (LE) “he doesn’t read detective novels any more”. Event Modality SpeechAct MacroAvg Accuracy Prec 0.709 0.395 0.430 0.511 f1 Rec 0.939 0.124 0.130 0.398 0.679 F1 0.807 0.189 0.199 0.399 Prec 0.853 0.731 0.829 0.804 +f2 Rec 0.969 0.473 0.664 0.702 0.836 F1 0.908 0.574 0.737 0.740 Prec 0.833 0.744 0.845 0.807 +f3 Rec 0.974 0.431 0.609 0.671 0.824 F1 0.898 0.545 0.707 0.717 Table 8: Coarse level classification result. 4.2 Experimental Result To give a real performance, the annotated syntactic and semantic information are not used. Instead, we use the Stanford word segmenter (Tseng et al., 2005) and Stanford parser (Chang et al., 2009) to get the syntactic structure of the sentences. All the experiment are results of 5-fold cross validation with a SVM classifier implemented in LibSVM (Chang and Lin, 2011). The result of the coarse level classification for modality, event and speech act is shown in Table 8. We can see that the overall performance is reasonable. The F-Measure for modality is not as good as the others. This is due to the fact that the modal markers and operators are quite critical for identifying modalities, which may be sparse in our corpus. We suggest that maintaining"
W14-5819,W13-5408,1,0.851361,"position. As a result, all factors that may affect the factuality of propositions are regarded as modalities. In our framework, we will adopt the definition in linguistic studies that modality expresses a speaker’s belief or attitude on an embedded proposition (Palmer, 2001). Factuality is determined by many factors other than modalities. However, we don’t want to mix all the factors together in linguistic perspective. In this paper, we will describe a Chinese corpus in which different sentence types are discriminated. Finer-grained event types are also incorporated with a theory proposed in (Xu and Huang, 2013). The details of the framework will be discussed in the next section. The remaining of the paper is organized as follows. Section 2 introduces the theoretical framework we shall adopt for our annotation. Section 3 describes a Chinese corpus we annotated with some statistical information. Section 4 describes a classification experiment based on the annotated corpus. Section 5 is the conclusion and our future work. 2 The Annotation Framework In this section, we will give an introduction to the theoretical framework from a linguistic perspective. There are two main levels for the classification."
W14-5819,zarcone-lenci-2008-computational,0,0.0195597,"of the finer-grained event types has different semantic entailments. To differentiate them is useful for deep semantic processing and will thus benefit NLP applications such as question answering and machine translation, etc. We also provide experiments to show that the different types of sentences are differentiable with a promising performance. 1 Introduction Event classification is a fundamental task for NLP applications, such as question answering and machine translation, which need deep understanding of the text. Previous work (Siegel, 1999; Siegel and McKeown, 2000; Palmer et al., 2007; Zarcone and Lenci, 2008; Cao et al., 2006; Zhu et al., 2000) aims to classify events into four categories, namely state, activity, accomplishment and achievement, i.e. Vendler’s framework adopted from linguistic studies (Vendler, 1967; Smith, 1991). High performance was reported on the classification, however based on the assumption that all sentences describe an event, which is not case in real text. Modalities and speech acts are not considered and no finer-grained classification is proposed. The aim for aspectual classification for a specific language is to build verb classes. In such framework, viewpoint aspect"
W14-5819,W00-1220,0,0.092792,"ent semantic entailments. To differentiate them is useful for deep semantic processing and will thus benefit NLP applications such as question answering and machine translation, etc. We also provide experiments to show that the different types of sentences are differentiable with a promising performance. 1 Introduction Event classification is a fundamental task for NLP applications, such as question answering and machine translation, which need deep understanding of the text. Previous work (Siegel, 1999; Siegel and McKeown, 2000; Palmer et al., 2007; Zarcone and Lenci, 2008; Cao et al., 2006; Zhu et al., 2000) aims to classify events into four categories, namely state, activity, accomplishment and achievement, i.e. Vendler’s framework adopted from linguistic studies (Vendler, 1967; Smith, 1991). High performance was reported on the classification, however based on the assumption that all sentences describe an event, which is not case in real text. Modalities and speech acts are not considered and no finer-grained classification is proposed. The aim for aspectual classification for a specific language is to build verb classes. In such framework, viewpoint aspect in terms of perfective vs. imperfecti"
W14-5819,W13-0301,0,\N,Missing
W15-3102,W10-0719,0,0.0526839,"Missing"
W15-3102,W10-0701,0,0.0340123,"eijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing mantic transparency (OST) of a multi-morphemic form; besides that, there is constituent semantic transparency (CST) too which means the extent to which the lexical meaning of each constituent as a independent lexical form retains itself in the lexical meaning of the whole form. tion, data cleansing, majority voting, peer review, spammer monitor, etc (Crump et al., 2013; Allahbakhsh et al., 2013; Mason and Suri, 2012; Behrend et al., 2011; Buhrmester et al., 2011; Callison-Burch and Dredze, 2010; Paolacci et al., 2010; Ipeirotis et al., 2010; Munro et al., 2010; Snow et al., 2008). In the context of theoretical linguistics, semantic transparency is used as an empirical criterion of wordhood (Duanmu, 1998; 吕叔湘, 1979; Chao, 1968), but for Chinese disyllabic forms this criterion seems to be ignored to some extent by some linguists based on word intuition (王洪君, 2006; 冯胜利, 2004; 王立, 2003; 冯胜利, 2001; 胡明 扬, 1999; 冯胜利, 1996; 吕叔湘, 1979); it is also treated as an indicator of lexicalization (Packard, 2000; 董秀芳, 2002; 李晋霞 and 李宇明, 2008). In the context of psycholinguistics, it is an “extremely"
W15-3102,Y96-1018,1,0.526752,"eated as words by some and non-words by others. This kind of segmentation consistency can be a convenient measurement of Chinese speakers’ word intuition. Word intuition per se is an important issue awaiting more research which can contribute to the investigation of cognitive mechanism of humans’ language competence and shed new light on the theoretical problem of word definition for the theoretical definition of word should generally accord with the speakers’ word intuition (王洪君, 2006; 王立, 2003; 胡明扬, 1999; 陆志韦, 1964). The widely used Chinese segmented corpora, for example, the Sinica corpus (Chen et al., 1996), are usually segmented firstly by segmentation programs and then revised by experts according to certain word segmentation standard. From the inconsistent segmentation cases we can find plenty useful information to explore word intuition. But from the perspective of the measurement of Chinese speakers’ word intuition, the data are biased by segmentation programs and word segmentation standards, so they are not so suitable and reliable for this purpose. Semantic transparency/compositionality of a multi-morphemic form, simply speaking, is the extent to which the lexical meaning of the whole for"
W15-3102,D08-1027,0,0.0462846,"Missing"
W15-3102,W14-5818,1,0.21527,"transparency plays in Chinese speakers’ word intuition towards Chinese disyllabic forms. When we build the dataset, we carefully select sentence stimuli which contain word stimuli that cover all possible kinds of semantic transparency types to enable us to examine the role semantic transparency plays in word intuition of Chinese speakers. We have already successfully applied crowdsourcing method to the semantic transparency of compound rating task and built a semantic transparency dataset which contains the semantic transparency rating data of about 1,200 disyllabic Chinese nominal compounds (Wang et al., 2014a); we want to further extend the application of crowdsourcing method to Chinese word segmentation task to further evaluate the crowdsourcing method and to build new language resource. The second purpose is to support the studies on word intuition of Chinese speakers in general and to examine the effect of semantic transparency on word intuition in particular. Word intuition is speakers’ intuitive knowledge on wordhood, i.e., what a word is. Laymen’s word segmentation behavior is not instructed by linguistic theories on word, but by their word intuition, hence reflects their word intuition; be"
W15-3102,W14-4715,1,0.804553,"transparency plays in Chinese speakers’ word intuition towards Chinese disyllabic forms. When we build the dataset, we carefully select sentence stimuli which contain word stimuli that cover all possible kinds of semantic transparency types to enable us to examine the role semantic transparency plays in word intuition of Chinese speakers. We have already successfully applied crowdsourcing method to the semantic transparency of compound rating task and built a semantic transparency dataset which contains the semantic transparency rating data of about 1,200 disyllabic Chinese nominal compounds (Wang et al., 2014a); we want to further extend the application of crowdsourcing method to Chinese word segmentation task to further evaluate the crowdsourcing method and to build new language resource. The second purpose is to support the studies on word intuition of Chinese speakers in general and to examine the effect of semantic transparency on word intuition in particular. Word intuition is speakers’ intuitive knowledge on wordhood, i.e., what a word is. Laymen’s word segmentation behavior is not instructed by linguistic theories on word, but by their word intuition, hence reflects their word intuition; be"
W15-4208,N09-1003,0,0.0525288,"Missing"
W15-4208,W11-2501,1,0.772704,"onym questions of the TOEFL as a benchmark in the synonyms identification task. Although good results in such set (Rapp, 2003) may have a strong impact on the audience, its small size and the fact that it contains only synonyms cannot make it an accurate benchmark to evaluate DSMs. For what concerns antonymy, based on similar principles to the TOEFL, Mohammed et al. (2008) proposes a dataset containing 950 closest-opposite questions, where five alternatives are provided for every target word. Their data are collected starting from 162 questions in the Graduate Record Examination (GRE). BLESS (Baroni and Lenci, 2011) contains several relations, such as hypernymy, co-hyponymy, meronymy, event, attribute, etc. This dataset covers 200 concrete and unambiguous concepts divided in 17 categories (e.g. vehicle, ground mammal, etc.). Every concept is linked through the various semantic relations to several relata (which can be either nouns, adjectives or verbs). Unfortunately this dataset does not contain synonymy and antonymy related pairs. With respect to entailment, Baroni et al.(2012) Related Work Up to now, DSMs performance has typically been evaluated against benchmarks developed for purposes other than DSM"
W15-4208,E12-1004,0,0.733867,"Missing"
W15-4208,P99-1008,0,0.0361593,"similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates). DSMs do not provide by themselves a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a"
W15-4208,W09-0215,0,0.154042,"Missing"
W15-4208,D08-1103,0,0.0551206,"Missing"
W15-4208,P06-1015,0,0.154748,", in fact, can be similar in many ways. Dog and animal are similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates). DSMs do not provide by themselves a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synony"
W15-4208,2003.mtsummit-papers.42,0,0.0194325,"inconsistencies in the way semantic relations have been encoded. Simply looking at the hypernymy relation (Cruse, 1986), for example, we can see that it is used in both a taxonomical (i.e. dog is a hyponym of animal) and a vague and debatable way (i.e. silly is a hyponym of child). ConceptNet (Liu and Singh, 2004) may be considered even less homogeneous, given its size and the automatic way in which it was developed. Landauer and Dumais (1997) introduces the 80 multiple-choice synonym questions of the TOEFL as a benchmark in the synonyms identification task. Although good results in such set (Rapp, 2003) may have a strong impact on the audience, its small size and the fact that it contains only synonyms cannot make it an accurate benchmark to evaluate DSMs. For what concerns antonymy, based on similar principles to the TOEFL, Mohammed et al. (2008) proposes a dataset containing 950 closest-opposite questions, where five alternatives are provided for every target word. Their data are collected starting from 162 questions in the Graduate Record Examination (GRE). BLESS (Baroni and Lenci, 2011) contains several relations, such as hypernymy, co-hyponymy, meronymy, event, attribute, etc. This data"
W15-4208,gheorghita-pierrel-2012-towards,0,0.149614,"Missing"
W15-4208,E14-4008,1,0.740615,"a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth"
W15-4208,C92-2082,0,0.607621,"lations. Words, in fact, can be similar in many ways. Dog and animal are similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates). DSMs do not provide by themselves a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pa"
W15-4208,Y14-1018,1,0.947947,"a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth"
W15-4208,W09-2415,0,0.0355443,"Missing"
W15-4208,W14-5814,0,0.117044,"Missing"
W15-4208,Q14-1041,0,0.0527536,"used for either filtering the pairs or performing an in-depth analysis of the results. The tuples were extracted from a combination of ConceptNet 5.0 and WordNet 4.0, and subsequently filtered through automatic methods and crowdsourcing in order to ensure their quality. The dataset is freely downloadable1 . An extension in RDF format, including also scripts for data processing, is under development. 1 Introduction Distributional Semantic Models (DSMs) represent lexical meaning in vector spaces by encoding corpora derived word co-occurrences in vectors (Sahlgren, 2006; Turney and Pantel, 2010; Lapesa and Evert, 2014). These models are based on the assumption that meaning can be inferred from the contexts in which terms occur. Such assumption is 1 The resource is available http://colinglab.humnet.unipi.it/resources/ and https://github.com/esantus at at 64 Proceedings of the 4th Workshop on Linked Data in Linguistics (LDL-2015), pages 64–69, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2009; Weeds et al., 2004; Weeds and Weir, 2003). Both the abovementioned approaches need to rely on datasets containing semantic relations"
W15-4208,W03-1011,0,0.421294,"corpora derived word co-occurrences in vectors (Sahlgren, 2006; Turney and Pantel, 2010; Lapesa and Evert, 2014). These models are based on the assumption that meaning can be inferred from the contexts in which terms occur. Such assumption is 1 The resource is available http://colinglab.humnet.unipi.it/resources/ and https://github.com/esantus at at 64 Proceedings of the 4th Workshop on Linked Data in Linguistics (LDL-2015), pages 64–69, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2009; Weeds et al., 2004; Weeds and Weir, 2003). Both the abovementioned approaches need to rely on datasets containing semantic relations for training and/or evaluation. EVALution is a dataset designed to support DSMs on both processes. This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis of the results. The qu"
W15-4208,C04-1146,0,0.471585,"spaces by encoding corpora derived word co-occurrences in vectors (Sahlgren, 2006; Turney and Pantel, 2010; Lapesa and Evert, 2014). These models are based on the assumption that meaning can be inferred from the contexts in which terms occur. Such assumption is 1 The resource is available http://colinglab.humnet.unipi.it/resources/ and https://github.com/esantus at at 64 Proceedings of the 4th Workshop on Linked Data in Linguistics (LDL-2015), pages 64–69, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing 2009; Weeds et al., 2004; Weeds and Weir, 2003). Both the abovementioned approaches need to rely on datasets containing semantic relations for training and/or evaluation. EVALution is a dataset designed to support DSMs on both processes. This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis"
W15-4208,S12-1012,1,0.886767,"ked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them use word pairs holding a specific relation as seeds, in order to discover patterns in which other pairs holding the same relation are likely to occur (Hearst, 1992; Pantel and Pennacchiotti, 2006; Cimiano and V¨olker, 2005; Berland and Charniak, 1999). Other approaches rely on linguistically grounded unsupervised measures, which adopt different types of distance measures by selectively weighting the vectors features (Santus et al., 2014a; Santus et al., 2014b; Lenci and Benotto, 2012; Kotlerman et al., 2010; Clarke, In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis of the results. The tuples were extract"
W15-4208,W06-1104,0,0.011487,"di Pisa Pisa, Italy Chu-Ren Huang The Hong Kong Polytechnic University Hong Kong alessandro.lenci@ling.unipi.it churen.huang@polyu.edu.hk Abstract typically referred to as the distributional hypothesis (Harris, 1954). DSMs are broadly used in Natural Language Processing (NLP) because they allow systems to automatically acquire lexical semantic knowledge in a fully unsupervised way and they have been proved to outperform other semantic models in a large number of tasks, such as the measurement of lexical semantic similarity and relatedness. Their geometric representation of semantic distance (Zesch and Gurevych, 2006) allows its calculation through mathematical measures, such as the vector cosine. A related but more complex task is the identification of semantic relations. Words, in fact, can be similar in many ways. Dog and animal are similar because the former is a specific kind of the latter (hyponym), while dog and cat are similar because they are both specific kinds of animal (coordinates). DSMs do not provide by themselves a principled way to single out the items linked by a specific relation. Several distributional approaches have tried to overcome such limitation in the last decades. Some of them u"
W16-5411,D09-1062,0,0.0139486,"ve two experiments for two types of tasks respectively. They are Extracting Chinese Sentiment Expressions and Annotating Sentences for Binary Classification. Because Chinese is our native language, both experiments use Chinese corpora to reduce annotation uncertainty. 3.1 Extracting Chinese Sentiment Expressions When building a sentiment-related system, we have some general Chinese sentiment lexicons to choose, such as 1)NTU Sentiment Dictionary6 ; 2)Chinese affective lexicon ontology7 . Furthermore, there are some work on automatically constructing task-oriented sentiment resources, such as (Choi and Cardie, 2009; Lu et al., 2011; Jijkoun et al., 2010; Cruz et al., 2011), which still needs human annotation to improve quality, and limits the coverage of the constructed resources due to the restriction of automation. However, when constructing sentiment resources in a specified domain, the best choice should be to construct directly from the corpora, which can guarantees coverage and exploits human sentiment knowledge. We report here how we extract Chinese sentiment expressions from a corpus. Normally, it is supposed that sentiment expressions are mainly adjectives, some of verbs (like, hate etc.) and n"
W16-5411,W11-1716,0,0.0716871,"Missing"
W16-5411,P10-1060,0,0.0516553,"Missing"
W18-6220,D17-1054,0,0.279419,"rd-level preference is considered rather than information at the semantic level (Chen et al., 2016). Gui et al. (2016) introduce an inter-subjectivity network to link users to the terms they used as well as the polarities of the terms. The network aims to learn writer embeddings which are subsequently incorporated into a CNN network for sentiment analysis. Chen et al. (2016) propose a model to incorporate user and product information into an LSTM with attention mechanism. This model is reported to produce the state-of-the-art results in the three benchmark datasets (IMDB, Yelp13, and Yelp14). Dou (2017) also proposes a deep memory network to integrate user profile and product information in a unified model. However, the model only achieves a comparable result to the state-of-the-art attention based LSTM (Chen et al., 2016). 3 3.3 Inspired by the successful use of memory networks in language modeling, question answering, and sentiment analysis (Sukhbaatar et al., 2015; Tang et al., 2016; Dou, 2017), we propose our DUPMN by extending a single memory network model to two memory networks to reflect different influences from users’ perspective and products’ perspective. The structure of the model"
W18-6220,D14-1080,0,0.0308845,"uct information into a CNN network for document level sentiment classification. User ids and product names are included as features in a unified document vector using the vector space model such that document vectors capture important global clues include individual preferences and product information. Neural Network Models In recent years, deep learning has greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (CNNs) (Socher et al., 2011), Recursive Neural Network (ReNNs) (Socher et al., 2013), and Recurrent Neural Networks (RNNs) (Irsoy and Cardie, 2014). RNN naturally benefits sentiment classification because of its ability to 141 bedding representation of documents. The first LSTM layer is used to obtain sentence representation by the hidden state of an LSTM network. The same mechanism is also used for document level representation with sentence-level representation as input. User and product attentions are included in the network so that all salient features are included in document representation. For document ~ d~ is a vector repd, its embedding is denoted as d. resentation with dimension size n. In principle, the embedding representatio"
W18-6220,N16-2016,0,0.0569405,"Missing"
W18-6220,D14-1181,0,0.0217797,"Missing"
W18-6220,I17-2043,1,0.936773,"e gradient vanishing problem. An LSTM model provides a gated mechanism to keep the long-term memory. Each LSTM layer is generally followed by mean pooling and the output is fed into the next layer. Experiments in datasets which contain sentences and long documents demonstrate that LSTM model outperforms the traditional RNNs (Tang et al., 2015a,c). Attention mechanism is also added to LSTM models to highlight important segments at both sentence level and document level. Attention models can be built from text in local context (Yang et al., 2016), user/production information (Chen et al., 2016; Long et al., 2017a) and other information such as cognition grounded eye tracking data (Long et al., 2017b). LSTM models with attention mechanism are currently the state-of-theart models in document sentiment analysis tasks (Chen et al., 2016; Long et al., 2017b). Memory networks are designed to handle larger context for a collection of documents. Memory networks introduce inference components combined with a so called long-term memory component (Weston et al., 2014). The long-term memory component is a large external memory to represent data as a collection. This collective information can contain local conte"
W18-6220,D17-1048,1,0.797194,"e gradient vanishing problem. An LSTM model provides a gated mechanism to keep the long-term memory. Each LSTM layer is generally followed by mean pooling and the output is fed into the next layer. Experiments in datasets which contain sentences and long documents demonstrate that LSTM model outperforms the traditional RNNs (Tang et al., 2015a,c). Attention mechanism is also added to LSTM models to highlight important segments at both sentence level and document level. Attention models can be built from text in local context (Yang et al., 2016), user/production information (Chen et al., 2016; Long et al., 2017a) and other information such as cognition grounded eye tracking data (Long et al., 2017b). LSTM models with attention mechanism are currently the state-of-theart models in document sentiment analysis tasks (Chen et al., 2016; Long et al., 2017b). Memory networks are designed to handle larger context for a collection of documents. Memory networks introduce inference components combined with a so called long-term memory component (Weston et al., 2014). The long-term memory component is a large external memory to represent data as a collection. This collective information can contain local conte"
W18-6220,P14-5010,0,0.00443626,"configuration by the development dataset is used for the test set to obtain the final result. Experiment and Result Analysis Performance evaluations are conducted on three datasets and DUPMN is compared with a set of commonly used baseline methods including the state-of-the-art LSTM based method (Chen et al., 2016; Wu et al., 2018). 4.1 IMDB 10 84,919 1,310 1,635 24.56 64.82 51.93 1,223 318 72 22 Datasets The three benchmarking datasets include movie reviews from IMDB, restaurant reviews from Yelp13 and Yelp14 developed by Tang (2015a). All datasets are tokenized using the Stanford NLP tool (Manning et al., 2014). Table 1 lists statistics of the datasets including the number of classes, number of documents, average length of sentences, the average number of documents per user, and the average number of documents per product. 4.2 Baseline Methods In order to make a systematic comparison, three groups of baselines are used in the evaluation. Group 1 includes all commonly used feature sets mentioned in Chen et al. (2016) including Majority, Trigram, Text features (TextFeatures), and AveWordvec. All feature sets in Group 1 except 143 G1 G2 G3 New Model Majority Trigram TextFeature AvgWordvec SSWE RNTN+RNN"
W18-6220,K16-1016,0,0.063936,"Missing"
W18-6220,D16-1172,0,0.0212771,"hus K=1 is used. Majority use the SVM classifier. Group 2 methods include the recently published sentiment analysis models which only use context information, including: • SSWE (Tang et al., 2014) — An SVM model using sentiment specific word embedding. • InterSub (Gui et al., 2016) — A CNN model making use of user and product information. • RNTN+RNN (Socher et al., 2013) — A Recursive Neural Tensor Network (RNTN) to represent sentences. • LSTM+UPA (Chen et al., 2016) — The state-of-the-art LSTM including both local context based attentions and user/product in the attention mechanism. • CLSTM (Xu et al., 2016) — A Cached LSTM model to capture overall semantic information in long text. For the DUPMN model, we also include two variations which use only one memory network. The first variation only includes user profiles in the memory network, denoted as DUPMN-U. The second variation only uses product information, denoted as DUPMN-P. • LSTM+LA (Chen et al., 2016) — A state-ofthe-art LSTM using local context as attention mechanism at both sentence level and document level. 4.3 • LSTM+CBA (Long et al., 2017b)— A state-of-the-art LSTM model using cognition based data to build attention mechanism. Performa"
W18-6220,D11-1014,0,0.138511,"on have crucial effects on sentiment polarities. Tang et al. (2015b) proposed a model by incorporating user and product information into a CNN network for document level sentiment classification. User ids and product names are included as features in a unified document vector using the vector space model such that document vectors capture important global clues include individual preferences and product information. Neural Network Models In recent years, deep learning has greatly improved the performance of sentiment analysis. Commonly used models include Convolutional Neural Networks (CNNs) (Socher et al., 2011), Recursive Neural Network (ReNNs) (Socher et al., 2013), and Recurrent Neural Networks (RNNs) (Irsoy and Cardie, 2014). RNN naturally benefits sentiment classification because of its ability to 141 bedding representation of documents. The first LSTM layer is used to obtain sentence representation by the hidden state of an LSTM network. The same mechanism is also used for document level representation with sentence-level representation as input. User and product attentions are included in the network so that all salient features are included in document representation. For document ~ d~ is a v"
W18-6220,N16-1174,0,0.18836,"Missing"
W18-6220,D13-1170,0,0.163024,"Written text is often meant to express sentiments of individuals. Recognizing the underlying sentiment expressed in the text is essential to understand the full meaning of the text. The SA community is increasingly interested in using natural language processing (NLP) techniques as well as sentiment theories to identify sentiment expressions in the text. Recently, deep learning based methods have taken over feature engineering approaches to gain further performance improvement in SA. Typical neural network models include Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a), and many more. Attention-based models are introduced to highlight important words and sentences in a piece 140 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 140–148 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 capture sequential information in text. However, standard RNNs suffer from the so-called gradient vanishing problem (Bengio et al., 1994) where gradients may grow or decay exponentially ove"
W18-6220,D15-1167,0,0.200971,"individuals. Recognizing the underlying sentiment expressed in the text is essential to understand the full meaning of the text. The SA community is increasingly interested in using natural language processing (NLP) techniques as well as sentiment theories to identify sentiment expressions in the text. Recently, deep learning based methods have taken over feature engineering approaches to gain further performance improvement in SA. Typical neural network models include Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a), and many more. Attention-based models are introduced to highlight important words and sentences in a piece 140 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 140–148 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 capture sequential information in text. However, standard RNNs suffer from the so-called gradient vanishing problem (Bengio et al., 1994) where gradients may grow or decay exponentially over long sequences. LSTM models are adopted to solve"
W18-6220,P15-1098,0,0.676968,"individuals. Recognizing the underlying sentiment expressed in the text is essential to understand the full meaning of the text. The SA community is increasingly interested in using natural language processing (NLP) techniques as well as sentiment theories to identify sentiment expressions in the text. Recently, deep learning based methods have taken over feature engineering approaches to gain further performance improvement in SA. Typical neural network models include Convolutional Neural Network (CNN) (Kim, 2014), Recursive auto-encoders (Socher et al., 2013), Long-Short Term Memory (LSTM) (Tang et al., 2015a), and many more. Attention-based models are introduced to highlight important words and sentences in a piece 140 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 140–148 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 capture sequential information in text. However, standard RNNs suffer from the so-called gradient vanishing problem (Bengio et al., 1994) where gradients may grow or decay exponentially over long sequences. LSTM models are adopted to solve"
W18-6220,D16-1021,0,0.0651268,"ly the state-of-theart models in document sentiment analysis tasks (Chen et al., 2016; Long et al., 2017b). Memory networks are designed to handle larger context for a collection of documents. Memory networks introduce inference components combined with a so called long-term memory component (Weston et al., 2014). The long-term memory component is a large external memory to represent data as a collection. This collective information can contain local context (Das et al., 2017) or external knowledge base (Jain, 2016). It can also be used to represent the context of users and products globally (Tang et al., 2016). Dou uses (2017) a memory network model in document level sentiment analysis and makes comparable result to the state-of-the-art model (Chen et al., 2016). product should give dual consideration to individual users as well as all reviews as a collection. In this paper, we address the aforementioned issue by proposing to learn user profiles and product review information separately before making a joint prediction on sentiment classification. In the proposed Dual User and Product Memory Network (DUPMN) model, we first build a hierarchical LSTM (Hochreiter and Schmidhuber, 1997) model to genera"
W18-6220,P14-1146,0,0.0395321,"686 0.686 0.678 0.784 0.662 0.690 0.654 0.639 MAE 0.744 0.513 0.520 0.568 N/A N/A N/A N/A N/A 0.464 0.369 N/A N/A 0.351 Table 2: Evaluation of different methods; best result/group in accuracy is marked in bold; second best is underlined. • UPDMN (Dou, 2017) — A deep memory network for document level sentiment classification by including user and product information in a unified model. Hop 1 gives the best result, and thus K=1 is used. Majority use the SVM classifier. Group 2 methods include the recently published sentiment analysis models which only use context information, including: • SSWE (Tang et al., 2014) — An SVM model using sentiment specific word embedding. • InterSub (Gui et al., 2016) — A CNN model making use of user and product information. • RNTN+RNN (Socher et al., 2013) — A Recursive Neural Tensor Network (RNTN) to represent sentences. • LSTM+UPA (Chen et al., 2016) — The state-of-the-art LSTM including both local context based attentions and user/product in the attention mechanism. • CLSTM (Xu et al., 2016) — A Cached LSTM model to capture overall semantic information in long text. For the DUPMN model, we also include two variations which use only one memory network. The first variat"
W19-1401,W19-1402,0,0.268136,"Missing"
W19-1401,P19-1068,1,0.913629,"guages and asked to predict the valid morphological analyses for a seventh, unseen language. In the “Semi-Closed” track, the process was the same, only participants were provided with additional raw data by the organisers. This was in the form of raw text Wikipedia dumps, bilingual dictionaries from the Apertium project and any treebanks available in the known languages from the Universal Dependencies project. Moldavian vs. Romanian Cross-dialect Topic identification (MRC): In the Moldavian vs. Romanian Cross-topic Identification shared task, we provided participants with the MOROCO data set (Butnaru and Ionescu, 2019) which contains Moldavian and Romanian samples of text collected from the news domain. The samples belong to one of the following six topics: culture, finance, politics, science, sports, and tech. The samples are pre-processed in order to eliminate named entities. For each sample, the data set provides corresponding dialectal and category labels. To this end, we proposed three subtasks for the 2019 VarDial Evaluation Campaign. The first sub-task was a binary classification by dialect task, in which a classification model is required to discriminate between the Moldavian and the Romanian dialec"
W19-1401,W18-3929,1,0.894916,"Missing"
W19-1401,W19-1413,1,0.836296,"Missing"
W19-1401,W19-1416,0,0.056458,"Missing"
W19-1401,W18-3907,1,0.895659,"Missing"
W19-1401,Y96-1018,1,0.197287,"k. 5.5 Summary Three teams participated in this first iteration of the cross-lingual analysis task. Two of the teams employed variations of neural encoderdecoder systems. Apart from lemmatization performance, it proved to be difficult to attain consistent improvements over the neural baseline systems. However, the suffix stripping approach used by the HSE team did deliver clear improvements in lemmatization for both Turkic and Romance languages. 6 6.1 Dataset Texts to distinguish between the two variations were compiled from the two existing corpora of news: Sinica Corpus for Taiwan Mandarin (Chen et al., 1996) and LCMC (The Lancaster Corpus of Mandarin Chinese, (McEnery and Xiao, 2003)) for Mainland Mandarin. Both corpora are segmented and tokenized. We remove the punctuation and unify the orthography used to eliminate orthographic cues. Since both corpora are balanced corpora, our initial thought was to provide genre-aware classification. However, inspection of both corpora suggested the genres were not defined in the same way and are not distributed homogeneously. In the next edition this idea may be exploited by using some additional resources as genre vs. regional variations which is an importa"
W19-1401,W19-1419,1,0.847943,"Missing"
W19-1401,W19-1414,0,0.0601978,"Missing"
W19-1401,W16-4801,1,0.6692,"Missing"
W19-1401,W19-1420,0,0.0913091,"ces from newspapers for each Mandarin variety. The main task is to determine if a sentence is written in the Mandarin Cuneiform Language Identification (CLI): This shared task focused on discriminating between languages and dialects originally written using the cuneiform script. The task included 2 dif2 Team Adaptcenter BAM dkosmajac DTeam SharifCL ghpaetzold gretelliz92 ekh IUCL HSE itsalexyang lonewolf MineriaUNAM NRC-CNRC R2I LIS PZ SC-UPB situx SUKI tearsofjoy T¨ubingenOslo Twist Bytes Total GDI CMA DMT X MRC CLI X X System Description Papers (Butnaru, 2019) X X X X X X (Tudoreanu, 2019) (Doostmohammadi and Nassajian, 2019) X X (Hu et al., 2019) (Mikhailov et al., 2019) (Yang and Xiang, 2019) X X X X X X X X (Bernier-Colborne et al., 2019) (Chifu, 2019) (Paetzold and Zampieri, 2019) (Onose and Cercel, 2019) X X X X X X X 7 5 X 8 X X 6 3 (Jauhiainen et al., 2019b) (Wu et al., 2019) (C¸o¨ ltekin and Barnes, 2019) (Benites et al., 2019) 14 Table 1: The teams that participated in the Third VarDial Evaluation Campaign. took part in, and a reference to each of the 14 system description papers published in the VarDial workshop proceedings. ferent languages: Sumerian and Akkadian. Furthermore, the Akkadian language was"
W19-1401,W19-1415,0,0.0347692,"Missing"
W19-1401,L18-1550,0,0.0287077,"sed on a majority voting scheme applied on five classification models: kNearest Neighbors, Logistic Regression, Support Vector Machines, Neural Networks and Random Forests. For the first and the third runs, the models are trained on both training and development sets. For the second run, the model is trained only on the training set. SC-UPB. The SC-UPB team first cleaned the dataset by removing stopwords as well as special characters. The first run submitted to each of the three subtasks is based on a model that represents text as the mean of word vectors given by a pretrained FastText model (Grave et al., 2018). The representation is provided as input to a Recurrent Neural Network with gated recurrent units, which is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. The second run submitted to each of the three subtasks is based on a hierarchical attention network introduced by Yang et al. (2016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting"
W19-1401,W18-4802,0,0.0115539,"zers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then removed any form whic"
W19-1401,W19-1417,0,0.0619352,"Missing"
W19-1401,E17-2034,0,0.0280356,"e-art for this task, however, developing rule-based analyzers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjective"
W19-1401,D18-1135,1,0.824726,"016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting scheme. Their model’s parameters are tuned independently for each subtask, using random search and 5-fold crossvalidation. The tearsofjoy team also tried a transductive learning approach which is based on retraining the model by adding confident predictions from the test set to the training set, an idea previously studied in (Ionescu and Butnaru, 2018). • Binary classification by dialect (subtask 1) – the task is to discriminate between the Moldavian and the Romanian dialects. • MD→RO cross-dialect multi-class categorization by topic (subtask 2) – the task is to classify the samples written in the Romanian dialect into six topics, using a model trained on samples written in the Moldavian dialect. • RO→MD cross-dialect multi-class categorization by topic (subtask 3) – the task is to classify the samples written in the Moldavian dialect into six topics, using a model trained on samples written in the Romanian dialect. 7.2 Participants and App"
W19-1401,W19-1409,1,0.877823,"Missing"
W19-1401,W19-1418,0,0.0615736,"Missing"
W19-1401,N16-1174,0,0.0228023,"t. SC-UPB. The SC-UPB team first cleaned the dataset by removing stopwords as well as special characters. The first run submitted to each of the three subtasks is based on a model that represents text as the mean of word vectors given by a pretrained FastText model (Grave et al., 2018). The representation is provided as input to a Recurrent Neural Network with gated recurrent units, which is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. The second run submitted to each of the three subtasks is based on a hierarchical attention network introduced by Yang et al. (2016). The model is trained using the Adam optimizer with a batch size of 64 for 20 epochs and early stopping. tearsofjoy. The tearsofjoy team used a linear SVM classifier with a combination of character and word n-gram features, which are weighted with the BM25 weighting scheme. Their model’s parameters are tuned independently for each subtask, using random search and 5-fold crossvalidation. The tearsofjoy team also tried a transductive learning approach which is based on retraining the model by adding confident predictions from the test set to the training set, an idea previously studied in (Ione"
W19-1401,W19-1423,1,0.825557,"e ranking for subtask 3, as shown in Table 8. 7.4 Dataset Summary We proposed three MRC subtasks for VarDial 2019. Three participants submitted runs for all three subtasks, and another two participants submitted runs only for subtask 1. Two teams (DTeam 5 12 http://oracc.museum.upenn.edu Language or Dialect Sumerian Old Babylonian Middle Babylonian peripheral Standard Babylonian Neo-Babylonian Late Babylonian Neo-Assyrian two systems in more detail. The PZ team used a SVM metaclassifier ensemble of several linear SVM classifiers trained using character n-gram and character skip-gram features. Paetzold and Zampieri (2019) give further details. The SharifCL team submitted three runs and their best performing system was an ensemble of a SVM and a NB classifier (Doostmohammadi and Nassajian, 2019). The ghpaetzold team submitted only one run using 2-layer compositional recurrent neural network that learns numerical representations of sentences based on their words, and of words based on their characters. Their system is described in more detail by Paetzold and Zampieri (2019). The ekh team used a sum of relative frequencies of character bigrams together with a penalty value for those bigrams or unigrams that were"
W19-1401,W17-1201,1,0.803354,"Missing"
W19-1401,L16-1641,1,0.880892,"Missing"
W19-1401,W14-5307,1,0.821127,"Missing"
W19-1401,W18-0209,1,0.707638,"r, developing rule-based analyzers is a substantial task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then"
W19-1401,W19-0301,1,0.916428,"task. It entails creation of extensive word lists and grammatical descriptions. This requires both linguistic expertise and technical expertise in the rule formalism which is used. Hence, there exists a demand for less labor intensive approaches especially for lowresource languages. Classically, rule-based analyzers have been augmented with statistical guessers which provide analyses for out-of-lexicon word forms (Lind´en, 2009). Recently, purely data-driven morphological analysis has received increasing attention (Nicolai and Kondrak, 2017; Silfverberg and Hulden, 2018; Moeller et al., 2018; Silfverberg and Tyers, 2019). Purely data-driven systems learn an analysis model from a data set of morphologically analyzed word forms and can then be applied 5.1 Dataset The dataset was compiled specifically for the shared task. We used the Wikipedias in all the languages to create a frequency list of surface tokens for each language. We then analysed these lists using the morphological analysers from the Apertium (Forcada et al., 2011) project. The lists of analyses were trimmed to include only openclass parts of speech (nouns, adjectives, adverbs and verbs). We then removed any form which did not include at least one"
W19-1401,E14-2006,0,0.0437089,"Missing"
W19-1401,W19-1422,0,0.0730531,"Missing"
W19-1401,W19-1412,0,0.0741523,"Missing"
W19-3312,W16-4102,1,0.845153,"similar distributional behaviour like Caused-Motion and Dative. As for frames, there has been some work on using distributional similarity between vectors for their unsupervised induction (Ustalov et al., 2018), for comparing frames across languages (Sikos and Pad´o, 2018), and even for the automatic identification of the semantic relations holding between them (Botschen et al., 2017). Modeling sentence comprehension A trend in computational semantics regards the application of DSMs to sentence processing (Mitchell et al., 2010; Lenci, 2011; Sayeed et al., 2015; Johns and Jones, 2015, i.a.). Chersoni et al. (2016, 2017) propose a Distributional Model of sentence comprehension inspired by the general principles of the Memory, Unification and Control framework (Hagoort, 2013, 2015). The memory component includes events in GEK with feature structures containing information directly extracted from parsed sentences in corpora: attributes are syntactic dependencies, while values are distributional vectors of dependent lexemes. Then, they model semantic composition as an event construction and update function F, whose aim is to build a coherent semantic representation by integrating the GEK cued by the lingu"
W19-3312,S17-1021,1,0.872718,"Missing"
W19-3312,P98-1013,0,0.190996,"ic University giulia.rambelli@phd.unipi.it emmanuelechersoni@gmail.com Philippe Blache Aix-Marseille University blache@lpl-aix.fr Chu-Ren Huang The Hong Kong Polytechnic University churen.huang@polyu.edu.hk Alessandro Lenci University of Pisa alessandro.lenci@unipi.it Abstract Obj1 Obj2]). It is worth stressing that, even if the concept of construction is based on the idea that linguistic properties actually emerge from language use, CxG theories have typically preferred to model the semantic content of constructions in terms of hand-made, formal representations like those of Frame Semantics (Baker et al., 1998). This leaves open the issue of how semantic representations can be learned from empirical evidence, and how do they relate to the usage-based nature of Cxs. In fact, for a usage-based model of grammar based on a strong syntax-semantics parallelism, it would be desirable to be grounded on a framework allowing to learn the semantic content of Cxs from language use. In this paper, we propose a new type of semantic representation of Construction Grammar that combines constructions with the vector representations used in Distributional Semantics. We introduce a new framework, Distributional Constr"
W19-3312,J10-4006,1,0.660784,"riples and identify frames as clustered triples. Of course, a limit of this approach is that it only uses subject and object arguments, while frames are generally associated with a wider variety of roles. Lebani and Lenci (2018) instead provide a distributional representation of verb-specific semantic roles as clusters of features automatically induced from corpora. In this paper, we assume that at least some aspects of semantic roles can be derived from combining (e.g., with summation) the distributional vectors of their most prototypical fillers, following an approach widely explored in DS (Baroni and Lenci, 2010; Erk et al., 2010; Sayeed et al., −−−→ 2016; Santus et al., 2017). For instance, the buyer role in the COMMERCIAL TRANSACTION frame can be taken as a vector encoding the properties of the typical nouns filling this role. We are aware that this solution is just an approximation of the content of frames elements. How to satisfactorily characterize semantic frames and roles using DS is in fact still an open research question. 3.3  ditransitive-cx  commercial-transaction-fr     −−−→     BUYER buyer       − − − →   *SELLER seller +      −−−→   SEM    GOODS goods"
W19-3312,W19-2913,0,0.0656285,"Missing"
W19-3312,W17-2618,0,0.0139057,"between linguistic items represented in a vector space. For example, Busso et al. (2018) built a semantic space for several Italian argument constructions and then computed the similarity of their vectors, observing that some Cxs have similar distributional behaviour like Caused-Motion and Dative. As for frames, there has been some work on using distributional similarity between vectors for their unsupervised induction (Ustalov et al., 2018), for comparing frames across languages (Sikos and Pad´o, 2018), and even for the automatic identification of the semantic relations holding between them (Botschen et al., 2017). Modeling sentence comprehension A trend in computational semantics regards the application of DSMs to sentence processing (Mitchell et al., 2010; Lenci, 2011; Sayeed et al., 2015; Johns and Jones, 2015, i.a.). Chersoni et al. (2016, 2017) propose a Distributional Model of sentence comprehension inspired by the general principles of the Memory, Unification and Control framework (Hagoort, 2013, 2015). The memory component includes events in GEK with feature structures containing information directly extracted from parsed sentences in corpora: attributes are syntactic dependencies, while values"
W19-3312,W11-0607,1,0.805924,"ed the similarity of their vectors, observing that some Cxs have similar distributional behaviour like Caused-Motion and Dative. As for frames, there has been some work on using distributional similarity between vectors for their unsupervised induction (Ustalov et al., 2018), for comparing frames across languages (Sikos and Pad´o, 2018), and even for the automatic identification of the semantic relations holding between them (Botschen et al., 2017). Modeling sentence comprehension A trend in computational semantics regards the application of DSMs to sentence processing (Mitchell et al., 2010; Lenci, 2011; Sayeed et al., 2015; Johns and Jones, 2015, i.a.). Chersoni et al. (2016, 2017) propose a Distributional Model of sentence comprehension inspired by the general principles of the Memory, Unification and Control framework (Hagoort, 2013, 2015). The memory component includes events in GEK with feature structures containing information directly extracted from parsed sentences in corpora: attributes are syntactic dependencies, while values are distributional vectors of dependent lexemes. Then, they model semantic composition as an event construction and update function F, whose aim is to build a"
W19-3312,J15-1004,0,0.0290206,"ugust 1st, 2019 2019 Association for Computational Linguistics has never formulated a systematic proposal for deriving representations of constructional meaning from corpus data. Previous literature has mostly focused either on the automatic identification of constructions on the basis of their formal features, or on modeling the meaning of a specific CxG. connection with usage-based theoretical frameworks. To the best of our knowledge, existing attempts of linking DS with models of grammar have rather targeted formal theories like Montague Grammar and Categorial Grammar (Baroni et al., 2014; Grefenstette and Sadrzadeh, 2015). To sum up, both CxG and DS share the assumption that linguistic structures naturally emerge from language usage, and that a representation of both form and meaning of any linguistic item can be modeled through its distributional statistics, and more generally, with the quantitative information derived from corpus data. However, these two models still live in parallel worlds. On the one hand, CxG is a model of grammar in search for a consistent usage-based model of meaning, and, conversely, DS is a computational framework to build semantic representations in search for an empirically adequate"
W19-3312,W19-0129,0,0.0233673,"umption that constructional meanings for argument Cxs arise from the meaning of high frequency verbs that co-occur with them (Goldberg, 1999; Casenhiser and Goldberg, 2005; Barak and Goldberg, 2017), they compute distributional vectors for CxS as the centroids of the vectors of their typical verbs, and use them to model the psycholinguistic data about construction priming in Johnson and Goldberg (2013). This representation of construction meaning has also been applied to study valency coercion by Busso et al. (2018). Following a parallel research line on probing tasks for distributed vectors, Kann et al. (2019) investigate whether word and sentence embeddings encode the grammatical distinctions necessary for inferring the idiosyncratic frame-selectional properties of verbs. Their findings show that, at least 112 3.1 Constructions construction always involve a possession interpretation (more precisely the transfer of something to somebody), represented in the TRANSFER frame. Differently from standard SBCG formalization of Cxs, we add the distributional feature DSVECTOR into the semantic layer in order to integrate lexical distributional representations. The semantic structure of a lexical item can be"
W19-3312,P10-1021,0,0.0386834,"uctions and then computed the similarity of their vectors, observing that some Cxs have similar distributional behaviour like Caused-Motion and Dative. As for frames, there has been some work on using distributional similarity between vectors for their unsupervised induction (Ustalov et al., 2018), for comparing frames across languages (Sikos and Pad´o, 2018), and even for the automatic identification of the semantic relations holding between them (Botschen et al., 2017). Modeling sentence comprehension A trend in computational semantics regards the application of DSMs to sentence processing (Mitchell et al., 2010; Lenci, 2011; Sayeed et al., 2015; Johns and Jones, 2015, i.a.). Chersoni et al. (2016, 2017) propose a Distributional Model of sentence comprehension inspired by the general principles of the Memory, Unification and Control framework (Hagoort, 2013, 2015). The memory component includes events in GEK with feature structures containing information directly extracted from parsed sentences in corpora: attributes are syntactic dependencies, while values are distributional vectors of dependent lexemes. Then, they model semantic composition as an event construction and update function F, whose aim"
W19-3312,D17-1068,1,0.851266,"of this approach is that it only uses subject and object arguments, while frames are generally associated with a wider variety of roles. Lebani and Lenci (2018) instead provide a distributional representation of verb-specific semantic roles as clusters of features automatically induced from corpora. In this paper, we assume that at least some aspects of semantic roles can be derived from combining (e.g., with summation) the distributional vectors of their most prototypical fillers, following an approach widely explored in DS (Baroni and Lenci, 2010; Erk et al., 2010; Sayeed et al., −−−→ 2016; Santus et al., 2017). For instance, the buyer role in the COMMERCIAL TRANSACTION frame can be taken as a vector encoding the properties of the typical nouns filling this role. We are aware that this solution is just an approximation of the content of frames elements. How to satisfactorily characterize semantic frames and roles using DS is in fact still an open research question. 3.3  ditransitive-cx  commercial-transaction-fr     −−−→     BUYER buyer       − − − →   *SELLER seller +      −−−→   SEM    GOODS goods         − − − − →    MONEY money     −−→ PLACE shop F"
W19-3312,P15-1074,0,0.0159296,"rity of their vectors, observing that some Cxs have similar distributional behaviour like Caused-Motion and Dative. As for frames, there has been some work on using distributional similarity between vectors for their unsupervised induction (Ustalov et al., 2018), for comparing frames across languages (Sikos and Pad´o, 2018), and even for the automatic identification of the semantic relations holding between them (Botschen et al., 2017). Modeling sentence comprehension A trend in computational semantics regards the application of DSMs to sentence processing (Mitchell et al., 2010; Lenci, 2011; Sayeed et al., 2015; Johns and Jones, 2015, i.a.). Chersoni et al. (2016, 2017) propose a Distributional Model of sentence comprehension inspired by the general principles of the Memory, Unification and Control framework (Hagoort, 2013, 2015). The memory component includes events in GEK with feature structures containing information directly extracted from parsed sentences in corpora: attributes are syntactic dependencies, while values are distributional vectors of dependent lexemes. Then, they model semantic composition as an event construction and update function F, whose aim is to build a coherent semantic re"
W19-3312,W16-2518,0,0.0248713,"Missing"
W19-3312,W16-1803,1,0.770156,"DING frame In a similar way, events can instantiate an abstract construction dynamically, according to the context. The different lexicalization of the AGENT and the RECIPIENT in the ditransitive construction causes a different selection of the THEME. For example, the fact that the sentence fragment The teacher gives students ... could be completed as in (2) expresses a distributional restriction that can be encoded as an event capturing the co-occurrences 115 tive meaning cannot be decomposed. In computational semantics, a large literature has been aiming at modeling idiomaticity using DSMs. Senaldi et al. (2016) carried out an idiom type identification task representing Italian V-NP and V-PP Cxs as vectors. They observed that the vectors of VN and AN idioms are less similar to the vectors of lexical variants of these expressions with respect to the vectors of compositional constructions. (Cordeiro et al., 2019) realized a framework for predict compound compositionality using DSMs, evaluating to what extent they capture idiomaticity compared to human judgments. Results revealed a high agreement between the models and human predictions, suggesting that they are able to incorporate information about idi"
W19-3312,W18-3813,0,0.0668121,"Missing"
W19-3312,P18-2010,0,0.0640524,"h i     SYN CAT 1 V       n o     LIN 2 ≺ 1 ≺ 3 ≺ 4  FORM   PROPERTIES   n L    L o   3; 3 4 ADJ 1     D E   ARG - ST 2 NPx[subj] , 3 NPy[obl] , 4 NPz[obj]              transfer-fr     *    +      AGENT x FRAMES         RECIPIENT y   SEM             THEME z        −−−−−−−−→  DS - VECTOR Figure 1: Description of read verb ditransitive Figure 2: Description of ditransitive Cx  ata (2015) use distributional representations to induce embeddings for predicates and their arguments. Ustalov et al. (2018) propose a different methodology for unsupervised semantic frame induction. They build embeddings as the concatenations of subject-verb-object triples and identify frames as clustered triples. Of course, a limit of this approach is that it only uses subject and object arguments, while frames are generally associated with a wider variety of roles. Lebani and Lenci (2018) instead provide a distributional representation of verb-specific semantic roles as clusters of features automatically induced from corpora. In this paper, we assume that at least some aspects of semantic roles can be derived fr"
W19-3312,D15-1295,0,0.0438303,"Missing"
W19-5521,P94-1002,0,0.123522,"Missing"
W19-5521,J06-4003,0,0.118719,"Missing"
W19-5521,A00-2035,0,0.155106,"Missing"
W19-5521,J02-3002,0,0.103879,"Missing"
W19-5521,J97-2002,0,0.253654,"Missing"
W19-5521,C12-2096,0,0.148169,"Missing"
W19-5521,A97-1004,0,0.503091,"Missing"
W19-5521,H89-2048,0,0.0679758,"Missing"
wang-etal-2010-automatic,J90-1003,0,\N,Missing
wang-etal-2010-automatic,E03-1073,0,\N,Missing
wang-etal-2010-automatic,J09-1005,0,\N,Missing
wang-etal-2010-automatic,P95-1007,0,\N,Missing
wang-etal-2010-automatic,P94-1033,0,\N,Missing
wang-etal-2010-automatic,chen-etal-2006-study,0,\N,Missing
xu-etal-2012-grammar,Y06-1024,1,\N,Missing
xu-etal-2012-grammar,Y96-1018,1,\N,Missing
Y00-1012,O98-3003,1,0.886369,"Missing"
Y00-1012,Y99-1005,1,0.782394,"Missing"
Y00-1012,Y96-1018,1,0.890113,"Missing"
Y00-1012,Y99-1004,1,0.873613,"Missing"
Y00-1012,J93-2005,0,0.09282,"Missing"
Y00-1012,O98-3004,1,0.921236,"Missing"
Y00-1012,O00-2004,1,\N,Missing
Y00-1012,O99-1005,1,\N,Missing
Y01-1002,O00-2002,1,\N,Missing
Y03-1013,O98-3003,1,0.92299,"ecided to leave from the dancing circle (formed by dancers) because he is tired. (7b) He decided to leave the dancing area circle because of being tired. (7c) He decided to leave the dancing circle because he is tired of all the dancing. Based on the above examples, quanlzi5 seems to have four meanings. In next section, we will display the semantic representation of quanlzi5 in terms of a semantic model and give a clear explanation. 3. Accounting for the Meaning of Quanlzi5 Quanlzi5 is an example of polysemy, which is a situation where a single word has a set of related meanings. According to Ahrens et al (1998), the lexical meaning of a polysemy can be distinguished into two levels: senses and meaning facets&apos;. They stated that a lexical sense entails the following properties: (A) different sense cannot appear in the same context (unless the complexity is triggered deliberately); (B) a sense is not an instance of metonymic or meronymic extension, but may be an instance of metaphorical extension (Lin and Ahrens, 2000); (C) the link between two senses cannot be inherited by a class of nouns. On the other hand, a meaning facet has the properties as follows: 1 The &apos;meaning facet&apos; used here differs from &apos;"
Y04-1003,huang-etal-2004-sinica,1,\N,Missing
Y04-1015,Y96-1018,1,0.807186,"Missing"
Y04-1015,W03-1705,0,0.0386639,"Missing"
Y04-1015,W00-0744,0,\N,Missing
Y04-1015,P94-1019,0,\N,Missing
Y06-1024,I05-3007,1,0.911359,"Missing"
Y06-1024,ma-huang-2006-uniform,1,0.874107,"Missing"
Y06-1024,O98-3004,1,0.861187,"Missing"
Y06-1027,C90-2010,1,0.576118,"Missing"
Y06-1027,P89-1010,0,0.388147,"Missing"
Y06-1027,I05-3007,1,0.853573,"ations and only one pattern for the simplest verb-object relation as shown as (2) (2) Collocating Pattern for Object from CSE I 1:""V[BCJ]"" ""Di""? ""N[abc]""? ""DE""? ""N[abc]""? 2: ""Na"" [tag!= ""Na""] (""XXX"" represents XXX is a regular expression, ""XXX""? represents XXX appears zero or one time, ""XXX""{a,b} represents XXX appears a~b times.) In (2), the 1: and 2: identify the two collocated components. Between the components, zero or one particle may appear (denoted by ""Di""?), zero or one processor may appears (denoted by any_noun? ""DE""?), and zero or one noun-modifier may appears (denoted by ""N[abc]""?) Huang et al. (2005) pointed out that the prototype version of CSE I did not deal with the prevalent non-canonical word orders in Chinese (3). In addition, we also noticed that it fails to identify grammatical relations when an argument lies some distance away from a verb because of internal modification (4). Chinese objects often occur in pre-verbal positions in various pre-posing constructions, such as topicalization. (3) a. 全穀麵包，吃了很健康。 quan.gu mian.bao, chi le hen jian.kang whole-grain bread, eat LE very healthy ‘Eating whole-grain bread is very healthy.’ b. 有人嘗試要將這荷花分類，卻越分越累。 you ren chang.shi yao jiang zhe h"
Y06-1027,P98-2127,0,0.0421635,"Missing"
Y06-1027,ma-huang-2006-uniform,1,0.7926,"OURCE[PP{自、於}] THEME< SOURCE[PP{歸、為}]<VI (A<B represents B appears behind A. A<<B represents B appears immediately behind A) 3.3 Implementation: Preparation of Corpus and Grammar There are two steps in the implementation of CSE II: the first step is corpus preparation, and the second is grammar adaptation. For corpus, we follow CSE I and use the LDC Chinese Gigaword Corpus because of its size (over 11 billion characters) and its coverage of both traditional and simplified characters. The Gigaword Corpus was fully automatically segmented and tagged using the Academia Sinica tagset and tagtool (Ma and Huang 2006). Our work in adaptation for CSE II includes resolution of categorical ambiguity for nominalization and improvement of unknown word resolution. 208 For grammar adaptation, we concentrate on exploiting lexically encoded ICG grammatical knowledge. Since the corpus was tagged with Academia Sinica tagset, the verb-subclass information for each verb is specified. Hence we can utilize the structural information from ICG BP. Since the tagged corpus has identified the verb-subclasses, we are able to correctly identify different grammatical relations, even though two verbs may share the same local stru"
Y06-1027,J90-1003,0,\N,Missing
Y06-1027,C98-2122,0,\N,Missing
Y06-1043,W93-0231,0,0.223651,"committing choice. In this example we placed all the terms in question under the AstronomicalBody SUMO concept and under SkyObject for our own taxonomy proposal. Moreover, it is clear that it exists different lexical organization for a given domain. See for example, the division studies of bodyparts in [8] or the one of geographical object in [9]. Closer to our experiment, EuroWordNet team noticed that in Dutch there is no concept for a generic container while in other languages this term was available for being included in the core lexical structure. Finally, see [12] for a discussion of the [5] example of wood, tree and forest in French, German and English. As a conclusion on this topic, the nature of the list and structure we are aiming too is essentially linguistic and do not pretend to say anything about deeper cognitive structure. This issue highlights again our need to separate the lexical and the ontological level. The fact that a language selected a term does not mean that concepts that did not received similar label are absent. Moreover the permeability between word senses [7] and the robustness of the semantic system allows for an adaptation of the language. In the ’sun’ ex"
Y06-1043,huang-etal-2004-sinica,1,0.874655,"e in the context of face-to-face interaction. 3 The experiments 3.1 The experiments on Chinese The Chinese Swadesh list was obtained by consulting with the Academia Sinica Chinese Wordnet group. One or more Chinese Wordnet entry for each item of the list were obtained, and non basic readings were eliminated. Subsequently, we obtain automatically the concept distribution of the items in SUMO taxonomy through SINICABOW,3 a resource developed at the Academia Sinica which combines the Chinese wordnet, the Princeton WordNet and SUMO. 2 3 See http://www.rosettaproject.org/ for more information. See [6] and http://bow.sinica.edu.tw/ for more information. 326 3.2 The experiments on English About the English list we studied three different ways for building a taxonomy out of the simple list: A. Really keep the structure as minimal as possible by not adding any further (generalizing) concept in the list. B. Keep the structure as minimal a possible but also try to get a reasonable organization from a knowledge representation viewpoint. C. Simply align the terms to SUMO ontology. The options (B) and (C) were performed in two steps: (i) disambiguate the words of the list by mapping them to WordNet"
Y06-1043,zhang-etal-2004-distributional,1,0.548079,"roach, the terms appearing frequently in definitions gloss are good candidate for being integrated in the core lexicon. The main problem of this approach is that the upper level of existing ontologies are generally fairly abstract concepts that are rarely lexicalized 1stClassEntity in EuroWordNet or SelfConnectedObject in SUMO[10], and that are intuitively far from being member of a core lexicon. 2.2 Frequency criterion The second approach uses more statistical data such as word frequency. However, simple word frequency is not good criterion for selecting the basic terms. A recent elaboration [18] proposed to use the notion of distributional consistency. This measure provides better result than other statistically based approaches but it requires balanced corpus of significant size. Such corpora are only available for few languages and we would like to have a method that could be used with languages deprived from extensive resources. 2.3 Swadesh list or the universality criterion The lack of resources for most of languages led us to consider the Swadesh list [15] (reproduced as an appendix) as a potential core lexicon. The Swadesh list has been developed by Moriss Swadesh in the fiftie"
Y06-1043,P06-2106,1,\N,Missing
Y07-1012,Y96-1018,1,0.802211,"Missing"
Y07-1012,P89-1010,0,0.19873,"iginally designed for psychologists, but later was used extensively by computational linguists. Similarly, corpora such as British National Corpus (BNC), the Academia Sinica Corpus of Mandarin Chinese (Chen et al., 1996) and the Gigaword corpus were also designed for the use of target groups such as lexicographers, linguists, language teachers, language learners, etc. These corpora usually provide some forms of statistical analyses so that users will be able to summarize their research results quickly. For example, many corpora provide collocational measures such as Mutual Information values (Church and Hanks, 1989) so that collocated words can be sorted according to their frequency of co-occurrence. Sketch Engine (Kilgarriff and Tugwell, 2001) is a powerful resource which displays search summary in collocated patterns, as well as according to grammatical relations. However, like many other resources, Sketch Engine is unable to determine which of the results in the list are meaningful linguistically. Therefore, when provided with collocation lists, most linguists report the top “few,” based on their preferences. Some linguists report the top one or two and keep the rest in appendixes. In fact, the curren"
Y07-1015,O00-2001,1,0.879639,"Missing"
Y07-1015,O00-2003,1,0.728097,"ide based on SUMO. First, we collect our data from Sinica Corpus and check their senses from Chinese Wordnet. Next, we take these physical event senses of da3 into SUMO concept system (Huang et al. 2004) to find all possible concepts and distinguish them into different categories. Finally, we analyzed these concepts for semantic features which can help us to compare our analysis with the analysis in Gao’s study (2001). 2. Previous research Regarding verb studies, previous research has focused on VV compound verbs in Modern Chinese (Hong and Huang, 2004), or on near synonyms in Modern Chinese (Chief et al, 2000; Huang et al. 2000; Liu 2002; Tsai, 2002; Huang and Hong, 2005). Also, some scholars have worked on da3 polysemy analyses. Da3 is one if the most frequently used verbs, being ranked 16 in the list of most frequently used verbs in Chinese (Bei and Zhang, 1988). Specifically, Gao * Copyright 2007 by Hong, Jia-Fei, Chu-Ren Huang , Kathleen Ahrens 155 (2001) explored the semantic properties of da3 and its prototypical meaning and categorized its semantic representations to show the systematic patterning of its meaning extensions. 3. Motivation and Goals Language knowledge representation is a mani"
Y07-1015,Y04-1015,1,0.877639,"Missing"
Y07-1015,huang-etal-2004-sinica,1,0.861813,"Missing"
Y07-1015,O00-2002,1,\N,Missing
Y08-1018,I05-7002,1,\N,Missing
Y08-1042,W03-1004,0,0.0868378,"ywords: Top-bag-of-word similarity, Text Source Classification, Contrastive Approach, Comparable Corpus, Chinese Gigaword. 1. Introduction Comparable corpora are corpora which select similar texts in more than one language or language variety 1 . These texts are typically gathered during the same time period. Comparable corpora are different from parallel corpora are widely used as resources for statistical machine translation, bilingual lexicons. Comparable corpora overcome the scarcity and limitations of parallel corpora, since sources for original, monolingual texts are much more abundant (Barzilay and Elhadad, 2003; Munteanu et al., 2004; Shao and Ng, 2004; Talvensaari et al., 2007). The degree and nature of lexical similarity and contrast among Mandarin Chinese used in different Chinese speaking societies were widely observed but not thoroughly studied due to the lack of comparable corpora. Recently, LDC’s Chinese Gigaword (2003)contains three sets of monolingual corpora selected according to the same set of criteria but in different language varieties from China, Singapore and Taiwan. We will explore it as a comparable corpus for variations of Chinese in this paper. In particular, we propose a measure"
Y08-1042,O02-2002,0,0.0946677,"measure. Shao and Ng (2004) proposed a method by combining both context and transliteration information for the task of mining new word translations. They translated Chinese Words into English and tested it on Chinese and English Gigaword. Munteanu et al. (2004) improved machine translation performance via parallel sentence extraction from comparable corpora that consist of two large monolingual news texts in English and Arabic. Talvensaari et al. (2007) used Relative Average Term Frequency (RATF) valve to create a comparable corpus from articles by a Swedish news agency and a U.S. newspaper. Chen and You (2002) proposed using only syntactic related co-occurrences as context vectors and adopted information theoretic methods for measuring word similarity to solve the problem of data sparseness and characteristic precision. Gao et al. (2002) extended the basic cooccurrence model by adding a decaying factor that decreases the mutual information when the distance between the terms increases. The experimental results also indicated that their proposed triple translation model brings further improvements than word-by-word translation. Weeds and Weir (2005) proposed a flexible framework called as co-occurre"
Y08-1042,P98-1069,0,0.0201269,"concludes this study. 1 Definition of comparable corpus according http://www.ilc.cnr.it/EAGLES/corpustyp/node21.html to EAGLES report, 22nd Pacific Asia Conference on Language, Information and Computation, pages 404–410 404 accessed at 2. Literature Review 2.1 Word Similarity Measures in Comparable Corpus A comparable corpus is one which gathered the similar texts in more than one language or language variety from the same time periods. Comparable corpora were widely used in researches issues consist of machine translation, natural language processing and cross language information retrieval. Fung and Yee (1998) used a comparable-corpus-based approach to estimate the similarity between a word and its translation candidates. Fung and Cheung (2004) used multi-level bootstrapping to iteratively improve alignment for extracting parallel sentences from a quasi-comparable corpus. Cheng et al. (2004) mined bilingual search results obtained from search engines to translate unknown query terms. Their approach was with Web corpora that can alleviate the problem of the lack of large bilingual corpora and benefit cross-language Web search. Barzilay and Elhadad (2003) focused on monolingual comparable corpus, i.e"
Y08-1042,huang-etal-2008-quality,1,0.821955,"Missing"
Y08-1042,ma-huang-2006-uniform,1,0.875784,"Missing"
Y08-1042,N04-1034,0,0.0469673,"larity, Text Source Classification, Contrastive Approach, Comparable Corpus, Chinese Gigaword. 1. Introduction Comparable corpora are corpora which select similar texts in more than one language or language variety 1 . These texts are typically gathered during the same time period. Comparable corpora are different from parallel corpora are widely used as resources for statistical machine translation, bilingual lexicons. Comparable corpora overcome the scarcity and limitations of parallel corpora, since sources for original, monolingual texts are much more abundant (Barzilay and Elhadad, 2003; Munteanu et al., 2004; Shao and Ng, 2004; Talvensaari et al., 2007). The degree and nature of lexical similarity and contrast among Mandarin Chinese used in different Chinese speaking societies were widely observed but not thoroughly studied due to the lack of comparable corpora. Recently, LDC’s Chinese Gigaword (2003)contains three sets of monolingual corpora selected according to the same set of criteria but in different language varieties from China, Singapore and Taiwan. We will explore it as a comparable corpus for variations of Chinese in this paper. In particular, we propose a measure of top-bag-of-word sim"
Y08-1042,C04-1089,0,0.0572338,"ssification, Contrastive Approach, Comparable Corpus, Chinese Gigaword. 1. Introduction Comparable corpora are corpora which select similar texts in more than one language or language variety 1 . These texts are typically gathered during the same time period. Comparable corpora are different from parallel corpora are widely used as resources for statistical machine translation, bilingual lexicons. Comparable corpora overcome the scarcity and limitations of parallel corpora, since sources for original, monolingual texts are much more abundant (Barzilay and Elhadad, 2003; Munteanu et al., 2004; Shao and Ng, 2004; Talvensaari et al., 2007). The degree and nature of lexical similarity and contrast among Mandarin Chinese used in different Chinese speaking societies were widely observed but not thoroughly studied due to the lack of comparable corpora. Recently, LDC’s Chinese Gigaword (2003)contains three sets of monolingual corpora selected according to the same set of criteria but in different language varieties from China, Singapore and Taiwan. We will explore it as a comparable corpus for variations of Chinese in this paper. In particular, we propose a measure of top-bag-of-word similarity for compari"
Y08-1042,W02-1811,0,0.0180443,"Missing"
Y08-1042,J05-4002,0,0.0629182,"icles by a Swedish news agency and a U.S. newspaper. Chen and You (2002) proposed using only syntactic related co-occurrences as context vectors and adopted information theoretic methods for measuring word similarity to solve the problem of data sparseness and characteristic precision. Gao et al. (2002) extended the basic cooccurrence model by adding a decaying factor that decreases the mutual information when the distance between the terms increases. The experimental results also indicated that their proposed triple translation model brings further improvements than word-by-word translation. Weeds and Weir (2005) proposed a flexible framework called as co-occurrence retrieval for lexical distributional similarity. Zheng et al. (2007) presented a novel word co-occurrence model based on an ontology representation of word sense and its related applications. 2.2 Introduction to Chinese Gigaword Automatic annotation is remains a challenging task in Chinese language processing. For instance, ACL SigHan has hosted four bakeoff competition for segmentation, but none for POS tagging. There is only a handful of POS tagging systems and automatic taggers which are widely accepted and accessible. In Taiwan, Academ"
Y08-1042,C04-1151,0,\N,Missing
Y08-1042,C98-1066,0,\N,Missing
Y08-1042,N03-1031,0,\N,Missing
Y09-1010,W03-0405,0,0.216644,"veloped for a specific type of text, to be applied to its similar text fragments. For example, most statistical IE systems are developed for a news corpus, therefore it is better to apply them to formal-style fragments. In addition, web data also have their ways of conveying certain information. For example, it is common that occupation and affiliation information is expressed in a homepage in the format of “Name, Position, Affiliation,” such as “Anita Coleman, Assistant Professor, University of Arizona.” As this kind of web-specific expression is often multi-lines, some existing IE patterns (Mann & Yarowsky, 2003; Mann, 2006; Rosenfeld & Feldman, 2006), which were limited to one sentence or were designed for formal style text, cannot be directly applied. To identify this web expression property, we develop web-specific patterns, which take into account of different kinds of information, such as webpage type information (i.e. homepage and biographical webpage), and text expression style (formal style and informal style). The experiment shows that those patterns could achieve high precision, which is very important for real applications. Instead of presenting a totally new IE solution for web data, the"
Y09-1010,W03-0430,0,0.0328991,", NER, which extracts possible attribute value candidates, is a basic component, and RDR, which detects relations involving the focus person and given attribute value candidates and further selects valid attribute values for that person. To effectively tackle IE, both NER and RDR are required to have a good performance. For NER, the naïve approach is rule-based, but its big disadvantage is the difficulty of ruledesign (Feldman, 2002) or rule-learning (Etzioni et al., 2005), which needs to handle various named-entity expressions. In recent years, statistical NER technology (Bikel et al., 1999; McCallum & Li, 2003) has been significantly improved through a series of evaluations, such as Automatic Content Extraction (ACE), Message Understanding Conference (MUC), and so on. However, because those NER technologies mainly focused on news documents, it was so dependent on text information, such as capitalization information and corpus type, that their performance dropped much when working on web data because those cues are rather noisy. 3 http://nlp.uned.es/weps/ 83 Besides, the adaptation of these NER systems to other kinds of corpus is not easy (Vilain et al., 2007). Although some NER systems, e.g. Minkov"
Y09-1010,H05-1056,0,0.0664052,"a. Nevertheless, few of them have explored how to effectively utilize or integrate different IE tools for web data. Copyright 2009 by Ying Chen, Sophia Y. M. Lee, and Chu-Ren Huang http://nlp.uned.es/weps/ 2 http://ilps.science.uva.nl/trec-entity/ 1 23rd Pacific Asia Conference on Language, Information and Computation, pages 82–91 82 In this paper, we propose a framework to integrate heterogeneous IE approaches for web IE. In this framework, we first segment web data according to the expression format. Similar to the genre categories – “formal text” and “informal text” – which were defined in Minkov et al. (2005), a text is either in “formal style” or “informal style.” A “formal style” text obeys prescribed writing standards, i.e. a complete sentence usually with a subject and an object. On the contrary, “informal style” has few limitations on writing format and can mix various representation levels. In order to do so, we develop a novel algorithm to segment a webpage into fragments according to their expression format: formal style and informal style. This segmentation allows an existing IE system, which often was developed for a specific type of text, to be applied to its similar text fragments. For"
Y09-1010,P06-2086,0,0.181375,", to be applied to its similar text fragments. For example, most statistical IE systems are developed for a news corpus, therefore it is better to apply them to formal-style fragments. In addition, web data also have their ways of conveying certain information. For example, it is common that occupation and affiliation information is expressed in a homepage in the format of “Name, Position, Affiliation,” such as “Anita Coleman, Assistant Professor, University of Arizona.” As this kind of web-specific expression is often multi-lines, some existing IE patterns (Mann & Yarowsky, 2003; Mann, 2006; Rosenfeld & Feldman, 2006), which were limited to one sentence or were designed for formal style text, cannot be directly applied. To identify this web expression property, we develop web-specific patterns, which take into account of different kinds of information, such as webpage type information (i.e. homepage and biographical webpage), and text expression style (formal style and informal style). The experiment shows that those patterns could achieve high precision, which is very important for real applications. Instead of presenting a totally new IE solution for web data, the goal of this paper aims at providing a f"
Y09-1010,N07-2046,0,0.0318777,"ical NER technology (Bikel et al., 1999; McCallum & Li, 2003) has been significantly improved through a series of evaluations, such as Automatic Content Extraction (ACE), Message Understanding Conference (MUC), and so on. However, because those NER technologies mainly focused on news documents, it was so dependent on text information, such as capitalization information and corpus type, that their performance dropped much when working on web data because those cues are rather noisy. 3 http://nlp.uned.es/weps/ 83 Besides, the adaptation of these NER systems to other kinds of corpus is not easy (Vilain et al., 2007). Although some NER systems, e.g. Minkov et al. (2005), attempted to do the adaptation work from news corpus to a non-news corpus, they still focused on a homogeneous corpus. Nevertheless, web data is heterogeneous in nature and it is impossible to know the source information of the documents. Therefore, it is very difficult to do NER adaptation for web data. In this paper, we explore a problem: how to effectively re-use the existing well-developed NER system for web data. Compared to NER, RDR is still a comparatively hot and difficult topic. Although some statistical RDR systems have been dev"
Y09-1011,D09-1150,0,0.124689,"Missing"
Y09-1011,C08-1111,0,0.116765,"esigned to excite some emotion. And most events do activate emotion, regardless of whether they are designed to do so. Given the critical roles emotions play in human activities, it is not surprising that sentiment analysis, as coarse-grained account of emotional tendencies (positive, negative, and neutral) became one of the most popular topics in NLP and IE. What is surprising is that there were few studies on emotion computing, which would offer finer-grained information and will be universally applicable regardless of domain and product types. With regard to emotion processing, some works (Tokuhisa et al., 2008; Mihalcea and Liu, 2006) have been done on text, and most of them use the resource from web, i.e. web blog and analysis that can be explored, such as emotion detection (Tokuhisa et al., 2008), emotion classification (Mishne, 2005; Mihalcea and Liu, 2006), and emotion trend prediction (Mishne & Rijke, 2005; Balog & Rijke, 2006). In this paper, we discuss a basic yet important question in emotion analysis: How to classify and represent emotions? Although scientific study of emotion can be traced all the way back to early philosophers, both in the West and in the East, we still lack a standard t"
Y09-1031,W04-2214,0,0.154547,"and Cavaglia, 2000). Synsets have been semi-automatically annotated with at least one domain labels. These domain labels, such as Music, Transport, and Law, are selected from a set of about 200 labels that are hierarchally organized referred to the Dewey Decimal Classification (DDC). Magnini and Cavaglia (2000) manually annotated a small number of high level synsets with their domain labels. Then, an automatic procedure exploited some of the WordNet relations to extend the manual assignment to the reachable synsets. In addition, an exception procedure was used to prevent a wrong propagation. Bentivogli et al. (2004) further revised the WordNet Domains Hierarchy (WDH) with a clear semantics and evaluated the coverage and balancing of Basic Domains of WDH. The latest version, WordNet Domains 3.2, contains the mapping between Princeton WordNet 2.0 synsets and their corresponding domains. 45 Basic Domains of total 168 domains are used to annotate WordNet synsets. Take “00197005-n history law” for example, “00197005-n” is the synset off set and Part-of-Speech and “history law” is the list of domains associated to the synset. Notice that an additional label named as “Factotum” was assign to Generic synset, whi"
Y09-1031,J06-1003,0,0.0141984,"andarin Chinese in Taiwan, Huang et al. (2004a) constructed the Academia Sinica Bilingual Ontological Wordnet (Sinica BOW), which integrates WordNet, English-Chinese Translation Equivalents Database (ECTED) and SUMO for cross-language linguistic studies. As a follow-up, Chinese WordNet has been built as a robust lexical knowledge system which embodies a precise expression of sense and sense relations as well (Huang et al., 2008b). In recent years, WordNet-like resources have become one of the most reliable and essential resource for linguistic studies for all languages (Niles and Pease, 2003; Budanitsky and Hirst, 2006; Soria et al., 2009a ). Semantic domain labels, characterized by domain-specific lexica, are profitably used to describe texts and word senses according to general subjects, such as sport, finance, and politics. WordNet Domains (Magnini and Cavaglia, 2000) was created by extending the Princeton WordNet with domains labels. Synsets have been semi-automatically annotated with at least one domain label. A domain can include synsets of different part-of-speech and from different WordNet sub-hierarchies. So far the existing WordNets such as Italian WordNet, ∗ This work was funded by National Scien"
Y09-1031,W02-1106,1,0.800153,"t and WordNet Domains. Chinese WordNet focuses to provide precise expression for the Chinese sense division and lexical semantic relations. In addition, partial senses are annotated with domain nodes from Domain Lexico-Taxonomy. WordNet Domains contains the mapping between Princeton WordNet 2.0 synsets and their corresponding domains. Synsets have been semi-automatically annotated with at least one domain label, which is selected from Dewey Decimal Classification (DDC). Since cross-lingual lexical semantic relation inferences were examined by bootstrapping a Sinica BOW with Princeton WordNet (Huang et al. 2002; 2003b), we decide to use bootstrapping methods for constructing a language resources named as Chinese WordNet Domains. By using an existing WordNet Domains as a medium, we automatically annotate word senses of Chinese WordNet with semantic domain labels from three aspects: 1) Princeton WordNet alignment, 2) lexical semantic relations and 3) domain taxonomy mapping. Details will be described as the following subsections. 3.1 Alignment-mediated Domain Prediction Word senses of Chinese WordNet are strictly aligned with the corresponding synsets of Princeton WordNet; therefore, we can use alignm"
Y09-1031,huang-etal-2004-sinica,1,0.935732,"ng Chinese WordNet Domains to the community for research purposes. Keywords: Bootstrapping, Chinese WordNet, WordNet Domains, Multi-label. 1 Introduction Princeton WordNet is an English lexical database that groups nouns, verbs, adjectives and adverbs into sets of cognitive synonyms, which are named as synsets (Fellbaum, 1998; Miller, 1995). The Global WordNet Association (GWA), built on the results of Princeton WordNet and Euro WordNet (Vossen, 2004), is a free and public association that provides a platform that shares and connects all languages in the world. For Mandarin Chinese in Taiwan, Huang et al. (2004a) constructed the Academia Sinica Bilingual Ontological Wordnet (Sinica BOW), which integrates WordNet, English-Chinese Translation Equivalents Database (ECTED) and SUMO for cross-language linguistic studies. As a follow-up, Chinese WordNet has been built as a robust lexical knowledge system which embodies a precise expression of sense and sense relations as well (Huang et al., 2008b). In recent years, WordNet-like resources have become one of the most reliable and essential resource for linguistic studies for all languages (Niles and Pease, 2003; Budanitsky and Hirst, 2006; Soria et al., 200"
Y09-1031,magnini-cavaglia-2000-integrating,0,0.630896,"s a follow-up, Chinese WordNet has been built as a robust lexical knowledge system which embodies a precise expression of sense and sense relations as well (Huang et al., 2008b). In recent years, WordNet-like resources have become one of the most reliable and essential resource for linguistic studies for all languages (Niles and Pease, 2003; Budanitsky and Hirst, 2006; Soria et al., 2009a ). Semantic domain labels, characterized by domain-specific lexica, are profitably used to describe texts and word senses according to general subjects, such as sport, finance, and politics. WordNet Domains (Magnini and Cavaglia, 2000) was created by extending the Princeton WordNet with domains labels. Synsets have been semi-automatically annotated with at least one domain label. A domain can include synsets of different part-of-speech and from different WordNet sub-hierarchies. So far the existing WordNets such as Italian WordNet, ∗ This work was funded by National Science Council, Taiwan under Grants NSC 97-2923-I-001-001-MY3, and also cooperated with EU-FP7 KYOTO project. Copyright 2009 by Lung-Hao Lee, Yu-Ting Yu, and Chu-Ren Huang 23rd Pacific Asia Conference on Language, Information and Computation, pages 288–296 288"
Y09-1031,W09-3418,1,\N,Missing
Y09-1031,I05-3014,1,\N,Missing
Y09-1031,O05-5001,1,\N,Missing
Y09-1032,O00-2004,1,0.839304,"Missing"
Y09-1033,P07-1056,0,0.274462,"opose the classification algorithm to do the classification on the text with two-bags-of-words modeling. The remainder of this paper is organized as follows. Section 2 introduces the related work on CVS applications in sentiment classification. Section 3 presents our approach in detail. Experimental results are presented and analyzed in Section 4. Finally, Section 5 draws our conclusions and outlines the future work. 2 Related Work During recent several years, various of issues have been studied for sentiment classification, such as feature extraction (Riloff et al., 2006), domain adaptation (Blitzer et al., 2007) and multi-domain learning (Li and Zong, 2008). For a detailed survey of this research field, see Pang and Lee (2008). However, most studies directly borrow machine learning approach from traditional topic-based text classification and very few work are focus on incorporating linguistic knowledge that sentiment text particularly contains, e.g., valence shifting phenomena and comparative sentences (Jindal and Liu, 2006). Pang et al. (2002) first employ machine learning approach to sentiment classification and find that machine learning methods definitely outperform human-produced baselines. In"
Y09-1033,P08-2065,1,0.809329,"Missing"
Y09-1033,W02-1011,0,0.0211866,"-bag-of-words modeling) across five different domains. Keywords: Sentiment classification, opinion mining, linear Classifier. 1 Introduction Sentiment classification is a task to classify text according to sentimental polarities of opinions they contain (e.g., favorable or unfavorable). This task has received considerable interests in computational linguistic community due to its wide applications. In the latest studies of this task, machine learning techniques become the state-of-the-art approach and have achieved much better results than some rule-based approaches (Kennedy and Inkpen, 2006; Pang et al., 2002) . In machine learning approach, a document (text) is usually modeled as a bag-of-words, a set of words without any word order or syntactic relation information. Therefore, the whole sentimental orientation is highly influenced by the sentiment polarity of each word. Notice that although each word takes a fixed sentiment polarity itself, its polarity contributed to the whole sentence or document might be completely the opposite. Negation and contrast transition are exactly the two kinds of linguistic phenomena which are able to reverse the sentiment polarity. For example, see a sentence contai"
Y09-1033,P04-1035,0,0.208111,"ed in terms of linear classifiers, the corresponding ideas for the first and third strategies are general for any other classification algorithms. Overall speaking, only the third one really utilizes both the reversed-sentiment and non-reversed sentiment information for learning. Also, it shares the similar computational complexity as traditional machine learning approaches based on one-bag-of-words modeling. 4 Experimental Studies 4.1 Experimental Setup Data Set: There are some famous public data sets available for sentiment classification studies. Among them, Cornell movie-review dataset 1 (Pang and Lee, 2004) and product reviews 2 (Blitzer et al., 2007) are most popularly used. Both of them are 2-category (positive and negative) tasks and each consists of 2,000 reviews in a domain. The results in some previous work are sometimes not consistent due to the application of different domains of reviews when negation is considered (Pang et al., 2002 and Na et al., 2004). Thus we follow the way of Blitzer et al. (2007) to collect more data involving data in our experiments. Specifically, we totally collect 5 domains of reviews from Amazon.cn, namely Book, Camera, HD (Hard Disk), Health and Kitchen. Each"
Y09-1033,W06-1652,0,0.0366118,"Missing"
Y09-1033,P02-1053,0,0.00962774,"olarity contributed to the whole sentence or document might be completely the opposite. Negation and contrast transition are exactly the two kinds of linguistic phenomena which are able to reverse the sentiment polarity. For example, see a sentence containing negation ""this movie is not good"" and another sentence containing contrast transition ""this mouse is good looking, but it works terribly"". The sentiment polarity of the word good in these two sentences is positive but the whole sentences are negative. Therefore, we can see that the whole sentiment is not necessarily the sum of the parts (Turney, 2002). This phenomenon is one main reason why machine learning often fails to classify some testing samples (Dredze et al., 2008). Fortunately, a language usually has some special words which indicate the possible polarity shift of a word or even a sentence. These words are called contextual valence shifters (CVSs) which can cause the valence of a lexical item to shift from one pole to the other or, less forcefully, even to modify the valence towards a more neutral position (Polanyi and Zaenen, 2006). Generally speaking, CVSs are classified into two categories: sentence-based and Copyright 2009 by"
Y09-2034,W08-0336,0,0.0137294,"eywords: Chinese word segmentation, conditional random field, word boundary decision. 1 Introduction Chinese word segmentation (CWS) is the task of segmenting text of character string into word list as original Chinese text contains no explicit boundaries between every two words. This task is an indispensible preprocessing requirement for many applications in Chinese language technology. A realistic CWS system necessarily performs well on both segmentation accuracy and speed. Segmentation accuracy is essential for many applications. For instance, in machine translation for Chinese to English (Chang et al., 2008), segmentation errors would cause translation mistakes directly. Translation systems without a wonderful CWS model are impossible to offer good results. State-of-the-arts approach called character tagging (Xue, 2003) has shown to be excellent in segmentation accuracy. This approach mainly aims to detect the character position in a certain word, e.g., beginning, middle or end of a word. It achieves much better performances than traditional word-based (or dictionary) approach, e.g., n-gram word maximum probability (Sun et al., 2006), because of its apparent advantages on detecting out-ofvocabula"
Y09-2034,P07-2018,1,0.904627,"ocessing step in the applications, long segmentation time would make the applications’ whole running time unacceptable by users. Therefore, it is meaningful to simplify the complexity of CWS approaches so as to reducing segmentation training and testing time. In terms of this view, character tagging approach (often using 4-tags (Xue, 2003; Ng and Low, 2004) or even 6-tags (Zhao et al., 2006)) is not so satisfactory in its training and testing time. Especially, given a very huge training data, this approach might not get a training model due to its large time and memory space demand. Recently, Huang et al. (2007) propose an interesting approach, named word boundary decision (WBD), which turns from words towards word boundaries. WBD tries to detect the Copyright 2009 by Shoushan Li and Chu-Ren Huang 23rd Pacific Asia Conference on Language, Information and Computation, pages 726–732 726 nature of boundary between two characters, which can be either a word boundary or not, i.e. a boundary between two words or a mere character boundary. This approach performs better than traditional word-based (or dictionary) approach but still worse than character tagging approach (Huang et al., 2008). However, this app"
Y09-2034,O08-1009,1,0.882268,"demand. Recently, Huang et al. (2007) propose an interesting approach, named word boundary decision (WBD), which turns from words towards word boundaries. WBD tries to detect the Copyright 2009 by Shoushan Li and Chu-Ren Huang 23rd Pacific Asia Conference on Language, Information and Computation, pages 726–732 726 nature of boundary between two characters, which can be either a word boundary or not, i.e. a boundary between two words or a mere character boundary. This approach performs better than traditional word-based (or dictionary) approach but still worse than character tagging approach (Huang et al., 2008). However, this approach takes a big advantage over character tagging approach in its training and testing time. In this paper, we deeply analyze the relationship between character tagging approach and WBD approach and propose a new implementation of WBD approach with conditional random field (CRF) learning approach. This implementation will make WBD approach achieve competitive performance compared to character tagging approach with 4-tags which represents the state-of-the-art approach in CWS studies but need much less training time and memory space. In the remaining part of the paper, we rev"
Y09-2034,P08-1102,0,0.202328,"o the classification. For example, when classifying the character ‘新’, we use the character features and also use the previous tag (the tag of the character ‘的’) in the classification features. 3 WBD Implementation with Character Tagging using CRF The segmentation task is to classify each character with a tag of &apos;1&apos; or &apos;0&apos;, which represents a word boundary appears after this character or not. There are several classification algorithms which can be applied to do the segmentation, such as maximum entropy (Xue, 2003), conditional random field (CRF) (Tseng et al., 2005) and perceptron algorithm (Jiang et al., 2008). We use CRF learning method as it gives state-of-the-arts performance for word segmentation and can also easily incorporate different types of features (Tseng et al., 2005). CRF is a statistical sequence modeling framework which aims to compute the following probability of a label sequence for a particular of character string: 1 pλ (Y |W ) = exp(∑∑ λk f k ( yt −1 ,W , t )) Z (W ) t∈T k where Y = { yt } is the label sequence for a character string. Here, yt ∈ {1, 0} which represents that whether there is a word boundary after the current character or not. W is the sequence of unsegmented chara"
Y09-2034,W06-0115,0,0.025879,"ion 1 Features C0 , C1 C−1C0 , C0 C1 , C1C2 T−1C0T0 , C−1T−1C0T0 , T−1C0T0C1 This tool is available at: http://crfpp.sourceforge.net/ 729 Function The single character features The character bi-gram features The character adding tag transition features 4 Experimental Studies In this section, we would empirically compare the two implementations: WBD with metaprobability classification (Huang et al., 2007) and WBD with character tagging with CRF. Furthermore, we would compare the WBD with character tagging implement with traditional 4tag character tagging approach. We use SIGHAN Bakeoff 2 data (Levow, 2006) for experimental studies. The data consists of four different sources: PKU, MSR, CityU, and AS. Their detailed information is given in Table 3. In all experiments, we mainly use F-measure (F1) as the performance measurement. F1 is defined as F1 = 2 PR / ( P + R ) where P is precision and R is recall. Another evaluation measurement is out-of-vocabulary (OOV) recall, which is used to evaluate the ability of OOV word recognition. Table 3: Corpus Information Corpus Abbrv. Beijing University Microsoft Research City University of Hong Kong Academia Sinica PKU MSR CityU AS Training Size (Words/Types"
Y09-2034,W04-3236,0,0.0573578,"vantages on detecting out-ofvocabulary (OOV) words. On the other side, the segmentation speed is also very important in some applications, such as information retrieval and online machine translation systems. Since CWS system is almost always used as a preprocessing step in the applications, long segmentation time would make the applications’ whole running time unacceptable by users. Therefore, it is meaningful to simplify the complexity of CWS approaches so as to reducing segmentation training and testing time. In terms of this view, character tagging approach (often using 4-tags (Xue, 2003; Ng and Low, 2004) or even 6-tags (Zhao et al., 2006)) is not so satisfactory in its training and testing time. Especially, given a very huge training data, this approach might not get a training model due to its large time and memory space demand. Recently, Huang et al. (2007) propose an interesting approach, named word boundary decision (WBD), which turns from words towards word boundaries. WBD tries to detect the Copyright 2009 by Shoushan Li and Chu-Ren Huang 23rd Pacific Asia Conference on Language, Information and Computation, pages 726–732 726 nature of boundary between two characters, which can be eithe"
Y09-2034,I05-3027,0,0.0546458,"rrent character but also its previous tag to do the classification. For example, when classifying the character ‘新’, we use the character features and also use the previous tag (the tag of the character ‘的’) in the classification features. 3 WBD Implementation with Character Tagging using CRF The segmentation task is to classify each character with a tag of &apos;1&apos; or &apos;0&apos;, which represents a word boundary appears after this character or not. There are several classification algorithms which can be applied to do the segmentation, such as maximum entropy (Xue, 2003), conditional random field (CRF) (Tseng et al., 2005) and perceptron algorithm (Jiang et al., 2008). We use CRF learning method as it gives state-of-the-arts performance for word segmentation and can also easily incorporate different types of features (Tseng et al., 2005). CRF is a statistical sequence modeling framework which aims to compute the following probability of a label sequence for a particular of character string: 1 pλ (Y |W ) = exp(∑∑ λk f k ( yt −1 ,W , t )) Z (W ) t∈T k where Y = { yt } is the label sequence for a character string. Here, yt ∈ {1, 0} which represents that whether there is a word boundary after the current character"
Y09-2034,O03-4002,0,0.712794,"ontains no explicit boundaries between every two words. This task is an indispensible preprocessing requirement for many applications in Chinese language technology. A realistic CWS system necessarily performs well on both segmentation accuracy and speed. Segmentation accuracy is essential for many applications. For instance, in machine translation for Chinese to English (Chang et al., 2008), segmentation errors would cause translation mistakes directly. Translation systems without a wonderful CWS model are impossible to offer good results. State-of-the-arts approach called character tagging (Xue, 2003) has shown to be excellent in segmentation accuracy. This approach mainly aims to detect the character position in a certain word, e.g., beginning, middle or end of a word. It achieves much better performances than traditional word-based (or dictionary) approach, e.g., n-gram word maximum probability (Sun et al., 2006), because of its apparent advantages on detecting out-ofvocabulary (OOV) words. On the other side, the segmentation speed is also very important in some applications, such as information retrieval and online machine translation systems. Since CWS system is almost always used as a"
Y09-2034,Y06-1012,0,0.704104,"lary (OOV) words. On the other side, the segmentation speed is also very important in some applications, such as information retrieval and online machine translation systems. Since CWS system is almost always used as a preprocessing step in the applications, long segmentation time would make the applications’ whole running time unacceptable by users. Therefore, it is meaningful to simplify the complexity of CWS approaches so as to reducing segmentation training and testing time. In terms of this view, character tagging approach (often using 4-tags (Xue, 2003; Ng and Low, 2004) or even 6-tags (Zhao et al., 2006)) is not so satisfactory in its training and testing time. Especially, given a very huge training data, this approach might not get a training model due to its large time and memory space demand. Recently, Huang et al. (2007) propose an interesting approach, named word boundary decision (WBD), which turns from words towards word boundaries. WBD tries to detect the Copyright 2009 by Shoushan Li and Chu-Ren Huang 23rd Pacific Asia Conference on Language, Information and Computation, pages 726–732 726 nature of boundary between two characters, which can be either a word boundary or not, i.e. a bo"
Y10-1045,O05-5005,0,0.0271875,"hes to present and refer the same target. In WordNet, the definition of a lexically ambiguous word is the ambiguity of an individual word or phrase that can be used (in different contexts) to express two or more different meanings. WordNet researchers also regard polysemy and lexical ambiguity as synonym. In the case of the ∗ Copyright 2010 by Jia-Fei Hong, Sue-Jin Ker, Chu-Ren Huang and Kathleen Ahrens 399 400 Poster Papers previously related lexical ambiguity, several studies have concentrated on the corpus-based and computational perspective included: Peng et al. (2007), Xue et al. (2006), Chen et al. (2005), Moldovan and Novischi (2004) …and so on and they took several different approaches: used the corpus-based approach, an adaptive system, divided the sense of lexically ambiguous word and found the possibility of the senses of a word. Although a suite of heuristical methods are presented for word sense disambiguation of Chinese Wordnet glosses, unfortunately we know of several researchers who use only manual analysis to find out the argumentative roles and predict their semantic features to determine their senses. Therefore, they can’t deal with more quantities of lexically ambiguous words at"
Y10-1045,U06-1008,0,0.021404,"Missing"
Y10-1045,W04-0804,0,0.0422421,"efer the same target. In WordNet, the definition of a lexically ambiguous word is the ambiguity of an individual word or phrase that can be used (in different contexts) to express two or more different meanings. WordNet researchers also regard polysemy and lexical ambiguity as synonym. In the case of the ∗ Copyright 2010 by Jia-Fei Hong, Sue-Jin Ker, Chu-Ren Huang and Kathleen Ahrens 399 400 Poster Papers previously related lexical ambiguity, several studies have concentrated on the corpus-based and computational perspective included: Peng et al. (2007), Xue et al. (2006), Chen et al. (2005), Moldovan and Novischi (2004) …and so on and they took several different approaches: used the corpus-based approach, an adaptive system, divided the sense of lexically ambiguous word and found the possibility of the senses of a word. Although a suite of heuristical methods are presented for word sense disambiguation of Chinese Wordnet glosses, unfortunately we know of several researchers who use only manual analysis to find out the argumentative roles and predict their semantic features to determine their senses. Therefore, they can’t deal with more quantities of lexically ambiguous words at the same time. We consulted Fu"
Y10-1045,C90-2067,0,0.185568,"Missing"
Y10-1045,Y04-1028,1,\N,Missing
Y10-1045,P06-2118,0,\N,Missing
Y10-1045,O05-2006,0,\N,Missing
Y10-1062,P08-1105,0,\N,Missing
Y10-1062,N09-2040,0,\N,Missing
Y10-1081,N01-1009,0,0.0316205,"idering this, (Pustejovsky, 1995) proposed selective binding, which takes place where a lexical item or phrase operates specifically on the substructure of a phrase, without changing the overall type in the composition. (Pustejovsky, 2000) further revealed that adjectives bind into the qualia structure of nouns to select a narrow facet of the noun&apos;s meaning. For example, a large (Formal) carved (Agentive) wooden (Constitutive) useful (Telic) arrow ([Formal; Constitutive; Agentive; Telic]) illustrates the richness of the qualia structure and each adjective selects individual qualia. Moreover, (Lapata, 2001) explored polysemous adjectives whose sense changes due to the nouns they modify. The probabilistic model he proposed can provide a ranking on the possible interpretation of the adjectives. (Saint-Dizier, 1998) proposed several extensions to the telic role through analyzing the French adjective “bon”. (Lenci, et al., 2000) extended the possible qualia role values to express fine-grained distinctions between semantic types. These studies give us a general idea of the behavior of adjectival modification to nouns and the way of extending qualia structure. However, no previous research on adjectiv"
Y10-1081,bel-etal-2000-simple,0,0.0157932,"elect a narrow facet of the noun&apos;s meaning. For example, a large (Formal) carved (Agentive) wooden (Constitutive) useful (Telic) arrow ([Formal; Constitutive; Agentive; Telic]) illustrates the richness of the qualia structure and each adjective selects individual qualia. Moreover, (Lapata, 2001) explored polysemous adjectives whose sense changes due to the nouns they modify. The probabilistic model he proposed can provide a ranking on the possible interpretation of the adjectives. (Saint-Dizier, 1998) proposed several extensions to the telic role through analyzing the French adjective “bon”. (Lenci, et al., 2000) extended the possible qualia role values to express fine-grained distinctions between semantic types. These studies give us a general idea of the behavior of adjectival modification to nouns and the way of extending qualia structure. However, no previous research on adjectival modification based on selective binding has been conducted regarding Mandarin Chinese. The goal of our research is to find out the interaction between adjectives and their head nouns, including: finding out the argument types an adjective can combine with; showing an adjective may modify individuals or events; lookin"
Y10-1081,P98-2187,0,0.0462946,"he composition. (Pustejovsky, 2000) further revealed that adjectives bind into the qualia structure of nouns to select a narrow facet of the noun&apos;s meaning. For example, a large (Formal) carved (Agentive) wooden (Constitutive) useful (Telic) arrow ([Formal; Constitutive; Agentive; Telic]) illustrates the richness of the qualia structure and each adjective selects individual qualia. Moreover, (Lapata, 2001) explored polysemous adjectives whose sense changes due to the nouns they modify. The probabilistic model he proposed can provide a ranking on the possible interpretation of the adjectives. (Saint-Dizier, 1998) proposed several extensions to the telic role through analyzing the French adjective “bon”. (Lenci, et al., 2000) extended the possible qualia role values to express fine-grained distinctions between semantic types. These studies give us a general idea of the behavior of adjectival modification to nouns and the way of extending qualia structure. However, no previous research on adjectival modification based on selective binding has been conducted regarding Mandarin Chinese. The goal of our research is to find out the interaction between adjectives and their head nouns, including: finding out"
Y10-1081,C98-2182,0,\N,Missing
Y11-1054,W96-0309,0,0.188974,"associated with some values. 1) The constitutive role is the relation between an object and its constituents or parts. The role values include material, weight, parts and component elements. 2) The formal role can distinguish the object within a larger domain. Orientation, magnitude, shape, dimensionality, color, and position are its role values. 3) The telic role is about the purpose and function of the object. 4) The agentive role describes factors involved in the origin of an object, such as creator, artifact, natural kind, causal chain. Under the theoretical framework of qualia structure, Johnston & Busat (1996) analyze the nominal compounding phrases in English and Italian. They find that the modifier can specify the type of an argument to a predicate in a telic, agentive or constitutive role. Lee et al. (2010) demonstrate the qualia modification within a noun-noun compound. Following (Johnston & Busat 1996; Lee et al. 2010), we examine the qualia roles involved between the modifier and head in event nouns. Telic modification: the modifier is the function or aim of the head. For example, 淘汰賽 táotài-sài ‘elimination game’, 選 拔 賽 xuǎnbá-sài ‘selective trial’, 衛 冕 賽 wèimiǎn-sài ‘championship fight’. 51"
Y11-1054,O10-2010,0,0.0276968,"can distinguish the object within a larger domain. Orientation, magnitude, shape, dimensionality, color, and position are its role values. 3) The telic role is about the purpose and function of the object. 4) The agentive role describes factors involved in the origin of an object, such as creator, artifact, natural kind, causal chain. Under the theoretical framework of qualia structure, Johnston & Busat (1996) analyze the nominal compounding phrases in English and Italian. They find that the modifier can specify the type of an argument to a predicate in a telic, agentive or constitutive role. Lee et al. (2010) demonstrate the qualia modification within a noun-noun compound. Following (Johnston & Busat 1996; Lee et al. 2010), we examine the qualia roles involved between the modifier and head in event nouns. Telic modification: the modifier is the function or aim of the head. For example, 淘汰賽 táotài-sài ‘elimination game’, 選 拔 賽 xuǎnbá-sài ‘selective trial’, 衛 冕 賽 wèimiǎn-sài ‘championship fight’. 512 Agentive modification: the modifier is origin or cause of the head. For instance, 訂婚宴 dìnghūn-yàn ‘a betrothal party’, 答謝宴 dáxiè-yàn ‘return Banquet’. Constitutive modification: the modifier is the comp"
Y11-1054,J91-4003,0,0.0408684,"‘French food’, 學生餐 xuéshengcān ‘student meal’ ~ 會 huì ‘meeting, gathering; council’: 舞 蹈 會 wǔdǎohuì ‘dance council’, 校 董 會 xiàodǒnghuì ‘school direct council’, 哲學會 zhéxuéhuì ‘philosophy council’ ~ 病 bìng ‘disease’: 心 臟 病 xīnzāngbìng ‘heart disease’, 心 血 管 病 xīnxuèguǎnbìng ‘cardiovascular disease’, 胃病 wèibìng ‘gastric disease’ These examples show that compound event nouns have double roots. The right root is the head of an event noun, expressing generic event types, while the modifying left root elaborates information on the event. 3 Qualia Modification of the ‘Modifier-head’ Type Event Nouns Pustejovsky (1991, 1995) shows how lexical items encode semantic information in the qualia structure. There are four roles in the qualia structure, and each is associated with some values. 1) The constitutive role is the relation between an object and its constituents or parts. The role values include material, weight, parts and component elements. 2) The formal role can distinguish the object within a larger domain. Orientation, magnitude, shape, dimensionality, color, and position are its role values. 3) The telic role is about the purpose and function of the object. 4) The agentive role describes factors in"
Y12-1007,W96-0309,0,0.16208,"http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId =LDC2009T14 2 http://db1x.sinica.edu.tw/kiwi/mkiwi/ 3 http://158.132.124.36/, http://wordsketch.ling.sinica.edu.tw/ 70 Copyright 2012 by Shan Wang, Chu-Ren Huang, and Hongzhi Xu 26th Pacific Asia Conference on Language,Information and Computation pages 70–79 3 Compositional Mechanisms of [N1 + Artifactual-Type Event Nouns] The internal structure of NN compounds has been widely investigated (Jackendoff 1975; Laurie Bauer 2008; Packard 2004; Warren 1978). In recent years, some research uses GL to analyze the relation between N1 and N2 (Johnston & Busa 1996; Lee et al. 2010; Qi 2012). The research using the GL gives a compositional treatment to capture the N1 and N2 relations, but it only concerns the situation when N1 is a qualia role of N2. It does not explain cases when N2 is a qualia role of N1. Moreover, it does not give a generalization for the qualia modification relation. The following section analyses the compositional mechanisms of NN compounds. To make the discussion more concentrate, Section 3.2 and 3.3 use [N1+ֺ bsài ‘competition’] as a case study. To introduce a new way of compositionality, sub-composition, Section 3.4 uses a wid"
Y12-1007,O10-2010,0,0.0303659,"du/Catalog/catalogEntry.jsp?catalogId =LDC2009T14 2 http://db1x.sinica.edu.tw/kiwi/mkiwi/ 3 http://158.132.124.36/, http://wordsketch.ling.sinica.edu.tw/ 70 Copyright 2012 by Shan Wang, Chu-Ren Huang, and Hongzhi Xu 26th Pacific Asia Conference on Language,Information and Computation pages 70–79 3 Compositional Mechanisms of [N1 + Artifactual-Type Event Nouns] The internal structure of NN compounds has been widely investigated (Jackendoff 1975; Laurie Bauer 2008; Packard 2004; Warren 1978). In recent years, some research uses GL to analyze the relation between N1 and N2 (Johnston & Busa 1996; Lee et al. 2010; Qi 2012). The research using the GL gives a compositional treatment to capture the N1 and N2 relations, but it only concerns the situation when N1 is a qualia role of N2. It does not explain cases when N2 is a qualia role of N1. Moreover, it does not give a generalization for the qualia modification relation. The following section analyses the compositional mechanisms of NN compounds. To make the discussion more concentrate, Section 3.2 and 3.3 use [N1+ֺ bsài ‘competition’] as a case study. To introduce a new way of compositionality, sub-composition, Section 3.4 uses a wider range of data."
Y12-1007,Y12-1063,1,\N,Missing
Y12-1046,Y96-1018,1,0.705692,"Missing"
Y12-1046,C10-2162,0,0.0278841,"Missing"
Y12-1063,Y11-1054,1,\N,Missing
Y14-1018,J10-4006,1,0.804961,"of the contrasting pairs in GRE closest-to-opposite questions are not listed as opposites in WordNet”. Copyright 2014 by Enrico Santus, Qin Lu, Alessandro Lenci and Chu-Ren Huang 28th Pacific Asia Conference on Language, Information and Computation pages 135–144 !135 PACLIC 28 The automatic identification of semantic relations is a core task in computational semantics. Distributional Semantic Models (DSMs) have often been exploited for their well known ability to identify semantically similar lexemes using corpus-derived co-occurrences encoded as distributional vectors (Santus et al., 2014a; Baroni and Lenci, 2010; Turney and Pantel, 2010; Padó and Lapata, 2007; Sahlgren, 2006). These models are based on the Distributional Hypothesis (Harris, 1954) and represent lexical semantic similarity in function of distributional similarity, which can be measured by vector cosine (Turney and Pantel, 2010). However, these models are characterized by a major shortcoming. That is, they are not able to discriminate among different kinds of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponym"
Y14-1018,P10-1018,0,0.0497849,"Missing"
Y14-1018,2020.inlg-1.14,0,0.0238601,"Missing"
Y14-1018,C92-2082,0,0.398782,"gether with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a distributional interpretation of the so-called paradox of simultaneous similarity and difference between the antonyms (Cruse, 1986). According to this paradox, antonyms are similar to synonyms"
Y14-1018,J91-1001,0,0.690848,"l need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a distributional interpretation of the so-called paradox of simultaneous similarity and difference between the antonyms (Cruse, 1986). According to this paradox, antonyms are similar to synonyms in every dimension of meaning except one. Our hypothesis is that the different dimension of meaning is a salient one and it can be identified with DSMs and exploited for discriminating antonyms from synonyms. The rest of the paper is organized as follows. Section 2 gives the definition and illustrates the various types of antonyms. Section 3 gives a brief o"
Y14-1018,D13-1169,0,0.0689646,"Missing"
Y14-1018,S12-1012,1,0.925115,"are characterized by a major shortcoming. That is, they are not able to discriminate among different kinds of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponyms like house, meronyms like brick, antonyms like shack, together with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Ch"
Y14-1018,W11-2128,0,0.0412439,"Missing"
Y14-1018,H05-1067,0,0.0744627,"Missing"
Y14-1018,D08-1103,0,0.428385,"makes use of Average Precision to estimate the extent and salience of the intersection among the most descriptive contexts of two target words. Evaluation shows that the proposed method is able to distinguish antonyms and synonyms with high accuracy across different parts of speech, including nouns, adjectives and verbs. APAnt outperforms the vector cosine and a baseline model implementing the cooccurrence hypothesis. 1 Introduction Antonymy is one of the fundamental relations shaping the organization of the semantic lexicon and its identification is very challenging for computational models (Mohammad et al., 2008; Deese, 1965; Deese, 1964). Yet, antonymy is essential for many Natural Language Processing (NLP) applications, such as Information Retrieval (IR), Ontology Learning (OL), Machine Translation (MT), Sentiment Analysis (SA) and Dialogue Systems (Roth and Schulte im Walde, 2014; Mohammad et al., 2013). In particular, the automatic identification of semantic opposition is a crucial component for the detection and generation of paraphrases (Marton et al., 2011), the understanding of contradictions (de Marneffe et al., 2008) and the detection of humor (Mihalcea and Strapparava, 2005). Several exist"
Y14-1018,J07-2002,0,0.0433936,"ite questions are not listed as opposites in WordNet”. Copyright 2014 by Enrico Santus, Qin Lu, Alessandro Lenci and Chu-Ren Huang 28th Pacific Asia Conference on Language, Information and Computation pages 135–144 !135 PACLIC 28 The automatic identification of semantic relations is a core task in computational semantics. Distributional Semantic Models (DSMs) have often been exploited for their well known ability to identify semantically similar lexemes using corpus-derived co-occurrences encoded as distributional vectors (Santus et al., 2014a; Baroni and Lenci, 2010; Turney and Pantel, 2010; Padó and Lapata, 2007; Sahlgren, 2006). These models are based on the Distributional Hypothesis (Harris, 1954) and represent lexical semantic similarity in function of distributional similarity, which can be measured by vector cosine (Turney and Pantel, 2010). However, these models are characterized by a major shortcoming. That is, they are not able to discriminate among different kinds of semantic relations linking distributionally similar lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponyms like house, meronyms like brick, antonyms like"
Y14-1018,P06-1015,0,0.714471,"e brick, antonyms like shack, together with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a distributional interpretation of the so-called paradox of simultaneous similarity and difference between the antonyms (Cruse, 1986). According to this paradox, antonyms are simi"
Y14-1018,P14-2086,0,0.361997,"Missing"
Y14-1018,W14-5814,0,0.121769,"Missing"
Y14-1018,C02-1061,0,0.103734,"Missing"
Y14-1018,C08-1114,0,0.435435,"r lexemes. For instance, the nearest neighbors of castle in the vector space typically include hypernyms like building, co-hyponyms like house, meronyms like brick, antonyms like shack, together with other semantically related words. While impressive results have been achieved in the automatic identification of synonymy (Baroni and Lenci, 2010; Pado and Lapata, 2007), methods for the identification of hypernymy (Santus et al., 2014a; Lenci and Benotto, 2012) and antonymy (Roth and Schulte im Walde, 2014; Mohammad et al. 2013) still need much work to achieve satisfying precision and coverage (Turney, 2008; Mohammad et al., 2008). This is the reason why semisupervised pattern-based approaches have often been preferred to purely unsupervised DSMs (Pantel and Pennacchiotti, 2006; Hearst, 1992). In this paper, we introduce APAnt, a new Average-Precision-based distributional measures that is able to successfully discriminate antonyms from synonyms, outperforming vector cosine and a baseline system based on the co-occurrence hypothesis, formulated by Charles and Miller in 1989 and confirmed in other studies, such as those of Justeson and Katz (1991) and Fellbaum (1995). Our measure is based on a dis"
Y14-1018,E14-4008,1,0.918306,"wo lexemes are antonyms or synonyms by looking at the extent and salience of this intersection: the broader and more salient the intersection, the higher the probability that the lexemes are synonyms; vice versa the narrower and less salient the intersection, the higher the probability that the lexemes are antonyms. To verify this hypothesis, we select the N most salient contexts of the two target words (N=1001). We define the salience of a context for a specific target word by ranking the contexts through Local Mutual Information (LMI; Evert, 2005) and picking the first N, as already done by Santus et al. (2014a). Once the N most salient contexts for the two target words have been identified, we verify the extent and the salience of the contexts shared by both the target words. We predict that synonyms share a significantly higher number of salient contexts than antonyms. To estimate the extent and the salience of the shared contexts, we adapt the Average Precision measure (AP; Voorhees and Harman, 1999), a common Information Retrieval (IR) evaluation metric already used by Kotlerman et al. (2010) to identify lexical entailment. In IR systems, this measure is used to evaluate the ranked documents re"
Y14-1018,J13-3004,0,\N,Missing
Y14-1018,P08-1118,0,\N,Missing
Y14-1057,Y08-1018,1,0.797333,", we followed some experimental configuration in (Li et al., 2006) and put more emphasis on evaluating the performance of subjectivity classification using radical representation Qiu et al. (2009) presented an approach to guess word’s sense by its components(characters), they used the LC(lexical compositionality) principle: “The words formed by similar constituents in the same mode fall into the same semantic category”. When we use radicals to generalize characters, we are following the similar principle. If two characters share the same radical, they may fall into the same semantic category. Huang et al. (2008) presented a qualia structure to analyze how characters derived from radicals, they classified the derived concepts of character radicals into 7 categories, expanded from the original four qualia aspects of Formal, Constitutive, Agentive, and Telic. This structure is useful when we label radicals for subjectivity classification, because characters derived by similar path may have similar concept, and when we use radicals to generalize characters, we can choose an accurate semantic category (finer than a radical) to avoid semantic roughness. For example, a frequent radical, such as <(human), ca"
Y14-1057,P06-1069,0,0.0286722,"iled, and much emphasis is put on polarity instead of subjectivity. Furthermore, compared with English, subjectivity classification on Chinese is relatively few. In the following, we pay more attention to work on Chinese, subjectivity and Chinese radicals. Yao and Peng (2007) used 7 features to describe a text, which include “if a personal pronoun occurs in the sentence?”, “if interjection occurs in the sentence?”, etc. A SVM-based method offered the best performance (F-value 0.938) in their experiments. The work used a small corpus which includes 359 texts (191 subjective and 168 objective). Li et al. (2006) made a detail comparison between words and character-bigrams when they are used to represent features in text classification, and concluded that Chinese character bigrams are better than words in feature representation for text classification. In our experiments, we followed some experimental configuration in (Li et al., 2006) and put more emphasis on evaluating the performance of subjectivity classification using radical representation Qiu et al. (2009) presented an approach to guess word’s sense by its components(characters), they used the LC(lexical compositionality) principle: “The words"
Y14-1057,P05-1015,0,0.0339957,"high quality is required. As we know, the size of common Chinese characters is around several thousands, while the size of radicals in Chinese is only around several hundreds, if we use the radicals to generalize character, it may overcome to some extent the sparseness problem Copyright 2014 by Ge Xu and Chu-Ren Huang 28th Pacific Asia Conference on Language, Information and Computation pages 495–502 !495 PACLIC 28 when training model and reduce the time and space required. 2 Related work Sentiment classification on texts has been studied by many researchers, such as (Goldberg and Zhu, 2006; Pang and Lee, 2005) etc. Normally, machine learning-based methods dominate the filed, and much emphasis is put on polarity instead of subjectivity. Furthermore, compared with English, subjectivity classification on Chinese is relatively few. In the following, we pay more attention to work on Chinese, subjectivity and Chinese radicals. Yao and Peng (2007) used 7 features to describe a text, which include “if a personal pronoun occurs in the sentence?”, “if interjection occurs in the sentence?”, etc. A SVM-based method offered the best performance (F-value 0.938) in their experiments. The work used a small corpus"
Y14-1057,Y09-2004,0,0.029782,"t performance (F-value 0.938) in their experiments. The work used a small corpus which includes 359 texts (191 subjective and 168 objective). Li et al. (2006) made a detail comparison between words and character-bigrams when they are used to represent features in text classification, and concluded that Chinese character bigrams are better than words in feature representation for text classification. In our experiments, we followed some experimental configuration in (Li et al., 2006) and put more emphasis on evaluating the performance of subjectivity classification using radical representation Qiu et al. (2009) presented an approach to guess word’s sense by its components(characters), they used the LC(lexical compositionality) principle: “The words formed by similar constituents in the same mode fall into the same semantic category”. When we use radicals to generalize characters, we are following the similar principle. If two characters share the same radical, they may fall into the same semantic category. Huang et al. (2008) presented a qualia structure to analyze how characters derived from radicals, they classified the derived concepts of character radicals into 7 categories, expanded from the or"
Y15-1007,Y96-1018,1,0.171647,"ugh with it, then the new method is also acceptable to be an alternative. We conducted two similar semantic transparency rating experiments using the Mechanical Turk-based method and the traditional laboratory-based method. We will compare the results from these two experiments to see their agreement hence we can further evaluate the Mechanical Turk-based experimentation. 2 Method 2.1 MTurk-based Semantic Transparency Rating Experiment2 2.1.1 Materials We selected a total of 1, 176 disyllabic Chinese nominal compounds which have mid-range word frequencies and appear in both Sinica Corpus 4.0 (Chen et al., 1996) and the “Lexicon of Common Words in Contemporary Chinese 现代汉语常用词 表”, see Wang et al. (2014) for details. 2.1.2 Experimental Design Normally, a crowdsourcing experiment should be reasonably small in size. We randomly divide these 1,176 words into 21 groups, Gi (i = 1, 2, 3, ..., 21); each group has 56 words. Questionnaires We collect overall semantic transparency (OST) and constituent semantic transparency (CST) data of these words. In order to avoid 2 We have reported this experiment in Wang et al. (2014). 55 interaction, we designed two kinds of questionnaires to collect OST data and CST dat"
Y15-1007,W10-0719,0,0.094872,".) and behavioral research (e.g., survey and experimentation) for social sciences has already been recognized and attempted (Snow et al., 2008; Kittur et al., 2008; Chen et al., 2009). Especially since around 2010, there have been more and more reports on conducting experimental research using MTurk (Mason and Suri, 2012; Rand, 2012; Buhrmester et al., 2011; Horton et al., 2011; 1 For the demographics of Crowdflower’s worker pool, see https://success.crowdflower.com/hc/en-us/articles/202703345Contributors-Crowd-Demographics, retrieved on Apr. 22, 2015. 54 Paolacci et al., 2010; Schmidt, 2010; Munro et al., 2010; Schnoebelen and Kuperman, 2010; Sprouse, 2011; Enochson and Culbertson, 2015; Kuperman et al., 2012) and several of them focus on linguistic experiments (Munro et al., 2010; Schnoebelen and Kuperman, 2010; Sprouse, 2011; Enochson and Culbertson, 2015; Kuperman et al., 2012). Experiments conducted on MTurk platforms are usually survey-based and use web questionnaires composed using the GUI toolkits provided by the platforms, and advanced users can make use of HTML, CSS, JavaScript, Adobe Flash (Simcox and Fiez, 2014; Enochson and Culbertson, 2015), etc., to realize additional elements, custom"
Y15-1007,Q14-1007,0,0.016737,"pay (Mason and Suri, 2012; Sprouse, 2011). The most famous MTurk platform is Amazon’s Mechanical Turk (AMT, www.mturk.com) which was lunched publicly in November 2005; it started early and is so popular in the academic world that it is the de facto standard of MTurk implementation, and the genre name MTurk actually originated from its name and is used by some writers to refer to AMT specially. There are other MTurk implementations, for example another well known MTurk platform is Crowdflower (www.crowdflower.com). Relevant demographics shows that the workers on either AMT (Ross et al., 2010; Pavlick et al., 2014; Ipeirotis, 2010) or Crowdflower1 are come from all over the world, so both can be treated as international MTurk platforms. In the early stage of the development of MTurk, it’s potential to be an efficient and economic tool for linguistic data collection (e.g., annotation, transcription, translation, etc.) and behavioral research (e.g., survey and experimentation) for social sciences has already been recognized and attempted (Snow et al., 2008; Kittur et al., 2008; Chen et al., 2009). Especially since around 2010, there have been more and more reports on conducting experimental research usin"
Y15-1007,D08-1027,0,0.0681583,"Missing"
Y15-1007,W14-5818,1,0.651386,"imilar semantic transparency rating experiments using the Mechanical Turk-based method and the traditional laboratory-based method. We will compare the results from these two experiments to see their agreement hence we can further evaluate the Mechanical Turk-based experimentation. 2 Method 2.1 MTurk-based Semantic Transparency Rating Experiment2 2.1.1 Materials We selected a total of 1, 176 disyllabic Chinese nominal compounds which have mid-range word frequencies and appear in both Sinica Corpus 4.0 (Chen et al., 1996) and the “Lexicon of Common Words in Contemporary Chinese 现代汉语常用词 表”, see Wang et al. (2014) for details. 2.1.2 Experimental Design Normally, a crowdsourcing experiment should be reasonably small in size. We randomly divide these 1,176 words into 21 groups, Gi (i = 1, 2, 3, ..., 21); each group has 56 words. Questionnaires We collect overall semantic transparency (OST) and constituent semantic transparency (CST) data of these words. In order to avoid 2 We have reported this experiment in Wang et al. (2014). 55 interaction, we designed two kinds of questionnaires to collect OST data and CST data respectively. So Gi (i = 1, 2, 3, ..., 21) has two questionnaires, one OST questionnaire f"
Y15-1021,baccianella-etal-2010-sentiwordnet,0,0.00970063,"s is to measure the contributions on polarity values by different POS tags. – The “PolarityDep” is similar to “PolarityWin”, but it differs in that the negation is checked based on the dependency structure. – The “PolarShiftWin” measures the difference between the most positive item and the most negative item in a window of size 5. – The “PolarShiftDep” measures the polarity difference of “parent-child” pairs in the dependency structures of the tweets. Four sentiment dictionaries were used: Opinion Lexicon (Hu and Liu, 2004), Afinn (Nielsen, 2011), MPQA (Wiebe et al., 2005), and SentiWordnet (Baccianella et al., 2010). The union and intersection of the four dictionaries are also used as two additional dictionaries. Formally, the polarity feature can be represented as a (key, val) pair, where the key is <pos, dict&gt;, or <dict&gt;. For example, (<adj, mpqa&gt;, 1.0) means that according to the dictionary MPQA, adjectives contribute to the polarity value 3 http://nlp.stanford.edu/software/corenlp.shtml 180 http://nlp.stanford.edu/software/lex-parser.shtml PACLIC 29 Figure 1: Flowchart of overall process of our method for 1.0. Finally, features that occur less than three times are excluded. In feature normalization,"
Y15-1021,S15-2080,0,0.0300927,"sitive or negative. Later on, the task was extended to address more challenging and complex goals, such as the identification of the sentiment of the messages or the writer’s affective state in a more fine scale, with labels including anger, happiness or depression. Such extension could not avoid considering one of the most pervasive tools used in communication, namely figurative language. In fact, this expressive tool is not only very frequent in various kinds of texts, but it also strongly affects the sentiment expressed in the text, often completely reversing its polarity (Xu et al., 2015; Ghosh et al., 2015). Because figurative language is used in unpredictable ways in communication (i.e. either in crystallized forms or in creative ways) and it can involve several linguistic and extra-linguistic levels (i.e. from syntax to concepts and pragmatics), its identification and understanding is often difficult, even for human beings. If humans are able to rely on prosody (e.g. stress or intonation), kinesis (e.g. facial gestures), co-text (i.e. immediate textual environment) and context (i.e. wider environment), as well as cultural background, machines cannot access the same type of information. These d"
Y15-1021,pak-paroubek-2010-twitter,0,0.0475299,"the task on ironic and sarcastic tweets. 1 Introduction Whenever a message is encoded into linguistic form for being communicated – either in a spoken or written text1 – information revealing judgments, evaluations, attitudes and emotions is also encoded (Martin and White, 2005). This is true for both informal and formal texts, independently of how much attention the writer pays in cleaning such information out. This is also true for texts posted on social networks (i.e. Facebook, Twitter, etc.), where judgments, evaluations, attitudes and emotions constitute an important part of the message (Pak and Paroubek, 2010). Sentiment analysis (also known as opinion mining and subjectivity analysis) is a Natural Language Processing (NLP) task that focuses on identification of 1 In this paper we will mainly refer to written texts, but most of what is said is also applicable to spoken ones. such judgments, evaluations, attitudes and emotions. It can be compared to other classification tasks, as it consists in associating the analyzed texts with a label that represents the sentiment of the message or the affective state of the writer (Hart, 2013). In its earliest incarnations, sentiment analysis was limited to the"
Y15-1021,J01-4004,0,0.0657435,"le 2 identifies coherence in a tweet and uses it to evaluate the similarity between them. A as a feature. method proposed by Resnik (1995) is used There are several studies related to coherence to define the similarity between two synsets identification. A set of heuristic rules based on based on the information content of their lowgrammatical relations was proposed to identify coest super-ordinate (most specific common subherence in tweets (Tungthamthiti et al., 2014). A sumer). more complex method, based on machine learning, • The feature is activated when the similarity of was presented by Soon et al. (2001) to link coreone of synset pairs is greater than a threshold. ferring noun phrases both within and across senIt is set to 1.37 by our intuition. tences. However, such method would not be ap9. Number agreement feature – w1 and w2 agree in propriate for our scope, because it focuses specifinumber (i.e., they are both singular or plural) cally on coreference resolution, rather than identify10. Acronyms and abbreviation – A tweet contains ing the coherence relationship. Nevertheless, it proan acronym or abbreviation (i.e., “lol”, “ynwa”). vides some useful insights, which can be exploited 11. Emot"
Y15-1021,S15-2113,1,0.795892,"bel was either positive or negative. Later on, the task was extended to address more challenging and complex goals, such as the identification of the sentiment of the messages or the writer’s affective state in a more fine scale, with labels including anger, happiness or depression. Such extension could not avoid considering one of the most pervasive tools used in communication, namely figurative language. In fact, this expressive tool is not only very frequent in various kinds of texts, but it also strongly affects the sentiment expressed in the text, often completely reversing its polarity (Xu et al., 2015; Ghosh et al., 2015). Because figurative language is used in unpredictable ways in communication (i.e. either in crystallized forms or in creative ways) and it can involve several linguistic and extra-linguistic levels (i.e. from syntax to concepts and pragmatics), its identification and understanding is often difficult, even for human beings. If humans are able to rely on prosody (e.g. stress or intonation), kinesis (e.g. facial gestures), co-text (i.e. immediate textual environment) and context (i.e. wider environment), as well as cultural background, machines cannot access the same type of"
Y15-1021,E14-3007,0,\N,Missing
Y15-1021,W14-2608,0,\N,Missing
Y15-1021,D13-1066,0,\N,Missing
Y15-1021,Y14-1047,1,\N,Missing
Y15-1036,Y06-1024,1,0.810836,"Missing"
Y15-1036,Y96-1018,1,0.622182,"ating data in Section 2, and figure out the similarities as well as differences between 聲 and 音 on synaesthesia in Section 3, which will be followed by some explanations in Section 4. In the last Section, we will summarize our main findings and propose our future work. 2 2.1 Methodology: Corpus Selection and Data Annotation Corpus Selection and Measurement Criteria In order to make our study more tenable, we attempt to exhaust the data as possible as we can, and hence rely on the corpus for data collection. We select four widely-used huge corpora as our data sources, of which Sinica Corpus 2 (Chen et al., 1996) is the main corpus and other three corpora, including Chinese GigaWord 2 Corpus (Mainland, simplified)3 ; Chinese GigaWord 2 Corpus (Taiwan, traditional)4 ; and the journal corpus of BCC Corpus5 , act as the complement. The reason for giving the priority to Sinica Corpus is on the consideration that the corpus is a wellrecognized balanced and high-quality corpus with 2 Accessed at: http://app.sinica.edu.tw/kiwi/ mkiwi/ 3 Accessed at: https://the.sketchengine.co. uk/bonito/run.cgi/first_form?corpname= preloaded/cgw2_sc 4 Accessed at: https://the.sketchengine.co. uk/bonito/run.cgi/first_form?co"
Y15-1045,Y96-1018,1,0.708896,"eptual metaphor understanding and interpretation (e.g. Lakoff and Johnson, 1980, 1999; Johnson, 2008), it does not account for exactly what triggers the metaphorical use of a corporeal term, or what constrains the selection of a body part term to represent a comparatively abstract concept. In order to answer this question, this research integrates the theory of embodiment, a key concept in cognitive linguistics, with the theory of Generative Lexicon (Pustejovsky, 1991, 1995), of which we focus on the qualia structure, by analyzing the lexical items containing body part terms in Sinica Corpus (Chen et al., 1996). Embodiment tackles how and where meaning arises, but it falls short in explaining what triggers the selection of a body part to represent an abstract notion. Generative lexicon functions as a way to study the representations and relations of meanings, but it lacks the explanation for the source of meanings. The integration of both, which has not been found in previ396 29th Pacific Asia Conference on Language, Information and Computation pages 396 - 403 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Ren-Feng Duann and Chu-Ren Huang PACLIC 29 ous research, in the analysis of"
Y15-1045,J99-4009,0,0.940831,"analysis of the corporeal metaphors in corpus data allows us to account for the cognitive motivation of body metaphors and to represent these metaphors by the qualia structure. More importantly, the combination of the two theories enables us to find out the motivation underlying the choice of certain human body parts for metaphorical uses. 2 Theoretical Background, Research Questions and Hypotheses 2.1 Embodiment Embodiment, or the embodied theory of meaning by Johnson (2008), is proposed as a counterargument against mind-body dualism, a key concept of the Western philosophy and epistemology (Lakoff and Johnson, 1999). The Western tradition has regarded mind and body as distinct entities, which are independent of each other and cannot be integrated. The proposal of embodiment, arguing against the dualist view of knowledge in the West, claims that body and mind should be regarded as continuity (e.g. Johnson, 1987, 2008). Its central tenet is how people make sense of their experiences in the world lies in their interaction between their bodies and the environment. The meaning emerging from the corporeal experiences furthermore form the basis for people to understand abstract concepts. The human body and body"
Y15-1045,J91-4003,0,0.838447,"ically. While embodiment provides cognitive reasons which underlies meaning production (e.g. Yu, 2003, 2007) and serves as the foundation of conceptual metaphor understanding and interpretation (e.g. Lakoff and Johnson, 1980, 1999; Johnson, 2008), it does not account for exactly what triggers the metaphorical use of a corporeal term, or what constrains the selection of a body part term to represent a comparatively abstract concept. In order to answer this question, this research integrates the theory of embodiment, a key concept in cognitive linguistics, with the theory of Generative Lexicon (Pustejovsky, 1991, 1995), of which we focus on the qualia structure, by analyzing the lexical items containing body part terms in Sinica Corpus (Chen et al., 1996). Embodiment tackles how and where meaning arises, but it falls short in explaining what triggers the selection of a body part to represent an abstract notion. Generative lexicon functions as a way to study the representations and relations of meanings, but it lacks the explanation for the source of meanings. The integration of both, which has not been found in previ396 29th Pacific Asia Conference on Language, Information and Computation pages 396 -"
Y15-1045,W13-5405,0,0.0283289,"od” is thus treated as a metaphorical expression. Once we identify a metaphorically used lexical unit, we need to formulate how it behaves in the metaphor. We propose to incorporate the qualia structure, which provides more information for the metaphorically used word and helps us formulate metaphors, as elaborated below. Retrieving qualia roles In order to find out the constraints underlying the selection of a body part term in a metaphor, we incorporate the qualia structure, through which we retrieve the qualia role(s) of the body part(s) in the corpus data. We expand the method proposed by Song and Zhao (2013a, 2013b), as we focus on two levels: the qualia role of a body part term at the lexical and clausal levels. In brief, (1) We first examine whether there is more than one sense of the body part at issue. E.g. in the corpus data, two senses are found in 血 xie “blood”: Sense 1 refers to the liquid circulating naturally inside human body, and sense 2 to the liquid flowing inside/out of human body due to injury or effort making. (2) We spell out the qualia structure, i.e. the four PACLIC 29 roles, of the body part at issue according to the sense(s) found in step (1). For example, the qualia struct"
Y15-1045,Y01-1015,0,0.0394103,"Missing"
Y15-2035,Y96-1018,1,\N,Missing
Y16-2021,N09-1003,0,0.387296,"Missing"
Y16-2021,J10-4006,1,0.894609,"Missing"
Y16-2021,P14-1023,0,0.703018,"shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (Bullinaria and Levy, 2012), against which APSyn still obtains competitive performances. The results are also discussed in relation to the state-of-the-art DSMs, as reported in Hill et al. (2015). In such comparison, the best settings of our models outperform the word embeddings in almost all datasets. A pilot study was also carried out to investigate whether APSyn is scalable. Results prove its high performance also when calculated on large corpora, such as those used by Baroni et al. (2014). On top of the performance, APSyn seems not to be subject to some of the biases that affect Vector Cosine. Finally, considering the debate about the ability of DSMs to calculate genuine similarity as opposed to word relatedness (Turney, 2001; Agirre et al., 2009; Hill et al., 2015), we test the ability of the models to quantify genuine semantic similarity. 2 2.1 Background DSMs, Measures of Association and Dimensionality Reduction Count-based DSMs are built in an unsupervised way. Starting from large preprocessed corpora, a matrix M(m×n) is built, in which each row is a vector representing a"
Y16-2021,J90-1003,0,0.702972,"of the Distributional Hypothesis (DH), which claims that words occurring in the same contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Sahlgren, 2008). Such hypothesis provides the theoretical ground for Distributional Semantic Models (DSMs), which represent word meaning by means of high-dimensional vectors encoding corpus-extracted co-occurrences between targets and their linguistic contexts (Turney and Pantel, 2010). Traditional DSMs initialize vectors with cooccurrence frequencies. Statistical measures, such as Positive Pointwise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These ﬁrst-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word"
Y16-2021,W16-2506,0,0.162529,"ting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models learn meaning representations through neural network training: the vectors dimensions are set to maximize the probability for the contexts that typically occur with the target word. Vector Cosine is generally adopted by both types of models as a similarity measure. However, this metric has been found to suffer from several problems (Li and Han, 2013; Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are actually shared by the vectors. Finally, Cosine is affected by the hubness effect (Dinu et al., 2014; Schn30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 229 abel et al., 2015), i.e. the fact that words with high frequency tend to be universal neighbours. Even though other measures have been proposed in the literature (Deza and Deza, 2009), Vector Cosine is still by far the most popular one (Turney and Pantel"
Y16-2021,J15-4004,0,0.0945737,", 2010). However, in a recent paper of Santus et al. (2016b), the authors have claimed that Vector Cosine is outperformed by APSyn (Average Precision for Synonymy), a metric based on the extent of the intersection between the most salient contexts of two target words. The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets. In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation - namely WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). For comparison, Vector Cosine is also calculated on several countbased DSMs. We implement a total of twenty-eight models with different parameters settings, each of which differs according to corpus size, context window width, weighting scheme and SVD application. The new metric is shown to outperform Vector Cosine in most settings, except when the latter metric is applied on a PPMI-SVD reduced matrix (Bullinaria and Levy, 2012), against which APSyn still obtains competitive performances. The results are also discussed in relation to the state-of-the-art DSMs, as reported in Hill et al. (201"
Y16-2021,P12-1092,0,0.686827,"wise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These ﬁrst-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models learn meaning representations through neural network training: the vectors dimensions are set to maximize the probability for the contexts that typically occur with the target word. Vector Cosine is generally adopted by both types of models as a similarity measure. However, this metric has been found to suffer from several problems (Li and Han, 2013; Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are actually shared by the"
Y16-2021,W14-1503,0,0.0315309,"0.335 0.519 0.525 0.564 0.546 0.562 0.553 WSim (REL) 2 3 0.167 0.175 0.251 0.269 0.378 0.396 0.051 0.084 0.141 0.151 0.325 0.323 0.259 0.241 0.261 0.284 0.233 0.27 0.337 0.397 0.361 0.382 0.287 0.309 Table 3: Spearman correlation scores for our eight models trained on RCV1, in the two subsets of WordSim353. might depend on the different type of similarity encoded in SimLex-999 (i.e. genuine similarity). On top of it, despite Hill et al. (2015)’s claim that no evidence supports the hypothesis that smaller context windows improve the ability of models to capture similarity (Agirre et al., 2009; Kiela and Clark, 2014), we need to mention that window 5 was abandoned because of its low performance. With reference to the hubness effect, we have conducted a pilot study inspired to the one carried out by Schnabel et al. (2015), using the words of the SimLex-999 dataset as query words and collecting for each of them the top 1000 nearest neighbors. Given all the neighbors at rank r, we have checked their rank in the frequency list extracted from our corpora. Figure 1 shows the relation between the rank in the nearest neighbor list and the rank in the frequency list. It can be easily noticed that the highest ranke"
Y16-2021,Q15-1016,0,0.382486,"s that words occurring in the same contexts tend to have similar meanings (Harris, 1954; Firth, 1957; Sahlgren, 2008). Such hypothesis provides the theoretical ground for Distributional Semantic Models (DSMs), which represent word meaning by means of high-dimensional vectors encoding corpus-extracted co-occurrences between targets and their linguistic contexts (Turney and Pantel, 2010). Traditional DSMs initialize vectors with cooccurrence frequencies. Statistical measures, such as Positive Pointwise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These ﬁrst-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models lear"
Y16-2021,L16-1723,1,0.788938,"higher values and the inability of considering how many features are actually shared by the vectors. Finally, Cosine is affected by the hubness effect (Dinu et al., 2014; Schn30th Pacific Asia Conference on Language, Information and Computation (PACLIC 30) Seoul, Republic of Korea, October 28-30, 2016 229 abel et al., 2015), i.e. the fact that words with high frequency tend to be universal neighbours. Even though other measures have been proposed in the literature (Deza and Deza, 2009), Vector Cosine is still by far the most popular one (Turney and Pantel, 2010). However, in a recent paper of Santus et al. (2016b), the authors have claimed that Vector Cosine is outperformed by APSyn (Average Precision for Synonymy), a metric based on the extent of the intersection between the most salient contexts of two target words. The measure, tested on a window-based DSM, outperformed Vector Cosine on the ESL and on the TOEFL datasets. In the present work, we perform a systematic evaluation of APSyn, testing it on the most popular test sets for similarity estimation - namely WordSim-353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). For comparison, Vector Cosine is also"
Y16-2021,D15-1036,0,0.145843,"andauer and Dumais, 1997; Jarmasz and Szpakowicz, 2004; Mikolov et al., 2013; Levy et al., 2015), looks at the normalized correlation between the dimensions of two word vectors, w1 and w2 and returns a score between -1 and 1. It is described by the following equation: n × f2i i=1 f1 i n i=1 f1 i × i=1 f2 i cos(w1 , w2 ) = n (3) where fi x is the i-th dimension in the vector x. Despite its extensive usage, Vector Cosine has been recently criticized for its hyper sensibility to features with high values and for the inability of identifying the actual feature intersection (Li and Han, 2013; Schnabel et al., 2015). Recalling an example by Li and Han (2013), the Vector Cosine for the toy-vectors a = [1, 2, 0] and b = [0, 1, 0] (i.e. 0.8944) is unexpectedly higher than the one for a and c = [2, 1, 0] (i.e. 0.8000), and even higher than the one for the toy-vectors a and d = [1, 2, 1] (i.e. 0.6325), which instead share a larger feature intersection. Since the Vector Cosine is a distance measure, it is also subject to the hubness problem, which was shown by Radovanovic et al. (2010) to be an inherent property of data distributions in highdimensional vector space. The problem consists in the fact that vector"
Y16-2021,P10-1040,0,0.0618988,"uch as Positive Pointwise Mutual Information (PPMI) or its variants (Church and Hanks, 1990; Bullinaria and Levy, 2012; Levy et al., 2015), have been adopted to normalize these values. Also, these models have exploited the power of dimensionality reduction techniques, such as Singular Value Decomposition (SVD; Landauer and Dumais, 1997) and Random Indexing (Sahlgren, 2005). These ﬁrst-generation models are currently referred to as count-based, as distinguished from the contextpredicting ones, which have been recently proposed in the literature (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010; Huang et al., 2012; Mikolov et al., 2013). More commonly known as word embeddings, these secondgeneration models learn meaning representations through neural network training: the vectors dimensions are set to maximize the probability for the contexts that typically occur with the target word. Vector Cosine is generally adopted by both types of models as a similarity measure. However, this metric has been found to suffer from several problems (Li and Han, 2013; Faruqui et al., 2016), such as a bias towards features with higher values and the inability of considering how many features are act"
Y16-3018,W14-5810,1,0.83276,"Missing"
Y16-3018,W11-0807,0,0.0505477,"Missing"
Y16-3024,Y96-1018,1,0.274543,"or the constraints which motivate the selection of a corporeal term to represent another concept. Incorporating the theories of embodiment and of generative lexicon (Pustejovsky 1991, 1995), Duann and Huang (2015) proposes a method to uncover what constrains the use of a body part in the stead of a comparatively abstract notion. They focus on the qualia structure of the corporeal terms as the source concept and testify their approach with authentic corpus data. Examining the behavior of four atypical body parts, Ո xue ‘blood’, Ժ rou ‘flesh’, ମ gu ‘bone’ and ે mai ‘meridian’ in Sinica Corpus (Chen et al. 1996), they contend that the visibility of these body parts and the telic role from the qualia structure constrain the selection. In this current research, we explore the metaphorical/metonymical uses of Ո xue ‘blood’ and ମ ‘bone’, the two corporeal terms with relatively high visibilities compared with Ժ rou ‘flesh’ and ે mai ‘meridian’ (Duann and Huang 2015) in the Chinese Gigaword Version 2 (Gigaword2, Huang 2009), and finds out: (1) Regarding the use of Ո xue ‘blood’, the agentive role predominates in both Taiwan and China, which differs from the argument in Duann and Huang (2015). (2) Concernin"
Y16-3024,Y15-1045,1,0.406208,"aword v. 2.0 Chu-Ren Huang Department of Chinese and Bilingual Studies The Hong Kong Polytechnic Universit churen.huang@polyu.edu.hk Ren-feng Duann Department of English Wenzao Ursuline University of Languages d94142001@ntu.edu.tw Abstract This article, examining the qualia roles retrieved from the metaphorically/metonymically used body part terms in news texts, addresses the similarities and differences of such uses in Taiwan and China. Analyzing the behavior of Ո xue ‘blood’ and ମ ‘bone’, two corporeal terms with relatively high visibilities compared with Ժ rou ‘flesh’ and ે mai ‘meridian’ (Duann and Huang 2015) in the Chinese Gigaword Version 2 (Huang 2009), this research have the following findings: (1) For the use of Ո xue ‘blood’, the agentive role predominates in both Taiwan and China, which is not in line with the argument in Duann and Huang (2015). (2) Regarding the use of ମ gu ‘bone’, the telic role predominates. However, China uses it in personification much more often than Taiwan does. (3) The unique dimension of a place triggers the use exclusive to the place. 1 Introduction Embodiment, referring to ‘understanding the role of an agent’s own body in its everyday, situated cognition’ (Gibbs"
Y16-3024,J99-4009,0,0.174213,"Missing"
Y16-3024,J91-4003,0,0.287036,"embodied creatures in interaction with their changing environments’ (Johnson 2008: 11), has been drawing scholars’ attention for more than three decades. While providing cognitive accounts for meaning generation and functioning as the foundation of conceptual metaphor understanding and interpretation (e.g. Yu 2003, 2007; Lakoff and Johnson 1980, 1999; Johnson 2006), embodiment does not address what triggers conceptual metaphors, or the constraints which motivate the selection of a corporeal term to represent another concept. Incorporating the theories of embodiment and of generative lexicon (Pustejovsky 1991, 1995), Duann and Huang (2015) proposes a method to uncover what constrains the use of a body part in the stead of a comparatively abstract notion. They focus on the qualia structure of the corporeal terms as the source concept and testify their approach with authentic corpus data. Examining the behavior of four atypical body parts, Ո xue ‘blood’, Ժ rou ‘flesh’, ମ gu ‘bone’ and ે mai ‘meridian’ in Sinica Corpus (Chen et al. 1996), they contend that the visibility of these body parts and the telic role from the qualia structure constrain the selection. In this current research, we explore the"
Y18-1091,W14-5818,1,0.798276,"acters on a fivepoint Likert scale (Likert, 1932). 3.4 Procedures Prior to the experiment, ethical approval was obtained from the Human Subjects Ethics Sub-committee of the Hong Kong Polytechnic University (Ref #: HSEARS20180406002). All the participants gave their consent to attend the experiment. To facilitate the data collection process, we created our on-line tests via the SurveyPlanet platform (SurveyPlanet, 2018) and invited informants from Hong Kong, Mainland and Taiwan to participate in the experiment remotely. After the data collection, we employed several criteria to clean the data (Wang et al., 2014, 2017, 2019). The responses of an informant were excluded if: 1) the time spent on the test was less than three minutes or longer than fifteen minutes; 2) the answer to any question from Part 2 was incorrect; 3) no more than two points on the rating scale were used; and 4) the informant was not born and raised in the tested area. According to Harpe (2015), aggregated rating scale data such as the data we collected can be treated as continuous data; he further advocates that more advanced statistical models would be more powerful than a simple group comparison via t-tests or analysis of varian"
Y18-2001,L16-1360,1,0.813449,"Missing"
Y18-2004,D17-1048,1,0.792398,"are represented by a dense vector learnt from embedding models such as word2vec or Doc2Vec. One of the ”linguistic” knowledge that can potentially improve the accuracy of sentiment analysis are sentiment lexicons or more complex emotion lexicons. They are integrated into the machine learning pipeline, e.g. (S.K.Rastogi et al., 2014). Sentiment lexicons are important resources for sentiment analysis. These lexicons consist of predetermined list of words assigned to sentiment labels or values, which is baseline for many machine learning based methods(Liu and Zhang, 2012; Tabak and Evrim, 2016; Long et al., 2017). Depending on sentiment models, there are two mainstream labeling schemas. The first schema is representing affective meanings of words by discrete sentiment labels, such as as positive, negative, etc (Ekman et al., 1983). The second schema is to represent affective meanings by means of more comprehensive multi-dimensional representation models, like the valence-arousal dominance model (VAD) (Russell, 1980) and the evaluationpotency-activity model (EPA) (Heise, 1965). Sentiment lexicons are heavily investigated in English (Gilbert, 2014; Li et al., 2017) and Chinese (Wang and Ku, 2016). Howev"
Y18-2004,N13-1090,0,0.00452476,"nlabeled documentsreviews (the reviews are not labeled). After preprocessing the reviews (cleaning, tokenization), we trained the model on the openrice data using Doc2Vec implementation. We choose the vector size of 100, so that each review is encoded into the array of this size. After training the model, we substituted the reviews in the corpus with the respective vector according to the model and thus the text data became numerical and could had served as input for standard classifiers. The side-effect of the trained model is its capability to represent relations between words or sentences (Mikolov et al., 2013). We tested on the following 3 The scripts with a small sample of data are available at: https://github.com/polyu-llt/openrice_ annotations Figure 2: Lexical relations obtained from the model using the function most similar 3.2 Preliminary Experiments In our preliminary experiments, we trained three classifiers using sklearn python library: • Logistic regression (logreg) • Support Vector Machines (SVM) • k-Nearest Neighbours We conducted the following experiments; results are summarized in Table 3: • Multi-class classification. For this, we used all the reviews that were ranked on the scale fr"
Y18-2004,L16-1428,0,0.0271889,"2016; Long et al., 2017). Depending on sentiment models, there are two mainstream labeling schemas. The first schema is representing affective meanings of words by discrete sentiment labels, such as as positive, negative, etc (Ekman et al., 1983). The second schema is to represent affective meanings by means of more comprehensive multi-dimensional representation models, like the valence-arousal dominance model (VAD) (Russell, 1980) and the evaluationpotency-activity model (EPA) (Heise, 1965). Sentiment lexicons are heavily investigated in English (Gilbert, 2014; Li et al., 2017) and Chinese (Wang and Ku, 2016). However, building sentiment lexicon for low-resources languages or dialects is not an easy task. That is why we decided to collect and investigate the text in Cantonese. Cantonese is a variety of Chinese spoken in the city of Guangzhou (historically known as Canton) and its surrounding area in southeastern China, including HK SAR and Macao SAR 1 . Compared to the studies on Man1 https://en.wikipedia.org/wiki/Cantonese 882 32nd Pacific Asia Conference on Language, Information and Computation The 25th Joint Workshop on Linguistics and Language Processing Hong Kong, 1-3 December 2018 Copyright"
Y96-1001,O96-1009,1,\N,Missing
Y96-1018,C92-1019,1,0.27843,"Missing"
Y96-1018,A88-1019,0,0.028653,"Missing"
Y96-1018,C96-2184,1,0.915665,"to neighboring words to form a segmentation unit when possible. (b) A string of characters that has a high frequency in the language or high cooccurrence frequency among the components should be treated as a segmentation unit when possible. (c) A string separated by overt segmentation markers should be segmented. (d) A string with complex internal structures should be segmented when possible. 170 Lastly, the lexicon contains a standard list of words as well as productive morphological suffixes, and obligatory segment markers. For more detail on the word segmentation standard, please refer to Huang et al. (1996). 3.2 Part-of-speech Tagging The possible part-of-speech (abbr:pos) of each word was given after segmentation process. To resolve the ambiguities, a two-stage automatic tagging process was designed to disambiguate multi-category words. In the first stage, a small portion of the corpus was resolved by a hybrid method which combines rule-based and relaxation methods to select the most plausible pos tag for each multi-category word (Liu et al. 95). This initial corpus was post-edited manually and then became training data for the second stage statistical tagging model adapted from (Church 88). Th"
Y96-1018,P94-1010,0,0.025655,"Missing"
Y96-1018,J90-1003,0,\N,Missing
Y96-1018,J90-3007,0,\N,Missing
Y99-1005,J93-2005,0,0.12393,"Missing"
Y99-1005,O98-3004,1,0.831966,"ocus on the inchoative stage or the homogeneous stage of the event. In addition, since VV compounding has the function of type-lifting an event to a referential term, or to refer to its generic properties, it is natural to predict that VV compounding is a predominant source for the verbs of indicating homogeneity. 1. Introduction Many recent linguistic studies explored how lexical meaning predicts syntactic regularities (Levin 1993, Pustejovsky 1995). One important approach is to study the contrasts in near synonym pairs to identify the minimal semantic attributes that motivate the contrasts (Tsai et al 1998, Liu et al 1997 & 1998). In this current study, we extend the range of the study to a semantic field, which contains more than one synonym pairs. Thus we can attest to the primary status of the proposed semantic attributes by showing that the generalization can be extended to the other synonym pairs in the same semantic field. Tsai et al (1998) discussed the contrast between the synonym pair KUAILE and GAOXING AR, and based on their findings we re-examine the contrast in a broader range, i.e. the verbs of emotion. We have four results from this study: 1) we find that the contrast is not speci"
