2008.iwslt-evaluation.7,P06-1077,1,0.887612,"el. This year, we participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanni"
2008.iwslt-evaluation.7,P07-1089,1,0.885881,"el. This year, we participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanni"
2008.iwslt-evaluation.7,P08-1023,1,0.802664,"e participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanning from position"
2008.iwslt-evaluation.7,D08-1022,1,0.818999,"e participated in three tracks (two translation directions): 1. BTEC task, Chinese-English direction; 2. Challenge task, Chinese-English direction; 3. Challenge task, English-Chinese direction. This paper is structured as follows. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanning from position"
2008.iwslt-evaluation.7,P89-1018,0,0.0164582,"ws. Section 2 describes the four SMT systems and the combiner; Section 3 gives the experimental results, and Section 4 is the conclusion. - 52 - Figure 1: A pair of linked source forest and target string. The solid lines denote hyperedges and the dashed lines denote word alignments. 2. SMT Systems 2.1. Silenus Deriving from the tree-to-string system Lynx [1, 2], Silenus [3, 4] uses packed forests instead of 1-best parse trees in both training and decoding. A packed parse forest is a compact representation of all derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. A forest can be formally defined as a tuple hV, E, v¯, Ri, where V is a finite set of nodes, E is a finite set of hyperedges, v¯ ∈ V is a distinguished node that denotes the goal item in parsing, R is the set of weights. For a given sentence w1:l = w1 . . . wl , each node v ∈ V is in the form if Xi,j , which denotes the recognition of non-terminal X spanning from position i through j (that is, wi+1 . . . wj ). Each hyperedge e ∈ E is a triple e = hT (e), h(e), f (e)i, where h(e) ∈ V is its head, T (e) ∈ V ∗ is a vector of tail nodes, and f (e) is a weight function from R|T (e) |to R. Figure"
2008.iwslt-evaluation.7,N04-1035,0,0.105473,"the outside probability of its root, the inside probabilities of its leaf nodes, and the probabilities of hyperedges involved in the fragment: Y Y αβ(t) = α(root(t)) × P (e) × β(v) (2) (1) where plm (s) is the language model score, |d |is the number of rules in a derivation, and |s |is the number of target words produced. The derivation probability P r(d|T ) is the product of probabilities of translation rules involved in d: Y P r(d|T ) = P r(r) (5) d∈D r∈d Table 1 gives a derivation for the example forest-string pair. To learn tree-to-string rules from annotated training data, we follow GHKM [6] to first identify minimal rules and then obtain composed rules. Like in tree-based extraction, we extract rules from a packed forest F in two steps: frontier set computation (where to cut) and fragmentation (how to cut). It turns out that the exact formulation developed for frontier set in tree-based case can be applied to a forest without change. The fragmentation step, however, becomes much more complicated since we now face a choice of multiple hyperedges at each node. We develop a breadth-first search algorithm for extracting tree-to-string rules from packed forests. The basic idea is to"
2008.iwslt-evaluation.7,P02-1038,0,0.152821,"ves(t) where α(·) and β(·) are the outside and inside probabilities of nodes, root(·) returns the root of a tree fragment and leaves(·) returns the leaf nodes of a tree fragment. Now, the fractional count of a rule r is simply lines denote word alignments. Each hyperedge is associated with a probability, which we omit in Figure 1 for clarity. In a forest, a node usually has multiple incoming hyperedges. For example, the source node IP0,6 has two incoming hyperedges: c(r) = αβ(lhs(r)) αβ(¯ v) (3) where v¯ denotes the root of the forest. We extend the simple model in Eq. 1 to a log-linear model [7]: dˆ = argmax P r(d|T )λ1 × plm (s)λ2 × eλ3 |d |× eλ4 |s| e1 = h(NP-B0,1 , VP1,6 ), IP0,6 , 0.6i e2 = h(NP0,3 , VP-B3,6 ), IP0,6 , 0.4i (4) d∈D Silenus searches for the best derivation (a sequence of translation rules) dˆ that converts a source tree T in the packed forest into a target-language string s: dˆ = argmax P r(d|T ) should be penalized accordingly and should have fractional counts instead of unit count. We penalize a rule r by the posterior probability of the corresponding tree fragment t = lhs(r), which can be computed as the product of the outside probability of its root, the insid"
2008.iwslt-evaluation.7,J07-2003,0,0.096572,"ach tree has its own probability (that is, product of hyperedge probabilities). As a result, a rule extracted from non 1-best parse - 53 - where each P r(r) can be decomposed into the product of six probabilities: P r(r) = p(r|lhs(r))λ5 × p(r|rhs(r))λ6 ×p(r|root(lhs(r)))λ7 ×plex (lhs(r)|rhs(r))λ8 ×plex (rhs(r)|lhs(r))λ9 ×p(T )λ10 (6) where the first three terms are conditional probabilities based on fractional counts, plex (·) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct tr"
2008.iwslt-evaluation.7,W05-1506,0,0.0292297,"))λ6 ×p(r|root(lhs(r)))λ7 ×plex (lhs(r)|rhs(r))λ8 ×plex (rhs(r)|lhs(r))λ9 ×p(T )λ10 (6) where the first three terms are conditional probabilities based on fractional counts, plex (·) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct translation: [] A → (A1 , A2 ) b3 b4 (7) target hi A → (A1 , A2 ) A → (x, y) (8) (9) b2 b The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and t"
2008.iwslt-evaluation.7,P06-1066,1,0.895245,"ties based on fractional counts, plex (·) denotes lexical weighting, and p(T ) denotes the probability of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct translation: [] A → (A1 , A2 ) b3 b4 (7) target hi A → (A1 , A2 ) A → (x, y) (8) (9) b2 b The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and target contiguous sequences of words. The last rule translates a source phrase x into a target phrase y and generate a block A. In the followin"
2008.iwslt-evaluation.7,J97-3002,0,0.019739,"of the matched source tree T . To search for 1-best derivation, the decoder employs the cube pruning method [8] that approximately intersects the translation forest with language model. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the best-first expansion idea from the Algorithm 2 of Liang Huang [9] to speed up the computation. K-best derivations can also be easily obtained by applying Algorithm 3 of Liang Huang [9]. 2.2. Bruin Bruin [10] is a formally syntax-based system that implements a maximum entropy based reordering model on BTG [11] Proceedings of IWSLT 2008, Hawaii - U.S.A. rules. Bruin employs the following three BTG rules to direct translation: [] A → (A1 , A2 ) b3 b4 (7) target hi A → (A1 , A2 ) A → (x, y) (8) (9) b2 b The first two rules are used to merge two neighboring blocks into one larger block either in a monotonic or an inverted order. A block is a pair of source and target contiguous sequences of words. The last rule translates a source phrase x into a target phrase y and generate a block A. In the following, we will define the model by separating different features (including the language model) from the ru"
2008.iwslt-evaluation.7,P08-2041,1,0.861149,"asures how precisely a feature f predicts a class c: IGR(f, c) = E(c) − E(c|f ) Ef (13) where E(·) is an entropy and E(·|·) is a conditional entropy. Surprisingly, the IGR for boundary words (0.2637) is very close to that of blocks (0.2655), suggesting that boundary words do provide sufficient information for predicting reordering. Based on CKY algorithm, the decoder finds the best derivation that produces the input sentence and its translation. To speed up the computation, Bruin also makes use of cube pruning. The lazy Algorithm 3 [9] are used for n-best list generation. 2.3. Mencius Mencius [12] is a phrase-based system that is very similar to Moses [13]. The major difference is that we introduce Proceedings of IWSLT 2008, Hawaii - U.S.A. similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. J Given two source phrases fˆ1J and fˆ0 1 , their matching similarity is given by J SIM(fˆ1J , fˆ0 1 ) = PJ j=1 δ(fj , fj0 ) J 2.5. System Combination (14) where  0 δ(f, f ) = 1 if f = f 0 0 otherwise than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the num"
2008.iwslt-evaluation.7,P07-2045,0,0.00671916,"c) = E(c) − E(c|f ) Ef (13) where E(·) is an entropy and E(·|·) is a conditional entropy. Surprisingly, the IGR for boundary words (0.2637) is very close to that of blocks (0.2655), suggesting that boundary words do provide sufficient information for predicting reordering. Based on CKY algorithm, the decoder finds the best derivation that produces the input sentence and its translation. To speed up the computation, Bruin also makes use of cube pruning. The lazy Algorithm 3 [9] are used for n-best list generation. 2.3. Mencius Mencius [12] is a phrase-based system that is very similar to Moses [13]. The major difference is that we introduce Proceedings of IWSLT 2008, Hawaii - U.S.A. similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. J Given two source phrases fˆ1J and fˆ0 1 , their matching similarity is given by J SIM(fˆ1J , fˆ0 1 ) = PJ j=1 δ(fj , fj0 ) J 2.5. System Combination (14) where  0 δ(f, f ) = 1 if f = f 0 0 otherwise than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the number of items taken from the heap. (15) We combine the output"
2008.iwslt-evaluation.7,D07-1105,0,0.0588403,". similarity-based partial matching for bilingual phrases to alleviate data sparseness problem. J Given two source phrases fˆ1J and fˆ0 1 , their matching similarity is given by J SIM(fˆ1J , fˆ0 1 ) = PJ j=1 δ(fj , fj0 ) J 2.5. System Combination (14) where  0 δ(f, f ) = 1 if f = f 0 0 otherwise than . We find that large number of items will often be enumerated under this condition in our experiments. To tackle this problem, we further limit the number of items taken from the heap. (15) We combine the outputs of single SMT systems at sentence level, similarly to the work by Macherey and Och [14]. Global linear models are used as a framework for reranking a merged n-best list: yˆ = argmax f (x, y) · W (17) y∈GEN(x) Note that we only consider two source phrases that have the same length. To make partially matching more reliable, we further restrict that they share with the same parts-ofspeech sequence. Our hope is that similar bilingual phrases can be used to create translation templates if one source phrase cannot find translations in the phrase table. For example, suppose that we cannot find translations for a source phrase “yu zuotian dida taiguo” in a phrase table, in which we find"
2008.iwslt-evaluation.7,P03-1021,0,0.031464,"se table, in which we find a similar source phrase “yu zuowan dida bulage” with its translation “arrived in Prague last evening”. According to the alignment information, we obtain a translation template: where x is a source sentence, y is a translation, f (x, y) is a feature vector, W is a weight vector, and GEN(x) is the set of possible candidate translations. There types of features are used: (1) relative BLEU scores against 1-best translations from other candidates, (2) language model scores, and (3) length of the translation. The feature weights are tuned using minimum-error-rate training [15]. In this year’s evaluation, each single SMT system generated 200-best list translations, which were merged and served as the input to the combiner. hyu X1 dida X2 , arrived in X2 X1 i 3. Experimental Results Then, the unmatched source substrings “zuotian” and “taiguo” can be translated into “yesterday” and “Thailand”, respectively. As a result, the translation for “yu zuotian dida taiguo” is “arrived in Thailand yesterday”. Given a source sentence, the decoder firstly search for all possible translation options from the phrase table by exact matching. For source phrases which have no translat"
2008.iwslt-evaluation.7,I05-1007,1,0.800287,"ran GIZA++ and used the “growdiagfinal” heuristic to get many-to-many word alignments. We observe that in a sentence some phrases are more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, “hsi” and “h/si”, to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. 3.3. Results Table 2 presents the BLEU scores (case-sensitive, with punctuations) of our"
2008.iwslt-evaluation.7,P05-1022,0,0.131775,"heuristic to get many-to-many word alignments. We observe that in a sentence some phrases are more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, “hsi” and “h/si”, to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. 3.3. Results Table 2 presents the BLEU scores (case-sensitive, with punctuations) of our five systems achieved on the IWSLT 2007 C"
2008.iwslt-evaluation.7,P08-1067,0,0.0287062,"more likely to appear at the beginning, while other phrases are more likely to be located at the end. Inspired by the literature in language modeling, we mark the beginning and ending of word aligned sentences with two tags, “hsi” and “h/si”, to capture such reordering information. The sentences to be translated will also be annotated with the two tags, which will be removed after decoding. To get packed forests for Silenus, we used the Chinese parser modified [17] by Haitao Mi and the English parser [18] modified by Liang Huang to produce entire parse forests. Then, we ran the Python scripts [19] provided by Liang Huang to output packed forests. To prune the packed forests, Huang [19] uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyperedge away from the globally best derivation. A hyperedge will be pruned if the difference is greater than a threshold. Nodes with all incoming hyperedges pruned are also pruned. 3.3. Results Table 2 presents the BLEU scores (case-sensitive, with punctuations) of our five systems achieved on the IWSLT 2007 Chinese-English development set. Prior to the evaluation, we used the development sets from 200"
2009.iwslt-evaluation.8,P06-1121,0,0.0208436,"ligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge task, English-Chinese direction. For each task of IWSLT 2009, the final submition is one of the four single systems who achieved a maximum BLEU score on development set. The four different systems are listed below: 1. Silenus, a linguistically syntax-based system that converts source-forest into target-string with tree-to-string rules acquired from packed forests; - 55 - (1) frontier set computation (where to cut), and (2) fragmentation (how to cut). Basically, we compute the frontier set according to GHKM [6] algorithm. We highlight the nodes in frontier set by gray shades in Figure 1(a). 1 http://www.statmt.org./moses/ Proceedings of IWSLT 2009, Tokyo - Japan P (lex(lhs(r)|rhs(r))) (4) P (lex(rhs(r)|lhs(r))) (5) P (f (r|root(lhs(r)))) =P f (r) (6) 0 root(lhs(r 0 ))=root(lhs(r)) f (r ) When computing the lexical translation probabilities described in [7], we only take the terminals into account. If there are no terminals, we set the feature value to 1. At the decoding time, we first parse the input sentences into forests. and then we convert the parse forest into a translation forest(Figure 1(b))"
2009.iwslt-evaluation.8,N03-1017,0,0.00351692,"that converts source-forest into target-string with tree-to-string rules acquired from packed forests; - 55 - (1) frontier set computation (where to cut), and (2) fragmentation (how to cut). Basically, we compute the frontier set according to GHKM [6] algorithm. We highlight the nodes in frontier set by gray shades in Figure 1(a). 1 http://www.statmt.org./moses/ Proceedings of IWSLT 2009, Tokyo - Japan P (lex(lhs(r)|rhs(r))) (4) P (lex(rhs(r)|lhs(r))) (5) P (f (r|root(lhs(r)))) =P f (r) (6) 0 root(lhs(r 0 ))=root(lhs(r)) f (r ) When computing the lexical translation probabilities described in [7], we only take the terminals into account. If there are no terminals, we set the feature value to 1. At the decoding time, we first parse the input sentences into forests. and then we convert the parse forest into a translation forest(Figure 1(b)) by pattern-matching. Finally, Silenus searches for the best derivation on the translation forest and outputs the target string. Beside the features we computed in rule extraction procedure, the additional features used in decoding step are listed here: • The number of rules in the derivation; Figure 1: Forest-based Rule Extraction and Translation • T"
2009.iwslt-evaluation.8,P08-1023,1,0.889278,"ranslation (IWSLT) 2009. For this year’s evaluation, our group participated in three tasks: 2. Bruin, a formally syntax-based system that implements a maximum entropy based reordering model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English d"
2009.iwslt-evaluation.8,D08-1022,1,0.876308,"ranslation (IWSLT) 2009. For this year’s evaluation, our group participated in three tasks: 2. Bruin, a formally syntax-based system that implements a maximum entropy based reordering model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English d"
2009.iwslt-evaluation.8,P06-1077,1,0.841838,"g model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English direction; d∈D 2. Challenge task, Chinese-English direction; (1) We extract rules from word-aligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge ta"
2009.iwslt-evaluation.8,P07-1089,1,0.882882,"g model on BTG rules; 3. Chiero, a formally syntax-based system that employs hierarchical phrases; 4. Moses, a phrase-based open source system 1 . This paper is organized as follows: Section 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English direction; d∈D 2. Challenge task, Chinese-English direction; (1) We extract rules from word-aligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge ta"
2009.iwslt-evaluation.8,P89-1018,0,0.0587337,"ection 2 gives an overview of our four SMT systems, Section 3 describes data preparation. In Section 4, we will report the experiments and results. Finally, Section 5 gives conclusions. 2. Single Systems Overview 2.1. Silenus Silenus [1, 2] is a linguistically syntax-based SMT system, which employs packed forests in both training and decoding rather than single-best trees used in conventional tree-tostring model [3, 4]. Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar [5]. Silenus searches for the best derivation (a sequence of treeto-string rules) d∗ that converts a source tree T in the forest into a target string s among all possible derivations D:: d∗ = arg max P (d|T ) 1. BTEC task, Chinese-English direction; d∈D 2. Challenge task, Chinese-English direction; (1) We extract rules from word-aligned bilingual corpus with source forests F (Figure 1 (a)) in two steps: 3. Challenge task, English-Chinese direction. For each task of IWSLT 2009, the final submition is one of the four single systems who achieved a maximum BLEU score on development set. The four diff"
2009.iwslt-evaluation.8,W05-1506,0,0.153435,"d-side of r, while the root(lhs(r) denotes the root node of the tree-fragment lhs(r). f (r) P (f (r|lhs(r)) = P lhs(r 0 )=lhs(r) P (f (r|rhs(r))) = P f (r0 ) f (r) rhs(r 0 )=rhs(r) f (r0 ) (2) (3) - 56 - • The language model score for the target translation; • The source side parsing probability of the tree traversed by the deviation. The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and kbest search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing [8]. For 1-best search, we use the cube pruning technique [9, 10] which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the bestfirst expansion idea from the Algorithm 2 of [8] to speed up the computation. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of [8] that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM"
2009.iwslt-evaluation.8,J07-2003,0,0.10516,"f the tree-fragment lhs(r). f (r) P (f (r|lhs(r)) = P lhs(r 0 )=lhs(r) P (f (r|rhs(r))) = P f (r0 ) f (r) rhs(r 0 )=rhs(r) f (r0 ) (2) (3) - 56 - • The language model score for the target translation; • The source side parsing probability of the tree traversed by the deviation. The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and kbest search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing [8]. For 1-best search, we use the cube pruning technique [9, 10] which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the bestfirst expansion idea from the Algorithm 2 of [8] to speed up the computation. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of [8] that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation fo"
2009.iwslt-evaluation.8,P07-1019,0,0.0142273,"f the tree-fragment lhs(r). f (r) P (f (r|lhs(r)) = P lhs(r 0 )=lhs(r) P (f (r|rhs(r))) = P f (r0 ) f (r) rhs(r 0 )=rhs(r) f (r0 ) (2) (3) - 56 - • The language model score for the target translation; • The source side parsing probability of the tree traversed by the deviation. The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and kbest search with LM to be used in minimum error rate training. Both tasks can be done efficiently by forest-based algorithms based on k-best parsing [8]. For 1-best search, we use the cube pruning technique [9, 10] which approximately intersects the translation forest with the LM. Basically, cube pruning works bottom up in a forest, keeping at most k +LM items at each node, and uses the bestfirst expansion idea from the Algorithm 2 of [8] to speed up the computation. For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of [8] that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation fo"
2009.iwslt-evaluation.8,J97-3002,0,0.00823424,"uting the second, third, through the kth best alternatives. However, this time we work on a finer-grained forest, called translation+LM forest, resulting from the intersection of the translation forest and the LM, with its nodes being the +LM items during cube pruning. Although this new forest is prohibitively large, Algorithm 3 is very efficient with minimal overhead on top of 1-best. For more details, please refer to [1] and [2]. Proceedings of IWSLT 2009, Tokyo - Japan 2.2. Bruin Bruin is a formally syntax-based SMT system, which implements the maximum entropy based reordering model on BTG [11] rules. This model considers the reorder as a problem of classification, where the Maximum Entropy model is introduced. To complete the decoding procedure, three BTG rules are used to derive the translation: [] A → (A1 , A2 ) hi (7) A → (A1 , A2 ) (8) A → (x, y) (9) The lexical rule (3) is used to translate source phrase y into target phrase x and generate a block A. The merging rules (1) and (2) are used to merge two consecutive blocks into a single larger block in the straight or inverted order. Three essential elements must be illustrated in Bruin. The first one is a stochastic BTG, whose r"
2009.iwslt-evaluation.8,P03-1021,0,0.0105949,"nguage model score of the two blocks according to their final order, λLM is its weight. For the lexical rule, applying it is assigned a probability P rl (A): P rl (A) = p(x|y)λ1 · p(y|x)λ2 · plex (x|y)λ3 ·plex (y|x)λ4 · exp(1)λ5 · exp(|x|)λ6 LM ·pλLM (x) (11) where p(·) are the phrase translation probabilities in both directions, plex (·) are the lexical translation probabilities in both directions, and exp(1) and exp(|x|) are the phrase penalty and word penalty, respectively. The feature weights λs are tuned to maximize the BLEU score on the development set, using minimum-error-rate training [12]. The MaxEnt-based Reordering Model (MRM) is defined on the two consecutive blocks A1 and A2 together with their - 57 - order o ∈ {straight, inverted} according to the maximum entropy framework. P exp( i θi hi (o, A1 , A2 )) 1 2 P Ω = pθ (o|A , A ) = P (12) 1 2 o exp( i θi hi (o, A , A )) where the functions hi ∈ {0, 1} are model features and the θi are the weights. The decoder is built upon the CKY chart-based algorithm. We use cube pruning technology to speed up the decoding. For more details, please refer to [13]. 2.3. Chiero Chiero is a re-implementation of the state-of-the-art hierarchica"
2009.iwslt-evaluation.8,P06-1066,1,0.846081,"imize the BLEU score on the development set, using minimum-error-rate training [12]. The MaxEnt-based Reordering Model (MRM) is defined on the two consecutive blocks A1 and A2 together with their - 57 - order o ∈ {straight, inverted} according to the maximum entropy framework. P exp( i θi hi (o, A1 , A2 )) 1 2 P Ω = pθ (o|A , A ) = P (12) 1 2 o exp( i θi hi (o, A , A )) where the functions hi ∈ {0, 1} are model features and the θi are the weights. The decoder is built upon the CKY chart-based algorithm. We use cube pruning technology to speed up the decoding. For more details, please refer to [13]. 2.3. Chiero Chiero is a re-implementation of the state-of-the-art hierarchical phrase-based model [9]. This model can be formalized as a synchronous contextfree grammar, which is automatically acquired from wordaligned parallel data without any syntactic information. X →&lt; γ, α, ∼&gt; (13) Where X is a non-terminal, γ, α are strings of terminals and non-terminals, and ∼ is one-to-one correspondence between the non-terminal in γ, α. Our work faithfully followed Chiang’s [9] work. The only exception is the condition for terminating cube pruning. Chiang’s [9] implementation quits upon considering t"
2009.iwslt-evaluation.8,I05-1007,1,0.670097,"T07 dev IWSLT08 dev selection 0.350 0.345 0.340 BLEU score ysis system ICTCLAS for splitting Chinese characters into words and a rule-based tokenizer for tokenizing English sentences. Then,we convert all alphanumeric characters to their 2-byte representation. Finally, we ran GIZA++ and used the “grow-diagfinal” heuristic to get many-to-many word alignments. We used the SRI Language Modeling Toolkit [14] to train the Chinese/English 5-gram language model with KneserNey smoothing on the Chinese/English side of the training corpus respectively. Regarding to Silenus, we used the Chinese parser of [15] and English parser of [16] to parse the source and target side of the bilingual corpus into packed forests respectively. Then we pruned the forests with the marginal probabilitybased inside-outside algorithm [17] with a pruning threshold pe = 3. At the decoding time, we use a large pruning threshold pd = 12 to generate the packed forest. 0.335 0.330 #143 0.325 #113 0.320 #413 0.315 #249 #191 0.310 0.305 0.300 0 1 2 3 Threshold 4 5 6 Figure 2: The BLEU scores and sentence # of dev selection with different thresholds. 4.2. Results on IWSLT09 3.1. Development Set Selection Our development set fo"
2009.iwslt-evaluation.8,P05-1022,0,0.209137,"on 0.350 0.345 0.340 BLEU score ysis system ICTCLAS for splitting Chinese characters into words and a rule-based tokenizer for tokenizing English sentences. Then,we convert all alphanumeric characters to their 2-byte representation. Finally, we ran GIZA++ and used the “grow-diagfinal” heuristic to get many-to-many word alignments. We used the SRI Language Modeling Toolkit [14] to train the Chinese/English 5-gram language model with KneserNey smoothing on the Chinese/English side of the training corpus respectively. Regarding to Silenus, we used the Chinese parser of [15] and English parser of [16] to parse the source and target side of the bilingual corpus into packed forests respectively. Then we pruned the forests with the marginal probabilitybased inside-outside algorithm [17] with a pruning threshold pe = 3. At the decoding time, we use a large pruning threshold pd = 12 to generate the packed forest. 0.335 0.330 #143 0.325 #113 0.320 #413 0.315 #249 #191 0.310 0.305 0.300 0 1 2 3 Threshold 4 5 6 Figure 2: The BLEU scores and sentence # of dev selection with different thresholds. 4.2. Results on IWSLT09 3.1. Development Set Selection Our development set for this year’s evaluation is"
2009.iwslt-evaluation.8,P08-1067,0,0.0326858,"ric characters to their 2-byte representation. Finally, we ran GIZA++ and used the “grow-diagfinal” heuristic to get many-to-many word alignments. We used the SRI Language Modeling Toolkit [14] to train the Chinese/English 5-gram language model with KneserNey smoothing on the Chinese/English side of the training corpus respectively. Regarding to Silenus, we used the Chinese parser of [15] and English parser of [16] to parse the source and target side of the bilingual corpus into packed forests respectively. Then we pruned the forests with the marginal probabilitybased inside-outside algorithm [17] with a pruning threshold pe = 3. At the decoding time, we use a large pruning threshold pd = 12 to generate the packed forest. 0.335 0.330 #143 0.325 #113 0.320 #413 0.315 #249 #191 0.310 0.305 0.300 0 1 2 3 Threshold 4 5 6 Figure 2: The BLEU scores and sentence # of dev selection with different thresholds. 4.2. Results on IWSLT09 3.1. Development Set Selection Our development set for this year’s evaluation is selected automatically from all the development sentences according to the n-gram similarity, which is calculated against the current test set sentences. Our method works as follows: Fi"
2020.acl-main.52,N19-1374,0,0.0725384,"Missing"
2020.acl-main.52,W19-2302,0,0.135774,"Missing"
2020.acl-main.52,W17-4912,0,0.021993,"mizer is Adam (Kingma and Ba, 2015) with 0.05 initial learning rate for pre-training and 10−5 for curriculum dual learning. The batch size is set to 64. λ in Eq. 4 is 0.5, γ in Eq. 6 is 1 and T in Eq. 7 is 100k. During curriculum dual learning, training runs until the performance on validation set does not improve. 4.3 Baselines We compare our approach with four representative baselines: (1) S2S-Attn: The Seq2Seq model with attention mechanism as in Shang et al. (2015). (2) EmoEmb: A Seq2Seq variant which takes the embedding of emotion categories as additional input at each decoding position (Ficler and Goldberg, 2017; Li et al., 2016b). (3) EmoDS: An emotional dialogue system with lexicon-based attention and a word-based classifier (Song et al., 2019). (4) ECM: Emotional Chatting Machine proposed by Zhou et al. (2018a). Additionally, we also conduct ablation study to better analyze our method as follows: (5) CDLemo: CDL with emotion reward only; (6) CDLcon: CDL with content reward only, which is similar to the work of Zhang et al. (2018); (7) CDL-DL: CDL with both rewards but without curriculum learning. 4.4 Table 2: Statistics of the NLPCC2017 Dataset. In the training set, we count the number of queries"
2020.acl-main.52,P17-1059,0,0.0297505,"653 0.690 5.1 Emotional Response Generation Early studies have proven that dialogue systems with proper emotional expressions and reactions can directly improve user satisfaction (Prendinger and Ishizuka, 2005; Prendinger et al., 2005) and contribute to effective users’ performance (Partala and Surakka, 2004). Polzin and Waibel (2000) and Polzin and Waibel (2000) apply rule-based methods to choose emotional responses from a conversation corpus, but those rules are hard to extend to large corpora. With the advent of deep learning, some researchers utilize neural networks to solve this problem (Ghosh et al., 2017; Hu et al., 2017; Zhou and Wang, 2018; Sun et al., 2018). Besides, the Valence, Arousal, and Dominance (VAD) lexicon (Warriner et al., 2013; Mohammad, 2018) is embedded to the sequence-to-sequence model (Sutskever et al., 2014) to provide extra affective information (Asghar et al., 2018; Zhong et al., 2019). Responses generated by above studies can simply continues the emotion of the query. To generate emotion-controllable responses, Zhou et al. (2018a) address the emotion factor in large-scale conversations, and propose ECM to generate responses based on different given emotions. After that,"
2020.acl-main.52,P18-1139,0,0.0140587,"pre-training of each emotion category. “Lex.”, “ACC(f )” and “ACC(b)” represent lexicon, classification accuracy of forward process and classification accuracy of backward process, respectively. 5 Related Work Responses generated by traditional open-domain dialogue systems are usually safe and generic. To produce diverse and informative responses, researchers tried to either import latent variables for model construction (Zhao et al., 2017; Serban et al., 2017; Shen et al., 2019) or utilize some extra knowledge, e.g., sentence types, personas, emotions, documents and knowledge triples/graphs (Ke et al., 2018; Li et al., 2016b; Zhou et al., 2018a; Meng et al., 2019; Zhou et al., 2018b; Niu et al., 2019). In this paper, we mainly touch on two branches of research: emotional response generation and dual learning in NLP. Further Analysis of CDL Here, we conduct a further analysis to show some characteristics of this task and the effect of CDL. Emotion lexicon size and classification accuracy after pre-training of each category (N (correct prediction) ÷ category size) are listed in Table 7. We can see that the classification accuracy is not totally related to the emotion lexicon size, indicating the e"
2020.acl-main.52,D14-1181,0,0.00539396,"Missing"
2020.acl-main.52,N16-1014,0,0.710808,"ly, Mb generates a query q 0 for a given response r and emotion category eq , and obtains the reward R that consists of Re and Rc from CLS and Mf (blue parts in Figure 1). These two models are trained alternatively via reinforcement learning (RL). Specifically, an action is the dialogue response to generate. The action space is infinite since arbitrary-length sequences can be generated. A state is denoted by the query, which is further transformed to a vector representation by the encoder. A policy takes the form of a GRU encoder-decoder and is defined by its parameters. Following the work of Li et al. (2016c); Zhang et al. (2018), we use a stochastic representation of the policy, i.e., a probability distribution over actions given states. In order to encourage both content consistency and emotion expression, we introduce two rewards and use them to train Mf and Mb . The definition (3) where n(wer ) is the number of emotion words belong to category er , and |r 0 |is the length of r 0 . Then, the emotion reward is defined as: Re(q,r0 ) = Re1 (q,r0 ) + λRe2 (q,r0 ) , (4) where λ controls the relative importance of implicit and explicit rewards. Reward for content consistency If the response are coh"
2020.acl-main.52,P16-1094,0,0.367346,"ly, Mb generates a query q 0 for a given response r and emotion category eq , and obtains the reward R that consists of Re and Rc from CLS and Mf (blue parts in Figure 1). These two models are trained alternatively via reinforcement learning (RL). Specifically, an action is the dialogue response to generate. The action space is infinite since arbitrary-length sequences can be generated. A state is denoted by the query, which is further transformed to a vector representation by the encoder. A policy takes the form of a GRU encoder-decoder and is defined by its parameters. Following the work of Li et al. (2016c); Zhang et al. (2018), we use a stochastic representation of the policy, i.e., a probability distribution over actions given states. In order to encourage both content consistency and emotion expression, we introduce two rewards and use them to train Mf and Mb . The definition (3) where n(wer ) is the number of emotion words belong to category er , and |r 0 |is the length of r 0 . Then, the emotion reward is defined as: Re(q,r0 ) = Re1 (q,r0 ) + λRe2 (q,r0 ) , (4) where λ controls the relative importance of implicit and explicit rewards. Reward for content consistency If the response are coh"
2020.acl-main.52,D16-1127,0,0.531616,"ly, Mb generates a query q 0 for a given response r and emotion category eq , and obtains the reward R that consists of Re and Rc from CLS and Mf (blue parts in Figure 1). These two models are trained alternatively via reinforcement learning (RL). Specifically, an action is the dialogue response to generate. The action space is infinite since arbitrary-length sequences can be generated. A state is denoted by the query, which is further transformed to a vector representation by the encoder. A policy takes the form of a GRU encoder-decoder and is defined by its parameters. Following the work of Li et al. (2016c); Zhang et al. (2018), we use a stochastic representation of the policy, i.e., a probability distribution over actions given states. In order to encourage both content consistency and emotion expression, we introduce two rewards and use them to train Mf and Mb . The definition (3) where n(wer ) is the number of emotion words belong to category er , and |r 0 |is the length of r 0 . Then, the emotion reward is defined as: Re(q,r0 ) = Re1 (q,r0 ) + λRe2 (q,r0 ) , (4) where λ controls the relative importance of implicit and explicit rewards. Reward for content consistency If the response are coh"
2020.acl-main.52,D17-1230,0,0.0312623,"y, for the backward learning process, the expected reward of the generated query Teacher Forcing When Mf and Mb are trained with only the rewards from the dual tasks, the training process would easily collapse as it may find an unexpected way to achieve a high reward but fail to guarantee the fluency or readability of the generated text (Ranzato et al., 2015; Pasunuru and Bansal, 2018; Luo et al., 2019b). To stabilize the training process, after each update according to Eq. 9 or 11, Mf or Mb is exposed to real queryresponse pairs and is trained via MLE, which is also known as Teacher Forcing (Li et al., 2017; Lamb et al., 2016). The training procedure of CDL is summarized in Algorithm 1. First, we use MLE to pre-train 559 Mf , Mb and CLS with query-response pairs and emotion labels in the training set. After the pretraining phase, we sort samples in the training set following the ranking standard in Section 3.2. For forward learning process, the ranking is based on responses, while for backward learning process, it is based on queries. Then, we can get two sorted training set Df and Db for each direction. Finally, Mf and Mb are optimized with rewards and the regularization of Teacher Forcing, alt"
2020.acl-main.52,P18-1017,0,0.0146226,"user satisfaction (Prendinger and Ishizuka, 2005; Prendinger et al., 2005) and contribute to effective users’ performance (Partala and Surakka, 2004). Polzin and Waibel (2000) and Polzin and Waibel (2000) apply rule-based methods to choose emotional responses from a conversation corpus, but those rules are hard to extend to large corpora. With the advent of deep learning, some researchers utilize neural networks to solve this problem (Ghosh et al., 2017; Hu et al., 2017; Zhou and Wang, 2018; Sun et al., 2018). Besides, the Valence, Arousal, and Dominance (VAD) lexicon (Warriner et al., 2013; Mohammad, 2018) is embedded to the sequence-to-sequence model (Sutskever et al., 2014) to provide extra affective information (Asghar et al., 2018; Zhong et al., 2019). Responses generated by above studies can simply continues the emotion of the query. To generate emotion-controllable responses, Zhou et al. (2018a) address the emotion factor in large-scale conversations, and propose ECM to generate responses based on different given emotions. After that, Colombo et al. (2019) augment ECM with 563 VAD embeddings and modified the loss function and decoding procedure. Song et al. (2019) use lexicon-based attent"
2020.acl-main.52,D19-1187,0,0.0844751,"Missing"
2020.acl-main.52,P02-1040,0,0.108078,"0 1.375 0.685 1.375 0.690 1.395 0.700 Sad Con. Emo. 1.125 0.120 0.990 0.225 1.210 0.395 1.205 0.425 1.245 0.565 Disgust Con. Emo. 1.160 0.115 1.125 0.295 1.200 0.340 1.205 0.325 1.235 0.490 Angry Con. Emo. 1.255 0.045 1.220 0.220 1.225 0.345 1.240 0.385 1.250 0.525 Happy Con. Emo. 1.155 0.305 1.275 0.400 1.260 0.535 1.255 0.590 1.305 0.630 Overall Con. Emo. 1.198 0.204 1.180 0.354 1.254 0.460 1.256 0.483 1.286 0.582 Table 4: Human evaluation results. “Con.” and “Emo.” denote content and emotion, respectively. Greedy, Extrema and Coherence)3 (Liu et al., 2016; Xu et al., 2018); 2) BLEU scores (Papineni et al., 2002) in 0 to 1 scale; 3) Dist-1, Dist-2 (Li et al., 2016a) and 4) Emotion-acc, Emotion-word (Zhou et al., 2018a; Song et al., 2019). Embedding scores and BLEU scores are used to measure the quality of generated responses in terms of content relevance. Whereas, Dist-1 and Dist-2 are used to evaluate the diversity of responses4 . Emotion-acc and Emotion-word are utilized to test the emotion expression. Specifically, Emo-acc is the agreement between the ground truth labels and the predicted labels through the TextCNN classifier trained before. Emo-word is the percentage of the generated responses tha"
2020.acl-main.52,N18-2102,0,0.0307638,"d model Mf , 0 R(q,r 0 ) = R(q,r 0 ) − bf , and bf is the baseline value from the greedy search decoding method for Mf , which is used to reduce the variance of the estimation (Zaremba and Sutskever, 2015; Paulus et al., 2017). Analogously, for the backward learning process, the expected reward of the generated query Teacher Forcing When Mf and Mb are trained with only the rewards from the dual tasks, the training process would easily collapse as it may find an unexpected way to achieve a high reward but fail to guarantee the fluency or readability of the generated text (Ranzato et al., 2015; Pasunuru and Bansal, 2018; Luo et al., 2019b). To stabilize the training process, after each update according to Eq. 9 or 11, Mf or Mb is exposed to real queryresponse pairs and is trained via MLE, which is also known as Teacher Forcing (Li et al., 2017; Lamb et al., 2016). The training procedure of CDL is summarized in Algorithm 1. First, we use MLE to pre-train 559 Mf , Mb and CLS with query-response pairs and emotion labels in the training set. After the pretraining phase, we sort samples in the training set following the ranking standard in Section 3.2. For forward learning process, the ranking is based on respons"
2020.acl-main.52,D16-1230,0,0.164189,"Missing"
2020.acl-main.52,N19-1119,0,0.097748,"Missing"
2020.acl-main.52,P19-1194,0,0.477443,"should be in a tight relationship and have equal qualities. Then, both the query-to-response mapping and response-to-query mapping would be easier and more natural. On the contrary, it is hard for a safe response to reach the original query through back-generation, neither on the content level nor the emotion level. At the same time, the difficulties of producing various emotions are different, especially in a noisy and uneven-quality dataset. Therefore, we can evaluate the response based on the feedback from the backward process to improve the coherence (Zhang et al., 2018; Cui et al., 2019; Luo et al., 2019b) and try to learn from easy to hard data to generate appropriate and emotion-rich responses. For emotion-controllable response generation, given a query q and an emotion category er , the goal is to generate a response r 0 that is not only meaningful, but also in accordance with the desired emotion. Emotional Chatting Machine (ECM) (Zhou et al., 2018a) addresses the emotion factor using three new mechanisms: Emotion Category Embedding, Internal Memory, and External Memory. Specifically, 1) Emotion Category Embedding models the high-level abstraction of emotion expression by embedding emotion"
2020.acl-main.52,D18-1432,0,0.0359635,"Missing"
2020.acl-main.52,P17-1061,0,0.0364115,"the validation set. Sad 294 0.691 0.655 Disgust Angry 1,142 30 0.609 0.736 0.602 0.756 Happy 405 0.818 0.808 Table 7: Emotion lexicon size and classification accuracy after pre-training of each emotion category. “Lex.”, “ACC(f )” and “ACC(b)” represent lexicon, classification accuracy of forward process and classification accuracy of backward process, respectively. 5 Related Work Responses generated by traditional open-domain dialogue systems are usually safe and generic. To produce diverse and informative responses, researchers tried to either import latent variables for model construction (Zhao et al., 2017; Serban et al., 2017; Shen et al., 2019) or utilize some extra knowledge, e.g., sentence types, personas, emotions, documents and knowledge triples/graphs (Ke et al., 2018; Li et al., 2016b; Zhou et al., 2018a; Meng et al., 2019; Zhou et al., 2018b; Niu et al., 2019). In this paper, we mainly touch on two branches of research: emotional response generation and dual learning in NLP. Further Analysis of CDL Here, we conduct a further analysis to show some characteristics of this task and the effect of CDL. Emotion lexicon size and classification accuracy after pre-training of each category (N ("
2020.acl-main.52,P15-1152,0,0.0418864,"used by (Zhou et al., 2018a) and (Song et al., 2019). Before curriculum dual learning, model Mf and Mb are pre-trained 10 epochs via MLE. The optimizer is Adam (Kingma and Ba, 2015) with 0.05 initial learning rate for pre-training and 10−5 for curriculum dual learning. The batch size is set to 64. λ in Eq. 4 is 0.5, γ in Eq. 6 is 1 and T in Eq. 7 is 100k. During curriculum dual learning, training runs until the performance on validation set does not improve. 4.3 Baselines We compare our approach with four representative baselines: (1) S2S-Attn: The Seq2Seq model with attention mechanism as in Shang et al. (2015). (2) EmoEmb: A Seq2Seq variant which takes the embedding of emotion categories as additional input at each decoding position (Ficler and Goldberg, 2017; Li et al., 2016b). (3) EmoDS: An emotional dialogue system with lexicon-based attention and a word-based classifier (Song et al., 2019). (4) ECM: Emotional Chatting Machine proposed by Zhou et al. (2018a). Additionally, we also conduct ablation study to better analyze our method as follows: (5) CDLemo: CDL with emotion reward only; (6) CDLcon: CDL with content reward only, which is similar to the work of Zhang et al. (2018); (7) CDL-DL: CDL w"
2020.acl-main.52,P19-1549,1,0.779514,"Disgust Angry 1,142 30 0.609 0.736 0.602 0.756 Happy 405 0.818 0.808 Table 7: Emotion lexicon size and classification accuracy after pre-training of each emotion category. “Lex.”, “ACC(f )” and “ACC(b)” represent lexicon, classification accuracy of forward process and classification accuracy of backward process, respectively. 5 Related Work Responses generated by traditional open-domain dialogue systems are usually safe and generic. To produce diverse and informative responses, researchers tried to either import latent variables for model construction (Zhao et al., 2017; Serban et al., 2017; Shen et al., 2019) or utilize some extra knowledge, e.g., sentence types, personas, emotions, documents and knowledge triples/graphs (Ke et al., 2018; Li et al., 2016b; Zhou et al., 2018a; Meng et al., 2019; Zhou et al., 2018b; Niu et al., 2019). In this paper, we mainly touch on two branches of research: emotional response generation and dual learning in NLP. Further Analysis of CDL Here, we conduct a further analysis to show some characteristics of this task and the effect of CDL. Emotion lexicon size and classification accuracy after pre-training of each category (N (correct prediction) ÷ category size) are"
2020.acl-main.52,P19-1359,0,0.719673,"ts to cool down today. [Like] I will try, thanks for your advice. [Sad] I am frozen to death ... [Disgust] Winner is the worst season. [Angry] You know nothing! [Happy] I really like to drink black tea. Yang Feng is the corresponding author. Recently, a framework called emotional chatting machine (ECM) (Zhou et al., 2018a) was proposed to address the emotion factor in a controlled manner, which focuses on generating a response with a specific emotion (Example 1 in Table 1). In the research field of emotion-controllable response generation, ECM and its successive methods (Colombo et al., 2019; Song et al., 2019) mainly represent the given emotion category as a vector and add it to the decoding steps to influence the procedure of response generation, which would aggravate the safe response problem. For the response generation task, safe response is notorious, as the model tends to produce some generic but meaningless responses, like “Thank you”, “I don’t know”, “Yes”, 556 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 556–566 c July 5 - 10, 2020. 2020 Association for Computational Linguistics etc. Due to the constraint of emotion factors, the scale of pr"
2020.acl-main.52,D17-1090,0,0.0384413,"sed on different given emotions. After that, Colombo et al. (2019) augment ECM with 563 VAD embeddings and modified the loss function and decoding procedure. Song et al. (2019) use lexicon-based attention and a word-based classifier to improve the ability of emotion expression. 5.2 sincerely thank the anonymous reviewers for their helpful and valuable suggestions. References Dual Learning in NLP He et al. (2016) propose Dual Learning (DL) for machine translation first which consider the source to target language translation and target to source language translation as a dual task. After that, Tang et al. (2017) implement a dual framework for the question answering system. Both Zhang et al. (2018) and Cui et al. (2019) use similar idea in dialogue generation task to produce coherent but not safe responses, since they find that a more diverse and specific response usually has a higher probability of being transformed back to the given query. Luo et al. (2019b) and Luo et al. (2019a) exploit DL in unsupervised text style transfer to relieve the need of parallel data. The differences between our method and those in Section 5.1 and Section 5.2 are: (1) We consider the emotion expression and content consi"
2020.acl-main.52,P18-1104,0,0.0438031,"ration Early studies have proven that dialogue systems with proper emotional expressions and reactions can directly improve user satisfaction (Prendinger and Ishizuka, 2005; Prendinger et al., 2005) and contribute to effective users’ performance (Partala and Surakka, 2004). Polzin and Waibel (2000) and Polzin and Waibel (2000) apply rule-based methods to choose emotional responses from a conversation corpus, but those rules are hard to extend to large corpora. With the advent of deep learning, some researchers utilize neural networks to solve this problem (Ghosh et al., 2017; Hu et al., 2017; Zhou and Wang, 2018; Sun et al., 2018). Besides, the Valence, Arousal, and Dominance (VAD) lexicon (Warriner et al., 2013; Mohammad, 2018) is embedded to the sequence-to-sequence model (Sutskever et al., 2014) to provide extra affective information (Asghar et al., 2018; Zhong et al., 2019). Responses generated by above studies can simply continues the emotion of the query. To generate emotion-controllable responses, Zhou et al. (2018a) address the emotion factor in large-scale conversations, and propose ECM to generate responses based on different given emotions. After that, Colombo et al. (2019) augment ECM wit"
2020.acl-main.563,N19-1423,0,0.0940548,"Missing"
2020.acl-main.563,P19-1546,0,0.408049,"´c et al., 2017; Zhong et al., 2018; Chao and Lane, 2019) which neglects the use of dialogue history. Recent researches attempt to address this problem through introducing historical dialogue information into the prediction of slot-value pairs. Most of them leverage a naive attention between slots and concatenated historical utterances (Wu et al., 2019; Zhou and Small, 2019; Gao et al., 2019b; Zhang et al., 2019; Le et al., 2020a,b) or only utilize partial history (Ren et al., 2019; Kim et al., 2019; Sharma et al., 2019) or lack direct interactions between slots and history (Ren et al., 2018; Lee et al., 2019; Goel et al., 2019). Briefly, these methods are deficient in exploiting relevant context from dialogue history. 6322 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6322–6333 c July 5 - 10, 2020. 2020 Association for Computational Linguistics This paper was retracted. For more information, see https://aclanthology.org/2020.acl-main.563. adjusting the weight of each slot. To the best of our knowledge, our method is the first to address the slot imbalance problem in DST. • Experimental results show that our model achieves state-of-the-art performan"
2020.acl-main.563,P18-1133,0,0.0934332,"redictions while SUMBT fails. guage understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons besides the ontology, and they are hard to extend and scale to new domains. Recent neural network models are proposed for further improvements (Mrkˇsi´c et al., 2015; Hori et al., 2016; Mrkˇsi´c et al., 2017; Lei et al., 2018; Xu and Hu, 2018; Zhong et al., 2018; Nouri and Hosseini-Asl, 2018; Wu et al., 2019; Ren et al., 2019; Balaraman and Magnini, 2019). Ren et al. (2018) and Lee et al. (2019) use an RNN to encode the slot-related information of each turn, where slots can not attend to relevant information of past turns directly. Sharma et al. (2019) employ a heuristic rule to extract partial dialogue history and then integrate the historical information into prediction in a coarse manner. Goel et al. (2019) encode the dialogue history into a hidden state and then simply combine it with the slot to make decision"
2020.acl-main.563,N19-1057,0,0.207776,"rs accomplish tasks through spoken interactions (Young, 2002; Young et al., 2013; Gao et al., 2019a). Dialogue state tracking (DST) is an essential part of dialogue management in task-oriented dialogue systems. Given current utterances and dialogue history, DST aims to determine the set of † Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. ∗ Yang Feng is the corresponding author. 1 Code is available at https://github.com/ictnlp/CHAN-DST As Table 1 shows, the dialogue state is usually dependent on relevant context in the dialogue history, which is proven in previous studies (Sharma et al., 2019; Wu et al., 2019). However, traditional DST models usually determine dialogue states by considering only utterances at current turn (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018; Chao and Lane, 2019) which neglects the use of dialogue history. Recent researches attempt to address this problem through introducing historical dialogue information into the prediction of slot-value pairs. Most of them leverage a naive attention between slots and concatenated historical utterances (Wu et al., 2019; Zhou and Small, 2019; Gao et al., 2019b; Zhang et al., 2019; Le et al., 2020a,b"
2020.acl-main.563,P18-1158,0,0.0295864,"estination restaurant-food restaurant-book-people hotel-stars attraction-area hotel-price-range hotel-type attraction-type hotel-area restaurant-price-range restaurant-area train-leave-at hotel-internet hotel-parking hotel-name hotel-book-stay hotel-book-people hotel-book-day restaurant-book-time restaurant-book-day -0.2 taxi-arrive-by taxi-leave-at taxi-departure taxi-destination attraction-name train-book-people restaurant-name train-arrive-by -0.1 Percentage 6329 This paper was retracted. For more information, see https://aclanthology.org/2020.acl-main.563. et al., 2016; Ying et al., 2018; Wang et al., 2018; Xing et al., 2018; Aujogue and Aussem, 2019; Naik et al., 2018; Liu and Chen, 2019). Conclusion Acknowledgments We thank the anonymous reviewers for their insightful comments. This work was supported by National Key R&D Program of China (NO. 2017YFE0192900). References Jianfeng Gao, Michel Galley, Lihong Li, et al. 2019a. Neural approaches to conversational ai. FoundaR in Information Retrieval, 13(2tions and Trends 3):127–298. Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagyoung Chung, Dilek Hakkani-Tur, and Amazon Alexa AI. 2019b. Dialog state tracking: A neural reading comprehension appr"
2020.acl-main.563,P19-1543,0,0.0185449,"-price-range hotel-type attraction-type hotel-area restaurant-price-range restaurant-area train-leave-at hotel-internet hotel-parking hotel-name hotel-book-stay hotel-book-people hotel-book-day restaurant-book-time restaurant-book-day -0.2 taxi-arrive-by taxi-leave-at taxi-departure taxi-destination attraction-name train-book-people restaurant-name train-arrive-by -0.1 Percentage 6329 This paper was retracted. For more information, see https://aclanthology.org/2020.acl-main.563. et al., 2016; Ying et al., 2018; Wang et al., 2018; Xing et al., 2018; Aujogue and Aussem, 2019; Naik et al., 2018; Liu and Chen, 2019). Conclusion Acknowledgments We thank the anonymous reviewers for their insightful comments. This work was supported by National Key R&D Program of China (NO. 2017YFE0192900). References Jianfeng Gao, Michel Galley, Lihong Li, et al. 2019a. Neural approaches to conversational ai. FoundaR in Information Retrieval, 13(2tions and Trends 3):127–298. Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagyoung Chung, Dilek Hakkani-Tur, and Amazon Alexa AI. 2019b. Dialog state tracking: A neural reading comprehension approach. In 20th Annual Meeting of the Special Interest Group on Discourse and Dialogue,"
2020.acl-main.563,D18-1299,0,0.0611422,"Missing"
2020.acl-main.563,P15-2130,0,0.0471484,"Missing"
2020.acl-main.563,P17-1163,0,0.102841,"Missing"
2020.acl-main.563,D19-1196,0,0.612489,"Missing"
2020.acl-main.563,W13-4067,0,0.165105,"Missing"
2020.acl-main.563,E17-1042,0,0.108691,"Missing"
2020.acl-main.563,W13-4065,0,0.225425,"Missing"
2020.acl-main.563,P19-1078,0,0.472121,"Missing"
2020.acl-main.563,N16-1174,0,0.0826114,"Missing"
2020.acl-main.563,N16-1000,0,0.223513,"Missing"
2020.acl-main.563,P18-1134,0,0.104761,"UMBT fails. guage understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons besides the ontology, and they are hard to extend and scale to new domains. Recent neural network models are proposed for further improvements (Mrkˇsi´c et al., 2015; Hori et al., 2016; Mrkˇsi´c et al., 2017; Lei et al., 2018; Xu and Hu, 2018; Zhong et al., 2018; Nouri and Hosseini-Asl, 2018; Wu et al., 2019; Ren et al., 2019; Balaraman and Magnini, 2019). Ren et al. (2018) and Lee et al. (2019) use an RNN to encode the slot-related information of each turn, where slots can not attend to relevant information of past turns directly. Sharma et al. (2019) employ a heuristic rule to extract partial dialogue history and then integrate the historical information into prediction in a coarse manner. Goel et al. (2019) encode the dialogue history into a hidden state and then simply combine it with the slot to make decisions. These models a"
2020.acl-main.563,P18-1135,0,0.22343,"systems. Given current utterances and dialogue history, DST aims to determine the set of † Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. ∗ Yang Feng is the corresponding author. 1 Code is available at https://github.com/ictnlp/CHAN-DST As Table 1 shows, the dialogue state is usually dependent on relevant context in the dialogue history, which is proven in previous studies (Sharma et al., 2019; Wu et al., 2019). However, traditional DST models usually determine dialogue states by considering only utterances at current turn (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018; Chao and Lane, 2019) which neglects the use of dialogue history. Recent researches attempt to address this problem through introducing historical dialogue information into the prediction of slot-value pairs. Most of them leverage a naive attention between slots and concatenated historical utterances (Wu et al., 2019; Zhou and Small, 2019; Gao et al., 2019b; Zhang et al., 2019; Le et al., 2020a,b) or only utilize partial history (Ren et al., 2019; Kim et al., 2019; Sharma et al., 2019) or lack direct interactions between slots and history (Ren et al., 2018; Lee et al., 2019; Goel et al., 2019"
2020.autosimtrans-1.3,2014.amta-researchers.20,0,0.10186,"Missing"
2020.autosimtrans-1.3,W16-2209,0,0.0225406,"Missing"
2020.autosimtrans-1.3,P18-1163,0,0.0525397,"lping us train a more robust model in the future work. Table 5: Comparison of “+SP Amendment”, “+HM Amendmen” and “+Both Amendment” on the WMT17 ZH→EN dataset. model significantly outperforms the baseline model on the noisy test sets on both of the NIST and WMT17 translation tasks. Furthernmore, we got the following conclusions: First, the baseline model performs well on the clean test set, but it suffers a great performance drop on the noisy test sets, which indicates that the conventional NMT is indeed fragile to permuted inputs, which is consistent with prior work (Belinkov and Bisk, 2017; Cheng et al., 2018). Second, the results of our proposed method show that our model can not only get a competitive performance compared to the baseline model on the clean test set, but also outperform all the baseline 4.4 Ablation Study In order to further understand the impact of the components of the proposed method, we performed some further studies by training multiple versions of our model by removing the some components of it. The first one is just with the amending pronunciation for SP errors. The second one is just with the 20 amending errors for HM error. The overall results are shown in the Table 4 and"
2020.autosimtrans-1.3,W16-2323,0,0.0171184,"e training and test is consistent. Second, we focus on ASR errors on homophone words and words with similar pronunciation and make use of their pronunciation information to help the translation model to recover from the input errors. Experiments on two Chinese-English data sets show that our method is more robust to input errors and can outperform the strong Transformer baseline significantly. 1 Introduction In recent years, neural machine translation (NMT) has achieved impressive progress and has shown superiority over statistical machine translation (SMT) systems on multiple language pairs (Sennrich et al., 2016). NMT models are usually built under the encoder-decoder architecture where the encoder produces a representation for the source sentence and the decoder generates target translation from this representation word by word (Cho et al., 2014; Sutskever et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Now NMT systems are widely used in real world and in many cases they receive as input the result of the automatic speech recognition (ASR) system. Despite the great success, NMT is subject to orthographic and morphological errors which can 15 Proceedings of the 1st Workshop on Automatic Sim"
2020.autosimtrans-1.3,D17-1145,0,0.0618284,"same erroneous ASR output, translations of the baseline NMT system and our robust NMT system. tempted to induce noise by considering the realistic ASR outputs as the source corpora used for training MT systems (Peitz et al., 2012; Tsvetkov et al., 2014). Although the problem of error propagation could be alleviated by the promising end-to-end speech translation models (Serdyuk et al., 2018; B´erard et al., 2018). Unfortunately, there are few training data in the form of speech paired with text translations. In contrast, our approach utilizes the large-scale written parallel corpora. Recently, Sperber et al. (2017) adapted the NMT model to noise outputs from ASR, where they introduced artificially corrupted inputs during the training process and only achieved minor improvements on noisy input but harmed the translation quality on clean text. However, our approach not only significantly enhances the robustness of NMT on noisy test sets, but also improves the generalization performance. been demonstrated effective to Chinese-sourced NMT (Liu et al., 2019; Zhang and Matsumoto, 2017; Du and Way, 2017) and Chinese ASR (Chan and Lane, 2016). We also incorporate Pinyin as an additional input feature in the rob"
2020.autosimtrans-1.3,E14-1065,0,0.0152885,"scenario. Prior work at21 Speech ASR Ref Baseline Our Approach 该 数 字 已 经 大 幅 下 降 g¯ai shu` z`ı yˇı j¯ıng d`a f´u xi`a ji`ang 该 书 字 已 经 大 幅 下 降 g¯ai shu¯ z`ı yˇı j¯ıng d`a f´u xi`a ji`ang The figure has fallen sharply by almost half. The book has fallen by nearly half. The figure has fallen by nearly half. 近 j`ın 近 j`ın 一 y¯ı 一 y¯ı 半。 b`an。 半。 b`an。 Table 6: For the same erroneous ASR output, translations of the baseline NMT system and our robust NMT system. tempted to induce noise by considering the realistic ASR outputs as the source corpora used for training MT systems (Peitz et al., 2012; Tsvetkov et al., 2014). Although the problem of error propagation could be alleviated by the promising end-to-end speech translation models (Serdyuk et al., 2018; B´erard et al., 2018). Unfortunately, there are few training data in the form of speech paired with text translations. In contrast, our approach utilizes the large-scale written parallel corpora. Recently, Sperber et al. (2017) adapted the NMT model to noise outputs from ASR, where they introduced artificially corrupted inputs during the training process and only achieved minor improvements on noisy input but harmed the translation quality on clean text."
2020.autosimtrans-1.3,P19-1291,0,0.0203418,"ing data in the form of speech paired with text translations. In contrast, our approach utilizes the large-scale written parallel corpora. Recently, Sperber et al. (2017) adapted the NMT model to noise outputs from ASR, where they introduced artificially corrupted inputs during the training process and only achieved minor improvements on noisy input but harmed the translation quality on clean text. However, our approach not only significantly enhances the robustness of NMT on noisy test sets, but also improves the generalization performance. been demonstrated effective to Chinese-sourced NMT (Liu et al., 2019; Zhang and Matsumoto, 2017; Du and Way, 2017) and Chinese ASR (Chan and Lane, 2016). We also incorporate Pinyin as an additional input feature in the robust NMT model, aiming at improving the robustness of NMT further. 6 Conclusion Voice input has become popular recently and as a result, machine translation systems have to deal with the input from the results of ASR systems which contains recognition errors. In this paper we aim to improve the robustness of NMT when its input contains ASR errors from two aspects. One is from the perspective of data by adding simulated ASR errors to the traini"
2020.autosimtrans-1.3,vilar-etal-2006-error,0,0.179009,"Missing"
2020.autosimtrans-1.3,W16-4122,0,0.066577,"Missing"
2020.autosimtrans-1.3,P02-1040,0,0.107071,"a single server with eight NVIDIA TITAN Xp GPUs where each was allocated with a batch size of 4096 tokens. Sentences longer than 100 tokens were removed from the training data. For the base model, we trained it for a total of 100k steps and save a checkpoint at every 1k step intervals. The single model obtained by averaging the last 5 checkpoints were used for measuring the results. During decoding, we set beam size to 5, and length penalty α=0.6 (Wu et al., 2016). Other training parameters are the same as the default configuration of the Transformer model. We report casesensitive NIST BLEU (Papineni et al., 2002) scores for all the systems. For evaluation, we first merge output tokens back to their untokenized representation using detokenizer.pl and then use multi-bleu.pl to compute the scores as per reference. (14) Then the updated hidden states of source words are fed to the decoder for the calculation of attention and generation of target words. 4 Experiments 4.1 Training Details Data Preparation We evaluated our method on two Chinese-English data sets which are from the NIST translation task and WMT17 translation task, respectively. For the NIST translation task, the training data consists of abou"
2020.autosimtrans-1.3,2012.iwslt-papers.18,0,0.0269957,"MT system in the SLT scenario. Prior work at21 Speech ASR Ref Baseline Our Approach 该 数 字 已 经 大 幅 下 降 g¯ai shu` z`ı yˇı j¯ıng d`a f´u xi`a ji`ang 该 书 字 已 经 大 幅 下 降 g¯ai shu¯ z`ı yˇı j¯ıng d`a f´u xi`a ji`ang The figure has fallen sharply by almost half. The book has fallen by nearly half. The figure has fallen by nearly half. 近 j`ın 近 j`ın 一 y¯ı 一 y¯ı 半。 b`an。 半。 b`an。 Table 6: For the same erroneous ASR output, translations of the baseline NMT system and our robust NMT system. tempted to induce noise by considering the realistic ASR outputs as the source corpora used for training MT systems (Peitz et al., 2012; Tsvetkov et al., 2014). Although the problem of error propagation could be alleviated by the promising end-to-end speech translation models (Serdyuk et al., 2018; B´erard et al., 2018). Unfortunately, there are few training data in the form of speech paired with text translations. In contrast, our approach utilizes the large-scale written parallel corpora. Recently, Sperber et al. (2017) adapted the NMT model to noise outputs from ASR, where they introduced artificially corrupted inputs during the training process and only achieved minor improvements on noisy input but harmed the translation"
2020.coling-main.381,D17-1156,0,0.0373362,"ed knowledge. This phenomenon is called catastrophic forgetting. Figure 1 shows the performance trends on the in-domain and general-domain. Many methods have been proposed to address the catastrophic forgetting problem under the scheme of fine-tuning. Freitag and Al-Onaizan (2016) ensembles the general-domain model and the fine-tuned model together so that the integrated model can consider both domains. Dakwale and Monz (2017) introduces domain-specific output layers for both of the domains and thus the domain-specific features of the two domains can be well preserved. Thompson et al. (2019), Barone et al. (2017), and Khayrallah et al. (2018) propose regularization-based methods that introduce an additional loss to the original objective to help the model trade off between the general-domain and in-domain. All these methods show their effectiveness and have mitigated the performance decline on general-domain, but we still don’t know what happened inside the model during continual training and why these methods can alleviate the ∗ Corresponding author: Yang Feng. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details:"
2020.coling-main.381,D14-1179,0,0.0538819,"Missing"
2020.coling-main.381,P17-2061,0,0.0400974,"sense, the work of Thompson et al. (2018) is most related to ours, which tries to understand the effectiveness of continued training for improving the in-domain performance. Compared with their work, our work also explores the cause of the catastrophic forgetting problem in general-domain. Besides, our work also analyzes the performance of NMT at the neuron (parameter) level. Continual Training Continual training, which is also referred to as fine-tuning, is widely used in NMT for the domain adaptation task. Luong and Manning (2015) fine tunes the general-domain model with the in-domain data. Chu et al. (2017) fine tunes the model with the mix of the general-domain data and oversampled in-domain data. Khayrallah et al. (2018) and Thompson et al. (2019) add regularization terms to let the model parameters stay close to their original values. Dakwale and Monz (2017) minimizes the cross-entropy between the output distribution of the general-domain model and the fine-tuned model. Gu et al. (2019) adds a discriminator to help preserve the domain-shared features and fine tunes the whole model on the mixed training data. Jiang et al. (2020) proposes to obtain the word representations by mixing their embed"
2020.coling-main.381,P17-1106,0,0.0201322,"ning is calculated and we find that the important parameters change more greatly, which causes the catastrophic forgetting problem. Inspired by our findings, we can freeze part of those important parameters during continual training to avoid catastrophic forgetting. Besides, we can retrain those unimportant parameters to further improve the in-domain translation. 5 Related Work Analyzing Work Recently, much work has been concerned with analyzing and evaluating the NMT model from different perspectives. Shi et al. (2016) investigates how NMT models output target strings of appropriate lengths. Ding et al. (2017) analyzes the contribution of each contextual word to arbitrary hidden states. Qi et al. (2018) analyzes when the pre-trained word embeddings can help in NMT tasks. Voita et al. (2019) analyzes the importance of different attention heads. Bau et al. (2019) investigates the importance and function of different neurons in NMT. Wuebker et al. (2018) finds that a large proportion 4323 of model parameters can be frozen during adaptation with minimal reduction in translation quality by encouraging structured sparsity. Wang and Sennrich (2020) links the exposure bias problem (Ranzato et al., 2016; Sh"
2020.coling-main.381,N19-1312,1,0.809868,"aining Continual training, which is also referred to as fine-tuning, is widely used in NMT for the domain adaptation task. Luong and Manning (2015) fine tunes the general-domain model with the in-domain data. Chu et al. (2017) fine tunes the model with the mix of the general-domain data and oversampled in-domain data. Khayrallah et al. (2018) and Thompson et al. (2019) add regularization terms to let the model parameters stay close to their original values. Dakwale and Monz (2017) minimizes the cross-entropy between the output distribution of the general-domain model and the fine-tuned model. Gu et al. (2019) adds a discriminator to help preserve the domain-shared features and fine tunes the whole model on the mixed training data. Jiang et al. (2020) proposes to obtain the word representations by mixing their embedding in individual domains based on the domain proportions. Compared with them, our work pays attention to exploring the inner change of the model during continual training as well as the cause of the catastrophic forgetting phenomenon. 6 Conclusion In this work, we focus on the catastrophic forgetting phenomenon of NMT and aim to find the inner reasons for this. Under the background of"
2020.coling-main.381,2020.emnlp-main.76,1,0.775839,"re-trained word embeddings can help in NMT tasks. Voita et al. (2019) analyzes the importance of different attention heads. Bau et al. (2019) investigates the importance and function of different neurons in NMT. Wuebker et al. (2018) finds that a large proportion 4323 of model parameters can be frozen during adaptation with minimal reduction in translation quality by encouraging structured sparsity. Wang and Sennrich (2020) links the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019) to the phenomenon of NMT tends to generate hallucinations under domain shift. Gu et al. (2020) finds that the NMT tends to generate more high-frequency tokens and less low-frequency tokens than reference. Compared with them, this work mainly focuses on investigating the functions of the different modules and parameters in the NMT model during continual training. In this sense, the work of Thompson et al. (2018) is most related to ours, which tries to understand the effectiveness of continued training for improving the in-domain performance. Compared with their work, our work also explores the cause of the catastrophic forgetting problem in general-domain. Besides, our work also analyze"
2020.coling-main.381,2020.acl-main.165,0,0.0330545,"(2015) fine tunes the general-domain model with the in-domain data. Chu et al. (2017) fine tunes the model with the mix of the general-domain data and oversampled in-domain data. Khayrallah et al. (2018) and Thompson et al. (2019) add regularization terms to let the model parameters stay close to their original values. Dakwale and Monz (2017) minimizes the cross-entropy between the output distribution of the general-domain model and the fine-tuned model. Gu et al. (2019) adds a discriminator to help preserve the domain-shared features and fine tunes the whole model on the mixed training data. Jiang et al. (2020) proposes to obtain the word representations by mixing their embedding in individual domains based on the domain proportions. Compared with them, our work pays attention to exploring the inner change of the model during continual training as well as the cause of the catastrophic forgetting phenomenon. 6 Conclusion In this work, we focus on the catastrophic forgetting phenomenon of NMT and aim to find the inner reasons for this. Under the background of domain adaptation, we propose two analyzing methods from the perspectives of modules and parameters (neurons) and conduct experiments across dif"
2020.coling-main.381,D13-1176,0,0.109602,"e modules of the NMT model shows that some modules have tight relation with the general-domain knowledge while some other modules are more essential in the domain adaptation. And the investigation on the parameters shows that some parameters are important for both the general-domain and in-domain translation and the great change of them during continual training brings about the performance decline in general-domain. We conduct experiments across different language pairs and domains to ensure the validity and reliability of our findings. 1 Introduction Neural machine translation (NMT) models (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields. Due to numerous parameters, NMT models can only play to their advantages based on large-scale training data. However, in practical applications, NMT models often need to perform translation for some specific domain with only a small quantity of in-domain data available. In this situation, continual training (Luong and Manning, 2015), which is also referred to as fine-tuning, is often employed to improve t"
2020.coling-main.381,W18-2705,0,0.211577,"non is called catastrophic forgetting. Figure 1 shows the performance trends on the in-domain and general-domain. Many methods have been proposed to address the catastrophic forgetting problem under the scheme of fine-tuning. Freitag and Al-Onaizan (2016) ensembles the general-domain model and the fine-tuned model together so that the integrated model can consider both domains. Dakwale and Monz (2017) introduces domain-specific output layers for both of the domains and thus the domain-specific features of the two domains can be well preserved. Thompson et al. (2019), Barone et al. (2017), and Khayrallah et al. (2018) propose regularization-based methods that introduce an additional loss to the original objective to help the model trade off between the general-domain and in-domain. All these methods show their effectiveness and have mitigated the performance decline on general-domain, but we still don’t know what happened inside the model during continual training and why these methods can alleviate the ∗ Corresponding author: Yang Feng. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 4315 Proceedings of th"
2020.coling-main.381,2015.iwslt-evaluation.11,0,0.250943,"r findings. 1 Introduction Neural machine translation (NMT) models (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) have achieved state-of-the-art results and have been widely used in many fields. Due to numerous parameters, NMT models can only play to their advantages based on large-scale training data. However, in practical applications, NMT models often need to perform translation for some specific domain with only a small quantity of in-domain data available. In this situation, continual training (Luong and Manning, 2015), which is also referred to as fine-tuning, is often employed to improve the in-domain translation performance. In this method, the model is first trained with large-scale general-domain training data and then continually trained with the in-domain data. With this method, the in-domain performance can be improved greatly, but unfortunately, the general-domain performance decline significantly, since NMT models tend to overfit to frequent observations (e.g. words, word co-occurrences, translation patterns) in the in-domain data but forget previously learned knowledge. This phenomenon is called"
2020.coling-main.381,N19-4009,0,0.031077,"st 2014 for the test. For the in-domain data, we use the parallel training data from the IWSLT 2015 which is mainly from the Spoken domain. It contains about 194K sentences. We choose the 2012dev for validation and 2013tst for the test. We tokenize and truecase the corpora. Besides, integrating operations of 32K, 16K, and 30K are performed to learn BPE (Sennrich et al., 2016) on the general-domain data and then applied to both the general-domain and in-domain data. The dictionaries are also built based on the general-domain data. 3.2.2 Systems We use the open-source toolkit called Fairseq-py (Ott et al., 2019) released by Facebook as our Transformer system. We train the model with two sets of parameters. For the quantitatively analyzing experiments, the system is implemented as the base model configuration in Vaswani et al. (2017) strictly. For the visualizing experiments, we employ a tiny setting: the embedding size is set to 32, the FFN size is set to 64, and the rest is the same with the base model. 4318 Figure 3: The BLEU of the experiments based on the type of the target module. The x-axis denotes the module of different types, where ’Emb’, ’SA’, ’CA’, and ’FFN’ denote the embedding layer, sel"
2020.coling-main.381,N18-2084,0,0.053524,"e catastrophic forgetting problem. Inspired by our findings, we can freeze part of those important parameters during continual training to avoid catastrophic forgetting. Besides, we can retrain those unimportant parameters to further improve the in-domain translation. 5 Related Work Analyzing Work Recently, much work has been concerned with analyzing and evaluating the NMT model from different perspectives. Shi et al. (2016) investigates how NMT models output target strings of appropriate lengths. Ding et al. (2017) analyzes the contribution of each contextual word to arbitrary hidden states. Qi et al. (2018) analyzes when the pre-trained word embeddings can help in NMT tasks. Voita et al. (2019) analyzes the importance of different attention heads. Bau et al. (2019) investigates the importance and function of different neurons in NMT. Wuebker et al. (2018) finds that a large proportion 4323 of model parameters can be frozen during adaptation with minimal reduction in translation quality by encouraging structured sparsity. Wang and Sennrich (2020) links the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019) to the phenomenon of NMT tends to generate hallucinations"
2020.coling-main.381,P16-1162,0,0.123825,"e corpora. English→German. For this task, the general-domain data is from the WMT 2016 English to German translation task which is mainly News texts. It contains about 4.5M sentence pairs. We choose the news-test 2013 for validation and news-test 2014 for the test. For the in-domain data, we use the parallel training data from the IWSLT 2015 which is mainly from the Spoken domain. It contains about 194K sentences. We choose the 2012dev for validation and 2013tst for the test. We tokenize and truecase the corpora. Besides, integrating operations of 32K, 16K, and 30K are performed to learn BPE (Sennrich et al., 2016) on the general-domain data and then applied to both the general-domain and in-domain data. The dictionaries are also built based on the general-domain data. 3.2.2 Systems We use the open-source toolkit called Fairseq-py (Ott et al., 2019) released by Facebook as our Transformer system. We train the model with two sets of parameters. For the quantitatively analyzing experiments, the system is implemented as the base model configuration in Vaswani et al. (2017) strictly. For the visualizing experiments, we employ a tiny setting: the embedding size is set to 32, the FFN size is set to 64, and th"
2020.coling-main.381,D18-1510,1,0.844085,"7) analyzes the contribution of each contextual word to arbitrary hidden states. Qi et al. (2018) analyzes when the pre-trained word embeddings can help in NMT tasks. Voita et al. (2019) analyzes the importance of different attention heads. Bau et al. (2019) investigates the importance and function of different neurons in NMT. Wuebker et al. (2018) finds that a large proportion 4323 of model parameters can be frozen during adaptation with minimal reduction in translation quality by encouraging structured sparsity. Wang and Sennrich (2020) links the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019) to the phenomenon of NMT tends to generate hallucinations under domain shift. Gu et al. (2020) finds that the NMT tends to generate more high-frequency tokens and less low-frequency tokens than reference. Compared with them, this work mainly focuses on investigating the functions of the different modules and parameters in the NMT model during continual training. In this sense, the work of Thompson et al. (2018) is most related to ours, which tries to understand the effectiveness of continued training for improving the in-domain performance. Compared with their work, our w"
2020.coling-main.381,D16-1248,0,0.0286703,"Finally, the average Euclidean distance of model parameters before and after continual learning is calculated and we find that the important parameters change more greatly, which causes the catastrophic forgetting problem. Inspired by our findings, we can freeze part of those important parameters during continual training to avoid catastrophic forgetting. Besides, we can retrain those unimportant parameters to further improve the in-domain translation. 5 Related Work Analyzing Work Recently, much work has been concerned with analyzing and evaluating the NMT model from different perspectives. Shi et al. (2016) investigates how NMT models output target strings of appropriate lengths. Ding et al. (2017) analyzes the contribution of each contextual word to arbitrary hidden states. Qi et al. (2018) analyzes when the pre-trained word embeddings can help in NMT tasks. Voita et al. (2019) analyzes the importance of different attention heads. Bau et al. (2019) investigates the importance and function of different neurons in NMT. Wuebker et al. (2018) finds that a large proportion 4323 of model parameters can be frozen during adaptation with minimal reduction in translation quality by encouraging structured"
2020.coling-main.381,W18-6313,0,0.0214004,"tation with minimal reduction in translation quality by encouraging structured sparsity. Wang and Sennrich (2020) links the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019) to the phenomenon of NMT tends to generate hallucinations under domain shift. Gu et al. (2020) finds that the NMT tends to generate more high-frequency tokens and less low-frequency tokens than reference. Compared with them, this work mainly focuses on investigating the functions of the different modules and parameters in the NMT model during continual training. In this sense, the work of Thompson et al. (2018) is most related to ours, which tries to understand the effectiveness of continued training for improving the in-domain performance. Compared with their work, our work also explores the cause of the catastrophic forgetting problem in general-domain. Besides, our work also analyzes the performance of NMT at the neuron (parameter) level. Continual Training Continual training, which is also referred to as fine-tuning, is widely used in NMT for the domain adaptation task. Luong and Manning (2015) fine tunes the general-domain model with the in-domain data. Chu et al. (2017) fine tunes the model wi"
2020.coling-main.381,N19-1209,0,0.233131,"forget previously learned knowledge. This phenomenon is called catastrophic forgetting. Figure 1 shows the performance trends on the in-domain and general-domain. Many methods have been proposed to address the catastrophic forgetting problem under the scheme of fine-tuning. Freitag and Al-Onaizan (2016) ensembles the general-domain model and the fine-tuned model together so that the integrated model can consider both domains. Dakwale and Monz (2017) introduces domain-specific output layers for both of the domains and thus the domain-specific features of the two domains can be well preserved. Thompson et al. (2019), Barone et al. (2017), and Khayrallah et al. (2018) propose regularization-based methods that introduce an additional loss to the original objective to help the model trade off between the general-domain and in-domain. All these methods show their effectiveness and have mitigated the performance decline on general-domain, but we still don’t know what happened inside the model during continual training and why these methods can alleviate the ∗ Corresponding author: Yang Feng. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4"
2020.coling-main.381,tian-etal-2014-um,0,0.146474,"decoder; the second is based on its type, e.g., we freeze or update the self-attention sublayers or the cross-attention sublayers in all the decoder layers. 3.2 3.2.1 Experiments Data Preparing We conduct experiments on the following data sets across different languages and domains. Chinese→English. For this task, general-domain data is from the LDC corpus1 that contains 1.25M sentence pairs. The LDC data is mainly related to the News domain. MT06 and MT02 are chosen as the development and test data, respectively. We choose the parallel sentences with the domain label Laws from the UM-Corpus (Tian et al., 2014) as our in-domain data. We filter out repeated sentences and chose 206K, 2K, and 2K sentences randomly as our training, development, and test data, respectively. We tokenize and lowercase the English sentences with Moses2 scripts. For the Chinese data, we perform word segmentation by using Stanford Segmenter3 . 1 The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06. 2 http://www.statmt.org/moses/ 3 https://nlp.stanford.edu/ 4317 Figure 2: The BLEU of the experiments based on the position of the target module when only freezing the t"
2020.coling-main.381,P19-1580,0,0.0606591,"se important parameters during continual training to avoid catastrophic forgetting. Besides, we can retrain those unimportant parameters to further improve the in-domain translation. 5 Related Work Analyzing Work Recently, much work has been concerned with analyzing and evaluating the NMT model from different perspectives. Shi et al. (2016) investigates how NMT models output target strings of appropriate lengths. Ding et al. (2017) analyzes the contribution of each contextual word to arbitrary hidden states. Qi et al. (2018) analyzes when the pre-trained word embeddings can help in NMT tasks. Voita et al. (2019) analyzes the importance of different attention heads. Bau et al. (2019) investigates the importance and function of different neurons in NMT. Wuebker et al. (2018) finds that a large proportion 4323 of model parameters can be frozen during adaptation with minimal reduction in translation quality by encouraging structured sparsity. Wang and Sennrich (2020) links the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019) to the phenomenon of NMT tends to generate hallucinations under domain shift. Gu et al. (2020) finds that the NMT tends to generate more high-frequ"
2020.coling-main.381,2020.acl-main.326,0,0.0212335,"es how NMT models output target strings of appropriate lengths. Ding et al. (2017) analyzes the contribution of each contextual word to arbitrary hidden states. Qi et al. (2018) analyzes when the pre-trained word embeddings can help in NMT tasks. Voita et al. (2019) analyzes the importance of different attention heads. Bau et al. (2019) investigates the importance and function of different neurons in NMT. Wuebker et al. (2018) finds that a large proportion 4323 of model parameters can be frozen during adaptation with minimal reduction in translation quality by encouraging structured sparsity. Wang and Sennrich (2020) links the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019) to the phenomenon of NMT tends to generate hallucinations under domain shift. Gu et al. (2020) finds that the NMT tends to generate more high-frequency tokens and less low-frequency tokens than reference. Compared with them, this work mainly focuses on investigating the functions of the different modules and parameters in the NMT model during continual training. In this sense, the work of Thompson et al. (2018) is most related to ours, which tries to understand the effectiveness of continued training"
2020.coling-main.381,D18-1104,0,0.0943888,"-domain translation. 5 Related Work Analyzing Work Recently, much work has been concerned with analyzing and evaluating the NMT model from different perspectives. Shi et al. (2016) investigates how NMT models output target strings of appropriate lengths. Ding et al. (2017) analyzes the contribution of each contextual word to arbitrary hidden states. Qi et al. (2018) analyzes when the pre-trained word embeddings can help in NMT tasks. Voita et al. (2019) analyzes the importance of different attention heads. Bau et al. (2019) investigates the importance and function of different neurons in NMT. Wuebker et al. (2018) finds that a large proportion 4323 of model parameters can be frozen during adaptation with minimal reduction in translation quality by encouraging structured sparsity. Wang and Sennrich (2020) links the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019) to the phenomenon of NMT tends to generate hallucinations under domain shift. Gu et al. (2020) finds that the NMT tends to generate more high-frequency tokens and less low-frequency tokens than reference. Compared with them, this work mainly focuses on investigating the functions of the different modules and p"
2020.coling-main.381,P19-1426,1,0.90675,"Missing"
2020.emnlp-main.76,P17-2061,0,0.0657001,"Missing"
2020.emnlp-main.76,P05-1066,0,0.263519,"Missing"
2020.emnlp-main.76,N19-1312,1,0.860959,"nce. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic criteria based on the"
2020.emnlp-main.76,P16-1014,0,0.0417886,"Missing"
2020.emnlp-main.76,P15-1001,0,0.160488,"ent frequencies, which roughly obey the Zipf’s Law (Zipf, 1949). Table 1 shows that there is a serious imbalance between high-frequency tokens and lowfrequency tokens. NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process. 1035 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word based NMT models hav"
2020.emnlp-main.76,W18-5712,0,0.0629753,"Missing"
2020.emnlp-main.76,D13-1176,0,0.243187,"Missing"
2020.emnlp-main.76,kocmi-bojar-2017-curriculum,0,0.0399389,"divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity."
2020.emnlp-main.76,D18-1149,0,0.0285784,"onsists of 1.25M sentence pairs from LDC corpora which has 27.9M Chinese words and 34.5M English words, respectively 2 . The data set MT02 was used as validation and MT03, MT04, MT05, MT06, MT08 were used for the test. We tokenized and lowercased English sentences using the Moses scripts3 , and segmented the Chinese sentences with the Stanford Segmentor4 . The two sides were further segmented into subword units using Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 30K merge operations separately. EN→RO. We used the preprocessed version of the WMT2016 English-Romanian dataset released by Lee et al. (2018) which includes 0.6M sentence pairs. We used news-dev 2016 for validation and news-test 2016 for the test. The two languages shared the same vocabulary generated with 40K merge operations of BPE. EN→DE. The training data is from WMT2016 which consists of about 4.5M sentences pairs with 118M English words and 111M German words. We chose the news test-2013 for validation and newstest 2014 for the test. 32K merge operations BPE were performed on both sides jointly. 4.2 • Baseline. The baseline system was implemented as the base model configuration in Vaswani et al. (2017) strictly. Since our meth"
2020.emnlp-main.76,2015.iwslt-evaluation.11,0,0.0427153,"airs with 118M English words and 111M German words. We chose the news test-2013 for validation and newstest 2014 for the test. 32K merge operations BPE were performed on both sides jointly. 4.2 • Baseline. The baseline system was implemented as the base model configuration in Vaswani et al. (2017) strictly. Since our method is further trained based on the pre-trained model at a low learning rate, we also trained another baseline model following the same procedures as our methods have except that all the target tokens share equal weights in the objective, denoted as Baseline-FT. • Fine Tuning (Luong and Manning, 2015). This model was first trained with all the training sentence pairs and then further trained with sentences containing more low-frequency tokens. To filter out sentences containing more low-frequency tokens, the method in Platanios et al. (2019) was adopted as our judging metric with a small modification: Systems We used the open-source toolkit called Fairseqpy (Edunov et al., 2017) released by Facebook as our Transformer system. 2 The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 3 http://www.statmt.org/moses/ 4 https://nlp.stan"
2020.emnlp-main.76,P16-1100,0,0.0343734,"Missing"
2020.emnlp-main.76,P15-1002,0,0.168608,"s appear with different frequencies, which roughly obey the Zipf’s Law (Zipf, 1949). Table 1 shows that there is a serious imbalance between high-frequency tokens and lowfrequency tokens. NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process. 1035 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word b"
2020.emnlp-main.76,J14-3004,0,0.0273182,"al., 2018), or the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we f"
2020.emnlp-main.76,P02-1040,0,0.106664,"ve (Equation 1), where all the target tokens have the same training weights. Then the model was further trained with the adaptive objective at a low learning rate. The weights were produced by the Exponential form (Equation 8). k) For computing stability, we used Count(y Cmedian instead of Count(yk ) in the weighting function, where Cmedian is the median of the token frequency. • Our K2. This system was trained following the same procedure as system Our Exp except that the training weights were produced by the Chi-Square form (Equation 9). The translation quality was evaluated by 4-gram BLEU (Papineni et al., 2002) with the multi-bleu.pl script. Besides, we used beam search with a beam size of 4 and a length penalty of 0.6 during the decoding process. 4.3 Hyperparameters There are two hyperparameters in our weighting functions, A and T. In our experiments, we fixed A to narrow search space and the overall weight range is [1, e]. We tuned another hyperparameter T on the validation data sets under the criteria proposed in section 3.2. The results are shown in Table 3. According to the results, the best hyperparameters differed across different language pairs. It is affected by the proportion of low-freque"
2020.emnlp-main.76,C18-1265,0,0.0163854,"et al., 2018). In contrast, our methods can improve the translation performance without extra cost and can be combined with other techniques. Class Imbalance. Class imbalance means the total number of some classes of data is far less than the total number of other classes. This problem can be observed in various tasks (Wei et al., 2013; Johnson and Khoshgoftaar, 2019). In NMT, the class imbalance problem might be the underlying cause of, among others, the gender-biased output problem (Vanmassenhove et al., 2019a), the inability of MT system to handle morphologically richer language correctly (Passban et al., 2018), or the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al.,"
2020.emnlp-main.76,W19-6622,0,0.043297,"slation of the rare words with the help of the memory network or the pointer network (Zhao et al., 2018; Pham et al., 2018). In contrast, our methods can improve the translation performance without extra cost and can be combined with other techniques. Class Imbalance. Class imbalance means the total number of some classes of data is far less than the total number of other classes. This problem can be observed in various tasks (Wei et al., 2013; Johnson and Khoshgoftaar, 2019). In NMT, the class imbalance problem might be the underlying cause of, among others, the gender-biased output problem (Vanmassenhove et al., 2019a), the inability of MT system to handle morphologically richer language correctly (Passban et al., 2018), or the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work als"
2020.emnlp-main.76,W18-2712,0,0.0299546,"Missing"
2020.emnlp-main.76,I08-2084,0,0.00895525,"16; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the"
2020.emnlp-main.76,N19-1119,0,0.109348,"Missing"
2020.emnlp-main.76,P16-1162,0,0.864739,"rocessing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word based NMT models have achieved significant improvements, they still face the token-level frequency imbalance phenomenon, as Table 1 shows. Furthermore, current NMT models generally assign equal training weights to target tokens without considering their frequencies. It is very likely for NMT models to ignore the loss produced by the low-frequency tokens because of their small proportion in the tra"
2020.emnlp-main.76,D18-1510,1,0.904744,"Missing"
2020.emnlp-main.76,D17-1155,0,0.019346,"er- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then pro"
2020.emnlp-main.76,2020.acl-main.278,0,0.035646,"based methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic criteria based on the observations. Next,"
2020.emnlp-main.76,C18-1269,0,0.017616,"ng to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic cri"
2020.emnlp-main.76,P19-1426,1,0.880268,"Missing"
2020.emnlp-main.76,D18-1036,0,0.0674838,"ance between high-frequency tokens and lowfrequency tokens. NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process. 1035 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word based NMT models have achieved significant improvements, they still face the token-level frequency imbalance phenomenon, as Table"
2020.emnlp-main.82,N12-1017,0,0.0167513,"uthillier et al., 2015). Gal and Ghahramani (2016) proposes that dropout can be understood as a bayesian inferences algorithm, and Gal et al. (2017) uses concrete dropout in updating dropout probabilities. Also, the author implements the dropout methods to represent uncertainty in different kinds of deep learning tasks in Gal (2016). In neural machine translation task, lack of diversity is a widely acknowledged problem, some researches like Ott et al. (2018) investigate the cause of uncertainty in NMT, and some provide metrics to evaluate the translation uncertainty like Galley et al. (2015); Dreyer and Marcu (2012). There are also other researches that put forward methods to obtain diverse translation. Li et al. (2016); Vijayakumar et al. (2016) adjust decoding algorithms, adding different kinds of diversity regularization terms to encourage generating diverse outputs. He et al. (2018); Shen et al. (2019) utilize mixture of experts (MoE) method, using differentiated latent variables to control generation of translation. Sun et al. (2019) generates diverse translation by sampling heads in encoder-decoder attention module in Transformer model, since different heads may present different target-source alig"
2020.emnlp-main.82,P15-2073,0,0.0202454,"ting training data (Bouthillier et al., 2015). Gal and Ghahramani (2016) proposes that dropout can be understood as a bayesian inferences algorithm, and Gal et al. (2017) uses concrete dropout in updating dropout probabilities. Also, the author implements the dropout methods to represent uncertainty in different kinds of deep learning tasks in Gal (2016). In neural machine translation task, lack of diversity is a widely acknowledged problem, some researches like Ott et al. (2018) investigate the cause of uncertainty in NMT, and some provide metrics to evaluate the translation uncertainty like Galley et al. (2015); Dreyer and Marcu (2012). There are also other researches that put forward methods to obtain diverse translation. Li et al. (2016); Vijayakumar et al. (2016) adjust decoding algorithms, adding different kinds of diversity regularization terms to encourage generating diverse outputs. He et al. (2018); Shen et al. (2019) utilize mixture of experts (MoE) method, using differentiated latent variables to control generation of translation. Sun et al. (2019) generates diverse translation by sampling heads in encoder-decoder attention module in Transformer model, since different heads may present dif"
2020.emnlp-main.82,P17-1012,0,0.0164332,"on, which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling. With variational inference, the posterior model distribution can be approximated with a variational distribution, from which the final models for inference are sampled. We conducted experiments on Chinese-English and EnglishGerman translation tasks and the results shows that our method makes a better trade-off between diversity and accuracy. 1 Introduction In the past several years, neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Gehring et al., 2017; Vaswani et al., 2017; Zhang et al., 2019) based on the end-to-end model has achieved impressive progress in improving the accuracy of translation. Despite its remarkable success, NMT still faces problems in diversity. In natural language, due to lexical, syntactic and synonymous factors, there are usually multiple proper translations for a sentence. However, existing NMT models mostly implement one-to-one mapping between natural languages, that is, one source language sentence corresponds to one target language sentence. Although beam search, a widely used decoding algorithm, can generate a"
2020.emnlp-main.82,K18-1056,0,0.11566,"existing NMT models mostly implement one-to-one mapping between natural languages, that is, one source language sentence corresponds to one target language sentence. Although beam search, a widely used decoding algorithm, can generate a group of translations, its search space is too narrow to extract diverse translations. ∗ Corresponding author: Yang Feng There are some researches working at enhancing translation diversity in recent years. Li et al. (2016) and Vijayakumar et al. (2016) proposed to add regularization terms to the beam search algorithm so that it can possess greater diversity. He et al. (2018) and Shen et al. (2019) introduced latent variables into the NMT model, thus the model can generate diverse outputs using different latent variables. Moreover, Sun et al. (2019) proposed to combine the structural characteristics of Transformer and use the different weights between each head in the multi-head attention mechanism to obtain diverse results. In spite of improvement in balancing accuracy and diversity, these methods do not represent the diversity in the NMT model directly. In this paper, we take a different approach to generate diverse translation by explicitly maintaining differen"
2020.emnlp-main.82,D13-1176,0,0.0591599,"nd each of them has specific confidence for its prediction, which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling. With variational inference, the posterior model distribution can be approximated with a variational distribution, from which the final models for inference are sampled. We conducted experiments on Chinese-English and EnglishGerman translation tasks and the results shows that our method makes a better trade-off between diversity and accuracy. 1 Introduction In the past several years, neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Gehring et al., 2017; Vaswani et al., 2017; Zhang et al., 2019) based on the end-to-end model has achieved impressive progress in improving the accuracy of translation. Despite its remarkable success, NMT still faces problems in diversity. In natural language, due to lexical, syntactic and synonymous factors, there are usually multiple proper translations for a sentence. However, existing NMT models mostly implement one-to-one mapping between natural languages, that is, one source language sentence corresponds to one target language sentence. Although beam search, a w"
2020.emnlp-main.82,P07-2045,0,0.0356511,"d contains about 1.34 million sentence pairs. It also includes 6 relatively small datasets, MT02, MT03, MT04, MT05, MT06, and MT08. In our experiments, we use MT02 as the development set, and the rest work as the test sets. Without special explanation, we use average result of test sets as final results. • WMT’14 English-to-German (WMT’14 EnDe). Its dataset comes from the WMT’14 news translation task, which contains about 4.5 million sentence pairs. In our experiment, we use newstest2013 as the development set and newstest2014 as the test set. For above two datasets, We adopt Moses tokenizer (Koehn et al., 2007) in English and German corpus. We also use the byte pair encoding (BPE) algorithm (Sennrich et al., 2015), and limit the size of the vocabulary K = 32000. And we train a joint dictionary for WMT’14 En-De. For NIST, we use THULAC toolkit (Sun et al., 2016) to segment Chinese sentence into words. In addition, we remove the examples in datasets from the above two tasks where length of the source language sentence or target language sentence exceed 100 words. Model Architecture In our experiments, we all adopt the Transformer Base model in Vaswani et al. (2017). Transformer base model has 6 layers"
2020.emnlp-main.82,P19-1177,0,0.141016,"are also other researches that put forward methods to obtain diverse translation. Li et al. (2016); Vijayakumar et al. (2016) adjust decoding algorithms, adding different kinds of diversity regularization terms to encourage generating diverse outputs. He et al. (2018); Shen et al. (2019) utilize mixture of experts (MoE) method, using differentiated latent variables to control generation of translation. Sun et al. (2019) generates diverse translation by sampling heads in encoder-decoder attention module in Transformer model, since different heads may present different target-source alignment. Shu et al. (2019) uses sentence codes to condition translation generation and obtain diverse translations. Shao et al. (2018) propose a new probabilistic ngrambased loss to conduct sequence-level training for generating diverse translation.Feng et al. (2020) propose to employ future information to evaluate fluency and faithfulness to encourage diverse translation. There are also a few papers in interpreting Transformer model, Voita et al. (2019) suggests that some heads play a consistent role in machine translation, and their roles can be interpreted linguistically; also, they implement L0 penalty to prune hea"
2020.emnlp-main.82,N19-4009,0,0.0247787,"e remove the examples in datasets from the above two tasks where length of the source language sentence or target language sentence exceed 100 words. Model Architecture In our experiments, we all adopt the Transformer Base model in Vaswani et al. (2017). Transformer base model has 6 layers in encoder and decoder, and it has hidden units with 512 dimension, except for the feed-forward network, where the inner-layer output dimension is 2048. The number of heads in Transformer base model is 8 and the default dropout probability is 0.1. And our model is implemented in python3 with the Fairseq-py (Ott et al., 2019) toolkit. Experimental Setting During training, in order to improve the accuracy, we use the label smoothing (Szegedy et al., 2016) with  = 0.1. In terms of optimizer, we adopt the Adam optimizer (Kingma and Ba, 2014), the main parameters of the optimizer is β1 = 0.9, β2 = 0.98, and  = 10−9 . As for the learning rate, we adopt the dynamic learning rate method in Vaswani et al. (2017) with warmup steps = 4000. Also, we use mini-batch training with max token = 4096. Metrics In terms of evaluation metrics, referring to Shen et al. (2019), we adopt the BLEU and Pairwise-BLEU to evaluate translat"
2020.emnlp-main.82,P02-1040,0,0.106495,"smoothing (Szegedy et al., 2016) with  = 0.1. In terms of optimizer, we adopt the Adam optimizer (Kingma and Ba, 2014), the main parameters of the optimizer is β1 = 0.9, β2 = 0.98, and  = 10−9 . As for the learning rate, we adopt the dynamic learning rate method in Vaswani et al. (2017) with warmup steps = 4000. Also, we use mini-batch training with max token = 4096. Metrics In terms of evaluation metrics, referring to Shen et al. (2019), we adopt the BLEU and Pairwise-BLEU to evaluate translation quality and diversity. Both two metrics are calculated with case-insensitive BLEU algorithm in Papineni et al. (2002). In our experiments, the BLEU is to measure the average similarity between the output translations and the standard translation. The higher the BLEU value, the better the accuracy of translation. And the Pairwise-BLEU reflects the average similarity between the output translations of different groups. The lower the Pairwise-BLEU value, the lower the similarities, and the more diverse the translations. In our experiment, we use the NLTK toolkit to calculate the two metrics. 6 6.1 Experiment Results Analysis of Training Modules and Hyper-parameter In this experiment, we train models with differ"
2020.emnlp-main.82,P19-1580,0,0.128278,"represent such distinguishable characteristics like HardMoE, which trains multiple different latent variables. Also, to intuitively display the improvement of diversity in our translations, we choose a case from NIST Zh-En task, the results are shown in Table 2. The case shows that compared with beam search, which only varies in few words, our method can obtain more diverse translations while ensuring the accuracy of translation, and diversities are not only shown in words, but also reflected in lexical characteristic. 6.3 Analyzing Module Importance with Dropout Probability Some researches (Voita et al., 2019; Michel et al., 2019; Fan et al., 2019) found that a well-trained Transformer model is over-parameterized. Useful information gathers in some parameters and some modules and layers can be pruned to improve the efficiency during test time. Since dropout can play the role of regularization and there are differences in the trained dropout probabilities of different neuron, we conjecture that the trained dropout probability and the importance of each module are correlated. To investigate this, we choose the model in which dropout probabilities of the full model is trained with l2 = 400 in NIST Zh"
2020.emnlp-main.82,D18-1510,1,0.896149,"Missing"
2020.emnlp-main.82,P19-1426,1,0.848552,"Missing"
2021.acl-long.11,N19-1423,0,0.0485602,"CGnlHeMQTno2qcWvcGfefqcZCptnDt2U8fACMX5RM</latexit> <latexit Figure 1: Illustration of the dynamic information flow in the semantic space. Ck (gray dotted line) denotes the dense representation of dialogue history which we named as context. Ik (pink solid line) denotes the semantic influence brought about by the k-th utterance, which is the difference between Ck and Ck+1 . Introduction Recent intelligent open-domain chatbots (Adiwardana et al., 2020; Bao et al., 2020; Smith et al., 2020) have made substantial progress thanks to the rapid development of the large-scale pre-training approaches (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and the large amount of conversational data (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020). However, ∗ Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Yang Feng is the corresponding author. Work was done when Zekang Li and Zhengcong Fei were intern at WeChat AI. 1 https://github.com/ictnlp/DialoFlow effectively modeling the dialogue history in largescale dialogue pre-training is still challenging. Most of the previous work on dialogue history modeling mainly fall into two groups. One group of works generally co"
2021.acl-long.11,W19-5944,0,0.0330157,"Missing"
2021.acl-long.11,W07-0734,0,0.111193,"Missing"
2021.acl-long.11,I17-1099,0,0.0267118,"ted by a third party and made publicly available on pushshift.io (Baumgartner et al., 2020). We clean the data following the pipeline used in the DialoGPT.2 For response generation, we employ the multireference Reddit Test Dataset (Zhang et al., 2020) which contains 6k examples with multiple references. We evaluate our pre-trained DialoFlow model on this dataset. The average length of the dialogue history in this dataset is 1.47. To further explore the dynamic information flow in the long dialogue history situation, we choose another popular open-domain dialogue dataset – DailyDialog Dataset (Li et al., 2017), in which the average dialogue history length is about 4.66. DialoFlow is fine-tuned on the DailyDialog training set and evaluated on the DailyDialog multi-reference test set (Gupta et al., 2019). For interactive dialogue quality evaluation, we employ the collected data from the Interactive Evaluation of Dialog Track @ The Ninth Dialog System Technology Challenge (DSTC9) (Gunasekara et al., 2021), which contains 2200 human-bot conversations from 11 chatbots. For each conversation, there are 3 human ratings on the overall quality (0-5). We calculate the correlation between the results of our p"
2021.acl-long.11,P19-1002,1,0.84599,"ence brought about by each utterance. Besides, it seems like that different speakers keep their own context flows. 5 Related Works Multi-turn dialogue modeling. The modeling of multi-turn dialogue history mainly falls into two categories: 1) Flat concatenation. These works directly concatenate the dialogue history as the input sequence (Zhang et al., 2020), which can not capture the information dynamics. 2) Hierarchical architectures. The hierarchical architecture is commonly used in the dialogue history understanding. Serban et al. (2016a) propose the hierarchical LSTM to generate responses. Li et al. (2019) introduce an incremental transformer to capture multi-turn dependencies. Shan et al. (2020); Gu et al. (2020) employ pre-trained BERT to encode individual utterances and design the utterance135 level encoder to capture the turn-level structure. These methods suffer from the lack of context wordlevel information when encoding utterances. Different from these methods, our DialoFlow takes full advantage of both word-level information and utterance-level dynamic information. Besides, the proposed DialoFlow is pre-trained on the largescale open-domain dialogue dataset. Pre-trained models for dialo"
2021.acl-long.11,P04-1077,0,0.105018,"Missing"
2021.acl-long.11,2020.sigdial-1.28,0,0.0804706,"Missing"
2021.acl-long.11,P02-1040,0,0.10886,"Missing"
2021.acl-long.11,P19-1004,0,0.0222185,"ent Inc. Yang Feng is the corresponding author. Work was done when Zekang Li and Zhengcong Fei were intern at WeChat AI. 1 https://github.com/ictnlp/DialoFlow effectively modeling the dialogue history in largescale dialogue pre-training is still challenging. Most of the previous work on dialogue history modeling mainly fall into two groups. One group of works generally concatenate the dialogue history as the model input and predict the response (Zhang et al., 2020; Smith et al., 2020; Bao et al., 2020), named as flat pattern, which is commonly adopted in the large-scale pre-training. However, Sankar et al. (2019) demonstrate that flat concatenation is likely to ignore the conversational dynamics across utterances in the dialogue history. Another group of works employ hierarchical modeling to encode the dialogue history (Serban et al., 2016b; Shan et al., 2020; Gu et al., 2020), in which the utterances are separately encoded and then fed into an utterance-level encoder. These approaches lack the history information when encoding each individual utterance, while the history information is essential for understanding dialogue utterances. Thus, all the aforementioned methods are deficient in modeling the"
2021.acl-long.11,P16-1056,0,0.0354585,"Missing"
2021.acl-long.11,2020.acl-main.563,1,0.895436,"ing. Most of the previous work on dialogue history modeling mainly fall into two groups. One group of works generally concatenate the dialogue history as the model input and predict the response (Zhang et al., 2020; Smith et al., 2020; Bao et al., 2020), named as flat pattern, which is commonly adopted in the large-scale pre-training. However, Sankar et al. (2019) demonstrate that flat concatenation is likely to ignore the conversational dynamics across utterances in the dialogue history. Another group of works employ hierarchical modeling to encode the dialogue history (Serban et al., 2016b; Shan et al., 2020; Gu et al., 2020), in which the utterances are separately encoded and then fed into an utterance-level encoder. These approaches lack the history information when encoding each individual utterance, while the history information is essential for understanding dialogue utterances. Thus, all the aforementioned methods are deficient in modeling the dynamic information in the dialogue history. In this work, inspired by the human cognitive 128 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Proc"
2021.acl-long.11,2020.acl-main.183,0,0.216897,"l26W+V+dqkWijzNdg6CaIs2o6ljmkuquqJubX6qS5BARp3CP4jFhppWzPptak+jaVW8dHX/TmYpVe5blpnhXt6QB2z/HOQ/qxyXbKtnVk2L5PBt1Dvs4wBHN8xRlXKKCGnlHeMQTno2qcWvcGfefqcZCptnDt2U8fACMX5RM</latexit> <latexit Figure 1: Illustration of the dynamic information flow in the semantic space. Ck (gray dotted line) denotes the dense representation of dialogue history which we named as context. Ik (pink solid line) denotes the semantic influence brought about by the k-th utterance, which is the difference between Ck and Ck+1 . Introduction Recent intelligent open-domain chatbots (Adiwardana et al., 2020; Bao et al., 2020; Smith et al., 2020) have made substantial progress thanks to the rapid development of the large-scale pre-training approaches (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and the large amount of conversational data (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020). However, ∗ Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Yang Feng is the corresponding author. Work was done when Zekang Li and Zhengcong Fei were intern at WeChat AI. 1 https://github.com/ictnlp/DialoFlow effectively modeling the dialogue history in largescale dialogue pre-training is still c"
2021.acl-long.11,2020.acl-demos.30,0,0.465497,"tional data (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020). However, ∗ Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Yang Feng is the corresponding author. Work was done when Zekang Li and Zhengcong Fei were intern at WeChat AI. 1 https://github.com/ictnlp/DialoFlow effectively modeling the dialogue history in largescale dialogue pre-training is still challenging. Most of the previous work on dialogue history modeling mainly fall into two groups. One group of works generally concatenate the dialogue history as the model input and predict the response (Zhang et al., 2020; Smith et al., 2020; Bao et al., 2020), named as flat pattern, which is commonly adopted in the large-scale pre-training. However, Sankar et al. (2019) demonstrate that flat concatenation is likely to ignore the conversational dynamics across utterances in the dialogue history. Another group of works employ hierarchical modeling to encode the dialogue history (Serban et al., 2016b; Shan et al., 2020; Gu et al., 2020), in which the utterances are separately encoded and then fed into an utterance-level encoder. These approaches lack the history information when encoding each individual utteranc"
2021.acl-long.223,P05-1066,0,0.0704582,"Missing"
2021.acl-long.223,D18-1048,0,0.0174354,"ic n-gram based GLEU (Wu et al., 2016) and Shao et al. (2020) propose to minimize the bag-of-ngrams difference for nonautoregressive NMT so that the two methods can abandon reinforcement learning and perform training directly by gradient descent. Another set of methods introduce future information into inference with additional pass of decoding or extra components at test. Niehues et al. (2016), Xia et al. (2017), Hassan et al. (2018) and Zhang et al. (2018) proposed a two-pass decoding algorithm to first generate a draft translation and then generate final translation referring to the draft. Geng et al. (2018) expand this line of methods by performing an adaptive multi-pass decoding where the number of decoding passes is determined by a policy network. Liu et al. (2016a), Liu et al. (2016b), Hoang et al. (2017), Zhang et al. (2019d) and He et al. (2019) perform bidirectional decoding simultaneously and the two decoders correlate to each other via an agreement term or a regularization term in the loss. Zhou et al. (2019a) , Zhou et al. (2019b) and Zhang et al. (2019b) also maintain a forward decoder and a backward decoder to decode simultaneously but they interact to each other when making predictio"
2021.acl-long.223,N19-1312,1,0.800023,"is a linear k=1 i=1 transformation. We first pretrained the two decoders together only with L = Lt +Ls , then trained them with the loss of L = Lt + Ls + αL2 where α = 0.2, too. Please note that the L2 loss did not update the seer decoder and the encoder so that the conventional decoder would approach the seer decoder, which followed Serdyuk et al. (2018). S EER +AL Seer forcing with adversarial learning. A discriminator is employed to distinguish the hidden state sequences generated by the conventional decoder and the seer decoder. The discriminator is based on CNN, implemented according to Gu et al. (2019). The translation model and the discriminator are trained jointly via a gradient reversal layer just like our method. The loss is L = Lt + Ls + αLd where Ld is the loss of the discriminator and α = 0.3 on the EN→RO data set and α = 0.2 on the other data sets. 2866 T RANSFORMER RL-NMT ABDNMT T WINNET EVANMT S EER +L2 S EER +AL Our Method MT03 46.54 45.75 47.16 47.78 47.05 47.98** 47.91** 48.12** MT04 46.95 47.41 47.58 48.74 47.76 48.66** 48.38** 48.85** MT05 46.39 46.44 46.77 48.59 46.59 48.16** 47.97** 48.25** CN→EN MT06 MT08 45.39 36.75 47.08 37.65 45.97 36.43 46.65 38.80 46.58 37.39 47.02**"
2021.acl-long.223,D19-1079,0,0.0190932,"ther set of methods introduce future information into inference with additional pass of decoding or extra components at test. Niehues et al. (2016), Xia et al. (2017), Hassan et al. (2018) and Zhang et al. (2018) proposed a two-pass decoding algorithm to first generate a draft translation and then generate final translation referring to the draft. Geng et al. (2018) expand this line of methods by performing an adaptive multi-pass decoding where the number of decoding passes is determined by a policy network. Liu et al. (2016a), Liu et al. (2016b), Hoang et al. (2017), Zhang et al. (2019d) and He et al. (2019) perform bidirectional decoding simultaneously and the two decoders correlate to each other via an agreement term or a regularization term in the loss. Zhou et al. (2019a) , Zhou et al. (2019b) and Zhang et al. (2019b) also maintain a forward decoder and a backward decoder to decode simultaneously but they interact to each other when making predictions. Zhang et al. (2019a) introduce a future-aware vector at test which is learned via the knowledge distillation framework during training. The difference between this set of methods and our method is that our method does not require any other cost"
2021.acl-long.223,P84-1044,0,0.417173,"Missing"
2021.acl-long.223,D17-1014,0,0.0199313,"orm training directly by gradient descent. Another set of methods introduce future information into inference with additional pass of decoding or extra components at test. Niehues et al. (2016), Xia et al. (2017), Hassan et al. (2018) and Zhang et al. (2018) proposed a two-pass decoding algorithm to first generate a draft translation and then generate final translation referring to the draft. Geng et al. (2018) expand this line of methods by performing an adaptive multi-pass decoding where the number of decoding passes is determined by a policy network. Liu et al. (2016a), Liu et al. (2016b), Hoang et al. (2017), Zhang et al. (2019d) and He et al. (2019) perform bidirectional decoding simultaneously and the two decoders correlate to each other via an agreement term or a regularization term in the loss. Zhou et al. (2019a) , Zhou et al. (2019b) and Zhang et al. (2019b) also maintain a forward decoder and a backward decoder to decode simultaneously but they interact to each other when making predictions. Zhang et al. (2019a) introduce a future-aware vector at test which is learned via the knowledge distillation framework during training. The difference between this set of methods and our method is that"
2021.acl-long.223,D13-1176,0,0.0498083,"dge distillation. In this way, at test the conventional decoder can perform like the seer decoder without the attendance of it. Experiment results on the Chinese-English, English-German and English-Romanian translation tasks show our method can outperform competitive baselines significantly and achieves greater improvements on the bigger data sets. Besides, the experiments also prove knowledge distillation the best way to transfer knowledge from the seer decoder to the conventional decoder compared to adversarial learning and L2 regularization. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) has achieved great success and is drawing larger attention recently. Most NMT models are under the attention-based encoder-decoder framework which assumes there is a common semantic space between the source and target languages. The encoder encodes the source sentence to the common space to get its meaning, and the decoder projects the source meaning to the target space to generate corresponding target words. Whenever generating a target word at a time step, the decoder ∗ The code: https://github.com/i"
2021.acl-long.223,D18-1149,0,0.0158255,"th 27.9M Chinese words and 34.5M English words respectively1 . We used MT02 for validation and MT03, MT04, MT05, MT06, MT08 for test. We tokenized and lowercased English sentences using the Moses scripts2 , and segmented the Chinese sentences with the Stanford Segmentor3 . The two sides were further segmented into subword units using Byte-Pair Encoding(BPE) (Sennrich et al., 2016) with 30K merge operations. 32K size of the Chinese dictionary and 29K size of the English dictionary were built for the two sides. English→Romanian We used the preprocessed version of WMT16 En-Ro dataset released by Lee et al. (2018) which includes 0.6M sentence pairs. We used news-dev 2016 for validation and newstest 2016 for test. The two languages share the 35K size of the joint vocabulary generated with 40K merge operations of BPE on the combined data. Big Data Sets Chinese→English The training data is from WMT 2017 Zh-En translation tasks that contains 20.18M sentence pairs after deleting duplicate ones. The newsdev2017 was used as the development set and newstest2017 was used as the test set. To avoid the effects of the translationese (Graham et al., 2019), we also tested the methods on the newstest2019 test set. We"
2021.acl-long.223,N16-1046,0,0.029461,"abandon reinforcement learning and perform training directly by gradient descent. Another set of methods introduce future information into inference with additional pass of decoding or extra components at test. Niehues et al. (2016), Xia et al. (2017), Hassan et al. (2018) and Zhang et al. (2018) proposed a two-pass decoding algorithm to first generate a draft translation and then generate final translation referring to the draft. Geng et al. (2018) expand this line of methods by performing an adaptive multi-pass decoding where the number of decoding passes is determined by a policy network. Liu et al. (2016a), Liu et al. (2016b), Hoang et al. (2017), Zhang et al. (2019d) and He et al. (2019) perform bidirectional decoding simultaneously and the two decoders correlate to each other via an agreement term or a regularization term in the loss. Zhou et al. (2019a) , Zhou et al. (2019b) and Zhang et al. (2019b) also maintain a forward decoder and a backward decoder to decode simultaneously but they interact to each other when making predictions. Zhang et al. (2019a) introduce a future-aware vector at test which is learned via the knowledge distillation framework during training. The difference between"
2021.acl-long.223,C16-1172,0,0.0641061,"Missing"
2021.acl-long.223,N19-4009,0,0.0411337,"Missing"
2021.acl-long.223,2020.findings-emnlp.217,0,0.0554663,"Missing"
2021.acl-long.223,P16-1162,0,0.0564377,"formation. 5 Experiments 5.1 5.1.1 Settings Data Preparation We conducted experiments on two small data sets and two big data sets. Small Data Sets Chinese→English The training set consists of about 1.25M sentence pairs from LDC corpora with 27.9M Chinese words and 34.5M English words respectively1 . We used MT02 for validation and MT03, MT04, MT05, MT06, MT08 for test. We tokenized and lowercased English sentences using the Moses scripts2 , and segmented the Chinese sentences with the Stanford Segmentor3 . The two sides were further segmented into subword units using Byte-Pair Encoding(BPE) (Sennrich et al., 2016) with 30K merge operations. 32K size of the Chinese dictionary and 29K size of the English dictionary were built for the two sides. English→Romanian We used the preprocessed version of WMT16 En-Ro dataset released by Lee et al. (2018) which includes 0.6M sentence pairs. We used news-dev 2016 for validation and newstest 2016 for test. The two languages share the 35K size of the joint vocabulary generated with 40K merge operations of BPE on the combined data. Big Data Sets Chinese→English The training data is from WMT 2017 Zh-En translation tasks that contains 20.18M sentence pairs after deletin"
2021.acl-long.223,D18-1510,1,0.896087,"Missing"
2021.acl-long.223,P19-1288,1,0.854445,"ut we still follow the above rule to keep the teacher (i.e. the seer decoder) unchanged in the process of distillation. To do this, we do not update the parameters of the seer decoder through the loss Lkd , that is, we only back propagate gradients to the seer decoder through Ls , but not through Lkd . 4 Related Work Reinforcement-learning-based methods also encode future information in the rewards to supervise fine-tuning of the translation model. The rewards are worked out either by sampling future translation with the REINFORCE algorithm (Williams, 1992; Yu et al., 2017; Yang et al., 2018; Shao et al., 2019), or by directly calculating a value with the actor-critic algorithm (Bahdanau et al., 2016; Li et al., 2017). This set of methods only give a weak supervision to the NMT model through rewards and suffer from unstable training. In contrast, Shao et al. (2018) propose to train autoregressive NMT with the probabilistic n-gram based GLEU (Wu et al., 2016) and Shao et al. (2020) propose to minimize the bag-of-ngrams difference for nonautoregressive NMT so that the two methods can abandon reinforcement learning and perform training directly by gradient descent. Another set of methods introduce futu"
2021.acl-long.223,P02-1040,0,0.116369,"+1.05 +0.45 +0.61 +1.14 T IME 1.0 2.57 2.48 1.53 2.39 1.57 Table 2: BLEU scores on big data sets. * and ** mean the improvements over T RANSFORMER is statistically significant (Collins et al., 2005) (ρ &lt; 0.05 and ρ &lt; 0.01, respectively). Our Method Implemented based on Fairseqpy. The weight λ in Equation 6 for the small Chinese→English data set is set to 0.25, and for other data sets is set to 0.5. All the Transformer-based systems have the same configuration as the base model described in Vaswani et al. (2017) except that dropout rate is 0.3. The translation quality was evaluated with BLEU (Papineni et al., 2002) with n=4 using the SacreBLEU tool (Post, 2018)4 , where small data sets employ case-insensitive BLEU while big data sets use case-sensitive BLEU. 5.2 Main Results We compare our method with other methods that can make global planning, including the reinforcement-based method (RL-NMT), the twopass decoding method (ABDNMT), twin networks which match past and future information (T WINNET) and the NMT model with an evaluate module to evaluate fluency and faithfulness (EVANMT). In addition, we also explore learning mechanisms which can transfer knowledge from the seer decoder to the conventional d"
2021.acl-long.223,W18-6319,0,0.0123242,"7 Table 2: BLEU scores on big data sets. * and ** mean the improvements over T RANSFORMER is statistically significant (Collins et al., 2005) (ρ &lt; 0.05 and ρ &lt; 0.01, respectively). Our Method Implemented based on Fairseqpy. The weight λ in Equation 6 for the small Chinese→English data set is set to 0.25, and for other data sets is set to 0.5. All the Transformer-based systems have the same configuration as the base model described in Vaswani et al. (2017) except that dropout rate is 0.3. The translation quality was evaluated with BLEU (Papineni et al., 2002) with n=4 using the SacreBLEU tool (Post, 2018)4 , where small data sets employ case-insensitive BLEU while big data sets use case-sensitive BLEU. 5.2 Main Results We compare our method with other methods that can make global planning, including the reinforcement-based method (RL-NMT), the twopass decoding method (ABDNMT), twin networks which match past and future information (T WINNET) and the NMT model with an evaluate module to evaluate fluency and faithfulness (EVANMT). In addition, we also explore learning mechanisms which can transfer knowledge from the seer decoder to the conventional decoder, including L2 regularization (S EER +L2"
2021.acl-long.223,D18-1397,0,0.0194376,"the NMT model with an evaluate module to evaluate fluency and faithfulness (EVANMT). In addition, we also explore learning mechanisms which can transfer knowledge from the seer decoder to the conventional decoder, including L2 regularization (S EER +L2 ), adversarial learning (S EER +AL) and knowledge distillation (Our Method). We report results together with training time on the small and big data sets in Table 1 and Table 2, respectively.5 As for different methods, in the small data sets, RL-NMT can only get small improvements over Transformer which are in line with the results reported in Wu et al. (2018), and ABDNMT cannot get consistent improvements over Transformer with an obvious difference on the EN→RO data set and a small difference on the CN→EN data set. T WINNET can get comparable BLEU scores with our method on the small data sets but mostly negative difference on the big data sets. EVANMT can achieve consistent improvements and greater improvements on the EN→DE data set. For the learning mechanisms, knowledge distillation show consistent superiority over L2 regularization and adversarial learning, which is remarkable especially on the big data sets. Adversarial learning can bring impr"
2021.acl-long.223,D19-1086,0,0.0154517,"n is conducted from the perspective of fluency and faithfulness which both need the participation of past and future information. The difference from the method proposed in this paper is their method uses self-generated translation as past information and does not train with knowledge distillation. Some researchers work in another perspective by introducing future information. Zhang et al. (2020b) propose to employ future source information to guide simultaneous machine translation with knowledge distillation, so that the incompleteness of source can be mitigated. Zheng et al. (2018) and 2865 Zheng et al. (2019) propose to model past and future information for the source to help the decoder focus on untranslated source information. 5 Experiments 5.1 5.1.1 Settings Data Preparation We conducted experiments on two small data sets and two big data sets. Small Data Sets Chinese→English The training set consists of about 1.25M sentence pairs from LDC corpora with 27.9M Chinese words and 34.5M English words respectively1 . We used MT02 for validation and MT03, MT04, MT05, MT06, MT08 for test. We tokenized and lowercased English sentences using the Moses scripts2 , and segmented the Chinese sentences with t"
2021.acl-long.223,Q18-1011,0,0.0187661,"e ground truth. The evaluation is conducted from the perspective of fluency and faithfulness which both need the participation of past and future information. The difference from the method proposed in this paper is their method uses self-generated translation as past information and does not train with knowledge distillation. Some researchers work in another perspective by introducing future information. Zhang et al. (2020b) propose to employ future source information to guide simultaneous machine translation with knowledge distillation, so that the incompleteness of source can be mitigated. Zheng et al. (2018) and 2865 Zheng et al. (2019) propose to model past and future information for the source to help the decoder focus on untranslated source information. 5 Experiments 5.1 5.1.1 Settings Data Preparation We conducted experiments on two small data sets and two big data sets. Small Data Sets Chinese→English The training set consists of about 1.25M sentence pairs from LDC corpora with 27.9M Chinese words and 34.5M English words respectively1 . We used MT02 for validation and MT03, MT04, MT05, MT06, MT08 for test. We tokenized and lowercased English sentences using the Moses scripts2 , and segmented"
2021.acl-long.223,N18-1122,0,0.0449366,"Missing"
2021.acl-long.223,2020.acl-main.227,0,0.0277065,"proven in the experiments more effective than L2 regularization. Feng et al. (2020) introduce an evaluation module to give each translation more reasonable evaluation when it cannot match the ground truth. The evaluation is conducted from the perspective of fluency and faithfulness which both need the participation of past and future information. The difference from the method proposed in this paper is their method uses self-generated translation as past information and does not train with knowledge distillation. Some researchers work in another perspective by introducing future information. Zhang et al. (2020b) propose to employ future source information to guide simultaneous machine translation with knowledge distillation, so that the incompleteness of source can be mitigated. Zheng et al. (2018) and 2865 Zheng et al. (2019) propose to model past and future information for the source to help the decoder focus on untranslated source information. 5 Experiments 5.1 5.1.1 Settings Data Preparation We conducted experiments on two small data sets and two big data sets. Small Data Sets Chinese→English The training set consists of about 1.25M sentence pairs from LDC corpora with 27.9M Chinese words and 3"
2021.acl-long.223,1983.tc-1.13,0,0.720682,"Missing"
2021.acl-long.223,P19-1426,1,0.887162,"Missing"
2021.acl-long.271,K16-1002,0,0.021264,"recognition networks can be estimated using the reparameterization trick (Kingma and Welling, 2014). During inference, latent variables obtained via prior networks and predicted question type qt0 are fed to the question decoder, which corresponds to red dashed arrows in Figure 2. The inference process is as follows: 3498 (1) Sample triple-level LV: zt ∼ qφ (zt |p)1 . (2) Sample answer LV: za ∼ pϕ (za |p, zt ). (3) Sample question LV: zq ∼ pϕ (zq |p, zt , za ). (4) Predict question type: qt ∼ pθ (qt|zq , zt , p). (5) Generate question: q ∼ pθ (zq , zt , p, qt). 3 apply KL annealing, word drop (Bowman et al., 2016) and bag-of-word (BOW) loss (Zhao et al., 2017)4 . The KL multiplier λ gradually increases from 0 to 1, and the word drop probability is 0.25. We use Pytorch to implement our model, and the model is trained on Titan Xp GPUs. Experiments In this section, we conduct experiments to evaluate our proposed method. We first introduce some empirical settings, including dataset, hyperparameters, baselines, and evaluation measures. Then we illustrate our results under both automatic and human evaluations. Finally, we give out some cases generated by different models and do further analyses over our meth"
2021.acl-long.271,D17-1070,0,0.0220143,"te Agreement” for all three criteria. mappings in PQ and QA pairs, and relationship modeling for zq and za , GTM can improve the relevance in QA pairs. 3.6 3.7 Question-Answer Coherence Evaluation Automatic metrics in Section “Automatic Metrics” are designed to compare generated questions with ground-truth ones (RUBER also takes the post information into consideration), but ignore answers in the evaluation process. To measure the semantic coherence between generated questions and answers, we apply two methods (Wang et al., 2019): (1) Cosine Similarity: We use the pre-trained Infersent model7 (Conneau et al., 2017) to obtain sentence embeddings and calculate cosine similarity between the embeddings of generated responses 7 The Infersent model is trained to predict the meaning of sentences based on natural language inference, and the cosine similarity computed with it is more consistent with human’s judgements, which performs better than the pre-trained Transformer/BERT model in our experiments. Table 3: Results for human evaluation. Model S2S-Attn CVAE kgCVAE STD HTD RL-CVAE GTM-zt GTM-a GTM-zq /za GTM Cosine Similarity 0.498 0.564 0.578 0.542 0.583 0.607 0.613 0.605 0.618 0.629 Matching Score 5.306 8.0"
2021.acl-long.271,N19-1125,0,0.030195,"Missing"
2021.acl-long.271,W04-3250,0,0.261348,"enerated by GTM are more coherent to answers. Attributing to the design of triple-level latent variable that captures the shared background, one-to-many Model S2S-Attn CVAE kgCVAE STD HTD RL-CVAE GTM-zt GTM-a GTM-zq /za GTM Fluency 0.482 0.462 0.474 0.488 0.526 0.534 0.538 0.532 0.542 0.548 Coherence 0.216 0.484 0.536 0.356 0.504 0.578 0.580 0.570 0.586 0.608 Willingness 0.186 0.428 0.476 0.286 0.414 0.508 0.516 0.512 0.520 0.526 Human Evaluation Results As shown in Table 3, GTM can alleviate the problem of generating dull and deviated questions compared with other models (significance tests (Koehn, 2004), p-value &lt; 0.05). Both our proposed model and the state-of-the-art model RL-CVAE utilize the answer information and the results of them could prove that answers assist the question generation process. Besides, GTM can produce more relevant and intriguing questions, which indicates the effectiveness of modeling the shared background and one-to-many mappings in CQG task. The interannotator agreement is calculated with the Fleiss’ kappa (Fleiss and Cohen, 1973). Fleiss’ kappa for Fluency, Coherence and Willingness is 0.493, 0.446 and 0.512, respectively, indicating “Moderate Agreement” for all t"
2021.acl-long.271,2020.acl-main.20,0,0.138114,"-answer (PQA) triple into post-question (PQ) and question-answer (QA) pairs rather than considering the triple as a whole and modeling the overall coherence. Furthermore, the training process of the matching model only utilizes one-to-one relation of each QA pair and neglects the one-to-many mapping feature. An open-domain PQA often takes place under a background that can be inferred from all utterances in the triple and help enhance the overall coherence. When it comes to the semantic relationship in each triple, the content of a specific question is under the control of its post and answer (Lee et al., 2020). Meanwhile, either a post or an answer could correspond to several meaningful questions. As shown in Table 1, the triple is about a person’s eating activity (the background of the entire conversation). There are one-to-many mappings in both PQ and QA pairs that construct different meaningful combinations, such as P-Q1.1-A1, P-Q1.2-A1, P-Q2.1-A2 and P-Q2.2-A2. An answer connects tightly to both its post and question, and in turn helps decide the expression of a question. On these grounds, we propose a generative triplewise model (GTM) for CQG. Specifically, we firstly introduce a triple-level"
2021.acl-long.271,D19-1317,0,0.13585,"ith users by not only responding but also asking (Li et al., 2017). Besides, raising questions is a proactive way to guide users to go deeper and further into conversations (Yu et al., 2016). Therefore, the ultimate goal of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018). Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗ Yang Feng is the corresponding author. CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is unavailable during inference (Wang et al., 2019). At the same time, each utterance in open-domain scenario is casual and can be followed by several appropriate sentences, i.e., one-to-many mapping (Gao et al., 2019; Chen et al., 2019). At first, the input information of CQG was mainly a given post (Wang et al., 2018; Hu et al., 2018), and the generated questions were usually dull or deviated (Q3 and Q4 in Table 1). Based on"
2021.acl-long.271,N16-1014,0,0.380012,"zp and za . (6) GTMa: the variant of GTM that does not take answer into account. That is, answer decoder and za are removed from the loss function and the prior and posterior distributions of zq . Besides, zt here does not capture the semantics from answer. (7) GTMzq /za : GTM variant in which distributions of zq are not conditioned on za , i.e., the fact that the content of question is also controlled by answer is not modelled explicitly by latent variables. In our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020) Inspired by Park et al. (2018), using zt inferred from post with the posterior distribution is better than sampling it from the prior one, i.e., a standard Gaussian distribution. 2 https://drive.google.com/drive/ folder/1wNG30YPHiMc_ZNyE3BH5wa1uVtR8l1pG 3 http://www.reddit.com 4 The total BOW loss is calculated as the sum of all BOW losses between each latent variable and q/a. Please refer to Park et al. (2018) for more details. 5 For those methods with open-source codes, we run the original codes; otherwise, we re-implement them based on the correspo"
2021.acl-long.271,P16-1094,0,0.169949,"zp and za . (6) GTMa: the variant of GTM that does not take answer into account. That is, answer decoder and za are removed from the loss function and the prior and posterior distributions of zq . Besides, zt here does not capture the semantics from answer. (7) GTMzq /za : GTM variant in which distributions of zq are not conditioned on za , i.e., the fact that the content of question is also controlled by answer is not modelled explicitly by latent variables. In our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020) Inspired by Park et al. (2018), using zt inferred from post with the posterior distribution is better than sampling it from the prior one, i.e., a standard Gaussian distribution. 2 https://drive.google.com/drive/ folder/1wNG30YPHiMc_ZNyE3BH5wa1uVtR8l1pG 3 http://www.reddit.com 4 The total BOW loss is calculated as the sum of all BOW losses between each latent variable and q/a. Please refer to Park et al. (2018) for more details. 5 For those methods with open-source codes, we run the original codes; otherwise, we re-implement them based on the correspo"
2021.acl-long.271,D16-1230,0,0.0794378,"Missing"
2021.acl-long.271,2020.emnlp-main.739,0,0.0594831,"Missing"
2021.acl-long.271,D19-1326,0,0.0475725,"Missing"
2021.acl-long.271,P02-1040,0,0.110323,".584 0.622 0.649 0.687 0.650 0.682 0.633 0.663 0.653 0.689 0.660 0.701 0.661 0.710 0.657 0.692 0.669 0.713 0.671 0.720 Table 2: Automatic evaluation results for different models based on four types of metrics. that provide the controllable feature, i.e., question types, in advance for inference. Therefore, we do not consider CT-based models as comparable ones. 3.4 Evaluation Measures To better evaluate our results, we use both quantitative metrics and human judgements in our experiments. Automatic Metrics For automatic evaluation, we mainly choose four kinds of metrics: (1) BLEU Scores: BLEU (Papineni et al., 2002) calculates the n-gram overlap score of generated questions against ground-truth questions. We use BLEU-1 and BLEU-2 here and normalize them to 0 to 1 scale. (2) Embedding Metrics: Average, Greedy and Extrema metrics are embedding-based and measure the semantic similarity between the words in generated questions and ground-truth questions (Serban et al., 2017; Liu et al., 2016). We use word2vec embeddings trained on the Google News Corpus6 in this part. Please refer to Serban et al. (2017) for more details. (3) Dist-1& Dist-2: Following the work of Li et al. (2016a), we apply Distinct to repor"
2021.acl-long.271,N18-1162,0,0.0355925,"Missing"
2021.acl-long.271,P17-2036,0,0.0167679,"a vital position in this field. Serban et al. (2016) proposed the hierarchical recurrent encoder-decoder (HRED) model with a context RNN to integrate historical information from utterance RNNs. To capture utterance-level variations, Serban et al. (2017) raised a new model Variational HRED (VHRED) that augments HRED with CVAEs. After that, VHCR (Park et al., 2018) added a conversation-level latent variable on top of the VHRED, while CSRR (Shen et al., 2019) used three-hierarchy latent variables to model the complex dependency among utterances. In order to detect relative utterances in context, Tian et al. (2017) and Zhang et al. (2018) applied cosine similarity and attention mechanism, respectively. HRAN (Xing et al., 2018) combined the attention results on both word-level and utterance-level. Besides, the future information has also been considered for context modeling. Shen et al. (2018) separated the context into history and future parts, and assumed that each of them conditioned on a latent variable is under a Gaussian distribution. Feng et al. (2020) used future utterances in the discriminator of a GAN, which is similar to Wang et al. (2019). The differences between our method and aforementioned"
2021.acl-long.271,P15-1152,0,0.0332229,"300, 300, and 100. The prior networks and MLPs have one hidden layer with size 300 and tanh non-linearity, while the number of hidden layers in recognition networks for both triple-level and utterance-level variables is 2. We apply dropout ratio of 0.2 during training. The mini-batch size is 64. For optimization, we use Adam (Kingma and Ba, 2015) with a learning rate of 1e-4. In order to alleviate degeneration problem of variational framework (Park et al., 2018), we We compare our methods with four groups of representative models: (1) S2S-Attn: A simple Seq2Seq model with attention mechanism (Shang et al., 2015). (2) CVAE&kgCVAE: The CVAE model integrates an extra BOW loss to generate diverse questions. The kgCVAE is a knowledge-guided CVAE that utilizes some linguistic cues (question types in our experiments) to learn meaningful latent variables (Zhao et al., 2017). (3) STD&HTD: The STD uses soft typed decoder that estimates a type distribution over word types, and the HTD uses hard typed decoder that specifies the type of each word explicitly with Gumbel-softmax (Wang et al., 2018). (4) RL-CVAE: A reinforcement learning method that regards the coherence score (computed by a one-to-one matching netw"
2021.acl-long.271,2020.acl-main.52,1,0.84569,"GTM that does not take answer into account. That is, answer decoder and za are removed from the loss function and the prior and posterior distributions of zq . Besides, zt here does not capture the semantics from answer. (7) GTMzq /za : GTM variant in which distributions of zq are not conditioned on za , i.e., the fact that the content of question is also controlled by answer is not modelled explicitly by latent variables. In our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020) Inspired by Park et al. (2018), using zt inferred from post with the posterior distribution is better than sampling it from the prior one, i.e., a standard Gaussian distribution. 2 https://drive.google.com/drive/ folder/1wNG30YPHiMc_ZNyE3BH5wa1uVtR8l1pG 3 http://www.reddit.com 4 The total BOW loss is calculated as the sum of all BOW losses between each latent variable and q/a. Please refer to Park et al. (2018) for more details. 5 For those methods with open-source codes, we run the original codes; otherwise, we re-implement them based on the corresponding paper. 3.1 Dataset We apply our mode"
2021.acl-long.271,P19-1549,1,0.844722,"Blended Evaluation Routine (Tao et al., 2018) has shown a high correlation with human annotation in open-domain conversation evaluation. There are two versions, one is RubG based on geometric averaging and the other is RubA based on arithmetic averaging. Embedding metrics and BLEU scores are used to measure the similarity between generated and ground-truth questions. RubG/A reflects the se6 https://code.google.com/archive/p/ word2vec/ mantic coherence of PQ pairs (Wang et al., 2019), while Dist-1/2 evaluates the diversity of questions. Human Evaluation Settings Inspired by Wang et al. (2019), Shen et al. (2019), and Wang et al. (2018), we use following three criteria for human evaluation: (1) Fluency measures whether the generated question is reasonable in logic and grammatically correct. (2) Coherence denotes whether the generated question is semantically consistent with the given post. Incoherent questions include dull cases. (3) Willingness measures whether a user is willing to answer the question. This criterion is to justify how likely the generated questions can elicit further interactions. We randomly sample 500 examples from test set, and generate questions using models mentioned above. Then"
2021.acl-long.271,D18-1463,0,0.0162246,"l HRED (VHRED) that augments HRED with CVAEs. After that, VHCR (Park et al., 2018) added a conversation-level latent variable on top of the VHRED, while CSRR (Shen et al., 2019) used three-hierarchy latent variables to model the complex dependency among utterances. In order to detect relative utterances in context, Tian et al. (2017) and Zhang et al. (2018) applied cosine similarity and attention mechanism, respectively. HRAN (Xing et al., 2018) combined the attention results on both word-level and utterance-level. Besides, the future information has also been considered for context modeling. Shen et al. (2018) separated the context into history and future parts, and assumed that each of them conditioned on a latent variable is under a Gaussian distribution. Feng et al. (2020) used future utterances in the discriminator of a GAN, which is similar to Wang et al. (2019). The differences between our method and aforementioned ones in Section 4.1 and 4.2 are: (1) Rather than dividing PQA triples into two parts, i.e., PQ (history and current utterances) and QA (current and future utterances) pairs, we model the entire coherence by utilizing a latent variable to capture the share background in a triple. (2"
2021.acl-long.271,D19-1511,0,0.0687683,"al of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018). Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗ Yang Feng is the corresponding author. CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is unavailable during inference (Wang et al., 2019). At the same time, each utterance in open-domain scenario is casual and can be followed by several appropriate sentences, i.e., one-to-many mapping (Gao et al., 2019; Chen et al., 2019). At first, the input information of CQG was mainly a given post (Wang et al., 2018; Hu et al., 2018), and the generated questions were usually dull or deviated (Q3 and Q4 in Table 1). Based on the observation that an answer has strong relevance to its question and post, Wang et al. (2019) tried to integrate answer into the question generation process. They applied a reinforcement learning framework that firstl"
2021.acl-long.271,P18-1204,0,0.389212,"to Q2.2) is decided by its post and answer. Q3 (dull) and Q4 (deviated) are generated given only the post. Introduction Questioning in open-domain dialogue systems is indispensable since a good system should have the ability to well interact with users by not only responding but also asking (Li et al., 2017). Besides, raising questions is a proactive way to guide users to go deeper and further into conversations (Yu et al., 2016). Therefore, the ultimate goal of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018). Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗ Yang Feng is the corresponding author. CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is unavailable during inference (Wang et al., 2019). At the same time, each utterance in open-domain scenario is casual and can be followed by several appropriate sentences, i.e., one-to-man"
2021.acl-long.271,W16-3649,0,0.0172703,"mple of CQG task which is talking about a person’s eating activity. There are one-to-many mappings in both PQ and QA pairs. The content of each meaningful and relevant question (Q1.1 to Q2.2) is decided by its post and answer. Q3 (dull) and Q4 (deviated) are generated given only the post. Introduction Questioning in open-domain dialogue systems is indispensable since a good system should have the ability to well interact with users by not only responding but also asking (Li et al., 2017). Besides, raising questions is a proactive way to guide users to go deeper and further into conversations (Yu et al., 2016). Therefore, the ultimate goal of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018). Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗ Yang Feng is the corresponding author. CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is un"
2021.acl-long.271,2021.naacl-main.446,0,0.169882,"eally messing me up. Table 5: Two cases comparison among GTM and other baselines. both posts and answers, and could attract people to give an answer to them. However, other baselines may generate dull or deviated responses, even the RL-CVAE model that considers the answer information would only contain the topic words in answers (e.g., the question in case two), but fail to ensure the PQA coherence. eration problem does not exist in our model and latent variables can play their corresponding roles. 4 The researches on open-domain dialogue systems have developed rapidly (Majumder et al., 2020; Zhan et al., 2021; Shen et al., 2021), and our work mainly touches two fields: open-domain conversational question generation (CQG), and context modeling in dialogue systems. We introduce these two fields as follows and point out the main differences between our method and previous ones. 4.1 Figure 3: Total KL divergence (per word) of all latent variables in GTM and GTM-a model (first 30 epochs of validation set). 3.8 Further Analysis of GTM Variational models suffer from the notorious degeneration problem, where the decoders ignore latent variables and reduce to vanilla Seq2Seq models (Zhao et al., 2017; Park"
2021.acl-long.271,C18-1206,0,0.0314446,"Missing"
2021.acl-long.271,P17-1061,0,0.11927,"q, a (not used in inference), and qt, while dashed arrows are for posterior distributions of latent variables. • Our variational hierarchical structure can not only utilize the “future” information (answer), but also capture one-to-many mappings in PQ and QA, which matches the open-domain scenario well. • Experimental results on a large-scale CQG corpus show that our method significantly outperforms the state-of-the-art baselines in both automatic and human evaluations. 2 Proposed Model Given a post as the input, the goal of CQG is to generate the corresponding question. Following the work of Zhao et al. (2017) and Wang et al. (2019), we leverage the question type qt to control the generated question, and take advantage of the answer information a to improve coherence. In training set, each conversation is represented |p| as {p, q, qt, a}, consisting of post p = {pi }i=1 , |q| question q = {qi }i=1 with its question type qt, |a| and answer a = {ai }i=1 . 2.1 Overview The graphical model of GTM for training process is shown in Figure 1. θ, ϕ, and φ are used to denote parameters of generation, prior, and recognition network, respectively. We integrate answer generation to assist question generation wi"
2021.acl-long.271,D19-1622,0,0.0445696,"Missing"
2021.acl-long.445,N19-1388,0,0.0564376,"language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method. 1 Introduction Neural machine translation(NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) has shown its superiority and drawn much attention in recent years. Although the NMT model can achieve promising results for highresource language pairs, it is unaffordable to train separate models for all the language pairs since there are thousands of languages in the world (Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019). A typical solution to reduce the model size ∗ Corresponding author: Yang Feng. Our code can be got at https://github.com/ictnlp/NAMNMT and the training cost is to handle multiple languages in a single multilingual neural machine translation (MNMT) model (Ha et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The standard paradigm of MNMT proposed by Johnson et al. (2017) contains a language-shared encoder and decoder with a special language indicator in the input sentence to determine the target language. Because different languages share all"
2021.acl-long.445,D19-1165,0,0.17933,"the MNMT model trained on the combined data generally captures the general knowledge, but ignores the language-specific knowledge, rendering itself sub-optimal for the translation of a specific language (Sachan and Neubig, 2018; Blackwood et al., 2018; Wang et al., 2020b). To retain the language-specific knowledge, some researches turn to augment the NMT model with language-specific modules, e.g., the language-specific attention module (Blackwood et al., 2018), decoupled multilingual encoders and/or decoders (V´azquez et al., 2019; Escolano et al., 2020) and the lightweight language adapters (Bapna and Firat, 2019). However, these methods suffer from the parameter increment problem, because the number of parameters increases linearly with the number of languages. Besides, the structure, size, and location of the module have a large influence on the final performance, which requires specialized manual design. As a result, these problems often prevent the application of these methods in some scenarios. Based on the above, we aim to propose a method that can retain the general and language-specific knowledge, and keep a stable model size as the number of language-pair increases without introducing any spec"
2021.acl-long.445,C18-1263,0,0.142084,"of MNMT proposed by Johnson et al. (2017) contains a language-shared encoder and decoder with a special language indicator in the input sentence to determine the target language. Because different languages share all of the model parameters in the standard MNMT model, the model tends to converge to a region where there are low errors for all the languages. Therefore, the MNMT model trained on the combined data generally captures the general knowledge, but ignores the language-specific knowledge, rendering itself sub-optimal for the translation of a specific language (Sachan and Neubig, 2018; Blackwood et al., 2018; Wang et al., 2020b). To retain the language-specific knowledge, some researches turn to augment the NMT model with language-specific modules, e.g., the language-specific attention module (Blackwood et al., 2018), decoupled multilingual encoders and/or decoders (V´azquez et al., 2019; Escolano et al., 2020) and the lightweight language adapters (Bapna and Firat, 2019). However, these methods suffer from the parameter increment problem, because the number of parameters increases linearly with the number of languages. Besides, the structure, size, and location of the module have a large influen"
2021.acl-long.445,P15-1166,0,0.0257854,"ee that when the neurons associated with the current language pair are erased, the performance of this language pair decreases greatly. However, the performance of other language pairs only declines slightly, because the specific knowledge captured by these specific neurons are not so important for other languages. 6 Related Work Our work closely relates to language-specific modeling for MNMT and model pruning which we will recap both here. Early MNMT studies focus on improving the sharing capability of individual bilingual models to handle multiple languages, which includes sharing encoders (Dong et al., 2015), sharing decoders (Zoph et al., 2016), and sharing sublayers (Firat et al., 2016). Later, Ha et al. (2016) and Johnson et al. (2017) propose an universal MNMT model with a target language token to indicate the translation direction. While this paradigm fully explores the general knowledge between languages and hard to obtain the specific knowledge of each language (Tan et al., 2019; Aharoni et al., 2019), the subsequent researches resort to Language-specific modeling, trying to find a better trade-off between sharing and specific. Such approaches involve inserting conditional languagespecific"
2021.acl-long.445,2021.eacl-main.80,0,0.0588337,"Missing"
2021.acl-long.445,N16-1101,0,0.0801387,"eriority and drawn much attention in recent years. Although the NMT model can achieve promising results for highresource language pairs, it is unaffordable to train separate models for all the language pairs since there are thousands of languages in the world (Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019). A typical solution to reduce the model size ∗ Corresponding author: Yang Feng. Our code can be got at https://github.com/ictnlp/NAMNMT and the training cost is to handle multiple languages in a single multilingual neural machine translation (MNMT) model (Ha et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The standard paradigm of MNMT proposed by Johnson et al. (2017) contains a language-shared encoder and decoder with a special language indicator in the input sentence to determine the target language. Because different languages share all of the model parameters in the standard MNMT model, the model tends to converge to a region where there are low errors for all the languages. Therefore, the MNMT model trained on the combined data generally captures the general knowledge, but ignores the language-specific knowledge, rendering itself sub-optimal for th"
2021.acl-long.445,N18-1032,0,0.0169507,"t years. Although the NMT model can achieve promising results for highresource language pairs, it is unaffordable to train separate models for all the language pairs since there are thousands of languages in the world (Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019). A typical solution to reduce the model size ∗ Corresponding author: Yang Feng. Our code can be got at https://github.com/ictnlp/NAMNMT and the training cost is to handle multiple languages in a single multilingual neural machine translation (MNMT) model (Ha et al., 2016; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018). The standard paradigm of MNMT proposed by Johnson et al. (2017) contains a language-shared encoder and decoder with a special language indicator in the input sentence to determine the target language. Because different languages share all of the model parameters in the standard MNMT model, the model tends to converge to a region where there are low errors for all the languages. Therefore, the MNMT model trained on the combined data generally captures the general knowledge, but ignores the language-specific knowledge, rendering itself sub-optimal for the translation of a specific language (Sa"
2021.acl-long.445,Q17-1024,0,0.0614858,"Missing"
2021.acl-long.445,D13-1176,0,0.0428887,"e model neurons into general and language-specific parts based on their importance across languages. The general part is responsible for preserving the general knowledge and participating in the translation of all the languages, while the language-specific part is responsible for preserving the languagespecific knowledge and participating in the translation of some specific languages. Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method. 1 Introduction Neural machine translation(NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) has shown its superiority and drawn much attention in recent years. Although the NMT model can achieve promising results for highresource language pairs, it is unaffordable to train separate models for all the language pairs since there are thousands of languages in the world (Tan et al., 2019; Aharoni et al., 2019; Arivazhagan et al., 2019). A typical solution to reduce the model size ∗ Corresponding author: Yang Feng. Our code can be got at https://github.com/ictnlp/NAMNMT and the training cost is to"
2021.acl-long.445,N19-4009,0,0.026737,"Missing"
2021.acl-long.445,P02-1040,0,0.112281,"Missing"
2021.acl-long.445,W18-6319,0,0.0211124,"Missing"
2021.acl-long.445,W18-6300,0,0.139913,"Missing"
2021.acl-long.445,K16-1029,0,0.0618184,"Missing"
2021.acl-long.445,P16-1162,0,0.0158833,"ns that are responsible for capturing general knowledge. Many-to-Many For this translation scenario, we test our approach on IWSLT-171 translation datasets, including English, Italian, Romanian, Dutch (briefly, En, It, Ro, Nl). We experimented in eight directions, including It↔En, Ro↔En, Nl↔En, and It↔Ro, with 231.6k, 220.5k, 237.2k, and 217.5k data for each language pair. We choose test2016 and test2017 as our development and test set, respectively. Sentences of all languages were tokenized by the Moses scripts2 and further segmented into subword symbols using Byte-Pair Encoding (BPE) rules (Sennrich et al., 2016) with 40K merge operations for all languages jointly. Language-specific Neurons Next, we regard other neurons except for the general neurons as the language-specific neurons and determine which language pair to assign them to. To achieve this, we compute an importance threshold for each neuron: λ(i) = k × max(Θm (i)), m ∈ {1, . . . , M }, k ∈ [0, 1] (9) , where max(Θm (i)) denotes the maximum importance of this neuron in all language pairs and k is a hyper-parameter. The neuron will be assigned to the language-pairs whose importance is larger than the threshold. When the importance of neurons"
2021.acl-long.445,D16-1163,0,0.0234714,"th the current language pair are erased, the performance of this language pair decreases greatly. However, the performance of other language pairs only declines slightly, because the specific knowledge captured by these specific neurons are not so important for other languages. 6 Related Work Our work closely relates to language-specific modeling for MNMT and model pruning which we will recap both here. Early MNMT studies focus on improving the sharing capability of individual bilingual models to handle multiple languages, which includes sharing encoders (Dong et al., 2015), sharing decoders (Zoph et al., 2016), and sharing sublayers (Firat et al., 2016). Later, Ha et al. (2016) and Johnson et al. (2017) propose an universal MNMT model with a target language token to indicate the translation direction. While this paradigm fully explores the general knowledge between languages and hard to obtain the specific knowledge of each language (Tan et al., 2019; Aharoni et al., 2019), the subsequent researches resort to Language-specific modeling, trying to find a better trade-off between sharing and specific. Such approaches involve inserting conditional languagespecific routing layer (Zhang et al., 2021), s"
2021.acl-long.445,D19-1089,0,0.0357038,"Missing"
2021.acl-long.445,W19-4305,0,0.033037,"Missing"
2021.acl-long.445,2020.emnlp-main.78,0,0.123316,"nson et al. (2017) contains a language-shared encoder and decoder with a special language indicator in the input sentence to determine the target language. Because different languages share all of the model parameters in the standard MNMT model, the model tends to converge to a region where there are low errors for all the languages. Therefore, the MNMT model trained on the combined data generally captures the general knowledge, but ignores the language-specific knowledge, rendering itself sub-optimal for the translation of a specific language (Sachan and Neubig, 2018; Blackwood et al., 2018; Wang et al., 2020b). To retain the language-specific knowledge, some researches turn to augment the NMT model with language-specific modules, e.g., the language-specific attention module (Blackwood et al., 2018), decoupled multilingual encoders and/or decoders (V´azquez et al., 2019; Escolano et al., 2020) and the lightweight language adapters (Bapna and Firat, 2019). However, these methods suffer from the parameter increment problem, because the number of parameters increases linearly with the number of languages. Besides, the structure, size, and location of the module have a large influence on the final per"
2021.autosimtrans-1.1,D14-1140,0,0.33715,"Missing"
2021.autosimtrans-1.1,D18-1337,0,0.488087,"Missing"
2021.autosimtrans-1.1,E17-1099,0,0.656758,"evel simultaneous translation and applied wait-k policy on it. Meanwhile, we apply two data processing methods and combine two training methods for domain adaptation. Our method enhance the ST model with stronger robustness and domain adaptability. Experiments on streaming transcription show that our method outperforms the baseline at all latency, especially at low latency, the proposed method improves about 6 BLEU. Besides, ablation studies we conduct verify the effectiveness of each module in the proposed method. 1 Introduction Automatic simultaneous translation (ST) (Cho and Esipova, 2016; Gu et al., 2017; Ma et al., 2019), a task in machine translation (MT), aims to output the target translation while reading the source sentence. The standard machine translation is a full-sentence MT, which waits for the complete source input and then starts translation. The huge latency caused by full-sentence MT is unacceptable in many realtime scenarios. On the contrary, ST is widely used in real simultaneous speech translation scenarios, such as simultaneous interpretation, synchronized subtitles, and live broadcasting. Previous methods (Ma et al., 2019; Arivazhagan et al., 2019) for ST are all evaluated"
2021.autosimtrans-1.1,P19-1126,0,0.172161,"Missing"
2021.autosimtrans-1.1,N12-1048,0,0.37223,"w/o Future-guided 14 12 4 2 0 2 4 6 Average Lagging 8 10 12 Figure 4: Ablation study on two training methods. k), and when the ‘Multi-path’ is removed, the translation quality decreases by 0.76 BLEU (average on all k). This shows that two training methods can both effectively improve the translation quality under low latency, especially ‘Future-guided’. 6 Related Work Previous ST methods are mainly divided into precise read / write policy and stronger predictive ability. For read / write policy, early policies used segmented translation, and applied full sentence translation to each segment (Bangalore et al., 2012; Cho and Esipova, 2016; Siahbani et al., 2018). Gu et al. (2017) trained an agent through reinforcement learning to decide read / write. Dalvi et al. (2018) proposed STATIC-RW, which first performing S’s READs, then alternately performing RW ’s WRITEs and READs. Ma et al. (2019) proposed wait-k policy, wherein first reads k tokens and then begin synchronizing write and read. Wait-k policy has achieved remarkable performance because it is easy to train and stable, and is widely used in simultaneous translation. Zheng et al. (2019a) generated the gold read / write sequence of input sentence by"
2021.autosimtrans-1.1,2020.autosimtrans-1.6,0,0.213222,"→ Text-to-Speech Synthesis (TTS), in which these three parts are all carried out simultaneously. As a downstream task of simultaneous ASR, the input of ST is always not exactly correct and in the spoken language domain. Thus, robustness and domain adaptability become two challenges for the ST system. For robustness, since the input of the ST system is ASR result (streaming transcription), which is incremental and may be unsegmented or incorrectly segmented, the subword-level segmentation result (Ma et al., 2019) of the streaming transcription seriously affect the ST result. Existing methods (Li et al., 2020) often remove the last token after segmentation to prevent it from being incomplete, which leads to a considerable increase in latency. Table 1 shows an example of the tokenization result of the streaming transcription input with different methods. In steps 4-7 of standard wait-2, the input prefix is different from its previous step, while the previous output prefix is not allowed to be modified in ST, which leads to serious translation errors. Although removing the last token improves the robustness, there is no new input in many consecutive steps, which greatly increases the latency. For dom"
2021.autosimtrans-1.1,D18-1461,0,0.0140288,"he meaning of a entire word, the ST system does not have to wait for the complete word before translating. 3) Char-level ST only performs char-level tokenization on the source, while the target still retains subword-level tokenization, so its translation performance will not be affected too much, as shown in Table 7. To enhance the robustness of dealing with streaming transcription, we first proposed char-level simultaneous translation and applied the wait-k policy on it. 4.1.1 Char-Level Simultaneous Translation Character-level neural machine translation (Ling et al., 2015; Lee et al., 2017; Cherry et al., 2018; Gao et al., 2020) tokenizes the source sentence and target sentence according to characters, thereby gaining advantages over subword-level neural machine translation in some specific aspects, such as avoiding out-of-vocabulary problems (Passban et al., 2018) and errors caused by subword-level segmentation (Tang et al., 2020). In terms of translation quality, the character-level MT is still difficult to compare with the subword-level MT. An important reason is that only one wrong generated character will directly cause the entire target word wrong (Sennrich, 2017). To improve the robustness o"
2021.autosimtrans-1.1,W18-6302,0,0.0226704,"oder is limited to less than g (t). 4 Char-level wait-k policy: Char-level streaming input Methods wait for k tokens To improve the robustness and domain adaptability of ST, we enhance our system from read / write policy, data processing and training methods respectively. 4.1 Predict Figure 1: Standard wait-k policy vs. our char-level wait-k policy (take k = 2 as an example). Char-Level Wait-k Policy The word segmentation and BPE operation at the target end are the same as subword-level MT (Vaswani et al., 2017), and char-level tokenization is similar to character-level MT (Yang et al., 2016; Nikolov et al., 2018; Saunders et al., 2020) but not completely consistent. The char-level tokenization we proposed divides each source language character into a token, and other characters (such as numbers, other language characters) are still divided into a token according to complete words. An example of char-level tokenization is shown in Table 3. In the result of char-level tokenization, each Chinese character is divided into a token, and the number (12) and English (UNIT) are entirely taken as a token, respectively. Char-level tokenization is more suitable for streaming transcription, which ensures that the"
2021.autosimtrans-1.1,2020.acl-main.145,0,0.011663,"e word, the ST system does not have to wait for the complete word before translating. 3) Char-level ST only performs char-level tokenization on the source, while the target still retains subword-level tokenization, so its translation performance will not be affected too much, as shown in Table 7. To enhance the robustness of dealing with streaming transcription, we first proposed char-level simultaneous translation and applied the wait-k policy on it. 4.1.1 Char-Level Simultaneous Translation Character-level neural machine translation (Ling et al., 2015; Lee et al., 2017; Cherry et al., 2018; Gao et al., 2020) tokenizes the source sentence and target sentence according to characters, thereby gaining advantages over subword-level neural machine translation in some specific aspects, such as avoiding out-of-vocabulary problems (Passban et al., 2018) and errors caused by subword-level segmentation (Tang et al., 2020). In terms of translation quality, the character-level MT is still difficult to compare with the subword-level MT. An important reason is that only one wrong generated character will directly cause the entire target word wrong (Sennrich, 2017). To improve the robustness of the ST system whe"
2021.autosimtrans-1.1,P15-1020,0,0.0172971,"ess of the two training methods, we conducted an ablation study on them, and show the results of removing one of them in Figure 4. When removing one of them, the translation quality decreases, especially at low latency. When the ‘Future-guided’ is removed, the translation quality decreases by 1.49 BLEU (average on all 7 24 decoding. Zhang et al. (2020a) propose a novel adaptive segmentation policy for ST. For predicting future, Matsubara et al. (2000) applied pattern recognition to predict verbs in advance. Grissom II et al. (2014) used a Markov chain to predict the next word and final verb. (Oda et al., 2015) predict unseen syntactic constituents to help generate complete parse trees and perform syntax-based simultaneous translation. Alinejad et al. (2018) added a Predict operation to the agent based on Gu et al. (2017), predicting the next word as an additional input. Elbayad et al. (2020) enhances the wait-k policy by sampling different k to train. Zhang et al. (2020b) proposed future-guided training, which introduces a full-sentence Transformer as the teacher of the ST model and uses future information to guide training through knowledge distillation. Although the previous methods performed wel"
2021.autosimtrans-1.1,2020.coling-main.375,0,0.0149222,"the robustness of dealing with streaming transcription, we first proposed char-level simultaneous translation and applied the wait-k policy on it. 4.1.1 Char-Level Simultaneous Translation Character-level neural machine translation (Ling et al., 2015; Lee et al., 2017; Cherry et al., 2018; Gao et al., 2020) tokenizes the source sentence and target sentence according to characters, thereby gaining advantages over subword-level neural machine translation in some specific aspects, such as avoiding out-of-vocabulary problems (Passban et al., 2018) and errors caused by subword-level segmentation (Tang et al., 2020). In terms of translation quality, the character-level MT is still difficult to compare with the subword-level MT. An important reason is that only one wrong generated character will directly cause the entire target word wrong (Sennrich, 2017). To improve the robustness of the ST system when dealing with unsegmented incremental input, while avoiding the performance degradation caused by character-level MT, we propose char-level simultaneous translation, which is more suitable for streaming input. The framework of char-level ST is shown in the lower part of Figure 1. Different from subword-leve"
2021.autosimtrans-1.1,N19-4009,0,0.0366397,"e model with greedy / beam search respectively in Table 7. Standard Wait-k: standard subword-level waitk policy proposed by Ma et al. (2019), used as our baseline. For comparison, we apply the same training method as our method (Sec.4.3) to train it. Standard Wait-k + rm Last Token: standard subword-level wait-k policy. In the inference time, the last token after the word segmentation is remove to prevent it from being incomplete. Char-Level Wait-k: our proposed method, refer to Sec.4 for details. The implementation of all systems is based on Transformer-Big, and adapted from Fairseq Library (Ott et al., 2019). The parameters are the same as the original Transformer (Vaswani et al., 2017). All systems are trained on 4 RTX-3090 GPUs. The dataset for Chinese → English task provided by the organizer contains three parts, shown in Table 6. CWMT19 2 is the general domain corpus that consists of 9,023,708 sentence pairs. Transcription consists of 37,901 sentence pairs and Dev. Set consists of 956 sentence pairs 3 , which are both spoken language domain corpus collected from real speeches (Zhang et al., 2021). We use CWMT19 to pre-train the ST model, then use Transcription for fine-tuning, and finally eva"
2021.autosimtrans-1.1,P02-1040,0,0.115082,"remove the ending punctuation according to the method in Sec.4.2.1. Given the processed corpus after cleaning and augmentation, we first perform char-level tokenization (Sec.4.1) on the Chinese sentences, and tokenize and lowercase English sentences with the Moses4 . We apply BPE (Sennrich et al., 2016) with 16K merge operations on English. 5.2 24.93 24.93 24.93 24.93 Table 7: Results of offline model. ‘+FT’: +fine-tuning. Experiments 5.1 Pre-train + FT Pre-train + FT BLEU Greedy Beam4 20.24 20.35 24.79 25.39 20.14 20.28 24.60 25.13 5.3 Evaluation Metric For evaluation metric, we use BLEU 5 (Papineni et al., 2002) and AL6 (Ma et al., 2019) to measure translation quality and latency, respectively. Latency metric AL of char-level wait-k policy is calculated with gk (t) in Eq.(2): τ 1X t−1 AL = gk (t) − |y| τ t=1 (9) |c| where τ = argmax (gk (t) = |c|) (10) t where c and y are the input character sequence and the output subword sequence, respectively. Note that since the streaming transcription provided by the organizer adds a source character at each step, for all systems, we use character-level AL to evaluate the latency. System Setting We set the standard wait-k policy as the baseline and compare our m"
2021.autosimtrans-1.1,C16-1288,0,0.0595535,"Missing"
2021.autosimtrans-1.1,N18-1006,0,0.0200038,"formance will not be affected too much, as shown in Table 7. To enhance the robustness of dealing with streaming transcription, we first proposed char-level simultaneous translation and applied the wait-k policy on it. 4.1.1 Char-Level Simultaneous Translation Character-level neural machine translation (Ling et al., 2015; Lee et al., 2017; Cherry et al., 2018; Gao et al., 2020) tokenizes the source sentence and target sentence according to characters, thereby gaining advantages over subword-level neural machine translation in some specific aspects, such as avoiding out-of-vocabulary problems (Passban et al., 2018) and errors caused by subword-level segmentation (Tang et al., 2020). In terms of translation quality, the character-level MT is still difficult to compare with the subword-level MT. An important reason is that only one wrong generated character will directly cause the entire target word wrong (Sennrich, 2017). To improve the robustness of the ST system when dealing with unsegmented incremental input, while avoiding the performance degradation caused by character-level MT, we propose char-level simultaneous translation, which is more suitable for streaming input. The framework of char-level ST"
2021.autosimtrans-1.1,2021.autosimtrans-1.5,0,0.102591,"ility of the input prefix but also avoids unnecessary latency. To adapt to the spoken language domain, we first pretrain an ST model on the general domain corpus and perform fine-tuning on the spoken language domain corpus. To improve the effect and efficiency of domain adaptation, we carry out data augmentation on both the general domain corpus and spoken language domain corpus and combine two different training methods for training. In the streaming transcription track for the Chinese → English translation task of AutoSimTrans 2021, we evaluate the proposed method on the real speech corpus (Zhang et al., 2021). Our method exceeds the baseline model at all latency and performs more prominently at lower latency. Our contributions can be summarized as follows: Welcome everyone come here. is shown in Table 2. Streaming transcription is manually transcribed without word segmentation. Between each step, the source input adds one more character. The task applies AL and BLEU respectively to evaluate the latency and translation quality of the submitted system. 3 Background Our system is based on a variant of wait-k policy (Ma et al., 2019), so we first briefly introduce waitk policy and its training method."
2021.autosimtrans-1.1,2020.emnlp-main.178,0,0.652603,"remental Transformer and full-sentence Transformer, respectively. Finally, the total loss L is calculated as:   L = L (θincr ) + L (θf ull ) + λL z incr , z f ull (7) where λ is the hyper-parameter, and we set λ = 0.1 in our system. (3) Y  For the knowledge distillation between fullsentence Transformer and incremental Transformer, we apply L2 regularization term between their encoder hidden states, calculated as: (c,y)∈Dg t=1 p (y |c, k) = log pθincr yt |y<t , c≤gk (t) (5) 4.3.1 Pre-training To improve the predictive ability of the ST model, we apply the future-guided training proposed by (Zhang et al., 2020b). Besides the incremental Transformer for simultaneous translation with charlevel wait-k policy, we introduce a full-sentence Transformer, used as the teacher of the incremental Transformer for ST through knowledge distillation. The full-sentence Transformer is trained with crossentropy loss: X |y| X (c,y)∈Dg t=1,k∼U (K) Our method is based on Transformer (Vaswani et al., 2017), and the training is divided into two stages. First, we pre-train an ST model on the general domain MT corpus, and then fine-tune the ST model on the spoken language domain corpus. For pre-training, we apply multi-pat"
2021.autosimtrans-1.1,2020.wat-1.21,0,0.0238238,"s than g (t). 4 Char-level wait-k policy: Char-level streaming input Methods wait for k tokens To improve the robustness and domain adaptability of ST, we enhance our system from read / write policy, data processing and training methods respectively. 4.1 Predict Figure 1: Standard wait-k policy vs. our char-level wait-k policy (take k = 2 as an example). Char-Level Wait-k Policy The word segmentation and BPE operation at the target end are the same as subword-level MT (Vaswani et al., 2017), and char-level tokenization is similar to character-level MT (Yang et al., 2016; Nikolov et al., 2018; Saunders et al., 2020) but not completely consistent. The char-level tokenization we proposed divides each source language character into a token, and other characters (such as numbers, other language characters) are still divided into a token according to complete words. An example of char-level tokenization is shown in Table 3. In the result of char-level tokenization, each Chinese character is divided into a token, and the number (12) and English (UNIT) are entirely taken as a token, respectively. Char-level tokenization is more suitable for streaming transcription, which ensures that the newly input content at"
2021.autosimtrans-1.1,E17-2060,0,0.0130662,"2015; Lee et al., 2017; Cherry et al., 2018; Gao et al., 2020) tokenizes the source sentence and target sentence according to characters, thereby gaining advantages over subword-level neural machine translation in some specific aspects, such as avoiding out-of-vocabulary problems (Passban et al., 2018) and errors caused by subword-level segmentation (Tang et al., 2020). In terms of translation quality, the character-level MT is still difficult to compare with the subword-level MT. An important reason is that only one wrong generated character will directly cause the entire target word wrong (Sennrich, 2017). To improve the robustness of the ST system when dealing with unsegmented incremental input, while avoiding the performance degradation caused by character-level MT, we propose char-level simultaneous translation, which is more suitable for streaming input. The framework of char-level ST is shown in the lower part of Figure 1. Different from subword-level ST, given the parallel sentence pair < X, Y >, the source of the ST model in the proposed char-level ST is the character sequence c = (c1 , · · · , cn ) after char-level tokenization, and the target is the subword sequence y = (y1 , · · · ,"
2021.autosimtrans-1.1,P16-1162,0,0.838724,"orpus for ST are quite different in terms of word order, punctuation and modal particles, so ST needs to efficiently complete the domain adaption. In our system, we propose a Char-Level Wait-k Policy for simultaneous translation, which is more robust with streaming transcription input. Besides, we apply data augmentation and combine two training methods to train the model to complete domain adaptation. Specifically, the source of the char-level wait-k policy is a character sequence segmented according to characters, and the target still maintains subword-level segmentation and BPE operations (Sennrich et al., 2016). When decoding, Simultaneous translation (ST) outputs the translation simultaneously while reading the input sentence, which is an important component of simultaneous interpretation. In this paper, we describe our submitted ST system, which won the first place in the streaming transcription input track of the Chinese-English translation task of AutoSimTrans 2021. Aiming at the robustness of ST, we first propose char-level simultaneous translation and applied wait-k policy on it. Meanwhile, we apply two data processing methods and combine two training methods for domain adaptation. Our method"
2021.autosimtrans-1.1,D19-1137,0,0.172188,"tion, and applied full sentence translation to each segment (Bangalore et al., 2012; Cho and Esipova, 2016; Siahbani et al., 2018). Gu et al. (2017) trained an agent through reinforcement learning to decide read / write. Dalvi et al. (2018) proposed STATIC-RW, which first performing S’s READs, then alternately performing RW ’s WRITEs and READs. Ma et al. (2019) proposed wait-k policy, wherein first reads k tokens and then begin synchronizing write and read. Wait-k policy has achieved remarkable performance because it is easy to train and stable, and is widely used in simultaneous translation. Zheng et al. (2019a) generated the gold read / write sequence of input sentence by rules, and then trained an agent with the input sentences and gold read / write sequence. Zheng et al. (2019b) introduces a “delay” token {ε} into the target vocabulary to read one more token. Arivazhagan et al. (2019) proposed MILK, which uses a Bernoulli distribution variable to determine whether to output. Ma et al. (2020) proposed MMA, the implementation of MILK based on Transformer. Zheng et al. (2020) proposed a decoding policy that uses multiple fixed models to accomplish adaptive 7 Conclusion and Future Work Our submitted"
2021.autosimtrans-1.1,W18-1815,0,0.245628,"gging 8 10 12 Figure 4: Ablation study on two training methods. k), and when the ‘Multi-path’ is removed, the translation quality decreases by 0.76 BLEU (average on all k). This shows that two training methods can both effectively improve the translation quality under low latency, especially ‘Future-guided’. 6 Related Work Previous ST methods are mainly divided into precise read / write policy and stronger predictive ability. For read / write policy, early policies used segmented translation, and applied full sentence translation to each segment (Bangalore et al., 2012; Cho and Esipova, 2016; Siahbani et al., 2018). Gu et al. (2017) trained an agent through reinforcement learning to decide read / write. Dalvi et al. (2018) proposed STATIC-RW, which first performing S’s READs, then alternately performing RW ’s WRITEs and READs. Ma et al. (2019) proposed wait-k policy, wherein first reads k tokens and then begin synchronizing write and read. Wait-k policy has achieved remarkable performance because it is easy to train and stable, and is widely used in simultaneous translation. Zheng et al. (2019a) generated the gold read / write sequence of input sentence by rules, and then trained an agent with the input"
2021.emnlp-main.581,D18-1337,0,0.0149278,"with MoE. weights. In the inference time, the universal model SiMT Early read / write policies in SiMT used is tested with arbitrary latency (test lagging). In segmented translation (Bangalore et al., 2012; Cho Sec.5, we compare the proposed two-stage training and Esipova, 2016; Siahbani et al., 2018). Grismethod with the one-stage training method which som II et al. (2014) predicted the final verb in SiMT. directly trains the parameters of experts and their Gu et al. (2017) trained a read / write agent with reweights together. We tried the block coordinate descent (BCD) inforcement learning. Alinejad et al. (2018) added a predict operation based on Gu et al. (2017). training (Peng et al., 2020) which is proposed to train the experts in the same function, but it is not Recent read / write policies fall into two cate7309 gories: fixed and adaptive. For the fixed policy, Dalvi et al. (2018) proposed STATIC-RW, and Ma et al. (2019) proposed wait-k policy, which always generates target k tokens lagging behind the source. Elbayad et al. (2020a) enhanced wait-k policy by sampling different k during training. Han et al. (2020) applied meta-learning in wait-k. Zhang et al. (2021) proposed future-guided training"
2021.emnlp-main.581,P19-1126,0,0.0490309,"Missing"
2021.emnlp-main.581,2020.iwslt-1.3,0,0.0240823,"-guided training for wait-k policy. Zhang and Feng (2021) proposed a charlevel wait-k policy. For the adaptive policy, Zheng et al. (2019a) trained an agent with gold read / write sequence. Zheng et al. (2019b) added a “delay” token {ε} to read. Arivazhagan et al. (2019) proposed MILk, which used a Bernoulli variable to determine writing. Ma et al. (2020) proposed MMA, which is the implementation of MILk on the Transformer. Zheng et al. (2020a) ensembled multiple wait-k models to develop a adaptive policy. Zhang and Zhang (2020) and Zhang et al. (2020) proposed adaptive segmentation policies. Bahar et al. (2020) and Wilken et al. (2020) proposed alignment-based chunking policy. A common weakness of the previous methods is that they all train separate models for different latency. Our method only needs a universal model to complete SiMT under all latency, and meanwhile achieve better translation quality. 5 Experiments 5.1 Datasets We evaluated our method on the following three datasets, the scale of which is from small to large. IWSLT151 English→Vietnamese (En-Vi) (133K pairs) (Cettolo et al., 2015) We use TED tst2012 (1553 pairs) as the validation set and TED tst2013 (1268 pairs) as the test set. Fol"
2021.emnlp-main.581,N12-1048,0,0.0369023,"domly sampling k (k in Eq.(8)) in every batch dur- heads in Transformer as an expert. Previous works always applied MoE for divering training. First-stage: Fix the weights Gti equal to h1 and pre-train expert parameters. Second-stage: sity. Our method makes the experts more regular in parameter space, which provides a method to jointly fine-tune the parameters of experts and their improves the translation quality with MoE. weights. In the inference time, the universal model SiMT Early read / write policies in SiMT used is tested with arbitrary latency (test lagging). In segmented translation (Bangalore et al., 2012; Cho Sec.5, we compare the proposed two-stage training and Esipova, 2016; Siahbani et al., 2018). Grismethod with the one-stage training method which som II et al. (2014) predicted the final verb in SiMT. directly trains the parameters of experts and their Gu et al. (2017) trained a read / write agent with reweights together. We tried the block coordinate descent (BCD) inforcement learning. Alinejad et al. (2018) added a predict operation based on Gu et al. (2017). training (Peng et al., 2020) which is proposed to train the experts in the same function, but it is not Recent read / write polic"
2021.emnlp-main.581,2015.iwslt-evaluation.1,0,0.0759931,"Missing"
2021.emnlp-main.581,D19-1308,0,0.0229894,"Eq.(8, 9) calculate Gti according to Eq.(10, 11) end calculate Ct according to Eq.(12) 14 Return Ct 9 10 11 12 suitable for our method, as the experts in MoE waitk have already assigned different functions. Therefore, our method can be stably trained through back-propagation directly. 4 Related Work Mixture of experts MoE was first proposed in multi-task learning (Jacobs et al., 1991; Caruana et al., 2004; Liu et al., 2018; Ma et al., 2018; Dutt et al., 2020). Recently, Shazeer et al. (2017) applied MoE in sequence learning. Some work (He 3.2.3 Training Method et al., 2018; Shen et al., 2019; Cho et al., 2019) applied MoE in diversity generation. Peng et al. We apply a two-stage training, both of which apply multipath training (Elbayad et al., 2020a), i.e., ran- (2020) applied MoE in MT and combined h − 1 domly sampling k (k in Eq.(8)) in every batch dur- heads in Transformer as an expert. Previous works always applied MoE for divering training. First-stage: Fix the weights Gti equal to h1 and pre-train expert parameters. Second-stage: sity. Our method makes the experts more regular in parameter space, which provides a method to jointly fine-tune the parameters of experts and their improves the tra"
2021.emnlp-main.581,N13-1073,0,0.0292163,"otherwise specified, all the results are reported on De-En with Transformer-Base(8 heads). 6.1 ical weights, avoiding the interference caused by multipath training. Ablation Study Performance on Various Difficulty Levels The difference between the target and source word order is one of the challenges of SiMT, where many word order inversions force to start translating before reading the aligned source words. To verify the performance of our method on SiMT with various difficulty levels, we evenly divided the test set into three parts: EASY, MIDDLE and HARD. Specifically, we used fast-align6 (Dyer et al., 2013) to align the source with the target, and then calculated the number of crosses in the alignments (number of reversed word orders), which is used as a basis to divide the test set (Chen et al., 2020; Zhang et al., 2021). After the division, the alignments in the EASY set are basically monotonous, Compared with ‘MMA’ and ‘MU’, our method performs better. ‘MU’ sets a threshold to perform SiMT under different latency and achieves good translation quality, but it is difficult to complete SiMT under low latency as it is a segmentation policy. As a fixed policy, our method maintains the advantage of"
2021.emnlp-main.581,2020.iwslt-1.2,0,0.149738,"ady assigned different functions. Therefore, our method can be stably trained through back-propagation directly. 4 Related Work Mixture of experts MoE was first proposed in multi-task learning (Jacobs et al., 1991; Caruana et al., 2004; Liu et al., 2018; Ma et al., 2018; Dutt et al., 2020). Recently, Shazeer et al. (2017) applied MoE in sequence learning. Some work (He 3.2.3 Training Method et al., 2018; Shen et al., 2019; Cho et al., 2019) applied MoE in diversity generation. Peng et al. We apply a two-stage training, both of which apply multipath training (Elbayad et al., 2020a), i.e., ran- (2020) applied MoE in MT and combined h − 1 domly sampling k (k in Eq.(8)) in every batch dur- heads in Transformer as an expert. Previous works always applied MoE for divering training. First-stage: Fix the weights Gti equal to h1 and pre-train expert parameters. Second-stage: sity. Our method makes the experts more regular in parameter space, which provides a method to jointly fine-tune the parameters of experts and their improves the translation quality with MoE. weights. In the inference time, the universal model SiMT Early read / write policies in SiMT used is tested with arbitrary latency (tes"
2021.emnlp-main.581,2020.iwslt-1.8,0,0.0534863,"Missing"
2021.emnlp-main.581,D14-1140,0,0.0797051,"Missing"
2021.emnlp-main.581,E17-1099,0,0.118495,"in = 3 ktrain = 5 ktrain = 7 ktrain = 9 7 9 Figure 1: Performance of wait-k models with different ktrain v.s. ktest on IWSLT15 En→Vi SiMT task. ktrain and ktest mean the number of source tokens to wait before performing translation during training and testing, respectively. the best translation performance under different latency with only one model (Ma et al., 2019, 2020). With fixed policy, e.g., wait-k policy (Ma et al., 1 Introduction 2019), the SiMT model has to wait for a fixed Simultaneous machine translation (SiMT) (Cho number of source words to be fed and then read and Esipova, 2016; Gu et al., 2017; Ma et al., 2019; one source word and output one target word alterArivazhagan et al., 2019) begins outputting trans- nately. In wait-k policy, the number of words to lation before reading the entire source sentence wait for can be different during training and testing, and hence has a lower latency compared to full- denoted as k train and ktest respectively, and the sentence machine translation. In practical appli- latency is determined by k . Figure 1 gives the test cations, SiMT usually has to fulfill the require- performance of the model trained with k train unments with different levels o"
2021.emnlp-main.581,2020.iwslt-1.5,0,0.035799,"hts together. We tried the block coordinate descent (BCD) inforcement learning. Alinejad et al. (2018) added a predict operation based on Gu et al. (2017). training (Peng et al., 2020) which is proposed to train the experts in the same function, but it is not Recent read / write policies fall into two cate7309 gories: fixed and adaptive. For the fixed policy, Dalvi et al. (2018) proposed STATIC-RW, and Ma et al. (2019) proposed wait-k policy, which always generates target k tokens lagging behind the source. Elbayad et al. (2020a) enhanced wait-k policy by sampling different k during training. Han et al. (2020) applied meta-learning in wait-k. Zhang et al. (2021) proposed future-guided training for wait-k policy. Zhang and Feng (2021) proposed a charlevel wait-k policy. For the adaptive policy, Zheng et al. (2019a) trained an agent with gold read / write sequence. Zheng et al. (2019b) added a “delay” token {ε} to read. Arivazhagan et al. (2019) proposed MILk, which used a Bernoulli variable to determine writing. Ma et al. (2020) proposed MMA, which is the implementation of MILk on the Transformer. Zheng et al. (2020a) ensembled multiple wait-k models to develop a adaptive policy. Zhang and Zhang (20"
2021.emnlp-main.581,K18-1056,0,0.0480247,"Missing"
2021.emnlp-main.581,D18-1149,0,0.0531574,"Missing"
2021.emnlp-main.581,D18-1317,0,0.0248621,"ltipath Wait-k’ have a (Zheng et al., 2020b). SiMT is often used as a little difference but most of them are fused together, downstream task of streaming automatic speech which shows some similarities in heads. In our recognition (ASR), but the results of streaming method, due to the clear division of labor, the expert ASR are not stable, especially the last recognized outputs are significantly different and regular in the source token (Li et al., 2020; Gaido et al., 2020; subspace distribution, which proves to be beneficial Zheng et al., 2020b). In each decoding step, we ran- to translation (Li et al., 2018). Besides, our method 7313 Expert Lagging KMoE ktest = 1 ktest = 3 ktest = 5 ktest = 7 ktest = 9 Test Lagging E1 1 E2 3 E3 5 E4 7 E5 9 E6 11 E7 13 E8 15 Optimal Model 10.66 9.83 9.35 8.65 8.34 13.90 12.88 12.63 12.55 12.32 13.82 13.75 13.52 12.82 12.55 11.67 11.62 11.61 11.58 11.08 13.07 13.70 13.82 14.04 14.33 13.49 13.51 13.6 14.10 14.69 11.89 12.55 12.93 13.53 13.79 11.50 12.16 12.54 12.73 12.90 ktrain = 3 ktrain = 5 ktrain = 9 ktrain = 9 ktrain = 9 Table 4: Weight of experts under different latency, averaged on 6 decoder layers at all decoding steps. ‘Optimal Model’: The optimal standard w"
2021.emnlp-main.581,2020.autosimtrans-1.6,0,0.0132644,"subspace distribution of the expert 6.2 Improvement on Robustness outputs in Figure 6. Robustness is another major challenge for SiMT The expert outputs in ‘Multipath Wait-k’ have a (Zheng et al., 2020b). SiMT is often used as a little difference but most of them are fused together, downstream task of streaming automatic speech which shows some similarities in heads. In our recognition (ASR), but the results of streaming method, due to the clear division of labor, the expert ASR are not stable, especially the last recognized outputs are significantly different and regular in the source token (Li et al., 2020; Gaido et al., 2020; subspace distribution, which proves to be beneficial Zheng et al., 2020b). In each decoding step, we ran- to translation (Li et al., 2018). Besides, our method 7313 Expert Lagging KMoE ktest = 1 ktest = 3 ktest = 5 ktest = 7 ktest = 9 Test Lagging E1 1 E2 3 E3 5 E4 7 E5 9 E6 11 E7 13 E8 15 Optimal Model 10.66 9.83 9.35 8.65 8.34 13.90 12.88 12.63 12.55 12.32 13.82 13.75 13.52 12.82 12.55 11.67 11.62 11.61 11.58 11.08 13.07 13.70 13.82 14.04 14.33 13.49 13.51 13.6 14.10 14.69 11.89 12.55 12.93 13.53 13.79 11.50 12.16 12.54 12.73 12.90 ktrain = 3 ktrain = 5 ktrain = 9 ktrai"
2021.emnlp-main.581,N19-4009,0,0.0208355,"to h1 . MoE Wait-k + FT Our method in Sec.3.2. 4 github.com/pytorch/fairseq/tree/ master/examples/simultaneous_translation 7310 Architecture Transformer-Small (4 heads) Transformer-Base (8 heads) Transformer-Big (16 heads) Expert Lagging KMoE [1, 6, 11, 16] [1, 3, 5, 7, 9, 11, 13, 15] [1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16] Table 1: The value of expert lagging KMoE for different Transformer settings. We compare our method with ‘MMA’ and ‘MU’ on De-En(Big) since they report their results on De-En with Transformer-Big. The implementation of all systems are adapted from Fairseq Library (Ott et al., 2019), and the setting is exactly the same as Ma et al. (2019) and Ma et al. (2020). To verify that our method is effective on Transformer with different head settings, we conduct experiments on three types of Transformer, where the settings are the same as Vaswani et al. (2017). For En-Vi, we apply Transformer-Small (4 heads). For En-Ro, we apply Transformer-Base (8 heads). For De-En, we apply both Transformer-Base and TransformerBig (16 heads). Table 2 reports the parameters of different SiMT systems on De-En(Big). To perform SiMT under different latency, both ‘Standard Wait-k’, ‘Optimal Wait-k’"
2021.emnlp-main.581,2020.acl-main.587,0,0.256571,"different latency. Experiments on IWSLT15 En→Vi, WMT16 En→Ro and WMT15 De→En show that although with only a universal SiMT model, our method can outperform strong baselines under all latency, including the state-of-the-art adaptive policy. Further analyses show the promising improvements of our method on efficiency and robustness. 2 Background Our method is based on mixture-of-experts approach, multi-head attention and wait-k policy, so we first briefly introduce them respectively. 2.1 Mixture of Experts Mixture of experts (MoE) (Jacobs et al., 1991; Eigen et al., 2013; Shazeer et al., 2017; Peng et al., 2020) is an ensemble learning approach that jointly trains a set of expert modules and mixes their outputs with various weights: MoE = n X i=1 Gi · Ei (1) 2.2 Multi-head Attention Multi-head attention is the key component of the state-of-the-art Transformer architecture (Vaswani et al., 2017), which allows the model to jointly attend to information from different representation subspaces. Multi-head attention contains h attention heads, where each head independently calculates its outputs between queries, keys and values through scaled dot-product attention. Since our method and wait-k policy are a"
2021.emnlp-main.581,W18-6319,0,0.012475,"ifferent SiMT systems on De-En(Big). To perform SiMT under different latency, both ‘Standard Wait-k’, ‘Optimal Wait-k’ and ‘MMA’ require multiple models, while ‘Multipath Wait-k’, ‘MU’ and ‘MoE Wait-k’ only need one trained model. Expert lagging KMoE in MoE wait-k is the hyperparameter we set, which represents the lagging of each expert. We did not conduct many searches on KMoE , but set it to be uniformly distributed in a reasonable lagging interval, as shown in Table 1. We will analyze the influence of different settings of KMoE in our method in Sec.6.5. We evaluate these systems with BLEU (Post, 2018) for translation quality and Average Lagging (AL5 ) (Ma et al., 2019) for latency. Given g (t), latency metric AL is calculated as: Systems #Para. Model per Model Num. Offline Wait-k Optimal Mulitpath MMA MU MoE Wait-k 209.91M 209.91M 209.91M 209.91M 222.51M 319.91M 209.91M 1 5 5 1 7 1 1 Total #Para. 209.91M 1049.55M 1049.55M 209.91M 1557.57M 319.91M 209.91M Table 2: The parameters of SiMT systems on DeEn(Transformer-Big) in our experiments. ‘#Para. per model’: The parameters of a single SiMT model. ‘Model Num.’: The number of SiMT models required to perform SiMT under multiple latency. ‘Total"
2021.emnlp-main.581,P16-1162,0,0.019277,"than 5 by hunki. After replacement, the vocabulary sizes are 17K and 7.7K for English and Vietnamese, respectively. WMT162 English→Romanian (En-Ro) (0.6M pairs) (Lee et al., 2018) We use newsdev2016 (1999 pairs) as the validation set and news-test2016 (1999 pairs) as the test set. WMT153 German→English (De-En) (4.5M pairs) Following the setting from Ma et al. (2019) and Ma et al. (2020), we use newstest2013 (3000 1 nlp.stanford.edu/projects/nmt/ www.statmt.org/wmt16/ 3 www.statmt.org/wmt15/ 2 pairs) as the validation set and newstest2015 (2169 pairs) as the test set. For En-Ro and De-En, BPE (Sennrich et al., 2016) is applied with 32K merge operations and the vocabulary is shared across languages. 5.2 System Settings We conducted experiments on following systems. Offline Conventional Transformer (Vaswani et al., 2017) model for full-sentence translation, decoding with greedy search. Standard Wait-k Standard wait-k policy proposed by Ma et al. (2019). When evaluating with the test lagging ktest , we apply the result from the model trained with ktrain , where ktrain = ktest . Optimal Wait-k An optimal variation of standard wait-k. When decoding with ktest , we traverse all models trained with different kt"
2021.emnlp-main.581,W18-1815,0,0.0197982,"rks always applied MoE for divering training. First-stage: Fix the weights Gti equal to h1 and pre-train expert parameters. Second-stage: sity. Our method makes the experts more regular in parameter space, which provides a method to jointly fine-tune the parameters of experts and their improves the translation quality with MoE. weights. In the inference time, the universal model SiMT Early read / write policies in SiMT used is tested with arbitrary latency (test lagging). In segmented translation (Bangalore et al., 2012; Cho Sec.5, we compare the proposed two-stage training and Esipova, 2016; Siahbani et al., 2018). Grismethod with the one-stage training method which som II et al. (2014) predicted the final verb in SiMT. directly trains the parameters of experts and their Gu et al. (2017) trained a read / write agent with reweights together. We tried the block coordinate descent (BCD) inforcement learning. Alinejad et al. (2018) added a predict operation based on Gu et al. (2017). training (Peng et al., 2020) which is proposed to train the experts in the same function, but it is not Recent read / write policies fall into two cate7309 gories: fixed and adaptive. For the fixed policy, Dalvi et al. (2018)"
2021.emnlp-main.581,2020.iwslt-1.29,0,0.0243418,"t-k policy. Zhang and Feng (2021) proposed a charlevel wait-k policy. For the adaptive policy, Zheng et al. (2019a) trained an agent with gold read / write sequence. Zheng et al. (2019b) added a “delay” token {ε} to read. Arivazhagan et al. (2019) proposed MILk, which used a Bernoulli variable to determine writing. Ma et al. (2020) proposed MMA, which is the implementation of MILk on the Transformer. Zheng et al. (2020a) ensembled multiple wait-k models to develop a adaptive policy. Zhang and Zhang (2020) and Zhang et al. (2020) proposed adaptive segmentation policies. Bahar et al. (2020) and Wilken et al. (2020) proposed alignment-based chunking policy. A common weakness of the previous methods is that they all train separate models for different latency. Our method only needs a universal model to complete SiMT under all latency, and meanwhile achieve better translation quality. 5 Experiments 5.1 Datasets We evaluated our method on the following three datasets, the scale of which is from small to large. IWSLT151 English→Vietnamese (En-Vi) (133K pairs) (Cettolo et al., 2015) We use TED tst2012 (1553 pairs) as the validation set and TED tst2013 (1268 pairs) as the test set. Following Raffel et al. (201"
2021.emnlp-main.581,P19-1582,0,0.0626386,"culated as:  t h as Ei i=1 . Meanwhile, under the premise of normalization, the weights of experts are no longer Eti = hHti WiO (9) 7308 3.2.2 Dynamic Weights for Experts Each expert has a clear division of labor through expert lagging KMoE . Then for different input and latency, we dynamically weight each expert with  h the predicted Gti i=1 , where Gti ∈ R can be considered as the confidence of expert outputs Eti . The factor to predict Gti consists of two components: • eti : The average cross-attention scores in the ith expert at step t, which are averaged over all source tokens read in (Zheng et al., 2019a). • k: External lagging k in Eq.(8). At step t, all eti and k are concatenated and fed through the multi-layer perceptron (MLP) to predict the confidence score βit of the ith expert, which are then normalized to calculate the weight Gti :  βit = tanh [et1 ; · · · ; eth ; k]Wi + bi (10) exp(βit ) Gti = Ph t l=1 exp(βl ) (11) where Wi and bi are parameters of MLP to predict  h Gti . Given expert outputs Eti i=1 and weights  t h Gi i=1 , the context vector Ct is calculatas: Ct = h X Gti · Eti (12) i=1 The algorithm details of proposMoE wait-k policy are shown in Algorithm 1. At decoding step"
2021.emnlp-main.581,2020.findings-emnlp.349,0,0.132711,"lbayad et al. (2020a) enhanced wait-k policy by sampling different k during training. Han et al. (2020) applied meta-learning in wait-k. Zhang et al. (2021) proposed future-guided training for wait-k policy. Zhang and Feng (2021) proposed a charlevel wait-k policy. For the adaptive policy, Zheng et al. (2019a) trained an agent with gold read / write sequence. Zheng et al. (2019b) added a “delay” token {ε} to read. Arivazhagan et al. (2019) proposed MILk, which used a Bernoulli variable to determine writing. Ma et al. (2020) proposed MMA, which is the implementation of MILk on the Transformer. Zheng et al. (2020a) ensembled multiple wait-k models to develop a adaptive policy. Zhang and Zhang (2020) and Zhang et al. (2020) proposed adaptive segmentation policies. Bahar et al. (2020) and Wilken et al. (2020) proposed alignment-based chunking policy. A common weakness of the previous methods is that they all train separate models for different latency. Our method only needs a universal model to complete SiMT under all latency, and meanwhile achieve better translation quality. 5 Experiments 5.1 Datasets We evaluated our method on the following three datasets, the scale of which is from small to large. IW"
2021.emnlp-main.581,2020.autosimtrans-1.1,0,0.0158915,". Han et al. (2020) applied meta-learning in wait-k. Zhang et al. (2021) proposed future-guided training for wait-k policy. Zhang and Feng (2021) proposed a charlevel wait-k policy. For the adaptive policy, Zheng et al. (2019a) trained an agent with gold read / write sequence. Zheng et al. (2019b) added a “delay” token {ε} to read. Arivazhagan et al. (2019) proposed MILk, which used a Bernoulli variable to determine writing. Ma et al. (2020) proposed MMA, which is the implementation of MILk on the Transformer. Zheng et al. (2020a) ensembled multiple wait-k models to develop a adaptive policy. Zhang and Zhang (2020) and Zhang et al. (2020) proposed adaptive segmentation policies. Bahar et al. (2020) and Wilken et al. (2020) proposed alignment-based chunking policy. A common weakness of the previous methods is that they all train separate models for different latency. Our method only needs a universal model to complete SiMT under all latency, and meanwhile achieve better translation quality. 5 Experiments 5.1 Datasets We evaluated our method on the following three datasets, the scale of which is from small to large. IWSLT151 English→Vietnamese (En-Vi) (133K pairs) (Cettolo et al., 2015) We use TED tst2012"
2021.emnlp-main.581,2020.emnlp-main.178,0,0.11638,"meta-learning in wait-k. Zhang et al. (2021) proposed future-guided training for wait-k policy. Zhang and Feng (2021) proposed a charlevel wait-k policy. For the adaptive policy, Zheng et al. (2019a) trained an agent with gold read / write sequence. Zheng et al. (2019b) added a “delay” token {ε} to read. Arivazhagan et al. (2019) proposed MILk, which used a Bernoulli variable to determine writing. Ma et al. (2020) proposed MMA, which is the implementation of MILk on the Transformer. Zheng et al. (2020a) ensembled multiple wait-k models to develop a adaptive policy. Zhang and Zhang (2020) and Zhang et al. (2020) proposed adaptive segmentation policies. Bahar et al. (2020) and Wilken et al. (2020) proposed alignment-based chunking policy. A common weakness of the previous methods is that they all train separate models for different latency. Our method only needs a universal model to complete SiMT under all latency, and meanwhile achieve better translation quality. 5 Experiments 5.1 Datasets We evaluated our method on the following three datasets, the scale of which is from small to large. IWSLT151 English→Vietnamese (En-Vi) (133K pairs) (Cettolo et al., 2015) We use TED tst2012 (1553 pairs) as the val"
2021.emnlp-main.581,2021.autosimtrans-1.1,1,0.744456,"ration based on Gu et al. (2017). training (Peng et al., 2020) which is proposed to train the experts in the same function, but it is not Recent read / write policies fall into two cate7309 gories: fixed and adaptive. For the fixed policy, Dalvi et al. (2018) proposed STATIC-RW, and Ma et al. (2019) proposed wait-k policy, which always generates target k tokens lagging behind the source. Elbayad et al. (2020a) enhanced wait-k policy by sampling different k during training. Han et al. (2020) applied meta-learning in wait-k. Zhang et al. (2021) proposed future-guided training for wait-k policy. Zhang and Feng (2021) proposed a charlevel wait-k policy. For the adaptive policy, Zheng et al. (2019a) trained an agent with gold read / write sequence. Zheng et al. (2019b) added a “delay” token {ε} to read. Arivazhagan et al. (2019) proposed MILk, which used a Bernoulli variable to determine writing. Ma et al. (2020) proposed MMA, which is the implementation of MILk on the Transformer. Zheng et al. (2020a) ensembled multiple wait-k models to develop a adaptive policy. Zhang and Zhang (2020) and Zhang et al. (2020) proposed adaptive segmentation policies. Bahar et al. (2020) and Wilken et al. (2020) proposed ali"
2021.emnlp-main.581,2020.acl-main.254,0,0.0689023,"lbayad et al. (2020a) enhanced wait-k policy by sampling different k during training. Han et al. (2020) applied meta-learning in wait-k. Zhang et al. (2021) proposed future-guided training for wait-k policy. Zhang and Feng (2021) proposed a charlevel wait-k policy. For the adaptive policy, Zheng et al. (2019a) trained an agent with gold read / write sequence. Zheng et al. (2019b) added a “delay” token {ε} to read. Arivazhagan et al. (2019) proposed MILk, which used a Bernoulli variable to determine writing. Ma et al. (2020) proposed MMA, which is the implementation of MILk on the Transformer. Zheng et al. (2020a) ensembled multiple wait-k models to develop a adaptive policy. Zhang and Zhang (2020) and Zhang et al. (2020) proposed adaptive segmentation policies. Bahar et al. (2020) and Wilken et al. (2020) proposed alignment-based chunking policy. A common weakness of the previous methods is that they all train separate models for different latency. Our method only needs a universal model to complete SiMT under all latency, and meanwhile achieve better translation quality. 5 Experiments 5.1 Datasets We evaluated our method on the following three datasets, the scale of which is from small to large. IW"
2021.emnlp-main.581,D19-1137,0,0.0755411,"culated as:  t h as Ei i=1 . Meanwhile, under the premise of normalization, the weights of experts are no longer Eti = hHti WiO (9) 7308 3.2.2 Dynamic Weights for Experts Each expert has a clear division of labor through expert lagging KMoE . Then for different input and latency, we dynamically weight each expert with  h the predicted Gti i=1 , where Gti ∈ R can be considered as the confidence of expert outputs Eti . The factor to predict Gti consists of two components: • eti : The average cross-attention scores in the ith expert at step t, which are averaged over all source tokens read in (Zheng et al., 2019a). • k: External lagging k in Eq.(8). At step t, all eti and k are concatenated and fed through the multi-layer perceptron (MLP) to predict the confidence score βit of the ith expert, which are then normalized to calculate the weight Gti :  βit = tanh [et1 ; · · · ; eth ; k]Wi + bi (10) exp(βit ) Gti = Ph t l=1 exp(βl ) (11) where Wi and bi are parameters of MLP to predict  h Gti . Given expert outputs Eti i=1 and weights  t h Gi i=1 , the context vector Ct is calculatas: Ct = h X Gti · Eti (12) i=1 The algorithm details of proposMoE wait-k policy are shown in Algorithm 1. At decoding step"
2021.findings-acl.91,N19-1423,0,0.00514339,"ently and reliably assess the consistency capacity of chatbots and achieve a high ranking correlation with the human evaluation. We release the framework and hope to help improve the consistency capacity of chatbots.1 1 Human: Blender: Human: Blender: Human: Plato: Human: Plato: Human: Plato: Table 1: Several human-bot conversations demonstrate that popular chatbots (DialoGPT, Blender, and Plato) generate inconsistent responses when talking to a human under some specific conditions. et al., 2020) have approached great progress due to the development of the large-scale pre-training approaches (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and the large amount of high-quality conversational datasets (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020). Though the success is indisputable and exciting, there is still a long way to build a truly human-like open-domain chatbot. Introduction In recent years, open-domain intelligent chatbots (Adiwardana et al., 2020b; Bao et al., 2020; Smith ∗ DialoGPT 762M What movies do you like most? The Unforgiven from Metallica. What do you think of the Unforgiven? I never heard of it. Blender 2.7B What do you like to cook? I only cook Ind"
2021.findings-acl.91,2021.ccl-1.108,0,0.0370867,"Missing"
2021.findings-acl.91,2020.sigdial-1.28,0,0.0644522,"Missing"
2021.findings-acl.91,2020.acl-demos.14,0,0.0135359,"similar questions. Therefore, to mimic such a contradiction occurrence process, we make chatbots to produce responses by asking chatbots related questions about previous facts and opinions. In this condition, generating appropriate questions is pretty important. Hence, we first extract entities about facts and opinions from the historical utterances, then employ a neural model to generate questions about the extracted entities. Entity Extraction Considering that chatbots usually generate contradictions when chatting about facts and opinions, we apply Named Entity Recognition tools in Stanza (Qi et al., 2020), a popular natural language analysis package, to extract named entities from utterance u2k containing person, organization, location, etc. 2 For example, for the utterance “I would love to visit New York next year.”, we can extract out two entities: “New York” and “next year”. Question Generation Model For question generation, we employ UniLM (Dong et al., 2019) model that is fine-tuned on the SQuAD dataset (Rajpurkar et al., 2016) with question generation task (Wangperawong, 2020). We utilize a public implementation and checkpoint.3 In our framework, given the entities extracted before and u"
2021.findings-acl.91,P19-1534,0,0.0283324,"Dialogue Evaluation track (Gunasekara et al., 2021). We reproduced the DialoFlow model based on GPT2-large (Radford et al., 2019) and fine-tuned it with BST dataset. Chatbots We select several popular open-domain chatbots in our experiments. Blender (BL) (Adiwardana et al., 2020a) is firstly pre-trained on Reddit dataset (Baumgartner et al., 2020) and then fine-tuned with high-quality human annotated dialogue datasets (BST), which containing four datasets: Blended Skill Talk (Smith et al., 2020), Wizard of Wikipedia (Dinan et al., 2019), ConvAI2 (Dinan et al., 2020), and Empathetic Dialogues (Rashkin et al., 2019). By fine-tuning, Blender can learn blend conversational skills of engagement, knowledge, empathy and personality. Blender has three model sizes: 90M, 2.7B, and 9.4B. Since 2.7B parameter model achieves the best performance in (Adiwardana et al., 2020a), we use the 2.7B version in our experiments. Plato (PL) (Bao et al., 2020) is an open-domain chatbot, pre-trained on Reddit dataset and finetuned with BST dataset, which is claimed to be 4.2 Experimental Settings We adopt four experimental paradigms to evaluate the effectiveness of the AIH. Bot-Bot Interaction. For bot-bot interaction, the maxi"
2021.findings-acl.91,2020.acl-main.183,0,0.135999,"improve the consistency capacity of chatbots.1 1 Human: Blender: Human: Blender: Human: Plato: Human: Plato: Human: Plato: Table 1: Several human-bot conversations demonstrate that popular chatbots (DialoGPT, Blender, and Plato) generate inconsistent responses when talking to a human under some specific conditions. et al., 2020) have approached great progress due to the development of the large-scale pre-training approaches (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and the large amount of high-quality conversational datasets (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020). Though the success is indisputable and exciting, there is still a long way to build a truly human-like open-domain chatbot. Introduction In recent years, open-domain intelligent chatbots (Adiwardana et al., 2020b; Bao et al., 2020; Smith ∗ DialoGPT 762M What movies do you like most? The Unforgiven from Metallica. What do you think of the Unforgiven? I never heard of it. Blender 2.7B What do you like to cook? I only cook Indian cuisine. How about you? I enjoy cooking Chinese food, especially the dumplings. Chinese food is delicious. I also like cooking the Chinese food. Plato 1.6B Do you like"
2021.findings-acl.91,2020.emnlp-main.539,0,0.126179,"nt is the lack of an effective and practical evaluation method. To estimate the consistency of chatbots, the most straightforward approach is to ask human annotators to distinguish whether the conversations generated from the chatbots are consistent or not. However, the instructions followed by annotators are often chosen ad-hoc, and there is no explicit definition, which leads to the relatively low interagreement in the human chatbot consistency evaluation (Mehri and Esk´enazi, 2020). As a result, several works have been proposed to develop automatic evaluation methods (Welleck et al., 2019; Song et al., 2020; Nie et al., 2020). While these methods can detect contradictions efficiently in the dialogue, they depend on the human-bot conversations, which is still cost-inefficient and tend to suffer from low quality (Deriu et al., 2020; Dinan et al., 2020). Besides, the occurrence rate of contradiction is low under this condition. All these problems slow down the development of consistency evaluation of dialogue systems severely. Towards that end, based on the observations: (i) chatbots are likely to generate contradictions when chatting about facts and opinions; (ii) answering the questions about the"
2021.findings-acl.91,P19-1363,0,0.114052,"consistency development is the lack of an effective and practical evaluation method. To estimate the consistency of chatbots, the most straightforward approach is to ask human annotators to distinguish whether the conversations generated from the chatbots are consistent or not. However, the instructions followed by annotators are often chosen ad-hoc, and there is no explicit definition, which leads to the relatively low interagreement in the human chatbot consistency evaluation (Mehri and Esk´enazi, 2020). As a result, several works have been proposed to develop automatic evaluation methods (Welleck et al., 2019; Song et al., 2020; Nie et al., 2020). While these methods can detect contradictions efficiently in the dialogue, they depend on the human-bot conversations, which is still cost-inefficient and tend to suffer from low quality (Deriu et al., 2020; Dinan et al., 2020). Besides, the occurrence rate of contradiction is low under this condition. All these problems slow down the development of consistency evaluation of dialogue systems severely. Towards that end, based on the observations: (i) chatbots are likely to generate contradictions when chatting about facts and opinions; (ii) answering the"
2021.findings-acl.91,N18-1101,0,0.0177248,"luator is generally a contradiction detection model. The Auto Evaluator take the response rk answered by Chatbot2 and the previous utterance u2k as input, and output the contradiction score yk . It can be formulated as: yk = fθ (rk , u2k ), (1) where fθ is the detection function and θ is the parameters. Compared to other contradiction detection methods that consider the whole dialogue, the Auto Evaluator can refrain from the noise contained in the whole dialogue. In practice, we select the Roberta-large model (Liu et al., 2019) fine-tuned on the Multi-Genre Natural Language Inference dataset (Williams et al., 2018) as the implementation of Auto Evaluator.4 Human Evaluator In traditional dialogue consistency evaluation methods, human judges are asked to read the whole dialogue and give an overall consistency score, usually 0 or 1. In our opinion, these methods suffer from high cost and low inter-agreement because there is no specific instruction, and it is too hard for human judges to give an overall score on the whole dialogue (Mehri and Esk´enazi, 2020). In our framework, human evaluators are only asked to decide if the response rk answered by Chatbot2 is consistent with the previous utterance u2k or n"
2021.findings-acl.91,2020.acl-demos.30,0,0.440182,"dogs very much. I have three dogs. I have three dogs too. One puppy and two Huskies. Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Yang Feng is the corresponding author. Work was done when Zekang Li and Zhengcong Fei were intern at WeChat AI. 1 https://github.com/ictnlp/AIH Current open-domain chatbots hold a superiority in generating fluent, engaging, and informative responses, but show the soft spot on consistency (Nie et al., 2020). As shown in Table 1, we present some interactive dialogue samples between human and several popular open-domain chatbots (e.g. DialoGPT (Zhang et al., 2020), Blender (Smith et al., 2020), and Plato (Bao et al., 2020)). All open1057 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1057–1067 August 1–6, 2021. ©2021 Association for Computational Linguistics domain chatbots occasionally generate responses that are contradictory with history when interacting with humans, which is really annoying and severely disrupts the communication once happening. Therefore, it is imperative to improve the consistency of the open-domain chatbots. However, one crucial reason that restricts consistency development is the lack of an ef"
2021.findings-emnlp.121,2020.acl-main.153,0,0.0325253,"Missing"
2021.findings-emnlp.121,2020.coling-main.389,0,0.0384556,"Missing"
2021.findings-emnlp.121,I17-1004,0,0.0177903,"Overall, with the proposed Gaussian mixture attention, our method has lower entropy than baseline, indicating that our method focuses more on some important words, which proves to be beneficial to translation (He et al., 2020; Zhang et al., 2019). 6.4 Alignment Quality The Gaussian mixture attention we proposed explicitly models the concentrated attention, so it is potential to help cross-attention achieve more accurate alignment between the target and the source. To explore this conjecture, we evaluate the alignment accuracy of our method on RWTH En→De alignment dataset 6 (Liu et al., 2016; Ghader and Monz, 2017; Tang et al., 2019). Following Luong et al. (2015) and Kuang et al. (2018), we force the models to produce the reference target words during inference to get the attention between source and target. We average the attention weights across all heads from the penultimate layer (Li et al., 2019; Ding et al., 2020a), where the source token with the highest attention weight is viewed as the alignment of the current target token. The alignment error rate (AER) (Och and Ney, 2003), precision (P.), and recall (R.) of our method are reported in Table 7. Our method achieves better alignment accuracy th"
2021.findings-emnlp.121,P18-1164,0,0.0159667,"entropy than baseline, indicating that our method focuses more on some important words, which proves to be beneficial to translation (He et al., 2020; Zhang et al., 2019). 6.4 Alignment Quality The Gaussian mixture attention we proposed explicitly models the concentrated attention, so it is potential to help cross-attention achieve more accurate alignment between the target and the source. To explore this conjecture, we evaluate the alignment accuracy of our method on RWTH En→De alignment dataset 6 (Liu et al., 2016; Ghader and Monz, 2017; Tang et al., 2019). Following Luong et al. (2015) and Kuang et al. (2018), we force the models to produce the reference target words during inference to get the attention between source and target. We average the attention weights across all heads from the penultimate layer (Li et al., 2019; Ding et al., 2020a), where the source token with the highest attention weight is viewed as the alignment of the current target token. The alignment error rate (AER) (Och and Ney, 2003), precision (P.), and recall (R.) of our method are reported in Table 7. Our method achieves better alignment accuracy than baseline, improving 5.58 on Transformer-Base and 3.47 on Transformer-Big"
2021.findings-emnlp.121,P19-1124,0,0.0724702,"n of each layer. X-axis is gi , which represents the weight of Gaussian mixture attention in the total attention, and Y-axis is the frequency with gi . the gating mechanism plays an important role and effectively fuses two types of attention. To analyze the relationship between the two types of attention in detail, we calculate the distribution of gating factor (gi in Eq.(13)) of each decoder layer, and show the result in Figure 5. To our surprise, our method makes the cross-attention in each decoder layer present a different division of labor, which confirms the conclusions of previous work (Li et al., 2019). Specifically, the bottom 6.2 Effect of Gating Mechanism layers (L1, L2) in the decoder emphasize on dotOur method applies a gating mechanism to fuse product attention and tend to capture global inforGaussian mixture attention and dot-product atten- mation; the middle layers (L3, L4) emphasize on tion. We conduct the ablation study of directly Gaussian mixture attention, which captures local averaging the two types of attention or only using information around the central word; two types of one of them in Table 4. Our method surpasses only attention in the top layer (L5, L6) are more balusing"
2021.findings-emnlp.121,C16-1291,0,0.0138385,"the source length. Overall, with the proposed Gaussian mixture attention, our method has lower entropy than baseline, indicating that our method focuses more on some important words, which proves to be beneficial to translation (He et al., 2020; Zhang et al., 2019). 6.4 Alignment Quality The Gaussian mixture attention we proposed explicitly models the concentrated attention, so it is potential to help cross-attention achieve more accurate alignment between the target and the source. To explore this conjecture, we evaluate the alignment accuracy of our method on RWTH En→De alignment dataset 6 (Liu et al., 2016; Ghader and Monz, 2017; Tang et al., 2019). Following Luong et al. (2015) and Kuang et al. (2018), we force the models to produce the reference target words during inference to get the attention between source and target. We average the attention weights across all heads from the penultimate layer (Li et al., 2019; Ding et al., 2020a), where the source token with the highest attention weight is viewed as the alignment of the current target token. The alignment error rate (AER) (Och and Ney, 2003), precision (P.), and recall (R.) of our method are reported in Table 7. Our method achieves bette"
2021.findings-emnlp.121,D15-1166,0,0.126522,"intermediate parameters (ω method predicts (ωi , µi , σi , Zi ) through a conversion layer. For the weight of every single Gaussian distribution, we normalize them with: ˆ i) ωi = Softmax (ω (10) For the mean, considering the word order differences between language, we directly predict its absolute position: ˆ i) µi = J · Sigmoid (µ (11) where J is the length of the source sentence. Note that, since the source position is discrete and will be truncated at the boundary, the attention weight sum is less than 1 without normalization, rather than fully normalized attention weight. Previous work (Luong et al., 2015; Yang et al., 2018; You et al., 2020) on applying Gaussian distribution hardly normalized it since the normalization of Gaussian attention weights results in unstable training. However, unnormalized attention weight leads to occasional spikes or dropouts in the attention and alignment (Battenberg et al., 2020). Therefore, to normalize Gaussian mixture attention meanwhile maintaining training stability, we propose an approximate normalization. Approximate normalization adjusts the variance according to the mean position to ensure that most of the weights are within the source sentence, which n"
2021.findings-emnlp.121,J03-1002,0,0.0474258,"his conjecture, we evaluate the alignment accuracy of our method on RWTH En→De alignment dataset 6 (Liu et al., 2016; Ghader and Monz, 2017; Tang et al., 2019). Following Luong et al. (2015) and Kuang et al. (2018), we force the models to produce the reference target words during inference to get the attention between source and target. We average the attention weights across all heads from the penultimate layer (Li et al., 2019; Ding et al., 2020a), where the source token with the highest attention weight is viewed as the alignment of the current target token. The alignment error rate (AER) (Och and Ney, 2003), precision (P.), and recall (R.) of our method are reported in Table 7. Our method achieves better alignment accuracy than baseline, improving 5.58 on Transformer-Base and 3.47 on Transformer-Big, which shows that modeling concentrated attention indeed improves the alignment quality of cross-attention. We use Gaussian mixture attention to model concentrated attention to make up for the dispersion of dot-product attention, especially on the long source. Entropy is often used to measure the dispersion of 6 distribution, where the higher entropy means that https://www-i6.informatik.rwth-aachen."
2021.findings-emnlp.121,N19-4009,0,0.0327605,"Missing"
2021.findings-emnlp.121,W18-6319,0,0.0395677,"Missing"
2021.findings-emnlp.121,P16-1162,0,0.15805,"Missing"
2021.findings-emnlp.121,N18-2074,0,0.0485002,"Missing"
2021.findings-emnlp.121,P19-1295,0,0.03094,"Missing"
2021.findings-emnlp.121,D18-1475,0,0.36146,"tion in cross-attention. Experiments and analyses we conducted on three datasets show that the proposed method outperforms the baseline and has significant improvement on alignment quality, N-gram accuracy, and long sentence translation. Distributed Attention Concentrated Attention Figure 1: An attention example of En→De translation when generating target words “Mäppchen” (English meaning: pencil case), showing the difference and complementarity between distributed attention and concentrated attention. long sentences, the attention distribution with dotproduct attention tends to be dispersed (Yang et al., 2018), which proved unfavorable for translation Recently, Neural Machine Translation (NMT) has (Zhang et al., 2019; Tang et al., 2019; He et al., been greatly improved with Transformer (Vaswani 2020). Second, dot-product attention is difficult et al., 2017), which mainly relies on the attento explicitly consider the source neighboring relation mechanism. The attention mechanism in tionship (Im and Cho, 2017; Sperber et al., 2018), Transformer consists of self-attention and crossresulting in ignoring the words with lower similarattention, where cross-attention is proved more ity but nearby the impor"
2021.findings-emnlp.121,2020.acl-main.687,0,0.224191,"He et al., been greatly improved with Transformer (Vaswani 2020). Second, dot-product attention is difficult et al., 2017), which mainly relies on the attento explicitly consider the source neighboring relation mechanism. The attention mechanism in tionship (Im and Cho, 2017; Sperber et al., 2018), Transformer consists of self-attention and crossresulting in ignoring the words with lower similarattention, where cross-attention is proved more ity but nearby the important word which determine important to translation quality than self-attention phrase structure or morphology. (He et al., 2020; You et al., 2020). Even if the Research in linguistics and cognitive science sugself-attention is modified to a fixed template, the gests that human attention to language can be ditranslation quality would not significantly reduce vided into two categories: distributed attention and (You et al., 2020), while cross-attention plays an concentrated attention (Jacob and Bruce, 1973; Ito irreplaceable role in NMT. Cross-attention in Transet al., 1998; Brand and Johnson, 2018). Specififormer is realized by dot-product attention, which cally, distributed attention is scattered on all source calculates the attention d"
2021.findings-emnlp.121,R19-1136,0,0.0230054,"Missing"
2021.findings-emnlp.121,P16-1008,0,0.0160366,"ted attention. To model the concentrated attention, we apply GMM to construct the Gaussian mixture attention, which effectively resolves the weakness of dot-product attention. Experiments show that the proposed method outperforms the strong baseline on three datasets. Further analyses show the specific advantages of the proposed method in attention distribution, alignment quality, N-gram accuracy, and long sentence translation. To analyze the improvement of our method on sentences with different lengths, we group the sentences into 6 sets according to the source length (Bahdanau et al., 2014; Tu et al., 2016), and report the BLEU scores on each set in Figure 7. Compared with Baseline, our method has a more significant improvement in long sentences, with +1.36 BLEU on (30, 40], +1.06 BLEU on (40, 50], Acknowledgements and +4.14 BLEU on (50, +∞]. Our method significantly improves the long sentence translation We thank all the anonymous reviewers for their by modeling concentrated cross-attention. When insightful and valuable comments. This work was the source sentence is very long, dot-product at- supported by National Key R&D Program of China tention fairly pays attention to every source word (NO."
2021.findings-emnlp.121,P19-1580,0,0.0189189,"nthesis Network Our Method Our Method+Norm. Zi ωi µi ˆ i) ˆ i) 1 exp (ω µi−1 + exp (µ q ˆ i ) J ·Sigmoid (µ ˆ i) 2πσi2 Softmax (ω q ˆ i ) J ·Sigmoid (µ ˆ i) 2πσi2 Softmax (ω σi p ˆ i ) /2 exp (−σ Normalized Eq.(12) Approximate ˆ i) J ·Sigmoid (σ Strict None Table 1: Conversion method of synthesis network, our method, and the strict-normalized variant of our method. There are three differences between the proposed method and previous methods. 1) Most previous methods only focus on self-attention, while we consider the cross-attention, which is proved to be more critical to translation quality (Voita et al., 2019; Tang et al., 2019; You et al., 2020; Ding et al., 2020b). 2) Previous methods usually multiply dotproduct attention with a position-related bias or mask to model position. Our method additionally introduces a concentrated attention to compensate for dot-product attention, rather than simple bias. 3) Gaussian distribution is widely used in previous position modeling, while we use the more flexible GMM for complex cross-attention. 5 Experiments We conducted experiments on three datasets and compare with the baseline and previous methods to evaluate the performance of the proposed method. 5.1 D"
2021.findings-emnlp.121,D19-1145,0,0.0412705,"Missing"
2021.findings-emnlp.29,D18-1149,0,0.0658785,"Missing"
2021.findings-emnlp.29,P19-1177,0,0.039526,"Missing"
2021.findings-emnlp.29,W19-6622,0,0.063172,"Missing"
2021.findings-emnlp.29,W18-6301,0,0.0672122,"Missing"
2021.findings-emnlp.29,W16-2323,0,0.0803376,"Missing"
2021.findings-emnlp.29,2020.emnlp-main.82,1,0.798934,"Missing"
2021.naacl-main.308,D19-1165,0,0.22527,"domains will overlap with each other, and retaining compared with several strong baselines. these overlapped parameters can balance over all the domains. This assumption is feasible when the 1 Introduction domains are similar, but when the divergence of Neural machine translation (NMT) models (Kalch- the domains is large, it is not reasonable anymore. brenner and Blunsom, 2013; Cho et al., 2014; In contrast, the methods with domain-specific netSutskever et al., 2014; Bahdanau et al., 2015; works (Dakwale and Monz, 2017; Wang et al., Gehring et al., 2017; Vaswani et al., 2017) are data- 2019; Bapna and Firat, 2019; Gu et al., 2019) can be often (but not always) immune to domain diverdriven and hence require large-scale training data to gence as it can capture domain-specific features. achieve good performance (Zhang et al., 2019a). In But unfortunately, as the number of domains inpractical applications, NMT models usually need creases, the parameters of this kind of methods to produce translation for some specific domains with only a small quantity of in-domain data avail- will surge. Besides, the structure of these networks needs to be carefully designed and tuned, which able, so domain adaptation is"
2021.naacl-main.308,D17-1156,0,0.0271646,"Missing"
2021.naacl-main.308,P17-2061,0,0.0302237,"Missing"
2021.naacl-main.308,P05-1066,0,0.207987,"in and in-domain, and ’Avg.’ denotes the average BLEU of the two test sets. ’NP’, ’WP’, ’KD’, and ’FT’ represent neuron pruning, weight pruning, knowledge distillation, and fine-tuning, respectively. The numbers on the right of ’PTE’ denote that this training phase is based on the previous corresponding models. After knowledge distillation, the parameters in the pruned model (system 10, 13) are fixed, so the general-domain BLEU is unchanged after fine-tuning (system 11, 14). * and ** mean the improvements over the MLL method is statistically significant (ρ < 0.05 and ρ < 0.01, respectively). (Collins et al., 2005) The second category indcludes the following three systems: • Full Bias (Michel and Neubig, 2018) This method adds domain-specific bias term to the output softmax layer and only updates the term as other parts of the general-domain model keep fixed. • Adapter (Bapna and Firat, 2019) This methods injects domain-specific adapter modules into each layer of the general-domain model. Each adapter contains a normalization layer and two linear projection layers. The adapter size is set to 64. • Multiple-output Layer Learning (MLL) (Dakwale and Monz, 2017) The method modifies the general-domain model"
2021.naacl-main.308,2020.emnlp-main.364,0,0.0672193,"Missing"
2021.naacl-main.308,2020.coling-main.381,1,0.796581,"adaptation that can not only deal with large general-domain data and then is adapted to specific domain divergence during domain transferring but ∗ also keep a stable model size even with multiple doCorresponding author: Yang Feng. Reproducible code: https://github.com/ictnlp/PTE-NMT. mains. Inspired by the analysis work on NMT (Bau 3942 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3942–3952 June 6–11, 2021. ©2021 Association for Computational Linguistics et al., 2019; Voita et al., 2019; Gu and Feng, 2020), we find that only some important parameters in a well-trained NMT model play an important role when generating the translation and unimportant parameters can be erased without affecting the translation quality too much. According to these findings, we can preserve important parameters for generaldomain translation, while tuning unimportant parameters for in-domain translation. To achieve this, we first train a model on the general domain and then shrink the model with neuron pruning or weight pruning methods, only retaining the important neurons/parameters. To ensure the model can still perf"
2021.naacl-main.308,N19-1312,1,0.912034,"ith each other, and retaining compared with several strong baselines. these overlapped parameters can balance over all the domains. This assumption is feasible when the 1 Introduction domains are similar, but when the divergence of Neural machine translation (NMT) models (Kalch- the domains is large, it is not reasonable anymore. brenner and Blunsom, 2013; Cho et al., 2014; In contrast, the methods with domain-specific netSutskever et al., 2014; Bahdanau et al., 2015; works (Dakwale and Monz, 2017; Wang et al., Gehring et al., 2017; Vaswani et al., 2017) are data- 2019; Bapna and Firat, 2019; Gu et al., 2019) can be often (but not always) immune to domain diverdriven and hence require large-scale training data to gence as it can capture domain-specific features. achieve good performance (Zhang et al., 2019a). In But unfortunately, as the number of domains inpractical applications, NMT models usually need creases, the parameters of this kind of methods to produce translation for some specific domains with only a small quantity of in-domain data avail- will surge. Besides, the structure of these networks needs to be carefully designed and tuned, which able, so domain adaptation is applied to address"
2021.naacl-main.308,D13-1176,0,0.183486,"Missing"
2021.naacl-main.308,W18-2705,0,0.0817229,"in-domain data. Despite its convenience we propose a method of “divide and conquer” for use and high-quality for in-domain translation, which is based on the importance of neurons this method suffers from catastrophic forgetting or parameters for the translation model. In which leads to poor performance in the previous this method, we first prune the model and domains. Regularization-based methods (Dakwale only keep the important neurons or parameters, making them responsible for both generaland Monz, 2017; Thompson et al., 2019; Barone domain and in-domain translation. Then we et al., 2017; Khayrallah et al., 2018) instead infurther train the pruned model supervised by troduce an additional loss to the original objecthe original whole model with knowledge distive so that the translation model can trade off betillation. Last we expand the model to the tween general-domain and in-domain. This kind of original size and fine-tune the added parammethods usually has all the parameters shared by eters for the in-domain translation. We congeneral-domain and in-domain, with the assumpducted experiments on different language pairs and domains and the results show that our tion that the optimal parameter spaces fo"
2021.naacl-main.308,D16-1139,0,0.0207491,"toolkit called Fairseqpy (Ott et al., 2019) released by Facebook as our Transformer system. The contrast methods can be divided into two categories. The models of the first category are capacity-fixed while the second category are capacity-increased. The first category includes the following systems: • General This baseline system is trained only with the general-domain training data. • In This baseline system is trained only with the in-domain training data. • Fine-tuning (Luong and Manning, 2015) This method just continues to train the general-domain model with the in-domain data. • SeqKD (Kim and Rush, 2016) The in-domain source sentences are first translated by the generaldomain model. Then the model is further trained with the combined pseudo and real data. • Multi-objective Learning (MOL) (Dakwale and Monz, 2017) This method is based on the Finetuning method. Besides minimizing the loss between the ground truth words and the output distribution of the network, this method also minimizes the cross-entropy between the output distribution of the general-domain model and the network. The final objective is: LMOL (θ) = L(θ) + αLKD (θ) (13) where α is the hyper-parameter which controls the contribut"
2021.naacl-main.308,kobus-etal-2017-domain,0,0.0174612,"d conquer”. train new models with them separately. We compare our weight pruning based method with the EWC and MLL methods. The results are shown in Figure 3. It shows that we can get larger improvements as the data divergence gets larger. 6 Related Work Domain Adaptation Recent work on DA can be divided into two categories according to the use of training data. The first category, which is also referred to as multi-domain adaptation, needs the training data from all of the domains. Chu et al. (2017) fine-tunes the model with the mix of the general-domain data and over-sampled in-domain data. Kobus et al. (2017) adds domain-specific tags to each sentence. Zhang et al. (2019b) applies curriculum learning to the DA problem. Britz et al. (2017) adds a discriminator to extract com5.4 Effects of Data Distribution Divergence mon features across domains. There are also some work (Zeng et al., 2018, 2019; Gu et al., 2019) that To further prove that our method is better at dealing with large domain divergence, we conduct experi- adds domain-specific modules to the model to preserve the domain-specific features. Currey et al. ments on the En-Fr translation task. Following the (2020) distills multiple expert mo"
2021.naacl-main.308,2015.iwslt-evaluation.11,0,0.242853,"source and target languages are also built on the corresponding general-domain data. 4.2 Systems We use the open-source toolkit called Fairseqpy (Ott et al., 2019) released by Facebook as our Transformer system. The contrast methods can be divided into two categories. The models of the first category are capacity-fixed while the second category are capacity-increased. The first category includes the following systems: • General This baseline system is trained only with the general-domain training data. • In This baseline system is trained only with the in-domain training data. • Fine-tuning (Luong and Manning, 2015) This method just continues to train the general-domain model with the in-domain data. • SeqKD (Kim and Rush, 2016) The in-domain source sentences are first translated by the generaldomain model. Then the model is further trained with the combined pseudo and real data. • Multi-objective Learning (MOL) (Dakwale and Monz, 2017) This method is based on the Finetuning method. Besides minimizing the loss between the ground truth words and the output distribution of the network, this method also minimizes the cross-entropy between the output distribution of the general-domain model and the network."
2021.naacl-main.308,P18-2050,0,0.0891379,"and ’FT’ represent neuron pruning, weight pruning, knowledge distillation, and fine-tuning, respectively. The numbers on the right of ’PTE’ denote that this training phase is based on the previous corresponding models. After knowledge distillation, the parameters in the pruned model (system 10, 13) are fixed, so the general-domain BLEU is unchanged after fine-tuning (system 11, 14). * and ** mean the improvements over the MLL method is statistically significant (ρ < 0.05 and ρ < 0.01, respectively). (Collins et al., 2005) The second category indcludes the following three systems: • Full Bias (Michel and Neubig, 2018) This method adds domain-specific bias term to the output softmax layer and only updates the term as other parts of the general-domain model keep fixed. • Adapter (Bapna and Firat, 2019) This methods injects domain-specific adapter modules into each layer of the general-domain model. Each adapter contains a normalization layer and two linear projection layers. The adapter size is set to 64. • Multiple-output Layer Learning (MLL) (Dakwale and Monz, 2017) The method modifies the general-domain model by adding domain-specific output layer for the in-domain and learning these domain specific param"
2021.naacl-main.308,P10-2041,0,0.0593808,"each sentence. Zhang et al. (2019b) applies curriculum learning to the DA problem. Britz et al. (2017) adds a discriminator to extract com5.4 Effects of Data Distribution Divergence mon features across domains. There are also some work (Zeng et al., 2018, 2019; Gu et al., 2019) that To further prove that our method is better at dealing with large domain divergence, we conduct experi- adds domain-specific modules to the model to preserve the domain-specific features. Currey et al. ments on the En-Fr translation task. Following the (2020) distills multiple expert models into a single method in Moore and Lewis (2010), we score and student model. The work of Liang et al. (2020) rank each in-domain sentence pair by calculating has a similar motivation with ours which also fix the per-word cross-entropy difference between the the important parameters and prune the unimporgeneral- and in-domain language model: tant parameters. Compared with their method, our method doesn’t need to store the general-domain Score = (HG (s) − HI (s)) + (HG (t) − HI (t)) training data and our method has less degradation (16) on general-domain because we adopt the knowlwhere H denotes the language model which is edge distillation"
2021.naacl-main.308,N19-4009,0,0.0178902,"h et al., 2016) on the general-domain data and then applied to both the general-domain and in-domain data. Then we filter out the sentences which are longer 1 2 http://www.statmt.org/moses/ https://nlp.stanford.edu/ than 128 sub-words. For the Zh-En translation task, 44K size of the Chinese dictionary and 33K size of the English dictionary are built based on the general-domain data. For the En-Fr and En-De tasks, 32K size of the dictionaries for the source and target languages are also built on the corresponding general-domain data. 4.2 Systems We use the open-source toolkit called Fairseqpy (Ott et al., 2019) released by Facebook as our Transformer system. The contrast methods can be divided into two categories. The models of the first category are capacity-fixed while the second category are capacity-increased. The first category includes the following systems: • General This baseline system is trained only with the general-domain training data. • In This baseline system is trained only with the in-domain training data. • Fine-tuning (Luong and Manning, 2015) This method just continues to train the general-domain model with the in-domain data. • SeqKD (Kim and Rush, 2016) The in-domain source sen"
2021.naacl-main.308,P02-1040,0,0.109898,"Missing"
2021.naacl-main.308,W18-6319,0,0.0204665,"Missing"
2021.naacl-main.308,2020.findings-emnlp.381,0,0.0161111,"rms to let the model parameters stay close to their original values. Dakwale and Monz (2017) minimizes the cross-entropy between the output distribution of the general-domain model and the fine-tuned model. Michel and Neubig (2018) adds domain-specific softmax bias term to the output layer. Bapna and Firat (2019) injects domain-specific adapter modules into each layer of the general-domain model. Wuebker et al. (2018) only saves the domain-specific offset based on the general-domain model. Wang et al. (2020b) achieves efficient lifelong learning by establishing complementary learning systems. Sato et al. (2020) adapts the vocabulary of a pre-trained NMT model to the target domain. Overall, our work is related to the second type of approach, which is more flexible and convenient in practice. The work of Thompson et al. (2019) and Dakwale and Monz (2017) are most related to our work. Compared with Thompson et al. (2019), our work is better at dealing with large domain divergence, since we add domain-specific parts to the model. In contrast to Dakwale and Monz (2017), our model divides each layer of the model into domain-shared and domain-specific parts, which increases the depth of the in-domain model"
2021.naacl-main.308,P19-1022,0,0.0344311,"Missing"
2021.naacl-main.308,K16-1029,0,0.0208279,"Missing"
2021.naacl-main.308,P16-1162,0,0.0609,"We tokenize and truecase the corpora. English→German. For this task, generaldomain data is from the WMT16 En-De translation task which is mainly News texts. It contains about 4.5M sentence pairs. We choose the newstest2013 for validation and newstest2014 for test. For the indomain data, we use the parallel training data from the IWSLT 2015 which is mainly from the Spoken domain. It contains about 194K sentences. We choose the 2014test for validation and the 2015test for test. We tokenize and truecase the corpora. Besides, integrating operations of 32K, 32K, and 30K are performed to learn BPE (Sennrich et al., 2016) on the general-domain data and then applied to both the general-domain and in-domain data. Then we filter out the sentences which are longer 1 2 http://www.statmt.org/moses/ https://nlp.stanford.edu/ than 128 sub-words. For the Zh-En translation task, 44K size of the Chinese dictionary and 33K size of the English dictionary are built based on the general-domain data. For the En-Fr and En-De tasks, 32K size of the dictionaries for the source and target languages are also built on the corresponding general-domain data. 4.2 Systems We use the open-source toolkit called Fairseqpy (Ott et al., 201"
2021.naacl-main.308,N19-1209,0,0.227063,"omain data and then continuing explosion. To address these three problems, to train on in-domain data. Despite its convenience we propose a method of “divide and conquer” for use and high-quality for in-domain translation, which is based on the importance of neurons this method suffers from catastrophic forgetting or parameters for the translation model. In which leads to poor performance in the previous this method, we first prune the model and domains. Regularization-based methods (Dakwale only keep the important neurons or parameters, making them responsible for both generaland Monz, 2017; Thompson et al., 2019; Barone domain and in-domain translation. Then we et al., 2017; Khayrallah et al., 2018) instead infurther train the pruned model supervised by troduce an additional loss to the original objecthe original whole model with knowledge distive so that the translation model can trade off betillation. Last we expand the model to the tween general-domain and in-domain. This kind of original size and fine-tune the added parammethods usually has all the parameters shared by eters for the in-domain translation. We congeneral-domain and in-domain, with the assumpducted experiments on different language"
2021.naacl-main.308,tian-etal-2014-um,0,0.0671639,"Missing"
2021.naacl-main.308,P19-1580,0,0.0241973,"ith large-scale main adaptation that can not only deal with large general-domain data and then is adapted to specific domain divergence during domain transferring but ∗ also keep a stable model size even with multiple doCorresponding author: Yang Feng. Reproducible code: https://github.com/ictnlp/PTE-NMT. mains. Inspired by the analysis work on NMT (Bau 3942 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3942–3952 June 6–11, 2021. ©2021 Association for Computational Linguistics et al., 2019; Voita et al., 2019; Gu and Feng, 2020), we find that only some important parameters in a well-trained NMT model play an important role when generating the translation and unimportant parameters can be erased without affecting the translation quality too much. According to these findings, we can preserve important parameters for generaldomain translation, while tuning unimportant parameters for in-domain translation. To achieve this, we first train a model on the general domain and then shrink the model with neuron pruning or weight pruning methods, only retaining the important neurons/parameters. To ensure the"
2021.naacl-main.308,2020.emnlp-main.78,0,0.0316749,"semble weighting for inference. Khayrallah et al. (2018) and Thompson et al. (2019) add regularization terms to let the model parameters stay close to their original values. Dakwale and Monz (2017) minimizes the cross-entropy between the output distribution of the general-domain model and the fine-tuned model. Michel and Neubig (2018) adds domain-specific softmax bias term to the output layer. Bapna and Firat (2019) injects domain-specific adapter modules into each layer of the general-domain model. Wuebker et al. (2018) only saves the domain-specific offset based on the general-domain model. Wang et al. (2020b) achieves efficient lifelong learning by establishing complementary learning systems. Sato et al. (2020) adapts the vocabulary of a pre-trained NMT model to the target domain. Overall, our work is related to the second type of approach, which is more flexible and convenient in practice. The work of Thompson et al. (2019) and Dakwale and Monz (2017) are most related to our work. Compared with Thompson et al. (2019), our work is better at dealing with large domain divergence, since we add domain-specific parts to the model. In contrast to Dakwale and Monz (2017), our model divides each layer o"
2021.naacl-main.308,2020.emnlp-main.39,0,0.0218408,"semble weighting for inference. Khayrallah et al. (2018) and Thompson et al. (2019) add regularization terms to let the model parameters stay close to their original values. Dakwale and Monz (2017) minimizes the cross-entropy between the output distribution of the general-domain model and the fine-tuned model. Michel and Neubig (2018) adds domain-specific softmax bias term to the output layer. Bapna and Firat (2019) injects domain-specific adapter modules into each layer of the general-domain model. Wuebker et al. (2018) only saves the domain-specific offset based on the general-domain model. Wang et al. (2020b) achieves efficient lifelong learning by establishing complementary learning systems. Sato et al. (2020) adapts the vocabulary of a pre-trained NMT model to the target domain. Overall, our work is related to the second type of approach, which is more flexible and convenient in practice. The work of Thompson et al. (2019) and Dakwale and Monz (2017) are most related to our work. Compared with Thompson et al. (2019), our work is better at dealing with large domain divergence, since we add domain-specific parts to the model. In contrast to Dakwale and Monz (2017), our model divides each layer o"
2021.naacl-main.308,D18-1104,0,0.0171022,"model and the fine-tuned model for generating. Saunders et al. (2019) investigates adaptive ensemble weighting for inference. Khayrallah et al. (2018) and Thompson et al. (2019) add regularization terms to let the model parameters stay close to their original values. Dakwale and Monz (2017) minimizes the cross-entropy between the output distribution of the general-domain model and the fine-tuned model. Michel and Neubig (2018) adds domain-specific softmax bias term to the output layer. Bapna and Firat (2019) injects domain-specific adapter modules into each layer of the general-domain model. Wuebker et al. (2018) only saves the domain-specific offset based on the general-domain model. Wang et al. (2020b) achieves efficient lifelong learning by establishing complementary learning systems. Sato et al. (2020) adapts the vocabulary of a pre-trained NMT model to the target domain. Overall, our work is related to the second type of approach, which is more flexible and convenient in practice. The work of Thompson et al. (2019) and Dakwale and Monz (2017) are most related to our work. Compared with Thompson et al. (2019), our work is better at dealing with large domain divergence, since we add domain-specific"
2021.naacl-main.308,D19-1078,0,0.0487036,"Missing"
2021.naacl-main.308,D18-1041,0,0.0172602,"n DA can be divided into two categories according to the use of training data. The first category, which is also referred to as multi-domain adaptation, needs the training data from all of the domains. Chu et al. (2017) fine-tunes the model with the mix of the general-domain data and over-sampled in-domain data. Kobus et al. (2017) adds domain-specific tags to each sentence. Zhang et al. (2019b) applies curriculum learning to the DA problem. Britz et al. (2017) adds a discriminator to extract com5.4 Effects of Data Distribution Divergence mon features across domains. There are also some work (Zeng et al., 2018, 2019; Gu et al., 2019) that To further prove that our method is better at dealing with large domain divergence, we conduct experi- adds domain-specific modules to the model to preserve the domain-specific features. Currey et al. ments on the En-Fr translation task. Following the (2020) distills multiple expert models into a single method in Moore and Lewis (2010), we score and student model. The work of Liang et al. (2020) rank each in-domain sentence pair by calculating has a similar motivation with ours which also fix the per-word cross-entropy difference between the the important paramete"
2021.naacl-main.308,P19-1426,1,0.888256,"Missing"
2021.naacl-main.308,N19-1189,0,0.0180527,"edge distillation method. trained with Srilm (Stolcke, 2002), s and t denote the source and target sentence. Then, we split the The second category, which is also referred to in-domain data into four parts with equal size and as continual learning, only needs the data from the 3949 new domain and the model in use. The biggest challenge for this kind of work is the catastrophic forgetting. Luong and Manning (2015) fine-tunes the general-domain model with the in-domain data. Freitag and Al-Onaizan (2016) ensembles the general-domain model and the fine-tuned model for generating. Saunders et al. (2019) investigates adaptive ensemble weighting for inference. Khayrallah et al. (2018) and Thompson et al. (2019) add regularization terms to let the model parameters stay close to their original values. Dakwale and Monz (2017) minimizes the cross-entropy between the output distribution of the general-domain model and the fine-tuned model. Michel and Neubig (2018) adds domain-specific softmax bias term to the output layer. Bapna and Firat (2019) injects domain-specific adapter modules into each layer of the general-domain model. Wuebker et al. (2018) only saves the domain-specific offset based on t"
C10-2033,P05-1066,0,0.0967175,"s, including beam size, phrase table limit and so on, were the same, so the performance was compared fairly. First, we will show the performance of shiftreduce algorithm on three data sets with large training data sets (Section 4.1). Then, we will analyze the performance elaborately in terms of accuracy, speed and search ability with a smaller 289 training data set (Section 4.2). All experiments were done on Chinese-to-English translation tasks and all results are reported with case insensitive BLEU score. Statistical significance were computed using the sign-test described in Collins et al. (Collins et al., 2005). 4.1 Performance Evaluation NIST06 30.24 30.27 30.35 —— 30.47 NIST08 25.08 23.80 26.23** 25.09 26.67** speed 4.827 1.501 4.335 3.856 4.126 NIST05 35.80 35.03 36.56** 35.84 36.42** speed 7.142 1.811 6.276 5.008 5.432 (a) We did three experiments to compare the performance of the shift-reduce decoder, Moses and the decoder with ITG constraints using cover vector (denoted as CV). 2 The shift-reduce decoder decoded with two sets of parameters: one was tuned by itself (denoted as SR) and the other was tuned by Moses (denoted as SR-same), using MERT (Och, 2003). Two searching algorithms of Moses ar"
C10-2033,D08-1089,0,0.198885,"ransitions LShift and RShift push [i, j] into St , they check whether [i, j] is adjacent to the top block of St . If so, they change the top block into the merged block directly. In practical implementation, in order to further restrict search space, distortion limit is applied besides ITG constraints: a source phrase can be covered next only when it is ITG-legal and its distortion does not exceed distortion limit. The distortion d is calculated by d = |starti − endi−1 − 1|, where starti is the start position of the current phrase and endi−1 is the last position of the last translated phrase. Galley and Manning (2008) present a hierarchical phrase reordering model aimed at improving non-local reorderings. Via the hierarchical mergence of two blocks, the orientation of long distance words can be computed. Their shift-reduce algorithm does not import ITG constraints and admits the translation violating ITG constraints. Zens et al. (2004) introduce a left-toright decoding algorithm with ITG constraints on the alignment template system (Och et al., 1999). Their algorithm processes candidate source phrases one by one through the whole search space and checks if the candidate phrase complies with ITG constraints"
C10-2033,W05-1506,0,0.0420627,"models easily. Xiong et al. (2006) propose a BTG-based model, which uses the context to determine the orientation of two adjacent spans. It employs the cube-time CYK algorithm. 4 3 Related Work St [0] [0][5] [0][5][1] [0][5][1][3] [0][5][1][3][4] [0][5][1][3, 4] [0][5][1][3, 4][2] [0][5][1][2, 4] [0][5][1, 4] [0][1, 5] [0, 5] [0, 5][6] [0, 6] Experiments We compare the shift-reduce decoder with the state-of-the-art decoder Moses (Koehn et al., 2007). The shift-reduce decoder was implemented by modifying the normal search algorithm of Moses to our shift-reduce algorithm, without cube pruning (Huang and Chiang, 2005). We retained the features of Moses: four translation features, three lexical reordering features (straight, inverted and discontinuous), linear distortion, phrase penalty, word penalty and language model, without importing any new feature. The decoding configurations used by all the decoders, including beam size, phrase table limit and so on, were the same, so the performance was compared fairly. First, we will show the performance of shiftreduce algorithm on three data sets with large training data sets (Section 4.1). Then, we will analyze the performance elaborately in terms of accuracy, sp"
C10-2033,J99-4005,0,0.793313,"resent a shift-reduce decoding algorithm that can generate ITG-legal translation from left to right in linear time. This algorithm runs in a reduce-eager style and is suited to phrase-based models. Using the state-ofthe-art decoder Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets. 1 Introduction In statistical machine translation, for the diversity of natural languages, the word order of source and target language may differ and searching through all possible translations is NP-hard (Knight, 1999). So some measures have to be taken to reduce search space: either using a search algorithm with pruning technique or restricting possible reorderings. Currently, beam search is widely used (Tillmann and Ney, 2003; Koehn, 2004) to reduce search space. However, the pruning technique adopted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the"
C10-2033,P07-2045,0,0.011696,"ple in Figure 2. The top nine transitions correspond to Figure 3 (a), ... , Figure 3 (i), respectively. the help of ITG structure, it can be extended to syntax-based models easily. Xiong et al. (2006) propose a BTG-based model, which uses the context to determine the orientation of two adjacent spans. It employs the cube-time CYK algorithm. 4 3 Related Work St [0] [0][5] [0][5][1] [0][5][1][3] [0][5][1][3][4] [0][5][1][3, 4] [0][5][1][3, 4][2] [0][5][1][2, 4] [0][5][1, 4] [0][1, 5] [0, 5] [0, 5][6] [0, 6] Experiments We compare the shift-reduce decoder with the state-of-the-art decoder Moses (Koehn et al., 2007). The shift-reduce decoder was implemented by modifying the normal search algorithm of Moses to our shift-reduce algorithm, without cube pruning (Huang and Chiang, 2005). We retained the features of Moses: four translation features, three lexical reordering features (straight, inverted and discontinuous), linear distortion, phrase penalty, word penalty and language model, without importing any new feature. The decoding configurations used by all the decoders, including beam size, phrase table limit and so on, were the same, so the performance was compared fairly. First, we will show the perfor"
C10-2033,koen-2004-pharaoh,0,0.0590687,"der Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets. 1 Introduction In statistical machine translation, for the diversity of natural languages, the word order of source and target language may differ and searching through all possible translations is NP-hard (Knight, 1999). So some measures have to be taken to reduce search space: either using a search algorithm with pruning technique or restricting possible reorderings. Currently, beam search is widely used (Tillmann and Ney, 2003; Koehn, 2004) to reduce search space. However, the pruning technique adopted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly"
C10-2033,W99-0604,0,0.0555584,"by d = |starti − endi−1 − 1|, where starti is the start position of the current phrase and endi−1 is the last position of the last translated phrase. Galley and Manning (2008) present a hierarchical phrase reordering model aimed at improving non-local reorderings. Via the hierarchical mergence of two blocks, the orientation of long distance words can be computed. Their shift-reduce algorithm does not import ITG constraints and admits the translation violating ITG constraints. Zens et al. (2004) introduce a left-toright decoding algorithm with ITG constraints on the alignment template system (Och et al., 1999). Their algorithm processes candidate source phrases one by one through the whole search space and checks if the candidate phrase complies with ITG constraints. Besides, their algorithm checks validity via cover vector and does not formalize ITG structure. The shift-reduce decoding algorithm holds ITG structure via three stacks. As a result, it can offer ITG-legal spans directly and decode faster. Furthermore, with Sl ∅ [1, 4] ∅ [2] [2] [2] ∅ ∅ ∅ ∅ ∅ ∅ ∅ Sr [1, 6] [6] [2, 4][6] [4][6] [6] [6] [6] [6] [6] [6] [6] ∅ ∅ Figure 5: Transition sequence for the example in Figure 2. The top nine transi"
C10-2033,P03-1021,0,0.0707834,"scribed in Collins et al. (Collins et al., 2005). 4.1 Performance Evaluation NIST06 30.24 30.27 30.35 —— 30.47 NIST08 25.08 23.80 26.23** 25.09 26.67** speed 4.827 1.501 4.335 3.856 4.126 NIST05 35.80 35.03 36.56** 35.84 36.42** speed 7.142 1.811 6.276 5.008 5.432 (a) We did three experiments to compare the performance of the shift-reduce decoder, Moses and the decoder with ITG constraints using cover vector (denoted as CV). 2 The shift-reduce decoder decoded with two sets of parameters: one was tuned by itself (denoted as SR) and the other was tuned by Moses (denoted as SR-same), using MERT (Och, 2003). Two searching algorithms of Moses are considered: one is the normal search algorithm without cubing pruning (denoted as Moses), the other is the search algorithm with cube pruning (denoted as Moses-cb). For all the decoders, the distortion limit was set to 6, the nbest size was set to 100 and the phrase table limit was 50. In the first experiment, the development set is part of NIST MT06 data set including 862 sentences, the test set is NIST MT08 data set and the training data set contains 5 million sentence pairs. We used a 5-gram language model which were trained on the Xinhua and AFP port"
C10-2033,J03-1005,0,0.0606595,"the state-ofthe-art decoder Moses as the baseline, experiment results show that the shift-reduce algorithm can significantly improve both the accuracy and the speed on different test sets. 1 Introduction In statistical machine translation, for the diversity of natural languages, the word order of source and target language may differ and searching through all possible translations is NP-hard (Knight, 1999). So some measures have to be taken to reduce search space: either using a search algorithm with pruning technique or restricting possible reorderings. Currently, beam search is widely used (Tillmann and Ney, 2003; Koehn, 2004) to reduce search space. However, the pruning technique adopted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are"
C10-2033,P96-1021,0,0.0604209,"pted by this algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a"
C10-2033,J97-3002,0,0.701493,"is algorithm is not risk-free. As a result, the best partial translation may be ruled out during pruning. The more aggressive the pruning is, the more likely the best translation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a left-to-ri"
C10-2033,P06-1066,1,0.934889,"radeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a left-to-right manner. This algorithm parses source words in the order of their corresponding translations on the target side. In the meantime, it gives all candidate ITG-legal reorderings. The shift-reduce algorithm is differen"
C10-2033,P03-1019,0,0.245335,"ranslation escapes. There should be a tradeoff between the speed and the accuracy. If some heuristic knowledge is employed to guide the search, the search algorithm can discard some implausible hypotheses in advance and focus on more possible ones. Inversion Transduction Grammars (ITGs) permit a minimal extra degree of ordering flexibility and are particularly well suited to modeling ordering shifts between languages (Wu, 1996; Wu, 1997). They can well balance the needed flexibility against complexity constraints. Recently, ITG has been successfully applied to statistical machine translation (Zens and Ney, 2003; Zens et al., 2004; Xiong et al., 2006). However, ITG generally employs the expensive CYK parsing algorithm which runs in cube time. In addition, the CYK algorithm can not calculate language model exactly in the process of decoding, as it can not catch the full history context of the left words in a hypothesis. In this paper, we introduce a shift-reduce decoding algorithm with ITG constraints which runs in a left-to-right manner. This algorithm parses source words in the order of their corresponding translations on the target side. In the meantime, it gives all candidate ITG-legal reorderings"
C10-2033,C04-1030,0,\N,Missing
C12-2003,aker-etal-2012-light,1,0.808146,"repare the parallel phrases used to train and test the SVM classifier. For each language pair we split the corpus into two parts: a training set and a test set. The test set contains 10K parallel sentences. The training set contains 99K sentences for EN-DE, 423K for EN-EL and 53K sentences for EN-LV. 4.1.2 Comparable Corpora We used comparable corpora in English-Greek, English-Latvian and English-German language pairs. These corpora were collected from news articles using a light weight approach that only compares titles and date of publication of two articles to judge them for comparability (Aker et al., 2012). The corpora are aligned at the document level and are detailed in Table 1. language pair EN-DE EN-EL EN-LV document pairs 66K 122K 87K EN sentences 623K 1600K 1122K target sentences 533K 313K 285K EN words 14837K 27300K 18704K target words 6769K 8258K 5356K Table 1: Size of comparable corpora. 4.2 Phrase Extraction for Classifier Training and Testing On both parallel training and testing data sets (see Section 4.1.1) we separately applied GIZA++ to obtain the word alignment information used in our parallel phrase extraction method (see Section 2.1). Then we ran the training example extractio"
C12-2003,aswani-gaizauskas-2010-english,1,0.688491,"rposes Dictionaries mostly fail to return translation entries for named entities (NEs) or specialized terminology. Because of this we also use cognate-based methods to perform the mapping between source and target words or vice versa. We only apply the cognate-based methods for the firstWordTranslationScore and lastWordTranslationScore features. For these two features it is easy to compare the first or the last words from both the source and target phrases. The score of the cognate methods becomes the translation score for the features. We adopt several string similarity measures described in Aswani and Gaizauskas (2010): (1) Longest Common Subsequence Ratio, (2) Longest Common Substring, (3) Dice Similarity, (4) Needleman-Wunsch Distance and (5) Levenshtein Distance. Each of these measures returns a score between 0 and 1. We use a weighted linear combination of the scores to compute the final score. We learn the weights using linear regression over training data consisting of pairs of truely and falsely aligned city names available from Wikipedia1 . For the truely aligned named entities we assign a score of 1 and for the falsely aligned ones a score of 0. We take the cognate similarity score as the translati"
C12-2003,P01-1008,0,0.076278,"es or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target la"
C12-2003,N06-1003,0,0.0261749,"re not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible"
C12-2003,P05-1033,0,0.119284,"e pairs are generated from arbitrary source and target language sentence pairs. 24 2.1 Training Example Extraction We use whatever parallel data is available for a language pair to extract training examples for the SVM classifier. To get positive training examples (parallel phrases), we first align the parallel sentence pairs using the Giza++ toolkit (Och and Ney, 2000, 2003) in both directions and then refine the alignments using a “grow-diag-final-and” strategy. Then, we extract all phrases, as defined in the statistical machine translation literature (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005), and take these phrases as positive examples. j Let S denote a sentence, Si the i-th word in S and Si the subsequence of words in S from j j0 position i to j. Given a word-aligned sentence pair 〈S, T 〉, 〈Si , Ti 0 〉 is a phrase iff: • Sk is aligned to Tk0 for some k ∈ [i, j] and k0 ∈ [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ [i, j] and k0 ∈ / [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ / [i, j] and k0 ∈ [i 0 , j 0 ] To get negative training examples (non-parallel phrases), for each sentence pair, we enumerate all segments on the source side and on the target side, the length"
C12-2003,W11-1209,0,0.0789101,"is an under-resourced language, while for Greek and German text resources are more readily available. Considering all three languages allows us to directly compare our method’s performance on resource-rich and under-resourced languages. We perform two different tests. First, we evaluate the performance of the classifier on phrases extracted from held-out parallel data using standard measures such as recall, precision and accuracy. Secondly, we test whether the phrases extracted by our method from comparable corpora lead to improved SMT quality, as measured using BLEU (Papineni et al., 2002) . Hewavitharana and Vogel (2011) also adopt a classification approach for phrase extraction. However, their approach requires manual intervention in data preparation, whereas we perform the preparation of training and testing data fully automatically. In addition, Hewavitharana and Vogel (2011) do not report any SMT performance evaluation of their approach, so it is difficult to estimate how useful their approach is for the actual task it is meant to improve. We test the impact of our extracted phrases on the performance of an SMT system, which allows us to draw conclusions about the likely utility of our approach for SMT in"
C12-2003,ion-2012-pexacc,0,0.0248044,"better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible phrases of a given length for S and for T and then compute all possible phrase pairings co"
C12-2003,N06-1058,0,0.0195444,"ficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we"
C12-2003,W04-3250,0,0.194912,"Missing"
C12-2003,N03-1017,0,0.0331396,"nguages. During testing candidate phrase pairs are generated from arbitrary source and target language sentence pairs. 24 2.1 Training Example Extraction We use whatever parallel data is available for a language pair to extract training examples for the SVM classifier. To get positive training examples (parallel phrases), we first align the parallel sentence pairs using the Giza++ toolkit (Och and Ney, 2000, 2003) in both directions and then refine the alignments using a “grow-diag-final-and” strategy. Then, we extract all phrases, as defined in the statistical machine translation literature (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005), and take these phrases as positive examples. j Let S denote a sentence, Si the i-th word in S and Si the subsequence of words in S from j j0 position i to j. Given a word-aligned sentence pair 〈S, T 〉, 〈Si , Ti 0 〉 is a phrase iff: • Sk is aligned to Tk0 for some k ∈ [i, j] and k0 ∈ [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ [i, j] and k0 ∈ / [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ / [i, j] and k0 ∈ [i 0 , j 0 ] To get negative training examples (non-parallel phrases), for each sentence pair, we enumerate all segments on the source side"
C12-2003,2007.tmi-papers.12,0,0.0404297,"cases, such as for under-resourced languages or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach."
C12-2003,W02-1018,0,0.0390414,"nder-resourced languages or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase"
C12-2003,D09-1040,0,0.0161754,"ystems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible phrases of a given length for S and for T and then co"
C12-2003,J05-4003,0,0.0603772,"Missing"
C12-2003,P06-1011,0,0.106121,"h parallel resources (corpora). However, in many cases, such as for under-resourced languages or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is"
C12-2003,J04-4002,0,0.0446812,"ing candidate phrase pairs are generated from arbitrary source and target language sentence pairs. 24 2.1 Training Example Extraction We use whatever parallel data is available for a language pair to extract training examples for the SVM classifier. To get positive training examples (parallel phrases), we first align the parallel sentence pairs using the Giza++ toolkit (Och and Ney, 2000, 2003) in both directions and then refine the alignments using a “grow-diag-final-and” strategy. Then, we extract all phrases, as defined in the statistical machine translation literature (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005), and take these phrases as positive examples. j Let S denote a sentence, Si the i-th word in S and Si the subsequence of words in S from j j0 position i to j. Given a word-aligned sentence pair 〈S, T 〉, 〈Si , Ti 0 〉 is a phrase iff: • Sk is aligned to Tk0 for some k ∈ [i, j] and k0 ∈ [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ [i, j] and k0 ∈ / [i 0 , j 0 ] • Sk is not aligned to Tk0 for all k ∈ / [i, j] and k0 ∈ [i 0 , j 0 ] To get negative training examples (non-parallel phrases), for each sentence pair, we enumerate all segments on the source side and on the target s"
C12-2003,C00-2163,0,0.75263,"etween parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible phrases of a given length for S and for T and then compute all possible phrase pairings consisting of one phrase from S and one phrase from T. In the test phase we use a binary SVM classifier to determine for each generated phrase pair whether it is or is not parallel. The SVM classifier is trained using phrase pairs taken from parallel data word aligned using Giza++ (Och and Ney, 2000, 2003). We have tested our approach on the English-German, English-Greek and English-Latvian language pairs. Latvian is an under-resourced language, while for Greek and German text resources are more readily available. Considering all three languages allows us to directly compare our method’s performance on resource-rich and under-resourced languages. We perform two different tests. First, we evaluate the performance of the classifier on phrases extracted from held-out parallel data using standard measures such as recall, precision and accuracy. Secondly, we test whether the phrases extracted"
C12-2003,J03-1002,0,0.0125168,"Missing"
C12-2003,P02-1040,0,0.0910989,"language pairs. Latvian is an under-resourced language, while for Greek and German text resources are more readily available. Considering all three languages allows us to directly compare our method’s performance on resource-rich and under-resourced languages. We perform two different tests. First, we evaluate the performance of the classifier on phrases extracted from held-out parallel data using standard measures such as recall, precision and accuracy. Secondly, we test whether the phrases extracted by our method from comparable corpora lead to improved SMT quality, as measured using BLEU (Papineni et al., 2002) . Hewavitharana and Vogel (2011) also adopt a classification approach for phrase extraction. However, their approach requires manual intervention in data preparation, whereas we perform the preparation of training and testing data fully automatically. In addition, Hewavitharana and Vogel (2011) do not report any SMT performance evaluation of their approach, so it is difficult to estimate how useful their approach is for the actual task it is meant to improve. We test the impact of our extracted phrases on the performance of an SMT system, which allows us to draw conclusions about the likely u"
C12-2003,P06-2095,0,0.0256329,"ora). However, in many cases, such as for under-resourced languages or in narrow domains, sufficient parallel resources are not readily available. This leads to machine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generat"
C12-2003,skadina-etal-2012-collecting,1,0.887104,"Missing"
C12-2003,P08-1116,0,0.0191513,"chine translation systems under-performing relative to those for better resourced languages and domains. To overcome the scarcity of parallel resources the machine translation community has recognized the potential of using comparable corpora as training data. As a result different methods for extracting parallel sentences or smaller text units such as phrases from comparable corpora have been investigated (Munteanu and Marcu, 2006; Sharoff et al., 2006; Kumano et al., 2007; Marcu and Wong, 2002; Barzilay and McKeown, 2001; Kauchak and Barzilay, 2006; Callison-Burch et al., 2006; Nakov, 2008; Zhao et al., 2008; Marton et al., 2009; Skadin ¸ a et al., 2012; Ion, 2012). A common idea in this related work is the use of some heuristics to pair target and source phrases. By contrast we approach the task of parallel phrase extraction as a classification task and use feature extraction on the training data to train an SVM classifier to distinguish between parallel and non-parallel phrases. Our method is fully automatic and is essentially a “generate and test” approach. In the generate phase, given source and target language sentences S and T, we first generate all possible phrases of a given length for S"
C18-1110,P17-2021,0,0.0118123,"h1 h2 → − h1 ← − h1 → − h2 ← − h2 ··· x1 x2 ··· ··· h ls → − h ls ← − h ls x ls Figure 1: The architecture of attention-based NMT NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neural Networks (CNNs) to collect local information around one word and relates each word with its neighbors, which ensures the subsequent operations are performed in the unit of multiple words. As for the second point, Relation Network (RN) (Santoro et al., 2017) is introduced to establish pairwise relationship between words, meanwhile, there’s no need to"
C18-1110,D17-1209,0,0.130141,"i=1 αij hi αj2 αjls ··· h1 h2 → − h1 ← − h1 → − h2 ← − h2 ··· x1 x2 ··· ··· h ls → − h ls ← − h ls x ls Figure 1: The architecture of attention-based NMT NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neural Networks (CNNs) to collect local information around one word and relates each word with its neighbors, which ensures the subsequent operations are performed in the unit of multiple words. As for the second point, Relation Network (RN) (Santoro et al., 2017) is introduced to establish pairwise relationship between words, me"
C18-1110,D14-1179,0,0.023952,"Missing"
C18-1110,P05-1066,0,0.167644,"Missing"
C18-1110,P17-1012,0,0.0173668,"between source and target words, with the main architecture remaining the RNN encoder-decoder framework. Sennrich et al. (2016) enriched source representations with POS tags, dependency labels and other linguistic features. Bastings et al. (2017) employed graph convolutional networks to model relations of words in dependency trees for the source embeddings to include these relations. These two models both require extra supervised syntax input while our method does not need external knowledge and learn the relationship by its own. Another line is to change the structure of the neural network. Gehring et al. (2017a) and Gehring et al. (2017b) proposed to substitute the conventional RNN encoder with the CNN encoder in order to train faster. They employed stacked CNNs to capture relationships between source words which can be calculated simultaneously, not like RNNs, the computation of which is constrained by temporal dependencies. The attention scores are also computed based on the output of the CNNs and the decoder is still the RNN-based decoder. Vaswani et al. (2017) is another work to eschew the recurrence. It instead 1296 relied entirely on the attention mechanism to draw the global dependencies bet"
C18-1110,D13-1176,0,0.0841323,"ization capability of recurrent neural network via associating source words with each other, this would also help retain their relationships. Then the source representations and all the relations are fed into the attention component together while decoding, with the main encoderdecoder framework unchanged. Experiments on several datasets show that our method can improve the translation performance significantly over the conventional encoder-decoder model and even outperform the approach involving supervised syntactic knowledge. 1 Introduction In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has achieved great success in some language pairs, rivalling the state-ofthe-art Statistical Machine Translation (SMT). The Recurrent Neural Network (RNN) encoder-decoder architecture is widely used framework for NMT, the principle behind which is that: encoding the meaning of the input bidirectionally into a concept space via RNNs and decoding into target words with RNNs based on this encoding (Sutskever et al., 2014; Bahdanau et al., 2015). This means that encoding principle leads to a deeper understanding and learning of the translation rules"
C18-1110,N03-1017,0,0.0290452,"h makes the decoding process only focus on the most related source words, the RNN encoder-decoder framework is expected to be able to handle long sequences and consider the globally related information. However, the practical situation is that RNNs tend to forget old history information, especially the far older one. Sometimes the older information is indispensable for generating proper translation, e.g., for the source sentence “take the heavy box away”, when translating “away”, “take” should be considered together. In addition, it has been proven that using phrases rather than words in SMT (Koehn et al., 2003) brings performance improvement, while in NMT the attention is only modeled in the unit of words. In the same sense, improvement is expected if attention is operated on more words rather than one. Moreover, NMT produces the representation for the source by running through the source words sequentially with a bidirectional RNN (Schuster and Paliwal, 1997), so it only employs word order information and ignores the relation between words. Although some researchers have demonstrated that ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International License. c"
C18-1110,P17-1064,0,0.0124943,"← − h2 ··· x1 x2 ··· ··· h ls → − h ls ← − h ls x ls Figure 1: The architecture of attention-based NMT NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neural Networks (CNNs) to collect local information around one word and relates each word with its neighbors, which ensures the subsequent operations are performed in the unit of multiple words. As for the second point, Relation Network (RN) (Santoro et al., 2017) is introduced to establish pairwise relationship between words, meanwhile, there’s no need to attain external in"
C18-1110,Q16-1037,0,0.0225492,"e Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1292 Proceedings of the 27th International Conference on Computational Linguistics, pages 1292–1303 Santa Fe, New Mexico, USA, August 20-26, 2018. yj−1 Decoder ··· Attention Layer αj1 Encoder Wv y j sj−1 ! ls aj = i=1 αij hi αj2 αjls ··· h1 h2 → − h1 ← − h1 → − h2 ← − h2 ··· x1 x2 ··· ··· h ls → − h ls ← − h ls x ls Figure 1: The architecture of attention-based NMT NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neur"
C18-1110,P02-1040,0,0.100729,"ning data. WMT16 We conducted experiments on WMT16 dataset, the same dataset as the work of Bastings et al. (2017) for comparison. We kept the same settings as those in Bastings et al. (2017): The original dataset consists of 4500966 sentence pairs, with 4173550 left after filtering pairs which contains more than 50 tokens on either side after tokenization. newstest2015 and newstest2016 were used as the validation set and test dataset, respectively. 16k BPE merging operations were conducted on the target side of the bilingual training data. For WMT16 dataset, case-sensitive 4-gram BLEU score (Papineni et al., 2002) was reported by using the multi-bleu.pl script. The results on the other two datasets were evaluated with case-insensitive 4-gram BLEU score. 5.2 Systems Results of five systems on different datasets were reported: RNNsearch We implemented the attention-based NMT of Bahdanau et al. (2015) by PyTorch framework3 with the following settings: the length of the sentences on both sides was limited up to 50 tokens with 30K vocabulary, and the source and target word embedding sizes were both set to 512, the size of all hidden units in both encoder and decoder RNNs was also set to 512, and all paramet"
C18-1110,W16-2209,0,0.0358996,"Missing"
C18-1110,P16-1162,0,0.68146,"Wv y j sj−1 ! ls aj = i=1 αij hi αj2 αjls ··· h1 h2 → − h1 ← − h1 → − h2 ← − h2 ··· x1 x2 ··· ··· h ls → − h ls ← − h ls x ls Figure 1: The architecture of attention-based NMT NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neural Networks (CNNs) to collect local information around one word and relates each word with its neighbors, which ensures the subsequent operations are performed in the unit of multiple words. As for the second point, Relation Network (RN) (Santoro et al., 2017) is introduced to establish pairwise relatio"
C18-1110,D16-1159,0,0.0184928,"4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 1292 Proceedings of the 27th International Conference on Computational Linguistics, pages 1292–1303 Santa Fe, New Mexico, USA, August 20-26, 2018. yj−1 Decoder ··· Attention Layer αj1 Encoder Wv y j sj−1 ! ls aj = i=1 αij hi αj2 αjls ··· h1 h2 → − h1 ← − h1 → − h2 ← − h2 ··· x1 x2 ··· ··· h ls → − h ls ← − h ls x ls Figure 1: The architecture of attention-based NMT NMT is able to capture certain syntactic phenomena (e.g. subject-verb agreement) without external syntactic information (Linzen et al., 2016; Shi et al., 2016), there are some other works which has shown their superior performance by modeling word relationship explicitly. However, these works usually need to introduce external syntactic knowledge or connect words according to their relations in the syntactic structure (Sennrich et al., 2016; Bastings et al., 2017; Aharoni and Goldberg, 2017; Li et al., 2017). In this paper, we present a method to refine the NMT based on the above two points. The main idea is to learn relationship between the source word pairs. Corresponding to the first point, our method employs Convolutional Neural Networks (CNNs)"
C18-1110,P03-1021,0,0.187267,"only memorize history information but capture the relationship between words, both of which are beneficial to translate long sentences. 5.5 Word Alignment Systems RNNsearch? R NMT BLEU 22.40 24.12 AER 46.76 45.66 Table 4: Comparison of alignment quality on NIST Zh-En translation task. In this section, we will verify the translation performance of our model from another perspective. Intuitively, the better translation should have better alignment to the source sentence, so we evaluated the quality of the alignments derived from the attention module of the NMT using Alignment Error Rate (AER) (Och, 2003). We did this experiment on the artificially aligned dataset from Liu and Sun (2015) which contains 900 Zh-En sentence pairs. The alignments were got in this way for both RNNsearch? 1299 (a) RNNsearch? (b) R NMT Figure 4: Word alignment comparison. The green boxes show the manual golden alignments. system and our system. When one target word was generated, we retained the alignment link with the highest probability αij in Equation 3. The comparison results are shown in Table 4. It illustrates that our system R NMT can produce better translations than the baseline RNNsearch? , a difference of 1"
D09-1115,P08-1115,0,0.0751179,"eels like apples He prefer lations. Note that the phrase “is fond of” is attached to an edge. Now, it is unlikely to obtain a translation like “He is like of apples”. A lattice G = hV, Ei is a directed acyclic graph, formally a weighted finite state automation (FSA), where V is the set of nodes and E is the set of edges. The nodes in a lattice are usually labeled according to an appropriate numbering to reflect how to produce a translation. Each edge in a lattice is attached with a sequence of words as well as the associated probability. As lattice is a more general form of confusion network (Dyer et al., 2008), we expect that replacing confusion networks with lattices will further improve system combination. He feels like apples apples He prefer apples He feels like apples He feels like apples He is fond of apples He is fond of apples (a) unidirectional alignments (b) bidirectional alignments He ε feels prefer like of ε apples is fond (c) confusion network he ε feels prefer like apples is fond of (d) lattice 2.2 Figure 1: Comparison of a confusion network and a lattice. 2 Background 2.1 Confusion Network and Lattice We use an example shown in Figure 1 to illustrate our idea. Suppose that there are"
D09-1115,A94-1016,0,0.202214,"in which several words are connected to another several words. Instead, we propose a lattice-based system combination model that allows for such phrase alignments and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way t"
D09-1115,D08-1011,0,0.510095,"s significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected to another several words. For example, “be capable of” and “"
D09-1115,W07-0711,0,0.0521771,"s were collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, ρ = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set whi"
D09-1115,W05-1506,0,0.0415363,"hich store more than one non-null word. The above two numbers are gotten by counting both backbone arcs and hypothesis arcs. α and β are the corresponding weights of the numbers, respectively. Nword (e) is the non-null word number and γ is its weight. Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps (arc). ps (arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination. In our experiments, the weights we"
D09-1115,N03-1017,0,0.027572,"he hypothesis with the minimum cost of edits against all hypotheses is selected. The backbone is significant for it influences not only the word order, but also the following alignments. The backbone is selected as follows: EB = argmin E ′ ∈E X T ER(E ′ , E) (8) E∈E (3) Get the alignments of the backbone and hypothesis pairs. First, each pair is aligned in both directions using the IHMM-based alignment method. In the IHMM alignment model, bilingual dictionaries in both directions are indispensable. Then, we apply a grow-diag-final algorithm which is widely used in bilingual phrase extraction (Koehn et al., 2003) to monolingual alignments. The bidirectional alignments are combined to one resorting to the grow-diag-final algorithm, allowing n-to-n mappings. (4)Normalize the alignment pairs. The word order of the backbone determines the word order of consensus outputs, so the word order of hypotheses must be consistent with that of the backbone. All words of a hypotheses are reordered according to the alignment to the backbone. For a word aligned to null, an actual null word may be inserted to the proper position. The alignment units are extracted first and then the hypothesis words in each unit are shi"
D09-1115,P07-2045,0,0.00971279,"1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008). Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005). The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations. 6 Experiments The candidate systems participating in the system combination are as listed in Table 1: System A is a BTG-based system using a MaxEnt-based reordering model; System B is a hierarchical phrase-based system; System C is the Moses decoder (Koehn et al., 2007); System D is a syntax-based system. 10best hypotheses from each candidate system on the dev and test sets were collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s meth"
D09-1115,E06-1005,0,0.230719,"nts and uses lattices to encode all candidate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more co"
D09-1115,P07-1040,0,0.376587,"idate translations. Experiments show that our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected"
D09-1115,N07-1029,0,0.250842,"ere collected as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, ρ = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set whi"
D09-1115,W08-0329,0,0.156101,"our approach achieves significant improvements over the state-ofthe-art baseline system on Chinese-to-English translation test sets. 1 Introduction System combination aims to find consensus translations among different machine translation systems. It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set to maximizing the overall score. However, a confusion network is restricted in such a way that only 1-to-1 mappings are allowed in hypothesis alignment. This is not the fact even for word alignments between the same languages. It is more common that several words are connected to another several words. For example, “be"
D09-1115,C96-2141,0,0.491588,"ed as the input of the system combination. In our experiments, the weights were all tuned on the NIST MT02 Chinese-to-English test set, including 878 sentences, and the test data was the NIST MT05 Chinese-to-English test set, including 1082 sentences, except the experiments in Table 2. A 5gram language model was used which was trained on the XinHua portion of Gigaword corpus. The results were all reported in case sensitive BLEU score and the weights were tuned in Powell’s method to maximum BLEU score. The IHMM-based alignment module was implemented according to He et al. (2008), He (2007) and Vogel et al. (1996). In all experiments, the parameters for IHMM-based alignment module were set to: the smoothing factor for the surface similarity model, ρ = 3; the controlling factor for the distortion model, K = 2. 6.1 Comparison with Confusion-network-based model In order to compare the lattice-based system with the confusion-network-based system fairly, we used IHMM-based system combination model on behalf of the confusion-network-based model described in He et al. (2008). In both lattice-based and IHMMbased systems, the bilingual dictionaries were extracted on the FBIS data set which included 289K sentenc"
D12-1109,P05-1022,0,0.0171017,"the vote) = f (r4 ) · 1 · lm(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost β of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as   v is completed 1 f (v) = lm(v) v is a terminal string  ∏"
D12-1109,P05-1033,0,0.199485,"ompete actions until the next action is grow. The predict and grow actions decide which rules can be used to expand hypotheses next, so we update the applicable rule set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses 1197 at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based mod"
D12-1109,J07-2003,0,0.371901,"ure consistent syntactic transformations between the source and target languages, e.g., from subject-verb-object to subject-object-verb word orderings. Decoding algorithms for grammar-based translation seek to find the best string in the intersection between a weighted context free grammar (the translation mode, given a source string/tree) and a weighted finite state acceptor (an n-gram language model). This intersection is problematic, as it results in an intractably large grammar, and makes exact search impossible. Most researchers have resorted to approximate search, typically beam search (Chiang, 2007). The decoder parses the source sentence, recording the target translations for each span.1 As the partial translation hypothesis grows, its component ngrams are scored and the hypothesis score is updated. This decoding method though is inefficient as it requires recording the language model context (n − 1 words) on the left and right edges of each chart cell. These contexts allow for boundary ngrams to be evaluated when the cell is used in another grammar production. In contrast, if the target string is generated in left-to-right order, then only one language model context is required, and th"
D12-1109,P05-1066,0,0.0607385,"5.2 Performance Comparison Our bottom-up left-to-right decoder employs the same features as the traditional decoder: rule probability, lexical probability, language model probability, rule count and word count. In order to compare them fairly, we used the same beam size which is 20 and employed cube pruning technique (Huang and Chiang, 2005). We show the results in Table 3. From the results, we can see that the bottom-up decoder outperforms top-down decoder and traditional decoder by 1.1 and 0.8 BLEU points respectively and the improvements are statistically significant using the sign-test of Collins et al. (2005) (p < 0.01). The improvement may result from dynamically searching for a whole derivation which leads to more accurate estimation of a partial derivation. The additional time consumption of the bottom-up decoder against the top-down decoder comes from dynamic future cost computation. Next we compare decoding speed versus translation quality using various beam sizes. The results are shown in Figure 5. We can see that our bottomup decoder can produce better BLEU score at the same decoding speed. At small beams (decoding time around 0.5 second), the improvement of translation quality is much bigg"
D12-1109,N10-1128,0,0.0532922,"e set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses 1197 at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tre"
D12-1109,P81-1022,0,0.741421,"nt Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1191–1200, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics nodes, it is necessary to consider the cost of uncovered nodes, i.e., the future cost. We show that a good future cost estimate is essential for accurate and efficient search, leading to high quality translation output. Other researchers have also considered the leftto-right decoding algorithm for tree-to-string models. Huang and Mi (2010) developed an Earleystyle parsing algorithm (Earley, 1970). In their approach, hypotheses covering the same number of tree nodes were binned together. Their method uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earle"
D12-1109,C10-2033,1,0.88656,"ovel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tree. Galley and Manning (2008) use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering. This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly. 5 Experiments In the experiments, we use two baseline systems: our in-house tree-to-str"
D12-1109,D08-1089,0,0.0379223,"scribe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CKY decoding. Feng et al. (2010) proposed a shift-reduce algorithm to add BTG constraints to phrase-based models. This algorithm constructs a BTG tree in a reduce-eager manner while the algorithm in this paper searches for a best derivation which must be derived from the source tree. Galley and Manning (2008) use the shift-reduce algorithm to conduct hierarchical phrase reordering so as to capture long-distance reordering. This algorithm shows good performance on phrase-based models, but can not be applied to syntax-based models directly. 5 Experiments In the experiments, we use two baseline systems: our in-house tree-to-string decoder implemented according to Liu et al. (2006) (denoted as traditional) and the Earley-style top-down decoder implemented according to Huang and Mi (2010) (denoted as topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the"
D12-1109,N04-1035,0,0.0554355,"t decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) and the Earley-style topdown decoding algorithm (Huang and Mi, 2010) for the tree-to-string model. 2.1 Traditional Decoding The traditional decoding algorithm processes source tree nodes one by one according to a post-order traversal. For each node, it applies matched STSG rules by substituting each non-terminal with its cor1192 traditional top-down bottom-up in theory O(nc˙|V |4(g−1) ) O(c(cr)d |V |g−1 ) O((cr)d |V |g−1 ) beam search O(ncb2 ) O(ncb) O(nub) Table 1: Time complexity of different algorithms. tradition"
D12-1109,W05-1506,0,0.043939,"Missing"
D12-1109,D10-1027,0,0.664834,"rding to a postorder traversal. 1191 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1191–1200, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics nodes, it is necessary to consider the cost of uncovered nodes, i.e., the future cost. We show that a good future cost estimate is essential for accurate and efficient search, leading to high quality translation output. Other researchers have also considered the leftto-right decoding algorithm for tree-to-string models. Huang and Mi (2010) developed an Earleystyle parsing algorithm (Earley, 1970). In their approach, hypotheses covering the same number of tree nodes were binned together. Their method uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our a"
D12-1109,2006.amta-papers.8,0,0.277257,"algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earley-style left-to-right decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) and the Earley-style topdown decoding algorithm (Huang and Mi, 2010) for the tree-to-string model. 2.1 Traditional Decoding The traditional decoding algorithm processes source tree nodes one by one according to a post-order trav"
D12-1109,P08-1067,0,0.0222632,"(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost β of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as   v is completed 1 f (v) = lm(v) v is a terminal string  ∏  maxr∈Rv f (r)"
D12-1109,J99-4005,0,0.212094,"e: 4 We bundle the successive terminals in one rule into a symbol 1195 grow [ IP] [NN2 ] −→ [ IP] [NN2  of the vote] From the above definition, we can find that there may be an ambiguity about whether to use a complete action or a grow action. Similarly, predict actions must select a viable prefix form the set for a node. For example in step 5, although we select to perform complete with r4 in the example, r7 is applicable, too. In our implementation, if both r4 and r7 are applicable, we apply them both to generate two seperate hypotheses. To limit the exponential explosion of hypotheses (Knight, 1999), we use beam search over bins of similar partial hypotheses (Koehn, 2004). IP r6 r4 , then r7 f (N P ) = f (r4 ) · f (N N2 ) · lm(of the vote) = f (r4 ) · 1 · lm(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a d"
D12-1109,koen-2004-pharaoh,0,0.674331,"[ IP] [NN2 ] −→ [ IP] [NN2  of the vote] From the above definition, we can find that there may be an ambiguity about whether to use a complete action or a grow action. Similarly, predict actions must select a viable prefix form the set for a node. For example in step 5, although we select to perform complete with r4 in the example, r7 is applicable, too. In our implementation, if both r4 and r7 are applicable, we apply them both to generate two seperate hypotheses. To limit the exponential explosion of hypotheses (Knight, 1999), we use beam search over bins of similar partial hypotheses (Koehn, 2004). IP r6 r4 , then r7 f (N P ) = f (r4 ) · f (N N2 ) · lm(of the vote) = f (r4 ) · 1 · lm(of the vote) NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the futu"
D12-1109,P06-1077,1,0.969992,"od uses a top-down depth-first search, with a mechanism for early elimation of some rules which lead to deadends in the search. Huang and Mi (2010)’s method was shown to outperform the traditional post-ordertraversal decoding algorithm, considering fewer hypotheses and thus decoding much faster at the same level of performance. However their algorithm used a very rough estimate of future cost, resulting in more search errors than our approach. Our experiments show that compared with the Earley-style left-to-right decoding (Huang and Mi, 2010) and the traditional post-order-traversal decoding (Liu et al., 2006) algorithms, our algorithm achieves a significant improvement on search capacity and better translation performance at the same level of speed. 2 Background A typical tree-to-string system (Liu et al., 2006; Huang et al., 2006) searches through a 1-best source parse tree for the best derivation. It transduces the source tree into a target-language string using a Synchronous Tree Substitution Grammar (STSG). The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We will briefly review the traditional decoding algorithm (Liu et al., 2006) a"
D12-1109,P08-1023,1,0.858066,"NP r3 VP r5 r4 of the vote NN2 of NN1 was released at night Figure 4: The translation forest composed of applicable CFG rules for the partial derivation of step 3 in Figure 3. 3.4 Future Cost Partial derivations covering different tree nodes may be grouped in the same bin for beam pruning5 . In order to perform more accurate pruning, we take into consideration future cost, the cost of the uncovered part. The merit of a derivation is the covered cost (the cost of the covered part) plus the future cost. We borrow ideas from the Inside-Outside algorithm (Charniak and Johnson, 2005; Huang, 2008; Mi et al., 2008) to compute the merit. In our algorithm, the merit of a derivation is just the Viterbi inside cost β of the root node calculated with the derivations continuing from the current derivation. Given a partial derivation, we calculate its future cost by searching through the translation forest defined by all applicable CFG rules. Figure 4 shows the translation forest for the derivation of step 3. We calculate the future cost for each node as follows: given a node v, we define its cost function f (v) as   v is completed 1 f (v) = lm(v) v is a terminal string  ∏  maxr∈Rv f (r) π∈rhs(r) f (π) ot"
D12-1109,J03-1002,0,0.00491025,"topdown), respectively. We compare our bottom-up left-to-right decoder (denoted as bottom-up) with the baseline in terms of performance, translation quality and decoding speed with different beam sizes, and search capacity. Lastly, we show the influence of future cost. All systems are implemented in C++. System Traditional Top-down Bottom-up 5.1 Data Setup Time (s) 0.84 0.41 0.81 Table 3: Performance comparison. 30.8 30.6 30.4 BLEU Score We used the FBIS corpus consisting of about 250K Chinese-English sentence pairs as the training set. We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-tostring rules according to the GHKM algorithm (Galley et al., 2004). We used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus. We used the 2002 NIST MT Chinese-English test set (571 sentences) as the development set and the 2005 NIST MT Chinese-English test set (1082 sentences) as the test set. We evaluated translation quality using BLEU-metric (Papineni et al., 2002) with case-insensitive n-gram matching up to n = 4. We used the standard minimum error rate training (Och, 2003) to tune feature weights to maximi"
D12-1109,P03-1021,0,0.0478665,"Missing"
D12-1109,P02-1040,0,0.0894745,"Missing"
D12-1109,P06-1098,0,0.566743,"as “closure” actions. That is to say, once there are some complete actions after a scan action, we finish all the compete actions until the next action is grow. The predict and grow actions decide which rules can be used to expand hypotheses next, so we update the applicable rule set during these two actions. Given a source sentence with n words, we maintain n beams, and let each beam hold b hypotheses 1197 at most. Besides, we prune viable prefixes of each node up to u, so each hypothesis can expand to u new hypotheses at most, so the time complexity of beam search is O(nub). 4 Related Work Watanabe et al. (2006) present a novel Earleystyle top-down decoding algorithm for hierarchical phrase-based model (Chiang, 2005). Their framework extracts Greibach Normal Form rules only, which always has at least one terminal on the left of each rule, and discards other rules. Dyer and Resnik (2010) describe a translation model that combines the merits of syntax-based models and phrase-based models. Their decoder works in two passes: for first pass, the decoder collects a context-free forest and performs tree-based source reordering without a LM. For the second pass, the decoder adds a LM and performs bottomup CK"
D17-1146,D16-1162,0,0.333745,"re often the most important parts of a sentence, e.g., domain-specific entity names. Table 1 shows an example, where the word ‘染 色 体( chromosomes)’ is an infrequent word. As the system does not know (or has effectively ‘forgotten’) this keyword, it does not translate correctly, and an irrelevant translation is produced, leading to the phenomenon of ‘meaning drift’. This weakness with regard to infrequent words/pairs with NMT has been noticed by a number of researchers, and some studies have been conducted to address this problem, e.g., Luong et al. (2014); Cho et al. (2014); Li et al. (2016); Arthur et al. (2016); Bentivogli et al. (2016); Zhang et al. (2017). 1390 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1390–1399 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Superficially, this problem appears to be caused by the imperfect embeddings of infrequent words or the limited vocabulary size of NMT systems, but we argue that the deeper reason should be attributed to the nature of neural models: the translation function, represented by various neural networks, is shared amongst all of the translation pairs, so"
D17-1146,D16-1025,0,0.0238653,"rtant parts of a sentence, e.g., domain-specific entity names. Table 1 shows an example, where the word ‘染 色 体( chromosomes)’ is an infrequent word. As the system does not know (or has effectively ‘forgotten’) this keyword, it does not translate correctly, and an irrelevant translation is produced, leading to the phenomenon of ‘meaning drift’. This weakness with regard to infrequent words/pairs with NMT has been noticed by a number of researchers, and some studies have been conducted to address this problem, e.g., Luong et al. (2014); Cho et al. (2014); Li et al. (2016); Arthur et al. (2016); Bentivogli et al. (2016); Zhang et al. (2017). 1390 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1390–1399 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Superficially, this problem appears to be caused by the imperfect embeddings of infrequent words or the limited vocabulary size of NMT systems, but we argue that the deeper reason should be attributed to the nature of neural models: the translation function, represented by various neural networks, is shared amongst all of the translation pairs, so high-frequency and low-fre"
D17-1146,W14-4012,0,0.0272862,"Missing"
D17-1146,W11-2123,0,0.00885277,"of wt were retained. 5.2 Systems We used a conventional SMT system and an attention-based RNN NMT system as the baselines, and investigated a variety of M-NMT architectures. SMT baseline: For the SMT system (denoted by Moses), Moses (Koehn et al., 2007), a state-ofthe-art open-source toolkit, was used. The default configuration was used where the phrase length was 7 and the following features were employed: relative translation frequencies and lexical translation probabilities on both directions, distortion distance, language model and word penalty. For the language model, the KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model (with the Keneser-Ney smoothing) on the target side of the training data. NMT baseline: For NMT, we reproduced the attention-based RNN model proposed by Bahdanau et al. (2015), which is denoted by NMT. The implementation was based on Tensorflow1 . We compared our implementations with a public implementation using Theano2 , and achieved comparable (even slightly better) performances on the same data sets with the same parameter settings. M-NMT system: The M-NMT system was implemented by combining the memory structure and the NMT system. The model part"
D17-1146,P15-1001,0,0.0294917,"m a word-based alignment model. The focus of their work was to use the extra knowledge to produce a better attention. In contrast, our work promotes the target words directly using the word mapping stored in the memory. Arthur et al. (2016) proposed to involve lexical knowledge to assist with translation, particularly for low-frequency words. This is similar to our proposed idea, with the key difference that their work uses the attention information to 1394 select the target words, while ours trains a separate attention, based on both the source and target words. Regarding handling OOV words, Jean et al. (2015) presented an efficient training method to support a larger vocabulary, which helps alleviate the OOV problem significantly. Stahlberg et al. (2016) used SMT to produce candidate results in the form of lattice and NMT to re-score the results. As SMT uses a larger vocabulary than NMT, some OOV words can be retained. Sennrich et al. (2016) proposed a subword approach, where OOV words are expected to be spelled out by subword units. Luong et al. (2014) proposed a post-processing approach that learns the position of the source word when an UNK symbol is produced during decoding. By this position i"
D17-1146,P07-2045,0,0.0119518,"oth directions, and kept the word pairs that appeared in the phrase tables of both directions. The global memory size is 80K for the IWSLT task, and 500K for the NIST task. These word pairs were then filtered according to the conditional probability p(wt |ws ) where ws and wt are source and target language words, respectively. For each ws , at most two candidates of wt were retained. 5.2 Systems We used a conventional SMT system and an attention-based RNN NMT system as the baselines, and investigated a variety of M-NMT architectures. SMT baseline: For the SMT system (denoted by Moses), Moses (Koehn et al., 2007), a state-ofthe-art open-source toolkit, was used. The default configuration was used where the phrase length was 7 and the following features were employed: relative translation frequencies and lexical translation probabilities on both directions, distortion distance, language model and word penalty. For the language model, the KenLM toolkit (Heafield, 2011) was used to build a 5-gram language model (with the Keneser-Ney smoothing) on the target side of the training data. NMT baseline: For NMT, we reproduced the attention-based RNN model proposed by Bahdanau et al. (2015), which is denoted by"
D17-1146,N03-1017,0,0.175533,"of training data is available (Wu et al., 2016; Johnson et al., 2016; Mi et al., 2016). Although there are different model architectures (Sutskever et al., 2014; Bahdanau et al., 2015), the common principle behind the NMT approach is the same: encoding the meaning of the input into a concept space and performing translation based on this encoding. This ‘meaning encoding’ principle leads to a deeper understanding and learning of the translation rules, and hence a better translation than conventional statistic machine translation (SMT) that considers only surface forms, i.e., words and phrases (Koehn et al., 2003). Despite positive results obtained so far, a particular problem of the NMT approach is that it has a tendency towards overfitting to frequent observations (words, word co-occurrences, translation pairs, etc.), but overlooking special cases that are not frequently observed. For example, NMT is good at learning translation pairs that are frequently observed, and can make use of them well at run-time, but for low-frequency pairs in the training data, the system may ‘forget’ to use them when they should be. Unfortunately, rare words are inevitable for all translation tasks due to Zipf’s law, and"
D17-1146,P17-1125,1,0.156538,"e.g., domain-specific entity names. Table 1 shows an example, where the word ‘染 色 体( chromosomes)’ is an infrequent word. As the system does not know (or has effectively ‘forgotten’) this keyword, it does not translate correctly, and an irrelevant translation is produced, leading to the phenomenon of ‘meaning drift’. This weakness with regard to infrequent words/pairs with NMT has been noticed by a number of researchers, and some studies have been conducted to address this problem, e.g., Luong et al. (2014); Cho et al. (2014); Li et al. (2016); Arthur et al. (2016); Bentivogli et al. (2016); Zhang et al. (2017). 1390 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1390–1399 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Superficially, this problem appears to be caused by the imperfect embeddings of infrequent words or the limited vocabulary size of NMT systems, but we argue that the deeper reason should be attributed to the nature of neural models: the translation function, represented by various neural networks, is shared amongst all of the translation pairs, so high-frequency and low-frequency pairs impact e"
D17-1146,D16-1096,0,0.0301998,"the M-NMT architecture outperformed the NMT baseline by 9.0 and 2.7 BLEU points on the two tasks, respectively. Additionally, we found this architecture resulted in a much more effective OOV treatment compared to competitive methods. 1 人类共有二十三对染色体。 Humans have 23 pairs of chromosomes. There are 23-year history of human history. Table 1: An example of Chinese-to-English ‘meaning drift’ with NMT. Introduction Neural Machine Translation (NMT) has been shown to have highly promising performance, particularly when a large amount of training data is available (Wu et al., 2016; Johnson et al., 2016; Mi et al., 2016). Although there are different model architectures (Sutskever et al., 2014; Bahdanau et al., 2015), the common principle behind the NMT approach is the same: encoding the meaning of the input into a concept space and performing translation based on this encoding. This ‘meaning encoding’ principle leads to a deeper understanding and learning of the translation rules, and hence a better translation than conventional statistic machine translation (SMT) that considers only surface forms, i.e., words and phrases (Koehn et al., 2003). Despite positive results obtained so far, a particular problem of"
D17-1146,J03-1002,0,0.00592511,"contribution from the model part and the memory part, but constrains have to be carefully settled to avoid overfitting to the training data. 3.4 Memory for SMT Integration The M-NMT architecture is a flexible framework that provides extra knowledge to the conventional model-based NMT. If the knowledge is generated by a conventional SMT system, it is essentially an elegant combination of SMT and NMT. In this work, we use the translation dictionary produced by an SMT system as the knowledge to create the memory, which involves first aligning the training sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) in both directions, and applying the “intersection” refinement rules (Koehn et al., 2003) to get a single one-to-one alignment for each sentence pair, and then extracting the translation dictionary based on these alignments. We can see the dictionary as the phrase pairs of the length 1 and leave the phrase pairs longer than 1 as future work. The key information provided by the dictionary is the conditional probability that a source and a target word are translated to each other. This information is used twice during local memory construction. Firstly, the conditional p(yjl |xj ) is used to se"
D17-1146,P02-1040,0,0.10856,"son, the models configurations in the NMT system and the M-NMT system were intentionally set to be identical. The number of hidden units, the word embedding dimensionality and the vocabulary size were empirically set to 500, 310 and 30000, respectively. In the training process, the batch size of the SGD algorithm was set to 80, and the parameters for AdaDelta were set to be ρ = 0.95 and  = 10−6 . The decoding is implemented as a beam search, where the beam size was set to be 5. Evaluation metrics The translation performance was evaluated using the BLEU score with caseinsensitive n ≤ 4-grams (Papineni et al., 2002). IWSLT05 52.5 43.9 45.9 NIST03 30.6 31.3 31.7 49.8 50.7 51.4 52.9 32.3 32.5 32.8 34.0 Table 3: BLEU scores with different translation systems on the two Chinese-English translation datasets. 5.3 SMT-NMT Integration Experiments In the first experiment, the M-NMT architecture combined SMT and NMT by using SMT to construct the memory to assist with NMT. For comparison purposes, the lexical prediction approach proposed by (Arthur et al., 2016) was also implemented. This uses the phrase table produced by SMT to improve NMT. Our implementation is a linear combination, and for a fair comparison, the"
D17-1146,P16-1162,0,0.0711918,"cy words. This is similar to our proposed idea, with the key difference that their work uses the attention information to 1394 select the target words, while ours trains a separate attention, based on both the source and target words. Regarding handling OOV words, Jean et al. (2015) presented an efficient training method to support a larger vocabulary, which helps alleviate the OOV problem significantly. Stahlberg et al. (2016) used SMT to produce candidate results in the form of lattice and NMT to re-score the results. As SMT uses a larger vocabulary than NMT, some OOV words can be retained. Sennrich et al. (2016) proposed a subword approach, where OOV words are expected to be spelled out by subword units. Luong et al. (2014) proposed a post-processing approach that learns the position of the source word when an UNK symbol is produced during decoding. By this position information, the UNK symbol (unknown words) can be replaced by the correct translation using a lexical table. Li et al. (2016) proposed a replace-and-restore approach that replaces infrequent words with similar words before the training and decoding, and restores rare words and their target words, obtained from a lexical table. Compared t"
D17-1146,P16-2049,0,0.018812,"motes the target words directly using the word mapping stored in the memory. Arthur et al. (2016) proposed to involve lexical knowledge to assist with translation, particularly for low-frequency words. This is similar to our proposed idea, with the key difference that their work uses the attention information to 1394 select the target words, while ours trains a separate attention, based on both the source and target words. Regarding handling OOV words, Jean et al. (2015) presented an efficient training method to support a larger vocabulary, which helps alleviate the OOV problem significantly. Stahlberg et al. (2016) used SMT to produce candidate results in the form of lattice and NMT to re-score the results. As SMT uses a larger vocabulary than NMT, some OOV words can be retained. Sennrich et al. (2016) proposed a subword approach, where OOV words are expected to be spelled out by subword units. Luong et al. (2014) proposed a post-processing approach that learns the position of the source word when an UNK symbol is produced during decoding. By this position information, the UNK symbol (unknown words) can be replaced by the correct translation using a lexical table. Li et al. (2016) proposed a replace-and"
D17-1146,D16-1027,0,0.0356471,"e neural model is fixed, so cannot output probabilities for OOV words. To solve this, we let the selected similar word entirely overwritten by the OOV word, and any prediction for the similar word will be ‘re-directed’ to the OOV word. 4 Related Work The idea of memory augmentation was inspired by recent advances in the neural Turing machine (Graves et al., 2014, 2016) and memory network (Weston et al., 2014). These new models equip neural networks with an external memory that can be accessed and manipulated via some trainable operations. The memory idea has been utilized in NMT. For example, Wang et al. (2016) used a memory to extend the state of the decoder RNN in the attention-based NMT. In this case, the contribution of the memory is to provide temporary variables to assist RNN decoding. In contrast, our work uses memory to store knowledge. The memory in Wang et al.’s work could be considered to be note paper, while the memory in our work is more like a dictionary. The idea of combining SMT and NMT was adopted by early NMT research, but these combinations were mostly based on the SMT framework, as discussed in depth in the review paper from Zhang et al. (2015). Cohn et al. (2016) proposed to enh"
D17-1146,1983.tc-1.13,0,0.687702,"Missing"
D18-1460,P05-1033,0,0.126753,"target hidden state sj is given by   ∗ sj = f eyj−1 , sj−1 , cj (6) The probability distribution Dj over all the words in the target vocabulary is predicted conditioned on the previous ground truth words, the context vector cj and the unrolled target information sj .   ∗ tj = g eyj−1 , cj , sj (7) oj = Wo tj (8) Dj = softmax (oj ) (9) where g stands for a linear transformation, Wo is used to map tj to oj so that each target word has one corresponding dimension in oj . 2.2 Cube Pruning The cube pruning algorithm, proposed by Chiang (2007) based on the k-best parsing algorithm of Huang and Chiang (2005), is actually an accelerated extension based on the naive beam search algorithm. Beam search, a heuristic dynamic programming searching algorithm, explores a graph by expanding the most promising nodes in a limited set and searches approximate optimal results from candidates. For the sequence-to-sequence learning task, given a pre-trained model, the beam search algorithm finds a sequence that approximately maximizes the conditional probability (Graves, 2012; Boulanger-Lewandowski et al., 2013). Both Sutskever et al. (2014) and Bahdanau et al. (2015) employed the beam search algorithm into the"
D18-1460,W14-4012,0,0.15052,"Missing"
D18-1460,D14-1179,0,0.0310691,"Missing"
D18-1460,P14-1129,0,0.0999653,"Missing"
D18-1460,P06-1121,0,0.0451297,"owski et al., 2013). Both Sutskever et al. (2014) and Bahdanau et al. (2015) employed the beam search algorithm into the NMT decoding to produce translations with relatively larger conditional probability with respect to the optimized model parameters. Remarkably, Huang and Chiang (2007) successfully applied the cube pruning algorithm to the decoding of SMT. They found that the beam search algorithm in SMT can be extended, and they utilized the cube pruning and some variants to optimize the search process in the decoding phase of phrase-based (Och and Ney, 2004) and syntaxbased (Chiang, 2005; Galley et al., 2006) systems, 4285 n) 0.1 0.2 1.1 0.1 0.2 1.1 0.1 0.2 1.1 3.2 2.1 2.2 2.3 3.2 2.1 2.2 2.3 3.2 2.1 2.2 2.3 3.2 (NP1,2 : The airplane) 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 3.5 3.6 4.5 3.4 3.5 3.6 4.5 3.4 3.5 3.6 4.5 3.4 3.5 3.6 4.5 (a) (b) (c) do w 4 3, P (V 1.1 2.3 3.4 ro ps off ) :t 3, 0.2 2.2 (NP1,2 : The apple) :d off ) ak es oo k :t 4 P 3, P (V (V 4 3, P (V 4 ) ps ro :d :t 4 3, P P (V (V off off ) ak es oo k :t 4 3, 4 3, P (V do w n) n) ) :d :t 4 3, ro ps off ) off ak es k oo :t 4 P 3, P (V (V 4 3, P (V do w n) do w ) ro :d :t 4 3, ps off off ) ak es oo k :t 4 3, P (V"
D18-1460,P17-1012,0,0.0219113,"translation to speed up the translation. To construct the equivalence class, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less softmax operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by 3.3× on GPUs and 3.5× on CPUs. 1 Introduction Neural machine translation (NMT) has shown promising results and drawn more attention recently (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Bahdanau et al., 2015; Gehring et al., 2017a,b; Vaswani et al., 2017). A widely used architecture is the attention-based encoder-decoder framework (Cho et al., 2014b; Bahdanau et al., 2015) which assumes there is a common semantic space between the source and target language pairs. The encoder encodes the source sentence to a representation in the common space with the recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997) and the decoder decodes this representation to generate the target sentence word by word. To generate a target word, a probability distribution over the target vocabulary is drawn based on the attention ov"
D18-1460,P82-1020,0,0.808202,"Missing"
D18-1460,2015.mtsummit-papers.23,0,0.737459,"Missing"
D18-1460,W05-1506,1,0.519405,"step, the target hidden state sj is given by   ∗ sj = f eyj−1 , sj−1 , cj (6) The probability distribution Dj over all the words in the target vocabulary is predicted conditioned on the previous ground truth words, the context vector cj and the unrolled target information sj .   ∗ tj = g eyj−1 , cj , sj (7) oj = Wo tj (8) Dj = softmax (oj ) (9) where g stands for a linear transformation, Wo is used to map tj to oj so that each target word has one corresponding dimension in oj . 2.2 Cube Pruning The cube pruning algorithm, proposed by Chiang (2007) based on the k-best parsing algorithm of Huang and Chiang (2005), is actually an accelerated extension based on the naive beam search algorithm. Beam search, a heuristic dynamic programming searching algorithm, explores a graph by expanding the most promising nodes in a limited set and searches approximate optimal results from candidates. For the sequence-to-sequence learning task, given a pre-trained model, the beam search algorithm finds a sequence that approximately maximizes the conditional probability (Graves, 2012; Boulanger-Lewandowski et al., 2013). Both Sutskever et al. (2014) and Bahdanau et al. (2015) employed the beam search algorithm into the"
D18-1460,P07-1019,1,0.863396,"employed to avoid unknown words (UNKs), which leads to a large number of normalization factors for the softmax operation when drawing the probability distribution. To accelerate the translation, the widely used method is to trade off between the translation quality and the decoding speed by reducing the size of vocabulary (Mi et al., 2016a) or/and the number of parameters, which can not realize the full potential of NMT. In this paper, we borrow ideas from phrasebased and syntax-based machine translation where cube pruning has been successfully applied to speed up the decoding (Chiang, 2007; Huang and Chiang, 2007). Informally, cube pruning “coarsens” the search space by clustering similar states according to some equivalence relations. To apply this idea to NMT, however, is much more involved. Specifically, in the process of beam search, we cluster similar target hidden states to construct equivalence classes, the three dimensions of which are target words in the target vocabulary, part translations retained in the beam search and different combinations of similar target hidden states, respectively. The clustering operation 4284 Proceedings of the 2018 Conference on Empirical Methods in Natural Languag"
D18-1460,P15-1001,0,0.0577112,"Missing"
D18-1460,W13-3214,0,0.0226871,"a popular technique to speed up dynamic programming, into neural machine translation to speed up the translation. To construct the equivalence class, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less softmax operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by 3.3× on GPUs and 3.5× on CPUs. 1 Introduction Neural machine translation (NMT) has shown promising results and drawn more attention recently (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Bahdanau et al., 2015; Gehring et al., 2017a,b; Vaswani et al., 2017). A widely used architecture is the attention-based encoder-decoder framework (Cho et al., 2014b; Bahdanau et al., 2015) which assumes there is a common semantic space between the source and target language pairs. The encoder encodes the source sentence to a representation in the common space with the recurrent neural network (RNN) (Hochreiter and Schmidhuber, 1997) and the decoder decodes this representation to generate the target sentence word by word. To generate a target word, a probability distributi"
D18-1460,D16-1096,0,0.0152537,"rd requires extensive computation to go through all the source words to calculate the attention. Worse still, due to the recurrence of RNNs, target words can only be generated sequentially rather than in parallel. The second reason is that large vocabulary on target side is employed to avoid unknown words (UNKs), which leads to a large number of normalization factors for the softmax operation when drawing the probability distribution. To accelerate the translation, the widely used method is to trade off between the translation quality and the decoding speed by reducing the size of vocabulary (Mi et al., 2016a) or/and the number of parameters, which can not realize the full potential of NMT. In this paper, we borrow ideas from phrasebased and syntax-based machine translation where cube pruning has been successfully applied to speed up the decoding (Chiang, 2007; Huang and Chiang, 2007). Informally, cube pruning “coarsens” the search space by clustering similar states according to some equivalence relations. To apply this idea to NMT, however, is much more involved. Specifically, in the process of beam search, we cluster similar target hidden states to construct equivalence classes, the three dimen"
D18-1460,P16-2021,0,0.0190433,"rd requires extensive computation to go through all the source words to calculate the attention. Worse still, due to the recurrence of RNNs, target words can only be generated sequentially rather than in parallel. The second reason is that large vocabulary on target side is employed to avoid unknown words (UNKs), which leads to a large number of normalization factors for the softmax operation when drawing the probability distribution. To accelerate the translation, the widely used method is to trade off between the translation quality and the decoding speed by reducing the size of vocabulary (Mi et al., 2016a) or/and the number of parameters, which can not realize the full potential of NMT. In this paper, we borrow ideas from phrasebased and syntax-based machine translation where cube pruning has been successfully applied to speed up the decoding (Chiang, 2007; Huang and Chiang, 2007). Informally, cube pruning “coarsens” the search space by clustering similar states according to some equivalence relations. To apply this idea to NMT, however, is much more involved. Specifically, in the process of beam search, we cluster similar target hidden states to construct equivalence classes, the three dimen"
D18-1460,J04-4002,0,0.0733736,"tional probability (Graves, 2012; Boulanger-Lewandowski et al., 2013). Both Sutskever et al. (2014) and Bahdanau et al. (2015) employed the beam search algorithm into the NMT decoding to produce translations with relatively larger conditional probability with respect to the optimized model parameters. Remarkably, Huang and Chiang (2007) successfully applied the cube pruning algorithm to the decoding of SMT. They found that the beam search algorithm in SMT can be extended, and they utilized the cube pruning and some variants to optimize the search process in the decoding phase of phrase-based (Och and Ney, 2004) and syntaxbased (Chiang, 2005; Galley et al., 2006) systems, 4285 n) 0.1 0.2 1.1 0.1 0.2 1.1 0.1 0.2 1.1 3.2 2.1 2.2 2.3 3.2 2.1 2.2 2.3 3.2 2.1 2.2 2.3 3.2 (NP1,2 : The airplane) 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 2.8 2.9 3.0 3.9 3.5 3.6 4.5 3.4 3.5 3.6 4.5 3.4 3.5 3.6 4.5 3.4 3.5 3.6 4.5 (a) (b) (c) do w 4 3, P (V 1.1 2.3 3.4 ro ps off ) :t 3, 0.2 2.2 (NP1,2 : The apple) :d off ) ak es oo k :t 4 P 3, P (V (V 4 3, P (V 4 ) ps ro :d :t 4 3, P P (V (V off off ) ak es oo k :t 4 3, 4 3, P (V do w n) n) ) :d :t 4 3, ro ps off ) off ak es k oo :t 4 P 3, P (V (V 4 3, P (V do w n) do w"
D18-1460,P02-1040,0,0.104328,"lish (ZhEn) translation task. 4.1 Data Preparation The Chinese-English training dataset consists of 1.25M sentence pairs3 . We used the NIST 2002 (MT02) dataset as the validation set with 878 sentences, and the NIST 2003 (MT03) dataset as the test dataset, which contains 919 sentences. The lengths of the sentences on both sides were limited up to 50 tokens, then actually 1.11M sentence pairs were left with 25.0M Chinese words and 27.0M English words. We extracted 30k most frequent words as the source and target vocabularies for both sides. In all the experiments, case-insensitive 4-gram BLEU (Papineni et al., 2002) was employed for the automatic evaluation, we used the script mteval-v11b.pl4 to calculate the BLEU score. 4.2 System The system is an improved version of attentionbased NMT system named RNNsearch (Bahdanau et al., 2015) where the decoder employs a conditional GRU layer with attention, consisting of two GRUs and an attention module for each step5 . Specifically, Equation (6) is replaced with the following two equations: ∗ s˜j = GRU1 (eyj−1 , sj−1 ) (13) sj = GRU2 (cj , s˜j ) (14) Besides, for the calculation of relevance in Equation (4), sj−1 is replaced with s˜j−1 . The other components of t"
D18-1460,1983.tc-1.13,0,0.635263,"Missing"
D18-1510,P18-1008,0,0.021728,"predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014) has now achieved impressive performance (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Hassan et al., 2018; Chen et al., 2018; Lample et al., 2018) and draws more attention. NMT models are built on the encoder-decoder framework where the encoder network encodes the source sentence to distributed representations and the decoder network reconstructs the target sentence form the representations word by word. Currently, NMT models are usually trained with the word-level loss (i.e., cross-entropy) under the teacher forcing algorithm (Williams and Zipser, *Corresponding Author In contrast, sequence-level objectives, such as BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), TER (Snover et al., 2006), and NIST (Doddingt"
D18-1510,D16-1139,0,0.0285603,"ct 1989), which forces the model to generate translation strictly matching the ground-truth at the word level. However, in practice it is impossible to generate translation totally the same as ground truth. Once different target words are generated, the word-level loss cannot evaluate the translation properly, usually under-estimating the translation. In addition, the teacher forcing algorithm suffers from the exposure bias (Ranzato et al., 2015) as it uses different inputs at training and inference, that is ground-truth words for the training and previously predicted words for the inference. Kim and Rush (2016) proposed a method of sequence-level knowledge distillation, which use teacher outputs to direct the training of student model, but the student model still have no access to its own predicted words. Scheduled sampling(SS) (Bengio et al., 2015; Venkatraman et al., 2015) attempts to alleviate the exposure bias problem through mixing ground-truth words and previously predicted words as inputs during training. However, the sequence generated by SS may not be aligned with the target sequence, which is inconsistent with the word-level loss. Neural machine translation (NMT) models are usually trained"
D18-1510,J82-2005,0,0.653907,"Missing"
D18-1510,P18-2053,0,0.0425741,", 2016; Wu et al., 2017; Yang et al., 2017) mainly rely on reinforcement learning algorithms (Williams, 1992; Sutton et al., 2000) to find an unbiased gradient estimator for the gradient update. Sparse rewards in this situation often cause the high variance of gradient estimation, which consequently leads to unstable 4778 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4778–4784 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics training and limited improvements. Lamb et al. (2016); Gu et al. (2017); Ma et al. (2018) respectively use the discriminator, critic and bag-of-words target as sequence-level training objectives, all of which are directly connected to the generation model and hence enable direct gradient update. However, these methods do not allow for direct optimization with respect to evaluation metrics. In this paper, we propose a method to combine the strengths of the word-level and sequencelevel training, that is the direct gradient update without gradient estimation from word-level training and the greater flexibility from sequence-level training. Our method introduces probabilistic ngram ma"
D18-1510,P02-1040,0,0.101363,"al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Hassan et al., 2018; Chen et al., 2018; Lample et al., 2018) and draws more attention. NMT models are built on the encoder-decoder framework where the encoder network encodes the source sentence to distributed representations and the decoder network reconstructs the target sentence form the representations word by word. Currently, NMT models are usually trained with the word-level loss (i.e., cross-entropy) under the teacher forcing algorithm (Williams and Zipser, *Corresponding Author In contrast, sequence-level objectives, such as BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), TER (Snover et al., 2006), and NIST (Doddington, 2002), evaluate translation at the sentence or n-gram level and allow for greater flexibility, and thus can mitigate the above problems of the word-level loss. However, due to the nondifferentiable of sequence-level objectives, previous works on sequence-level training (Ranzato et al., 2015; Shen et al., 2016; Bahdanau et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) mainly rely on reinforcement learning algorithms (Williams, 1992; Sutton et al., 2000) to find an unbiased gradient esti"
D18-1510,P16-1159,0,0.195704,"T models are usually trained with the word-level loss (i.e., cross-entropy) under the teacher forcing algorithm (Williams and Zipser, *Corresponding Author In contrast, sequence-level objectives, such as BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), TER (Snover et al., 2006), and NIST (Doddington, 2002), evaluate translation at the sentence or n-gram level and allow for greater flexibility, and thus can mitigate the above problems of the word-level loss. However, due to the nondifferentiable of sequence-level objectives, previous works on sequence-level training (Ranzato et al., 2015; Shen et al., 2016; Bahdanau et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) mainly rely on reinforcement learning algorithms (Williams, 1992; Sutton et al., 2000) to find an unbiased gradient estimator for the gradient update. Sparse rewards in this situation often cause the high variance of gradient estimation, which consequently leads to unstable 4778 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4778–4784 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics training and limited i"
D18-1510,2006.amta-papers.25,0,0.0686056,"; Hassan et al., 2018; Chen et al., 2018; Lample et al., 2018) and draws more attention. NMT models are built on the encoder-decoder framework where the encoder network encodes the source sentence to distributed representations and the decoder network reconstructs the target sentence form the representations word by word. Currently, NMT models are usually trained with the word-level loss (i.e., cross-entropy) under the teacher forcing algorithm (Williams and Zipser, *Corresponding Author In contrast, sequence-level objectives, such as BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), TER (Snover et al., 2006), and NIST (Doddington, 2002), evaluate translation at the sentence or n-gram level and allow for greater flexibility, and thus can mitigate the above problems of the word-level loss. However, due to the nondifferentiable of sequence-level objectives, previous works on sequence-level training (Ranzato et al., 2015; Shen et al., 2016; Bahdanau et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) mainly rely on reinforcement learning algorithms (Williams, 1992; Sutton et al., 2000) to find an unbiased gradient estimator for the gradient update. Sparse rewards in th"
D18-1510,D17-1210,0,0.0467661,"Missing"
D18-1510,D13-1176,0,0.122543,"d with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework. In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014) has now achieved impressive performance (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Hassan et al., 2018; Chen et al., 2018; Lample et al., 2018) and draws more attention. NMT models are built on the encoder-decoder framework where the encoder network encodes the source sentence to distributed representations and the decoder network reconstructs the target sentence form the representations word by word. Currently, NMT models are usually trained with the word-level loss (i.e., cross-entropy) under the teacher fo"
D18-1510,1983.tc-1.13,0,0.789694,"Missing"
D18-1510,N18-1122,0,0.0724559,"Missing"
D19-1164,2012.eamt-1.60,0,0.059146,"X, Y ) = 1 |D| j × J X I X {log P (yij |, y j<i , xj , D <j ; Θ) j=1 i=1 + PCCs(Capsenc (xj ), Capsdec (y j ))} (11) where Θ are parameters of the model, D <j are historical sentences of the to-be-translated source sentence, xj is the to-be-translated sentence and y j<i denotes the generated target hypothesis. 4 Experiments 4.1 Settings Datasets and Evaluation Metrics We carry out experiments on English-German translation tasks in three different domains: talks, news, and speeches. The corpora statistics are shown in Table 1. • TED. This corpus is a Machine Translation part of the IWSLT 2017 (Cettolo et al., 2012) evaluation compaigns1 , each TED talk is considered to be a document. we take the tst2016-207 as the test set, and other as our development set. • News. We take the sentence-aligned document-delimited News Commentary v11 corpus2 as our training set. The WMT’16 news-test2015 and news-test2016 are used for development and testing respectively. • Europarl. The corpus are extracted from the Europarl v7 (Koehn, 2005) according to the method mentioned in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above ext"
D19-1164,2010.iwslt-papers.10,0,0.0797008,"Missing"
D19-1164,D12-1108,0,0.0219715,"ords as inputs of the QCN and 4 duplicated query. Blue color denotes positive correlation and red means negative. From top to bottom are four heat maps in 0th to 3th iterations. value. See Figure 7, we can find that PCCs varies as iteration changes. 5 Related Work Document-level Machine Translation Document-level machine translation became a hot research direction in the later stage of statistical machine translation era. Hardmeier and Federico (2010) represented the links between word pairs in the context using a word dependency model for SMT to improve the translation of anaphoric pronouns. Hardmeier et al. (2012, 2013) first proposed a new document-level SMT paradigm that translates whole documents as units. However, in this period, most of the work has not achieved too many compelling results or has been only focused on a part of difficulties. With the coming of the era of Neural Machine Translation, many works began to focus on Document-level NMT tasks. Xiong et al. (2019) trained a reward teacher to refine the translation quality from a document perspective. Tiedemann and Scherrer (2017) simply concatenated sentences in one document as models’ input or output. Jean et al. (2017) used additional co"
D19-1164,P13-4033,0,0.11176,"Missing"
D19-1164,2005.mtsummit-papers.11,0,0.131344,"tasks in three different domains: talks, news, and speeches. The corpora statistics are shown in Table 1. • TED. This corpus is a Machine Translation part of the IWSLT 2017 (Cettolo et al., 2012) evaluation compaigns1 , each TED talk is considered to be a document. we take the tst2016-207 as the test set, and other as our development set. • News. We take the sentence-aligned document-delimited News Commentary v11 corpus2 as our training set. The WMT’16 news-test2015 and news-test2016 are used for development and testing respectively. • Europarl. The corpus are extracted from the Europarl v7 (Koehn, 2005) according to the method mentioned in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge opera"
D19-1164,P07-2045,0,0.00823295,"ed for development and testing respectively. • Europarl. The corpus are extracted from the Europarl v7 (Koehn, 2005) according to the method mentioned in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge operations. We use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate the translation quality. Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018). We performed the same configuration on our models according to the settings of the Maruf and Haffari (2018). Specifically, for the Transformer, we set the hidden size and"
D19-1164,W07-0734,0,0.0738311,"training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge operations. We use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate the translation quality. Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018). We performed the same configuration on our models according to the settings of the Maruf and Haffari (2018). Specifically, for the Transformer, we set the hidden size and point-wise FFN size as 512 and 2048 respectively. We use 4 layers and 8 attention heads in both encoder and decoder. All dropout rates are set to 0.1 for context-agnostic model and 0.2 for"
D19-1164,P18-1118,0,0.829638,"slation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentenc"
D19-1164,N19-1313,0,0.76905,"have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical attentions retrieved by the current generation. That is, it utilizes a wordle"
D19-1164,W13-3303,0,0.0224921,"1 Introduction The encoder-decoder based Neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation ("
D19-1164,D18-1325,0,0.678009,"skever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the"
D19-1164,P02-1040,0,0.104751,"d in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge operations. We use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate the translation quality. Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018). We performed the same configuration on our models according to the settings of the Maruf and Haffari (2018). Specifically, for the Transformer, we set the hidden size and point-wise FFN size as 512 and 2048 respectively. We use 4 layers and 8 attention heads in both encoder and decoder. All dropout rates are set to 0.1 fo"
D19-1164,P16-1162,0,0.0824309,"xtracted from the Europarl v7 (Koehn, 2005) according to the method mentioned in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge operations. We use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate the translation quality. Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018). We performed the same configuration on our models according to the settings of the Maruf and Haffari (2018). Specifically, for the Transformer, we set the hidden size and point-wise FFN size as 512 and 2048 respectively. We use 4 layers and 8 atte"
D19-1164,W17-4814,0,0.0550498,"ecoder based Neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al."
D19-1164,W17-4811,0,0.135797,"ween word pairs in the context using a word dependency model for SMT to improve the translation of anaphoric pronouns. Hardmeier et al. (2012, 2013) first proposed a new document-level SMT paradigm that translates whole documents as units. However, in this period, most of the work has not achieved too many compelling results or has been only focused on a part of difficulties. With the coming of the era of Neural Machine Translation, many works began to focus on Document-level NMT tasks. Xiong et al. (2019) trained a reward teacher to refine the translation quality from a document perspective. Tiedemann and Scherrer (2017) simply concatenated sentences in one document as models’ input or output. Jean et al. (2017) used additional context encoder to capture larger-context information. Kuang et al. (2017); Tu et al. (2018) used a cache to memorize most relevant words or features in previous sentences or translations. Recently, several studies integrated additional modules into the Transformer-based NMTs for modeling contextual information. (Voita et al., 2018; Zhang et al., 2018a), Maruf and Haffari (2018) proposed a document-level NMT using a 1534 memory-networks, and Wang et al. (2017) and Miculicich et al. (20"
D19-1164,Q18-1029,0,0.259966,"ang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical attentions retrieved by the current generation. That is,"
D19-1164,P18-1117,0,0.312421,"et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical attentions retrieved"
D19-1164,D17-1301,0,0.328004,"ni et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical attentions retrieved by the current gen"
D19-1164,D18-1486,0,0.0519226,"necessary to distinguish the role of each context word and model their relationship especially when one context word could take on multiple roles (Zhang et al., 2018b). However, this is difficult to realize for the HAN model as its final representation for the context is produced with an isolated relevance with the query word which ignores relations with other context words. To address the problem, we introduce Capsule Networks into document-level translation which have proven good at modelling the parts-wholes relations between low-level capsules and highlevel capsules (Hinton et al., 2011; Xiao et al., 2018; Sabour et al., 2017; Hinton et al., 2018; Gu and Feng, 2019). With capsule networks, the words in a context source sentence is taken as lowlevel capsules and the information of different perspectives is treated as high-level capsules. Then in the dynamic routing process of capsule networks, all the low-level capsules trade off against each other and consider over all the high-level capsules and drop themselves at a proper proportion to the high-level capsules. In this way, the relation among low-level capsules and that between low1527 Proceedings of the 2019 Conference on Empirical Methods i"
D19-1164,D18-1350,0,0.031748,"slty divided documentlevel translation tasks into two types: offline and online. Capsule Networks Hinton et al. (2011) proposed the capsule conception to use vector for describing the pose of an object. The dynamic routing algorithm was proposed by Sabour et al. (2017) to build the partwhole relationship through the iterative routing procedure. Hinton et al. (2018) designed a new routing style based on the EM algorithm. Some researchers investigated to apply the capsule network for various tasks. Wang et al. (2018) investigated a novel capsule network with dynamic routing for linear time NMT. Yang et al. (2018) explored capsule networks for text classification with strategies to stabilize the dynamic routing process. Gu and Feng (2019) introduces capsule networks into Transformer to model the relations between different heads in multi-head attention. We specifically investigated dynamic routing algorithms for the document-level NMT. 6 Conclusion We have proposed a novel Query-guided Capsule Network with an improved dynamic routing algorithm for enhancing context modeling for the document-level Neural Machine Translation Model. Experiments on English-German in different domains showed our model signi"
D19-1164,D18-1049,0,0.652933,"anau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical"
D19-1164,P19-1426,1,0.873378,"Missing"
D19-1164,C18-1110,1,0.888176,"anau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical"
N19-1312,W15-3001,0,0.0170069,"in, containing about 1.25M sentence pairs, and in-domain data is mainly related to the News Commentary domain which is more informal compared to the news corpus, containing about 59.1K sentences. We also used the development set of the domain adaptation shared task. Finally, we tested our method on the NC test set of WMT 3085 1 https://www.ldc.upenn.edu/ http://www.statmt.org/moses/ 3 https://nlp.stanford.edu/ 2 2006 and WMT 2007. We tokenized and lowercased the corpora. German→English For this task, out-of-domain corpus is from the WMT 2015 en-de translation task which are mainly News texts (Bojar et al., 2015) containing about 4.2M sentence pairs. For the in-domain corpus, we used the parallel training data from the IWSLT 2015 which is mainly from the the TED talks containing about 190K sentences. In addition, dev2012 and test2013/2014/2015 of IWSLT 2015 were selected as the development and test data, respectively. We tokenized and truecased the corpora. Besides, 16K, 16K and 32K merging operations were performed to learn byte-pair encoding(BPE) (Sennrich et al., 2015b) on both sides of the parallel training data and sentences longer than 50, 50 and 80 tokens were removed from the training data, re"
N19-1312,W17-4712,0,0.505001,"l applications, NMT models usually need to perform translation for some specific domain with only a small quantity of in*Corresponding Author domain training data but a large amount of out-ofdomain data. Simply combining in-domain training data with out-of-domain data will lead to overfitting to the out-of-domain data. Therefore, some domain adaptation technique should be adopted to improve in-domain translation. Fortunately, out-of-domain data still embodies common knowledge shared between domains. And incorporating the common knowledge from out-of-domain data can help in-domain translation. Britz et al. (2017) have done this kind of attempts and managed to improve in-domain translation. The common architecture of this method is to share a single encoder and decoder among all the domains and add a discriminator to the encoder to distinguish the domains of the input sentences. The training is based on adversarial learning between the discriminator and the translation , ensuring the encoder can learn common knowledge across domains that can help to generate target translation. Zeng et al. (2018) extend this line of work by introducing a private encoder to learn some domain specific knowledge. They hav"
N19-1312,W07-0718,0,0.0279898,"contains 1.25M sentence pairs. The LDC data is mainly related to the News domain. We chose the parallel sentences with the domain label Laws from the UMCorpus (Tian et al., 2014) as our in-domain data. We chose 109K, 1K and 1K sentences from the UM-Corpus randomly as our training, development and test data. We tokenized and lowercased the English sentences with Moses2 scripts. For the Chinese data, we performed word segmentation using Stanford Segmenter3 . English→German For this task, the training data is from the Europarl corpus distributed for the shared domain adaptation task of WMT 2007 (Callison-Burch et al., 2007) where the outof-domain data is mainly related to the News domain, containing about 1.25M sentence pairs, and in-domain data is mainly related to the News Commentary domain which is more informal compared to the news corpus, containing about 59.1K sentences. We also used the development set of the domain adaptation shared task. Finally, we tested our method on the NC test set of WMT 3085 1 https://www.ldc.upenn.edu/ http://www.statmt.org/moses/ 3 https://nlp.stanford.edu/ 2 2006 and WMT 2007. We tokenized and lowercased the corpora. German→English For this task, out-of-domain corpus is from th"
N19-1312,2016.amta-researchers.10,0,0.0212992,"information or out-of-domain parallel data. To exploit in-domain monolingual data, Gülçehre et al. (2015) train a RNNLM on the target side monolingual data first and then use it in decoding. Domhan and Hieber (2017) further extend this work by training the RNNLM part and translation part jointly. Sennrich et al. (2015a) propose to conduct back translation for the monolingual target data so as to generate the corresponding parallel data. Zhang and Zong (2016) employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. To introduce meta information, Chen et al. (2016) use the topic or category information of the input text to assistant the decoder and Kobus et al. (2017) extend the generic NMT models, which are trained on a diverse set of data to, specific domains with the specialized terminology and style. To make use of out-of-domain parallel data, Luong and Manning (2015) first train an NMT model with a large amount of out-of-domain data, then fine tune the model with in-domain data. Wang et al. (2017a) select sentence pairs from the outof-domain data set according to their similarity to the in-domain data and then add them to the indomain training data"
N19-1312,P17-1110,0,0.0255623,"common encoder, Zeng et al. (2018) further introduce a domain-specific encoder to each domain together with a domain-specific classifier to ensure the features extracted by the domain-specific encoder is proper. Compared to our method, they focus on the encoder and do not distinguish the information in the decoder. Adversarial Networks have achieved great success in some areas (Ganin et al., 2016; Goodfellow et al., 2014). Inspired by these work, we also employ a domain discriminator to extract some domain invariant features which has already shown its effectiveness in some related NLP tasks. Chen et al. (2017) use a classifier to exploit the shared information between different Chinese word segment criteria. Gui et al. (2017) tries to learn common features of the out-domain data and indomain data through adversarial discriminator for the part-of-speech tagging problem. Kim et al. (2017) train a cross-lingual model with languageadversarial training to generate the general information across different languages for the POS tagging problem. All these work try to utilize a discriminator to distinguish invariant features across the divergence. 3082 Decoder MLE ∗ yj−1 ... sj−1 sj RNN Unit Attention aj ="
N19-1312,P17-2061,0,0.249934,"Missing"
N19-1312,P05-1066,0,0.181363,"Missing"
N19-1312,D17-1158,0,0.0275401,"et al., 2017) and get consistently significant improvements over several strong baselines. 2 Related Work The task of domain adaptation for NMT is to translate a text in-domain for which only a small number of parallel sentences is available. The main idea of the work for domain adaptation is to introduce external information to help in-domain translation which may include in-domain monolingual data, meta information or out-of-domain parallel data. To exploit in-domain monolingual data, Gülçehre et al. (2015) train a RNNLM on the target side monolingual data first and then use it in decoding. Domhan and Hieber (2017) further extend this work by training the RNNLM part and translation part jointly. Sennrich et al. (2015a) propose to conduct back translation for the monolingual target data so as to generate the corresponding parallel data. Zhang and Zong (2016) employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. To introduce meta information, Chen et al. (2016) use the topic or category information of the input text to assistant the decoder and Kobus et al. (2017) extend the generic NMT models, which are trained on a diverse set of data to, specific doma"
N19-1312,D17-1256,0,0.02753,"ecific classifier to ensure the features extracted by the domain-specific encoder is proper. Compared to our method, they focus on the encoder and do not distinguish the information in the decoder. Adversarial Networks have achieved great success in some areas (Ganin et al., 2016; Goodfellow et al., 2014). Inspired by these work, we also employ a domain discriminator to extract some domain invariant features which has already shown its effectiveness in some related NLP tasks. Chen et al. (2017) use a classifier to exploit the shared information between different Chinese word segment criteria. Gui et al. (2017) tries to learn common features of the out-domain data and indomain data through adversarial discriminator for the part-of-speech tagging problem. Kim et al. (2017) train a cross-lingual model with languageadversarial training to generate the general information across different languages for the POS tagging problem. All these work try to utilize a discriminator to distinguish invariant features across the divergence. 3082 Decoder MLE ∗ yj−1 ... sj−1 sj RNN Unit Attention aj = αji h1 Encoder − → h1 ← − h1 x1 yj∗ yj Logistic Layer ! ℓs i=1 Out-of Domain Data ... Out-of Decoder + αij hi Shared E"
N19-1312,D13-1176,0,0.0862069,"rivate decoder for each domain which are used to model domain-specific information. In the meantime, we introduce a common encoder and a common decoder shared by all the domains which can only have domainindependent information flow through. Besides, we add a discriminator to the shared encoder and employ adversarial training for the whole model to reinforce the performance of information separation and machine translation simultaneously. Experiment results show that our method can outperform competitive baselines greatly on multiple data sets. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017) has made great progress and drawn much attention recently. Most NMT models are based on the encoder-decoder architecture, where all the sentence pairs share the same set of parameters for the encoder and decoder which makes NMT models have a tendency towards overfitting to frequent observations (e.g., words, word cooccurrences, translation patterns), but overlooking special cases that are not frequently observed. However, in practical applications, NMT models usually need to perform translation for some sp"
N19-1312,D17-1302,0,0.0188713,"h the information in the decoder. Adversarial Networks have achieved great success in some areas (Ganin et al., 2016; Goodfellow et al., 2014). Inspired by these work, we also employ a domain discriminator to extract some domain invariant features which has already shown its effectiveness in some related NLP tasks. Chen et al. (2017) use a classifier to exploit the shared information between different Chinese word segment criteria. Gui et al. (2017) tries to learn common features of the out-domain data and indomain data through adversarial discriminator for the part-of-speech tagging problem. Kim et al. (2017) train a cross-lingual model with languageadversarial training to generate the general information across different languages for the POS tagging problem. All these work try to utilize a discriminator to distinguish invariant features across the divergence. 3082 Decoder MLE ∗ yj−1 ... sj−1 sj RNN Unit Attention aj = αji h1 Encoder − → h1 ← − h1 x1 yj∗ yj Logistic Layer ! ℓs i=1 Out-of Domain Data ... Out-of Decoder + αij hi Shared Enocder Shared Decoder h 2 . . . h i . . . h ℓs ← − h2 − → h2 ←− h ℓs ... x2 GRL −→ h ℓs ... In Domain Data Domain Discriminator x ℓs ... Prediction Layer + Figure 1"
N19-1312,kobus-etal-2017-domain,0,0.516405,") train a RNNLM on the target side monolingual data first and then use it in decoding. Domhan and Hieber (2017) further extend this work by training the RNNLM part and translation part jointly. Sennrich et al. (2015a) propose to conduct back translation for the monolingual target data so as to generate the corresponding parallel data. Zhang and Zong (2016) employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. To introduce meta information, Chen et al. (2016) use the topic or category information of the input text to assistant the decoder and Kobus et al. (2017) extend the generic NMT models, which are trained on a diverse set of data to, specific domains with the specialized terminology and style. To make use of out-of-domain parallel data, Luong and Manning (2015) first train an NMT model with a large amount of out-of-domain data, then fine tune the model with in-domain data. Wang et al. (2017a) select sentence pairs from the outof-domain data set according to their similarity to the in-domain data and then add them to the indomain training data. Chu et al. (2017) construct the training data set for the NMT model by combining out-of-domain data wit"
N19-1312,2015.iwslt-evaluation.11,0,0.865654,"et al. (2015a) propose to conduct back translation for the monolingual target data so as to generate the corresponding parallel data. Zhang and Zong (2016) employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. To introduce meta information, Chen et al. (2016) use the topic or category information of the input text to assistant the decoder and Kobus et al. (2017) extend the generic NMT models, which are trained on a diverse set of data to, specific domains with the specialized terminology and style. To make use of out-of-domain parallel data, Luong and Manning (2015) first train an NMT model with a large amount of out-of-domain data, then fine tune the model with in-domain data. Wang et al. (2017a) select sentence pairs from the outof-domain data set according to their similarity to the in-domain data and then add them to the indomain training data. Chu et al. (2017) construct the training data set for the NMT model by combining out-of-domain data with the over-sampled in-domain data. Wang et al. (2017b) combine the in-domain and out-of-domain data together as the training data but apply instance weighting to get a weight for each sentence pair in the out"
N19-1312,P02-1040,0,0.105149,"mains. The biggest difference is that we add private parts to preserve the domain specific features. Besides we also applied a different training strategy as the section 5 describes so that our method can handle more generic situations. Noting that our model has a private encoderdecoder which brings extra parameters, we just slightly extend the hidden size of the contrast model to make sure that the total parameter number of the contrast model is equal to the number of our model’s translation part. 6.3 Main Results The En-Zh Experiments Results are measured using char based 5-gram BLEU score (Papineni et al., 2002) by the multi-bleu.pl script. The main results are shown in Table 1. On both of the development set and test set, our model significantly outperforms the baseline models and other contrast models. Furthermore, we got the following conclusions: First, the baseline model ’In’ surpass the ’Out + In’ model which shows that the NMT model tends to fit out-of-domain features if we directly include 3086 (a) without discriminator (b) full model Figure 3: The shared encoder’s hidden state of the two models. Data from the out-of-domain are presented as blue dots while data from the in-domain are presente"
N19-1312,P16-1009,0,0.127958,"Missing"
N19-1312,tian-etal-2014-um,0,0.278045,"Missing"
N19-1312,P17-2089,0,0.070391,"Missing"
N19-1312,D17-1155,0,0.348359,"Missing"
N19-1312,D18-1041,0,0.334609,"Missing"
N19-1312,D16-1160,0,0.0242738,"n idea of the work for domain adaptation is to introduce external information to help in-domain translation which may include in-domain monolingual data, meta information or out-of-domain parallel data. To exploit in-domain monolingual data, Gülçehre et al. (2015) train a RNNLM on the target side monolingual data first and then use it in decoding. Domhan and Hieber (2017) further extend this work by training the RNNLM part and translation part jointly. Sennrich et al. (2015a) propose to conduct back translation for the monolingual target data so as to generate the corresponding parallel data. Zhang and Zong (2016) employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. To introduce meta information, Chen et al. (2016) use the topic or category information of the input text to assistant the decoder and Kobus et al. (2017) extend the generic NMT models, which are trained on a diverse set of data to, specific domains with the specialized terminology and style. To make use of out-of-domain parallel data, Luong and Manning (2015) first train an NMT model with a large amount of out-of-domain data, then fine tune the model with in-domain data. Wang et al. (201"
N19-1312,C18-1110,1,0.874985,"Missing"
P09-1065,P06-1077,1,0.898463,"e word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than postprocessing system outputs. 576 Proceedings"
P09-1065,P07-1089,1,0.914806,"Missing"
P09-1065,D08-1076,0,0.100386,"Missing"
P09-1065,D08-1022,1,0.459626,"case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of 31.50, slightly lower than that of jo"
P09-1065,D08-1023,0,0.00900115,"j − i = l do for all m ∈ M do A DD(G, i, j, m) end for P RUNE(G, i, j) end for end for end procedure IP(x1 :VV, x2 :NN) → x1 x2 X → hfabiao, givei X → hyanjiang, a talki Figure 4: A derivation composed of both SCFG and tree-to-string rules. pairs and tree-to-string rules. Hierarchical phrase pairs are used for translating smaller units and tree-to-string rules for bigger ones. It is appealing to combine them in such a way because the hierarchical phrase-based model provides excellent rule coverage while the tree-to-string model offers linguistically motivated non-local reordering. Similarly, Blunsom and Osborne (2008) use both hierarchical phrase pairs and tree-to-string rules in decoding, where source parse trees serve as conditioning context rather than hard constraints. Depending on the target side output, we distinguish between string-targeted and tree-targeted models. String-targeted models include phrasebased, hierarchical phrase-based, and tree-tostring models. Tree-targeted models include string-to-tree and tree-to-tree models. All models can be combined at the translation level. Models that share with same target output structure can be further combined at the derivation level. The joint decoder u"
P09-1065,P08-1024,0,0.0833649,"ces of decisions that translate a source sentence into a target sentence step by step. For example, Figure 1 shows a sequence of SCFG rules (Chiang, 2005; Chiang, 2007) that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. Such sequence of decisions is called a derivation. In phrase-based models, a decision can be translating a source phrase into a target phrase or reordering the target phrases. In syntax-based models, decisions usually correspond to transduction rules. Often, there are many derivations that are distinct yet produce the same translation. Blunsom et al. (2008) present a latent variable model that describes the relationship between translation and derivation clearly. Given a source sentence f , the probability of a target sentence e being its translation is the sum over all possible derivations: ˆ = argmax e e P r(d, e|f ) = m λm hm (d, e, f ) Z(f ) ( X d∈∆(e,f ) ˆ ≈ argmax e e,d exp X ) λm hm (d, e, f ) m X m  λm hm (d, e, f ) (6) We refer to Eq. (5) as max-translation decoding and Eq. (6) as max-derivation decoding, which are first termed by Blunsom et al. (2008). By now, most current SMT systems, adopting either max-derivation decoding or max-t"
P09-1065,P05-1033,0,0.867394,"of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phas"
P09-1065,J07-2003,0,0.876253,"of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than p"
P09-1065,P08-1023,1,0.682905,"decoding in this case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of 31.50, slightly"
P09-1065,P02-1038,0,0.474353,"and corresponding translation e conditioned on a source sentence f : P (4) where Z(f ) is not needed in decoding because it is independent of e. Most SMT systems approximate the summation over all possible derivations by using 1-best derivation for efficiency. They search for the 1best derivation and take its target yield as the best translation: d∈∆(e,f ) exp p(d) where d is a decision in the derivation d. Although originally proposed for supporting large sets of non-independent and overlapping features, the latent variable model is actually a more general form of conventional linear model (Och and Ney, 2002). Accordingly, decoding for the latent variable model can be formalized as 2 Background P r(d, e|f ) Y d∈d ing with multiple models achieves an absolute improvement of 1.5 BLEU points over individual decoding with single models (Section 5). X (3) A feature value is usually decomposed as the product of decision probabilities: 2 h(d, e, f ) = P r(e|f ) = λm hm (d, e, f ) m e d∈∆(e,f ) Figure 1: A derivation composed of SCFG rules that translates a Chinese sentence “fabiao yanjiang” into an English sentence “give a talk”. X 3 Joint Decoding There are two major challenges for combining multiple mo"
P09-1065,J03-1002,0,0.00639286,"Missing"
P09-1065,J04-4002,0,0.208011,"ning further improvements. In this paper, we propose a framework for combining multiple translation models directly in deWe evaluated our joint decoder that integrated a hierarchical phrase-based model (Chiang, 2005; Chiang, 2007) and a tree-to-string model (Liu et al., 2006) on the NIST 2005 Chinese-English testset. Experimental results show that joint decod1 It might be controversial to use the term “model”, which usually has a very precise definition in the field. Some researchers prefer to saying “phrase-based approaches” or “phrase-based systems”. On the other hand, other authors (e.g., (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2007)) do use the expression “phrase-based models”. In this paper, we use the term “model” to emphasize that we integrate different approaches directly in decoding phase rather than postprocessing system outputs. 576 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 576–584, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP where hm is a feature function, λm is the associated feature weight, and Z(f ) is a constant for normalization: S → hX1 , X1 i X → hfabiao X1 , give a X1 i X → hyanjiang, talki Z(f ) = X X exp Stati"
P09-1065,P03-1021,0,0.815341,"rtial translations (Section 3.2). Although such translation-level combination will not produce new translations, it does change the way of selecting promising candidates. • Two models could even share derivations with each other if they produce the same structures on the target side (Section 3.3), which we refer to as derivation-level combination. This method enlarges the search space by allowing for mixing different types of translation rules within one derivation. • As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a seq"
P09-1065,A94-1016,0,0.256094,"derivation-level combination. This method enlarges the search space by allowing for mixing different types of translation rules within one derivation. • As multiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in"
P09-1065,P06-1121,0,0.228974,"ion hypergraph produced by another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating that the two models produce the same translations. and thus need to collect different information during decoding. For example, taking a source parse as input, a tree-to-string decoder (e.g., (Liu et al., 2006)) pattern-matches the source parse with treeto-string rules and produces a string on the target side. On the contrary, a string-to-tree decoder (e.g., (Galley et al., 2006; Shen et al., 2008)) is a parser that applies string-to-tree rules to obtain a target parse for the source string. As a result, the hypothesis structures of the two models are fundamentally different. tion 3.3). 3.1 Translation Hypergraph Despite the diversity of translation models, they all have to produce partial translations for substrings of input sentences. Therefore, we represent the search space of a translation model as a structure called translation hypergraph. Figure 2(a) demonstrates a translation hypergraph for one model, for example, a hierarchical phrase-based model. A node in a"
P09-1065,D08-1011,0,0.0936928,"are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translation models directly"
P09-1065,W99-0623,0,0.227171,"Missing"
P09-1065,W05-1506,0,0.6237,"g translation rule derivation model first parses f to obtain a source tree T (f ) and then transforms T (f ) to the target sentence e. Conversely, a string-to-tree model first parses f into a target tree T (e) and then takes the surface string e as the translation. Despite different inside, their derivations must begin with f and end with e. This situation remains the same for derivations between a source substring fij and its partial translation t during joint decoding: Table 1: Correspondence between translation hypergraph and decoding. More formally, a hypergraph (Klein and Manning., 2001; Huang and Chiang, 2005) is a tuple hV, E, Ri, where V is a set of nodes, E is a set of hyperedges, and R is a set of weights. For a given source sentence f = f1n = f1 . . . fn , each node v ∈ V is in the form of ht, [i, j]i, which denotes the recognition of t as one translation of the source substring spanning from i through j (that is, fi+1 . . . fj ). Each hyperedge e ∈ E is a tuple e = htails(e), head(e), w(e)i, where head(e) ∈ V is the consequent node in the deductive step, tails(e) ∈ V ∗ is the list of antecedent nodes, and w(e) is a weight function from R|tails(e) |to R. As a general representation, a translat"
P09-1065,P07-1019,0,0.0420047,"o surpass max-derivation decoding in this case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the system combination method achieved a BLEU score of"
P09-1065,P08-1067,0,0.0234869,"ax-translation decoding still failed to surpass max-derivation decoding in this case. Blunsom et al. (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. They show that max-translation decoding outperforms max-derivation decoding for the latent variable model. While they train the parameters using a maximum a posteriori estimator, we extend the MERT algorithm (Och, 2003) to take the evaluation metric into account. Comparison with System Combination Hypergraphs have been successfully used in parsing (Klein and Manning., 2001; Huang and Chiang, 2005; Huang, 2008) and machine translation (Huang and Chiang, 2007; Mi et al., 2008; Mi and Huang, 2008). Both Mi et al. (2008) and Blunsom et al. (2008) use a translation hypergraph to represent search space. The difference is that their hypergraphs are specifically designed for the forest-based tree-to-string model and the hierarchical phrase-based model, respectively, while ours is more general and can be applied to arbitrary models. We re-implemented a state-of-the-art system combination method (Rosti et al., 2007). As shown in Table 3, taking the translations of the two individual decoders as input, the sy"
P09-1065,W01-1812,0,0.192643,"Missing"
P09-1065,P02-1040,0,0.106668,"ections while two lines have at most one intersection. Fortunately, as the curve is monotonically increasing, 5 Experiments 5.1 Data Preparation Our experiments were on Chinese-to-English translation. We used the FBIS corpus (6.9M + 8.9M words) as the training corpus. For language model, we used the SRI Language Modeling Toolkit (Stolcke, 2002) to train a 4-gram model on the Xinhua portion of GIGAWORD corpus. We used the NIST 2002 MT Evaluation test set as our development set, and used the NIST 2005 test set as test set. We evaluated the translation quality using case-insensitive BLEU metric (Papineni et al., 2002). Our joint decoder included two models. The 581 Model Combination hierarchical tree-to-string N/A N/A translation derivation both Max-derivation Time BLEU 40.53 30.11 6.13 27.23 N/A N/A 48.45 31.63 Max-translation Time BLEU 44.87 29.82 6.69 27.11 55.89 30.79 54.91 31.49 Table 2: Comparison of individual decoding and joint decoding on average decoding time (seconds/sentence) and BLEU score (case-insensitive). 5.2 percentage first model was the hierarchical phrase-based model (Chiang, 2005; Chiang, 2007). We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) a"
P09-1065,P07-1040,0,0.331325,"ultiple derivations are used for finding optimal translations, we extend the minimum error rate training (MERT) algorithm (Och, 2003) to tune feature weights with respect to BLEU score for max-translation decoding (Section 4). 1 Introduction System combination aims to find consensus translations among different machine translation systems. It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). Recent several years have witnessed the rapid development of system combination methods based on confusion networks (e.g., (Rosti et al., 2007; He et al., 2008)), which show state-of-theart performance in MT benchmarks. A confusion network consists of a sequence of sets of candidate words. Each candidate word is associated with a score. The optimal consensus translation can be obtained by selecting one word from each set of candidates to maximizing the overall score. While it is easy and efficient to manipulate strings, current methods usually have no access to most information available in decoding phase, which might be useful for obtaining further improvements. In this paper, we propose a framework for combining multiple translati"
P09-1065,P08-1066,0,0.0477497,"ed by another model; (c) the packed translation hypergraph based on (a) and (b). Solid and dashed lines denote the translation rules of the two models, respectively. Shaded nodes occur in both (a) and (b), indicating that the two models produce the same translations. and thus need to collect different information during decoding. For example, taking a source parse as input, a tree-to-string decoder (e.g., (Liu et al., 2006)) pattern-matches the source parse with treeto-string rules and produces a string on the target side. On the contrary, a string-to-tree decoder (e.g., (Galley et al., 2006; Shen et al., 2008)) is a parser that applies string-to-tree rules to obtain a target parse for the source string. As a result, the hypothesis structures of the two models are fundamentally different. tion 3.3). 3.1 Translation Hypergraph Despite the diversity of translation models, they all have to produce partial translations for substrings of input sentences. Therefore, we represent the search space of a translation model as a structure called translation hypergraph. Figure 2(a) demonstrates a translation hypergraph for one model, for example, a hierarchical phrase-based model. A node in a hypergraph denotes"
P09-1065,P05-1044,0,0.021838,"Missing"
P09-1065,P06-1098,0,0.0135744,"ensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The input is a source language sentence f1n , and a set of translation models M Translation-Level Combination The conventional interpretation of Eq. (1) is that the probability of a translation is the sum over all possible derivations coming from the same model"
P09-1065,J97-3002,0,0.0490287,"model. The shaded nodes are shared by both models. Therefore, the two models are combined at the translation level. Intuitively, shared nodes should be favored in decoding because they offer consensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The input is a source language sentence f1n , and a"
P09-1065,P06-1066,1,0.502138,"rst model and the dashed lines denote those of the second model. The shaded nodes are shared by both models. Therefore, the two models are combined at the translation level. Intuitively, shared nodes should be favored in decoding because they offer consensus translations among different models. Now the question is how to decode with multiple models jointly in just one decoder. We believe that both left-to-right and bottom-up strategies can be used for joint decoding. Although phrase-based decoders usually produce translations from left to right, they can adopt bottom-up decoding in principle. Xiong et al. (2006) develop a bottom-up decoder for BTG (Wu, 1997) that uses only phrase pairs. They treat reordering of phrases as a binary classification problem. On the other hand, it is possible for syntax-based models to decode from left to right. Watanabe et al. (2006) propose leftto-right target generation for hierarchical phrasebased translation. Although left-to-right decoding might enable a more efficient use of language models and hopefully produce better translations, we adopt bottom-up decoding in this paper just for convenience. Figure 3 demonstrates the search algorithm of our joint decoder. The i"
P09-1065,N03-1017,0,\N,Missing
P12-1100,P08-1009,0,0.283488,"longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to combine the me"
P12-1100,P05-1033,0,0.38994,"A derivation yields a pair of strings on the right-hand side which are translation of each other. In a weighted SCFG, each rule has a weight and the total weight of a derivation is the production of the weights of the rules used by the derivation. A translation may be produced by many different derivations and we only use the best derivation to evaluate its probability. With d denoting a derivation and r denoting a rule, we have p(e|f ) = max p(d, e|f , cˆ) d Y = max p(r, e|f , cˆ) d X (6) k We employ the same set of features for the loglinear model as the hierarchical phrase-based model does(Chiang, 2005). We further refine our hierarchical chunk-to-string model into two models: a loose model which is more similar to the hierarchical phrase-based model and a tight model which is more similar to the tree-tostring model. The two models differ in the form of rules and the way of estimating rule probabilities. While for decoding, we employ the same decoding algorithm for the two models: given a test sentence, the decoders first perform shallow parsing to get the best chunk sequence, then apply a CYK parsing algorithm with beam search. 2.1 A Loose Model In our model, we employ rules containing nont"
P12-1100,J07-2003,0,0.433856,"grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks. X → hX 1 for X 2 , X 2 de X 1 i can be applied to both of the following strings in Figure 1 “A request for a purchase of shares” “filed for bankruptcy”, and get the following translation, respectively “goumai gufen de shenqing” “pochan de shenqing”. 1 Introduction The hierarchical phrase-based model (Chiang, 2007) makes an advance of statistical machine translation by employing hierarchical phrases, which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope. Besides, this model is formal syntax-based and does not need to specify the syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗ This work was done when the first"
P12-1100,P05-1066,0,0.0777595,"ed decoder, tree represents the tree-to-string decoder, tight represents our tight decoder and loose represents our loose decoder. The speed is reported by seconds per sentence. The speed for the tree-tostring decoder includes the parsing time (0.23s) and the speed for the tight and loose models includes the shallow parsing time, too. extraction as: the height up to 3 and the number of leaf nodes up to 5. We give the results in Table 2. From the results, we can see that both the loose and tight decoders outperform the baseline decoders and the improvement is significant using the sign-test of Collins et al. (2005) (p < 0.01). Specifically, the loose model has a better performance while the tight model has a faster speed. Compared with the hierarchical phrase-based model, the loose model only imposes syntactic cohesion cohesion to nonterminals while the tight model imposes syntax cohesion to both rules and nonterminals which reduces search space, so it decoders faster. We can conclude that linguistic syntax can indeed improve the translation performance; syntactic cohesion for nonterminals can explain linguistic phenomena well; noncohesive rules are useful, too. The extra time consumption against hierar"
P12-1100,P03-2041,0,0.0452022,"ntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hier"
P12-1100,C10-2033,1,0.84756,"reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to combine the merits of the two mode"
P12-1100,W02-1039,0,0.1364,"a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-t"
P12-1100,N04-1035,0,0.0742159,"tion We used the NIST training set for Chinese-English translation tasks excluding the Hong Kong Law and Hong Kong Hansard2 as the training data, which contains 470K sentence pairs. For the training data set, we first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignments using “final-and”. We trained a 5-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of LDC Chinese Gigaword corpus. For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al., 2004). We used our in-house English-Chinese data set as the development set and used the 2008 NIST English-Chinese MT test set (1859 sentences) as the test set. Our evaluation metric was BLEU-4 (Papineni et al., 2002) based on characters (as the target language is Chinese), which performed caseinsensitive matching of n-grams up to n = 4 and used the shortest reference for the brevity penalty. We used the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. 6.2 Shallow Parsing The standard evaluation metrics for shallow parsi"
P12-1100,N04-1014,0,0.119779,"e parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring t"
P12-1100,2006.amta-papers.8,0,0.0819158,"he syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗ This work was done when the first author visited Microsoft Research Asia as an intern. In the former, “A request” is a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997;"
P12-1100,N03-1017,0,0.0974779,"oduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1 We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . 954 Given a sentence pair hf , e, ∼i, we extract rules for the loose model as follows 1. If hfjj12 , eii21 i is a basic phrase, then we can have a rule X → hfjj12 , eii21 i 2. Assume X → hα, βi is a rule with α = α1 fjj12 α2 and β = β1 eii21 β2 , and hfjj12 , eii21 i is a chunk-based phrase with a chunk sequence Yu · · · Yv , then we have the following rule X → hα1 Yu -Yv k α2 , β1 Yu -"
P12-1100,C10-1080,1,0.843744,"al phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder translates a packed forest of exponentially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 955 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB) English parsing task by splitting the sections 02-21 on the Wall Street Journal Portion (Marcus et al., 1993) into two sets: the last 1000 sentences as"
P12-1100,P06-1077,1,0.904774,"need to specify the syntactic constituents of subphrases, so it can directly learn synchronous context-free grammars (SCFG) from a parallel text without relying on any linguistic annotations or assumptions, which makes it used conveniently and widely. ∗ This work was done when the first author visited Microsoft Research Asia as an intern. In the former, “A request” is a NP and this rule acts correctly while in the latter “filed” is a VP and this rule gives a wrong reordering. If we specify the first X on the right-hand side to NP, this kind of errors can be avoided. The tree-to-string model (Liu et al., 2006; Huang et al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some langu"
P12-1100,J93-2004,0,0.0396526,"tially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 955 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB) English parsing task by splitting the sections 02-21 on the Wall Street Journal Portion (Marcus et al., 1993) into two sets: the last 1000 sentences as the test set and the rest as the training set. We filtered the features whose frequency was lower than 3 and substituted ‘‘ and ’’ with ˝ to keep consistent with translation data. We used L2 algorithm to train CRF. Data for Translation We used the NIST training set for Chinese-English translation tasks excluding the Hong Kong Law and Hong Kong Hansard2 as the training data, which contains 470K sentence pairs. For the training data set, we first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignm"
P12-1100,P08-1114,0,0.0508566,"model. Their model reorders chunks resorting to a distortion model while our model reorders chunks according to SCFG rules which retain the relative positions of chunks. Nguyen et al. (2008) presented a tree-to-string phrase-based method which is based on SCFGs. This method generates SCFGs through syntactic transformation including a word-to-phrase tree transformation model and a phrase reordering model while our model learns SCFG-based rules from word-aligned bilingual corpus directly There are also some works aiming to introduce linguistic knowledge into the hierarchical phrasebased model. Marton and Resnik (2008) took the source parse tree into account and added soft constraints to hierarchical phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder"
P12-1100,P08-1023,1,0.860438,"edge into the hierarchical phrasebased model. Marton and Resnik (2008) took the source parse tree into account and added soft constraints to hierarchical phrase-based model. Cherry (2008) used dependency tree to add syntactic cohesion. These methods work with the original SCFG defined by hierarchical phrase-based model and use linguistic knowledge to assist translation. Instead, our model works under the new defined SCFG with chunks. Besides, some other researchers make efforts on the tree-to-string model by employing exponentially alternative parses to alleviate the drawback of 1-best parse. Mi et al. (2008) presented forest-based translation where the decoder translates a packed forest of exponentially many parses instead of i-best parse. Liu and Liu (2010) proposed to parse and to translate jointly by taking tree-based translation as parsing. Given a source sentence, this decoder produces a parse tree on the source side and a translation on the target side simultaneously. Both the models perform in the unit of tree nodes rather than chunks. 955 6 Experiments 6.1 Data Preparation Data for shallow parsing We got training data and test data for shallow parsing from the standard Penn Tree Bank (PTB"
P12-1100,P00-1056,0,0.604931,"sk and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1 We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . 954 Given a sentence pair hf , e, ∼i, we extract rules for the loose model as follows 1. If hfjj12 , eii21 i is a basic phrase, the"
P12-1100,P02-1038,0,0.20294,"ploy rules containing nonterminals to handle long-distance reordering where boundary words play an important role. So for the subphrases which cover more than one chunk, we just maintain boundary chunks: we bundle adjacent chunks into one nonterminal and denote it as the first chunk tag immediately followed by “-” and next followed by the last chunk tag. Then, for the string pair <filed for bankruptcy, shenqing pochan>, we can get the rule r1 : X → hVBN 1 for NP 2 , VBN 1 NP 2 i while for the string pair <A request for a purchase of shares, goumai gufen de shenqing>, we can get r2 : Following Och and Ney (2002), we frame our model as a log-linear model: P exp k λk Hk (d, e, cˆ, f ) P (5) p(e|f ) = exp d′ ,e′ ,k λk Hk (d′ , e′ , cˆ, f ) Hk (d, e, ˆ c, f ) = λk Hk (d, e, cˆ, f ) X → hNP 1 for NP-NP 2 , NP-NP 2 de NP 1 i. (4) r∈d where X hk (f , cˆ, r) r 952 The rule matching “A request for a purchase of shares was” will be r3 : X → hNP-NP 1 VBD 2 , NP-NP 1 VBD 2 i. We can see that in contrast to the method of representing each chunk separately, this representation form can alleviate data sparseness and the influence of parsing errors. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hX 4 X 3 , X 4 X 3 i ⇒ hNP-NP"
P12-1100,J04-4002,0,0.217737,"kier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heuristic as previous systems (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005). A chunk-based phrase is such a basic phrase that covers one or more chunks on the source side. 1 We use the open source toolkit CRF++ got in http://code.google.com/p/crfpp/ . 954 Given a sentence pair hf , e, ∼i, we extract rules for the loose model as follows 1. If hfjj12 , eii21 i is a basic phrase, then we can have a rule X → hfjj12 , eii21 i 2. Assume X → hα, βi is a rule with α = α1 fjj12 α2 and β = β1 eii21 β2 , and hfjj12 , eii21 i is a chunk-based phrase with a chunk sequence Yu · · · Yv , then we have the following rule X → hα1 Yu -Yv k α2 , β1 Yu -Yv k β2 i. We evalu"
P12-1100,P03-1021,0,0.171403,"Missing"
P12-1100,P02-1040,0,0.0852269,"e first performed word alignment in both directions using GIZA++ toolkit (Och and Ney, 2000) then refined the alignments using “final-and”. We trained a 5-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of LDC Chinese Gigaword corpus. For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al., 2004). We used our in-house English-Chinese data set as the development set and used the 2008 NIST English-Chinese MT test set (1859 sentences) as the test set. Our evaluation metric was BLEU-4 (Papineni et al., 2002) based on characters (as the target language is Chinese), which performed caseinsensitive matching of n-grams up to n = 4 and used the shortest reference for the brevity penalty. We used the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. 6.2 Shallow Parsing The standard evaluation metrics for shallow parsing are precision P, recall R, and their harmonic mean F1 score, given by: number of exactly recognized chunks number of output chunks number of exactly recognized chunks R= number of reference chunks P = 2 The so"
P12-1100,P05-1034,0,0.106643,"ordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we present a hierarchical chunk-tostring translation model to"
P12-1100,N03-1028,0,0.0397363,"gnized as a chunk, we skip its children. In this way, we can get a sole chunk sequence given a parse tree. Then we label each word with a label indicating whether the word starts a chunk (B-) or continues a chunk (I-). Figure 2(a) gives an example. In this method, we get the training data for shallow parsing from Penn Tree Bank. We take shallow Parsing (chunking) as a sequence label task and employ Conditional Random Field (CRF)1 to train a chunker. CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in Sha and Pereira (2003) for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971), Lafferty et al. (2001), Taskar et al. (2002), Sha and Pereira (2003). 4 Rule Extraction In what follows, we introduce how to get the rule set. We learn rules from a corpus that first is bidirectionally word-aligned by the GIZA++ toolkit (Och and Ney, 2000) and then is refined using a “final-and” strategy. We generate the rule set in two steps: first, we extract two sets of phrases, basic phrases and chunk-based phrases. Basic phrases are defined using the same heurist"
P12-1100,P03-1039,0,0.0253069,"e way as Liu et al. (2006). For the loose model, the nonterminals must be cohesive, while the whole rule can be noncohesive: if both ends of a rule are nonterminals, the whole rule is cohesive, otherwise, it may be noncohesive. In contrast, for the tight model, both the whole rule and the nonterminal are cohesive. Even with the cohesion constraints, our model still generates a large number of rules, but not all of the rules are useful for translation. So we follow the method described in Chiang (2007) to filter the rule set except that we allow two nonterminals to be adjacent. 5 Related Works Watanabe et al. (2003) presented a chunk-to-string translation model where the decoder generates a translation by first translating the words in each chunk, then reordering the translation of chunks. Our model distinguishes from their model mainly in reordering model. Their model reorders chunks resorting to a distortion model while our model reorders chunks according to SCFG rules which retain the relative positions of chunks. Nguyen et al. (2008) presented a tree-to-string phrase-based method which is based on SCFGs. This method generates SCFGs through syntactic transformation including a word-to-phrase tree tran"
P12-1100,J97-3002,0,0.132427,"al., 2006) introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse e"
P12-1100,P01-1067,0,0.0718737,"introduces linguistic syntax via source parse to direct word reordering, especially longdistance reordering. Furthermore, this model is formalised as Tree Substitution Grammars, so it observes syntactic cohesion. Syntactic cohesion means that the translation of a string covered by a subtree in a source parse tends to be continuous. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Many previous works show promising results with an assumption that syntactic cohesion explains almost all translation movement for some language pairs (Wu, 1997; Yamada and Knight, 2001; Eisner, 2003; Graehl and Knight, 2004; Quirk et al., 2005; Cherry, 2008; Feng et al., 2010). 950 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950–958, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics But unfortunately, the tree-to-string model requires each node must be strictly matched during rule matching, which makes it strongly dependent on the relationship of tree nodes and their roles in the whole sentence. This will lead to data sparseness and being vulnerable to parse errors. In this paper, we"
P12-1100,W08-2119,0,\N,Missing
P12-1100,J08-3004,0,\N,Missing
P13-1033,P09-1088,1,0.917596,"target word take and change the aligned source word from prends to Je, then the items for which we need to decrement and increment the counts by one are shown in Table 1 and the posterior probability corresponding to the new alignment is the product of the hierarchical PYP probabilities of all increment items divided by the probability of the fertility of prends being single. Maintaining the current state of the hPYP as events are incremented and decremented is nontrivial and the naive approach requires significant book-keeping and has poor runtime behaviour. For this we adopt the approach of Blunsom et al. (2009b), who present a method for maintaining table counts without needing to record the table assignments for each translation decision. Briefly, this algorithm samples the table assignment during the increment and decrement operations, which is then used to maintain aggregate table statistics. This can be done efficiently and without the need for explicit table assignment tracking. 4.1 4.2 Parallel Implementation As mentioned above, the hierarchical PYP takes into consideration a rich history to evaluate the probabilities of translation decisions. But this leads to difficulties when applying the"
P13-1033,P03-1021,0,0.0618165,"aging, which provides a better estimate of uncertainty cf. using a single sample.5 The alignment used for the baseline results was produced by combining bidirectional GIZA++ alignments using the grow-diag-final heuristic. We used the Moses machine translation decoder (Koehn et al., 2007), using the default features and decoding settings. We compared the performance of Moses using the alignment produced by our model and the baseline alignment, evaluating translation quality using BLEU (Papineni et al., 2002) with case-insensitive n-gram matching with n = 4. We used minimum error rate training (Och, 2003) to tune the feature weights to maximise the BLEU score on the development set. 5.2 Dev 45.78 49.13 49.68 51.32 for the test set we use the IWSLT 2005 test set. The language model is a 3-gram language model trained using the SRILM toolkit (Stolcke, 2002) on the English side of the training data. Because the data set is small, we performed Gibbs sampling on a single processor. First we check the effect of the model factors jump and fertility. Both emission and finish factors are indispensable to the generative translation process, and consequently these two factors are included in all runs. Tab"
P13-1033,P09-2085,1,0.919724,"target word take and change the aligned source word from prends to Je, then the items for which we need to decrement and increment the counts by one are shown in Table 1 and the posterior probability corresponding to the new alignment is the product of the hierarchical PYP probabilities of all increment items divided by the probability of the fertility of prends being single. Maintaining the current state of the hPYP as events are incremented and decremented is nontrivial and the naive approach requires significant book-keeping and has poor runtime behaviour. For this we adopt the approach of Blunsom et al. (2009b), who present a method for maintaining table counts without needing to record the table assignments for each translation decision. Briefly, this algorithm samples the table assignment during the increment and decrement operations, which is then used to maintain aggregate table statistics. This can be done efficiently and without the need for explicit table assignment tracking. 4.1 4.2 Parallel Implementation As mentioned above, the hierarchical PYP takes into consideration a rich history to evaluate the probabilities of translation decisions. But this leads to difficulties when applying the"
P13-1033,P02-1040,0,0.0885467,"lignments using the grow-diag-final heuristic. Using multiple samples in this way constitutes Monte Carlo averaging, which provides a better estimate of uncertainty cf. using a single sample.5 The alignment used for the baseline results was produced by combining bidirectional GIZA++ alignments using the grow-diag-final heuristic. We used the Moses machine translation decoder (Koehn et al., 2007), using the default features and decoding settings. We compared the performance of Moses using the alignment produced by our model and the baseline alignment, evaluating translation quality using BLEU (Papineni et al., 2002) with case-insensitive n-gram matching with n = 4. We used minimum error rate training (Och, 2003) to tune the feature weights to maximise the BLEU score on the development set. 5.2 Dev 45.78 49.13 49.68 51.32 for the test set we use the IWSLT 2005 test set. The language model is a 3-gram language model trained using the SRILM toolkit (Stolcke, 2002) on the English side of the training data. Because the data set is small, we performed Gibbs sampling on a single processor. First we check the effect of the model factors jump and fertility. Both emission and finish factors are indispensable to th"
P13-1033,J93-2003,0,0.061,"xplicitly in a number of previous approaches, in grammar based (Chiang, 2005) and phrase-based systems (Galley and Manning, 2010). The latter is most similar to this paper, and shows that discontinuous phrases compliment standard contiguous phrases, improving expressiveness and translation performance. Unlike their work, here we develop a complimentary approach by constructing a generative model which can induce these rich rules directly from sentence-aligned corpora. Related Work Word based models have a long history in machine translation, starting with the venerable IBM translation models (Brown et al., 1993) and the hidden Markov model (Vogel et al., 1996). These models are still in wide-spread use today, albeit only as a preprocessing step for inferring word level alignments from sentence-aligned parallel corpora. They combine a number of factors, including distortion and fertility, which have been shown to improve word-alignment and translation performance over simpler models. Our approach is similar to these works, as we also develop a word-based model, and explicitly consider similar translation decisions, alignment jumps and fertility. We extend these works in two important respects: 1) whil"
P13-1033,P05-1033,0,0.533882,"translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1 Introduction Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based (Koehn et al., 2003) and syntax-based approaches (Chiang, 2005; Galley et al., 2006; Liu et al., 2006). These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on con333 Proceedings of the 51st Annual Meeting of the Association fo"
P13-1033,P11-1105,0,0.247736,"e translation. Vaswani et al. (2011) propose a rule Markov model for a tree-to-string model which models correlations between pairs of mininal rules, and use Kneser-Ney smoothing to alleviate the problems of data sparsity. Similarly, Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, which they use as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Also relevant is Durrani et al. (2011), who present a sequence model of translation including reordering. 3 Model Given a source sentence, our model infers a latent derivation which produces a target translation and meanwhile gives a word alignment between the source and the target. We consider a process in which the target string is generated using a left-to-right order, similar to the decoding strategy used by phrase-based machine translation systems (Koehn et al., 2003). During this process we maintain a position in the source sentence, which can jump around to allow for different sentence ordering in the target vs. source lang"
P13-1033,P06-1124,0,0.309461,"s the use of discontinuous phrases and leads to poor generalisation to unseen data (where large phrases tend not to match). In this paper we propose a new model to drop the independence assumption, by instead modelling correlations between translation decisions, which we use to induce translation derivations from aligned sentences (akin to word alignment). We develop a Markov model over translation decisions, in which each decision is conditioned on previous n most recent decisions. Our approach employs a sophisticated Bayesian non-parametric prior, namely the hierarchical Pitman-Yor Process (Teh, 2006; Teh et al., 2006) to represent backoff from larger to smaller contexts. As a result, we need only use very simple translation units – primarily single words, but can still describe complex multi-word units through correlations between their component translation decisions. We further decompose the process of generating each target word into component factors: finishing the translating, jumping elsewhere in the source, emitting a target word and deciding the fertility of the source words. Overall our model has the following features: 1. enabling model parameters to be shared between similar t"
P13-1033,N10-1140,0,0.0442751,"ng context. In contrast to these approaches which primarily address the decoding problem, we focus on the learning problem of inferring alignments from parallel sentences. Additionally, we develop a full generative model using a Bayesian prior, and incorporate additional factors besides lexical items, namely jumps in the source and word fertility. Another aspect of this paper is the implicit support for phrase-pairs that are discontinous in the source language. This idea has been developed explicitly in a number of previous approaches, in grammar based (Chiang, 2005) and phrase-based systems (Galley and Manning, 2010). The latter is most similar to this paper, and shows that discontinuous phrases compliment standard contiguous phrases, improving expressiveness and translation performance. Unlike their work, here we develop a complimentary approach by constructing a generative model which can induce these rich rules directly from sentence-aligned corpora. Related Work Word based models have a long history in machine translation, starting with the venerable IBM translation models (Brown et al., 1993) and the hidden Markov model (Vogel et al., 1996). These models are still in wide-spread use today, albeit onl"
P13-1033,P11-1086,0,0.0569197,"wo important respects: 1) while they assume a simple parameterisation by making iid assumptions about each translation factor, we instead allow for rich correlations by modelling sequences of translation decisions; and 2) we develop our model in the Bayesian framework, using a hierarchical PitmanYor Process prior with rich backoff semantics between high and lower order sequences of translation decisions. Together this results in a model with rich expressiveness but can still generalize well to unseen data. More recently, a number of authors have proposed Markov models for machine translation. Vaswani et al. (2011) propose a rule Markov model for a tree-to-string model which models correlations between pairs of mininal rules, and use Kneser-Ney smoothing to alleviate the problems of data sparsity. Similarly, Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, which they use as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Also relevant is Durrani et al. (2011), who present a"
P13-1033,P06-1121,0,0.0742409,"ecisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1 Introduction Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based (Koehn et al., 2003) and syntax-based approaches (Chiang, 2005; Galley et al., 2006; Liu et al., 2006). These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on con333 Proceedings of the 51st Annual Meeting of the Association for Computational Lingu"
P13-1033,C96-2141,0,0.406643,"grammar based (Chiang, 2005) and phrase-based systems (Galley and Manning, 2010). The latter is most similar to this paper, and shows that discontinuous phrases compliment standard contiguous phrases, improving expressiveness and translation performance. Unlike their work, here we develop a complimentary approach by constructing a generative model which can induce these rich rules directly from sentence-aligned corpora. Related Work Word based models have a long history in machine translation, starting with the venerable IBM translation models (Brown et al., 1993) and the hidden Markov model (Vogel et al., 1996). These models are still in wide-spread use today, albeit only as a preprocessing step for inferring word level alignments from sentence-aligned parallel corpora. They combine a number of factors, including distortion and fertility, which have been shown to improve word-alignment and translation performance over simpler models. Our approach is similar to these works, as we also develop a word-based model, and explicitly consider similar translation decisions, alignment jumps and fertility. We extend these works in two important respects: 1) while they assume a simple parameterisation by making"
P13-1033,N09-1036,0,0.0335699,"each iteration. In this way each process uses slightly “out-of-date” counts, but can process the data independently of the other processes. We found that this approximation improved the runtime significantly with no noticeable effect on accuracy. posterior probability p(ai = j|t−i , o−i ) ∝ the discount parameter, we employ a uniform Beta distribution ax ∼ Beta(1, 1) while for the strength parameter, we employ a vague Gamma distribution bx ∼ Gamma(10, 0.1). All restaurants in the same level share the same hyper-prior and the hyper-parameters for all levels are resampled using slice sampling (Johnson and Goldwater, 2009) every 10 iterations. 5 Experiments In principle our model could be directly used as a MT decoder or as a feature in a decoder. However in this paper we limit our focus to inducing word alignments, i.e., by using the model to infer alignments which are then used in a standard phrasebased translation pipeline. We leave full decoding for later work, which we anticipate would further improve performance by exploiting gapping phrases and other phenomena that implicitly form part of our model but are not represented in the phrase-based decoder. Decoding under our model would be straight-forward in"
P13-1033,N03-1017,0,0.648272,"s word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1 Introduction Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based (Koehn et al., 2003) and syntax-based approaches (Chiang, 2005; Galley et al., 2006; Liu et al., 2006). These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on con333 Proceedings of the"
P13-1033,P07-2045,0,0.0194756,"for the first 1000 iterations, after which we ran a further 500 iterations selecting every 50th sample. A phrase table was constructed using these 10 sets of multiple alignments after combining each pair of directional alignments using the grow-diag-final heuristic. Using multiple samples in this way constitutes Monte Carlo averaging, which provides a better estimate of uncertainty cf. using a single sample.5 The alignment used for the baseline results was produced by combining bidirectional GIZA++ alignments using the grow-diag-final heuristic. We used the Moses machine translation decoder (Koehn et al., 2007), using the default features and decoding settings. We compared the performance of Moses using the alignment produced by our model and the baseline alignment, evaluating translation quality using BLEU (Papineni et al., 2002) with case-insensitive n-gram matching with n = 4. We used minimum error rate training (Och, 2003) to tune the feature weights to maximise the BLEU score on the development set. 5.2 Dev 45.78 49.13 49.68 51.32 for the test set we use the IWSLT 2005 test set. The language model is a 3-gram language model trained using the SRILM toolkit (Stolcke, 2002) on the English side of"
P13-1033,N12-1005,0,0.0632862,"ich expressiveness but can still generalize well to unseen data. More recently, a number of authors have proposed Markov models for machine translation. Vaswani et al. (2011) propose a rule Markov model for a tree-to-string model which models correlations between pairs of mininal rules, and use Kneser-Ney smoothing to alleviate the problems of data sparsity. Similarly, Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, which they use as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Also relevant is Durrani et al. (2011), who present a sequence model of translation including reordering. 3 Model Given a source sentence, our model infers a latent derivation which produces a target translation and meanwhile gives a word alignment between the source and the target. We consider a process in which the target string is generated using a left-to-right order, similar to the decoding strategy used by phrase-based machine translation systems (Koehn et al., 2003). During this proc"
P13-1033,P06-1077,0,0.0277194,"decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU. 1 Introduction Recent years have witnessed burgeoning development of statistical machine translation research, notably phrase-based (Koehn et al., 2003) and syntax-based approaches (Chiang, 2005; Galley et al., 2006; Liu et al., 2006). These approaches model sentence translation as a sequence of simple translation decisions, such as the application of a phrase translation in phrase-based methods or a grammar rule in syntax-based approaches. In order to simplify modelling, most MT models make an independence assumption, stating that the translation decisions in a derivation are independent of one another. This conflicts with the intuition behind phrase-based MT, namely that translation decisions should be dependent on con333 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 333–3"
P13-1033,J03-1002,0,0.0444765,"Missing"
P17-1125,C08-1048,0,0.145537,"he Tang Dynasty, and so are often called 1 Corresponding author: Dong Wang; RM 1-303, FIT BLDG, Tsinghua University, Beijing (100084), P.R. China. Table 1: An example of a 5-char quatrain. The tonal pattern is shown at the end of each line, where ’P’ indicates a level tone, ’Z’ indicates a downward tone, and ’*’ indicates the tone can be either. The translation is from (Tang, 2005). In this paper we are interested in machine poetry generation. Several approaches have been studied by researchers. For example, rule-based methods (Zhou et al., 2010), statistical machine translation (SMT) models (Jiang and Zhou, 2008; He et al., 2012) and neural models (Zhang and Lapata, 2014; Wang et al., 2016a,c). Compared to previ1364 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1364–1373 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1125 ous approaches (e.g., rule-based or SMT), the neural model approach tends to generate more fluent poems and some generations are so natural that even professional poets can not tell they are the work of machines (Wang et al., 2016a). In spite of these promi"
P17-1125,W09-2005,0,0.0583013,"narios where adding a memory may contribute: the first scenario involves a well trained neural model where we aim to promote innovation by adding a memory, the second scenario involves an over-fitted neural model where we hope the memory can regularize the innovation, and in the third scenario, the memory is used to encourage generation of poems of different styles. 2 Related Work A multitude of methods have been proposed for automatic poem generation. The first approach is based on rules and/or templates. For example, phrase search (Tosa et al., 2009; Wu et al., 2009), word association norm (Netzer et al., 2009), template search (Oliveira, 2012), genetic search (Zhou et al., 2010), text summarization (Yan et al., 2013). Another approach involves various SMT methods, e.g., (Jiang and Zhou, 2008; He et al., 2012). A disadvantage shared by the above methods is that they are based on the surface forms of words or characters, having no deep understanding of the meaning of a poem. More recently, neural models have been the subject of much attention. A clear advantage of the neural-based methods is that they can ‘discover’ the meaning of words or characters, and can therefore more deeply understand the mean"
P17-1125,C16-1100,0,0.226277,"03, FIT BLDG, Tsinghua University, Beijing (100084), P.R. China. Table 1: An example of a 5-char quatrain. The tonal pattern is shown at the end of each line, where ’P’ indicates a level tone, ’Z’ indicates a downward tone, and ’*’ indicates the tone can be either. The translation is from (Tang, 2005). In this paper we are interested in machine poetry generation. Several approaches have been studied by researchers. For example, rule-based methods (Zhou et al., 2010), statistical machine translation (SMT) models (Jiang and Zhou, 2008; He et al., 2012) and neural models (Zhang and Lapata, 2014; Wang et al., 2016a,c). Compared to previ1364 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1364–1373 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1125 ous approaches (e.g., rule-based or SMT), the neural model approach tends to generate more fluent poems and some generations are so natural that even professional poets can not tell they are the work of machines (Wang et al., 2016a). In spite of these promising results, neural models suffer from a particular problem in poem generation"
P17-1125,D14-1074,0,0.746578,"uthor: Dong Wang; RM 1-303, FIT BLDG, Tsinghua University, Beijing (100084), P.R. China. Table 1: An example of a 5-char quatrain. The tonal pattern is shown at the end of each line, where ’P’ indicates a level tone, ’Z’ indicates a downward tone, and ’*’ indicates the tone can be either. The translation is from (Tang, 2005). In this paper we are interested in machine poetry generation. Several approaches have been studied by researchers. For example, rule-based methods (Zhou et al., 2010), statistical machine translation (SMT) models (Jiang and Zhou, 2008; He et al., 2012) and neural models (Zhang and Lapata, 2014; Wang et al., 2016a,c). Compared to previ1364 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1364–1373 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1125 ous approaches (e.g., rule-based or SMT), the neural model approach tends to generate more fluent poems and some generations are so natural that even professional poets can not tell they are the work of machines (Wang et al., 2016a). In spite of these promising results, neural models suffer from a particular problem"
P18-1138,E17-2029,0,0.0229563,"es of sequence-to-sequence architecture (Cho et al., 2014; Sutskever et al., 2014) motivated investigation in dialogue systems that can effectively learn to generate a response sequence given the previous utterance sequence (Shang et al., 2015; Sordoni et al., 2015b; Vinyals and Le, 2015). The model is trained to minimize the negative log-likelihood of the training data. Despite the current progress, the lack of response diversity is a notorious problem, where the model inherently tends to generate short, general responses in spite of different inputs. Li et al. (2016a); Serban et al. (2017); Cao and Clark (2017) suggested that theses boring responses are common in training data and shorter responses are more likely to be given a higher likelihood. To tackle the problem, Li et al. (2016a) introduced a maximum mutual information training objective. Serban et al. (2017), Cao and Clark (2017) and Chen et al. (2018) used latent variables to introduce stochasticity to enhance the response diversity. Vijayakumar et al. (2016),Shao et al. (2017) and Li et al. (2016b) recognized that the greedy search decoding process, especially beam-search with a wide beam size, leads the short responses possess higher like"
P18-1138,D14-1179,0,0.0292992,"Missing"
P18-1138,W17-5506,0,0.153075,"rst analyzed, followed by retrieving related facts from knowledge bases (KBs), and finally the answers are generated.The facts are usually presented in the form of “subject-relationobject” triplets, where the subject and object are entities. With the aid of knowledge triplets, neural generative question answering systems are capable of answering facts related inquiries (Yin et al., 2016; Zhu et al., 2017; He et al., 2017a), WH questions in particular, like “who is Yao Ming’s wife ?”. Although answering enquiries is essential for dialogue systems, especially for task-oriented dialogue systems (Eric et al., 2017), it is still far behind a natural knowledge grounded dialogue system, which should be able to understand the facts involved in current dialogue session (socalled facts matching), as well as diffuse them to other similar entities for knowledge-based chitchats (i.e. entity diffusion): 1) facts matching: in dialogue systems, matching utterances to exact facts is much harder than explicit factoid inquiries answering. Though some utterances are facts related inquiries, whose subjects and relations can be easily recognized, for some utterances, the subjects and relations are elusive, which leads th"
P18-1138,P17-1019,0,0.448474,"bases ( e.g., the freebase (Google, 2013) and DBpedia (Lehmann et al., 2017) ) are leveraged. A related application of knowledge bases is question answering, where the given questions are first analyzed, followed by retrieving related facts from knowledge bases (KBs), and finally the answers are generated.The facts are usually presented in the form of “subject-relationobject” triplets, where the subject and object are entities. With the aid of knowledge triplets, neural generative question answering systems are capable of answering facts related inquiries (Yin et al., 2016; Zhu et al., 2017; He et al., 2017a), WH questions in particular, like “who is Yao Ming’s wife ?”. Although answering enquiries is essential for dialogue systems, especially for task-oriented dialogue systems (Eric et al., 2017), it is still far behind a natural knowledge grounded dialogue system, which should be able to understand the facts involved in current dialogue session (socalled facts matching), as well as diffuse them to other similar entities for knowledge-based chitchats (i.e. entity diffusion): 1) facts matching: in dialogue systems, matching utterances to exact facts is much harder than explicit factoid inquiries"
P18-1138,W18-2605,0,0.0371289,"Missing"
P18-1138,N16-1014,0,0.0685156,"abulary cases. 4 Related Work The successes of sequence-to-sequence architecture (Cho et al., 2014; Sutskever et al., 2014) motivated investigation in dialogue systems that can effectively learn to generate a response sequence given the previous utterance sequence (Shang et al., 2015; Sordoni et al., 2015b; Vinyals and Le, 2015). The model is trained to minimize the negative log-likelihood of the training data. Despite the current progress, the lack of response diversity is a notorious problem, where the model inherently tends to generate short, general responses in spite of different inputs. Li et al. (2016a); Serban et al. (2017); Cao and Clark (2017) suggested that theses boring responses are common in training data and shorter responses are more likely to be given a higher likelihood. To tackle the problem, Li et al. (2016a) introduced a maximum mutual information training objective. Serban et al. (2017), Cao and Clark (2017) and Chen et al. (2018) used latent variables to introduce stochasticity to enhance the response diversity. Vijayakumar et al. (2016),Shao et al. (2017) and Li et al. (2016b) recognized that the greedy search decoding process, especially beam-search with a wide beam size,"
P18-1138,P15-1152,0,0.222259,"Missing"
P18-1138,N15-1020,0,0.0921078,"Missing"
P18-1138,W16-0106,0,0.23812,"puters, different kinds of knowledge bases ( e.g., the freebase (Google, 2013) and DBpedia (Lehmann et al., 2017) ) are leveraged. A related application of knowledge bases is question answering, where the given questions are first analyzed, followed by retrieving related facts from knowledge bases (KBs), and finally the answers are generated.The facts are usually presented in the form of “subject-relationobject” triplets, where the subject and object are entities. With the aid of knowledge triplets, neural generative question answering systems are capable of answering facts related inquiries (Yin et al., 2016; Zhu et al., 2017; He et al., 2017a), WH questions in particular, like “who is Yao Ming’s wife ?”. Although answering enquiries is essential for dialogue systems, especially for task-oriented dialogue systems (Eric et al., 2017), it is still far behind a natural knowledge grounded dialogue system, which should be able to understand the facts involved in current dialogue session (socalled facts matching), as well as diffuse them to other similar entities for knowledge-based chitchats (i.e. entity diffusion): 1) facts matching: in dialogue systems, matching utterances to exact facts is much har"
P19-1002,P17-4012,0,0.0491261,"Missing"
P19-1002,N16-1014,0,0.0876363,"coder) to improve context coherence and knowledge correctness. Our empirical study on a real-world Document Grounded Dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance. 1 Introduction Past few years have witnessed the rapid development of dialogue systems. Based on the sequenceto-sequence framework (Sutskever et al., 2014), most models are trained in an end-to-end manner with large corpora of human-to-human dialogues and have obtained impressive success (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016; Serban et al., 2016). While there is still a long way for reaching the ultimate goal of dialogue systems, which is to be able to talk like humans. And one of the essential intelligence to achieve this goal is the ability to make use of knowledge. ∗ Fandong Meng is the corresponding author of the paper. This work was done when Zekang Li was interning at Pattern Recognition Center, WeChat AI, Tencent. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 12–21 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics"
P19-1002,P18-1138,1,0.924905,"Huazhong University of Science and Technology ‡ Pattern Recognition Center, WeChat AI, Tencent Inc, China ♦ Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences ♠ School of Computer Science and Engineering, Northeastern University, China zekangli97@gmail.com, {chengniu,fandongmeng,jiezhou}@tencent.com fengyang@ict.ac.cn, qianli@stumail.neu.edu.cn Abstract There are several works on dialogue systems exploiting knowledge. The Mem2Seq (Madotto et al., 2018) incorporates structured knowledge into the end-to-end task-oriented dialogue. Liu et al. (2018) introduces factmatching and knowledge-diffusion to generate meaningful, diverse and natural responses using structured knowledge triplets. Ghazvininejad et al. (2018), Parthasarathi and Pineau (2018), Yavuz et al. (2018), Dinan et al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing precise facts. Documents as a knowledge source provide a wide spectrum of knowledge, including b"
P19-1002,D15-1166,0,0.0293207,"using Self-Attentive Encoder, u ˆ (2) is the output words after the second-pass decoder. Training In contrast to the original Deliberation Network (Xia et al., 2017), where they propose a complex joint learning framework using Monte Carlo Method, we minimize the following loss as Xiong et al. (2018) do: Lmle = Lmle1 + Lmle2 (k+1) (29) log P (ˆ u(2)i ) Experiments 3.2 Baselines We compare our proposed model with the following state-of-the-art baselines: Models not using document knowledge: Seq2Seq: A simple encoder-decoder model (Shang et al., 2015; Vinyals and Le, 2015) with global attention (Luong et al., 2015). We concatenate utterances context to a long sentence as input. HRED: A hierarchical encoder-decoder model (Serban et al., 2016), which is composed of a word-level LSTM for each sentence and a sentence-level LSTM connecting utterances. Transformer: The state-of-the-art NMT model based on multi-head attention (Vaswani et al., 2017). We concatenate utterances context to a long sentence as its input. Models using document knowledge: Seq2Seq (+knowledge) and HRED (+knowledge) are based on Seq2Seq and HRED respectively. They both concatenate document knowledge representation and last decoding outp"
P19-1002,P17-1171,0,0.0317822,"arathi and Pineau (2018), Yavuz et al. (2018), Dinan et al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing precise facts. Documents as a knowledge source provide a wide spectrum of knowledge, including but not limited to factoid, event updates, subjective opinion, etc. Recently, intensive research has been applied on using documents as knowledge sources for QuestionAnswering (Chen et al., 2017; Huang et al., 2018; Yu et al., 2018; Rajpurkar et al., 2018; Reddy et al., 2018). The Document Grounded Conversation is a task to generate natural dialogue responses when chatting about the content of a specific document. This task requires to integrate document knowledge with the multi-turn dialogue history. Different from previous knowledge grounded dialogue systems, Document Grounded Conversations utilize documents as the knowledge source, and hence are able to employ a wide spectrum of knowledge. And the Document Grounded Conversations is also different from document QA since the context"
P19-1002,P18-1136,0,0.0323799,"Yang Feng♦ , Qian Li♠ , Jie Zhou‡ † Dian Group, School of Electronic Information and Communications Huazhong University of Science and Technology ‡ Pattern Recognition Center, WeChat AI, Tencent Inc, China ♦ Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences ♠ School of Computer Science and Engineering, Northeastern University, China zekangli97@gmail.com, {chengniu,fandongmeng,jiezhou}@tencent.com fengyang@ict.ac.cn, qianli@stumail.neu.edu.cn Abstract There are several works on dialogue systems exploiting knowledge. The Mem2Seq (Madotto et al., 2018) incorporates structured knowledge into the end-to-end task-oriented dialogue. Liu et al. (2018) introduces factmatching and knowledge-diffusion to generate meaningful, diverse and natural responses using structured knowledge triplets. Ghazvininejad et al. (2018), Parthasarathi and Pineau (2018), Yavuz et al. (2018), Dinan et al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing"
P19-1002,P02-1040,0,0.106854,"liberation decoder. Knowledge-Attention Transformer (KAT): As shown in Figure 2 (d), the encoder of this model is a simplified version of Incremental Transformer Encoder (ITE), which doesn’t have context-attention sub-layer. We concatenate utterances context to a long sentence as its input. The decoder of the model is a simplified Context-Knowledge-Attention Decoder (CKAD). It doesn’t have context-attention sub-layer either. This setup is to test how effective the context has been exploited in the full model. 3.3 3.4 Evaluation Metrics Automatic Evaluation: We adopt perplexity (PPL) and BLEU (Papineni et al., 2002) to automatically evaluate the response generation performance. Models are evaluated using perplexity of the gold response as described in (Dinan et al., 2018). Lower perplexity indicates better performance. BLEU measures n-gram overlap between a generated response and a gold response. However, since there is only one reference for each response and there may exist multiple feasible responses, BLEU scores are extremely low. We compute BLEU score by the multi-bleu.perl3 Manual Evaluation: Manual evaluations are essential for dialogue generation. We randomly sampled 30 conversations containing 6"
P19-1002,D18-1073,0,0.251661,"chnology, Chinese Academy of Sciences ♠ School of Computer Science and Engineering, Northeastern University, China zekangli97@gmail.com, {chengniu,fandongmeng,jiezhou}@tencent.com fengyang@ict.ac.cn, qianli@stumail.neu.edu.cn Abstract There are several works on dialogue systems exploiting knowledge. The Mem2Seq (Madotto et al., 2018) incorporates structured knowledge into the end-to-end task-oriented dialogue. Liu et al. (2018) introduces factmatching and knowledge-diffusion to generate meaningful, diverse and natural responses using structured knowledge triplets. Ghazvininejad et al. (2018), Parthasarathi and Pineau (2018), Yavuz et al. (2018), Dinan et al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing precise facts. Documents as a knowledge source provide a wide spectrum of knowledge, including but not limited to factoid, event updates, subjective opinion, etc. Recently, intensive research has been applied on using documents as knowledge sources for QuestionAnswering (Chen et al., 2017; Huang"
P19-1002,P18-2124,0,0.0369028,"al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing precise facts. Documents as a knowledge source provide a wide spectrum of knowledge, including but not limited to factoid, event updates, subjective opinion, etc. Recently, intensive research has been applied on using documents as knowledge sources for QuestionAnswering (Chen et al., 2017; Huang et al., 2018; Yu et al., 2018; Rajpurkar et al., 2018; Reddy et al., 2018). The Document Grounded Conversation is a task to generate natural dialogue responses when chatting about the content of a specific document. This task requires to integrate document knowledge with the multi-turn dialogue history. Different from previous knowledge grounded dialogue systems, Document Grounded Conversations utilize documents as the knowledge source, and hence are able to employ a wide spectrum of knowledge. And the Document Grounded Conversations is also different from document QA since the contextual consistent conversation response should be generated. To"
P19-1002,P15-1152,0,0.0751858,"Missing"
P19-1002,D18-1049,0,0.0387442,"Missing"
P19-1002,D18-1076,0,0.190263,"ess of the responses. The first-pass decoder focuses on contextual coherence, while the second-pass decoder refines the result of the firstpass decoder by consulting the relevant document knowledge, and hence increases the knowledge relevance and correctness. This is motivated by human cognition process. In real-world human conversations, people usually first make a draft on how to respond the previous utterance, and then consummate the answer or even raise questions by consulting background knowledge. We test the effectiveness of our proposed model on Document Grounded Conversations Dataset (Zhou et al., 2018). Experiment results show that our model is capable of generating responses of more context coherence and knowledge relevance. Sometimes document knowledge is even well used to guide the following conversations. Both automatic and manual evaluations show that our model substantially outperforms the competitive baselines. Our contributions are as follows: Incremental Transformer Encoder Document k-2 Self-Attentive Encoder Document k-1 Incremental Transformer Utterance k-1 Self-Attentive Encoder Document k Utterance k Incremental Transformer Self-Attentive Encoder Self-Attentive Encoder First-pa"
P19-1288,W17-4123,0,0.128847,"ty. Recently, the Transformer model (Vaswani et al., 2017) further enhances the translation performance on multiple language pairs, while suffering from the slow decoding procedure, which reJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author stricts its application scenarios. The slow decoding problem of the Transformer model is caused by its autoregressive nature, which means that the target sentence is generated word by word according to the source sentence representations and the target translation history. Non-autoregressive Transformer model (Gu et al., 2017a) is proposed to accelerate the decoding process, which can simultaneously generate target words by discarding the autoregressive mechanism. Since the generation of target words is independent, NAT models utilize alternative information such as encoder inputs (Gu et al., 2017a), translation results from other systems (Lee et al., 2018; Guo et al., 2018) and latent variables (Kaiser et al., 2018) as decoder inputs. Without considering the target translation history, NAT models are weak to exploit the target words collocation knowledge and tend to generate repeated target words at adjacent time"
P19-1288,D17-1210,0,0.0371484,"Missing"
P19-1288,P84-1044,0,0.302576,"Missing"
P19-1288,D16-1139,0,0.285399,"of predicted token fed to the fusion layer. 4 Related Work Gu et al. (2017a) introduced the nonautoregressive Transformer model to accelerate the translation. Lee et al. (2018) proposed a nonautoregressive sequence model based on iterative refinement, where the outputs of the decoder are fed back as inputs in the next iteration. Guo et al. (2018) proposed to enhance the decoder inputs with phrase-table lookup and embedding mapping. Kaiser et al. (2018) used a sequence of autoregressively generated discrete latent variables as inputs of the decoder. Knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016) is a method for training a smaller and faster student network to perform better by learning from a teacher network, which is crucial in NAT models. Gu et al. (2017a) applied Sequence-level knowledge distillation to eliminate the multimodality in the training corpus. Li et al. (2018) further proposed to improve non-autoregressive models through distilling knowledge from intermediary hidden states and attention weights of autoregressive models. Apart from non-autoregressive translation, there are works toward speeding up the translation from other perspectives. Wang et al. (2018) proposed the s"
P19-1288,P02-1040,0,0.103733,"th Annual Meeting of the Association for Computational Linguistics, pages 3013–3024 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics In this paper, we present two approaches to retrieve the target sequential information for NAT models to enhance their translation ability and meanwhile preserve the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. We leverage the sequence-level objectives (e.g., BLEU (Papineni et al., 2002), GLEU (Wu et al., 2017), TER (Snover et al., 2006)) instead of the cross-entropy objective to encourage NAT model to generate high quality sentences rather than the correct token for each position. Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. The bottom layers of the FS-decoder run in parallel to keep the decoding speed and the top layer of the FS-decoder can exploit target sequential information to guide the target words generation procedure. We conduct experiments on three machine transla"
P19-1288,P16-1162,0,0.0899481,"et al., 2017). Importance sampling estimates the properties of a particular distribution through sampling on a different proposal distribution. Complementary sum sampling reducdes the variance through suming over the important subset and estimating the rest via sampling. 5 5.1 Experiments Settings Dataset. We conduct experiments on three translation tasks3 : IWSLT16 En→De (196k pairs), WMT14 En↔De (4.5M pairs) and WMT16 En↔Ro (610k pairs). We use the preprocessed datasets released by Lee et al. (2018), where all sentences are tokenized and segmented into subword units using the BPE algorithm (Sennrich et al., 2016). For all tasks, source and target languages share the vocabulary with size 40k. For WMT14 En-De, we employ newstest-2013 and newstest-2014 as development and test sets. For WMT16 En-Ro, we take newsdev-2016 and newstest-2016 as development and test sets. For IWSLT16 En-De, we use the test2013 for validation. Baselines. We take the Transformer model (Vaswani et al., 2017) as the autoregressive baseline. The non-autoregressive model based on iterative refinement (Lee et al., 2018) is the nonautoregressive baseline, and we set the number of iterations to 2. Pre-train. To evaluate the sequence-le"
P19-1288,D18-1510,1,0.827268,"Missing"
P19-1288,P16-1159,0,0.121143,"search is fed into the decoder to guide the generation of the next word. The prominent feature of the autoregressive model is that it requires the target side historical information in the decoding procedure. Therefore target words are generated in the one-by-one style. Due to the autoregressive property, the decoding speed is limited, which restricts the application of the autoregressive model. 2.2 Reinforcement learning techniques (Sutton et al., 2000; Ng et al., 1999; Sutton, 1984) have been widely applied to improve the performance of the autoregressive NMT with sequence-level objectives (Shen et al., 2016; Ranzato et al., 2015; Bahdanau et al., 2016). As sequence-level objectives are usually non-differentiable, the loss function is defined as the negative expected reward: Lθ = − θ = arg max{L(θ)} θ L(θ) = M X T X m=1 t=1 m log(p(ytm |y<t , X m , θ)), (2) X p(Y|X, θ) · r(Y), (3) Y=y1:T where Y = y1:T denotes possible sequences generated by the model, and r(Y) is the corresponding reward such as BLEU, GLEU and TER for generating sequence Y. Enumerating all the possible target sequences is impossible due to the exponential search space, and REINFORCE (Williams, 1992) gives an elegant way to estim"
P19-1288,2006.amta-papers.25,0,0.0258753,"l Linguistics, pages 3013–3024 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics In this paper, we present two approaches to retrieve the target sequential information for NAT models to enhance their translation ability and meanwhile preserve the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. We leverage the sequence-level objectives (e.g., BLEU (Papineni et al., 2002), GLEU (Wu et al., 2017), TER (Snover et al., 2006)) instead of the cross-entropy objective to encourage NAT model to generate high quality sentences rather than the correct token for each position. Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. The bottom layers of the FS-decoder run in parallel to keep the decoding speed and the top layer of the FS-decoder can exploit target sequential information to guide the target words generation procedure. We conduct experiments on three machine translation tasks (IWSLT16 En→De, WMT14 En↔De, WMT16 En→Ro"
P19-1288,D18-1149,0,0.507576,"ding problem of the Transformer model is caused by its autoregressive nature, which means that the target sentence is generated word by word according to the source sentence representations and the target translation history. Non-autoregressive Transformer model (Gu et al., 2017a) is proposed to accelerate the decoding process, which can simultaneously generate target words by discarding the autoregressive mechanism. Since the generation of target words is independent, NAT models utilize alternative information such as encoder inputs (Gu et al., 2017a), translation results from other systems (Lee et al., 2018; Guo et al., 2018) and latent variables (Kaiser et al., 2018) as decoder inputs. Without considering the target translation history, NAT models are weak to exploit the target words collocation knowledge and tend to generate repeated target words at adjacent time steps (Wang et al., 2019). Over-translation and undertranslation problems are aggravated and often occur due to the above reasons. Table 1 shows an inferior translation example generated by a NAT model. Compared to the autoregressive Transformer, NAT models achieve significant speedup while suffering from a large gap in translation qu"
P19-1288,P18-2053,0,0.0697674,"neural machine translation without damaging the translation quality. Sequence-level training techniques have been widely explored in autoregressive neural machine translation, where most works (Ranzato et al., 2015; Shen et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) relied on reinforcement learning (Williams, 1992; Sutton et al., 2000) to build the gradient estimator. Recently, techniques for sequence-level training with continuous objectives have been explored, including deterministic policy gradient algorithms (Gu et al., 2017b), bag-of-words objective (Ma et al., 2018) and probabilistic n-gram matching (Shao et al., 2018). However, to the best of our knowledge, sequence-level training has not been applied to non-autoregressive models yet. The methods of variance reduction through focusing on the important parts of the distribution include importance sampling (Bengio et al., 2003; Glynn and Iglehart, 1989) and complementary sum sampling (Botev et al., 2017). Importance sampling estimates the properties of a particular distribution through sampling on a different proposal distribution. Complementary sum sampling reducdes the variance through suming over the i"
P19-1288,D18-1044,0,0.0803608,"et al., 2015; Kim and Rush, 2016) is a method for training a smaller and faster student network to perform better by learning from a teacher network, which is crucial in NAT models. Gu et al. (2017a) applied Sequence-level knowledge distillation to eliminate the multimodality in the training corpus. Li et al. (2018) further proposed to improve non-autoregressive models through distilling knowledge from intermediary hidden states and attention weights of autoregressive models. Apart from non-autoregressive translation, there are works toward speeding up the translation from other perspectives. Wang et al. (2018) proposed the semi-autoregressive Transformer that generates a group of words in parallel at each time step. Press and Smith (2018) proposed the eager translation model that does not use the attention mechanism and has low latency. Zhang et al. (2018a) proposed the average attention network to accelerate decoding, which achieves significant speedup over the uncached Transformer. Zhang et al. (2018b) proposed cube pruning to speedup the beam search for neural machine translation without damaging the translation quality. Sequence-level training techniques have been widely explored in autoregress"
P19-1288,D18-1397,0,0.0355153,"e Y from the probability distribution and estimate the gradient with the gradient of log-probability weighted by the reward r(Y): ∇θ Lθ = t=1 where θ is a set of model parameters and y<t = {y1 , · · · , yt−1 } is the translation history. Given the training set D = {XM , YM } with M sentence pairs, the training objective is to maximize the loglikelihood of the training data as: Sequence-Level Training for Autoregressive NMT T X (4) ∇θ log(p(yt |y<t , X, θ)) · r(Y)]. − E[ Y t=1 Current reinforcement learning (RL) methods are designed for autoregressive models. Moreover, previous investigations (Wu et al., 2018; Weaver and Tao, 2013) show that the RL-based training procedure is unstable due to its high variance of gradient estimation. 3014 2.3 Non-Autoregressive Neural Machine Translation Non-autoregressive neural machine translation (Gu et al., 2017a) is proposed to accelerate the decoding process, which can simultaneously generate target words by discarding the autoregressive mechanism. The translation probability from X to Y is modeled as follows: P (Y |X, θ) = T Y p(yt |X, θ). (5) t=1 Given the training set D = {XM , YM } with M sentence pairs, the training objective is to maximize the log-likel"
P19-1288,1983.tc-1.13,0,0.575939,"Missing"
P19-1288,N18-1122,0,0.0997661,"Missing"
P19-1288,P18-1166,0,0.0194701,"liminate the multimodality in the training corpus. Li et al. (2018) further proposed to improve non-autoregressive models through distilling knowledge from intermediary hidden states and attention weights of autoregressive models. Apart from non-autoregressive translation, there are works toward speeding up the translation from other perspectives. Wang et al. (2018) proposed the semi-autoregressive Transformer that generates a group of words in parallel at each time step. Press and Smith (2018) proposed the eager translation model that does not use the attention mechanism and has low latency. Zhang et al. (2018a) proposed the average attention network to accelerate decoding, which achieves significant speedup over the uncached Transformer. Zhang et al. (2018b) proposed cube pruning to speedup the beam search for neural machine translation without damaging the translation quality. Sequence-level training techniques have been widely explored in autoregressive neural machine translation, where most works (Ranzato et al., 2015; Shen et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) relied on reinforcement learning (Williams, 1992; Sutton et al., 2000) to build the gradi"
P19-1288,D18-1460,1,0.730675,"liminate the multimodality in the training corpus. Li et al. (2018) further proposed to improve non-autoregressive models through distilling knowledge from intermediary hidden states and attention weights of autoregressive models. Apart from non-autoregressive translation, there are works toward speeding up the translation from other perspectives. Wang et al. (2018) proposed the semi-autoregressive Transformer that generates a group of words in parallel at each time step. Press and Smith (2018) proposed the eager translation model that does not use the attention mechanism and has low latency. Zhang et al. (2018a) proposed the average attention network to accelerate decoding, which achieves significant speedup over the uncached Transformer. Zhang et al. (2018b) proposed cube pruning to speedup the beam search for neural machine translation without damaging the translation quality. Sequence-level training techniques have been widely explored in autoregressive neural machine translation, where most works (Ranzato et al., 2015; Shen et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) relied on reinforcement learning (Williams, 1992; Sutton et al., 2000) to build the gradi"
P19-1426,W04-1013,0,0.0135768,"d select an oracle word oracle to simulate the context word. The oracle yj−1 word should be a word similar to the ground truth or a synonym. Using different strategies will prooracle . One option is duce a different oracle word yj−1 that word-level greedy search could be employed to output the oracle word of each step, which is called Word-level Oracle (called WO). Besides, we can further optimize the oracle by enlarging the search space with beam search and then reranking the candidate translations with a sentencelevel metric, e.g. BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), ROUGE (Lin, 2004), etc, the selected translation is called oracle sentence, the words in the translation are Sentence-level Oracle (denoted as SO). Word-Level Oracle For the {j−1}-th decoding step, the direct way to select the word-level oracle is to pick the word with the highest probability from the word distribution Pj−1 drawn by Equation (9), which is shown in Figure 2. The predicted score in oj−1 is the value before the softmax operation. In practice, we can acquire more robust word-level oracles by introducing the Gumbel-Max technique (Gumbel, 1954; Maddison et al., 2014), which provides a simple and eff"
P19-1426,P02-1040,0,0.104092,"o the ground truth word yj−1 predict yj , thus, we could select an oracle word oracle to simulate the context word. The oracle yj−1 word should be a word similar to the ground truth or a synonym. Using different strategies will prooracle . One option is duce a different oracle word yj−1 that word-level greedy search could be employed to output the oracle word of each step, which is called Word-level Oracle (called WO). Besides, we can further optimize the oracle by enlarging the search space with beam search and then reranking the candidate translations with a sentencelevel metric, e.g. BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), ROUGE (Lin, 2004), etc, the selected translation is called oracle sentence, the words in the translation are Sentence-level Oracle (denoted as SO). Word-Level Oracle For the {j−1}-th decoding step, the direct way to select the word-level oracle is to pick the word with the highest probability from the word distribution Pj−1 drawn by Equation (9), which is shown in Figure 2. The predicted score in oj−1 is the value before the softmax operation. In practice, we can acquire more robust word-level oracles by introducing the Gumbel-Max technique (Gumbel, 1954; Maddison et"
P19-1426,P16-1162,0,0.357348,"Missing"
P19-1426,D18-1510,1,0.715141,"Missing"
P19-1426,P16-1159,0,0.340677,"ntence-level oracle to relieve the overcorrection problem and neither the noise perturbations on the predicted distribution. Another direction of attempts is the sentencelevel training with the thinking that the sentencelevel metric, e.g., BLEU, brings a certain degree of flexibility for generation and hence is more robust to mitigate the exposure bias problem. To avoid the problem of exposure bias, Ranzato et al. (2015) presented a novel algorithm Mixed Incremental Cross-Entropy Reinforce (MIXER) for sequence-level training, which directly optimized the sentence-level BLEU used at inference. Shen et al. (2016) introduced the Minimum Risk Training (MRT) into the end-to-end NMT model, which optimized model parameters by minimizing directly the expected loss with respect to arbitrary evaluation metrics, e.g., sentence-level BLEU. Shao et al. (2018) proposed to eliminate the exposure bias through a probabilistic n-gram matching objective, which trains NMT NMT under the greedy decoding strategy. 5 Experiments 5.1 For Zh→En, the training dataset consists of 1.25M sentence pairs extracted from LDC corpora1 . We choose the NIST 2002 (MT02) dataset as the validation set, which has 878 sentences, and the NIS"
P19-1426,P16-1008,0,0.105283,"odel parameters with batch size setting to 80. Moreover, the learning rate is adjusted by adadelta optimizer (Zeiler, 2012) with ρ=0.95 and =1e-6. Dropout is applied on the output layer with dropout rate being 0.5. For Transformer model, we train base model with 1 We carry out experiments on the NIST Chinese→English (Zh→En) and the WMT’14 English→German (En→De) translation tasks. Settings These sentence pairs are mainly extracted from LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06 2 http://www.statmt.org/wmt14/ translation-task.html 4338 Systems Tu et al. (2016) Shen et al. (2016) Zhang et al. (2017) this work Architecture MT03 MT04 MT05 Existing end-to-end NMT systems Coverage 33.69 38.05 35.01 MRT 37.41 39.87 37.45 Distortion 37.93 40.40 36.81 Our end-to-end NMT systems RNNsearch 37.93 40.53 36.65 + SS-NMT 38.82 41.68 37.28 + MIXER 38.70 40.81 37.59 + OR-NMT 40.40‡†? 42.63‡†? 38.87‡†? Transformer 46.89 47.88 47.40 + word oracle 47.42 48.34 47.89 + sentence oracle 48.31∗ 49.40∗ 48.72∗ MT06 Average 34.83 36.80 35.77 35.40 37.88 37.73 35.80 37.98 38.38 38.44‡ 46.66 47.34 48.45∗ 37.73 38.94 38.87 40.09 47.21 47.75 48.72 Table 1: Case-insensitive BLEU s"
P19-1426,1983.tc-1.13,0,0.438724,"Missing"
P19-1426,P17-1140,1,0.844345,"nhance the overcorrection recovery capacity. For the sentencelevel oracle selection, we set the beam size to be 3, set τ =0.5 in Equation (11) and µ=12 for the decay function in Equation (15). OR-NMT is the abbreviation of NMT with Overcorrection Recovery. 3 https://github.com/pytorch/fairseq Results on Zh→En Translation Results on the RNNsearch As shown in Table 1, Tu et al. (2016) propose to model coverage in RNN-based NMT to improve the adequacy of translations. Shen et al. (2016) propose minimum risk training (MRT) for NMT to directly optimize model parameters with respect to BLEU scores. Zhang et al. (2017) model distortion to enhance the attention model. Compared with them, our baseline system RNNsearch 1) outperforms previous shallow RNN-based NMT system equipped with the coverage model (Tu et al., 2016); and 2) achieves competitive performance with the MRT (Shen et al., 2016) and the Distortion (Zhang et al., 2017) on the same datasets. We hope that the strong shallow baseline system used in this work makes the evaluation convincing. We also compare with the other two related methods that aim at solving the exposure bias problem, including the scheduled sampling (Bengio et al., 2015) (SS-NMT)"
P19-1426,P05-1066,0,\N,Missing
P19-1549,K16-1002,0,0.0866134,"8142/19560/18920 for Ubuntu Dialog, and 36004/4501/4501 for Cornell Movie Dialog. Hyper-parameters: In our model and all baselines, Gated Recurrent Unit (GRU) (Cho et al., 2014) is selected as the fundamental cell in encoder and decoder layers, and the hidden dimension is 1,000. We set the word embedding dimension to 500, and all latent variables have a dimension of 100. For optimization, we use Adam (Kingma and Ba, 2015) with gradient clipping. The sentence padding length is set to 15, and the max conversation length is 10. In order to alleviate degeneration problem of variational framework (Bowman et al., 2016), we also apply KL annealing (Bowman et al., 2016) in all models with latent variables. The KL annealing steps are 15,000 for Cornell Movie Dialog and 250,000 for Ubuntu Dialog. Baseline Models: We compare our model with three baselines. They all focus on multi-turn conversations, and the third one is a state-of-theart variational model. 1) Hierarchical recurrent encoder-decoder (HRED) (Serban et al., 2016). 2) Variational HRED (VHRED) (Serban et al., 2017) with word drop (w.d) and KL annealing (Bowman et al., 2016), the word drop ratio equals to 0.25. 3) Variational Hierarchical Conversation"
P19-1549,W11-0609,0,0.197768,"Missing"
P19-1549,N16-1014,0,0.143037,"rated responses. To measure the performance effectively, we use 5 automatic evaluation metrics along with human evaluation. Average, Greedy and Extrema: Rather than calculating the token-level or n-gram similarity as the perplexity and BLEU, these three metrics are embedding-based and measure the semantic similarity between the words in the generated response and the ground truth (Serban et al., 2017; Liu et al., 2016). We use word2vec embeddings trained on the Google News Corpus 1 in this section. Please refer to Serban et al. (2017) for more details. Dist-1 and Dist-2: Following the work of Li et al. (2016), we apply Distinct to report the degree of diversity. Dist-1/2 is defined as the ratio of unique uni/bi-grams over all uni/bi-grams in generated responses. Human Evaluation: Since automatic evaluation results may not be fully consistent with human judgements (Liu et al., 2016), human evaluation is necessary. Inspired by Luo et al. (2018), we use following three criteria. Fluency measures whether the generated responses have grammatical errors. Coherence denotes the semantic consistency and relevance between a response and its context. Informativeness indicates whether the response is meaningf"
P19-1549,P17-2036,0,0.147917,"coherence and diversity compared to baseline methods. 1 Introduction Inspired by the observation that real-world human conversations are usually multi-turn, some studies have focused on multi-turn conversations and taken context (history utterances in previous turns) into account for response generation. How to model the relationship between the response and context is essential to generate coherent and logical conversations. Currently, the researchers employ some hierarchical architectures to model the relationship. Serban et al. (2016) use a context RNN to integrate historical information, Tian et al. (2017) sum up all utterances weighted by the similarity score between an utterance and the query, while Zhang et al. (2018) apply attention mechanism on history utterances. Besides, Xing et al. ∗ Corresponding Author zhanhl@ios.ac.cn (2018) add a word-level attention to capture finegrained features. In practice, we usually need to understand the meaning of utterances and capture their semantic dependency, not just word-level alignments (Luo et al., 2018). As shown in Table 1, this short conversation is about speaker A asks the current situation of speaker B. At the beginning, they talk about B’s pos"
P19-1549,D16-1230,0,0.0538865,"2.99 3.75 3.65 3.76 3.73 3.82 2.85 3.24 3.06 3.78 Table 2: Automatic and human evaluation results on Ubuntu Dialog Corpus and Cornell Movie Dialog Corpus. herence/relevance and diversity of generated responses. To measure the performance effectively, we use 5 automatic evaluation metrics along with human evaluation. Average, Greedy and Extrema: Rather than calculating the token-level or n-gram similarity as the perplexity and BLEU, these three metrics are embedding-based and measure the semantic similarity between the words in the generated response and the ground truth (Serban et al., 2017; Liu et al., 2016). We use word2vec embeddings trained on the Google News Corpus 1 in this section. Please refer to Serban et al. (2017) for more details. Dist-1 and Dist-2: Following the work of Li et al. (2016), we apply Distinct to report the degree of diversity. Dist-1/2 is defined as the ratio of unique uni/bi-grams over all uni/bi-grams in generated responses. Human Evaluation: Since automatic evaluation results may not be fully consistent with human judgements (Liu et al., 2016), human evaluation is necessary. Inspired by Luo et al. (2018), we use following three criteria. Fluency measures whether the ge"
P19-1549,W15-4640,0,0.101235,"Missing"
P19-1549,C18-1206,0,0.0957999,"Missing"
P19-1549,D18-1075,0,0.145467,"hers employ some hierarchical architectures to model the relationship. Serban et al. (2016) use a context RNN to integrate historical information, Tian et al. (2017) sum up all utterances weighted by the similarity score between an utterance and the query, while Zhang et al. (2018) apply attention mechanism on history utterances. Besides, Xing et al. ∗ Corresponding Author zhanhl@ios.ac.cn (2018) add a word-level attention to capture finegrained features. In practice, we usually need to understand the meaning of utterances and capture their semantic dependency, not just word-level alignments (Luo et al., 2018). As shown in Table 1, this short conversation is about speaker A asks the current situation of speaker B. At the beginning, they talk about B’s position. Then in the last two utterances, both speakers think about the way for B to come back. A mentions “umbrella”, while B wants A to “pick him/her up”. What’s more, there is no “word-to-word” matching in query and response. Unfortunately, the aforementioned hierarchical architectures do not model the meaning of each utterance explicitly and has to summarize the meaning of utterances on the fly during generating the response, and hence there is n"
P19-1549,P02-1040,0,0.103812,"Missing"
P19-1549,N18-1162,0,0.537644,"le background in which the conversation takes place, zp is for the consistency of topic between query and response pair, zq and zr try to model the content difference in each of them, respectively. For simplicity of equation description, we use n − 1 and n as the substitution of q and r. 2.1 Context Representation Each utterance ut is encoded into a vector vt by a bidirectional GRU (BiGRU), fθutt : vt = fθutt (ut ) (1) Figure 1: Graphical model of CSRR. ut is the t-th utterance, hct encodes context information up to time t. For the inter-utterance representation, we follow the way proposed by Park et al. (2018), which is calculated as: ( MLPθ (zc ), if t = 0 (2) hct = c ctx fθ (hct−1 , vt−1 , z ), otherwise fθctx (·) is the activation function of GRU. zc is the discourse-level latent variable with a standard Gaussian distribution as its prior distribution, that is: pθ (zc ) = N (z|0, I) (3) For the inference of zc , we use a BiGRU f c to run over all utterance vectors {vt }nt=0 in training set. n−1 ({vt }t=0 in test set): qφ (zc |v0 , ..., vn ) = N (z|µc , σ c I) c c where v = f (v0 , ..., vn ) c (5) c µ = MLPφ (v ) c (4) (6) c σ = Softplus(MLPφ (v )) (7) MLP(·) is a feed-forward network, and Softpl"
W14-1616,P11-1105,0,0.503106,"Missing"
W14-1616,N13-1001,0,0.492006,"s a 4-SCFG). In contrast, the hierarchical phrase-based model allows only 2-SCFG as each production can rewrite as a maximum of two nonterminals. On the other hand, our approach does not enforce a valid hierarchically nested derivation which is the case for Chiang’s approach. 4 Related Work The method introduced in this paper uses factors defined in the same manner as in Feng and Cohn (2013), but the two methods are quite different. That method (Feng and Cohn, 2013) is wordbased and under the frame of Bayesian model while this method is MP-based and uses a simpler Kneser-Ney smoothing method. Durrani et al. (2013) also present a Markov model based on MPs (they call minimal translation units) and further define operation sequence over MPs which are taken as the events in the Markov model. For the probability estimation, they use Kneser-Ney smoothing with a single backoff path. Different from operation sequence, our method gives a neat definition of factors which uses jump distance directly and avoids the bundle of source words and target words like in their method, and hence mitigates sparsity. Moreover, the use of parallel backoff infers richer structures and provides robust modeling. There are several"
W14-1616,P13-1033,1,0.787915,"oehn et al., 2007) have drastically improved beyond word-based approaches, primarily by using phrase-pairs as translation units, which can memorize local lexical context and reordering patterns. However, this literal memorization mechanism makes it generalize poorly to unseen data. Moreover, phrase-based models make an independent assumption, stating that the application of phrases in a derivation is independent to each other which conflicts with the underlying truth that the translation decisions of phrases should be dependent on context. There are some work aiming to solve the two problems. Feng and Cohn (2013) propose a word-based Markov model to integrate translation and reordering into one model and use the sophisticated hierarchical Pitman-Yor process which backs off from larger to smaller context to provide dynamic adaptive smoothing. This model shows good generalization to unseen data while 151 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 151–159, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics Figure 1, the MP sequence is shown in Figure 2. To evaluate the Markov model, we condition each MP on the previous k − 1"
W14-1616,W02-1039,0,0.134573,"Missing"
W14-1616,N10-1140,0,0.0188303,"500; For Moses, search-algorithm=1 and distortion-limit=6; For Moses-chart, search-algorithm=3 and max-charspan8 =20 for Moses-chart. We used both the distortion model and the lexical reordering model for Moses (denoted as Moses-l) except in §5.5 we only used the distortion model (denoted as Moses-d). We implemented the OSM according to Durrani et al. (2013) and used the same configuration with Moses-l. For our method we used the same configuration as Moses-l but adding an additional feature of the Markov model over MPs. do they consider dynamic strategies for estimating k-gram probabilities. Galley and Manning (2010) propose a method to introduce discontinuous phrases into the phrasebased model. It makes use of the decoding mechanism of the phrase-based model which jumps over the source words and hence can hold discontinuous phrases naturally. However, their method doesn’t touch the correlations between phrases and probability modeling which are the key points we focus on. 5 Experiments We design experiments to first compare our method with the phrase-based model (PB), the operation sequence model (OSM) and the hierarchical phrase-based model (HPB), then we present several experiments to test: 1. how each"
W14-1616,N03-1017,0,0.108987,"Missing"
W14-1616,P07-2045,0,0.088292,"= (j1 , j2 , . . . , jI ) is the vector of jump distance between MPi−1 and MPi , or insert for MPs with null source sides.2 To evaluate each of the k-gram models, we use modified Keneser-Ney smoothing to back off from larger context to smaller context recursively. In summary, adding the Markov model into the decoder involves two passes: 1) training a model over the MP sequences extracted from a word aligned parallel corpus; and 2) calculating the probability of the Markov model for each translation hypothesis during decoding. This Markov model is combined with a standard phrase-based model3 (Koehn et al., 2007) and used as an additional feature in the linear model. In what follows, we will describe how to estatimate the k-gram Markov model, focusing on backoff (§2.1) and smoothing (§2.2). Our model is phrase-based and works like a phrase-based decoder by generating target translation left to right using phrase-pairs while jumping around the source sentence. For each derivation, we can easily get its minimal phrase (MPs) sequence where MPs are ordered according to the order of their target side. Then this sequence of events is modeled as a Markov model and the log probability under this Markov model"
W14-1616,N12-1005,0,0.0899217,"e, our method gives a neat definition of factors which uses jump distance directly and avoids the bundle of source words and target words like in their method, and hence mitigates sparsity. Moreover, the use of parallel backoff infers richer structures and provides robust modeling. There are several other work focusing on modeling bilingual information into a Markov model. Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, and use it as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Neither work includes the jump distance, and nor Figure 5: Approximate SCFG patterns for step 0, 3 of Figure 3. X is a non-terminal which can only be rewritten by one MP. · and · · · denote gaps introduced by the left-to-right decoding algorithm and · can only cover one MP while · · · can cover zero or more MPs. In step 1, as the jump factor 1 is dropped, we do not know the orientation between bˇa and t¯a. However several jump distances are known: from X 1 to bˇa is distance -2 and t¯a to k"
W14-1616,N03-2002,0,0.0637057,"troduce gaps in estimating rule probabilities; these backoff patterns often bear close resemblance to SCFG productions in the hierarchical phrase-based model (Chiang, 2007). For example, in step 0 in Figure 3, as all the jump factors are present, this encodes the full ordering of the MPs and gives rise to the aligned MP pairs shown in Figure 5 (a). Note that an X 1 placeholder is included to ensure the jump distance from the previous MP to the MP <bˇa, take> is -2. The approximate SCFG production for the MP pairs is Probability Estimation We adopt the technique used in factor language models (Bilmes and Kirchhoff, 2003; Kirchhoff et al., 2007) to estimate the probability of a k-gram i i p(¯ ei |c) where c = f¯i−k+1 , ji−k+1 , e¯−1 i−k+1 . According to the definition of backoff, only when the count of the k-gram exceeds some given threshold, its maximum-likelihood estimate, pML (¯ ek |c) = N (¯ ek ,c) N (c) is used, where N (·) is the count of an event and/or context. Otherwise, only a portion of pML (¯ ek |c) is used and the remainder is constructed from a lower-level (by dropping a factor). In order to ensure valid probability estimates, i.e. sums <bˇa t¯a X 1 kˇaol`v j`ınq`u, X 1 take it into account>. 4"
W14-1616,J03-1002,0,0.00594419,"ten by one MP. · and · · · denote gaps introduced by the left-to-right decoding algorithm and · can only cover one MP while · · · can cover zero or more MPs. In step 1, as the jump factor 1 is dropped, we do not know the orientation between bˇa and t¯a. However several jump distances are known: from X 1 to bˇa is distance -2 and t¯a to kˇaol`v j`ınq`u is 2. In this case, the source side can be bˇa t¯a X 1 kˇaol`v j`ınq`u, 155 model with the order = 3 over the MP sequences.6 The threshold count of backoff for all nodes was τ = 2. We aligned the training data sets by first using GIZA++ toolkit (Och and Ney, 2003) to produce word alignments on both directions and then combining them with the diag-final-and heuristic. All experiments used a 5-gram language model which was trained on the Xinhua portion of the GIGAWORD corpus using the SRILM toolkit. Translation performance was evaluated using BLEU (Papineni et al., 2002) with case-insensitive n ≤ 4grams. We used minimum error rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. We used Moses for PB and Moses-chart for HPB with the configuration as follows. For both, max-phrase-length=7, ttable-limit7 =2"
W14-1616,P96-1041,0,0.21057,"back off to and for the node e¯k |f¯2k , j2k , e¯2k−1 where only the factors of MP1 are dropped, there are k-2 nodes to back off to. 2.2 to unity, probability mass needs to be “stolen” from the higher level and given to the lower level. Hence, the whole definition is ( d pml (¯ ei |c) if N (¯ ei , c) > τk p(¯ ei |c) = N (¯ei ,c) α(c)g(¯ ei , c) otherwise (3) where dN (¯ei ,c) is a discount parameter which reserves probability from the maximum-likelihood estimate for backoff smoothing at the next lowerlevel, and we estimate dN (¯ei ,c) using modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996); τk is the threshold for the count of the k-gram, α(c) is the backoff weight used to make sure the entire distribution still sums to unity, P 1 − e¯:N (¯e,c)>τk dN (¯e,c) pM L (¯ e|c) P α(c) = , e, c) e¯:N (¯ e,c)≤τk g(¯ and g(¯ ei , c) is the backoff probability which we estimate by averaging over the nodes in the next lower level, g(¯ ei , c) = 1X p(¯ ei |c0 ) , φ 0 c where φ is the number of nodes to back off, c0 is the lower-level context after dropping one factor from c. The k-gram for the source and jump factors are estimated in the same way, using the same backoff semantics.4 Note (3)"
W14-1616,P03-1021,0,0.351521,"follows, we will describe how to estatimate the k-gram Markov model, focusing on backoff (§2.1) and smoothing (§2.2). Our model is phrase-based and works like a phrase-based decoder by generating target translation left to right using phrase-pairs while jumping around the source sentence. For each derivation, we can easily get its minimal phrase (MPs) sequence where MPs are ordered according to the order of their target side. Then this sequence of events is modeled as a Markov model and the log probability under this Markov model is included as an additional feature into the linear SMT model (Och, 2003). A MP denotes a phrase which cannot contain other phrases. For example, in the sentence pair in Figure 1, <bˇa t¯a , take it> is a phrase but not a minimal phrase, as it contains smaller phrases of <bˇa , take> and <t¯a , it>. MPs are a complex event representation for sequence modelling, and using these naively would be a poor choice because few bigrams and trigrams will be seen often enough for reliable estimation. In order to reason more effectively from sparse data, we consider more generalized representations by decomposing MPs into their component events: the source phrase (source f¯),"
W14-1616,J07-2003,0,0.0583757,"lower level, g(¯ ei , c) = 1X p(¯ ei |c0 ) , φ 0 c where φ is the number of nodes to back off, c0 is the lower-level context after dropping one factor from c. The k-gram for the source and jump factors are estimated in the same way, using the same backoff semantics.4 Note (3) is applied independently to each of the three models, so the use of backoff may differ in each case. 3 Discussion As a part of the backoff process our method can introduce gaps in estimating rule probabilities; these backoff patterns often bear close resemblance to SCFG productions in the hierarchical phrase-based model (Chiang, 2007). For example, in step 0 in Figure 3, as all the jump factors are present, this encodes the full ordering of the MPs and gives rise to the aligned MP pairs shown in Figure 5 (a). Note that an X 1 placeholder is included to ensure the jump distance from the previous MP to the MP <bˇa, take> is -2. The approximate SCFG production for the MP pairs is Probability Estimation We adopt the technique used in factor language models (Bilmes and Kirchhoff, 2003; Kirchhoff et al., 2007) to estimate the probability of a k-gram i i p(¯ ei |c) where c = f¯i−k+1 , ji−k+1 , e¯−1 i−k+1 . According to the defini"
W14-1616,P11-2031,0,0.069853,"he distortion and lexical reordering models of Moses, and are they complemenatary; 4. whether using MPs as translation units is better in our approach than the simpler tactic of using only word pairs. 5.1 5.2 Data Setup We first give the results of performance comparison. Here we add another system (denoted as Moses-l+trgLM): Moses-l together with the target language model trained on the training data set, using the same configuration with Moses-l. This system is used to test whether our model gains improvement just for using additional information on the training set. We use the open tool of Clark et al. (2011) to control for optimizer stability and test statistical significance. The results are shown in Tables 1 and 2. The two language pairs we used are quite different: Chinese has a much bigger word order difference c.f. English than does Arabic. The results show that our system can outperform the baseline We consider two language pairs: Chinese-English and Arabic-English. The Chinese-English parallel training data is made up of the non-UN portions and non-HK Hansards portions of the NIST training corpora, distributed by the LDC, having 1,658k sentence pairs with 40m and 44m Chinese and English wo"
W14-1616,P02-1040,0,0.0915753,"bˇa is distance -2 and t¯a to kˇaol`v j`ınq`u is 2. In this case, the source side can be bˇa t¯a X 1 kˇaol`v j`ınq`u, 155 model with the order = 3 over the MP sequences.6 The threshold count of backoff for all nodes was τ = 2. We aligned the training data sets by first using GIZA++ toolkit (Och and Ney, 2003) to produce word alignments on both directions and then combining them with the diag-final-and heuristic. All experiments used a 5-gram language model which was trained on the Xinhua portion of the GIGAWORD corpus using the SRILM toolkit. Translation performance was evaluated using BLEU (Papineni et al., 2002) with case-insensitive n ≤ 4grams. We used minimum error rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. We used Moses for PB and Moses-chart for HPB with the configuration as follows. For both, max-phrase-length=7, ttable-limit7 =20, stacksize=50 and max-pop-limit=500; For Moses, search-algorithm=1 and distortion-limit=6; For Moses-chart, search-algorithm=3 and max-charspan8 =20 for Moses-chart. We used both the distortion model and the lexical reordering model for Moses (denoted as Moses-l) except in §5.5 we only used the distortion mo"
