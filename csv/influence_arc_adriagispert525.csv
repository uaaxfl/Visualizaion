2004.iwslt-evaluation.14,N04-1033,0,0.0910244,"This has growing importance when dealing with very different pairs of languages, in terms of word ordering, as with a ChineseEnglish task. In section 4.3 the impact of this technique is evaluated in practice. 1. It induces a monotonous segmentation of the pair of sentences. 2. Words are consecutive along both source and target sides of the tuple. 3. No word on either side of the tuple is aligned to a word out of the tuple. 4. Each tuple cannot be decomposed into smaller phrases without violating the previous constraint. 3.4. Xgram estimation Note that this set is unique under these conditions [8]. The only ambiguity appears when a target word is aligned to NULL, in which case we append it to the next tuple (if exists, else to the previous). An example of the tuple extraction process is drawn in figure 3. When extracting tuples with more than one word in each language (as the third tuple in figure 3), a certain local reordering of the target is necessarily encoded. While helping the system to avoid local reordering mistakes, this strategy can suffer from an information loss, as the source words appearing in this tuple may not have any Finally, given the parallel corpus described in a s"
2004.iwslt-evaluation.14,C96-2141,0,0.578405,"on the Chinese-English supplied task of the Int. Workshop on Spoken Language Translation (IWSLT’04) Evaluation Campaign are shown and discussed. Figure 1: A translation FST from Spanish to English the same structure and search method, acoustic models can be omitted to perform text translation tasks only. This translation FST is learned automatically from a parallel corpus in three main steps (and an optional preprocessing). First, an automatic word alignment is produced. Currently this is done by the freely-available GIZA++ software [2], implementing well-known IBM and HMM translation models [3, 4]. From this alignment, a tuple extraction algorithm generates the set tuples that induces a sequential segmentation of both source and target sentences. These tuples must respect word order in both languages, as this is necessary for the transducer to produce a correct-order translated output. Finally, Xgrams are learned using standard language modeling techniques. Previous publications on this system include [5] and [6]. The organization of the paper is as follows. Section 2 offers an overview of the system architecture, whereas sections 2 and 3 deepen into details on translation generation a"
2004.iwslt-papers.3,W03-0301,0,0.062964,"ocessing, such as bilingual dictionaries extraction or transfer rules learning. However, it is in the context of statistical machine translation where it becomes particularly crucial. As an essential block in the learning process of current statistical translation models (single-word or phrase-based, conditionalor joined-probability based), its correct production has a sound correlation with translation quality [1]. This relevance has been corresponded by many previous works on the matter, including a shared task in the frame of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts [2]. Several competing systems were presented and evaluated against a manual reference using AER, the most widely used evaluation measure. Among the wide range of approaches presented, two basic trends should be highlighted. On the other side of the spectrum, we find the approach based on word cooccurrences and link probabilities, presented in [10]. Its relative simplicity, its flexibility to introduce more knowledge sources, its symmetry and its promising results [2] make it appealing despite its dependence on empirical data and tuning strategies. However, the most important disadvantage of this"
2004.iwslt-papers.3,C96-2141,0,0.435176,"Missing"
2004.iwslt-papers.3,W99-0604,0,0.110404,"Missing"
2004.iwslt-papers.3,N03-1017,0,0.0373062,"Missing"
2004.iwslt-papers.3,W02-1012,0,0.0356565,"target sentence to positions in source sentence, it is strictly asymmetric, generating one-to-many word alignments that do not account for many translation phenomena. This effect has been tackled by several kinds of symmetrization heuristics (all of them linguistically blind), in search of a strategy to provide posterior phrase-based translation systems with the most accurate possible source. Moreover, the complexity of IBM models and their overload of parameters to estimate turn it very hard to introduce linguistic information into this setting in a reasonable way (some efforts being done in [9]). This paper introduces a phrase alignment strategy that seeks phrase and word links in two stages using cooccurrence measures and linguistic information. On a first stage, the algorithm finds high-precision links involving a linguistically-derived set of phrases, leaving word alignment to be performed in a second phase. Experiments have been carried out for an English-Spanish parallel corpus, and we show how phrase cooccurrence measures convey a complementary information to word cooccurrences, and a stronger evidence of a good alignment. Alignment Error Rate (AER) results are presented, bein"
2004.iwslt-papers.3,P03-1012,0,0.0281238,"correct production has a sound correlation with translation quality [1]. This relevance has been corresponded by many previous works on the matter, including a shared task in the frame of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts [2]. Several competing systems were presented and evaluated against a manual reference using AER, the most widely used evaluation measure. Among the wide range of approaches presented, two basic trends should be highlighted. On the other side of the spectrum, we find the approach based on word cooccurrences and link probabilities, presented in [10]. Its relative simplicity, its flexibility to introduce more knowledge sources, its symmetry and its promising results [2] make it appealing despite its dependence on empirical data and tuning strategies. However, the most important disadvantage of this approach is the one-to-one constrain, producing high precision alignments with low recall, what can represent a severe limitation to its use in practical translation systems. We present in this paper an alignment strategy that is also based on bilingual cooccurrences, but aims at finding phrase-to-phrase alignment by using linguistic knowledge"
2004.iwslt-papers.3,W02-1018,0,0.0440718,"rases. Typically, initial Giza-based alignments are generated and a symmetrization strategy is followed to obtain the core alignment from which bilingual phrases are built. These in turn are fed to the statistical translation model for estimation. However, current symmetrization strategies lack linguistic knowledge to decide ambiguities, and the translation model faces the task of learning translation probabilities from a noisy source. To face this problem, we present an alignment strategy that generates directly a phrase alignment from corpus cooccurrence counts. In contrast to previous work [11], we do so by generating very high-confidence links between phrases before proceeding onto word alignment. This search for phrase links is limited to a small adequate set of possible phrases. In the following subsections our proposal is described in detail. Table 1: Examples of φ2 between words and phrases. por favor Association or cooccurrence measures extracted from parallel corpora give strong evidence of so-called translation equivalence [12], or simply alignment adequacy, between a pair of phrases or words. Among these measures we find Dice-score, φ2 score and some others, offering a simi"
2004.iwslt-papers.3,J03-1002,0,0.0116221,"subwords or phrases) in a given parallel corpus, detecting which words from each language are connected together in a given situation. This has many applications in natural language processing, such as bilingual dictionaries extraction or transfer rules learning. However, it is in the context of statistical machine translation where it becomes particularly crucial. As an essential block in the learning process of current statistical translation models (single-word or phrase-based, conditionalor joined-probability based), its correct production has a sound correlation with translation quality [1]. This relevance has been corresponded by many previous works on the matter, including a shared task in the frame of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts [2]. Several competing systems were presented and evaluated against a manual reference using AER, the most widely used evaluation measure. Among the wide range of approaches presented, two basic trends should be highlighted. On the other side of the spectrum, we find the approach based on word cooccurrences and link probabilities, presented in [10]. Its relative simplicity, its flexibility to introduce more knowled"
2004.iwslt-papers.3,N03-2017,0,0.0297579,"d include regular expressions such as numbers, dates or times of the day (that could also be classified) or even collocations and phrasal verbs. As this selection is language-dependent, every language will 109 define its own adequate rules. sented in [10]. Basically, an initial alignment is generated using word cooccurrence measures, from which link probabilities are estimated. Then, a best first search is performed, following an heuristic function based on the global aligned sentence link probabilities. The search is further improved with a syntactic constrain (also called cohesion constrain [15]) and can introduce features on the links, such as a dependence on adjacent links. Our implementation allows certain positions to be prohibited, so that previous phrase alignment is fixed, although its links also compute in link probability estimation at each iteration. Given the enormous space of possible word alignments to choose from, the heuristic function becomes the key to efficiency, so long as it is correctly defined. Basic parameters are: If no linguistic knowledge is available, statistical procedures can also be used to obtain a set of possible phrases. For example, we can select the"
2004.iwslt-papers.3,A00-1031,0,0.0489206,"ment strategy proposed against state-of-the-art alignments. Lmean 7.6 7.3 3.2. Preprocessing All preprocessing that has been carried out is described as follows: 4.1. Verb groups • Normalization of contracted forms for English (ie. wouldn’t = would not, we’ve = we have) and Spanish (del = de el) Verb groups detection rules include 14 rules for English language and just 6 for Spanish, which usually employs declined verb forms omitting thus personal pronouns and using thus a single word. Verb groups rules have detected a total of: • English data has been tagged using freely-available TnT tagger [16], and base forms have been obtained using wnmorph, included in the WordNet package [17]. • 1156 verb groups in English, classified into 238 different verb lemmas • Spanish data has been tagged using maco+ and relax package already mentioned. This software also generates a lemma or base form for each input word. • 658 verb groups in Spanish, classified into 188 different verb lemmas The classification of these phrases produces an efficient reduction of the cooccurrence table from 0.35M to 0.33M, we do not compute cooccurrence counts for all words internal to the phrase, but just for the lemma o"
2004.iwslt-papers.3,H92-1116,0,0.0518371,"ssing All preprocessing that has been carried out is described as follows: 4.1. Verb groups • Normalization of contracted forms for English (ie. wouldn’t = would not, we’ve = we have) and Spanish (del = de el) Verb groups detection rules include 14 rules for English language and just 6 for Spanish, which usually employs declined verb forms omitting thus personal pronouns and using thus a single word. Verb groups rules have detected a total of: • English data has been tagged using freely-available TnT tagger [16], and base forms have been obtained using wnmorph, included in the WordNet package [17]. • 1156 verb groups in English, classified into 238 different verb lemmas • Spanish data has been tagged using maco+ and relax package already mentioned. This software also generates a lemma or base form for each input word. • 658 verb groups in Spanish, classified into 188 different verb lemmas The classification of these phrases produces an efficient reduction of the cooccurrence table from 0.35M to 0.33M, we do not compute cooccurrence counts for all words internal to the phrase, but just for the lemma of its head verb. The results of the phrase alignment with these phrases are shown in th"
2004.iwslt-papers.3,P00-1056,0,0.146299,"Missing"
2004.iwslt-papers.3,H91-1026,0,\N,Missing
2005.iwslt-1.25,2005.mtsummit-papers.36,1,0.794738,"m (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In contrast with standard phrase-based approaches, our translation model is expressed in tuples as bilingual units. Given a word alignment, tuples define a unique and monotonic segmentation of each bilingual sentence, building up a much smaller set of units than with phrases and allowing N-gram estimation to account for the history of the translation process [5, 6]. This approach has its origins in SMT by using finite state transducers [7, 8, 9]. The organization of the paper is as follows. Section 2 describes in detail the tuple n-gram translation model, while section 3 introduces the additional features used in the system. Section 4 provides a brief overview of the decoding tool and search strategy used. Next, sections 5 and 6 report and discuss results on IWSLT’05 Chineseto-English and Arabic-to-English tracks, respectively. Finally, Section 7 concludes and outlines future research lines. 2. The Tuple N-gram translation model The tuple N-gram transla"
2005.iwslt-1.25,2004.iwslt-evaluation.14,1,0.888051,"this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In contrast with standard phrase-based approaches, our translation model is expressed in tuples as bilingual units. Given a word alignment, tuples define a unique and monotonic segmentation of each bilingual sentence, building up a much smaller set of units than with phrases and allowing N-gram estimation to account for the history of the translation process [5, 6]. This approach has its origins in SMT by using finite state transducers [7, 8, 9]. The organization of the paper is as follows. Section 2 describes in detail the tuple n-gram translation model, while section 3 introduces the additional features used in the system. Section 4 provides a brief overview of the decoding tool and search strategy used. Next, sections 5 and 6 report and discuss results on IWSLT’05 Chineseto-English and Arabic-to-English tracks, respectively. Finally, Section 7 concludes and outlines future research lines. 2. The Tuple N-gram translation model The tuple N-gram translation model is a language model of a particular language composed by bilingual unit"
2005.iwslt-1.25,N04-1033,0,0.0439184,"referred to as tuples. This model approximates the joint probability between source and target languages by using N-grams as described by the following equation: p(sJ1 , tI1 ) = · · · = K Y i=1 p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (2) (3) where (s, t)i refers to the ith tuple of a given bilingual sentence pair, which is segmented into K tuples. It is important to notice that, since both languages are linked up in tuples, the context information provided by this translation model is bilingual. Tuples are extracted from a word-to-word aligned corpus according to the following constraints [10]: • a monotonic segmentation of each bilingual sentence pair is produced • no word inside the tuple is aligned to words outside the tuple • no smaller tuples can be extracted without violating the previous constraints As a consequence of these constraints, only one segmentation is possible for a given parallel sentence pair and a word alignment. Usually, automatic word-to-word alignments are generated in both source-to-target and target-to-source directions by using GIZA++ [11], and tuples are usually extracted from the union set of alignments. However, in section 5 results are also reported w"
2005.iwslt-1.25,P00-1056,0,0.157829,"on model is bilingual. Tuples are extracted from a word-to-word aligned corpus according to the following constraints [10]: • a monotonic segmentation of each bilingual sentence pair is produced • no word inside the tuple is aligned to words outside the tuple • no smaller tuples can be extracted without violating the previous constraints As a consequence of these constraints, only one segmentation is possible for a given parallel sentence pair and a word alignment. Usually, automatic word-to-word alignments are generated in both source-to-target and target-to-source directions by using GIZA++ [11], and tuples are usually extracted from the union set of alignments. However, in section 5 results are also reported when extracting tuples with the alignment from sourceto-target direction. Figure 1 presents a simple example illustrating the tuple extraction process. I would like NULL NULL quisiera ir t1 t2 t3 to have a comer un helado t4 a t5 huge ice−cream gigante t6 Figure 1: Example of tuple extraction from an aligned bilingual sentence pair. Once tuples have been extracted, the tuple vocabulary can be pruned by using histogram counts, thus keeping the N most frequent tuples sharing the s"
2005.iwslt-1.25,2005.mtsummit-papers.37,1,0.781011,"uentiality contraint may lead to an unpractical tuple length and excessive amount of embedded words. In this case, it is more reasonable to allow for a certain reordering in the training data. This means that the tuples are broken into smaller tuples, and these are sequenced in the order of the target words. In order not to lose the information on the correct order, the decoder performs then a reordered search, which is guided by the N-gram model of the unfolded tuples and the additional feature models. On the other hand, the tuple unfolding process highly reduces the effect of embedded words [12]. Figure 2 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual N-gram language model with reordered source words. Usually, this feature function is accompanied by a word penalty model. This model introduces a sentence length penalty in order to compensate the system’s preference for short target sentences, caused by the presence of the previous target language model. This penalization depends on the total number of words contained in the partial translation hypothesis, and it is computed as follows: pW P (tk ) = exp(n"
2005.iwslt-1.25,2005.iwslt-1.23,1,0.865304,"nments are used. In the case of the backward lexicon model, GIZA++ target-to-source alignments are used instead. 4. N-gram based Decoding • a target-to-source lexicon model 3.1. Target language model The first of these feature functions is a standard target language model, estimated as an N-gram over the target words, as expressed by this equation: k Y J X I Y 1 p(tin |sjn ) (I + 1)J j=1 i=0 For decoding given the combination of models presented above, we used MARIE, a decoder implemeting a beam search strategy with distortion (or reordering) capabilities developed at the TALP Research Center [13]. For efficient pruning of the search space, several pruning techniques are used, such as: (5) • Threshold pruning: Hypotheses with lower scores than a certain threshold are eliminated. where tk refers to the partial translation hypothesis and wn to the nth word in it. Although this model could be trained from a larger monolingual data set, this has not been done for IWSLT’05 experiments, which use as target text the same amount of data used as parallel text. As with the tuple translation model, the SRI Language Modeling toolkit was used. • Histogram pruning: Only the K-best ranked hypotheses"
2005.iwslt-1.25,2002.tmi-tutorials.2,0,0.0510661,"the target language is a possible translation of a given sentence s in the source language, and the main difference between two translation hypotheses is a probability assigned to each, which is to be learned from a bilingual corpus. The first SMT systems were based on the noisy channel approach on a word-based basis, modeling the translation of a target language sentence t given a source language sentence t as a translation model probability p(s|t) times a target language model probability p(t) [1]. Recently, word-based translation models have been replaced by phrase-based translation models [2, 3], which are estimated from aligned bilingual corpora by using relative frequencies. On the other hand, according to the maximum entropy framework [4], we can define the translation hypothesis t given a source sentence s, as the target sentence maximizing a log-linear combination of feature functions, as described in the following equation: tˆI1 = arg max tI1 ( M X λm hm (sJ1 , tI1 ) m=1 ) (1) where λm correspond to the weighting coefficients of the log-linear combination, and the feature functions hm (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, t"
2005.iwslt-1.25,N03-1017,0,0.0257197,"the target language is a possible translation of a given sentence s in the source language, and the main difference between two translation hypotheses is a probability assigned to each, which is to be learned from a bilingual corpus. The first SMT systems were based on the noisy channel approach on a word-based basis, modeling the translation of a target language sentence t given a source language sentence t as a translation model probability p(s|t) times a target language model probability p(t) [1]. Recently, word-based translation models have been replaced by phrase-based translation models [2, 3], which are estimated from aligned bilingual corpora by using relative frequencies. On the other hand, according to the maximum entropy framework [4], we can define the translation hypothesis t given a source sentence s, as the target sentence maximizing a log-linear combination of feature functions, as described in the following equation: tˆI1 = arg max tI1 ( M X λm hm (sJ1 , tI1 ) m=1 ) (1) where λm correspond to the weighting coefficients of the log-linear combination, and the feature functions hm (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, t"
2005.iwslt-1.25,P96-1041,0,0.0830768,"), in order to separate articles from words. Note that this process is neither guided by tagging information (it does not use any tagging software) nor complete (several other Arabic particles are usually attached to words). However, it already produces a significative vocabulary reduction, leading to improved performance. During word alignment, IBM model 1 tables are used directly to compute the lexicon feature. Finally, in order to learn the target and the tuples language models we used SRILM [14]. All models were learnt using interpolation of higher and lower order n-grams with Knesser-Ney [15] smoothing. 5.3. Development work Several configurations were tested on the development set optimizing BLEU, namely baseline and three alternatives. Results are shown in table 3. The baseline configuration system is built using: • The union alignment [11] to extract unfolded tuples and the intersection to solve embedded words. • All source-nulled tuples are linked the the target word of the next tuple. • The order of the target and the translation Ngram language models is set to 4 and 3, respectively. • The reordering parameters of the decoder are fixed to m = 5 and j = 3 for the Chinese-to-En"
2005.iwslt-1.25,J96-1002,0,0.0198229,"s a probability assigned to each, which is to be learned from a bilingual corpus. The first SMT systems were based on the noisy channel approach on a word-based basis, modeling the translation of a target language sentence t given a source language sentence t as a translation model probability p(s|t) times a target language model probability p(t) [1]. Recently, word-based translation models have been replaced by phrase-based translation models [2, 3], which are estimated from aligned bilingual corpora by using relative frequencies. On the other hand, according to the maximum entropy framework [4], we can define the translation hypothesis t given a source sentence s, as the target sentence maximizing a log-linear combination of feature functions, as described in the following equation: tˆI1 = arg max tI1 ( M X λm hm (sJ1 , tI1 ) m=1 ) (1) where λm correspond to the weighting coefficients of the log-linear combination, and the feature functions hm (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In con"
2005.iwslt-1.25,W05-0823,1,0.827145,"m (s, t) to a logarithmic scaling of the probabilities of each model. Following this approach, the translation system described in this paper implements a log-linear combination of one translation model and four additional feature models. In contrast with standard phrase-based approaches, our translation model is expressed in tuples as bilingual units. Given a word alignment, tuples define a unique and monotonic segmentation of each bilingual sentence, building up a much smaller set of units than with phrases and allowing N-gram estimation to account for the history of the translation process [5, 6]. This approach has its origins in SMT by using finite state transducers [7, 8, 9]. The organization of the paper is as follows. Section 2 describes in detail the tuple n-gram translation model, while section 3 introduces the additional features used in the system. Section 4 provides a brief overview of the decoding tool and search strategy used. Next, sections 5 and 6 report and discuss results on IWSLT’05 Chineseto-English and Arabic-to-English tracks, respectively. Finally, Section 7 concludes and outlines future research lines. 2. The Tuple N-gram translation model The tuple N-gram transla"
2005.iwslt-1.25,2005.iwslt-1.24,0,0.0195904,"s are correlated for both dev and test sets in the zh2en task, both improving in the primary run. However, they are incorrelated for both dev and test sets in the ar2en task. 6. Discussion When studying the test results, we can note that the Chinese test set seems to be ’easier’ to translate than the development (obtaining higher scores), whereas the effect is opposite in the case of Arabic. This behaviour could easily be explained by the nature of the data. However, when comparing the two TALP systems which competed in the same tracks and under the same conditions (TALP-Ngram and TALP-Phrase [17]), a surprisingly different behaviour between development and test can be found. Regarding development results, the TALPNgram system improves the performance of the TALPPhrase system (table 6) in the Chinese-to-English task (0.384 &gt; 0.373), while it achieves the same score in the Arabic-to-English task (0.573 ≈ 0.572), both measured in BLEU. However, regarding the test set, the TALPNgram system is clearly beaten by the TALP-Phrase system in both tasks (0.444 &lt; 0.452 in Chinese-to-English, and 0.533 &lt; 0.573 in Arabic-to-English). Experiments have been conducted in order to find out the reason e"
2005.mtsummit-papers.36,N01-1018,0,0.0681459,"ation of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper implements a translation model based on the ﬁnite-state perspective, (de Gispert and Mari˜ no, 2002) and (de Gispert et al., 2004), which is used along with a log-linear combination of four additional feature functions (Crego et al., 2005). The implemented translation model, which is referred to as tuple n-gram model, diﬀers from the well known phrase-model approach (Koehn et al., 2003) in two basic issues."
2005.mtsummit-papers.36,J96-1002,0,0.115652,"entioned in the introduction, the translation system presented here implements a log-linear combination of feature functions 1 In the present version of the system, target words aligned to NULL are always attached to the following word. Further work in this area is proposed in the last section. along with the tuple n-gram model. This section describes the log-linear model and each of the four speciﬁc feature functions that are used. Finally, a brief description of the customized decoding tool that is used is presented. 3.1 Log-Linear Model Framework According to the maximum entropy framework (Berger et al., 1996), the corresponding translation hypothesis T , for a given source sentence S, is deﬁned by the target sentence that maximizes a log-linear combination of feature functions hi (S, T ), as described in the following equation:  argmax λi hi (S, T ) (2) T i where the λi ’s constitute the weighting coeﬃcients of the log-linear combination and the feature function hi (S, T ) corresponds to a logarithmic scaling of the ith -model probabilities. These weights are computed via an optimization procedure which maximizes the translation BLEU (Papineni et al., 2002) over a given development set. This opti"
2005.mtsummit-papers.36,J90-2002,0,0.523079,"Missing"
2005.mtsummit-papers.36,J93-2003,0,0.0465928,"ementation of translation algorithms based on statistical methods (Brown et al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is l"
2005.mtsummit-papers.36,N04-1033,0,0.157603,"Missing"
2005.mtsummit-papers.36,2005.iwslt-1.23,1,0.857305,"Missing"
2005.mtsummit-papers.36,2004.iwslt-evaluation.14,1,0.687498,"Missing"
2005.mtsummit-papers.36,P05-2012,1,0.78658,"Missing"
2005.mtsummit-papers.36,N03-1017,0,0.067546,"The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper implements a translation"
2005.mtsummit-papers.36,P02-1038,0,0.221853,"otivated by the development of computer resources needed to allow the implementation of translation algorithms based on statistical methods (Brown et al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of"
2005.mtsummit-papers.36,N04-1021,0,0.0290838,"length penalization in order to compensate the system preference for short target sentences caused by the presence of the previous target language model. This penalization depends on the total number of words contained in the partial translation hypothesis, and it is computed as follows: wp(Tk ) = exp(number of words in Tk ) (4) where, again, Tk refers to the partial translation hypothesis. The fourth and ﬁfth feature functions correspond to a forward and backward lexicon models. These models provide IBM 1 translation probabilities for each tuple based on the IBM 1 lexical parameters p(t|s) (Och et al., 2004). These lexicon models are computed according to the following equation: pIBM 1 ((t, s)n ) = I J   1 p(tin |sjn ) (5) (I + 1)J j=1 i=0 (3) where sjn and tin are the j th and ith words in the source and target sides of tuple (t, s)n , being J and I the corresponding total number words in each side of it. For computing the forward lexicon model, IBM model 1 lexical parameters from GIZA++ source-to-target alignments are used. In the case of the backward lexicon model, GIZA++ target-to-source alignments are used instead. where Tk refers to the partial translation hypothesis and wn to the nth wor"
2005.mtsummit-papers.36,P02-1040,0,0.115781,"cording to the maximum entropy framework (Berger et al., 1996), the corresponding translation hypothesis T , for a given source sentence S, is deﬁned by the target sentence that maximizes a log-linear combination of feature functions hi (S, T ), as described in the following equation:  argmax λi hi (S, T ) (2) T i where the λi ’s constitute the weighting coeﬃcients of the log-linear combination and the feature function hi (S, T ) corresponds to a logarithmic scaling of the ith -model probabilities. These weights are computed via an optimization procedure which maximizes the translation BLEU (Papineni et al., 2002) over a given development set. This optimization is performed by using an in-house developed optimization algorithm, which is based on a simplex method (Press et al., 2002). 3.2 Implemented Feature Functions The proposed translation system implements a total of ﬁve feature functions: • tuple 3-gram model, • target language model, • word penalty model, • source-to-target lexicon model, and • target-to-source lexicon model. The ﬁrst of these functions is the tuple 3-gram model, which was already described in the previous section. The second feature function implemented is a target language model"
2005.mtsummit-papers.36,2002.tmi-tutorials.2,0,0.0433398,"al., 1990) and (1993). The ﬁrst SMT systems were based on the noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as a translation model probability p(S|T ) times a target language model probability p(T ). In recent systems such an approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney, 2002). Additionally, original word-based translation models (Brown et al., 1993) have been replaced by phrase-based translation models (Zens et al., 2002) and (Koehn et al., 2003), which are estimated from aligned bilingual corpora by using relative frequencies. 275 On the other hand, the translation problem has also been approached from the ﬁnite-state perspective as the most natural way for integrating speech recognition and machine translation into a speech-to-speech translation system (Riccardi et al., 1996), (Vidal, 1997), (Bangalore and Riccardi, 2001) and (Casacuberta, 2001). In this kind of systems the translation model constitutes a ﬁnite-state network which is learned from training data. The translation system described in this paper"
2005.mtsummit-papers.36,P00-1056,0,\N,Missing
2005.mtsummit-papers.37,J96-1002,0,0.0120462,"rce sentence f1J is transformed into (or generates) a target sentence eI1 by means of a stochastic process. The translation of a source sentence f1J can be formulated as the search of the target sentence eI1 that maximizes the conditional probability p(eI1 |f1J ), which can be rewritten using the Bayes rule as:  arg max eI1 p(f1J |eI1 ) · p(eI1 )  (1) Alternatively to this classical source channel approach, the posterior probability p(eI1 |f1J ) can be modeled directly as a log-linear combination of feature models (Och and Ney, H., 2002), based on the maximum entropy framework, as shown in (Berger et al., 1996). This simpliﬁes the introduction of several additional models explaining the translation process, as the search becomes:  arg max{exp( eI1 λi hi (e, f ))} (2) i where the feature functions hi are the system models (translation model, language model, reordering model, ...), and the λi weights are typically optimized to maximize a scoring function on a development set. In this work a combination of 4 feature models is used, which include: • a translation Ngram tuple-based model Figure 1: Tuples extraction from a pair of word aligned source sentences. 2.1 Translation model As for the Translatio"
2005.mtsummit-papers.37,N04-1033,0,0.160951,"Missing"
2005.mtsummit-papers.37,2005.iwslt-1.23,1,0.8677,"Missing"
2005.mtsummit-papers.37,2004.iwslt-evaluation.14,1,0.855497,"Missing"
2005.mtsummit-papers.37,koen-2004-pharaoh,0,0.0499024,"means that the target sentence is generated by translating parts of the source sentence in a non-sequential fashion. However, this search can cause a combinatory explosion of the search graph if arbitrary reorderings are allowed, being an NP-hard problem (Knight, 1999). This issue has been tackled in previous work for a phrase-based SMT approach, typically reducing the combinatory explosion of the search space by using a distortion model that penalizes the longest reorderings, which are only allowed if well supported by the other feature models involved in the search (Och and Ney, H., 2004), (Koehn, 2004). In this paper we deal with the question of reordering for an Ngram-based SMT approach with two complementary strategies, namely reordered search and tuple unfolding. These strategies interact to improve translation quality in a Chinese to English task. On the one hand, we introduce reordering capabilities into an Ngram-based decoder. The decoder then performs a reordered search over the source sentence, and combines a translation tuples Ngram model, a target language model, a word penalty and a word distance model. Interestingly, even though the translation units are learnt sequentially, its"
2005.mtsummit-papers.37,P00-1056,0,0.333674,"Missing"
2005.mtsummit-papers.37,P02-1038,0,0.132368,"Missing"
2005.mtsummit-papers.37,J04-4002,0,0.160354,"Missing"
2005.mtsummit-papers.37,2001.mtsummit-papers.68,0,0.0211217,"of the whole training corpus, and reﬁned the links by the union of both alignment directions (Och and Ney, H., 2004). Afterwards we segmented the bilingual sentence pairs of the training set, extracting translation units (tuples) using the extract-tuples method described in (Crego et al., 2004). To train the Ngram models, we used the SRILM toolkit (Stolcke, 2002). The type of discounting algorithm used was the modiﬁed 1 Kneser-Ney combining higher and lower order estimates via interpolation. The weights λi for the log-linear combination of models were set in order to minimize the BLEU score (Papineni et al., 2001) on the development set, using the simplex (Nelder and Mead, 1965) algorithm. All the experiments were performed on a Pentium IV (Xeon 3.06GHz), with 4Gb of RAM memory. www.slt.atr.jp/IWSLT2004 287 5.3 Results Conﬁg. baseline tpl.mon tpl.reo utpl.mon utpl.reo BLEU 28.85 33.14 36.33 33.16 37.82 mWER 53.42 51.5 49.68 50.16 47.31 mPER 42.89 41.53 41.41 41.25 39.9 time 11 333 12 354 Table 3: Translation quality (using BLEU, mWER and mPER) and eﬃciency (measured in decoding time) results. The ﬁrst row shows the results of the baseline TALP system. Table 3 shows the results in BLEU, mWER and mPER, a"
2006.iwslt-evaluation.17,W05-0820,0,0.0116848,", namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes. 1. Introduction Rooted in the Finite-State Transducers approach to SMT [1, 2] and estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternative to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3]. This is specially true when dealing with pairs of languages with a relatively similar word order [4, 5]. Given the language pairs involved in this year’s evaluation, efforts have been focused on improving the word reordering strategies for Ngram-based SMT. Specifically, a novel reordering strategy based on extending the search graph with automatically-extracted reordering patterns is explored. Results are very promising while keeping computational expenses at a similar level of monotone search. Additionally, a novel tuple segmentation strategy based on the entropy of Part-Of-Speech distributions was used with slight improvements in model estimation. This paper is organized as follows. Section 2"
2006.iwslt-evaluation.17,W06-3114,0,0.0159274,", namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes. 1. Introduction Rooted in the Finite-State Transducers approach to SMT [1, 2] and estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternative to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in [3]. This is specially true when dealing with pairs of languages with a relatively similar word order [4, 5]. Given the language pairs involved in this year’s evaluation, efforts have been focused on improving the word reordering strategies for Ngram-based SMT. Specifically, a novel reordering strategy based on extending the search graph with automatically-extracted reordering patterns is explored. Results are very promising while keeping computational expenses at a similar level of monotone search. Additionally, a novel tuple segmentation strategy based on the entropy of Part-Of-Speech distributions was used with slight improvements in model estimation. This paper is organized as follows. Section 2"
2006.iwslt-evaluation.17,2005.mtsummit-papers.36,1,0.472143,"g patterns. Section 4 focuses on tuple segmentation strategies, and contrasts the criterion on IBM model 1 probabilities from 2005 with a novel criterion based on Part-Of-Speech entropy distributions. Later on, Section 5 reports on all experiments carried out from Arabic, Chinese, Italian and Japanese into English for IWSLT 2006. Finally, Section 6 sums up the main conclusions from the paper and discusses future research lines. 2. 2005 system review The TALP Ngram-based SMT system performs a log-linear combination of a translation model and additional feature functions (see further details in [6, 7]). In contrast to phrasebased models, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel"
2006.iwslt-evaluation.17,2005.iwslt-1.23,1,0.868702,"l and additional feature functions (see further details in [6, 7]). In contrast to phrasebased models, our translation model is estimated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints 116 3. Word ordering strategies 2.2. Feature functions As additional feature functions to better guide the translation process, the system incorporates a target language model, a word bonus model and two lexicon models. The target language model is estimated"
2006.iwslt-evaluation.17,N04-1033,0,0.120187,"ated as a standard n-gram model of a bilingual language expressed in tuples. This way it approximates the joint probability between source and target languages capturing bilingual context, as described by the following equation: p(sJ1 , tI1 ) = K Y p((s, t)i |(s, t)i−N +1 , ..., (s, t)i−1 ) (1) i=1 where (s, t)i refers to the ith tuple of a sentence pair being segmented into K tuples. A detailed comparison between Ngram-based and phrase-based SMT can be found in [8]. 2.1. Tuple extraction Given a certain word-aligned parallel corpus, tuples are extracted according to the following constraints [9]: • a monotonic segmentation of each bilingual sentence pair is produced • no word in a tuple is aligned to words outside of it • no smaller tuples can be extracted without violating the previous constraints 116 3. Word ordering strategies 2.2. Feature functions As additional feature functions to better guide the translation process, the system incorporates a target language model, a word bonus model and two lexicon models. The target language model is estimated as a standard ngram over the target words, as follows: pLM (tk ) ≈ k Y p(wn |wn−N +1 , ..., wn−1 ) (2) n=1 where tk refers to the par"
2006.iwslt-evaluation.17,2005.mtsummit-papers.37,1,0.788658,"n |wn−N +1 , ..., wn−1 ) (2) n=1 where tk refers to the partial hypothesis and wn to the nth word in it. Usually, this feature is accompanied by a word bonus model based on sentence length, compensating the target language model preference for short sentences (in number of target words). This bonus depends on the number of target words in the partial hypothesis, denoted as: pW P (tk ) = exp(number of words in tk ) When dealing with pairs of languages with non-monotonic word order, a certain reordering strategy is required. Apart from that, tuples need to be extracted by an unfolding technique [11]. This means that the tuples are broken into smaller tuples, and these are sequenced in the order of the target words. In order not to lose the information on the correct order, the decoder performs then a reordered search (or a monotone search extended with reordering paths), which is guided by the n-gram model of the unfolded tuples and the additional feature models. Figure 1 shows an example of tuple unfolding compared to the monotonic extraction. The unfolding technique produces a different bilingual n-gram language model with reordered source words. (3) where tk refers to the partial hypo"
2006.iwslt-evaluation.17,A00-1031,0,0.0103336,"Language-dependent preprocessing For all language pairs, training sentences were split by using final dots on both sides of the bilingual text (when the number of dots was equal), increasing the number of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. English Table 1: Arabic→English corpus statistics. sent. it en it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronom"
2006.iwslt-evaluation.17,N06-2013,0,0.0357882,"mber of sentences and reducing its length. Specific preprocessing for each language is detailed in the following respective section. 5.2.1. English Table 1: Arabic→English corpus statistics. sent. it en it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. The Arabic Treebank tag set used contains 20 different tags. Corpora statistics for all language pairs can be found in 2 Ver"
2006.iwslt-evaluation.17,P05-1071,0,0.0218458,"n it it it it wrds 155k 166k 5,193 2,807 5,978 5,767 refs. 1 7 16 7 7 Table 2: Chinese→English corpus statistics. English preprocessing includes Part-Of-Speech tagging using freely-available TnT tagger [15] and lemmatization using wnmorph, included in the WordNet package [16]. The English Penn Treebank Tag Set used contains 36 different tags. 5.2.2. Arabic Following a similar approach to that in [17], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [18], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. The Arabic Treebank tag set used contains 20 different tags. Corpora statistics for all language pairs can be found in 2 Version 119 2.0. Linguistic Data Consortium Catalog: LDC2004L02. 5.2.3. Chinese official Chinese preprocessing included resegmentation and POStagging. These tasks were done by using ICTCLAS [19]. Resultan"
2006.iwslt-evaluation.17,W03-1730,0,0.0274114,"Missing"
2006.iwslt-evaluation.17,2004.iwslt-evaluation.14,1,\N,Missing
2006.iwslt-evaluation.17,E99-1010,0,\N,Missing
2006.iwslt-evaluation.17,J96-1002,0,\N,Missing
2006.iwslt-evaluation.17,W05-0823,1,\N,Missing
2006.iwslt-evaluation.17,N07-2022,1,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.1,0,\N,Missing
2006.iwslt-evaluation.17,J06-4004,1,\N,Missing
2006.iwslt-evaluation.17,N03-1017,0,\N,Missing
2006.iwslt-evaluation.17,J03-1002,0,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-evaluation.1,0,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.24,1,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-papers.2,1,\N,Missing
2006.iwslt-evaluation.17,atserias-etal-2006-freeling,0,\N,Missing
2006.iwslt-evaluation.17,2005.iwslt-1.11,0,\N,Missing
2006.iwslt-evaluation.17,P00-1056,0,\N,Missing
2006.iwslt-evaluation.17,2006.iwslt-papers.5,1,\N,Missing
2006.iwslt-evaluation.18,2005.iwslt-1.24,1,0.787344,"TALP phrase-based statistical machine translation system, enriched with the statistical machine reordering technique. We also report the combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report th"
2006.iwslt-evaluation.18,W06-1609,1,0.921033,"e combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to Eng"
2006.iwslt-evaluation.18,W06-3125,1,0.836838,"ional Workshop on Spoken Language Translation. 1. Introduction This paper describes the TALP-phrase system for the IWSLT 2006, which is an enhanced version of the system reported in the 2005 evaluation [1]. The main difference is the integration of a new reordering technique called statistical machine reordering, which was presented in [2] in a different framework. Additionally, we report the results of combining the outputs of the two statistical machine translation TALP systems: phrase-based and n-gram-based. The latter of the two also participated in the 2005 evaluation and is described in [3]. Statistical machine translation systems are now usually modelled through a log-linear maximum entropy framework. e˜ = argmax e ( M X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from th"
2006.iwslt-evaluation.18,P02-1038,0,0.0577834,"re consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. (1) The feature functions, hm , and weights, λi , are typically optimized to maximize the scoring function [4]. Two basic issues differentiate the n-gram-based system from the phrase-based system: the bilingual units are extracted from a monotonic segmentation of the training data; the unit probabilities are based on a standard back-off language model rather than directly on relative frequencies. In both systems, the introduction of reordering capabilities is crucial for certain language pairs. This paper is organized as follows. Section 2 describes the TALP-phrase system, with particular emphasis on a new reordering technique: the statistical machine reordering approach. In Section 3, we combine the"
2006.iwslt-evaluation.18,J04-4002,0,0.0302302,"X m=1 λm hm (e, f ) ) TALP-tuple. Finally, in Section 4, we report the results obtained for all the tasks of the evaluation, which include the translations from Chinese, Arabic, Italian and Japanese to English. 2. Description of the TALP-phrase System 2.1. Phrase-based Model The basic idea of phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from these phrase translations. Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in [5]. A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints (1) words are consecutive along both sides of the bilingual phrase and (2) no word on either side of the phrase is aligned to a word outside the phrase. 2.2. Feature functions The baseline phrase-based system implements a log-linear combination of four feature functions, which are described as follows. • The translation model is estimated with relative frequencies. Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequen"
2006.iwslt-evaluation.18,2005.iwslt-1.6,0,0.0322379,"using the GIZA++ tool [7]. During word alignment, we used 50 classes per language. We aligned both translation directions and combined the two alignments with the union operation. train dev4 dev123 test ASRtest • Word classes (which were used to help the aligner and to perform the SMR process) were determined using “mkcls”, a tool freely-available with GIZA++. • The language model was estimated using the SRILM toolkit [8]. • The decoder was MARIE [9]. • The optimization tool used for computing log-linear weights was based on the simplex method [6]. Following the consensus strategy proposed in [10], the objective function was set to 100 · BLEU + 4 · N IST . 489 500 500 500 voc. 9.7k 9.6k 1,096 909 1,292 1,311 slen. 6.7 7.0 11.2 6.0 11.7 11.6 refs. 1 7 16 7 7 Corpus statistics for all language pairs can be found in Tables 1, 2, 3 and 4, respectively, where number of sentences, running words, vocabulary, sentence length and human references are shown. sent. train dev4 dev123 test ASRtest Experiments were carried out for all tasks of the IWSLT06 evaluation (Zh2En, Jp2En, Ar2En and It2En) using the BTEC Corpus provided for the open data track1 . it en it it it it 24.6k 489 500 500 500 wrds"
2006.iwslt-evaluation.18,A00-1031,0,0.0229282,"while randomly selecting 500 sentences from developments 1, 2 and 3 (around 160 sentences from each) to build an internal test set (dev123). zh en zh zh zh zh 4.4. Language-dependent preprocessing For all language pairs, training sentences were split by using full stops on both sides of the bilingual text (when the number of stops was equal), increasing the number of sentences and reducing their length. Specific preprocessing for each language is detailed in the respective section below. 4.4.1. English English preprocessing includes part-of-speech tagging using the freely-available TnT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly p"
2006.iwslt-evaluation.18,N06-2013,0,0.0411805,"ing their length. Specific preprocessing for each language is detailed in the respective section below. 4.4.1. English English preprocessing includes part-of-speech tagging using the freely-available TnT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. search (with m = 5 and j = 3) for all tasks and for all systems (with or without SMR technique); except for the Italian to E"
2006.iwslt-evaluation.18,P05-1071,0,0.0382995,"nT tagger [11] and lemmatization using wnmorph, included in the WordNet package [12]. 126 sent. train dev4 dev123 test ASRtest jp en jp jp jp jp 45.2k 489 500 500 500 wrds 390k 325k 6,758 3,818 7,367 7,494 voc. 10.6k 9.6k 1,169 936 1,301 1,331 slen. 8.6 7.2 13.8 7.6 14.7 15.0 refs. 1 7 16 7 7 Table 4: Japanese→English corpus statistics. 4.4.2. Arabic Following a similar approach to that taken in [13], we use the Buckwalter Arabic Morphological Analyzer2 to obtain possible word analyses for Arabic, and disambiguate them using the Morphological Analysis and Disambiguation for Arabic (MADA) tool [14], kindly provided by the University of Columbia. Once analyzed, Arabic words are segmented by separating all prefixes (prepositions, conjunctions, the article and the future marker) and suffixes (pronominal clitics). The tool also provides POS tags for the resultant tokens. search (with m = 5 and j = 3) for all tasks and for all systems (with or without SMR technique); except for the Italian to English task where a monotonic search was used. The primary system of each task is that which had the best performance in the internal test. In all tasks, the SMR improved the results in the internal te"
2006.iwslt-evaluation.18,W03-1730,0,0.0516051,"Missing"
2006.iwslt-evaluation.18,atserias-etal-2006-freeling,0,0.0138517,"for the internal test set (specially, for the Arabic and Japanese tasks). The higher the number of unknown words, the worse the SMR output and, consequently, the quality of translation. Here, a possible solution would be to predict word classes for unknown words in order to avoid their bad influence in the SMR output. 4.4.3. Chinese Set development test evaluation Chinese preprocessing included re-segmentation and POStagging. These tasks were performed using ICTCLAS [15]. 4.4.4. Italian Italian was POS-tagged and lemmatized using the freelyavailable FreeLing morpho-syntactic analysis package [16]. Additionally, Italian contracted prepositions were separated into preposition + article, for example ’alla’→’a la’, ’degli’→’di gli’ or ’dallo’→’da lo’. 4.4.5. Japanese When dealing with Japanese, one has to come up with new methods for overcoming the absence of delimiters between words. We addressed this issue by word segmentation using the freely available JUMAN tool [17] version 5.1. This tool was also used for POS-tagging of the Japanese text. 4.5. Results In Table 6 we show the results for all the TALP systems that participated in the IWSLT 2006: the TALP-phrase, the TALP-tuple and the"
2006.iwslt-evaluation.18,2004.iwslt-evaluation.8,0,\N,Missing
2012.eamt-1.34,D07-1090,0,0.0231043,"urced human adequacy judgments. 5.1 MT Systems We used state-of-the art Arabic-English MT systems with widely different implementations. MT A was built using HiFST (de Gispert et al., 115 2010), a hierarchical phrase-based SMT system implemented using finite state transducers. It is trained on all the parallel corpora in the NIST MT08 Arabic Constrained Data track (5.9M parallel sentences, 150M words per language). The first-pass 4-gram language model (LM) is trained on the English side of the parallel text and a subset of Gigaword 3. The second-pass 5-gram LM is a zero-cutoff stupid-backoff (Brants et al., 2007) estimated using 6.6B words of English newswire text. MT B was built using Moses (Koehn et al., 2007), and is a non-hierarchical phrase-based system. It is trained on 3.2M sentences of parallel text (65M words on the English side) using several LDC corpora including some available only through the GALE program (e.g., LDC2004T17, LDC2004E72, LDC2005E46 and LDC2004T18). The data includes some sentences from the ISI corpus (LDC2007T08) and UN corpus (LDC2004E13) selected to specifically add vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN"
2012.eamt-1.34,W07-0718,0,0.119192,"Missing"
2012.eamt-1.34,condon-etal-2010-evaluation,0,0.0924787,"Missing"
2012.eamt-1.34,J10-3008,1,0.869312,"Missing"
2012.eamt-1.34,W11-2107,0,0.0302277,"LDC2004E13) selected to specifically add vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN system (Habash et al., 2009). Lemmas are used for Giza++ alignment only. The tokenization scheme used is the Penn Arabic Treebank scheme (Habash, 2010; Sadat and Habash, 2006). The system uses a 5-gram LM that was trained on Gigaword 4. Both systems are tuned for BLEU score using MERT. 5.2 Automatic and Human Evaluation We ran several automatic metrics on the baseline MT output and the post-edited MT output: BLEU (Papineni et al., 2002), Meteor-a (Denkowski and Lavie, 2011) and TERp-a (Snover et al., 2009). BLEU is based on n-gram precision, while Meteor takes both precision and recall into account. TERp also implicitly takes precision and recall into account, since it is similar to edit distance. Both Meteor and TERp allow more flexible n-gram matching than BLEU, since they allow matching across stems, synonyms and paraphrases. Meteor-a and TERp-a are both tuned to have high correlation with human adequacy judgments. In contrast to automatic system-level metrics, human judgments can give a nuanced sentencelevel view of particular aspects of the MT. In order to"
2012.eamt-1.34,2008.amta-govandcom.8,0,0.625824,"Missing"
2012.eamt-1.34,2006.eamt-1.27,0,0.119724,"Missing"
2012.eamt-1.34,P05-1045,0,0.00482056,"ons for the errors, and 3) apply the suggestions. All the APEs use identical algorithms for steps 1 and 2, and only differ in how they apply the suggestions. The algorithms are language-pair independent, though we carried out all of our experiments on Arabic-English MT. 4.1 Pre-Processing The Arabic source text was analyzed and tokenized using MADA+TOKAN (Habash et al., 2009). Each MT system used a different tokenization scheme, so the source sentences were processed in two separate pipelines. Separate named entity recognizers (NER) were built for each pipeline using the Stanford NER toolkit (Finkel et al., 2005), by training on CoNLL and ACE data. Each translated English sentence was re-cased using Moses and then analyzed using the Stanford CoreNLP pipeline to get part-of-speech (POS) tags (Toutanova et al., 2003) and NER (Finkel et al., 2005). 4.2 Detecting Errors and Suggesting Corrections The APEs address specific adequacy errors that we have found to be most detrimental for the CLQA task: content words that are not translated at all, content words that are translated to function words, and mistranslated named entities. In the error detection step, these types of errors are detected via an algorit"
2012.eamt-1.34,2007.mtsummit-papers.34,0,0.285131,"Missing"
2012.eamt-1.34,P07-2045,0,0.0106204,"widely different implementations. MT A was built using HiFST (de Gispert et al., 115 2010), a hierarchical phrase-based SMT system implemented using finite state transducers. It is trained on all the parallel corpora in the NIST MT08 Arabic Constrained Data track (5.9M parallel sentences, 150M words per language). The first-pass 4-gram language model (LM) is trained on the English side of the parallel text and a subset of Gigaword 3. The second-pass 5-gram LM is a zero-cutoff stupid-backoff (Brants et al., 2007) estimated using 6.6B words of English newswire text. MT B was built using Moses (Koehn et al., 2007), and is a non-hierarchical phrase-based system. It is trained on 3.2M sentences of parallel text (65M words on the English side) using several LDC corpora including some available only through the GALE program (e.g., LDC2004T17, LDC2004E72, LDC2005E46 and LDC2004T18). The data includes some sentences from the ISI corpus (LDC2007T08) and UN corpus (LDC2004E13) selected to specifically add vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN system (Habash et al., 2009). Lemmas are used for Giza++ alignment only. The tokenization scheme used"
2012.eamt-1.34,P09-2084,0,0.399041,"ionary extracted from the Buckwalter analyzer (Buckwalter, 2004) and an English synonym dictionary from the CIA World Factbook.1 They are high precision and low recall: most errors do not have matches in the dictionaries, but when they do, they are often correct, particularly for NEs. 1 http://www.cia.gov/library/publications/the-world-factbook 113 Background MT corpus: Since our motivation is CLQA, we also draw on a resource specific to CLQA: a background corpus of about 120,000 Arabic newswire and web documents that have been translated into English by a state-of-the-art industry MT system. Ma and McKeown (2009) were able to exploit a similar pseudo-parallel corpus to correct deleted verbs, since words deleted in one sentence are frequently correctly translated in other sentences. For each error, the source-language phrase is converted into a query to search all three resources. Then the target-language results are aggregated and ranked by overall confidence scores. The confidence scores are a weighted combination of phrase translation probability, number of dictionary matches and term frequencies in the background corpus. The weights were set manually on a development corpus. 4.3 Rule-Based APE Tabl"
2012.eamt-1.34,P02-1040,0,0.0914811,"orpus (LDC2007T08) and UN corpus (LDC2004E13) selected to specifically add vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN system (Habash et al., 2009). Lemmas are used for Giza++ alignment only. The tokenization scheme used is the Penn Arabic Treebank scheme (Habash, 2010; Sadat and Habash, 2006). The system uses a 5-gram LM that was trained on Gigaword 4. Both systems are tuned for BLEU score using MERT. 5.2 Automatic and Human Evaluation We ran several automatic metrics on the baseline MT output and the post-edited MT output: BLEU (Papineni et al., 2002), Meteor-a (Denkowski and Lavie, 2011) and TERp-a (Snover et al., 2009). BLEU is based on n-gram precision, while Meteor takes both precision and recall into account. TERp also implicitly takes precision and recall into account, since it is similar to edit distance. Both Meteor and TERp allow more flexible n-gram matching than BLEU, since they allow matching across stems, synonyms and paraphrases. Meteor-a and TERp-a are both tuned to have high correlation with human adequacy judgments. In contrast to automatic system-level metrics, human judgments can give a nuanced sentencelevel view of part"
2012.eamt-1.34,C10-2109,1,0.93042,"was re-cased using Moses and then analyzed using the Stanford CoreNLP pipeline to get part-of-speech (POS) tags (Toutanova et al., 2003) and NER (Finkel et al., 2005). 4.2 Detecting Errors and Suggesting Corrections The APEs address specific adequacy errors that we have found to be most detrimental for the CLQA task: content words that are not translated at all, content words that are translated to function words, and mistranslated named entities. In the error detection step, these types of errors are detected via an algorithm from prior work that uses bilingual POS tags and word alignments (Parton and McKeown, 2010). Each flagged error consists of one or more source-language tokens and zero or more target-language tokens. In the error correction step, the source and target sentences and all the flagged errors are passed to the suggestion generator, which uses the following three resources. Phrase Table: The phrase table from MT B is used as a phrase dictionary (described in more detail in ??). Dictionaries: We also use a translation dictionary extracted from Wikipedia, a bilingual name dictionary extracted from the Buckwalter analyzer (Buckwalter, 2004) and an English synonym dictionary from the CIA Worl"
2012.eamt-1.34,W07-0707,0,0.119951,"Missing"
2012.eamt-1.34,P06-1001,1,0.825975,"is trained on 3.2M sentences of parallel text (65M words on the English side) using several LDC corpora including some available only through the GALE program (e.g., LDC2004T17, LDC2004E72, LDC2005E46 and LDC2004T18). The data includes some sentences from the ISI corpus (LDC2007T08) and UN corpus (LDC2004E13) selected to specifically add vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN system (Habash et al., 2009). Lemmas are used for Giza++ alignment only. The tokenization scheme used is the Penn Arabic Treebank scheme (Habash, 2010; Sadat and Habash, 2006). The system uses a 5-gram LM that was trained on Gigaword 4. Both systems are tuned for BLEU score using MERT. 5.2 Automatic and Human Evaluation We ran several automatic metrics on the baseline MT output and the post-edited MT output: BLEU (Papineni et al., 2002), Meteor-a (Denkowski and Lavie, 2011) and TERp-a (Snover et al., 2009). BLEU is based on n-gram precision, while Meteor takes both precision and recall into account. TERp also implicitly takes precision and recall into account, since it is similar to edit distance. Both Meteor and TERp allow more flexible n-gram matching than BLEU,"
2012.eamt-1.34,N07-1064,0,0.200693,"Missing"
2012.eamt-1.34,W09-0441,0,0.0362319,"d vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN system (Habash et al., 2009). Lemmas are used for Giza++ alignment only. The tokenization scheme used is the Penn Arabic Treebank scheme (Habash, 2010; Sadat and Habash, 2006). The system uses a 5-gram LM that was trained on Gigaword 4. Both systems are tuned for BLEU score using MERT. 5.2 Automatic and Human Evaluation We ran several automatic metrics on the baseline MT output and the post-edited MT output: BLEU (Papineni et al., 2002), Meteor-a (Denkowski and Lavie, 2011) and TERp-a (Snover et al., 2009). BLEU is based on n-gram precision, while Meteor takes both precision and recall into account. TERp also implicitly takes precision and recall into account, since it is similar to edit distance. Both Meteor and TERp allow more flexible n-gram matching than BLEU, since they allow matching across stems, synonyms and paraphrases. Meteor-a and TERp-a are both tuned to have high correlation with human adequacy judgments. In contrast to automatic system-level metrics, human judgments can give a nuanced sentencelevel view of particular aspects of the MT. In order to compare adequacy across APEs, we"
2012.eamt-1.34,2011.mtsummit-papers.58,0,0.271781,"Missing"
2012.eamt-1.34,stymne-ahrenberg-2010-using,0,0.0853294,"Missing"
2012.eamt-1.34,2011.mtsummit-papers.16,0,0.0686158,"Missing"
2012.eamt-1.34,N03-1033,0,0.00674048,"though we carried out all of our experiments on Arabic-English MT. 4.1 Pre-Processing The Arabic source text was analyzed and tokenized using MADA+TOKAN (Habash et al., 2009). Each MT system used a different tokenization scheme, so the source sentences were processed in two separate pipelines. Separate named entity recognizers (NER) were built for each pipeline using the Stanford NER toolkit (Finkel et al., 2005), by training on CoNLL and ACE data. Each translated English sentence was re-cased using Moses and then analyzed using the Stanford CoreNLP pipeline to get part-of-speech (POS) tags (Toutanova et al., 2003) and NER (Finkel et al., 2005). 4.2 Detecting Errors and Suggesting Corrections The APEs address specific adequacy errors that we have found to be most detrimental for the CLQA task: content words that are not translated at all, content words that are translated to function words, and mistranslated named entities. In the error detection step, these types of errors are detected via an algorithm from prior work that uses bilingual POS tags and word alignments (Parton and McKeown, 2010). Each flagged error consists of one or more source-language tokens and zero or more target-language tokens. In"
2012.eamt-1.34,vilar-etal-2006-error,0,0.177624,"Missing"
2012.eamt-1.34,W11-2152,0,\N,Missing
2012.eamt-1.34,2010.iwslt-keynotes.2,0,\N,Missing
2012.eamt-1.34,P10-2033,1,\N,Missing
arranz-etal-2004-bilingual,ide-etal-2000-xces,0,\N,Missing
arranz-etal-2004-bilingual,W02-1012,0,\N,Missing
arranz-etal-2004-bilingual,N01-1026,0,\N,Missing
arranz-etal-2004-bilingual,W02-0808,0,\N,Missing
C08-2005,P03-1006,0,0.0847665,"Missing"
C08-2005,N03-1019,1,0.836264,"Missing"
C08-2005,H05-1021,1,\N,Missing
C08-2005,D07-1090,0,\N,Missing
C08-2005,N03-1017,0,\N,Missing
C08-2005,P03-1021,0,\N,Missing
C10-1009,P09-1019,0,0.0158161,"orated directly into this framework. If the MBR hypothe¯ for sis space H contains a generated hypothesis E ¯ ¯ which P (F |E) = 0, E could still be produced as a translation, since it can be ‘voted for’ by nearby hypotheses produced by the underlying system. Table 3 shows the proportion of NIST testset sentences that can be aligned to any of the reference translations using our high quality baseline hierarchical decoder with a powerful grammar. The low level of reachability suggests that NLG may be required to achieve high levels of translation quality and fluency. Other rescoring approaches (Kumar et al., 2009; Li et al., 2009) may also benefit from NLG when the baseline is incapable of generating the reference. We note that our approach could also be used to improve the fluency of ASR, OCR and other language processing tasks where the goal is to produce fluent natural language output. 9 Summary and Discussion We have described a general framework for improving SMT fluency. Decoupling the hypothesis space from the evidence space allows for much greater flexibility in lattice MBR search. We have shown that high path posterior probability n-grams in the ML translation can be used to guide the segment"
C10-1009,P03-1006,0,0.0311514,"Missing"
C10-1009,P10-2006,1,0.888231,"Missing"
C10-1009,P09-1067,0,0.0120818,"this framework. If the MBR hypothe¯ for sis space H contains a generated hypothesis E ¯ ¯ which P (F |E) = 0, E could still be produced as a translation, since it can be ‘voted for’ by nearby hypotheses produced by the underlying system. Table 3 shows the proportion of NIST testset sentences that can be aligned to any of the reference translations using our high quality baseline hierarchical decoder with a powerful grammar. The low level of reachability suggests that NLG may be required to achieve high levels of translation quality and fluency. Other rescoring approaches (Kumar et al., 2009; Li et al., 2009) may also benefit from NLG when the baseline is incapable of generating the reference. We note that our approach could also be used to improve the fluency of ASR, OCR and other language processing tasks where the goal is to produce fluent natural language output. 9 Summary and Discussion We have described a general framework for improving SMT fluency. Decoupling the hypothesis space from the evidence space allows for much greater flexibility in lattice MBR search. We have shown that high path posterior probability n-grams in the ML translation can be used to guide the segmentation of a lattice"
C10-1009,D07-1090,0,0.0182313,"newswire lattices. of the odd numbered sentences of the MT02– MT05 testsets; the even numbered sentences form test. MT08 performance on nw08 (newswire) and ng08 (newsgroup) data is also reported. First-pass translation is performed using HiFST (Iglesias et al., 2009), a hierarchical phrase-based decoder. The first-pass LM is a modified KneserNey (Kneser and Ney, 1995) 4-gram estimated over the English side of the parallel text and an 881M word subset of the English GigaWord 3rd Edition. Prior to LMBR, the first-pass lattices are rescored with zero-cutoff stupid-backoff 5-gram language models (Brants et al., 2007) estimated over more than 6B words of English text. The LMBR factors θ0 , . . . , θ4 are set as in Tromble et al. (2008) using unigram precision p = 0.85 and recall ratio r = 0.74. The effect of performing LMBR over the segmented hypothesis space is shown in Table 1. The hypothesis subspaces Hr are constructed at various confidence thresholds as described in Section 4 with H formed via Equation (7); no coverage constraints are applied yet. Constraining the search space using β = 0.6 leads to little degradation in LMBR performance under BLEU. This shows lattice segmentation works as intended. W"
C10-1009,ma-cieri-2006-corpus,0,0.0124009,"srisk decoding framework. By segmenting translation lattices according to confidence measures over the maximum likelihood translation hypothesis we are able to focus on regions with potential translation errors. Hypothesis space constraints based on monolingual coverage are applied to the low confidence regions to improve overall translation fluency. 1 Introduction and Motivation Translation quality is often described in terms of fluency and adequacy. Fluency reflects the ‘nativeness’ of the translation while adequacy indicates how well a translation captures the meaning of the original text (Ma and Cieri, 2006). From a purely utilitarian view, adequacy should be more important than fluency. But fluency and adequacy are subjective and not easy to tease apart (Callison-Burch et al., 2009; Vilar et al., 2007). There is a human tendency to rate less fluent translations as less adequate. One explanation is that errors in grammar cause readers to be more critical. A related phenomenon is that the nature of translation errors changes as fluency improves so that any errors in fluent translations must be relatively subtle. It is therefore not enough to focus solely on adequacy. SMT systems must also be fluen"
C10-1009,E06-1005,0,0.029155,"L translation can be used to guide the segmentation of a lattice into regions of high and low confidence. Segmenting the lattice simplifies the process of refining the hypothesis space since low confidence regions can be refined in the context of their high confidence neighbours. This can be done independently before reassembling the refined regions. Lattice segmentation facilitates the application of post-processing and rescoring techniques targeted to address particular deficiencies in ML decoding. The techniques we presented are related to consensus decoding and system combination for SMT (Matusov et al., 2006; Sim et al., 2007), and to segmental MBR for automatic speech recognition (Goel et al., 2004). Mohit et al. (2009) describe an alternative approach to improving specific portions of translation hypotheses. They use an SVM classifier to identify a single phrase in each source language sentence that is “difficult to translate”; such phrases are then translated using an adapted language model estimated from parallel data. In contrast to their approach, our approach is able to exploit large collections of monolingual data to refine multiple low confidence regions using posterior probabilities obt"
C10-1009,J90-2002,0,0.623924,"obust, in that a translation is nearly always produced. But unlike translators who should be skilled in at least one of the languages, SMT systems are limited in both source and target language competence. Fluency and accuracy therefore tend to suffer together as translation quality degrades. This should not be the case. Ideally, an SMT system should never be any less fluent than the best stochastic text generation system available in the target language (Oberlander and Brew, 2000). What is needed is a good way to enhance the fluency of SMT hypotheses. The maximum likelihood (ML) formulation (Brown et al., 1990) of translation of source lanˆ guage sentence F to target language sentence E A novel and robust approach to improving statistical machine translation fluency is developed within a minimum Bayesrisk decoding framework. By segmenting translation lattices according to confidence measures over the maximum likelihood translation hypothesis we are able to focus on regions with potential translation errors. Hypothesis space constraints based on monolingual coverage are applied to the low confidence regions to improve overall translation fluency. 1 Introduction and Motivation Translation quality is o"
C10-1009,2009.eamt-1.22,0,0.0149753,"the lattice simplifies the process of refining the hypothesis space since low confidence regions can be refined in the context of their high confidence neighbours. This can be done independently before reassembling the refined regions. Lattice segmentation facilitates the application of post-processing and rescoring techniques targeted to address particular deficiencies in ML decoding. The techniques we presented are related to consensus decoding and system combination for SMT (Matusov et al., 2006; Sim et al., 2007), and to segmental MBR for automatic speech recognition (Goel et al., 2004). Mohit et al. (2009) describe an alternative approach to improving specific portions of translation hypotheses. They use an SVM classifier to identify a single phrase in each source language sentence that is “difficult to translate”; such phrases are then translated using an adapted language model estimated from parallel data. In contrast to their approach, our approach is able to exploit large collections of monolingual data to refine multiple low confidence regions using posterior probabilities obtained from a high-quality evidence space of first-pass translations. Acknowledgments We would like to thank Matt Gi"
C10-1009,J07-2003,0,0.0468446,"t modelling problem. The language model P (E), the closest thing to a ‘fluency component’ in the original formulation, only affects candidates likely under the translation model P (F |E). Given the weakness of current translation models this is a severe limitation. It often happens that SMT sys¯ = 0 to a correct reference tems assign P (F |E) ¯ translation E of F (see the discussion in Section 9). The problem is that in ML decoding the language model can only encourage the production of fluent translations; it cannot easily enforce constraints on fluency or introduce new hypotheses. In Hiero (Chiang, 2007) and syntax-based SMT (Knight and Graehl, 2005; Knight, 2007a), the primary role of syntax is to drive the translation process. Translations produced by these systems respect the syntax of their translation models, but 71 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 71–79, Beijing, August 2010 strings amongst the vast number of hypotheses encoded in SMT lattices. Oracle BLEU scores computed over k-best lists (Och et al., 2004) show that many high quality hypotheses are produced by first-pass SMT decoding. We propose reducing the difficulty"
C10-1009,N04-1021,0,0.0635666,"oduction of fluent translations; it cannot easily enforce constraints on fluency or introduce new hypotheses. In Hiero (Chiang, 2007) and syntax-based SMT (Knight and Graehl, 2005; Knight, 2007a), the primary role of syntax is to drive the translation process. Translations produced by these systems respect the syntax of their translation models, but 71 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 71–79, Beijing, August 2010 strings amongst the vast number of hypotheses encoded in SMT lattices. Oracle BLEU scores computed over k-best lists (Och et al., 2004) show that many high quality hypotheses are produced by first-pass SMT decoding. We propose reducing the difficulty of enhancing the fluency of complete hypotheses by first identifying regions of highconfidence in the ML translations and using these to guide the fluency refinement process. This has two advantages: (1) we keep portions of the baseline hypotheses that we trust and search for alternatives elsewhere, and (2) the task is made much easier since the fluency of sentence fragments can be refined in context. In what follows, we use posterior probabilities over SMT lattices to identify u"
C10-1009,P09-1064,0,0.0114735,"that exceed β. Even at this very high confidence level, high posterior n-grams occur frequently enough that we can expect them to be useful. These precision results motivate our use of path posterior n-gram probabilities as a confidence measure. We assign confidence p(Eˆij |E) to subˆi . . . E ˆj of the ML hypothesis. sequences E Prior work focuses on word-level confidence extracted from k-best lists and lattices (Ueffing and Ney, 2007), while Zens and Ney (2006) rescore k-best lists with n-gram posterior probabilities. Similar experiments with a slightly different motivation are reported by DeNero et al. (2009); they show that expected n-gram counts in a lattice can be used to predict which n-grams appear in the references. 3 Posterior Probability Confidence Measures In the formulation of Equations (3) and (4) the path posterior n-gram probabilities play a crucial role. MBR decoding under the linear approximation to BLEU is driven mainly by the presence of high posterior n-grams in the lattice; the low posterior n-grams contribute relatively little to the MBR decision criterion. Here we investigate the predictive power of these statistics. We will show that the n-gram posterior is a good predictor a"
C10-1009,P02-1040,0,0.0795703,"2009; Vilar et al., 2007). There is a human tendency to rate less fluent translations as less adequate. One explanation is that errors in grammar cause readers to be more critical. A related phenomenon is that the nature of translation errors changes as fluency improves so that any errors in fluent translations must be relatively subtle. It is therefore not enough to focus solely on adequacy. SMT systems must also be fluent if they are to be accepted and trusted. It is possible that the reliance on automatic metrics may have led SMT researchers to pay insufficient attention to fluency: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and METEOR (Lavie and Denkowski, 2009) show broad correlation with human rankings of MT quality, but are ˆ = argmax P (F |E)P (E) E (1) E makes it clear why improving SMT fluency is a difficult modelling problem. The language model P (E), the closest thing to a ‘fluency component’ in the original formulation, only affects candidates likely under the translation model P (F |E). Given the weakness of current translation models this is a severe limitation. It often happens that SMT sys¯ = 0 to a correct reference tems assign P (F |E) ¯ translation E of F (see the disc"
C10-1009,N09-1049,1,0.894712,"Missing"
C10-1009,2006.amta-papers.25,0,0.0317506,"here is a human tendency to rate less fluent translations as less adequate. One explanation is that errors in grammar cause readers to be more critical. A related phenomenon is that the nature of translation errors changes as fluency improves so that any errors in fluent translations must be relatively subtle. It is therefore not enough to focus solely on adequacy. SMT systems must also be fluent if they are to be accepted and trusted. It is possible that the reliance on automatic metrics may have led SMT researchers to pay insufficient attention to fluency: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and METEOR (Lavie and Denkowski, 2009) show broad correlation with human rankings of MT quality, but are ˆ = argmax P (F |E)P (E) E (1) E makes it clear why improving SMT fluency is a difficult modelling problem. The language model P (E), the closest thing to a ‘fluency component’ in the original formulation, only affects candidates likely under the translation model P (F |E). Given the weakness of current translation models this is a severe limitation. It often happens that SMT sys¯ = 0 to a correct reference tems assign P (F |E) ¯ translation E of F (see the discussion in Section 9). The p"
C10-1009,D08-1065,0,0.180822,"effect on translation robustness. This is because constraints on fluency do not affect the production of the evidence space by the baseline system. Robustness and fluency are no longer conflicting objectives. This framework also allows the MBR hypothesis space to be augmented with hypotheses produced by an NLG system, although this is beyond the scope of the present paper. This paper focuses on searching out fluent 2 Lattice MBR Decoding The formulation of the MBR decoder in Equation (2) separates the hypothesis space from the evidence space. We apply the linearised lattice MBR decision rule (Tromble et al., 2008)   X ′ ′ ˆLMBR = argmax θ0 |E |+ E θu #u (E )p(u|E) , E ′ ∈H u∈N (3) where H is the hypothesis space, E is the evidence space, N is the set of all n-grams in H (typically, n = 1 . . . 4), and θ are constants estimated on held-out data. The quantity p(u|E) is the path posterior probability of n-gram u X P (E|F ), (4) p(u|E) = E∈Eu where Eu = {E ∈ E : #u (E) > 0} is the subset of paths containing n-gram u at least once. The path posterior probabilities p(u|E) of Equation (4) can be efficiently calculated (Blackwood et al., 2010) using general purpose WFST operations (Mohri et al., 2002). 72 30"
C10-1009,J07-1003,0,0.022856,"are few n-grams with p(u|E) ≥ β; this is as expected. However, even at a high threshold of β = 0.9 there are still on average three 4-grams per sentence with posterior probabilities that exceed β. Even at this very high confidence level, high posterior n-grams occur frequently enough that we can expect them to be useful. These precision results motivate our use of path posterior n-gram probabilities as a confidence measure. We assign confidence p(Eˆij |E) to subˆi . . . E ˆj of the ML hypothesis. sequences E Prior work focuses on word-level confidence extracted from k-best lists and lattices (Ueffing and Ney, 2007), while Zens and Ney (2006) rescore k-best lists with n-gram posterior probabilities. Similar experiments with a slightly different motivation are reported by DeNero et al. (2009); they show that expected n-gram counts in a lattice can be used to predict which n-grams appear in the references. 3 Posterior Probability Confidence Measures In the formulation of Equations (3) and (4) the path posterior n-gram probabilities play a crucial role. MBR decoding under the linear approximation to BLEU is driven mainly by the presence of high posterior n-grams in the lattice; the low posterior n-grams con"
C10-1009,2007.mtsummit-ucnlg.1,0,0.215772,"ing to a ‘fluency component’ in the original formulation, only affects candidates likely under the translation model P (F |E). Given the weakness of current translation models this is a severe limitation. It often happens that SMT sys¯ = 0 to a correct reference tems assign P (F |E) ¯ translation E of F (see the discussion in Section 9). The problem is that in ML decoding the language model can only encourage the production of fluent translations; it cannot easily enforce constraints on fluency or introduce new hypotheses. In Hiero (Chiang, 2007) and syntax-based SMT (Knight and Graehl, 2005; Knight, 2007a), the primary role of syntax is to drive the translation process. Translations produced by these systems respect the syntax of their translation models, but 71 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 71–79, Beijing, August 2010 strings amongst the vast number of hypotheses encoded in SMT lattices. Oracle BLEU scores computed over k-best lists (Och et al., 2004) show that many high quality hypotheses are produced by first-pass SMT decoding. We propose reducing the difficulty of enhancing the fluency of complete hypotheses by first ide"
C10-1009,W07-0713,0,0.013575,"tion errors. Hypothesis space constraints based on monolingual coverage are applied to the low confidence regions to improve overall translation fluency. 1 Introduction and Motivation Translation quality is often described in terms of fluency and adequacy. Fluency reflects the ‘nativeness’ of the translation while adequacy indicates how well a translation captures the meaning of the original text (Ma and Cieri, 2006). From a purely utilitarian view, adequacy should be more important than fluency. But fluency and adequacy are subjective and not easy to tease apart (Callison-Burch et al., 2009; Vilar et al., 2007). There is a human tendency to rate less fluent translations as less adequate. One explanation is that errors in grammar cause readers to be more critical. A related phenomenon is that the nature of translation errors changes as fluency improves so that any errors in fluent translations must be relatively subtle. It is therefore not enough to focus solely on adequacy. SMT systems must also be fluent if they are to be accepted and trusted. It is possible that the reliance on automatic metrics may have led SMT researchers to pay insufficient attention to fluency: BLEU (Papineni et al., 2002), TE"
C10-1009,W06-3110,0,0.0360732,"β; this is as expected. However, even at a high threshold of β = 0.9 there are still on average three 4-grams per sentence with posterior probabilities that exceed β. Even at this very high confidence level, high posterior n-grams occur frequently enough that we can expect them to be useful. These precision results motivate our use of path posterior n-gram probabilities as a confidence measure. We assign confidence p(Eˆij |E) to subˆi . . . E ˆj of the ML hypothesis. sequences E Prior work focuses on word-level confidence extracted from k-best lists and lattices (Ueffing and Ney, 2007), while Zens and Ney (2006) rescore k-best lists with n-gram posterior probabilities. Similar experiments with a slightly different motivation are reported by DeNero et al. (2009); they show that expected n-gram counts in a lattice can be used to predict which n-grams appear in the references. 3 Posterior Probability Confidence Measures In the formulation of Equations (3) and (4) the path posterior n-gram probabilities play a crucial role. MBR decoding under the linear approximation to BLEU is driven mainly by the presence of high posterior n-grams in the lattice; the low posterior n-grams contribute relatively little t"
C10-1009,N04-1022,1,0.841632,"oad-coverage parsing needed for robust translation. We have described two problems in translation fluency: (1) SMT may fail to generate fluent hypotheses and there is no simple way to introduce them into the search; (2) SMT produces many translations which are not fluent but enforcing constraints to improve fluency can hurt robustness. Both problems are rooted in the ML decoding framework in which robustness and fluency are conflicting objectives. We propose a novel framework to improve the fluency of any SMT system, whether syntactic or phrase-based. We will perform Minimum Bayesrisk search (Kumar and Byrne, 2004) over a space of fluent hypotheses H: X ˆMBR = argmin L(E, E ′ )P (E|F ) (2) E E ′ ∈H E∈E In this approach the MBR evidence space E is generated by an SMT system as a k-best list or lattice. The system runs in its best possible configuration, ensuring both translation robustness and good baselines. Rather than decoding in the output of the baseline SMT system, translations will be sought among a collection of fluent sentences that are close to the top SMT hypotheses as determined by the loss function L(E, E ′ ). Decoupling the MBR hypothesis space from first-pass translation offers great flexi"
C10-1009,E09-1044,1,\N,Missing
C10-1009,W09-0401,0,\N,Missing
C14-1195,D07-1090,0,0.00939615,"The test sets (newswire: 1,779 sentences, web: 1768 sentences) contain all newswire and web evaluation data of MT08 (mt08), MT12 (mt12), and MT08 progress test (mt08.p). All Chinese sentences in the training, development and test sets were parsed using the Berkeley parser (Petrov and Klein, 2007). A Kneser-Ney 4-gram language model was trained on the AFP and Xinhua portions of the English Gigaword in addition to the English side of the parallel corpus. A stronger 5-gram language model was trained on all English data of NIST MT12 and the Google counts corpus using the ”stupid” backoff method (Brants et al., 2007). For decoding we use HiFST, which is implemented with weighted finite state transducers (de Gispert et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009). The span limit was set to 10 in extracting basic phrases and decoding. All"
C14-1195,P05-1033,0,0.528215,"traction and decoding for hierarchical phrase-based translation. We obtain tree-to-string rules by the GHKM method and use them to complement Hiero-style rules. All these rules are then employed to decode new sentences with source language parse trees. We experiment with our approach in a state-of-the-art Chinese-English system and demonstrate +1.2 and +0.8 BLEU improvements on the NIST newswire and web evaluation data of MT08 and MT12. 1 Introduction Synchronous context free grammars (SCFGs) are widely used in statistical machine translation (SMT), with hierarchical phrase-based translation (Chiang, 2005) as the dominant approach. Hiero grammars are easily extracted from word-aligned parallel corpora and can capture complex nested translation relationships. Hiero grammars are formally syntactic, but rules are not constrained by source or target language syntax. This lack of constraint can lead to intractable decoding and bad performance due to the over-generation of derivations in translation. To avoid these problems, the extraction and application of SCFG rules is typically constrained by a source language span limit; (non-glue) rules are lexicalised; and rules are limited to two non-terminal"
C14-1195,P10-1146,0,0.0917775,"obustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM formalism and find, for example, that our approach works well with binarized trees. This work is licenced under a Creative Commons Attribution 4"
C14-1195,J10-3008,1,0.897734,"Missing"
C14-1195,P03-2041,0,0.455159,"Missing"
C14-1195,D12-1109,0,0.0143501,"ed in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingual sentences with both word alignment and source (or targe"
C14-1195,P06-1121,0,0.157895,"to intractable decoding and bad performance due to the over-generation of derivations in translation. To avoid these problems, the extraction and application of SCFG rules is typically constrained by a source language span limit; (non-glue) rules are lexicalised; and rules are limited to two non-terminals which are not allowed to be adjacent in the source language. These constraints can yield good performing translation systems, although at a sacrifice in the ability to model long-distance movement and complex reordering of multiple constituents. By contrast, the GHKM approach to translation (Galley et al., 2006) relies on a syntactic parse on either the source or target language side to guide SCFG extraction and translation. The parse tree provides linguistically-motivated constraints both in grammar extraction and in translation. This allows for looser span constraints; rules need not be lexicalised; and rules can have more than two non-terminals to model complex reordering multiple constituents. There are also modelling benefits as more meaningful features can be used to encourage derivations with ”well-formed” syntactic tree structures. However, GHKM can have robustness problems in that translatio"
C14-1195,W10-1761,0,0.0406427,"Missing"
C14-1195,P07-1019,0,0.0225341,"type 1 rules directly. For tree-to-string rules associated with type 2, we attempt to match rules to the source syntactic tree. If a match is found: the source span of the matching tree fragment is noted and the CYK cell for that span is selected; the tree-to-string rule is converted to a Hiero-style rule; and that rule is added to the list of rules in the selected CYK cell. Once this process is finished, RTN construction, expansion, and language model composition proceeds as usual. Similar modifications could be made to incorporate these rules into cube pruning (Chiang, 2007), cube growing (Huang and Chiang, 2007), and PDT intersection and expansion (Iglesias et al., 2011). We now elaborate on the rule matching strategy. Type 1 Rules The source sentence is parsed as is usual in Hiero-style translation, with the exception that we impose no span limit on rule applications for source spans corresponding to constituents in the Chinese syntactic tree. Rule matching, the procedure that determines if a rule applies to a source span, is based on string matching (see Figure 4(a)). For example, the type 1 rule h9 in Figure 4(c) can be applied to spans (1,13) and (2,13) since both of them agree with tree constitu"
C14-1195,D10-1027,0,0.0168031,"an limit to be applied in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingual sentences with both word alignment"
C14-1195,2006.amta-papers.8,0,0.366008,"Missing"
C14-1195,E09-1044,1,0.894124,"Missing"
C14-1195,D11-1127,1,0.87318,"Missing"
C14-1195,N13-1060,0,0.0232239,"Missing"
C14-1195,P06-1077,0,0.614988,"Typically these are: (a) a rule span limit to be applied in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingua"
C14-1195,P09-1063,0,0.365919,"Missing"
C14-1195,P09-1065,0,0.0321399,"Missing"
C14-1195,D08-1076,0,0.0276395,"sducers (de Gispert et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009). The span limit was set to 10 in extracting basic phrases and decoding. All features weights were optimized using lattice-based minimum error rate training (Macherey et al., 2008). For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules with a Chinese-to-English translation probability of < 0.10. 4.2 Results We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments a"
C14-1195,W06-1606,0,0.0321305,"ifference lies in that the tree-to-string rule indicator feature does not distinguish between different syntactic labels, whereas soft syntactic features do. • Features in syntactic MT. In general tree-to-string rules have their own features which are different from those used in Hiero-style systems. For example, the features in syntactic MT systems can be defined as the generation probabilities conditioned on the root symbol of the tree-fragment. Here we choose five popular features used in syntactic MT systems, including the bi-directional phrase-based conditional translation probabilities (Marcu et al., 2006) and three syntax-based conditional probabilities (Mi and Huang, 2008). All these probabilities can be computed by relative-frequency estimates. For example, the phrase-based features are the probabilities of translating between the frontier nodes of sr and tr . The syntax-based features are the probabilities of generating r conditioned on its root, 2 We experimented with soft syntactic features (Marton and Resnik, 2008) but found no improvement over our baseline system. 2068 source and target language sides, respectively. More formally, we use the following estimates for these probabilities:"
C14-1195,P08-1114,0,0.143232,"multiple constituents. There are also modelling benefits as more meaningful features can be used to encourage derivations with ”well-formed” syntactic tree structures. However, GHKM can have robustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We al"
C14-1195,D08-1022,0,0.0227804,"not distinguish between different syntactic labels, whereas soft syntactic features do. • Features in syntactic MT. In general tree-to-string rules have their own features which are different from those used in Hiero-style systems. For example, the features in syntactic MT systems can be defined as the generation probabilities conditioned on the root symbol of the tree-fragment. Here we choose five popular features used in syntactic MT systems, including the bi-directional phrase-based conditional translation probabilities (Marcu et al., 2006) and three syntax-based conditional probabilities (Mi and Huang, 2008). All these probabilities can be computed by relative-frequency estimates. For example, the phrase-based features are the probabilities of translating between the frontier nodes of sr and tr . The syntax-based features are the probabilities of generating r conditioned on its root, 2 We experimented with soft syntactic features (Marton and Resnik, 2008) but found no improvement over our baseline system. 2068 source and target language sides, respectively. More formally, we use the following estimates for these probabilities: P Pphr (tr |sr ) = r00 :ϕ(sr00 )=ϕ(sr )∧tr00 =tr P P Pphr (sr |tr ) ="
C14-1195,P08-1023,0,0.02228,"re: (a) a rule span limit to be applied in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingual sentences with"
C14-1195,P02-1040,0,0.0950826,"ror rate training (Macherey et al., 2008). For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules with a Chinese-to-English translation probability of < 0.10. 4.2 Results We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments are organized as follows: • Baseline and Span Limits (exp01 and exp02) First we study the effect of removing the span limit for tree constituents, that is, SCFG rules can be 2069 Entry System exp01 exp02 exp03 exp04 exp05 exp06 exp07 exp08 exp09 exp10 baseline += no span limit += t-to-s rules += t-to-s features t-to-s baseline exp04 on spans &gt; 10 exp04 with null trans. exp04 + left binariz. exp04 + right binariz. exp04 + forest binariz. tune (1755) 35.84 36.05 36.63 36.82 34.63 36.17 36.10 37.11 36.58 37.03 mt08 (691) 35.85 36.08 36.51 36.49 34.44 36.11 36.03 37.46 36.56 37.2"
C14-1195,N07-1051,0,0.0249299,"track. Word alignments are obtained using MTTK (Deng and Byrne, 2008) in both Chinese-to-English and English-to-Chinese directions, and then unioning the links. The data from newswire and web genres was used for tuning and test. The development sets contain 1,755 sentences and 2160 sentences for the two genres respectively. The test sets (newswire: 1,779 sentences, web: 1768 sentences) contain all newswire and web evaluation data of MT08 (mt08), MT12 (mt12), and MT08 progress test (mt08.p). All Chinese sentences in the training, development and test sets were parsed using the Berkeley parser (Petrov and Klein, 2007). A Kneser-Ney 4-gram language model was trained on the AFP and Xinhua portions of the English Gigaword in addition to the English side of the parallel corpus. A stronger 5-gram language model was trained on all English data of NIST MT12 and the Google counts corpus using the ”stupid” backoff method (Brants et al., 2007). For decoding we use HiFST, which is implemented with weighted finite state transducers (de Gispert et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the translation model are activated; and then, the 5-gram language model is app"
C14-1195,W13-2225,1,0.875894,"Missing"
C14-1195,2010.iwslt-papers.18,0,0.0179447,"inese syntactic structure indicating the reordered translations of NP and VP. However, such a rule would not normally be included in a Hiero grammar, as it would require consecutive source language non-terminals (see Figure 3). 3 The Proposed Approach Both the tree-to-string model and the hierarchical phrase-based model have their own strengths and weaknesses. For example, tree-to-string systems are good at modelling long distance reordering, while hierarchical phrase-based systems are relatively more powerful in handling ill-formed sentences1 and free translations (Zhao and Al-Onaizan, 2008; Vilar et al., 2010). Here we present a method to enhance hierarchical phrase-based systems with tree-to-string rules and benefit from both models. The idea is simple: we obtain both the tree-to-string grammar and the Hiero-style SCFG from the training data, and then use tree-to-string rules as additional rules in decoding with the SCFG. Figure 2 shows an overview of our approach and the usual hierarchical phrase-based approach. Our approach requires source language parse trees to be input in both rule extraction and decoding. In rule extraction, we acquire tree-to-string rules using the GHKM method and Hiero-sty"
C14-1195,J10-2004,0,0.0237385,"es the baseline result using the Hiero model (i.e., type 1 rules only). To investigate the effect of failed parse trees on system performance, we also report the BLEU score including null translations for which the parser fails. As shown in exp07, there are significantly lower BLEU scores when null translations are included. It indicates that our approach is more robust than standard treeto-string systems which would generate an empty translation if the source language parser fails. • Results on Binarization (exp08-10) Tree binarization is a widely used method to improve syntactic MT systems (Wang et al., 2010). exp08-10 show the results of our improved system with left-heavy, right-heavy and forest-based bina2070 Reference: After North Korea demanded concessions from U.S. again before the start of a new round of six-nation talks , ... Baseline: In the new round of six-nation talks on North Korea again demanded that U.S. in the former promise concessions , ... GHKM+Hiero: After North Korea again demanded that U.S. promised concessions before the new round of six-nation talks , ... a Hiero rule X → h 在 X1 后, after X1 i is applied on span (1,15) Input: 北韩2 再度3 要求4 美国5 于6 新7 回合8 六9 国10 会谈11 前12 承诺13 让步"
C14-1195,P12-3004,1,0.88289,"guage model and the translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009). The span limit was set to 10 in extracting basic phrases and decoding. All features weights were optimized using lattice-based minimum error rate training (Macherey et al., 2008). For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules with a Chinese-to-English translation probability of < 0.10. 4.2 Results We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments are organized as follows: • Baseline and Span Limits (exp01 and exp02) First we study the effect of"
C14-1195,D11-1020,0,0.361476,"Missing"
C14-1195,P08-1064,0,0.375097,"Missing"
C14-1195,D08-1060,0,0.302527,"s. However, GHKM can have robustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM formalism and find, for example, that our approach works well with binarized trees. This work is licenced under a Creative Common"
C14-1195,W06-3119,0,0.264258,"rmed” syntactic tree structures. However, GHKM can have robustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM formalism and find, for example, that our approach works well with binarized trees. This work is licen"
C14-1195,2010.iwslt-keynotes.2,0,\N,Missing
C14-1195,J07-2003,0,\N,Missing
D10-1053,P10-2006,1,0.88762,"Missing"
D10-1053,P09-1088,0,0.0470954,") describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have been recently developed to induce a grammar directly from an unaligned parallel corpus (Blunsom et al., 2008; Blunsom et al., 2009). Finally, Cmejrek et al. (2009) extract rules directly from bilingual chart parses of the parallel corpus without using word alignments. We take a different approach in that we aim to start with very strong word alignment models and use them to guide grammar extraction. 3 Rule Extraction from Alignment Posteriors The goal of rule extraction is to generate a set of good-quality translation rules from a parallel corpus. Rules are of the form X→hγ,α,∼i , where γ, α ∈ {X ∪ T}+ are the source and target sides of the rule, T denotes the set of terminals (words) and ∼ is a bijective function1 relati"
D10-1053,D07-1090,0,0.0568111,"Missing"
D10-1053,J93-2003,0,0.0263085,"ed rules, and consequently better relative frequency estimates over translations. 3.1 Ranking and Counting Functions We describe two alternative approaches to modify the functions fR and fC so that they incorporate the probabilities provided by the alignment models. 3.1.1 Word-to-word Alignment Posterior Probabilities Word-to-word alignment posterior probabilities p(lji |f1J , eI1 ) express how likely it is that the words in source position j and target position i are aligned 547 given a sentence pair. These posteriors can be efficiently computed for Model 1, Model 2 and HMM, as described in (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2008). We will use these posteriors in functions to score phrase pairs. For a simple non-disjoint case (fjj12 , eii21 ) we use: fR (fjj12 , eii21 ) = j2 X i2 Y p(lji |f1J , eI1 ) j=j1 i=i1 i2 − i1 + 1 (1) which is very similar to the score used for lexical features in many systems (Koehn, 2010), with the link posteriors for the sentence pair playing the role of the Model 1 translation table. For a particular source phrase, Equation 1 is not a proper conditional probability distribution over all phrases in the target sentence. Therefore it cannot be use"
D10-1053,J07-2003,0,0.865052,"t lists are also used in Liu et al. (2009) to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, 546 Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi"
D10-1053,2009.iwslt-papers.2,0,0.0515386,"hrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have been recently developed to induce a grammar directly from an unaligned parallel corpus (Blunsom et al., 2008; Blunsom et al., 2009). Finally, Cmejrek et al. (2009) extract rules directly from bilingual chart parses of the parallel corpus without using word alignments. We take a different approach in that we aim to start with very strong word alignment models and use them to guide grammar extraction. 3 Rule Extraction from Alignment Posteriors The goal of rule extraction is to generate a set of good-quality translation rules from a parallel corpus. Rules are of the form X→hγ,α,∼i , where γ, α ∈ {X ∪ T}+ are the source and target sides of the rule, T denotes the set of terminals (words) and ∼ is a bijective function1 relating source and target nonterminal"
D10-1053,J10-3008,1,0.900494,"Missing"
D10-1053,P08-2007,0,0.0639221,"enforces a minimum span of two words per nonterminal, 546 Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have been recently developed to induce a grammar directly from an unaligned parallel corpus (Blunsom et al., 2008; Blunsom et al., 2009). Finally, Cmejrek et al. (2009) extract rules directly from bilingual chart parses of the parallel corpus without using word alignments. We take a different app"
D10-1053,E09-1044,1,0.915839,"Missing"
D10-1053,N03-1017,0,0.401572,"sequences eii21 as translation candidates. Each target-side sequence that satisfies the alignment constraints CA is ranked by the function fR . For practical reasons, a set of selection criteria CS is then applied to these ranked candidates and defines the set of translations of the source sequence that are extracted as rules. Each extracted rule is assigned a count fC . In this section we will explore variations of this rule extraction procedure involving alternative definitions of the ranking and counting functions, fR and fC , based on probabilities over alignment models. Common practice (Koehn et al., 2003) takes a set of word alignment links L and defines the alignment constraints CA so that there is a consistency between the links in the (fjj12 , eii21 ) phrase pair. This is expressed by ∀(j, i) ∈ L : (j ∈ [j1 , j2 ] ∧ i ∈ [i1 , i2 ]) ∨ (j 6∈ [j1 , j2 ] ∧ i 6∈ [i1 , i2 ]). If these constraints are met, then alignment probabilities are ignored and fR = fC = 1. We call this extraction Viterbi-based, as the set of alignment links is generally obtained after applying a symmetrization heuristic to sourceto-target and target-to-source Viterbi alignments. In the following section we depart from this"
D10-1053,J10-4005,0,0.0272019,"posterior probabilities p(lji |f1J , eI1 ) express how likely it is that the words in source position j and target position i are aligned 547 given a sentence pair. These posteriors can be efficiently computed for Model 1, Model 2 and HMM, as described in (Brown et al., 1993; Venugopal et al., 2003; Deng and Byrne, 2008). We will use these posteriors in functions to score phrase pairs. For a simple non-disjoint case (fjj12 , eii21 ) we use: fR (fjj12 , eii21 ) = j2 X i2 Y p(lji |f1J , eI1 ) j=j1 i=i1 i2 − i1 + 1 (1) which is very similar to the score used for lexical features in many systems (Koehn, 2010), with the link posteriors for the sentence pair playing the role of the Model 1 translation table. For a particular source phrase, Equation 1 is not a proper conditional probability distribution over all phrases in the target sentence. Therefore it cannot be used as such without further normalization. Indeed we find that this distribution is too sharp and overemphasises short phrases, so we use fC = 1. However, it does allow us to rank target phrases as possible translations. In contrast to the common extraction procedure described in the previous section, the ranking approach described here"
D10-1053,D07-1005,0,0.0174547,"viously addressed the limitation caused by decoupling word alignment models from grammar extraction. For instance Venugopal et al. (2008) extract rules from n-best lists of alignments for a syntax-augmented hierarchical system. Alignment n-best lists are also used in Liu et al. (2009) to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, 546 Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We"
D10-1053,D09-1106,0,0.236089,"Missing"
D10-1053,C08-1064,0,0.014773,"et al. (2009) to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, 546 Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard ("
D10-1053,W02-1018,0,0.0646462,"ke translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, 546 Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have been recently developed to induce a grammar directly from an unaligned parallel corpus (Blunsom et al., 2008; B"
D10-1053,J03-1002,0,0.00717132,", the best-performing strategy is to extract two sets of translation rules under the phrase pair posteriors in each translation direction, and then to perform translation twice and merge the results. 6 Conclusion 5.4 Symmetrizing Alignments of Parallel Text In this section we investigate extraction from alignments (and posterior distributions) over parallel text which are generated using alignment models trained in the source-to-target (st) and target-to-source (ts) directions. Our motivation is that symmetrization strategies have been reported to be beneficial for Viterbi extraction methods (Och and Ney, 2003; Koehn et al., 2003). Results are shown in Table 3 for grammar G2 . We find that rules extracted under the source-to-target alignment models (V-st, WP-st and PP-st) consistently perform better than the V-ts, WP-ts and PPts cases. Also, for Viterbi extraction we find that the source-to-target V-st case performs better than any of the symmetrization strategies, which contradicts previous findings for non-hierarchical phrase-based systems(Koehn et al., 2003). We use the PP rule extraction method to extract two sets of rules, under the st and ts alignment models respectively. We now investigate t"
D10-1053,P03-1021,0,0.0678468,"Missing"
D10-1053,N10-1014,0,0.0178658,"patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have been recently developed to induce a grammar directly from an unaligned parallel corpus (Blunsom et al., 2008; Blunsom et al., 2009). Finally, Cmejrek et al. (2009) extract rules directly from bilingual chart parses of the parallel corpus without using word alignments. We take a different approach in that we aim to start with very strong word alignment models and use them to guide grammar extraction. 3 Rule Extraction from Alignment Posteriors The goal of rule extraction is to generate a set of"
D10-1053,W09-2304,0,0.0311795,"Missing"
D10-1053,P08-1066,0,0.0319089,"03; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, 546 Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on a phrase-based system where word alignment has been trained with an inversion transduction grammar (ITG) rather than IBM models. Pauls et al. (2010) also use an ITG to directly align phrases to nodes in a string-to-tree model. Bayesian methods have"
D10-1053,D08-1065,0,0.0259073,"d rescoring and system combination with lattice-based MBR (lower-cased BLEU shown) both rule sets by assigning to each rule the maximum count assigned by either alignment model. We then extend the previous strategy by adding three binary feature functions to the system, indicating whether the rule was extracted under the ’st’ model, the ’ts’ model or both. The motivation is that MERT can weight rules differently according to the alignment model they were extracted from. However, we do not find any improvement with either strategy. Finally, we use linearised lattice minimum Bayesrisk decoding (Tromble et al., 2008; Blackwood et al., 2010) to combine translation lattices (de Gispert et al., 2010) as produced by rules extracted under each alignment direction (see rows named LMBR(V-st,V-ts) and LMBR(PP-st,PP-ts)). Gains are consistent when comparing this to applying LMBR to each of the best individual systems (rows named LMBR(V-st) and LMBR(PP-st)). Overall, the best-performing strategy is to extract two sets of translation rules under the phrase pair posteriors in each translation direction, and then to perform translation twice and merge the results. 6 Conclusion 5.4 Symmetrizing Alignments of Parallel"
D10-1053,P03-1041,0,0.603903,"rk Some authors have previously addressed the limitation caused by decoupling word alignment models from grammar extraction. For instance Venugopal et al. (2008) extract rules from n-best lists of alignments for a syntax-augmented hierarchical system. Alignment n-best lists are also used in Liu et al. (2009) to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, 546 Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Sh"
D10-1053,2008.amta-papers.18,0,0.588252,"In Section 5 we assess these grammars in terms of their expressive power and the quality of the translations they yield in Chinese-toEnglish, showing that rule extraction from posteriors gives translation improvements. We also find that the best way to exploit source-to-target and targetto-source alignment models is to build two separate systems and combine their output translation lattices. Section 6 presents the main conclusions of this work. 2 Related Work Some authors have previously addressed the limitation caused by decoupling word alignment models from grammar extraction. For instance Venugopal et al. (2008) extract rules from n-best lists of alignments for a syntax-augmented hierarchical system. Alignment n-best lists are also used in Liu et al. (2009) to create a structure called weighted alignment matrices that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with rel"
D10-1053,C08-1144,0,0.0300031,"that approximates word-to-word link posterior probabilities, from which phrases are extracted for a phrase-based system. Alignment posteriors have been used before for extracting phrases in non-hierarchical phrase-based translation (Venugopal et al., 2003; Kumar et al., 2007; Deng and Byrne, 2008). In order to simplify hierarchical phrase-based grammars and make translation feasible with relatively large parallel corpora, some authors discuss the need for various filters during rule extraction (Chiang, 2007). In particular Lopez (2008) enforces a minimum span of two words per nonterminal, 546 Zollmann et al. (2008) use a minimum count threshold for all rules, and Iglesias et al. (2009) propose a finer-grained filtering strategy based on rule patterns. Other approaches include insisting that targetside rules are well-formed dependency trees (Shen et al., 2008). We also note approaches to tighter coupling between translation grammars and alignments. Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). More recently, Saers et al. (2009) report improvement on"
D10-1053,2010.iwslt-keynotes.2,1,\N,Missing
D11-1127,D07-1090,0,0.0940747,"nrestricted one without the previous constraint. We call this G2 . This is a superset of the previous grammar, and exact search under it is not feasible for HiFST: pruning is required in search. The initial English language model is a KneserNey 4-gram estimated over the target side of the parallel text and the AFP and Xinhua portions of monolingual data from the English Gigaword Fourth Edition (LDC2009T13). This is a total of 1.3B words. We will call this language model M1 . For large language model rescoring we also use the LM M2 obtained by interpolating M1 with a zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram estimated using 6.6B words of English newswire text. We next describe how we build translation systems using entropy-pruned language models. 1. We build a baseline HiFST system that uses M1 and a hierarchical grammar G, parameters being optimized with MERT under BLEU. 4 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl 2. We then use entropy-based pruning of the language model (Stolcke, 1998) under a relative perplexity threshold of θ to reduce the size of M1 . We will call the resulting language model as M1θ . Table 2 shows the number of n-grams (in millions) obtained for diff"
D11-1127,J07-2003,0,0.824569,"resented for PDAs. Chinese-toEnglish translation experiments using HiFST and HiPDT, FSA and PDA-based decoders, are presented using admissible (or exact) search, possible for HiFST with compact SCFG rulesets and HiPDT with compact LMs. For large rulesets with large LMs, we introduce a two-pass search strategy which we then analyze in terms of search errors and translation performance. 1 Introduction Hierarchical phrase-based translation, using a synchronous context-free translation grammar (SCFG) together with an n-gram target language model (LM), is a popular approach in machine translation (Chiang, 2007). Given a SCFG G and an ngram language model M , this paper focuses on how to decode with them, i.e. how to apply them to the source text to generate a target translation. Decoding has three basic steps, which we first describe in terms of the formal languages and relations involved, with data representations and algorithms to follow. 1. Translating the source sentence s with G to give target translations: T = {s} ◦ G, a (weighted) context-free language resulting Of course, decoding requires explicit data representations and algorithms for combining and searching them. In common to the approac"
D11-1127,J10-3008,1,0.489351,"Missing"
D11-1127,P07-1019,0,0.0331175,"large enough that exact translation under the FSA representation is not possible. We find that translation is possible using the two-pass strategy with the PDA translation representation and that gains in BLEU score result from using the larger translation grammar. 1374 1.1 Related Work There is extensive prior work on computational efficiency and algorithmic complexity in hierarchical phrase-based translation. The challenge is to find algorithms that can be made to work with large translation grammars and large language models. Following the original algorithms and analysis of Chiang (2007), Huang and Chiang (2007) developed the cube-growing algorithm, and more recently Huang and Mi (2010) developed an incremental decoding approach that exploits left-to-right nature of the language models. Search errors in hierarchical translation, and in translation more generally, have not been as extensively studied; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison. Using a relatively simple phrasebased translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar et al., 2006) and found that cube-pruning suf"
D11-1127,D10-1027,0,0.0670495,". We find that translation is possible using the two-pass strategy with the PDA translation representation and that gains in BLEU score result from using the larger translation grammar. 1374 1.1 Related Work There is extensive prior work on computational efficiency and algorithmic complexity in hierarchical phrase-based translation. The challenge is to find algorithms that can be made to work with large translation grammars and large language models. Following the original algorithms and analysis of Chiang (2007), Huang and Chiang (2007) developed the cube-growing algorithm, and more recently Huang and Mi (2010) developed an incremental decoding approach that exploits left-to-right nature of the language models. Search errors in hierarchical translation, and in translation more generally, have not been as extensively studied; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison. Using a relatively simple phrasebased translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar et al., 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive compari"
D11-1127,W05-1507,0,0.0249118,"time and space (given the underlying acyclicity) (Huang, 2008; Mohri, 2009). As presented in Section 2.4, the PDA representation can require time cubic and space quadratic in |M |.2 Table 1 summarizes the complexity results. Note the PDA representation is equivalent in time and superior in space to the CFG/hypergraph representation, in general, and it can be superior in both space 1 The modified Bar-Hillel construction described by Chiang (2007) has time and space complexity O(|Th ||M |4 ); the modifications were introduced presumably to benefit the subsequent pruning method employed (but see Huang et al. (2005)). 2 The time (resp. space) complexity is not cubic (resp. quadratic) in |Tp ||M |. Given a state q in Tp , there exists a unique sq such that q belongs to Csq . Given a state (q1 , q2 ) in Tp ∩ M , (q1 , q2 ) ∈ C(s1 ,s2 ) only if s1 = sq1 , and hence (q1 , q2 ) belongs to at most |M |components. Representation CFG/hypergraph PDA FSA Time Complexity O(|s|3 |G ||M |3 ) O(|s|3 |G ||M |3 ) 3 O(e|s ||G ||M |) Space Complexity O(|s|3 |G ||M |3 ) O(|s|3 |G ||M |2 ) 3 O(e|s ||G ||M |) Table 1: Complexity using various target translation representations. and time to the FSA representation depending on"
D11-1127,C08-5001,0,0.0607416,"As such, T and L have finite automaton representations Tf and Lf . In this case, weighted finite-state intersection and single-source shortest path algorithms (using negative log probabilities) can be used to solve Steps 2 and 3 (Mohri, 2009). This is the approach taken in (Iglesias et al., 2009a; de Gispert et al., 2010). Instead T and L can be represented by hypergraphs Th and Lh (or very similarly context-free rules, and-or trees, or deductive systems). In this case, hypergraph intersection with a finite automaton and hypergraph shortest path algorithms can be used to solve Steps 2 and 3 (Huang, 2008). This is the approach taken by Chiang (2007). In this paper, we will consider another representation for context-free languages T and L as well, pushdown automata (PDA) Tp and Lp , familiar from formal 1373 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1373–1383, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics language theory (Aho and Ullman, 1972). We will describe PDA intersection with a finite automaton and PDA shortest-path algorithms in Section 2 that can be used to solve Steps 2 and 3. It cannot"
D11-1127,N09-1049,1,0.890319,"Missing"
D11-1127,E09-1044,1,0.588109,"Missing"
D11-1127,D10-1125,0,0.00831737,"ubtedly due to the difficulties inherent in finding exact translations for use in comparison. Using a relatively simple phrasebased translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar et al., 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). Relaxation techniques have also recently been shown to finding exact solutions in parsing (Koo et al., 2010) and in SMT with tree-to-string translation grammars and trigram language models (Rush and Collins, 2011), much smaller models compared to the work presented in this paper. Although entropy-pruned language models have been used to produce real-time translation systems (Prasad et al., 2007), we believe our use of entropy-pruned language models in two-pass translation to be novel. This is an approach that is widelyused in automatic speech recognition (Ljolje et al., 1999) and we note that it relies on efficient representation of very large search spaces T for subsequent rescoring, as is possible"
D11-1127,W03-3016,0,0.0328395,"if q is final in T and z = ǫ (ρ′ ((q, ǫ)) = ρ(q)). The set of states of T ′ is the set of pairs (q, z) that can be reached from an initial state by transitions defined as above. The condition that T has a bounded stack ensures that this set is finite (since it implies that for any (q, z), |z |≤ K). The complexity of the algorithm is linear in O(|T ′ |) = O(e|T |). Figure 1d show the result of the algorithm when applied to the PDA of Figure 1c. 2.3 Intersection Algorithm The class of weighted pushdown automata is closed under intersection with weighted finite automata (Bar-Hillel et al., 1964; Nederhof and Satta, 2003). Considering a pair (T1 , T2 ) where one element is an FSA and the other element a PDA, then there exists a PDA T1 ∩ T2 , the intersection of T1 and T2 , such that for all x ∈ Σ∗ : (T1 ∩ T2 )(x) = T1 (x) + T2 (x). We assume in the following that T2 is an FSA. We also assume that T2 has no input-ǫ transitions. When T2 has input-ǫ transitions, an epsilon filter (Mohri, 2009; Allauzen et al., 2011) generalized to handle parentheses can be used. A state in T = T1 ∩T2 is a pair (q1 , q2 ) where q1 is a state of T1 and q2 a state of T2 . The initial state is I = (I1 , I2 ). Given a transition e1 ="
D11-1127,P03-1021,0,0.00479488,"ranslation results on a development set tune-nw (1,755 sentences) and a test set test-nw (1,671 sentences). These contain translations produced by the GALE program and portions of the newswire sections of MT02 through MT06. In tuning the sys3 See http://projects.ldc.upenn.edu/gale/data/catalog.html. We excluded the UN material and the LDC2002E18, LDC2004T08, LDC2007E08 and CUDonga collections. 1379 0 207.5 7.5 × 10−9 20.2 7.5 × 10−8 4.1 7.5 × 10−7 0.9 Table 2: Number of ngrams (in millions) in the 1st pass 4-gram language models obtained with different θ values (top row). tems, standard MERT (Och, 2003) iterative parameter estimation under IBM BLEU4 is performed on the development set. The parallel corpus is aligned using MTTK (Deng and Byrne, 2008) in both source-to-target and target-to-source directions. We then follow standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009b) to extract hierarchical phrases from the union of the directional word alignments. We call a translation grammar the set of rules extracted from this process. We extract two translation grammars: • A restricted grammar where we apply the following additional constraint: rules are only consid"
D11-1127,P11-1008,0,0.0100206,"a relatively simple phrasebased translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar et al., 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). Relaxation techniques have also recently been shown to finding exact solutions in parsing (Koo et al., 2010) and in SMT with tree-to-string translation grammars and trigram language models (Rush and Collins, 2011), much smaller models compared to the work presented in this paper. Although entropy-pruned language models have been used to produce real-time translation systems (Prasad et al., 2007), we believe our use of entropy-pruned language models in two-pass translation to be novel. This is an approach that is widelyused in automatic speech recognition (Ljolje et al., 1999) and we note that it relies on efficient representation of very large search spaces T for subsequent rescoring, as is possible with FSAs and PDAs. 2 Pushdown Automata In this section, we formally define pushdown automata and give i"
D11-1127,2010.iwslt-keynotes.2,1,\N,Missing
D15-1273,J07-2003,0,0.0602421,"al neural network language model (BiLM) of Devlin et al. (2014) to the output lattices of the CUED OpenMT12 Arabic-English hierarchical phrase-based translation system3 using HiFST (de Gispert et al., 2010). We use a development set mt0205tune (2075 sentences) and a validation set mt0205test (2040 sentences) from the NIST MT02 through MT05 evaluation sets. The edges in these WFSTs are of the form t:i/w, where t is the target word, i is the source sentence position t aligns to, and w contains the translation and language model score. HiFST outputs these WFSTs by using a standard hiero grammar (Chiang, 2007) augmented with target side heuristic alignments or affiliations to the source (Devlin et al., 2014). In a rule over source and target words X →&lt; s1 X s2 s3 , t1 X t2 &gt; / 2, 1 the feature ‘2, 1’ indicates that the target word t1 is aligned to source word s2 and that t2 aligns to s1 . As rules are applied in translation, this information can be used to link target words to absolute positions within the source sentence. Allowing for admissible pruning, all possible affiliation sequences under the grammar for every translation are available in the WFSTs; disambiguation keeps the best affiliation"
D15-1273,J10-3008,1,0.937855,"Missing"
D15-1273,D13-1140,0,0.111952,"Missing"
D15-1273,P14-1129,0,0.160875,"n We present a new disambiguation algorithm that can efficiently accomplish this. In Section 2 we describe how the tropical sparse tuple vector semiring can keep track of individual arcs in the original WFST as topological features during the mapping step (a). This allows us to describe in Section 3 an efficient expansion algorithm for step (c). We show in Section 4 empirical evidence that our algorithm is more efficient than Shafran et al. (2011) in their same PoS-tagging task. We also show how our method can be applied in rescoring translation lattices under a bilingual neuralnetwork model (Devlin et al., 2014), obtaining BLEU score gains consistent with the literature. Section 5 reviews related work and concludes. 2 Semiring Definitions A WFST T = (Σ, ∆, Q, I, F, E, ρ) over a semiring (K, ⊕, ⊗, 0, 1) has input and output alphabets Σ and ∆, a set of states Q, the initial state I ∈ Q, a set of final states F ⊂ Q, a set of transitions (edges) E ⊂ (Q × Σ × ∆ × K × Q), and a final state function ρ : F → K. We focus on extensions to the tropical semiring (R ± ∞, min, +, ∞, 0). the lattice, searches for the best output string for each input string, and converts the resulting sequences back into a WFST, wh"
D15-1273,W11-2123,0,0.0307226,"categorial method has only finished 1405. The slowest WFST to disambiguate takes 6700 seconds with the categorial procedure, which compares to 1000 seconds in our case. The BiLM model is trained with NPLM (Vaswani et al., 2013) with a context 3 See http://www.nist.gov/itl/iad/mig/openmt12results.cfm. mt0205tune 52.2 53.0 mt0205test 51.9 52.9 Table 1: Translation scores in lower-case BLEU. of 3 source and 4 target words. Lattice rescoring with this model requires a special variation of the standard WFST composition which looks at both input and output labels on a transducer arc; we use KenLM (Heafield, 2011) to retrieve neural network scores for on-the-fly composition. We retune the parameters with Lattice MERT (Macherey et al., 2008) . Results are shown in Table 1. Acknowledging the task differences with respect to (Devlin et al., 2014), we find BLEU gains consistent with rescoring results reported in their Table 5. 5 Conclusions and Related Work We have described a tagging disambiguation algorithm that supports non-functional WFSTs, which cannot be handled directly by neither WFST determinization (Mohri, 1997) nor WFST disambiguation (Mohri, 2012). We show it is faster than the implementation w"
D15-1273,D08-1076,0,0.100196,"re, which compares to 1000 seconds in our case. The BiLM model is trained with NPLM (Vaswani et al., 2013) with a context 3 See http://www.nist.gov/itl/iad/mig/openmt12results.cfm. mt0205tune 52.2 53.0 mt0205test 51.9 52.9 Table 1: Translation scores in lower-case BLEU. of 3 source and 4 target words. Lattice rescoring with this model requires a special variation of the standard WFST composition which looks at both input and output labels on a transducer arc; we use KenLM (Heafield, 2011) to retrieve neural network scores for on-the-fly composition. We retune the parameters with Lattice MERT (Macherey et al., 2008) . Results are shown in Table 1. Acknowledging the task differences with respect to (Devlin et al., 2014), we find BLEU gains consistent with rescoring results reported in their Table 5. 5 Conclusions and Related Work We have described a tagging disambiguation algorithm that supports non-functional WFSTs, which cannot be handled directly by neither WFST determinization (Mohri, 1997) nor WFST disambiguation (Mohri, 2012). We show it is faster than the implementation with a lexicographic-tropicalcategorial semiring described by Shafran et al. (2011) and describe a use case in a practical rescori"
D15-1273,J97-2003,0,0.0723358,"ion which looks at both input and output labels on a transducer arc; we use KenLM (Heafield, 2011) to retrieve neural network scores for on-the-fly composition. We retune the parameters with Lattice MERT (Macherey et al., 2008) . Results are shown in Table 1. Acknowledging the task differences with respect to (Devlin et al., 2014), we find BLEU gains consistent with rescoring results reported in their Table 5. 5 Conclusions and Related Work We have described a tagging disambiguation algorithm that supports non-functional WFSTs, which cannot be handled directly by neither WFST determinization (Mohri, 1997) nor WFST disambiguation (Mohri, 2012). We show it is faster than the implementation with a lexicographic-tropicalcategorial semiring described by Shafran et al. (2011) and describe a use case in a practical rescoring task of an MT system with bilingual neural networks that yield 1.0 BLEU gain. Povey et al. (2012) also use a special semiring that allows to map non-functional WFSTs into WFSAs by inserting the tag into a string weight. However, in contrast to our implementation and that of Shafran et al (2011), no expansion into an WFST with aligned input/output is described. Lexicographic semir"
D15-1273,P11-2001,0,0.0720733,"Missing"
D15-1273,J14-3008,1,\N,Missing
D15-1273,J14-4002,0,\N,Missing
D15-1273,2010.iwslt-keynotes.2,0,\N,Missing
D19-5203,W19-5206,0,0.0404943,"- Formal There’s nothing to apologize for. 謝ることなんかなにもないさ ayamaru koto nanka nanimo nai sa MT Output and Transcription 謝ることなんて何もない ayamaru koto nante nanimo nai 別に謝ることはない betsu ni ayamaru koto wa nai 謝ることはありません ayamaru koto wa arimasen お詫びのことは何もございません owabi no koto wa nanimo gozaimasen Table 9: Example output from JESC NMT baseline model and formality-aware NMT model, when each formality level is attached to the source segment. translation. The same approach has been successfully used in other applications, such as in distinguishing standard versus back-translated translation parallel corpora (Caswell et al., 2019). equally adequate. 5 Related Work Sennrich et al. (2016a) showed that side constraints can be added to the source side of a parallel text to provide control over the politeness of translation output in an English-German translation task. Following this paper’s suggestion, we take a similar approach towards Japanese honorifics. Niu et al. (2017) also use a similar approach, termed “Formality-Sensitive Machine Translation”, in a French-English translation task. In (Niu et al., 2018) French-English parallel text with formality features is combined with English-English parallel text, where the so"
D19-5203,P11-2093,0,0.0202993,"rule-based tool’s dictionary will lead to errors in changing verbal inflection. In our evaluation, we show how using this rule-based method compares to our formalityaware NMT. 3 Train 23,781,990 1,000,000 3,237,376 329,882 Datasets 3.3 We use three publicly-available parallel data sets for our NMT experiments. The Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016), a corpus of scientific paper abstracts, the Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2018), a corpus of sentence-aligned movie and television subtitles, and the Kyoto Free Translation Task (KFTT) (Neubig et al., 2011a), a corpus of Wikipedia data about the city of Kyoto. In our experiments we use the standard training and test sets for each parallel corpus. We also use a proprietary parallel training data set which contains web-crawled data from a mix of domains and a corresponding test set. Training and test set sizes are reported in Table 5. Experimental Results To evaluate our formality-aware NMT models, we first need to choose the right level of formality for each sentence in the test sets. We do this by applying the formality classifier to the test reference and prepending the predicted labels to the"
D19-5203,E17-2068,0,0.0104635,"sentence it is ignored. From the original 21 million sentences, 1 million were unable to be categorized by our heuristics. Precision Recall F1 We hypothesize that a text classifier trained on the resulting 20 million sentences selected by our heuristics will learn more nuanced distinctions in word choice and style than using the heuristics alone, which only identify a small set of verb forms. We tokenize this data set with the KyTea morphological analyzer (Neubig, 2011b) and train a model on the tokenized monolingual data and labels with the text classification tools provided by the FastText (Joulin et al., 2017) toolkit, using word trigram features. Informal 1.00 0.74 0.85 Polite 0.82 0.91 0.86 Formal 0.72 0.97 0.83 Table 4: Evaluation scores of labels produced by the formality classifier compared to gold test set labels for each formality category (n=150). heuristic rules on this test set, but we hypothesize that it generalizes better to unseen text and therefore use it in our translation experiments. The results show that our classifier has higher precision on the informal category, but lower recall, and higher recall on the polite and formal categories, but lower precision. To evaluate our classif"
D19-5203,D17-1299,0,0.0218646,"y-aware NMT model, when each formality level is attached to the source segment. translation. The same approach has been successfully used in other applications, such as in distinguishing standard versus back-translated translation parallel corpora (Caswell et al., 2019). equally adequate. 5 Related Work Sennrich et al. (2016a) showed that side constraints can be added to the source side of a parallel text to provide control over the politeness of translation output in an English-German translation task. Following this paper’s suggestion, we take a similar approach towards Japanese honorifics. Niu et al. (2017) also use a similar approach, termed “Formality-Sensitive Machine Translation”, in a French-English translation task. In (Niu et al., 2018) French-English parallel text with formality features is combined with English-English parallel text, where the source and target are of similar meaning but different formality, to create a multi-task model that performs both formalitysensitive MT and monolingual formality transfer. In related work on Japanese-English NMT, Yamagishi et al. (2016) use a side-constraint approach to control the voice (active or passive) of an English translation. Takeno (2017)"
D19-5203,C18-1086,0,0.0349166,"Missing"
D19-5203,P02-1040,0,0.105272,"raining data set which contains web-crawled data from a mix of domains and a corresponding test set. Training and test set sizes are reported in Table 5. Experimental Results To evaluate our formality-aware NMT models, we first need to choose the right level of formality for each sentence in the test sets. We do this by applying the formality classifier to the test reference and prepending the predicted labels to the source side of each test sentence. We then provide this input to our formality-aware NMT models and compare the output to test set translations from our baseline NMT using B LEU (Papineni et al., 2002). We tokenize the Japanese MT output and reference using KyTea before computing B LEU. We evaluate on the overall test set, as well as each separate portion of the test set where the test reference was classified as informal, polite or formal. Table 6 shows 48 3.3.3 our results on the test set using B LEU. 3.3.1 Since choosing the appropriate formality level in Japanese is very important to conform with social norms, we want to show that our formality-aware NMT models can provide translations in the desired level of formality. As our test sets do not have gold labels from a human annotator for"
D19-5203,P07-2045,0,0.00653247,"Proprietary ASPEC JESC KFTT a single verb, which differ based on the class the verb belongs to. For example, to convert the verb 歩きました (arukimashita ”walked” polite)” to an informal inflection, the polite suffix ま し た (mashita) is removed from the stem of the verb and a new suffix is appended to create 歩いた (aruita). The き (ki) at the end of the verb stem marks this as a verb in a particular verb class. All verbs with ki at the end of their stem belong to the same class and have the same conjugation pattern. 3.2 Experimental Setup The English source of each bitext was tokenized with the Moses (Koehn et al., 2007) tokenizer.perl script and the Japanese target was tokenized with KyTea. We limit sentence length to 80 tokens on either side of the bitext and train a Moses truecaser for the English source side, except for the JESC data set because the English side of the JESC corpus is already entirely lowercased. We use 32k subword unit vocabularies (Sennrich et al., 2016b) separate for source and target. We use the resulting tokenized, truecased, subworded training data to train our NMT models. Our experiments use the Transformer (Vaswani et al., 2018) to train NMT models. We use 512 hidden units, 6 hidde"
D19-5203,L18-1182,0,0.50156,"Missing"
D19-5203,N16-1005,0,0.223729,"rks this as a verb in a particular verb class. All verbs with ki at the end of their stem belong to the same class and have the same conjugation pattern. 3.2 Experimental Setup The English source of each bitext was tokenized with the Moses (Koehn et al., 2007) tokenizer.perl script and the Japanese target was tokenized with KyTea. We limit sentence length to 80 tokens on either side of the bitext and train a Moses truecaser for the English source side, except for the JESC data set because the English side of the JESC corpus is already entirely lowercased. We use 32k subword unit vocabularies (Sennrich et al., 2016b) separate for source and target. We use the resulting tokenized, truecased, subworded training data to train our NMT models. Our experiments use the Transformer (Vaswani et al., 2018) to train NMT models. We use 512 hidden units, 6 hidden layers, 8 heads, and a batch size of 4096. We train for 200k training steps using the Adam optimizer (Kingma and Ba, 2015). For each parallel corpus, we train a formalityaware NMT model by classifying the formality of the Japanese target side and attaching a corresponding feature to the beginning of each English source segment, identifying the target as bei"
D19-5203,P16-1162,0,0.165799,"rks this as a verb in a particular verb class. All verbs with ki at the end of their stem belong to the same class and have the same conjugation pattern. 3.2 Experimental Setup The English source of each bitext was tokenized with the Moses (Koehn et al., 2007) tokenizer.perl script and the Japanese target was tokenized with KyTea. We limit sentence length to 80 tokens on either side of the bitext and train a Moses truecaser for the English source side, except for the JESC data set because the English side of the JESC corpus is already entirely lowercased. We use 32k subword unit vocabularies (Sennrich et al., 2016b) separate for source and target. We use the resulting tokenized, truecased, subworded training data to train our NMT models. Our experiments use the Transformer (Vaswani et al., 2018) to train NMT models. We use 512 hidden units, 6 hidden layers, 8 heads, and a batch size of 4096. We train for 200k training steps using the Adam optimizer (Kingma and Ba, 2015). For each parallel corpus, we train a formalityaware NMT model by classifying the formality of the Japanese target side and attaching a corresponding feature to the beginning of each English source segment, identifying the target as bei"
D19-5203,W17-5702,0,0.228802,"Missing"
D19-5203,W18-1819,0,0.026642,"ish source of each bitext was tokenized with the Moses (Koehn et al., 2007) tokenizer.perl script and the Japanese target was tokenized with KyTea. We limit sentence length to 80 tokens on either side of the bitext and train a Moses truecaser for the English source side, except for the JESC data set because the English side of the JESC corpus is already entirely lowercased. We use 32k subword unit vocabularies (Sennrich et al., 2016b) separate for source and target. We use the resulting tokenized, truecased, subworded training data to train our NMT models. Our experiments use the Transformer (Vaswani et al., 2018) to train NMT models. We use 512 hidden units, 6 hidden layers, 8 heads, and a batch size of 4096. We train for 200k training steps using the Adam optimizer (Kingma and Ba, 2015). For each parallel corpus, we train a formalityaware NMT model by classifying the formality of the Japanese target side and attaching a corresponding feature to the beginning of each English source segment, identifying the target as being informal, polite, or formal. For comparison, we also train a baseline NMT model without these formality annotations. Evaluation In this section, we evaluate the translation quality o"
D19-5203,W16-4620,0,0.0587366,"sh-German translation task. Following this paper’s suggestion, we take a similar approach towards Japanese honorifics. Niu et al. (2017) also use a similar approach, termed “Formality-Sensitive Machine Translation”, in a French-English translation task. In (Niu et al., 2018) French-English parallel text with formality features is combined with English-English parallel text, where the source and target are of similar meaning but different formality, to create a multi-task model that performs both formalitysensitive MT and monolingual formality transfer. In related work on Japanese-English NMT, Yamagishi et al. (2016) use a side-constraint approach to control the voice (active or passive) of an English translation. Takeno (2017) apply side constraints more broadly to control translation length, bidirectional decoding, domain adaptation, and unaligned target word generation. Our paper follows the modeling approach introduced by Johnson et al. (2017), who showed that by adding a token to the source side of parallel text allows for training a single NMT model on data for multiple language pairs. Their token specifies the desired target language, allowing the user control over the language of machine translati"
E09-1044,P08-1024,0,0.00832531,"arsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 380 extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in translation. Shen at al. (2008) make use of target dependency trees and a target dependency language model during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006)"
E09-1044,D07-1090,0,0.0144296,"Missing"
E09-1044,P05-1033,0,0.391566,"translation performance. We describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation. Rules are put into syntactic classes based on the number of non-terminals and the pattern, and various filtering strategies are then applied to assess the impact on translation speed and quality. Results are reported on the 2008 NIST Arabic-toEnglish evaluation task. 1 Introduction Hierarchical phrase-based translation (Chiang, 2005) has emerged as one of the dominant current approaches to statistical machine translation. Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text. The approach has been widely adopted and reported to be competitive with other large-scale data driven approaches, e.g. (Zollmann et al., 2008). Large-scale hierarchical SMT involves automatic rule extraction from aligned p"
E09-1044,J07-2003,0,0.732923,"eeded in search. We describe several techniques to reduce memory usage and search errors in hierarchical trans1.1 Related Work The search and rule pruning techniques described in the following sections add to a growing literature of refinements to the hierarchical phrasebased SMT systems originally described by Chiang (2005; 2007). Subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other SMT architectures. Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, c Athens, Greece, 30 March – 3 April 2009"
E09-1044,W07-0414,0,0.0162592,"bered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test. The mt02-05-tune set has 2,075 sentences. We first compare the cube pruning decoder to the TTM (Kumar et al., 2006), a phrase-based SMT system implemented with Weighted FiniteState Tansducers (Allauzen et al., 2007). The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). Relative to the complex movement and translation allowed by Hiero and other models, MJ1 is clearly inferior (Dreyer et al., 2007); MJ1 was developed with efficiency in mind so as to run with a minimum of search errors in translation and to be easily and exactly realized via WFSTs. Even for the A Study of Hiero Search Errors in Phrase-Based Translation Experiments reported in this paper are based on the NIST MT08 Arabic-to-English translation task. Alignments are generated over all allowed parallel data, (∼150M words per language). Features extracted from the alignments and used in translation are in common use: target language model, source-to-target and target-to-source phrase translation models, word and rule penaltie"
E09-1044,P08-1115,0,0.00479747,"eded for translation, and has also reported contrastive experiments with other SMT architectures. Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 380 extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in trans"
E09-1044,P07-1019,0,0.100645,"Missing"
E09-1044,H05-1021,1,0.321274,"the English Gigaword Third Edition. In addition to the MT08 set itself, we use a development set mt0205-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test. The mt02-05-tune set has 2,075 sentences. We first compare the cube pruning decoder to the TTM (Kumar et al., 2006), a phrase-based SMT system implemented with Weighted FiniteState Tansducers (Allauzen et al., 2007). The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). Relative to the complex movement and translation allowed by Hiero and other models, MJ1 is clearly inferior (Dreyer et al., 2007); MJ1 was developed with efficiency in mind so as to run with a minimum of search errors in translation and to be easily and exactly realized via WFSTs. Even for the A Study of Hiero Search Errors in Phrase-Based Translation Experiments reported in this paper are based on the NIST MT08 Arabic-to-English translation task. Alignments are generated over all allowed parallel data, (∼150M words per language). Features extracted from the alignments and used in translatio"
E09-1044,W08-0402,0,0.0578977,"lly described by Chiang (2005; 2007). Subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other SMT architectures. Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 380 extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugment"
E09-1044,D07-1104,0,0.00790198,"during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006) propose binarization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets. Hierarchical rule extraction Zhang et al. (2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted. Lopez (2007) extracts rules on-the-fly from the training bitext during decoding, searching efficiently for rule patterns using suffix arrays. Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hi"
E09-1044,C08-1064,0,0.0203211,"orithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted. Lopez (2007) extracts rules on-the-fly from the training bitext during decoding, searching efficiently for rule patterns using suffix arrays. Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures (e.g. (Sim et al., 2007; Rosti et al., 2007)). 1.2 rule set size. Finally, Section 4 concludes. 2 Two Refinements in Cube Pruning Chiang (2007) introduced cube pruning to apply language models in pruning during the generation of k-best translation hypotheses via the application of hierarchical rules in the CYK algorithm. In the implementatio"
E09-1044,P08-1114,0,0.00845312,"pril 2009. 2009 Association for Computational Linguistics 380 extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in translation. Shen at al. (2008) make use of target dependency trees and a target dependency language model during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006) propose binarization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets. Hierarchical rule extraction Zhang et al. (2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted. Lopez (2007) extracts rules on-the-fly fro"
E09-1044,P03-1021,0,0.00610394,"row during search we attempt to avoid search errors by ensuring that the universe of items within the frontier queues does not decrease as the k-best lists are filled. 2.3 Figure 1: Spreading neighborhood exploration within a cube, just before and after extraction of the item C. Grey squares represent the frontier queue; black squares are candidates already extracted. Chiang (2007) would only consider adding items X to the frontier queue, so the queue would shrink. Spreading neighborhood exploration adds candidates S to the frontier queue. count features inspired by Bender et al. (2007). MET (Och, 2003) iterative parameter estimation under IBM BLEU is performed on the development set. The English language used model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. In addition to the MT08 set itself, we use a development set mt0205-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test. The mt02-05-tune set has 2,075 sentences. We first compare the cube pruning decoder to the TTM (Kumar et al., 2006), a phra"
E09-1044,N07-1029,0,0.0125801,"sis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures (e.g. (Sim et al., 2007; Rosti et al., 2007)). 1.2 rule set size. Finally, Section 4 concludes. 2 Two Refinements in Cube Pruning Chiang (2007) introduced cube pruning to apply language models in pruning during the generation of k-best translation hypotheses via the application of hierarchical rules in the CYK algorithm. In the implementation of Hiero described here, there is the parser itself, for which we use a variant of the CYK algorithm closely related to CYK+ (Chappelier and Rajman, 1998); it employs hypothesis recombination, without pruning, while maintaining back pointers. Before k-best list generation with cube pruning, we appl"
E09-1044,P08-1066,0,0.0454631,"for mt02-05-tune exceeds 175M rules, of which only 0.62M are simple phrase pairs. The question is whether all these rules are needed for translation. If the rule set can be reduced without reducing translation quality, both memory efficiency and translation speed can be increased. Previously published approaches to reducing the rule set include: enforcing a minimum span of two words per non-terminal (Lopez, 2008), which would reduce our set to 115M rules; or a minimum count (mincount) threshold (Zollmann et al., 2008), which would reduce our set to 78M (mincount=2) or 57M (mincount=3) rules. Shen et al. (2008) describe the result of filtering rules by insisting that target-side rules are well-formed dependency trees. This reduces their rule set from 140M to 26M rules. This filtering leads to a degradation in translation performance (see Table 2 of Shen et al. (2008)), which they counter by adding a dependency LM in translation. As another reference point, Chiang (2007) reports Chinese-to-English translation experiments based on 5.5M rules. Zollmann et al. (2008) report that filtering rules tinct rules. Additionally, patterns with two nonterminals which also have a monotonic relationship between sou"
E09-1044,2006.amta-papers.25,0,0.0327896,"ules are removed. Based on these experiments, we conclude that it is better to apply separate mincount thresholds to the classes to obtain optimal performance with a minimum size rule set. 3.6 list. • Minimum Bayes Risk (MBR). We then rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function to minimise (Kumar and Byrne, 2004). Table 8 shows results for mt02-05-tune, mt0205-test, the NIST subsets from the MT06 evaluation (mt06-nist-nw for newswire data and mt06nist-ng for newsgroup) and mt08, as measured by lowercased IBM BLEU and TER (Snover et al., 2006). Mixed case NIST BLEU for this system on mt08 is 42.5. This is directly comparable to official MT08 evaluation results1 . 4 Conclusions This paper focuses on efficient large-scale hierarchical translation while maintaining good translation quality. Smart memoization and spreading neighborhood exploration during cube pruning are described and shown to reduce memory consumption and Hiero search errors using a simple phrasebased system as a contrast. We then define a general classification of hierarchical rules, based on their number of nonterminals, elements and their patterns, for refined extr"
E09-1044,N07-1063,0,0.0174641,"rs in hierarchical trans1.1 Related Work The search and rule pruning techniques described in the following sections add to a growing literature of refinements to the hierarchical phrasebased SMT systems originally described by Chiang (2005; 2007). Subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other SMT architectures. Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 380 extend the translation of source sentences"
E09-1044,N06-1033,0,0.0334194,"o Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in translation. Shen at al. (2008) make use of target dependency trees and a target dependency language model during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006) propose binarization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets. Hierarchical rule extraction Zhang et al. (2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted. Lopez (2007) extracts rules on-the-fly from the training bitext during decoding, searching efficiently for rule patterns using suffix arrays. Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchic"
E09-1044,C08-1136,0,0.0436458,"Missing"
E09-1044,W06-3119,0,0.00909367,"s in translation speed by taking unseen ngrams into account within cube pruning to minimize language model requests. Dyer et al. (2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 380–388, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 380 extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Blunsom et al. (2008) discuss procedures to combine discriminative latent models with hierarchical SMT. The SyntaxAugmented Machine Translation system (Zollmann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in translation. Shen at al. (2008) make use of target dependency trees and a target dependency language model during decoding. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). Zhang and Gildea (2006) propose binarization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets. Hierarchic"
E09-1044,C08-1144,0,0.572721,"Arabic-toEnglish evaluation task. 1 Introduction Hierarchical phrase-based translation (Chiang, 2005) has emerged as one of the dominant current approaches to statistical machine translation. Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text. The approach has been widely adopted and reported to be competitive with other large-scale data driven approaches, e.g. (Zollmann et al., 2008). Large-scale hierarchical SMT involves automatic rule extraction from aligned parallel text, model parameter estimation, and the use of cube pruning k-best list generation in hierarchical translation. The number of hierarchical rules extracted far exceeds the number of phrase translations typically found in aligned text. While this may lead to improved translation quality, there is also the risk of lengthened translation times and increased memory usage, along with possible search errors due to the pruning procedures needed in search. We describe several techniques to reduce memory usage and"
E09-1044,N04-1022,1,\N,Missing
E14-1026,E09-1011,0,0.0143201,"ple children. We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word align"
E14-1026,C10-1071,0,0.0144243,"ated work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source t"
E14-1026,D13-1049,0,0.65874,"ply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classification task (Lerner and Petrov, 2013). These are appealing for their use of finergrained lexical information, but they struggle to adequately handle nodes with multiple children. We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for Fr"
E14-1026,P05-1066,0,0.434988,"exical information, but they struggle to adequately handle nodes with multiple children. We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neit"
E14-1026,P07-1091,0,0.146309,"y-aligned, and ∗ This work was done during an internship of the first author at SDL Research, Cambridge. 239 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classification task (Lerner and Petrov, 2013). These are appealing for their use of finergrained lexical information, but they struggle to adequately handle nodes with mul"
E14-1026,W06-1609,0,0.47243,"Missing"
E14-1026,2006.amta-papers.4,0,0.0474153,"Missing"
E14-1026,D12-1077,0,0.719881,"h is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the relative position of each pair of words in the sentence, and search for the sequence that optimizes the global score as a linear ordering problem (Tromble and Eisner, 2009) or as a traveling salesman problem (Visweswariah et al., 2011). Yet another line of work attempts to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li Our approach is closely related to this latter work, as we are interested in feature-rich discriminative approaches that automatically learn preor"
E14-1026,D11-1018,0,0.709824,"alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the relative position of each pair of words in the sentence, and search for the sequence that optimizes the global score as a linear ordering problem (Tromble and Eisner, 2009) or as a traveling salesman problem (Visweswariah et al., 2011). Yet another line of work attempts to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering rules can be automatically extracted from word alignments and constituent trees (Li Our approach is closely related to this latter work, as we are interested in feature-rich discriminative approaches that aut"
E14-1026,E99-1010,0,0.0472819,"ng two nodes a and b in the given order is dst gap The dependency labels of each node The part-of-speech tags of each node. The head words and classes of each node. The left-most and right-most words and classes of a node. The distances between each node and the head. If there is a gap between nodes, the left-most and right-most words and classes in the gap. In order to keep the size of our feature space manageable, we only consider features which occur at least 5 times1 . For the lexical features, we use the top 100 vocabulary items from our training data, and 51 clusters generated by mkcls (Och, 1999). Similarly to previous work (Genzel, 2010; Yang et al., 2012), we also explore feature conjunctions. For the tag and label classes, we generate all possible combinations up to a given size. For the lexical and distance features, we explicitly specify conjunctions with the tag and label features. Results for various feature configurations are discussed in Section 4.3.1. cs(a, b) := |{(i, j) ∈ Aa × Ab : i > j}| where Aa and Ab are the target-side positions to which the words spanned by a and b are aligned. The label is then given as y(a, b) = ... Figure 2: Branch-and-bound search: Partial searc"
E14-1026,D08-1089,0,0.0724942,"ic. Source sentences were extracted from the web and one target reference was produced by a bilingual speaker. These sentences were chosen to evenly represent 10 domains, including world news, chat/SMS, health, sport, science, business, and others. The dev/test sets contain 602/903 sentences and 14K/20K words each. We do English part-of-speech tagging using SVMTool (Gim´enez and M`arquez, 2004) and dependency parsing using MaltParser (Nivre et al., 2007). For translation experiments, we use a phrasebased decoder that incorporates a set of standard features and a hierarchical reordering model (Galley and Manning, 2008) with weights tuned using MERT to optimize the character-based BLEU score on the dev set. The Japanese and Korean language models are 5-grams estimated on > 350M words of generic web text. For training the logistic regression model, we automatically align the parallel training data and intersect the source-to-target and target-to-source alignments. We reserve a random 5K-sentence j∈V |i>j Y Experiments 1 − p(i, j) j∈V |i&lt;j where V = {1, ..., k}(π 0 · hii) is the set of source child positions that have not yet been visited. Observe that the nodes at search depth k correspond exactly to the set"
E14-1026,C10-1043,0,0.162365,"ter and better translations, if preordering is done well and efficiently. Preordering also can facilitate better estimation of alignment and translation models as the parallel data becomes more monotonically-aligned, and ∗ This work was done during an internship of the first author at SDL Research, Cambridge. 239 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang"
E14-1026,P09-1090,0,0.0142626,"rforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cas"
E14-1026,gimenez-marquez-2004-svmtool,0,0.0424559,"Missing"
E14-1026,2007.tmi-papers.21,0,0.0930351,"alignment and translation models as the parallel data becomes more monotonically-aligned, and ∗ This work was done during an internship of the first author at SDL Research, Cambridge. 239 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classification task (Lerner and Petrov, 2013). These are appealing for their use of finergrained lexi"
E14-1026,2007.mtsummit-papers.29,0,0.121175,"s word movement needs to be considered, which results in faster and better translations, if preordering is done well and efficiently. Preordering also can facilitate better estimation of alignment and translation models as the parallel data becomes more monotonically-aligned, and ∗ This work was done during an internship of the first author at SDL Research, Cambridge. 239 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat"
E14-1026,D09-1105,0,0.25904,"mon criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the relative position of each pair of words in the sentence, and search for the sequence that optimizes the global score as a linear ordering problem (Tromble and Eisner, 2009) or as a traveling salesman problem (Visweswariah et al., 2011). Yet another line of work attempts to automatically induce a parse tree and a preordering model from word alignments (DeNero and Uszkoreit, 2011; Neubig et al., 2012). These approaches are attractive due to their minimal reliance on linguistic knowledge. However, their findings reveal that the best performance is obtained when using humanaligned data which is expensive to create. Somewhere in the middle of the spectrum are works that rely on automatic source-language syntactic parses, but no direct human intervention. Preordering"
E14-1026,P09-2059,0,0.0206331,"ch and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, whe"
E14-1026,C10-1126,0,0.0572432,"t needs to be considered, which results in faster and better translations, if preordering is done well and efficiently. Preordering also can facilitate better estimation of alignment and translation models as the parallel data becomes more monotonically-aligned, and ∗ This work was done during an internship of the first author at SDL Research, Cambridge. 239 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a"
E14-1026,W10-1736,0,0.175411,"ul way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered versio"
E14-1026,P13-1125,0,0.0145624,"system is able to decode 80 times faster while producing translation output of the same quality. Secondly, we observe that the preordering gains, which are correlated with the crossing score reductions of Table 1, are largely orthogonal to the gains obtained when incorporating a lexicalised reordering model (LRM). In fact, preordering gains are slightly larger with LRM, suggesting that this reordering model can be better estimated with preordered text. This echoes the notion that reordering models are particularly sensitive to alignment noise (DeNero and Uszkoreit, 2011; Neubig et al., 2012; Visweswariah et al., 2013), and that a ‘more monotonic’ training corpus leads to better translation models. Finally, ‘df-bnb’ outperforms all other preordering approaches, and achieves an extra 0.5–0.8 BLEU over the rule-based one even at zero distortion limit. This is consistent with the substantial crossing score reductions reported in Section 4.3. We argue that these improvements are due to the usage of lexical features to facilitate finergrained ordering decisions, and to our better search through the children permutation space which is not restricted by sliding windows, does Translation performance Table 3 reports"
E14-1026,D07-1077,0,0.0736374,"o adequately handle nodes with multiple children. We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntac"
E14-1026,I11-1004,0,0.0125726,"one well and efficiently. Preordering also can facilitate better estimation of alignment and translation models as the parallel data becomes more monotonically-aligned, and ∗ This work was done during an internship of the first author at SDL Research, Cambridge. 239 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239–248, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics et al., 2007; Habash, 2007; Visweswariah et al., 2010), dependency trees (Genzel, 2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classificatio"
E14-1026,C04-1073,0,0.324679,"pealing for their use of finergrained lexical information, but they struggle to adequately handle nodes with multiple children. We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, ther"
E14-1026,N09-1028,0,0.691977,"d of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse tree rewriting or transformation. Examples of these can be found for French-English (Xia and McCord, 2004), German-English (Collins et al., 2005), ChineseEnglish (Wang et al., 2007), English-Arabic (Badr et al., 2009), English-Hindi (Ramanathan et al., 2009), English-Korean (Hong et al., 2009), and English-Japanese (Lee et al., 2010; Isozaki et al., 2010). A generic set of rules for transforming SVO to SOV languages has also been described (Xu et al., 2009). The main advantage of these approaches is that a relatively small set of good rules can yield significant improvements in translation. The common criticism they receive is that they are language-specific. On the other end of the spectrum, there are preordering models that rely neither on human knowledge nor on syntactic analysis, but only on word alignments. One such approach is to form a cascade of two translation systems, where the first one translates the source to its preordered version (Costa-juss`a and Fonollosa, 2006). Alternatively, one can define models that assign a cost to the rel"
E14-1026,P12-1096,0,0.523998,"2010) or predicate-argument structures (Wu et al., 2011), or simply part-of-speech sequences (Crego and Mari˜no, 2006; Rottmann and Vogel, 2007). Rules are assigned a cost based on Maximum Entropy (Li et al., 2007) or Maximum Likelihood estimation (Visweswariah et al., 2010), or directly on their ability to make the training corpus more monotonic (Genzel, 2010). The latter performs very well in practice but comes at the cost of a brute-force extraction heuristic that cannot incorporate lexical information. Recently, other approaches treat ordering the children of a node as a learning to rank (Yang et al., 2012) or discriminative multi-classification task (Lerner and Petrov, 2013). These are appealing for their use of finergrained lexical information, but they struggle to adequately handle nodes with multiple children. We also show it outperforms a multi-class classification approach and analyse why this is the case. 2 Related work One useful way to organize previous preordering techniques is by how they incorporate linguistic knowledge. On one end of the spectrum we find those approaches that rely on syntactic parsers and human knowledge, typically encoded via a set of hand-crafted rules for parse t"
E14-1026,D11-1045,0,\N,Missing
E14-1028,P04-1051,0,0.194587,"We describe the generation grammars and introduce parsing procedures that address the computational complexity of generation under permutation of phrases. Against the best previous results reported on this task, obtained using syntax driven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. 2 Phrase-based Word Ordering We take as input a bag of N words Ω"
E14-1028,C00-1007,0,0.200795,"Missing"
E14-1028,N12-1017,0,0.0209902,"Missing"
E14-1028,W11-2832,0,0.0715511,"y in congress calling for a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling for a new strategy just days before a new confrontation in congress SBLEU 47.2 69.8 39.1 82.9 61.5 80.8 66.7 77.8 82.3 100 Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6 . These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be us"
E14-1028,D10-1055,0,0.211617,"contain at most 12 words. By contrast, Pruned Expansion (Algorithm 2) with β = 10 is feasible for bags of up to 18 words. For 3.2 Generation Performance We now compare the GYRO system with the Combinatory Categorial Grammar (CCG)-based system described in (Zhang et al., 2012). By means of extracted CCG rules, the CCG system searches for an optimal parse guided by large-margin training. Each partial hypothesis (or ‘edge’) is scored using the syntax model and a 4gram LM trained similarly on one billion words of English Gigaword data. Both systems are evaluated using BLEU (Papineni et al., 2002; Espinosa et al., 2010). For GYRO, we use the pruned parsing algorithm of Section 2.4 with r = 6 and β = 10 and a memory usage limit of 20G. The phrasebased rules of the grammar contain only 3-grams, 263 LM 4g 3g 4g System CCG GYRO GYRO +3g GYRO +4g 100-best oracle lattice oracle MT08-nw 48.0 59.0 63.0 65.5 76.1 80.4 MT09-nw 48.8 58.4 64.1 65.9 76.1 80.2 provement from rescoring, in that even for small 100-best lists the improvement found by the Oracle can exceed 10 BLEU points; and (b) that the output lattices are not perfect in that the Oracle score is not 100. 3.2.1 Table 1: CCG and GYRO BLEU scores. Rescoring GY"
E14-1028,C10-1043,0,0.0300121,"ge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. 2 Phrase-based Word Ordering We take as input a bag of N words Ω = {w1 , . . . , wN }. The words are sorted, e.g. alphabetically, so that it is possible to refer to the ith word in the bag, and repeated words are distinct tokens. We also take a set of phrases, L(Ω) that 259 Proceedings of the 14th Conference of the Eu"
E14-1028,W12-1525,0,0.0160275,"ng for a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling for a new strategy just days before a new confrontation in congress SBLEU 47.2 69.8 39.1 82.9 61.5 80.8 66.7 77.8 82.3 100 Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6 . These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be useful to have systems"
E14-1028,D10-1051,0,0.0609237,"Missing"
E14-1028,P10-2006,1,0.898233,"Missing"
E14-1028,C10-1009,1,0.840911,"Missing"
E14-1028,D11-1127,1,0.897552,"Missing"
E14-1028,W11-2835,0,0.0501234,"before a new strategy in congress calling for a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling for a new strategy just days before a new confrontation in congress SBLEU 47.2 69.8 39.1 82.9 61.5 80.8 66.7 77.8 82.3 100 Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6 . These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these prob"
E14-1028,J99-4005,0,0.240416,"ures that address the computational complexity of generation under permutation of phrases. Against the best previous results reported on this task, obtained using syntax driven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. 2 Phrase-based Word Ordering We take as input a bag of N words Ω = {w1 , . . . , wN }. The words are sorted, e.g. alphabet"
E14-1028,P98-1116,0,0.464961,"Missing"
E14-1028,D07-1090,0,0.0922874,"Missing"
E14-1028,J07-2003,0,0.639668,"rder Right), a phrase-based approach to word ordering. Given a bag of words, the system first scans a large, trusted text collection and extracts phrases consisting of words from the bag. Strings are then generated by concatenating these phrases in any order, subject to the constraint that every string is a valid reordering of the words in the bag, and the results are scored under an n-gram language model (LM). The motivation is that it is easier to make fluent sentences from phrases (snippets of fluent text) than from words in isolation. GYRO builds on approaches developed for syntactic SMT (Chiang, 2007; de Gispert et al., 2010; Iglesias et al., 2011). The system generates strings in the form of weighted automata which can be rescored using higher-order n-gram LMs, dependency LMs (Shen et al., 2010), and Minimum Bayes Risk decoding, either using posterior probabilities obtained from GYRO or SMT systems. We report extensive experiments using BLEU and conclude with human assessments. We show that despite its relatively simple formulation, GYRO gives BLEU scores over 20 points higher than the best previously reported results, generated by a syntax-based ordering system. Human fluency assessment"
E14-1028,N09-2019,1,0.875164,"Missing"
E14-1028,P02-1038,0,0.0746264,"as automatic speech recognition and SMT that could possibly benefit from NLG, but which do not generate reliable linguistic annotation in their output. For these problems it would be useful to have systems, as described in this paper, which do not require rich input representations. We plan to investigate these applications in future work. There is much opportunity for future development. To improve coverage, the grammars of Section 2.1 could perform generation with overlapping, rather than concatenated, n-grams; and features could be included to define tuneable loglinear rule probabilities (Och and Ney, 2002; Chiang, 2007). The GYRO grammar could be extended using techniques from string-to-tree SMT, in particular by modifying the grammar so that output derivations respect dependencies (Shen et al., 2010); this will make it easier to integrate dependency LMs into GYRO. Finally, it would be interesting to couple the GYRO architecture with automata-based models of poetry and rhythmic text (Greene et al., 2010). Acknowledgement The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7-ICT-2009-4) under grant agreement number 247762, the FAUST"
E14-1028,J10-3008,1,0.902259,"Missing"
E14-1028,P02-1040,0,0.0911806,"feasible for bags that contain at most 12 words. By contrast, Pruned Expansion (Algorithm 2) with β = 10 is feasible for bags of up to 18 words. For 3.2 Generation Performance We now compare the GYRO system with the Combinatory Categorial Grammar (CCG)-based system described in (Zhang et al., 2012). By means of extracted CCG rules, the CCG system searches for an optimal parse guided by large-margin training. Each partial hypothesis (or ‘edge’) is scored using the syntax model and a 4gram LM trained similarly on one billion words of English Gigaword data. Both systems are evaluated using BLEU (Papineni et al., 2002; Espinosa et al., 2010). For GYRO, we use the pruned parsing algorithm of Section 2.4 with r = 6 and β = 10 and a memory usage limit of 20G. The phrasebased rules of the grammar contain only 3-grams, 263 LM 4g 3g 4g System CCG GYRO GYRO +3g GYRO +4g 100-best oracle lattice oracle MT08-nw 48.0 59.0 63.0 65.5 76.1 80.4 MT09-nw 48.8 58.4 64.1 65.9 76.1 80.2 provement from rescoring, in that even for small 100-best lists the improvement found by the Oracle can exceed 10 BLEU points; and (b) that the output lattices are not perfect in that the Oracle score is not 100. 3.2.1 Table 1: CCG and GYRO B"
E14-1028,N10-1141,0,0.0161358,"0 40 65 30 60 20 55 10 50 0 1-5 6-10 11-15 16-20 21-25 26+ all sizes Size of the bag of words terior probabilities extracted from (a) the same generation lattice, and (b) from lattices produced by an Arabic-to-English hierarchical-phrase based MT system developed for the NIST 2012 OpenMT Evaluation. As noted, LMBR relies on a posterior distribution over n-grams as part of its computation or risk. Here, we use LMBR with a posterior of the form α pGYRO + (1–α) pMT . This is effectively performing a system combination between the GYRO generation system and the MT system (de Gispert et al., 2009; DeNero et al., 2010) but restricting the hypothesis space to be that of the GYRO lattice (Blackwood et al., 2010b). Results are reported in the last two rows of Table 2. Relative to 5-gram LM rescoring alone, we see gains in BLEU of 2.3 and 4.4 in MT08-nw and MT09nw, suggesting that posterior distributions over ngrams provided by SMT systems can give good guidance in generation. These results also suggest that if we knew what words to use, we could generate very good quality translation output. 3.3 90 85 Sentence Precision Rate (SPR) MT08-nw 68.5 68.7 ? 68.7 68.6 70.8 ? 70.8 BLEU score 4g GYRO rescoring: +5g +5g"
E14-1028,A00-2026,0,0.187782,"Missing"
E14-1028,J10-4005,0,0.201998,"s are then generated by concatenating these phrases in any order, subject to the constraint that every string is a valid reordering of the words in the bag, and the results are scored under an n-gram language model (LM). The motivation is that it is easier to make fluent sentences from phrases (snippets of fluent text) than from words in isolation. GYRO builds on approaches developed for syntactic SMT (Chiang, 2007; de Gispert et al., 2010; Iglesias et al., 2011). The system generates strings in the form of weighted automata which can be rescored using higher-order n-gram LMs, dependency LMs (Shen et al., 2010), and Minimum Bayes Risk decoding, either using posterior probabilities obtained from GYRO or SMT systems. We report extensive experiments using BLEU and conclude with human assessments. We show that despite its relatively simple formulation, GYRO gives BLEU scores over 20 points higher than the best previously reported results, generated by a syntax-based ordering system. Human fluency assessments confirm these substantial improvements. We describe an approach to word ordering using modelling techniques from statistical machine translation. The system incorporates a phrase-based model of stri"
E14-1028,E12-1013,0,0.0389721,"Missing"
E14-1028,P05-1009,0,0.420309,"R EPLACE(FN,1 ) 5 return L ← F ◦ G 6 function P RUNE S -ROW(x) : 7 F ← y Fx,y 8 F ← FSA-R EPLACE(F ) 9 F ←F ◦G 10 F ← FSA-P RUNE(F, β)  11 for each cell y = 1, . . . , N x 12 Fx,y ← Fx,y · F 13 return Figure 4: Pseudocode for Algorithm 1 (excluding lines 2-3) and Algorithm 2 (including all lines). the LM during parsing. The pseudocode is identical to that of Algorithm 1 except for the following changes: in parsing (Figure 2) we pass G as input and we call the row pruning function of Figure 4 after line 5 if x ≥ r. We note that there is a strong connection between GYRO and the IDL approach of Soricut and Marcu (2005; 2006). Our bag of words parser could be cast in the IDL-formalism, and the FSA ‘Replace’ operation would be expressed by an IDL ‘Unfold’ operation. However, whereas their work applies pruning in the creation of the IDLexpression prior to LM application, GYRO uses unweighted phrase constraints so the LM must be considered for pruning while parsing. Algorithm 3: Pruned Parsing and Generation 3 The two generation algorithms presented above rely on a completed initial parsing step. However, given that the complexity of the parsing stage is O(2N · K), this may not be achievable in practice. Leavi"
E14-1028,P06-1139,0,0.097572,"f greater than 85. Performance is 4 Related Work and Conclusion Our work is related to surface realisation within natural language generation (NLG). NLG typically assumes a relatively rich input representation intended to provide syntactic, semantic, and other relationships to guide generation. Example input representations are Abstract Meaning Representations (Langkilde and Knight, 1998), attributevalue pairs (Ratnaparkhi, 2000), lexical predicateargument structures (Bangalore and Rambow, 2000), Interleave-Disjunction-Lock (IDL) expressions (Nederhof and Satta, 2004; Soricut and Marcu, 2005; Soricut and Marcu, 2006), CCGbank derived grammars (White et al., 2007), 265 REF (a) (b) (c) (d) REF (a-c) (d) REF (a) (b) (c) (d) Hypothesis a third republican senator joins the list of critics of bush ’s policy in iraq . critics of bush ’s iraq policy in a third of republican senator joins the list . critics of bush ’s policy in iraq joins the list of a third republican senator . critics of bush ’s iraq policy in a list of republican senator joins the third . the list of critics of bush ’s policy in iraq a third republican senator joins . it added that these messages were sent to president bashar al-asad through tu"
E14-1028,D09-1105,0,0.0241349,"riven models, we report huge quality improvements, with BLEU score gains of 20+ which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. 2 Phrase-based Word Ordering We take as input a bag of N words Ω = {w1 , . . . , wN }. The words are sorted, e.g. alphabetically, so that it is possible to refer to the ith word in the bag, and repeated words are distinct tokens. We also take a set of phrases, L(Ω) that 259 Proceedings of the 14th Confe"
E14-1028,D08-1065,0,0.0748302,"Missing"
E14-1028,E09-1097,0,0.334788,"age models, large n-gram language models, and minimum Bayes risk decoding. 1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. 2 Phrase-based Word Ordering We take as input a bag of N words Ω = {w1 , . . . , wN }. The words are sorted, e.g. alphabetically, so that it is possible to refer to the ith word in the bag, and repeated words are distinct tokens. We also take a set of phrases, L(Ω) that 259 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 259–268, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Compu"
E14-1028,2007.mtsummit-ucnlg.4,0,0.0470482,"Conclusion Our work is related to surface realisation within natural language generation (NLG). NLG typically assumes a relatively rich input representation intended to provide syntactic, semantic, and other relationships to guide generation. Example input representations are Abstract Meaning Representations (Langkilde and Knight, 1998), attributevalue pairs (Ratnaparkhi, 2000), lexical predicateargument structures (Bangalore and Rambow, 2000), Interleave-Disjunction-Lock (IDL) expressions (Nederhof and Satta, 2004; Soricut and Marcu, 2005; Soricut and Marcu, 2006), CCGbank derived grammars (White et al., 2007), 265 REF (a) (b) (c) (d) REF (a-c) (d) REF (a) (b) (c) (d) Hypothesis a third republican senator joins the list of critics of bush ’s policy in iraq . critics of bush ’s iraq policy in a third of republican senator joins the list . critics of bush ’s policy in iraq joins the list of a third republican senator . critics of bush ’s iraq policy in a list of republican senator joins the third . the list of critics of bush ’s policy in iraq a third republican senator joins . it added that these messages were sent to president bashar al-asad through turkish and german officials . it added that pres"
E14-1028,N07-1022,0,0.0571414,"enator has joined the ranks of critics of george bush ’s policy in iraq , just days before a new strategy in congress calling for a new confrontation a prominent republican senator has joined the ranks of critics of george bush ’s policy in iraq , calling for a new strategy just days before a new confrontation in congress SBLEU 47.2 69.8 39.1 82.9 61.5 80.8 66.7 77.8 82.3 100 Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b) GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical hypotheses. meaning representation languages (Wong and Mooney, 2007) and unordered syntactic dependency trees (Guo et al., 2011; Bohnet et al., 2011; Belz et al., 2011; Belz et al., 2012)6 . These input representations are suitable for applications such as dialog systems, where the system maintains the information needed to generate the input representation for NLG (Lemon, 2011), or summarisation, where representations can be automatically extracted from coherent, well-formed text (Barzilay and Elhadad, 2011; Althaus et al., 2004). However, there are other applications, such as automatic speech recognition and SMT that could possibly benefit from NLG, but whic"
E14-1028,D11-1106,0,0.247024,"1 Introduction Word ordering is a fundamental problem in NLP and has been shown to be NP-complete in discourse ordering (Althaus et al., 2004) and in SMT with arbitrary word reordering (Knight, 1999). Typical solutions involve constraints on the space of permutations, as in multi-document summarisation (Barzilay and Elhadad, 2011) and preordering in SMT (Tromble and Eisner, 2009; Genzel, 2010). Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) use a dependency grammar to address word ordering, while Zhang and Clark (2011; 2012) use CCG and large-scale n-gram language models. 2 Phrase-based Word Ordering We take as input a bag of N words Ω = {w1 , . . . , wN }. The words are sorted, e.g. alphabetically, so that it is possible to refer to the ith word in the bag, and repeated words are distinct tokens. We also take a set of phrases, L(Ω) that 259 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 259–268, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics are extracted from large text collections, and contain onl"
E14-1028,C08-1038,0,\N,Missing
E14-1028,E12-1075,0,\N,Missing
E14-1028,C98-1112,0,\N,Missing
E14-1028,2010.iwslt-keynotes.2,0,\N,Missing
E14-1028,N04-1022,0,\N,Missing
E17-2058,P13-2121,0,0.0327812,"Missing"
E17-2058,D16-1162,0,0.143651,"Missing"
E17-2058,N04-1022,0,0.0944175,"t avoids using NMT scores for risk calculation. We show how to reformulate the original LMBR decision rule for using it in a word-based NMT decoder which is not restricted to an n-best list or a lattice. Our hybrid system outperforms lattice rescoring on multiple data sets for EnglishGerman and Japanese-English. We report similar gains from applying our method to subword-unitbased NMT rather than word-based NMT. Introduction Lattice minimum Bayes-risk (LMBR) decoding has been applied successfully to translation lattices in traditional SMT to improve translation performance of a single system (Kumar and Byrne, 2004; Tromble et al., 2008; Blackwood et al., 2010). However, minimum Bayes-risk (MBR) decoding is also a very powerful framework for combining diverse systems (Sim et al., 2007; de Gispert et al., 2009). Therefore, we study combining traditional SMT and NMT in a hybrid decoding scheme based on MBR. We argue that MBR-based methods in their present form are not well-suited for NMT because of the following reasons: 2 Combining NMT and SMT by Minimising the Lattice Bayes-risk We propose to collect statistics for MBR from a potentially large translation lattice generated with SMT, and use the n-gram p"
E17-2058,P10-2006,1,0.928326,"Missing"
E17-2058,D15-1249,0,0.0130546,"the Blocks and Theano frameworks (van Merri¨enboer et al., 2015; Bastien et al., 2012) with hyper-parameters as in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-based NMT (Chitnis and DeNero, 2015; Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016; Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Wu et al., 2016) does not use isolated words as modelling units but applies a finer grained tokenization scheme. One of the main motivation for these approaches is to overcome the limited vocabulary in word-based NMT. We consider our hybrid system as an alternative way to fix NMT OOVs. However, our method can also be used with subword-unit-based NMT. In this work, we use byte pair encodings (Sennrich et al., 2016, BPE) to test combining word-based SMT with subword-unit-bas"
E17-2058,P16-1100,0,0.0118496,"ien et al., 2012) with hyper-parameters as in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-based NMT (Chitnis and DeNero, 2015; Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016; Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Wu et al., 2016) does not use isolated words as modelling units but applies a finer grained tokenization scheme. One of the main motivation for these approaches is to overcome the limited vocabulary in word-based NMT. We consider our hybrid system as an alternative way to fix NMT OOVs. However, our method can also be used with subword-unit-based NMT. In this work, we use byte pair encodings (Sennrich et al., 2016, BPE) to test combining word-based SMT with subword-unit-based NMT via both lattice rescoring and MBR. First, we construct a fin"
E17-2058,P16-1160,0,0.0129826,"r et al., 2015; Bastien et al., 2012) with hyper-parameters as in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-based NMT (Chitnis and DeNero, 2015; Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016; Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Wu et al., 2016) does not use isolated words as modelling units but applies a finer grained tokenization scheme. One of the main motivation for these approaches is to overcome the limited vocabulary in word-based NMT. We consider our hybrid system as an alternative way to fix NMT OOVs. However, our method can also be used with subword-unit-based NMT. In this work, we use byte pair encodings (Sennrich et al., 2016, BPE) to test combining word-based SMT with subword-unit-based NMT via both lattice rescoring and MBR."
E17-2058,P15-1002,0,0.0493514,"Missing"
E17-2058,P16-2058,0,0.0228423,"in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-based NMT (Chitnis and DeNero, 2015; Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016; Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Wu et al., 2016) does not use isolated words as modelling units but applies a finer grained tokenization scheme. One of the main motivation for these approaches is to overcome the limited vocabulary in word-based NMT. We consider our hybrid system as an alternative way to fix NMT OOVs. However, our method can also be used with subword-unit-based NMT. In this work, we use byte pair encodings (Sennrich et al., 2016, BPE) to test combining word-based SMT with subword-unit-based NMT via both lattice rescoring and MBR. First, we construct a finite state 2 3 364 Comparable to ht"
E17-2058,D08-1076,0,0.139487,"Missing"
E17-2058,N09-2019,1,0.830192,"Missing"
E17-2058,J10-3008,1,0.885453,"Missing"
E17-2058,P14-2024,0,0.0203616,"Missing"
E17-2058,W15-5003,0,0.252697,"Missing"
E17-2058,P13-4016,0,0.0726014,"Missing"
E17-2058,N15-3009,0,0.0363475,"Missing"
E17-2058,P16-1162,0,0.0971626,"eworks (van Merri¨enboer et al., 2015; Bastien et al., 2012) with hyper-parameters as in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-based NMT (Chitnis and DeNero, 2015; Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016; Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Wu et al., 2016) does not use isolated words as modelling units but applies a finer grained tokenization scheme. One of the main motivation for these approaches is to overcome the limited vocabulary in word-based NMT. We consider our hybrid system as an alternative way to fix NMT OOVs. However, our method can also be used with subword-unit-based NMT. In this work, we use byte pair encodings (Sennrich et al., 2016, BPE) to test combining word-based SMT with subword-unit-based NMT via both lattice"
E17-2058,W16-2324,1,0.873055,"hese n-grams u, we smooth P (u|Ye ) by mixing it with the uniform distribution to flatten the distribution and increase the offset to n-grams which are not in the lattice. 5 6 Experimental Setup We test our approach on English-German (En-De) and Japanese-English (Ja-En). For En-De, we use the WMT news-test2014 (the filtered version) as a development set, and keep news-test2015 and news-test2016 as test sets. For Ja-En, we use the ASPEC corpus (Nakazawa et al., 2016) to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). The NMT systems are as described by Stahlberg et al. (2016b) using the Blocks and Theano frameworks (van Merri¨enboer et al., 2015; Bastien et al., 2012) with hyper-parameters as in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-ba"
E17-2058,P16-2049,1,0.867259,"hese n-grams u, we smooth P (u|Ye ) by mixing it with the uniform distribution to flatten the distribution and increase the offset to n-grams which are not in the lattice. 5 6 Experimental Setup We test our approach on English-German (En-De) and Japanese-English (Ja-En). For En-De, we use the WMT news-test2014 (the filtered version) as a development set, and keep news-test2015 and news-test2016 as test sets. For Ja-En, we use the ASPEC corpus (Nakazawa et al., 2016) to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT). The NMT systems are as described by Stahlberg et al. (2016b) using the Blocks and Theano frameworks (van Merri¨enboer et al., 2015; Bastien et al., 2012) with hyper-parameters as in (Bahdanau et al., 2015) and a vocabulary size of 30k for Ja-En and 50k for En-De. We use the coverage penalty proposed by Wu et al. (2016) to improve the length and coverage of translations. Our final ensembles combine five (En-De) to six (Ja-En) independently trained NMT systems. Our En-De SMT baseline is a hierarchical system based on the HiFST package3 which produces rich output lattices. The system uses rules exSubword-unit-based NMT Character-based or subword-unit-ba"
E17-2058,D08-1065,0,0.158605,"es for risk calculation. We show how to reformulate the original LMBR decision rule for using it in a word-based NMT decoder which is not restricted to an n-best list or a lattice. Our hybrid system outperforms lattice rescoring on multiple data sets for EnglishGerman and Japanese-English. We report similar gains from applying our method to subword-unitbased NMT rather than word-based NMT. Introduction Lattice minimum Bayes-risk (LMBR) decoding has been applied successfully to translation lattices in traditional SMT to improve translation performance of a single system (Kumar and Byrne, 2004; Tromble et al., 2008; Blackwood et al., 2010). However, minimum Bayes-risk (MBR) decoding is also a very powerful framework for combining diverse systems (Sim et al., 2007; de Gispert et al., 2009). Therefore, we study combining traditional SMT and NMT in a hybrid decoding scheme based on MBR. We argue that MBR-based methods in their present form are not well-suited for NMT because of the following reasons: 2 Combining NMT and SMT by Minimising the Lattice Bayes-risk We propose to collect statistics for MBR from a potentially large translation lattice generated with SMT, and use the n-gram posteriors as additiona"
E17-2058,D13-1140,0,0.0608796,"g NMT with traditional SMT by biasing NMT scores towards translations with low Bayes-risk with respect to the SMT lattice. We reported significant improvements of the new method over lattice rescoring on Japanese-English and EnglishGerman and showed that it can make good use even of very small lattices and n-best lists. In this work, we calculated the Bayes-risk over non-neural SMT lattices. In the future, we are planning to introduce neural models to the risk estimation while keeping the computational complexity under control, e.g. by using neural n-gram language models (Bengio et al., 2003; Vaswani et al., 2013) or approximations of NMT scores (Lecorv´e and Motlicek, 2012; Liu et al., 2016) for n-gram posterior calculation. Related Work Combining the advantages of NMT and traditional SMT has received some attention in current research. A recent line of research attempts to integrate SMT-style translation tables into the NMT system (Zhang and Zong, 2016; Arthur et al., 2016; He et al., 2016). Wang et al. (2016) interpolated NMT posteriors with word recommendations from SMT and jointly trained NMT together with a gating function which assigns the weight between SMT and NMT scores dynamically. Neu7 Conc"
E17-2058,P16-5005,0,0.0171812,"Missing"
J06-4004,W05-0823,1,0.838951,"Missing"
J06-4004,W00-0508,0,0.0142428,"y approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state perspective—more specifically, from"
J06-4004,J96-1002,0,0.02988,"Missing"
J06-4004,J90-2002,0,0.81424,"nslation was conceived as the problem of finding a sentence by decoding a given “encrypted” version of it (Weaver 1955). Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time (Hutchins 1986). Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data. The first SMT systems were developed in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions. For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora (Brown et al. 1993). In the case of target"
J06-4004,J93-2003,0,0.0556776,"d in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for adequacy of translation contents, times a target language probability p(T), which accounts for fluency of target constructions. For these first SMT systems, translation-model probabilities at the sentence level were approximated from word-based translation models that were trained by using bilingual corpora (Brown et al. 1993). In the case of target language probabilities, these were generally trained from monolingual data by using n-grams. Present SMT systems have evolved from the original ones in such a way that mainly differ from them in two respects: first, word-based translation models have been ∗ Department of Signal Theory and Communications, Campus Nord, Barcelona 08034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 replaced by phrase-b"
J06-4004,J04-2004,0,0.856987,"models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state perspective—more specifically, from the work of Casacuberta (2001) and Casacuberta and Vidal (2004). However, whereas in this earlier work the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilin˜ 2002). An alternagual units, which will be referred to as tuples (de Gispert and Marino tive approach, which relies on bilingual-unit un"
J06-4004,N04-1033,0,0.0140655,"Missing"
J06-4004,2005.iwslt-1.23,1,0.883592,"Missing"
J06-4004,2005.mtsummit-papers.37,1,0.855856,"Missing"
J06-4004,2006.amta-papers.4,1,0.763242,"Missing"
J06-4004,P05-2012,1,0.804737,"Missing"
J06-4004,C86-1155,0,0.079146,"ament Plenary Sessions (EPPS). 1. Introduction The beginnings of statistical machine translation (SMT) can be traced back to the early fifties, closely related to the ideas from which information theory arose (Shannon and Weaver 1949) and inspired by works on cryptography (Shannon 1949, 1951) during World War II. According to this view, machine translation was conceived as the problem of finding a sentence by decoding a given “encrypted” version of it (Weaver 1955). Although the idea seemed very feasible, enthusiasm faded shortly afterward because of the computational limitations of the time (Hutchins 1986). Finally, during the nineties, two factors made it possible for SMT to become an actual and practical technology: first, significant increment in both the computational power and storage capacity of computers, and second, the availability of large volumes of bilingual data. The first SMT systems were developed in the early nineties (Brown et al. 1990, 1993). These systems were based on the so-called noisy channel approach, which models the probability of a target language sentence T given a source language sentence S as the product of a translation-model probability p(S|T), which accounts for"
J06-4004,knight-al-onaizan-1998-translation,0,0.230855,"more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitestate transducers for which transition probabilities are learned from bilingual data. As opposed to phrase-based translation models, which consider probabilities between target and source units referred to as phrases, finite-state translation models rely on probabilities among sequences of bilingual units, which are defined by the transitions of the transducer. The translation system described in this article implements a translation model that has been derived from the finite-state persp"
J06-4004,N03-1017,0,0.0258625,"Missing"
J06-4004,2005.mtsummit-papers.36,1,0.177804,"Missing"
J06-4004,P00-1056,0,0.0440511,"tence pairs are removed from the training data to allow for a better performance of the alignment tool. Sentence pairs are removed according to the following two criteria: r r Fertility filtering: removes sentence pairs with a word ratio larger than a predefined threshold value. Length filtering: removes sentence pairs with at least one sentence of more than 100 words in length. This helps to maintain bounded alignment computational times. After preprocessing, word-to-word alignments are performed in both directions, source-to-target and target-to-source. In our system implementation, GIZA++ (Och and Ney 2000) is used for computing the alignments. A total of five iterations for models IBM-1 and HMM, and three iterations for models IBM-3 and IBM-4, are performed. Then, the obtained alignment sets are used for computing the intersection and the union of alignments from which tuples and embedded-word tuples are extracted, respectively. 4.2.2 Tuple Extraction and Pruning. A tuple set for each translation direction is extracted from the union set of alignments while avoiding source-nulled tuples by using the procedure described in Section 2.2.2. Then, the resulting tuple vocabularies are pruned accordin"
J06-4004,P02-1038,0,0.884384,"034, Spain. Submission received: 9 August 2005; revised submission received: 26 April 2006; accepted for publication: 5 July 2006 © 2006 Association for Computational Linguistics Computational Linguistics Volume 32, Number 4 replaced by phrase-based translation models (Zens, Och, and Ney 2002; Koehn, Och, and Marcu 2003) which are directly estimated from aligned bilingual corpora by considering relative frequencies, and second, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple feature functions is implemented (Och and Ney 2002). As an extension of the machine translation problem, technological advances in the fields of automatic speech recognition (ASR) and text to speech synthesis (TTS) made it possible to envision the challenge of spoken language translation (SLT) (Kay, Gawron, and Norvig 1992). According to this, SMT has also been approached from a finite-state point of view as the most natural way of integrating ASR and SMT (Riccardi, Pieraccini, and Bocchieri 1996; Vidal 1997; Knight and Al-Onaizan 1998; Bangalore and Riccardi 2000). In this SMT approach, translation models are implemented by means of finitesta"
J06-4004,J03-1002,0,0.00706932,"gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3. Both translation directions, Spanish to English (ES → EN) and English to Spanish (EN → ES), are considered in each table. In the case of Table 2, model size and translation accuracy are evaluated against the type of alignment set used for extracting tuples. Three different alignment sets are considered: source-to-target, the union of source-to-target and target-to-source, and the “refined” alignment method described by Och and Ney (2003). For the results presented in Table 2, a pruning parameter value of N = 20 was used for the Spanish-to-English direction, while a value of N = 30 was used for the English-to-Spanish direction. As can be clearly seen in Table 2, the union alignment set happens to be the most favorable one for extracting tuples in both translation directions since it provides a significantly better translation accuracy, in terms of BLEU score, than the other two alignment sets considered. Notice also in Table 2 that the union set is the one providing the smallest model sizes according to the number of bigrams a"
J06-4004,P02-1040,0,0.105596,"alignment sets. Notice that BLEU measurements in this table correspond to translations computed by using the tuple n-gram model alone. Direction Alignment set Tuple voc. Bigrams Trigrams BLEU ES → EN Source-to-target union refined Source-to-target union refined 1.920 2.040 2.111 1.813 2.023 2.081 6.426 6.009 6.851 6.263 6.092 6.920 2.353 1.798 2.398 2.268 1.747 2.323 0.4424 0.4745 0.4594 0.4152 0.4276 0.4193 EN → ES when tuples are extracted from different alignment sets and when different pruning parameters are used, respectively. Translation accuracy is measured in terms of the BLEU score (Papineni et al. 2002), which is computed here for translations generated by using the tuple n-gram model alone, in the case of Table 2, and by using the tuple n-gram model along with the additional four feature functions described in Section 3.2, in the case of Table 3. Both translation directions, Spanish to English (ES → EN) and English to Spanish (EN → ES), are considered in each table. In the case of Table 2, model size and translation accuracy are evaluated against the type of alignment set used for extracting tuples. Three different alignment sets are considered: source-to-target, the union of source-to-targ"
J06-4004,N03-2036,0,0.00459439,"k the translation model is implemented by using a finite-state transducer, in the system presented here the translation model is implemented by using n-grams. In this way, the proposed translation system can take full advantage of the smoothing and consistency provided by standard back-off n-gram models. The translation model presented here actually constitutes a language model of a sort of “bilanguage” composed of bilin˜ 2002). An alternagual units, which will be referred to as tuples (de Gispert and Marino tive approach, which relies on bilingual-unit unigram probabilities, was developed by Tillmann and Xia (2003); in contrast, the approach presented here considers bilingualunit n-gram probabilities. In addition to the tuple n-gram translation model, the translation system presented here implements four specific feature functions that are log-linearly combined along with the translation model for performing the decoding ˜ et al. 2005). (Marino This article is intended to provide a detailed description of the n-gram-based translation system, as well as to demonstrate the system performance in a widedomain, large-vocabulary translation task. The article is structured as follows. First, Section 2 presents"
J06-4004,2002.tmi-tutorials.2,0,0.201063,"Missing"
J06-4004,2004.iwslt-evaluation.14,1,\N,Missing
J06-4004,N04-1021,0,\N,Missing
J10-3008,P03-1006,0,0.1808,"Missing"
J10-3008,P08-1024,0,0.030331,"Missing"
J10-3008,D07-1090,0,0.0360014,"et al. (2007). 1 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl 2 See http://projects.ldc.upenn.edu/gale/data/catalog.html 521 Computational Linguistics Volume 36, Number 3 We use two types of language model in translation. In ﬁrst-pass translation we use 4-gram language models estimated over the English side of the parallel text (for each language pair) and a 965 million word subset of monolingual data from the English Gigaword Third Edition (LDC2007T07). These are the language models used if pruning is needed during search. The main language model is a zero-cutoff stupid-backoff (Brants et al. 2007) 5-gram language model, estimated using 6.6B words of English text from the English Gigaword corpus. These language models are converted to WFSTs as needed (Allauzen, Mohri, and Roark 2003); failure transitions are used for correct application of back-off weights. In tuning the systems, standard MERT (Och 2003) iterative parameter estimation under IBM BLEU is performed on the development sets. 4.2 Contrast between HiFST and Cube Pruning We contrast two hierarchical phrase-based decoders. The ﬁrst decoder, HCP, is a k-best decoder using cube pruning following the description by Chiang (2007); i"
J10-3008,W08-0309,0,0.0184059,"higher than that of the individual systems. A higher proportion of the n-grams assigned high posterior probability under the interpolated distribution are found in the references and this is one of the reasons for the large gains in BLEU in lattice-based MBR system combination. 4.6 European Language Translation The HiFST described here has also been found to achieve competitive performance for other language pairs, such as Spanish-to-English and Finnish-to-English. For Spanish-to-English we carried out experiments on the shared task of the ACL 2008 Workshop on Statistical Machine Translation (Callison-Burch et al. 2008) based on the Europarl corpus. For the ofﬁcial test2008 evaluation set we obtain a BLEU score of 34.2 using a shallow-1 grammar. Similarly to the Arabic case, deeper grammars are not found to improve scores for this task. In Finnish-to-English, we conducted translation experiments based on the Europarl corpus using 3,000 sentences from the Q4/2000 period for testing with a single reference. In this case, the shallow-1 grammar obtained a BLEU score of 28.0, whereas the full hierarchical grammar only achieved 27.6. This is further evidence that fullhierarchical grammars are not appropriate in al"
J10-3008,P05-1033,0,0.828266,"rameter optimization, and improved translation performance. The direct generation of translation lattices in the target language can improve subsequent rescoring procedures, yielding further gains when applying long-span language models and Minimum Bayes Risk decoding. We also provide insights as to how to control the size of the search space deﬁned by hierarchical rules. We show that shallow-n grammars, low-level rule catenation, and other search constraints can help to match the power of the translation system to speciﬁc language pairs. 1. Introduction Hierarchical phrase-based translation (Chiang 2005) is one of the current promising approaches to statistical machine translation (SMT). Hiero SMT systems are based on probabilistic synchronous context-free grammars (SCFGs) whose translation rules ∗ University of Cambridge, Department of Engineering. CB2 1PZ Cambridge, U.K. E-mail: {ad465,gwb24,wjb31}@eng.cam.ac.uk. ∗∗ University of Vigo, Department of Signal Processing and Communications, 36310 Vigo, Spain. E-mail: {giglesia,erbanga}@gts.tsc.uvigo.es. Submission received: 30 October 2009; revised submission received: 24 February 2010; accepted for publication: 10 April 2010. © 2010 Associatio"
J10-3008,J07-2003,0,0.819577,"should generate large collections of candidate hypotheses that are simultaneously diverse and of good quality. Relative to these concerns, previously published descriptions of Hiero have noted certain limitations. Spurious ambiguity (Chiang 2005) was described as a situation where the decoder produces many derivations that are distinct yet have the same model feature vectors and give the same translation. This can result in n-best lists with very few different translations which is problematic for the minimum-error-rate training algorithm ... This is due in part to the cube pruning procedure (Chiang 2007), which enumerates all distinct hypotheses to a ﬁxed depth by means of k-best hypothesis lists. If enumeration was not necessary, or if the lists could be arbitrarily deep, there might still be many duplicate derivations, but at least the hypothesis space would not be impoverished. Spurious ambiguity is also related to overgeneration (Varile and Lau 1988; Wu 1997; Setiawan et al. 2009). For our purposes we say that overgeneration occurs when different derivations based on the same set of rules give rise to different translations. An example is given in Figure 1. This process is not necessarily"
J10-3008,N09-2019,1,0.873435,"Missing"
J10-3008,J08-3004,0,0.0412936,"Missing"
J10-3008,P05-1071,0,0.00531649,"that for system combination, decoding often produces output that is slightly shorter than required. A ﬁxed per-word factor optimized on the tuning set is applied when computing the gain and this results in output with improved BLEU score and reduced brevity penalty. Table 7 shows translation results in Arabic-to-English using three alternative morphological decompositions of the Arabic text (upper rows a, b, and c). For each decomposition an independent set of hierarchical rules is obtained from the respective parallel corpus alignments. The decompositions were generated by the MADA toolkit (Habash and Rambow 2005) with two alternative tokenization schemes, and by the Sakhr Arabic Morphological Tagger, developed by Sakhr Software in Egypt. The following rows show the results when combining with MBR the translation hypotheses obtained from two or three decompositions. The table also shows a contrast 529 Computational Linguistics Volume 36, Number 3 Figure 13 Average per-sentence 4-gram reference precisions for Arabic-to-English mt02-05-tune single-system MBR 1-best translations and the 1-best obtained through MBR system combination. between decoding the joint k-best lists (rows named MBR, with k = 1, 000"
J10-3008,N09-1049,1,0.778828,"Missing"
J10-3008,knight-al-onaizan-1998-translation,0,0.103549,"neration, we revisit the synchronous context-free grammar deﬁned by hierarchical rules and take a shallow-1 grammar as a starting point. We then increase the complexity of the rules until the desired translation quality is found. 507 Computational Linguistics Volume 36, Number 3 With these reﬁnements we ﬁnd that hierarchical phrase-based translation can be efﬁciently carried out with no (or minimal) search errors in large-data tasks and can achieve state-of-the-art translation performance. There are many beneﬁts to formulating Hiero translation in terms of WFSTs. Following the manner in which Knight and Al-Onaizan (1998), Kumar, Deng, and Byrne (2006), and Graehl, Knight, and May (2008) elucidate other machine translation models, we can use WFST operations to make the operations of the Hiero decoder very clear. The simplicity of the analysis makes it possible to focus on the underlying grammars and avoid the complexities of heuristic search procedures. Once the decoder is formulated, implementation is mostly straightforward using standard WFST techniques developed for language processing (Mohri, Pereira, and Riley 2002). What difﬁculties arise are due to using ﬁnite state techniques with grammars which are no"
J10-3008,N03-1017,0,0.0139717,"Missing"
J10-3008,N04-1022,1,0.793008,"plexity of movement and translation. If gains in using Hiero are small, however, the computational and modeling complexity involved are difﬁcult to justify. Such concerns would vanish if there were reliable methods to match Hiero complexity for speciﬁc translation problems. Loosely put, it would be a good thing if the complexity of a system was somehow proportional to the improvement in translation quality the system delivers. Another notable current trend in SMT is system combination. Minimum Bayes Risk decoding is widely used to rescore and improve hypotheses produced by individual systems (Kumar and Byrne 2004; Tromble et al. 2008; de Gispert et al. 2009), and more aggressive system combination techniques which synthesize entirely new hypotheses from those of contributing systems can give even greater translation improvements (Rosti et al. 2007; Sim et al. 2007). It is now commonplace to note that even the best available individual SMT system can be signiﬁcantly improved upon by such techniques. This puts a burden on the underlying SMT systems which is somewhat unusual in NLP. The requirement is not merely to produce a single hypothesis that is as good as possible. Ideally, the SMT systems should g"
J10-3008,J97-2003,0,0.184051,"paths may arise through the concatenation and union of sublattices with different spans. At the upper-most cell, the lattice L(S, 1, J) contains pointers to lower-level lattices. A single FST replace operation (Allauzen et al. 2007) recursively substitutes all pointers by their lower-level lattices until no pointers are left, thus producing the complete target word lattice for the whole source sentence. The use of the lattice pointer arc was inspired by the “lazy evaluation” techniques developed by Mohri, Pereira, and Riley (2000), closely related to Recursive Transition Networks (Woods 1970; Mohri 1997). Its implementation uses the infrastructure provided by the OpenFST libraries for delayed composition. 2.2.2 Top-level Pruning and Search Pruning. The ﬁnal translation lattice L(S, 1, J) can be quite large after the pointer arcs are expanded. We therefore apply a word-based 512 de Gispert et al. Hierarchical Translation with WFSTs and Shallow-n Grammars Figure 6 Transducers for ﬁltering up to one (left) or two (right) consecutive deletions. language model via WFST composition (Allauzen et al. 2007) and perform likelihoodbased pruning based on the combined translation and language model scores"
J10-3008,P03-1021,0,0.021257,"of the parallel text (for each language pair) and a 965 million word subset of monolingual data from the English Gigaword Third Edition (LDC2007T07). These are the language models used if pruning is needed during search. The main language model is a zero-cutoff stupid-backoff (Brants et al. 2007) 5-gram language model, estimated using 6.6B words of English text from the English Gigaword corpus. These language models are converted to WFSTs as needed (Allauzen, Mohri, and Roark 2003); failure transitions are used for correct application of back-off weights. In tuning the systems, standard MERT (Och 2003) iterative parameter estimation under IBM BLEU is performed on the development sets. 4.2 Contrast between HiFST and Cube Pruning We contrast two hierarchical phrase-based decoders. The ﬁrst decoder, HCP, is a k-best decoder using cube pruning following the description by Chiang (2007); in our implementation, these k-best lists contain only unique hypotheses (Iglesias et al. 2009a), which are obtained by extracting the 10,000 best candidates from each cell (including the language model cost), using a priority queue to explore the cross-product of the k-best lists from the cells pointed by nonte"
J10-3008,P02-1038,0,0.0627717,"nal Linguistics Volume 36, Number 3 Figure 9 One arc from a rule acceptor that assigns a vector of K feature weights to each rule (top) and the result of composition with the transducer of Figure 7 (after weight-pushing) (bottom). The components of the ﬁnal K-dimensional weight vector agree with the feature weights of the rule sequence, for example ck = c2,k + c1,k + c3,k + c4,k for k = 1 . . . K. 2.3.1 Extracting Feature Values from Alignments. As described by Chiang (2007), scores are associated with hierarchical translation rules through a factoring into features within a log-linear model (Och and Ney 2002). We assume that we have a collection of K features  and that the cost cr of each rule Rr is cr = Kk=1 λk cr,k , where cr,k is the value of the kth feature value for the rth rule and λk is the weight assigned to the  kth feature for all rules. r1 rN rn For a parse which . . . R , its cost N can therefore n=1 c K makes N user ,kof the rules R  N rn ,k n is the contribution by the kth be written as k=1 λk n=1 c . The quantity n=1 c feature to the overall translation score for that parse. These are the quantities which need to be extracted from alignment lattices for use in procedures such a"
J10-3008,2001.mtsummit-papers.68,0,0.0152801,"level 0, but only if one of the source terminals is tagged as a verb. Shallow-2, K1 = 2, K2 = 3, vo : two levels of reordering with monotonic production of up to three target language phrases of level 1, but only if one of the source terminals is tagged as a verb. 4. Translation Experiments In this section we report on hierarchical phrase-based translation experiments with WFSTs. We focus mainly on the NIST Arabic-to-English and Chinese-to-English translation tasks; some results for other language pairs are summarized in Section 4.6. Translation performance is evaluated using the BLEU score (Papineni et al. 2001) as implemented for the NIST 2009 evaluation.1 The experiments are organized as follows: - Lattice-based and cube pruning hierarchical decoding (Section 4.2) - Grammar conﬁgurations and search parameters and their effect on translation performance and processing time (Section 4.3) - Marginalization over translation derivations (Section 4.4) - Combining translation lattices obtained from alternative morphological decompositions of the input (Section 4.5) 4.1 Experimental Framework For Arabic-to-English translation we use all allowed parallel corpora in the NIST MT08 (and MT09) Arabic Constraine"
J10-3008,N07-1029,0,0.00946164,"or speciﬁc translation problems. Loosely put, it would be a good thing if the complexity of a system was somehow proportional to the improvement in translation quality the system delivers. Another notable current trend in SMT is system combination. Minimum Bayes Risk decoding is widely used to rescore and improve hypotheses produced by individual systems (Kumar and Byrne 2004; Tromble et al. 2008; de Gispert et al. 2009), and more aggressive system combination techniques which synthesize entirely new hypotheses from those of contributing systems can give even greater translation improvements (Rosti et al. 2007; Sim et al. 2007). It is now commonplace to note that even the best available individual SMT system can be signiﬁcantly improved upon by such techniques. This puts a burden on the underlying SMT systems which is somewhat unusual in NLP. The requirement is not merely to produce a single hypothesis that is as good as possible. Ideally, the SMT systems should generate large collections of candidate hypotheses that are simultaneously diverse and of good quality. Relative to these concerns, previously published descriptions of Hiero have noted certain limitations. Spurious ambiguity (Chiang 2005)"
J10-3008,P09-1037,0,0.0215378,"nd give the same translation. This can result in n-best lists with very few different translations which is problematic for the minimum-error-rate training algorithm ... This is due in part to the cube pruning procedure (Chiang 2007), which enumerates all distinct hypotheses to a ﬁxed depth by means of k-best hypothesis lists. If enumeration was not necessary, or if the lists could be arbitrarily deep, there might still be many duplicate derivations, but at least the hypothesis space would not be impoverished. Spurious ambiguity is also related to overgeneration (Varile and Lau 1988; Wu 1997; Setiawan et al. 2009). For our purposes we say that overgeneration occurs when different derivations based on the same set of rules give rise to different translations. An example is given in Figure 1. This process is not necessarily a bad thing in that it allows new translations to be synthesized from rules extracted from training data; a strong target language model, such as a high order n-gram, is typically relied upon to discard unsuitable hypotheses. Overgeneration does complicate translation, however, in that many hypotheses are introduced only to be subsequently discarded. The situation is further complicat"
J10-3008,D08-1065,0,0.72227,"d translation. If gains in using Hiero are small, however, the computational and modeling complexity involved are difﬁcult to justify. Such concerns would vanish if there were reliable methods to match Hiero complexity for speciﬁc translation problems. Loosely put, it would be a good thing if the complexity of a system was somehow proportional to the improvement in translation quality the system delivers. Another notable current trend in SMT is system combination. Minimum Bayes Risk decoding is widely used to rescore and improve hypotheses produced by individual systems (Kumar and Byrne 2004; Tromble et al. 2008; de Gispert et al. 2009), and more aggressive system combination techniques which synthesize entirely new hypotheses from those of contributing systems can give even greater translation improvements (Rosti et al. 2007; Sim et al. 2007). It is now commonplace to note that even the best available individual SMT system can be signiﬁcantly improved upon by such techniques. This puts a burden on the underlying SMT systems which is somewhat unusual in NLP. The requirement is not merely to produce a single hypothesis that is as good as possible. Ideally, the SMT systems should generate large collect"
J10-3008,A88-1022,0,0.217291,"e same model feature vectors and give the same translation. This can result in n-best lists with very few different translations which is problematic for the minimum-error-rate training algorithm ... This is due in part to the cube pruning procedure (Chiang 2007), which enumerates all distinct hypotheses to a ﬁxed depth by means of k-best hypothesis lists. If enumeration was not necessary, or if the lists could be arbitrarily deep, there might still be many duplicate derivations, but at least the hypothesis space would not be impoverished. Spurious ambiguity is also related to overgeneration (Varile and Lau 1988; Wu 1997; Setiawan et al. 2009). For our purposes we say that overgeneration occurs when different derivations based on the same set of rules give rise to different translations. An example is given in Figure 1. This process is not necessarily a bad thing in that it allows new translations to be synthesized from rules extracted from training data; a strong target language model, such as a high order n-gram, is typically relied upon to discard unsuitable hypotheses. Overgeneration does complicate translation, however, in that many hypotheses are introduced only to be subsequently discarded. Th"
J10-3008,J97-3002,0,0.0553706,"vectors and give the same translation. This can result in n-best lists with very few different translations which is problematic for the minimum-error-rate training algorithm ... This is due in part to the cube pruning procedure (Chiang 2007), which enumerates all distinct hypotheses to a ﬁxed depth by means of k-best hypothesis lists. If enumeration was not necessary, or if the lists could be arbitrarily deep, there might still be many duplicate derivations, but at least the hypothesis space would not be impoverished. Spurious ambiguity is also related to overgeneration (Varile and Lau 1988; Wu 1997; Setiawan et al. 2009). For our purposes we say that overgeneration occurs when different derivations based on the same set of rules give rise to different translations. An example is given in Figure 1. This process is not necessarily a bad thing in that it allows new translations to be synthesized from rules extracted from training data; a strong target language model, such as a high order n-gram, is typically relied upon to discard unsuitable hypotheses. Overgeneration does complicate translation, however, in that many hypotheses are introduced only to be subsequently discarded. The situati"
J10-3008,E09-1044,1,\N,Missing
J10-3008,P02-1040,0,\N,Missing
J14-3008,P10-2006,1,0.883775,"Missing"
J14-3008,D07-1090,0,0.0385823,"m, bigram, or unigram model. For both the Katz and the Kneser-Ney 4-gram language models: at θ = 7.5E − 05 the number of 4-grams in the LM is effectively reduced to zero; at θ = 7.5E − 4 the number of 3-grams is effectively 0; and at θ = 7.5E − 3, only unigrams remain. Development set perplexities increase as entropy pruning becomes more aggressive, with the Katz smoothed model performing better under pruning (Chelba et al. 2010; Roark, Allauzen, and Riley 2013). We will also use a larger language model, denoted M2 , obtained by interpolating M1 with a zero-cutoff stupid-backoff 5-gram model (Brants et al. 2007) estimated over 6.6B words of English newswire text; M2 is estimated as needed for the n-grams required for the test sets. 8 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl. 708 Allauzen et al. Pushdown Automata in Statistical Machine Translation Table 3 Success in finding the 1-best translation under G with various M1θ under a memory size limit of 10GB as measured over tune-nw (1,755 sentences). We note which operations in translation exceeded the memory limit: either Expansion and Intersection for HiFST, or Intersection and Shortest Path operation for HiPDT. Decoding with G + M1θ u"
J14-3008,D11-1003,0,0.0199507,"via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been 717 Computational Linguistics Volume 40, Number 3 studied in phrase-based translation by Zens and Ney (2008). Relaxation techniques have also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrasebased SMT (Chang and Collins 2011), and in tree-to-string translation under trigram language models (Rush and Collins 2011); this prior work involved much smaller grammars and languages models than have been considered here. Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions. We developed similar procedures for our HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010) via a different route, after noting that with t"
J14-3008,J07-2003,0,0.451126,"k, NY 10011. E-mail: {allauzen,riley}@google.com. ∗∗ University of Cambridge, Department of Engineering. CB2 1PZ Cambridge, U.K. and SDL Research, Cambridge U.K. E-mail: {wjb31,ad465,gi212}@eng.cam.ac.uk. Submission received: 6 August 2012; revised version received: 20 February 2013; accepted for publication: 2 December 2013. doi:10.1162/COLI a 00197 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 3 1. Introduction Synchronous context-free grammars (SCFGs) are now widely used in statistical machine translation, with Hiero as the preeminent example (Chiang 2007). Given an SCFG and an n-gram language model, the challenge is to decode with them, that is, to apply them to source text to generate a target translation. Decoding is complex in practice, but it can be described simply and exactly in terms of the formal languages and relations involved. We will use this description to introduce and analyze pushdown automata (PDAs) for machine translation. This formal description will allow close comparison of PDAs to existing decoders which are based on other forms of automata. Decoding can be described in terms of the following steps: 1. Translation: T = Π2"
J14-3008,J10-3008,1,0.943912,"Missing"
J14-3008,N10-1033,0,0.175195,"utomata that accepts ‘t1 t2 t3 t6 t7 ’, Step 2 will yield all derivations that yield this string 691 Computational Linguistics 0 1 X 2 t2 t1 Volume 40, Number 3 4 t4 t3 t6 X 3 6 t7 7 t2 0 5 t3 1 S 2 X (a) Optimized RTN t1 0 1 t2 2 t2 t3 4 6 t4 t3 8 t6 t2 3 t3 5 t7 9 7 (b) Optimized FSA 2 t2 0 t1 1 t3 [ 4 ) ( 8 t2 9 t3 10 t4 t6 ] 6 t7 7 5 3 (c) Optimized PDA Figure 3 Optimized representations of the regular language of possible translation candidates. as a translation of the source string. This is the approach taken in Iglesias et al. (2009a) and de Gispert et al. (2010) for the RTN/FSA and in Dyer (2010b) for hypergraphs. In Section 4 we analyze how PDAs can be used for alignment. 1.2 Goals We summarize here the aims of this article. We will show how PDAs can be used as compact representations of the space T of candidate translations generated by a hierarchical phrase-based SCFG when applied to an input sentence s and intersected with a language model M. We have described the architecture of HiPDT, a hierarchical phrase-based decoder based on PDAs, and have identified the general-purpose algorithms needed [ 2 t2 0 t1 1 ( [ 3 t3 ] 5 X→hs1 , t3 t4 i X→hs3 , t5 t6 i S→hX1 s2 X2 , t1 X1 X2 i S→h"
J14-3008,N04-1035,0,0.065967,"ially larger than the RTN/PDT optimized as described. Although our interest is primarily in Hiero-style translation grammars, which have rank 2 and a relatively small number of nonterminals, this complexity analysis can be extended to other grammars. For SCFGs of arbitrary rank lN , translation complexity in time for hypergraphs becomes O(|G||s|lN +1 |M|lN +1 ); with FSAs the time complexity bel +1 comes O(e|G||s |N |M |); and with PDAs the time complexity becomes O(|G||s|lN+1 |M|3 ). For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA representations may offer computational advantages in the worst case relative to hypergraph representations, although this must be balanced against other available strategies such as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and Langmead 2010). Of course, practical translation systems introduce various pruning procedures to achieve much better decoding efficiency than the worst cases given here. We will next describe the translation grammar and language model for our experiments, which will be used throughout the remainder of this art"
J14-3008,D10-1063,0,0.0163841,"complexity in time for hypergraphs becomes O(|G||s|lN +1 |M|lN +1 ); with FSAs the time complexity bel +1 comes O(e|G||s |N |M |); and with PDAs the time complexity becomes O(|G||s|lN+1 |M|3 ). For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA representations may offer computational advantages in the worst case relative to hypergraph representations, although this must be balanced against other available strategies such as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and Langmead 2010). Of course, practical translation systems introduce various pruning procedures to achieve much better decoding efficiency than the worst cases given here. We will next describe the translation grammar and language model for our experiments, which will be used throughout the remainder of this article (except when stated otherwise). In the following sections we assess the complexity discussion with a contrast between HiFST (FSA representation) and HiPDT (PDA representation) under large grammars. 4.1 Translation Grammars and Language Models Translation grammars are extracted from a subset of the"
J14-3008,C08-5001,0,0.014298,"lations for use in comparison. Using a relatively simple phrase-based translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been 717 Computational Linguistics Volume 40, Number 3 studied in phrase-based translation by Zens and Ney (2008). Relaxation techniques have also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrasebased SMT (Chang and Collins 2011), and in tree-to-string translation under trigram language models (Rush and Collins 2011); this prior work involved much smaller grammars and languages models than have been considered here. Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions. W"
J14-3008,P07-1019,0,0.0286452,"h a compromise between decoding speed and final performance of our HiPDT system. For instance, with θ = 7.5 × 10−7 and β = 12, for which we decode at a rate of 3 words/sec as seen in Figure 16, we are losing only 0.5 BLEU after LMBR compared to θ = 7.5 × 10−7 and β = 15. 6. Related Work There is extensive prior work on computational efficiency and algorithmic complexity in hierarchical phrase-based translation. The challenge is to find algorithms that can be made to work with large translation grammars and large language models. Following the original algorithms and analysis of Chiang (2007), Huang and Chiang (2007) developed the cube-growing algorithm, and more recently Huang and Mi (2010) developed an incremental decoding approach that exploits the left-to-right nature of n-gram language models. Search errors in hierarchical translation, and in translation more generally, have not been as extensively studied; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison. Using a relatively simple phrase-based translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that"
J14-3008,D10-1027,0,0.0185477,"For instance, with θ = 7.5 × 10−7 and β = 12, for which we decode at a rate of 3 words/sec as seen in Figure 16, we are losing only 0.5 BLEU after LMBR compared to θ = 7.5 × 10−7 and β = 15. 6. Related Work There is extensive prior work on computational efficiency and algorithmic complexity in hierarchical phrase-based translation. The challenge is to find algorithms that can be made to work with large translation grammars and large language models. Following the original algorithms and analysis of Chiang (2007), Huang and Chiang (2007) developed the cube-growing algorithm, and more recently Huang and Mi (2010) developed an incremental decoding approach that exploits the left-to-right nature of n-gram language models. Search errors in hierarchical translation, and in translation more generally, have not been as extensively studied; this is undoubtedly due to the difficulties inherent in finding exact translations for use in comparison. Using a relatively simple phrase-based translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an"
J14-3008,W05-1507,0,0.0768257,"Missing"
J14-3008,N09-1049,1,0.939348,"Missing"
J14-3008,E09-1044,1,0.880685,"Missing"
J14-3008,D10-1125,0,0.0345495,"ias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been 717 Computational Linguistics Volume 40, Number 3 studied in phrase-based translation by Zens and Ney (2008). Relaxation techniques have also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrasebased SMT (Chang and Collins 2011), and in tree-to-string translation under trigram language models (Rush and Collins 2011); this prior work involved much smaller grammars and languages models than have been considered here. Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions. We developed similar procedures for our HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010) via a"
J14-3008,N04-1022,0,0.0588184,"Does the HiPDT two-pass decoding generate lattices that can be useful in rescoring? We now report on rescoring experiments using WFSAs produced by the two-pass HiPDT translation system under the large translation grammar G. We demonstrate that HiPDT can be used to generate large, compact representations of the translation space that are suitable for rescoring with large language models or by alternative decoding procedures. We investigate translation performance by applying versions of the language model M2 estimated with stupid backoff. We also investigate minimum Bayes risk (MBR) decoding (Kumar and Byrne 2004) as an alternative search strategy. We are particularly interested in lattice MBR (LMBR) (Tromble et al. 2008), which is well suited for the large WFSAs that the system can generate; we use the implementation described by Blackwood, de Gispert, & Byrne (2010). There are two parameters to be tuned: a scaling parameter to normalize the evidence scores and a word penalty applied to the hypotheses space; these are tuned jointly on the tune-nw set. Results are reported in Figure 18. We note first that rescoring with the large language model M2 , which is effectively interpolated with M1 , gives con"
J14-3008,H05-1021,0,0.0389465,"ork involved much smaller grammars and languages models than have been considered here. Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions. We developed similar procedures for our HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010) via a different route, after noting that with the space of translations represented as WFSAs, alignment can be performed using operations over WFSTs (Kumar and Byrne 2005). Although entropy-pruned language models have been used to produce real-time translation systems (Prasad et al. 2007), we believe our use of entropy-pruned language models in two-pass translation to be novel. This is an approach that is widely used in automatic speech recognition (Ljolje, Pereira, and Riley 1999) and we note that it relies on efficient representation of very large search spaces T for subsequent rescoring, as is possible with FSAs and PDAs. 7. Conclusion In this article, we have described a novel approach to hierarchical machine translation using pushdown automata. We have pre"
J14-3008,W03-3016,0,0.0431321,"whose size is always linear in the size of R. In this article, we assume this optimization is always performed. We note here that RTNs can be defined and the replacement operation can be applied in any semiring. 3.3 Composition Once we have created the PDA with translation scores, Step 2 in Section 1.1 applies the language model scores to the translation space. This is done by composition with an FSA containing the relevant language model weights. The class of weighted pushdown transducers is closed under composition with weighted finite-state transducers (Bar-Hillel, Perles, and Shamir 1964; Nederhof and Satta 2003). OpenFST supports composition between automata T1 and T2 , where T1 is a weighted pushdown transducer and T2 is a weighted finite-state transducer. If both T1 and T2 are acceptors, rather than transducers, the composition of a PDA and an FSA produces a PDA containing their intersection, and so no separate intersection algorithm is required for these automata. Given this, we describe only the simpler, special case of intersection between a PDA and an FSA, as this is sufficient for most of the translation applications described in this article. The alignment experiments of RTN R PDT T X1 a 2 1"
J14-3008,P03-1021,0,0.00584984,"une-nw reference translations are also reported. The Kneser-Ney and Katz 4-gram LM have 416,190 unigrams, which are not removed by pruning. θ 0 7.5 × 10−9 7.5 × 10−8 7.5 × 10−7 7.5 × 10−6 7.5 × 10−5 7.5 × 10−4 7.5 × 10−3 KN 2-grams 3-grams 4-grams perplexity 28M 61M 117M 98.1 10M 6M 3M 122.2 2.5M 969K 219K 171.5 442K 74K 5K 290.4 37K 2.7K 44 605.1 1.3K 38 0 1270.2 21 0 0 1883.6 0 0 0 2200.0 KATZ 2-grams 3-grams 4-grams perplexity 28M 64M 117M 106.7 7M 10M 4.6M 120.4 2M 1.5M 398K 146.9 391K 148K 19K 210.5 52K 8.4K 510 336.6 4K 197 1 596.5 117 1 0 905.0 1 0 0 1046.1 In tuning the systems, MERT (Och 2003) iterative parameter estimation under IBM BLEU8 is performed on the development set. The parallel corpus is aligned using MTTK (Deng and Byrne 2008) in both sourceto-target and target-to-source directions. We then follow published procedures (Chiang 2007; Iglesias et al. 2009b) to extract hierarchical phrases from the union of the directional word alignments. We call a translation grammar (G) the set of rules extracted from this process. For reference, the number of rules in G that can apply to the tune-nw is 1.1M, of which 593K are standard non-hierarchical phrases and 511K are strictly hiera"
J14-3008,P13-1005,1,0.88019,"Missing"
J14-3008,P11-2001,0,0.112438,"Missing"
J14-3008,P11-1008,0,0.015885,"at cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been 717 Computational Linguistics Volume 40, Number 3 studied in phrase-based translation by Zens and Ney (2008). Relaxation techniques have also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrasebased SMT (Chang and Collins 2011), and in tree-to-string translation under trigram language models (Rush and Collins 2011); this prior work involved much smaller grammars and languages models than have been considered here. Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions. We developed similar procedures for our HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010) via a different route, after noting that with the space of translations represented as WFSAs, alignment can be performed using operation"
J14-3008,H05-1101,0,0.0267061,"rable to ITG alignment (Wu 1997) and the intersection algorithm of Dyer (2010b). Our experimental results support the complexity analysis summarized in Table 1. HiPDT is more efficient in ITG alignment and this is consistent with its linear dependence on the grammar size, whereas HiFST suffers from its exponential dependence. This use of PDAs in alignment does not rely on properties specific either to Hiero or to ITGs. We expect that the approach should be applicable with other types of SCFGs, although we note that alignment under SCFGs with an arbitrary number of nonterminals can be NP-hard (Satta and Peserico 2005). 5. HiPDT Two-Pass Translation Architecture and Experiments The previous complexity analysis suggests that PDAs should excel when used with large translation grammars and relatively small n-gram language models. In hierarchical phrase-based translation, this is a somewhat unusual scenario: It is far more typical that translation tasks requiring a large translation grammar also require large language models. To accommodate these requirements we have developed a twopass decoding strategy in which a weak version of a large language model is applied prior to the expansion of the PDA, after which"
J14-3008,J95-2002,0,0.458154,"Missing"
J14-3008,D08-1065,0,0.0141347,"experiments using WFSAs produced by the two-pass HiPDT translation system under the large translation grammar G. We demonstrate that HiPDT can be used to generate large, compact representations of the translation space that are suitable for rescoring with large language models or by alternative decoding procedures. We investigate translation performance by applying versions of the language model M2 estimated with stupid backoff. We also investigate minimum Bayes risk (MBR) decoding (Kumar and Byrne 2004) as an alternative search strategy. We are particularly interested in lattice MBR (LMBR) (Tromble et al. 2008), which is well suited for the large WFSAs that the system can generate; we use the implementation described by Blackwood, de Gispert, & Byrne (2010). There are two parameters to be tuned: a scaling parameter to normalize the evidence scores and a word penalty applied to the hypotheses space; these are tuned jointly on the tune-nw set. Results are reported in Figure 18. We note first that rescoring with the large language model M2 , which is effectively interpolated with M1 , gives consistent gains over initial results obtained with M1 alone. After 5-gram rescoring there is already +0.5 BLEU i"
J14-3008,J97-3002,0,0.556067,"es successfully. The subsequent shortest path (or pruned expansion) operation is prone to failure, but the risk of this can be greatly reduced by using smaller language models. In the next section we contrast both HiPDT and HiFST for alignment. 4.3 Alignment with Inversion Transduction Grammars We continue to explore applications characterized by large translation grammars G and small language models M. As an extreme instance of a problem involving a large translation grammar and a simple target language model, we consider parallel text alignment under an Inversion Transduction Grammar (ITG) (Wu 1997). This task, or something like it, is often done in translation grammar induction. The process should yield the set of derivations, with scores, that generate the target sentence as a translation 9 We use the UNIX ulimit command. The experiment was carried out over machines with different configurations and loads, so these numbers should be considered as approximate values. 709 Computational Linguistics Volume 40, Number 3 of the source sentence. In alignment the target language model is extremely simple: It is simply an acceptor for the target language sentence so that |M |is linear in the le"
J14-3008,D09-1038,0,0.0192808,"of arbitrary rank lN , translation complexity in time for hypergraphs becomes O(|G||s|lN +1 |M|lN +1 ); with FSAs the time complexity bel +1 comes O(e|G||s |N |M |); and with PDAs the time complexity becomes O(|G||s|lN+1 |M|3 ). For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA representations may offer computational advantages in the worst case relative to hypergraph representations, although this must be balanced against other available strategies such as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and Langmead 2010). Of course, practical translation systems introduce various pruning procedures to achieve much better decoding efficiency than the worst cases given here. We will next describe the translation grammar and language model for our experiments, which will be used throughout the remainder of this article (except when stated otherwise). In the following sections we assess the complexity discussion with a contrast between HiFST (FSA representation) and HiPDT (PDA representation) under large grammars. 4.1 Translation Grammars and Language Models Translatio"
J14-3008,2008.iwslt-papers.8,0,0.027566,"g exact translations for use in comparison. Using a relatively simple phrase-based translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementation was presented by Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been 717 Computational Linguistics Volume 40, Number 3 studied in phrase-based translation by Zens and Ney (2008). Relaxation techniques have also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrasebased SMT (Chang and Collins 2011), and in tree-to-string translation under trigram language models (Rush and Collins 2011); this prior work involved much smaller grammars and languages models than have been considered here. Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been studied previously by Dyer (2010b), who showed that a single synchronous parsing algorithm (Wu 1997) can be significantly improved upon in practice through hypergraph compositions. W"
J14-3008,N06-1033,0,0.0299036,"grammars. For SCFGs of arbitrary rank lN , translation complexity in time for hypergraphs becomes O(|G||s|lN +1 |M|lN +1 ); with FSAs the time complexity bel +1 comes O(e|G||s |N |M |); and with PDAs the time complexity becomes O(|G||s|lN+1 |M|3 ). For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA representations may offer computational advantages in the worst case relative to hypergraph representations, although this must be balanced against other available strategies such as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and Langmead 2010). Of course, practical translation systems introduce various pruning procedures to achieve much better decoding efficiency than the worst cases given here. We will next describe the translation grammar and language model for our experiments, which will be used throughout the remainder of this article (except when stated otherwise). In the following sections we assess the complexity discussion with a contrast between HiFST (FSA representation) and HiPDT (PDA representation) under large grammars. 4.1 Translation Grammars and Languag"
J14-3008,W06-3119,0,0.0418105,"or CFG representation can be exponentially larger than the RTN/PDT optimized as described. Although our interest is primarily in Hiero-style translation grammars, which have rank 2 and a relatively small number of nonterminals, this complexity analysis can be extended to other grammars. For SCFGs of arbitrary rank lN , translation complexity in time for hypergraphs becomes O(|G||s|lN +1 |M|lN +1 ); with FSAs the time complexity bel +1 comes O(e|G||s |N |M |); and with PDAs the time complexity becomes O(|G||s|lN+1 |M|3 ). For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA representations may offer computational advantages in the worst case relative to hypergraph representations, although this must be balanced against other available strategies such as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and Langmead 2010). Of course, practical translation systems introduce various pruning procedures to achieve much better decoding efficiency than the worst cases given here. We will next describe the translation grammar and language model for our experiments, which will be used through"
J14-3008,2010.iwslt-keynotes.2,0,\N,Missing
N09-1013,J96-1002,0,0.0600688,"to-one alignments and can capture dependencies within target phrases. Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. This leads to gains in machine translation quality when systems are trained on parallel text containing the modified Arabic and processing of Arabic text is carried out prior to translation. Nießen and Ney (2001a) perform pre-processing of German and English text before translation; Nießen and Ney (2001b) use morphological information of the current word to estimate hierarchical translation probabilities. Berger et al. (1996) introduce maximum entropy models for machine translation, and use a window either side of the target word as context information. Varea et al. (2002) test for the presence of specific words within a window of the current source word to form features for use inside a maximum entropy model of alignment. Toutanova et al. (2002) use part-of-speech information in both the source and target languages to estimate alignment probabilities, but this information is not incorporated into translation probabilities. Popovi´c and Ney (2004) use the base form of a word and its part-ofspeech tag during the es"
N09-1013,W08-0316,1,0.887525,"Missing"
N09-1013,A00-1031,0,0.363316,"4.04e+07 CI Model 1 Threshold 10 Threshold 20 Threshold 60 -4.06e+07 Log probability of training data -4.08e+07 -4.1e+07 -4.12e+07 -4.14e+07 -4.16e+07 -4.18e+07 -4.2e+07 -4.22e+07 11 12 13 14 15 16 Iteration 17 18 19 20 -2.75e+06 -2.8e+06 Log probability of training data ‘D2’, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). The alignment models are trained on this processed data, and the prefixes and suffixes are treated as words in their own right; in particular their contexts are examined and clustered. The TnT tagger (Brants, 2000), used as distributed with its model trained on the Wall Street Journal portion of the Penn treebank, was used to obtain part-of-speech tags for the English side of the parallel text. Marcus et al. (1993) gives a complete list of part-of-speech tags produced. No morphological analysis is performed for English. Automatic word alignments were compared to a manually-aligned corpus made up of the IBM ArabicEnglish Word Alignment Corpus (Ittycheriah et al., 2006) and the word alignment corpora LDC2006E86 and LDC2006E93. This contains 28k parallel text sentences pairs: 724k words of Arabic and 847k"
N09-1013,J93-2003,0,0.148147,"l text. Section 4 uses alignments produced by 110 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 110–118, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics our improved alignment models to initialise a statistical machine translation system and evaluate the quality of translation on several data sets. We also apply part-ofspeech tagging and decision tree clustering of contexts to Chinese-English parallel text; translation results for these languages are presented in Section 4.2. 1.2 Previous and related work Brown et al. (1993) introduce IBM Models 1-5 for alignment modelling; Vogel et al. (1996) propose a Hidden Markov Model (HMM) model for word-to-word alignment, where the words of the source sentence are viewed as states of an HMM and emit target sentence words; Deng and Byrne (2005a) extend this to an HMM word-tophrase model which allows many-to-one alignments and can capture dependencies within target phrases. Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. This leads to gains in machine translation quality when systems are trained on para"
N09-1013,W05-0827,0,0.025874,"Missing"
N09-1013,H05-1022,1,0.944705,"nt models to initialise a statistical machine translation system and evaluate the quality of translation on several data sets. We also apply part-ofspeech tagging and decision tree clustering of contexts to Chinese-English parallel text; translation results for these languages are presented in Section 4.2. 1.2 Previous and related work Brown et al. (1993) introduce IBM Models 1-5 for alignment modelling; Vogel et al. (1996) propose a Hidden Markov Model (HMM) model for word-to-word alignment, where the words of the source sentence are viewed as states of an HMM and emit target sentence words; Deng and Byrne (2005a) extend this to an HMM word-tophrase model which allows many-to-one alignments and can capture dependencies within target phrases. Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. This leads to gains in machine translation quality when systems are trained on parallel text containing the modified Arabic and processing of Arabic text is carried out prior to translation. Nießen and Ney (2001a) perform pre-processing of German and English text before translation; Nießen and Ney (2001b) use morphological information of the cu"
N09-1013,N06-2013,0,0.394994,"h tagging and decision tree clustering of contexts to Chinese-English parallel text; translation results for these languages are presented in Section 4.2. 1.2 Previous and related work Brown et al. (1993) introduce IBM Models 1-5 for alignment modelling; Vogel et al. (1996) propose a Hidden Markov Model (HMM) model for word-to-word alignment, where the words of the source sentence are viewed as states of an HMM and emit target sentence words; Deng and Byrne (2005a) extend this to an HMM word-tophrase model which allows many-to-one alignments and can capture dependencies within target phrases. Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. This leads to gains in machine translation quality when systems are trained on parallel text containing the modified Arabic and processing of Arabic text is carried out prior to translation. Nießen and Ney (2001a) perform pre-processing of German and English text before translation; Nießen and Ney (2001b) use morphological information of the current word to estimate hierarchical translation probabilities. Berger et al. (1996) introduce maximum entropy models for machine translation, and use a wind"
N09-1013,N09-1049,1,0.89686,"Missing"
N09-1013,N03-1017,0,0.0428325,"these are the questions that individually give the greatest ability to discriminate between the different contexts of a word. The list shows the importance of the left and right contexts of the word in predicting its translation: of the most common 50 questions, 25 concern the previous word, 19 concern the next, and only 6 concern the partof-speech of the current word. For Arabic, of the most frequent 50 questions, 21 concern the previous word, 20 concern the next and 9 the current word. 3.2.2 Alignment Error Rate Since MT systems are usually built on the union of the two sets of alignments (Koehn et al., 2003), we consider the union of alignments in the two directions as well as those in each direction. Figure 3 shows the change in AER of the alignments in each direction, as well as the alignment formed by taking their union at corresponding thresholds and training iterations. 115 There is a trade-off between modelling the data accurately, which requires more clusters, and eliminating data sparsity problems, which requires each cluster to contain contexts that occur frequently enough in the training data to estimate the translation probabilities accurately. Use of a smaller threshold Timp leads to"
N09-1013,J93-2004,0,0.0349666,"Iteration 17 18 19 20 -2.75e+06 -2.8e+06 Log probability of training data ‘D2’, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). The alignment models are trained on this processed data, and the prefixes and suffixes are treated as words in their own right; in particular their contexts are examined and clustered. The TnT tagger (Brants, 2000), used as distributed with its model trained on the Wall Street Journal portion of the Penn treebank, was used to obtain part-of-speech tags for the English side of the parallel text. Marcus et al. (1993) gives a complete list of part-of-speech tags produced. No morphological analysis is performed for English. Automatic word alignments were compared to a manually-aligned corpus made up of the IBM ArabicEnglish Word Alignment Corpus (Ittycheriah et al., 2006) and the word alignment corpora LDC2006E86 and LDC2006E93. This contains 28k parallel text sentences pairs: 724k words of Arabic and 847k words of English. The alignment links were modified to reflect the MADA tokenisation; after modification, there are 946k word-toword alignment links. Alignment quality was evaluated by computing Alignment"
N09-1013,2001.mtsummit-papers.45,0,0.0314227,"odel for word-to-word alignment, where the words of the source sentence are viewed as states of an HMM and emit target sentence words; Deng and Byrne (2005a) extend this to an HMM word-tophrase model which allows many-to-one alignments and can capture dependencies within target phrases. Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. This leads to gains in machine translation quality when systems are trained on parallel text containing the modified Arabic and processing of Arabic text is carried out prior to translation. Nießen and Ney (2001a) perform pre-processing of German and English text before translation; Nießen and Ney (2001b) use morphological information of the current word to estimate hierarchical translation probabilities. Berger et al. (1996) introduce maximum entropy models for machine translation, and use a window either side of the target word as context information. Varea et al. (2002) test for the presence of specific words within a window of the current source word to form features for use inside a maximum entropy model of alignment. Toutanova et al. (2002) use part-of-speech information in both the source and"
N09-1013,W01-1407,0,0.0249894,"odel for word-to-word alignment, where the words of the source sentence are viewed as states of an HMM and emit target sentence words; Deng and Byrne (2005a) extend this to an HMM word-tophrase model which allows many-to-one alignments and can capture dependencies within target phrases. Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. This leads to gains in machine translation quality when systems are trained on parallel text containing the modified Arabic and processing of Arabic text is carried out prior to translation. Nießen and Ney (2001a) perform pre-processing of German and English text before translation; Nießen and Ney (2001b) use morphological information of the current word to estimate hierarchical translation probabilities. Berger et al. (1996) introduce maximum entropy models for machine translation, and use a window either side of the target word as context information. Varea et al. (2002) test for the presence of specific words within a window of the current source word to form features for use inside a maximum entropy model of alignment. Toutanova et al. (2002) use part-of-speech information in both the source and"
N09-1013,C00-2163,0,0.0202467,"ist of part-of-speech tags produced. No morphological analysis is performed for English. Automatic word alignments were compared to a manually-aligned corpus made up of the IBM ArabicEnglish Word Alignment Corpus (Ittycheriah et al., 2006) and the word alignment corpora LDC2006E86 and LDC2006E93. This contains 28k parallel text sentences pairs: 724k words of Arabic and 847k words of English. The alignment links were modified to reflect the MADA tokenisation; after modification, there are 946k word-toword alignment links. Alignment quality was evaluated by computing Alignment Error Rate (AER) (Och and Ney, 2000) relative to the manual alignments. Since the links supplied contain only ‘sure’ links and no ‘possible’ links, we use the following formula for computing AER given reference alignment links S and hypothesised alignment links A: 2|S∩A| AER = 1 − |S|+|A| . CI Model 1 Threshold 10 Threshold 20 Threshold 60 -2.85e+06 -2.9e+06 -2.95e+06 -3e+06 -3.05e+06 -3.1e+06 11 12 13 14 15 16 17 18 19 20 Iteration Figure 2: Increase in log probability of training data during training for varying Timp , with Model 1, for Arabic to English (top) and English to Arabic (bottom) criminating between the two cases, w"
N09-1013,N04-1021,0,0.0442188,"recognition. Kannan et al. (1994) introduce a binary treegrowing procedure for clustering Gaussian models for triphone contexts based on the value of a likelihood ratio. We adopt a similar approach to estimate contextdependent translation probabilities. We focus on alignment with IBM Model 1 and HMMs. HMMs are commonly used to generate alignments from which state of the art SMT systems are built. Model 1 is used as an intermediate step in the creation of more powerful alignment models, such as HMMs and further IBM models. In addition, it is used in SMT as a feature in Minimum Error Training (Och et al., 2004) and for rescoring lattices of translation hypotheses (Blackwood et al., 2008). It is also used for lexically-weighted phrase extraction (Costa-juss`a and Fonollosa, 2005) and sentence segmentation of parallel text (Deng et al., 2007) prior to machine translation. 1.1 Overview We first develop an extension to Model 1 that allows the use of arbitrary context information about a source word to estimate context-dependent word-to-word translation probabilities. Since there is insufficient training data to accurately estimate translation probabilities for less frequently occurring contexts, we deve"
N09-1013,P03-1021,0,0.0201074,"text-dependent models produce a decrease in AER measured on manually-aligned data; we wish to show this improved model performance leads to an increase in translation quality, measured by BLEU score (Papineni et al., 2001). In addition to the Arabic systems already evaluated by AER, we also report results for a Chinese-English translation system. Alignment models were evaluated by aligning the training data using the models in each translation direction. HiFST, a WFST-based hierarchical translation system described in (Iglesias et al., 2009), was trained on the union of these alignments. MET (Och, 2003) was carried out using a development set, and the BLEU score evaluated on two test sets. Decoding used a 4-gram language model estimated from the English side of the entire MT08 parallel text, and a 965M word subset of monolingual data from the English Gigaword Third Edition. For both Arabic and English, the CD HMM models were evaluated as follows. Iteration 5 of the CI HMM was used to produce alignments for the parallel text training data: these were used to train the baseline system. The same data is aligned using CD HMMs after two further iterations of training and a second WFST-based trans"
N09-1013,2001.mtsummit-papers.68,0,0.0128761,"d the baseline context-independent HMM, for Arabic to English and English to Arabic. p0 varies from 0.00 to 0.95 in steps of 0.05. CI Model 1 Threshold 10 Threshold 20 Threshold 60 Threshold 100 50.8 p0=0.00 English-Arabic CI HMM Arabic-English CD HMM Arabic-English CI HMM 5 6 7 8 9 10 Iteration Figure 4: AER of the union alignment for varying Timp with the HMM model 116 We have shown that the context-dependent models produce a decrease in AER measured on manually-aligned data; we wish to show this improved model performance leads to an increase in translation quality, measured by BLEU score (Papineni et al., 2001). In addition to the Arabic systems already evaluated by AER, we also report results for a Chinese-English translation system. Alignment models were evaluated by aligning the training data using the models in each translation direction. HiFST, a WFST-based hierarchical translation system described in (Iglesias et al., 2009), was trained on the union of these alignments. MET (Och, 2003) was carried out using a development set, and the BLEU score evaluated on two test sets. Decoding used a 4-gram language model estimated from the English side of the entire MT08 parallel text, and a 965M word sub"
N09-1013,C04-1045,0,0.0433354,"Missing"
N09-1013,W96-0213,0,0.151951,"newswire portion of the MT08 set, MT08-nw. Table 3 shows a comparison of the system trained using CD HMMs with the baseline system, which was trained using CI HMM models on untagged data. The context-dependent models result in a gain in BLEU score of 0.3 for mt02 05 test and 0.6 for MT08-nw. 4.2 Chinese to English translation The Chinese training set was 600k random parallel text sentences of the newswire LDC collection allowed for NIST MT08, a total of 15.2M words of Chinese and 16.6M words of English. The Chinese text was tagged using the MXPOST maximum-entropy part of speech tagging tool (Ratnaparkhi, 1996) trained on the Penn Chinese Treebank 5.1; the English text was tagged using the TnT part of speech tagger (Brants, 2000) trained on the Wall Street Journal portion of the English Penn treebank. The development set tune-nw and validation set test-nw contain a mix of the newswire portions of MT02 through MT05 and additional developments sets created by translation within the GALE program. We also report results on the newswire portion of the MT08 set. Again we see an increase in BLEU score for both test sets: 0.5 for test117 Alignments CI HMM CD HMM Alignments CI HMM CD HMM Arabic-English tune"
N09-1013,2007.tmi-papers.28,0,0.0821367,"ence of specific words within a window of the current source word to form features for use inside a maximum entropy model of alignment. Toutanova et al. (2002) use part-of-speech information in both the source and target languages to estimate alignment probabilities, but this information is not incorporated into translation probabilities. Popovi´c and Ney (2004) use the base form of a word and its part-ofspeech tag during the estimation of word-to-word translation probabilities for IBM models and HMMs, but do not defined context-dependent estimates of translation probabilities. Stroppa et al. (2007) consider context-informed features of phrases as components of the log-linear model during phrase-based translation, but do not address alignment. 2 Use of source language context in alignment modelling Consider the alignment of the target sentence e = eI1 with the source sentence f = f1J . Let a = aI1 be the alignments of the target words to the source words. Let cj be the context information of fj for j = 1, . . . , J. This context information can be any information about the word, 111 e.g. part-of-speech, previous and next words, part-ofspeech of previous and next words, or longer range co"
N09-1013,W02-1012,0,0.0433965,"ssing of Arabic text is carried out prior to translation. Nießen and Ney (2001a) perform pre-processing of German and English text before translation; Nießen and Ney (2001b) use morphological information of the current word to estimate hierarchical translation probabilities. Berger et al. (1996) introduce maximum entropy models for machine translation, and use a window either side of the target word as context information. Varea et al. (2002) test for the presence of specific words within a window of the current source word to form features for use inside a maximum entropy model of alignment. Toutanova et al. (2002) use part-of-speech information in both the source and target languages to estimate alignment probabilities, but this information is not incorporated into translation probabilities. Popovi´c and Ney (2004) use the base form of a word and its part-ofspeech tag during the estimation of word-to-word translation probabilities for IBM models and HMMs, but do not defined context-dependent estimates of translation probabilities. Stroppa et al. (2007) consider context-informed features of phrases as components of the log-linear model during phrase-based translation, but do not address alignment. 2 Use"
N09-1013,C02-1032,0,0.0151684,"as splitting of prefixes and suffixes. This leads to gains in machine translation quality when systems are trained on parallel text containing the modified Arabic and processing of Arabic text is carried out prior to translation. Nießen and Ney (2001a) perform pre-processing of German and English text before translation; Nießen and Ney (2001b) use morphological information of the current word to estimate hierarchical translation probabilities. Berger et al. (1996) introduce maximum entropy models for machine translation, and use a window either side of the target word as context information. Varea et al. (2002) test for the presence of specific words within a window of the current source word to form features for use inside a maximum entropy model of alignment. Toutanova et al. (2002) use part-of-speech information in both the source and target languages to estimate alignment probabilities, but this information is not incorporated into translation probabilities. Popovi´c and Ney (2004) use the base form of a word and its part-ofspeech tag during the estimation of word-to-word translation probabilities for IBM models and HMMs, but do not defined context-dependent estimates of translation probabilitie"
N09-1013,C96-2141,0,0.472337,"ologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 110–118, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics our improved alignment models to initialise a statistical machine translation system and evaluate the quality of translation on several data sets. We also apply part-ofspeech tagging and decision tree clustering of contexts to Chinese-English parallel text; translation results for these languages are presented in Section 4.2. 1.2 Previous and related work Brown et al. (1993) introduce IBM Models 1-5 for alignment modelling; Vogel et al. (1996) propose a Hidden Markov Model (HMM) model for word-to-word alignment, where the words of the source sentence are viewed as states of an HMM and emit target sentence words; Deng and Byrne (2005a) extend this to an HMM word-tophrase model which allows many-to-one alignments and can capture dependencies within target phrases. Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. This leads to gains in machine translation quality when systems are trained on parallel text containing the modified Arabic and processing of Arabic text"
N09-1013,H94-1062,0,0.788283,"that word. It is intuitive that a word in one context, with a particular part-of-speech and particular words surrounding it, may translate differently when in a different context. We aim to take advantage of this information to provide a better estimate of the word’s translation. The challenge of incorporating context information is maintaining computational tractability of estimation and alignment, and we develop algorithms to overcome this. The development of efficient estimation procedures for context-dependent acoustic models revolutionised the field of Automatic Speech Recognition (ASR) (Young et al., 1994). Clustering is used extensively for improving parameter estimation of triphone (and higher order) acoustic models, enabling robust estimation of parameters and reducing the computation required for recognition. Kannan et al. (1994) introduce a binary treegrowing procedure for clustering Gaussian models for triphone contexts based on the value of a likelihood ratio. We adopt a similar approach to estimate contextdependent translation probabilities. We focus on alignment with IBM Model 1 and HMMs. HMMs are commonly used to generate alignments from which state of the art SMT systems are built. M"
N09-1013,J07-3002,0,\N,Missing
N09-1013,P02-1040,0,\N,Missing
N09-1013,N06-4004,1,\N,Missing
N09-1049,P03-1006,0,0.155674,"ruction proceeds by traversing the CYK grid along the backpointers established in parsing. In each cell (N, x, y) in the CYK grid, we build a target language word lattice L(N, x, y). This lattice contains every translation of sxx+y−1 from every derivation headed by N . These lattices also contain the translation scores on their arc weights. The ultimate objective is the word lattice L(S, 1, J) which corresponds to all the analyses that cover the source sentence sJ1 . Once this is built, we can apply a target language model to L(S, 1, J) to obtain the final target language translation lattice (Allauzen et al., 2003). We use the approach of Mohri (2002) in applying WFSTs to statistical NLP. This fits well with the use of the OpenFST toolkit (Allauzen et al., 2007) to implement our decoder. 2.1 Lattice Construction Over the CYK Grid In each cell (N, x, y), the set of rule indices used by the parser is denoted R(N, x, y), i.e. for r ∈ R(N, x, y), N → hγ r ,αr i was used in at least one derivation involving that cell. For each rule Rr , r ∈ R(N, x, y), we build a lattice L(N, x, y, r). This lattice is derived from the target side of the rule αr by concatenating lattices R1 : R2 : R3 : R4 : R5 : X → hs1 s2 s3"
N09-1049,N01-1018,0,0.0271024,"d Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-toEnglish and Chinese-to-English, and Section 4 concludes. 2 Hierarchical Translation with WFSTs The translation system is based on a variant of the CYK a"
N09-1049,P08-1024,0,0.0360371,"as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine"
N09-1049,D07-1090,0,0.0828658,"ts used for each language pair are given in their respective section. Standard MET (Och, 2003) iterative parameter estimation under IBM BLEU (Papineni et al., 2001) is performed on the corresponding development set. For the HCP system, MET is done following Chiang (2007). For the HiFST system, we obtain a kbest list from the translation lattice and extract each feature score with the aligner variant of the k-best decoder. After translation with optimized feature weights, we carry out the two following rescoring steps. • Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using ∼4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST. Lattices provide a vast search space relative to k-best lists, with translation lattice sizes of 1081 hypotheses reported in the literature (Tromble et al., 2008). • Minimum Bayes Risk (MBR). We rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function (Kumar and Byrne, 2004). 3.1 Building the Rule Sets We extract hierarchical phrases from word alignments,"
N09-1049,P05-1033,0,0.39769,"translation search, resulting in fewer search errors, direct generation of translation lattices in the target language, better parameter optimization, and improved translation performance when rescoring with long-span language models and MBR decoding. We report translation experiments for the Arabic-to-English and Chinese-to-English NIST translation tasks and contrast the WFSTbased hierarchical decoder with hierarchical translation under cube pruning. 1 Introduction Hierarchical phrase-based translation generates translation hypotheses via the application of hierarchical rules in CYK parsing (Chiang, 2005). Cube pruning is used to apply language models at each cell of the CYK grid as part of the search for a k-best list of translation candidates (Chiang, 2005; Chiang, 2007). While this approach is very effective and has been shown to produce very good quality translation, the reliance on k-best lists is a limitation. We take an alternative approach and describe a lattice-based hierarchical decoder implemented with Weighted Finite State Transducers (WFSTs). In every CYK cell we build a single, minimal word lattice containing all possible translations of the source sentence span covered by that c"
N09-1049,J07-2003,0,0.886436,"tion performance when rescoring with long-span language models and MBR decoding. We report translation experiments for the Arabic-to-English and Chinese-to-English NIST translation tasks and contrast the WFSTbased hierarchical decoder with hierarchical translation under cube pruning. 1 Introduction Hierarchical phrase-based translation generates translation hypotheses via the application of hierarchical rules in CYK parsing (Chiang, 2005). Cube pruning is used to apply language models at each cell of the CYK grid as part of the search for a k-best list of translation candidates (Chiang, 2005; Chiang, 2007). While this approach is very effective and has been shown to produce very good quality translation, the reliance on k-best lists is a limitation. We take an alternative approach and describe a lattice-based hierarchical decoder implemented with Weighted Finite State Transducers (WFSTs). In every CYK cell we build a single, minimal word lattice containing all possible translations of the source sentence span covered by that cell. When derivations We describe how this decoder can be easily implemented with WFSTs. For this we employ the OpenFST libraries (Allauzen et al., 2007). Using standard F"
N09-1049,P08-1115,0,0.011952,"L, pages 433–441, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) ex"
N09-1049,J08-3004,0,0.00772752,"mented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-toEnglish and Chinese-to-English, and Section 4 concludes. 2 Hierarchical Translation with WFSTs The translation system is based on a variant of the CYK algorithm closely related to CYK+ (Chappe434 lier and Rajman, 1998). Parsing follows the d"
N09-1049,P07-1019,0,0.143237,"Missing"
N09-1049,E09-1044,1,0.642687,"Missing"
N09-1049,N04-1022,1,0.580412,"g steps. • Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using ∼4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST. Lattices provide a vast search space relative to k-best lists, with translation lattice sizes of 1081 hypotheses reported in the literature (Tromble et al., 2008). • Minimum Bayes Risk (MBR). We rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function (Kumar and Byrne, 2004). 3.1 Building the Rule Sets We extract hierarchical phrases from word alignments, applying the same restrictions as introduced by Chiang (2005). Additionally, following Iglesias et al. (2009) we carry out two rule filtering strategies: • we exclude rules with two non-terminals with the same order on the source and target side • we consider only the 20 most frequent translations for each rule For each development set, this produces approximately 4.3M rules in Arabic-to-English and 2.0M rules in Chinese-to-English. 3.2 Arabic-to-English Translation We translate Arabic-to-English with shallow hi"
N09-1049,H05-1021,1,0.380859,"ompare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-toEnglish and Chinese-to-English, and Section 4 concludes. 2 Hierarchical Translation with WFSTs The translation system is based on a variant of the CYK algorithm closely related to CYK+ (Chappe43"
N09-1049,W08-0402,0,0.0109888,"marize some extensions to the basic approach to put our work in context. 433 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433–441, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experi"
N09-1049,C08-1064,0,0.00830928,"r et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hi"
N09-1049,P08-1114,0,0.00448828,"on during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation The"
N09-1049,P03-1021,0,0.0432023,"cted and used in translation: target language model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule count features inspired by Bender et al. (2007). The initial English language model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. Details of the parallel corpus and development sets used for each language pair are given in their respective section. Standard MET (Och, 2003) iterative parameter estimation under IBM BLEU (Papineni et al., 2001) is performed on the corresponding development set. For the HCP system, MET is done following Chiang (2007). For the HiFST system, we obtain a kbest list from the translation lattice and extract each feature score with the aligner variant of the k-best decoder. After translation with optimized feature weights, we carry out the two following rescoring steps. • Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using ∼4.7B words of English newswire t"
N09-1049,2001.mtsummit-papers.68,0,0.060378,"e-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule count features inspired by Bender et al. (2007). The initial English language model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition. Details of the parallel corpus and development sets used for each language pair are given in their respective section. Standard MET (Och, 2003) iterative parameter estimation under IBM BLEU (Papineni et al., 2001) is performed on the corresponding development set. For the HCP system, MET is done following Chiang (2007). For the HiFST system, we obtain a kbest list from the translation lattice and extract each feature score with the aligner variant of the k-best decoder. After translation with optimized feature weights, we carry out the two following rescoring steps. • Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using ∼4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HC"
N09-1049,N07-1029,0,0.0191119,"6; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007). WFSTs for Translation There is extensive work in using Weighted Finite State Transducer for machine translation (Bangalore and Riccardi, 2001; Casacuberta, 2001; Kumar and Byrne, 2005; Mathias and Byrne, 2006; Graehl et al., 2008). To our knowledge, this paper presents the first description of hierarchical phrase-based translation in terms of lattices rather than k-best lists. The next section describes hierarchical phrase-based translation with WFSTs, including the lattice construction over the CYK grid and pruning strategies. Section 3 reports translation experiments for Arabic-toEnglish a"
N09-1049,P08-1066,0,0.0225293,"othesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; Rosti et al., 2007)."
N09-1049,2006.amta-papers.25,0,0.102733,"Missing"
N09-1049,D08-1065,0,0.106905,"ce and extract each feature score with the aligner variant of the k-best decoder. After translation with optimized feature weights, we carry out the two following rescoring steps. • Large-LM rescoring. We build sentencespecific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using ∼4.7B words of English newswire text, and apply them to rescore either 10000-best lists generated by HCP or word lattices generated by HiFST. Lattices provide a vast search space relative to k-best lists, with translation lattice sizes of 1081 hypotheses reported in the literature (Tromble et al., 2008). • Minimum Bayes Risk (MBR). We rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function (Kumar and Byrne, 2004). 3.1 Building the Rule Sets We extract hierarchical phrases from word alignments, applying the same restrictions as introduced by Chiang (2005). Additionally, following Iglesias et al. (2009) we carry out two rule filtering strategies: • we exclude rules with two non-terminals with the same order on the source and target side • we consider only the 20 most frequent translations for each rule For each development set, this p"
N09-1049,N07-1063,0,0.0140789,"ation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text. We summarize some extensions to the basic approach to put our work in context. 433 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 433–441, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al. (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic informat"
N09-1049,N06-1033,0,0.0174385,"axed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e.g. (Sim et al., 2007; R"
N09-1049,W06-3119,0,0.0105749,"oduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests. Dyer et al. (2008) extend the translation of source sentences to translation of input lattices following Chappelier et al. (1999). Extensions to Hiero Several authors describe extensions to Hiero, to incorporate additional syntactic information (Zollmann and Venugopal, 2006; Zhang and Gildea, 2006; Shen et al., 2008; Marton and Resnik, 2008), or to combine it with discriminative latent models (Blunsom et al., 2008). Analysis and Contrastive Experiments Zollman et al. (2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English. Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems. Hierarchical translation has also been used to great effect in combination with other translation architectures, e"
N09-1049,C08-1144,0,0.033018,"Missing"
N09-1049,W01-0721,0,\N,Missing
N09-1049,koen-2004-pharaoh,0,\N,Missing
N09-1049,A00-2018,0,\N,Missing
N09-1049,W00-0736,0,\N,Missing
N09-1049,J93-2003,0,\N,Missing
N09-1049,C04-1073,0,\N,Missing
N09-1049,C08-1136,0,\N,Missing
N09-1049,W05-1506,0,\N,Missing
N09-1049,J03-4003,0,\N,Missing
N09-1049,2009.eamt-1.10,0,\N,Missing
N09-1049,P02-1040,0,\N,Missing
N09-1049,P09-1020,0,\N,Missing
N09-1049,P04-1083,0,\N,Missing
N09-1049,W07-0412,0,\N,Missing
N09-1049,N06-4004,1,\N,Missing
N09-1049,P01-1067,0,\N,Missing
N09-1049,J95-4004,0,\N,Missing
N09-1049,H05-1095,0,\N,Missing
N09-1049,N09-1027,0,\N,Missing
N09-1049,W05-0909,0,\N,Missing
N09-1049,P07-2045,0,\N,Missing
N09-1049,C04-1072,0,\N,Missing
N09-1049,P96-1021,0,\N,Missing
N09-1049,N03-1017,0,\N,Missing
N09-1049,P02-1038,0,\N,Missing
N09-1049,W04-1013,0,\N,Missing
N09-1049,E09-1061,0,\N,Missing
N09-1049,2008.iwslt-papers.7,0,\N,Missing
N09-1049,J97-3002,0,\N,Missing
N09-1049,W02-1039,0,\N,Missing
N09-1049,P93-1005,0,\N,Missing
N09-1049,2006.iwslt-evaluation.20,0,\N,Missing
N09-1049,P00-1056,0,\N,Missing
N09-1049,W01-0726,0,\N,Missing
N09-2019,P08-1115,0,0.00950665,"tHDyryp jAmEp l*l+ jmEyp Al+ EAmp fY dwrt +hA Al+ vAnyp w*Al+ xmsyn a preparatory committee of the whole of the general assembly is to be established at its fifty-second session Table 1: Example of alternative segmentation schemes for a given Arabic sentence, in Buckwalter transliteration. 2007). Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morphosyntactic information. Gim´enez and M`arquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-English corpus, but only standard tokens are used in decoding. Dyer et al. (2008) report improvements from multiple Arabic segmentations in translation to English translation, but their goal was to demonstrate the value of lattice-based translation. From a modeling perspective their approach is unwieldy: multiple analyses of the parallel text collections are merged to create a large, heterogeneous training set; a single set of models and alignments is produced; lattice translation is then performed using a single system to translate all morphological analyses. We find that similar gains can be obtained much more easily. The approach we take is Minimum Bayes Risk (MBR) Syst"
N09-2019,W05-0826,0,0.144023,"Missing"
N09-2019,J01-2001,0,0.0126074,"d words, sometimes consisting of several parts, such as ”ulko+maa+n+kauppa+politiikka” (foreign trade policy). Due to these properties, the number of different word forms that can be observed is enormous. Morfessor (Creutz and Lagus, 2007) is a method for modeling concatenative morphology in an unsupervised manner. It tries to find morpheme-like units, morphs, that are segments of the words. Inspired by the minimum description length principle, Morfessor tries to find a concise lexicon of morphs that can effectively code the words in the training data. Unlike other unsupervised methods (e.g., Goldsmith (2001)), there is no restrictions on how many morphs a word can have. After training the model, the most likely segmentation of new words to morphs can be found using the Viterbi algorithm. There exist a few different versions of Morfessor. The baseline algorithm has been found to be very useful in automatic speech recognition of agglutinative languages (Kurimo et al., 2006). However, it 2 Full MT08 results are available at http://www.nist.gov/ speech/tests/mt/2008/doc/mt08 official results v0.html 75 often oversegments morphemes that are rare or not seen at all in the training data. Following the a"
N09-2019,H05-1085,0,0.0116237,"ions to SMT in Section 1.1, but we note the recent general survey (Roark and Sproat, 2007) and the Morpho Challenge competitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming worksho"
N09-2019,P05-1071,0,0.0578425,"m multiple SMT systems are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). It is very likely that even greater gains can be achieved by more complicated combination schemes (Rosti et al., 2007), although significantly more effort in tuning would be required. 2 Arabic-to-English Translation For Arabic-to-English translation, we consider two alternative segmentations of the Arabic words. We first use the MADA toolkit (Habash and Rambow, 2005). After tagging, we split word prefixes and suffixes according to scheme ‘D2’ (Habash and Sadat, 2006). Secondly, we take the segmentation generated by Sakhr Software in Egypt using their Arabic Morphological Tagger, as an alternative segmentation into subword units. This scheme generates more tokens as it segments all Arabic articles which other74 wise remain attached in the MADA D2 scheme (Table 1). Translation experiments are based on the NIST MT08 Arabic-to-English translation task, including all allowed parallel data as training material (∼150M English words, and 153M or 178M Arabic words"
N09-2019,N09-1049,1,0.3452,"Missing"
N09-2019,D07-1091,0,0.00845091,", June 2009. 2009 Association for Computational Linguistics Arabic MADA D2 SAKHR English wqrrt An tn$A ljnp tHDyryp jAmEp lljmEyp AlEAmp fY dwrthA AlvAnyp wAlxmsyn w+ qrrt &gt;n tn$A ljnp tHDyryp jAmEp l+ AljmEyp AlEAmp fy dwrthA AlvAnyp w+ Alxmsyn w+ qrrt An tn$A ljnp tHDyryp jAmEp l*l+ jmEyp Al+ EAmp fY dwrt +hA Al+ vAnyp w*Al+ xmsyn a preparatory committee of the whole of the general assembly is to be established at its fifty-second session Table 1: Example of alternative segmentation schemes for a given Arabic sentence, in Buckwalter transliteration. 2007). Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morphosyntactic information. Gim´enez and M`arquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-English corpus, but only standard tokens are used in decoding. Dyer et al. (2008) report improvements from multiple Arabic segmentations in translation to English translation, but their goal was to demonstrate the value of lattice-based translation. From a modeling perspective their approach is unwieldy: multiple analyses of the parallel text collections are merged to create a large, heterogeneous training set; a s"
N09-2019,N04-1022,1,0.409778,"erged to create a large, heterogeneous training set; a single set of models and alignments is produced; lattice translation is then performed using a single system to translate all morphological analyses. We find that similar gains can be obtained much more easily. The approach we take is Minimum Bayes Risk (MBR) System Combination (Sim et al., 2007). Nbest lists from multiple SMT systems are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). It is very likely that even greater gains can be achieved by more complicated combination schemes (Rosti et al., 2007), although significantly more effort in tuning would be required. 2 Arabic-to-English Translation For Arabic-to-English translation, we consider two alternative segmentations of the Arabic words. We first use the MADA toolkit (Habash and Rambow, 2005). After tagging, we split word prefixes and suffixes according to scheme ‘D2’ (Habash and Sadat, 2006). Secondly, we take the segmentation generated by Sakhr Software in Egypt using their Arabic Morphological Tagger, as an altern"
N09-2019,N06-1062,1,0.865461,"Missing"
N09-2019,J04-2003,0,0.022079,"(Roark and Sproat, 2007) and the Morpho Challenge competitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming workshop. 73 Proceedings of NAACL HLT 2009: Short Papers, pages 7"
N09-2019,popovic-ney-2004-towards,0,0.0232948,"etitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming workshop. 73 Proceedings of NAACL HLT 2009: Short Papers, pages 73–76, c Boulder, Colorado, June 2009. 2009 Association f"
N09-2019,P07-1040,0,0.0246309,"is then performed using a single system to translate all morphological analyses. We find that similar gains can be obtained much more easily. The approach we take is Minimum Bayes Risk (MBR) System Combination (Sim et al., 2007). Nbest lists from multiple SMT systems are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). It is very likely that even greater gains can be achieved by more complicated combination schemes (Rosti et al., 2007), although significantly more effort in tuning would be required. 2 Arabic-to-English Translation For Arabic-to-English translation, we consider two alternative segmentations of the Arabic words. We first use the MADA toolkit (Habash and Rambow, 2005). After tagging, we split word prefixes and suffixes according to scheme ‘D2’ (Habash and Sadat, 2006). Secondly, we take the segmentation generated by Sakhr Software in Egypt using their Arabic Morphological Tagger, as an alternative segmentation into subword units. This scheme generates more tokens as it segments all Arabic articles which other7"
N09-2019,P06-1122,0,0.013867,"t we note the recent general survey (Roark and Sproat, 2007) and the Morpho Challenge competitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming workshop. 73 Proceedings of NAACL"
N09-2019,E03-1007,0,0.0278612,"yzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the evaluation tracks in the upcoming workshop. 73 Proceedings of NAACL HLT 2009: Short Papers, pages 73–76, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Arabic MADA D2 SAKHR English wqrrt An tn$A ljnp tHDyryp jAmEp lljmEyp AlEAmp"
N09-2019,2007.mtsummit-papers.65,1,0.720881,"Missing"
N09-2019,N06-2051,0,0.0194137,"evaluated. We focus on applications to SMT in Section 1.1, but we note the recent general survey (Roark and Sproat, 2007) and the Morpho Challenge competitive evaluations1 . Prior evaluations of morphological analyzers have focused on determining which analyzer was Several earlier works investigate word segmentation and transformation schemes, which may include Part-Of-Speech or other information, to alleviate the effect of morphological variation on translation models. With different training corpus sizes, they focus on translation into English from Arabic (Lee, 2004; Habash and Sadat, 2006; Zollmann et al., 2006), Czech (Goldwater and McClosky, 2005; Talbot and Osborne, 2006), German (Nießen and Ney, 2004) or Catalan, Spanish and Serbian (Popovic and Ney, 2004). Some address the generation challenge when translating from English into Spanish (Ueffing and Ney, 2003; de Gispert and Mari˜no, 2008). Unsupervised morphology learning is proposed as a language-independent solution to reduce the problems of rich morphology in (Virpioja et al., 1 See http://www.cis.hut.fi/morphochallenge2009/ and links Prior Work there to earlier workshops. The combination scheme described in this paper will be one of the eval"
N09-2019,N04-4015,0,\N,Missing
N09-2019,N06-2013,0,\N,Missing
N15-1105,D14-1082,0,0.195659,"g to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SM"
N15-1105,P05-1066,0,0.242204,"equired. Preordering schemes can be automatically learnt from source-side parsed, word-aligned parallel corpora. Recently Jehl et al (2014) described a scheme based on a feature-rich logistic regression model that predicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted. Based on the node-pair swapping probability predictions of this model, a branch-and-bound search returns the best ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NL"
N15-1105,D11-1018,0,0.063682,"omatically learnt from source-side parsed, word-aligned parallel corpora. Recently Jehl et al (2014) described a scheme based on a feature-rich logistic regression model that predicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted. Based on the node-pair swapping probability predictions of this model, a branch-and-bound search returns the best ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including languag"
N15-1105,P14-1129,0,0.0562621,"Missing"
N15-1105,D08-1089,0,0.0440634,"the faster decoding will run as less distortion will be needed. Normalizing over the number of source words allows us to compare this metric across language pairs, and so the potential impact of preordering in translation performance becomes apparent. See Figure 2 for results across several language pairs. In all cases our proposed NN-based preorderer achieves the lowest normalized crossing score among all preordering schemes. 3.3 Translation performance For translation experiments, we use a phrase-based decoder that incorporates a set of standard features and a hierarchical reordering model (Galley and Manning, 2008). The decoder stack size is set to 1000. Weights are tuned using MERT to optimize BLEU on the dev set. In English-to-Japanese and Chinese we use character-BLEU instead. To minimise optimization noise, we tune all our systems from flat parameters three times and report average BLEU score and standard deviation on the test set. Table 1 contrasts the performance obtained by the system when using no preordering capabilities (baseline), and when using three alternative preordering schemes: the rule-based approach of Genzel (2010), the linear-model logistic-regression approach of Jehl et al (2014) a"
N15-1105,C10-1043,0,0.451565,"dicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted. Based on the node-pair swapping probability predictions of this model, a branch-and-bound search returns the best ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denve"
N15-1105,gimenez-marquez-2004-svmtool,0,0.0508697,"Missing"
N15-1105,E14-1026,1,0.431273,"Missing"
N15-1105,D13-1176,0,0.0490322,"North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowledge, it is the first one to describe the usage of NNs in preordering for SMT. 2 Preordering as node-pair swapping Jehl et al (2014) describe a preordering scheme based on a logistic regression model that predicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted in order to have a more"
N15-1105,D13-1049,0,0.175053,"st ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved wor"
N15-1105,D13-1054,0,0.0234436,"increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowledge, it is the first one to describe the usage of NNs in preordering for SMT. 2 Preordering as node-pair sw"
N15-1105,D12-1077,0,0.0601436,"e-side parsed, word-aligned parallel corpora. Recently Jehl et al (2014) described a scheme based on a feature-rich logistic regression model that predicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted. Based on the node-pair swapping probability predictions of this model, a branch-and-bound search returns the best ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et"
N15-1105,D14-1197,0,0.0255573,"In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowledge, it is the first one to describe the usage"
N15-1105,D14-1003,0,0.0228415,"ding language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowledge, it is the first one to describe the usage of NNs in preordering for SMT. 2 Preordering as node-pair swapping Jehl et al (2014) describe a preordering scheme based on a logistic regression model t"
N15-1105,P14-1138,0,0.0137744,"al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowledge, it is the first"
N15-1105,D13-1140,0,0.0374055,"nother logistic regression model or a linear regression model” (Murphy, 2012). Given this, we propose a straightforward alternative to the above framework: replace the linear logistic regression model by a neural network (NN). This way a superior modeling performance of the nodeswapping phenomenon is to be expected. Additionally, feature combination need not be engineered anymore because that is learnt by the NN in training (line 6 in Figure 1 is skipped). Training the neural network requires the same labeled samples that were used by Jehl et al (2014). We use the NPLM toolkit out-of-the-box (Vaswani et al., 2013). The architecture is a feed-forward neural network (Bengio et al., 2003) with four layers. The first layer i contains the input embeddings. The next two hidden layers (h1 , h2 ) use rectified linear units; the last one is the softmax layer (o). We did not experiment with deeper NNs. For our purposes, the input vocabulary of the NN is the set of all possible feature indicator names that are used for preordering1 . There are no OOVs. Given the sequence of ∼ 20 features seen by the 1 Using a vocabulary of the 5K top-frequency English words, 50 word classes, approximately 40 POS tags and 50 depen"
N15-1105,N09-1028,0,0.0389467,"chemes can be automatically learnt from source-side parsed, word-aligned parallel corpora. Recently Jehl et al (2014) described a scheme based on a feature-rich logistic regression model that predicts whether a pair of sibling nodes in the source-side dependency tree need to be permuted. Based on the node-pair swapping probability predictions of this model, a branch-and-bound search returns the best ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been"
N15-1105,P12-1096,0,0.04679,"a branch-and-bound search returns the best ordering of nodes in the tree. 1.1 Related Work There is a strong research and commercial interest in preordering, as reflected by the extensive previous work on the subject (Collins et al., 2005; Xu et al., 2009; DeNero and Uszkoreit, 2011; Neubig et al., 2012). We are interested in practical, language-independent preordering approaches that rely only on automatic sourcelanguage parsers (Genzel, 2010). The most recent work in this area uses large-scale feature-rich disriminative models, effectively treating preordering either as a learning to rank (Yang et al., 2012), multiclassification (Lerner and Petrov, 2013) or logistic regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In t"
N15-1105,P13-1017,0,0.0293528,"regression (Jehl et al., 2014) problem. In this paper we incorporate NNs into the latter approach. Lately an increasing body of work that uses NNs for various NLP tasks has been published, including language modeling (Bengio et al., 2003), POS 1012 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1012–1017, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics tagging (Collobert et al., 2011), or dependency parsing (Chen and Manning, 2014). In translation, NNs have been used for improved word alignment (Yang et al., 2013; Tamura et al., 2014; Songyot and Chiang, 2014), to model reordering under an ITG grammar (Li et al., 2013), and to define additional feature functions to be used in decoding (Sundermeyer et al., 2014; Devlin et al., 2014). End-to-end translation systems based on NNs have also been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Despite the gains reported, only the approaches that do not dramatically affect decoding times can be directly applied to today’s commercial SMT systems. Our paper is a step towards this direction, and to the best of our knowl"
N15-1105,bojar-etal-2014-hindencorp,0,\N,Missing
N16-1100,W12-3159,0,0.0167268,"ed and memory constraints on neural network trainings and report better results compared to those of naive models which explicitly put high costs on regions that violate constraints. A different approach based on augmented Lagrangians is proposed by Gramacy et al. (2014). The authors apply BO in a water decontamination setting where the goal is to find the optimal pump positioning subject to restrictions on water and contaminant flows. All these previous work in constrained BO use GPs as the prior model. Optimizing decoding parameters for speed is an understudied problem in the MT literature. Chung and Galley (2012) propose direct search methods to optimize feature weights and decoder parameters jointly but aiming at the traditional goal of maximizing translation quality. To enable search parameter optimization they enforce a deterministic time penalty on BLEU scores, which is not ideal due to the stochastic nature of time measurements shown 864 on Section 3.2 (this issue is also cited by the authors in their manuscript). It would be interesting to incorporate their approach into BO for optimizing translation quality under speed constraints. 5 Conclusion We have shown that Bayesian Optimisation performs"
N16-1100,D08-1089,0,0.0136634,"ish-to-German we use mixed-domain tuning/test sets, which have about 1K sentences each and were created to evenly represent different domains, including world news, health, sport, science and others. For Chinese-toEnglish we use in-domain sets (2K sentences) created by randomly extracting unique parallel sentences from in-house parallel text collections; this in-domain data leads to higher BLEU scores than in the other tasks, as will be reported later. In all cases we have one reference translation. We use an in-house implementation of a phrase-based decoder with lexicalized reordering model (Galley and Manning, 2008). The system uses 21 features, whose weights are optimized for BLEU via MERT (Och and Ney, 2004) at very slow decoder parameter settings in order to minimize search errors in tuning. The feature weights remain fixed during the speed tuning process. 3.1 n: number of translations. The maximum number of alternative translations per source phrase considered in decoding. 3.2 Measuring Decoding Speed To get a better understanding of the speed measurements we decode the English-German tuning set 100 times with a slow decoder parameter setting, i.e. θ = (5, 100, 100), and repeat for a fast setting wit"
N16-1100,2001.mtsummit-papers.68,0,0.0233509,"ically finding the decoder parameters and feature weights that yield the best BLEU at a specified minimum decoding speed. This is potentially very expensive because each change in a decoder parameter requires re-decoding to assess both BLEU and translation speed. This is under-studied in the literature, despite its importance for real-life commercial SMT engines whose speed and latency can be as significant for user satisfaction as overall translation quality. Introduction Research in Statistical Machine Translation (SMT) aims to improve translation quality, typically measured by BLEU scores (Papineni et al., 2001), over a baseline system. Given a task defined by a language pair and its corpora, the quality of a system is assessed by contrasting choices made in rule/phrase extraction criteria, feature functions, decoding algorithms and parameter optimization techniques. Some of these choices result in systems with significant differences in performance. For example, in phrase-based translation (PBMT) (Koehn et al., ∗ This work was done during an internship of the first author at SDL Research, Cambridge. We propose to use Bayesian Optimization (Brochu et al., 2010b; Shahriari et al., 2015) for this const"
N16-1100,W10-1748,0,0.020055,"andomized sets of sentences. Warped GPs (Snelson et al., 2003) could be a more accurate model as they can learn transformations for heteroscedastic data without relying on a fixed transformation, as we do with log speed measurements. Modelling of the objective function could also be improved. In our experiments we used a GP with a Mat`ern52 kernel, but this assumes f is doubly-differentiable and exhibits Lipschitzcontinuity (Brochu et al., 2010b). Since that does not hold for the BLEU score, using alternative smoother metrics such as linear corpus BLEU (Tromble et al., 2008) or expected BLEU (Rosti et al., 2010) could yield better results. Other recent developments in Bayesian Optimisation could be applied to our settings, like multi-task optimization (Swersky et al., 2013) or freeze-thaw optimization (Swersky et al., 2014). In our application we treat Bayesian Optimisation as a sequential model. Parallel approaches do exist (Snoek et al., 2012; Gonz´alez et al., 2015), but we find it easy enough to harness parallel computation in decoding tuning sets and by decoupling BLEU measurements from speed measurements. However for more complex optimisation scenarios or for problems that require lengthy searc"
N16-1100,D08-1065,0,0.0262866,"sets, one possibility would be to use randomized sets of sentences. Warped GPs (Snelson et al., 2003) could be a more accurate model as they can learn transformations for heteroscedastic data without relying on a fixed transformation, as we do with log speed measurements. Modelling of the objective function could also be improved. In our experiments we used a GP with a Mat`ern52 kernel, but this assumes f is doubly-differentiable and exhibits Lipschitzcontinuity (Brochu et al., 2010b). Since that does not hold for the BLEU score, using alternative smoother metrics such as linear corpus BLEU (Tromble et al., 2008) or expected BLEU (Rosti et al., 2010) could yield better results. Other recent developments in Bayesian Optimisation could be applied to our settings, like multi-task optimization (Swersky et al., 2013) or freeze-thaw optimization (Swersky et al., 2014). In our application we treat Bayesian Optimisation as a sequential model. Parallel approaches do exist (Snoek et al., 2012; Gonz´alez et al., 2015), but we find it easy enough to harness parallel computation in decoding tuning sets and by decoupling BLEU measurements from speed measurements. However for more complex optimisation scenarios or f"
N16-1100,D15-1253,0,0.0308686,"ng procedure of Miao et al. (2014) but obtained mixed results on our test sets. Effective ways to combine BO with well-established feature tuning algorithms such as MERT could be a promising research direction. 4 Related Work Bayesian Optimization has been previously used for hyperparameter optimization in machine learning systems (Snoek et al., 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems. They introduce a multi-stage approach where hyperparameters are optimized using small datasets and then used as starting points for subsequent BO stages using increasing amounts of data. Yogatama et al. (2015) employ BO to optimize text representations in a set of classification tasks. They find that there is no representation that is optimal for all tasks, which further justifies an automatic tuning approach. Wang et al. (2014) use a model based on optimistic optimization to tune parameters of a term extraction system. In SMT,"
N16-1100,D15-1251,0,0.0165713,"for hyperparameter optimization in machine learning systems (Snoek et al., 2012; Bergstra et al., 2011), automatic algorithm configuration (Hutter et al., 2011) and for applications in which system tuning involves human feedback (Brochu et al., 2010a). Recently, it has also been used successfully in several NLP applications. Wang et al. (2015) use BO to tune sentiment analysis and question answering systems. They introduce a multi-stage approach where hyperparameters are optimized using small datasets and then used as starting points for subsequent BO stages using increasing amounts of data. Yogatama et al. (2015) employ BO to optimize text representations in a set of classification tasks. They find that there is no representation that is optimal for all tasks, which further justifies an automatic tuning approach. Wang et al. (2014) use a model based on optimistic optimization to tune parameters of a term extraction system. In SMT, Miao et al. (2014) use BO for feature weight tuning and report better results in some language pairs when compared to traditional tuning algorithms. Our approach is heavily based on the work of Gelbart et al. (2014) and Hern´andez-Lobato et al. (2015) which uses BO in the pr"
N16-1100,P02-1040,0,\N,Missing
N16-1100,J04-4002,0,\N,Missing
N16-1100,N03-1017,0,\N,Missing
N18-2081,P17-1141,0,0.0649765,"y (Chatterjee et al., 2017). While providing lexical guidance to the decoder, these methods do not strictly enforce a terminology. This is a requisite, however, for companies wanting to ensure that brandrelated information is rendered correctly and consistently when translating web content or manuals and is often more important than translation quality alone. Although domain adaptation and guided decoding can help to reduce errors in these use cases, they do not provide reliable solutions. Another recent line of work strictly enforces a given set of words in the output (Anderson et al., 2017; Hokamp and Liu, 2017; Crego et al., 2016). 2 Constrained Beam Search A naive approach to decoding with constraints would be to use a large beam size and select from the set of complete hypotheses the best that satis506 Proceedings of NAACL-HLT 2018, pages 506–512 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics fies all constraints. However, this is infeasible in practice because it would require searching a potentially very large space to ensure that even hypotheses with low model score due to the inclusion of a constraint would be part of the set of outputs. A better st"
N18-2081,W17-4711,0,0.0535327,"tives and encoded as different arcs connecting the same states. When alternatives consist of multiple tokens, the alternative paths will contain intermediate states. Figure 1 shows an FSA with constraints C1 and C2 where C1 is a phrase (yielding intermediate states s1 , s4 ) and C2 consists of two single-token alternatives. Both permutations C1 C2 and C2 C1 lead to final state s5 with both constraints satisfied. 2.2 a s0 V - {a} - C2 s0 C [i,j) s1 j &gt; i, 0 ≤ i, j ≤ |S| Because the attention weights in attention-based decoders function as soft alignments from the target to the source sentence (Alkhouli and Ney, 2017), we use them to decide at which position a constraint should be inserted in the output. At each time step in a hypothesis, we determine the source position with the maximum attention. If it falls into a constrained source span and this span matches an outgoing arc in the current acceptor state, we extend the current hypothesis with the arc label. Thus, the outgoing arcs in non-intermediate states are active or inactive depending on the current attentions. This reduces the complexity from O(tk2c ) to O(tkc) by ignoring all but one constraint permutation and in practice, disabling vocabulary lo"
N18-2081,N18-3013,1,0.878414,"Missing"
N18-2081,D17-1098,0,0.4558,"tions from a terminology (Chatterjee et al., 2017). While providing lexical guidance to the decoder, these methods do not strictly enforce a terminology. This is a requisite, however, for companies wanting to ensure that brandrelated information is rendered correctly and consistently when translating web content or manuals and is often more important than translation quality alone. Although domain adaptation and guided decoding can help to reduce errors in these use cases, they do not provide reliable solutions. Another recent line of work strictly enforces a given set of words in the output (Anderson et al., 2017; Hokamp and Liu, 2017; Crego et al., 2016). 2 Constrained Beam Search A naive approach to decoding with constraints would be to use a large beam size and select from the set of complete hypotheses the best that satis506 Proceedings of NAACL-HLT 2018, pages 506–512 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics fies all constraints. However, this is infeasible in practice because it would require searching a potentially very large space to ensure that even hypotheses with low model score due to the inclusion of a constraint would be part of the set o"
N18-2081,Q17-1024,0,0.0992256,"Missing"
N18-2081,2015.iwslt-evaluation.11,0,0.0610273,"en problem. We describe our approach to constrained neural decoding based on finite-state machines and multistack decoding which supports target-side constraints as well as constraints with corresponding aligned input text spans. We demonstrate the performance of our framework on multiple translation tasks and motivate the need for constrained decoding with attentions as a means of reducing misplacement and duplication when translating user constraints. 1 Introduction Adapting an NMT system with domain-specific data is one way to adjust its output vocabulary to better match the target domain (Luong and Manning, 2015; Sennrich et al., 2016). Another way to encourage the beam decoder to produce certain words in the output is to explicitly reward n-grams provided by an SMT system (Stahlberg et al., 2017) or language model (Gulcehre et al., 2017) or to modify the vocabulary distribution of the decoder with suggestions from a terminology (Chatterjee et al., 2017). While providing lexical guidance to the decoder, these methods do not strictly enforce a terminology. This is a requisite, however, for companies wanting to ensure that brandrelated information is rendered correctly and consistently when translating"
N18-2081,P16-1009,0,0.272321,"ur approach to constrained neural decoding based on finite-state machines and multistack decoding which supports target-side constraints as well as constraints with corresponding aligned input text spans. We demonstrate the performance of our framework on multiple translation tasks and motivate the need for constrained decoding with attentions as a means of reducing misplacement and duplication when translating user constraints. 1 Introduction Adapting an NMT system with domain-specific data is one way to adjust its output vocabulary to better match the target domain (Luong and Manning, 2015; Sennrich et al., 2016). Another way to encourage the beam decoder to produce certain words in the output is to explicitly reward n-grams provided by an SMT system (Stahlberg et al., 2017) or language model (Gulcehre et al., 2017) or to modify the vocabulary distribution of the decoder with suggestions from a terminology (Chatterjee et al., 2017). While providing lexical guidance to the decoder, these methods do not strictly enforce a terminology. This is a requisite, however, for companies wanting to ensure that brandrelated information is rendered correctly and consistently when translating web content or manuals"
N18-2081,W17-4716,0,0.057497,"coding with attentions as a means of reducing misplacement and duplication when translating user constraints. 1 Introduction Adapting an NMT system with domain-specific data is one way to adjust its output vocabulary to better match the target domain (Luong and Manning, 2015; Sennrich et al., 2016). Another way to encourage the beam decoder to produce certain words in the output is to explicitly reward n-grams provided by an SMT system (Stahlberg et al., 2017) or language model (Gulcehre et al., 2017) or to modify the vocabulary distribution of the decoder with suggestions from a terminology (Chatterjee et al., 2017). While providing lexical guidance to the decoder, these methods do not strictly enforce a terminology. This is a requisite, however, for companies wanting to ensure that brandrelated information is rendered correctly and consistently when translating web content or manuals and is often more important than translation quality alone. Although domain adaptation and guided decoding can help to reduce errors in these use cases, they do not provide reliable solutions. Another recent line of work strictly enforces a given set of words in the output (Anderson et al., 2017; Hokamp and Liu, 2017; Crego"
N18-2081,1981.tc-1.3,0,0.819746,"Missing"
N18-2081,E17-2058,1,0.891813,"Missing"
N18-3013,W17-4716,0,0.0641777,"ng without GPUs, and still an avenue for research (Devlin, 2017). Great speeds have been reported by Junczys-Dowmunt et al. (2016) on GPUs, for which batching queries to the neural model is essential. Disk usage and memory footprint of pure neural systems are certainly lower than that of SMT systems, but at the same time GPU memory is limited and high-end GPUs are expensive. Further to that, consumers still need the ability to constrain translations; in particular, brandrelated information is often as important for companies as translation quality itself, and is currently under investigation (Chatterjee et al., 2017; Hokamp and Liu, 2017; Hasler et al., 2018). It is also well known that pure neural systems 2 Neural Machine Translation and LMBR Given a source sentence x, a sequence-tosequence NMT model scores a candidate translation sentence y = y1T with T words as: PN M T (y1T |x) = T Y t=1 PN M T (yt |y1t−1 , x) (1) where PN M T (yt |y1t−1 , x) uses a neural function fN M T (·). To account for batching B neu106 Proceedings of NAACL-HLT 2018, pages 106–113 c New Orleans, Louisiana, June 1 - 6, 2018. 2017 Association for Computational Linguistics ral queries together, our abstract function takes the form"
N18-3013,2006.iwslt-papers.6,0,0.0478008,"o particular constraint on speed for the research systems reported in Table 1. We now address the question of deploying NMT systems so that MT users get the best quality improvements at real-time speed and with acceptable memory footprint. As an example, we analyse in detail the English-German FNMT and LNMT case and discuss the main trade-offs if one wanted to accelerate them. Although the actual measurements vary across all our productised NMT engines, the trends are similar to the ones reported here. In this particular case we specify a beam width of 0.01 for early pruning (Wu et al., 2016; Delaney et al., 2006) and reduce the beam size to 4. We also shrink the ensemble into one single big model5 using the data-free shrinking method described by Stahlberg and Byrne (2017), an inexpensive way to improve both speed and GPU memory footprint. In the process, both accelerated systems have lost 0.9 BLEU relative to the baseline. As an example, let us break down the effects of accelerating the LNMT system: using only 200-best hypotheses from the phrase-based translation lattice reduces 0.3 BLEU. Replacing the ensemble with a data-free shrunken model reduces another 0.2 BLEU and decreasing the beam size redu"
N18-3013,D17-1300,0,0.0784178,"n effective way to counter these problems by taking advantage of the higher adequacy inherent to SMT systems via Lattice Minimum Bayes Risk (LMBR) decoding (Tromble et al., 2008). This makes the system more robust to pitfalls, such as over- and under-generation (Feng et al., 2016; Meng et al., 2016; Tu et al., 2016) which is important for commercial applications. In this paper, we describe a batched beam decoding algorithm that uses NMT models with LMBR n-gram posterior probabilities (Stahlberg et al., 2017). Batching in NMT beam decoding has been mentioned or assumed in the literature, e.g. (Devlin, 2017; Junczys-Dowmunt et al., 2016), but to the best of our knowledge it has not been formally described, and there are interesting aspects for deployment worth taking into consideration. We also report on the effect of LMBR posteriors on state-of-the-art neural systems, for five translation tasks. Finally, we discuss how to prepare (LMBR-based) NMT systems for deployment, and how our batching algorithm performs in terms of memory and speed. We describe a batched beam decoding algorithm for NMT with LMBR n-gram posteriors, showing that LMBR techniques still yield gains on top of the best recently"
N18-3013,C16-1290,0,0.0222591,"gispert|ehasler|bbyrne}@sdl.com † ‡ Department of Engineering, University of Cambridge, U.K. Abstract reach very high fluency, often sacrificing adequacy (Tu et al., 2017; Zhang et al., 2017; Koehn and Knowles, 2017), and have been reported to behave badly under noisy conditions (Belinkov and Bisk, 2018). Stahlberg et al. (2017) show an effective way to counter these problems by taking advantage of the higher adequacy inherent to SMT systems via Lattice Minimum Bayes Risk (LMBR) decoding (Tromble et al., 2008). This makes the system more robust to pitfalls, such as over- and under-generation (Feng et al., 2016; Meng et al., 2016; Tu et al., 2016) which is important for commercial applications. In this paper, we describe a batched beam decoding algorithm that uses NMT models with LMBR n-gram posterior probabilities (Stahlberg et al., 2017). Batching in NMT beam decoding has been mentioned or assumed in the literature, e.g. (Devlin, 2017; Junczys-Dowmunt et al., 2016), but to the best of our knowledge it has not been formally described, and there are interesting aspects for deployment worth taking into consideration. We also report on the effect of LMBR posteriors on state-of-the-art neural systems,"
N18-3013,P17-1141,0,0.0503143,"l an avenue for research (Devlin, 2017). Great speeds have been reported by Junczys-Dowmunt et al. (2016) on GPUs, for which batching queries to the neural model is essential. Disk usage and memory footprint of pure neural systems are certainly lower than that of SMT systems, but at the same time GPU memory is limited and high-end GPUs are expensive. Further to that, consumers still need the ability to constrain translations; in particular, brandrelated information is often as important for companies as translation quality itself, and is currently under investigation (Chatterjee et al., 2017; Hokamp and Liu, 2017; Hasler et al., 2018). It is also well known that pure neural systems 2 Neural Machine Translation and LMBR Given a source sentence x, a sequence-tosequence NMT model scores a candidate translation sentence y = y1T with T words as: PN M T (y1T |x) = T Y t=1 PN M T (yt |y1t−1 , x) (1) where PN M T (yt |y1t−1 , x) uses a neural function fN M T (·). To account for batching B neu106 Proceedings of NAACL-HLT 2018, pages 106–113 c New Orleans, Louisiana, June 1 - 6, 2018. 2017 Association for Computational Linguistics ral queries together, our abstract function takes the form of fN M T (St−1 , yt−1"
N18-3013,W17-3204,0,0.0781637,"Missing"
N18-3013,P10-2006,1,0.911633,"to focus on, informed by A and St−1 . Bahdanau et al. (2015) use recurrent layers to both compute A and the next target word yt . Gehring et al. (2017) use convolutional layers instead, and Vaswani et al. (2017) prescind from GRU or LSTM layers, relying heavily on multi-layered attention mechanisms, stateful only on the translation side. Finally, this function can also represent an ensemble of neural models. Lattice Minimum Bayes Risk decoding computes n-gram posterior probabilities from an evidence space and uses them to score a hypothesis space (Kumar and Byrne, 2004; Tromble et al., 2008; Blackwood et al., 2010). It improves single SMT systems, and also lends itself quite nicely to system combination (Sim et al., 2007; de Gispert et al., 2009). Stahlberg et al. (2017) have recently shown a way to use it with NMT decoding: a traditional SMT system is first used to create an evidence space ϕe , and the NMT space is then scored left-to-right with both the NMT model(s) and the n-gram posteriors gathered from ϕe . More formally: z it turns into a dense matrix with the summation of Θ0 . Both sparse and dense operations can be performed on the GPU. We have found it more efficient to compute first all the sp"
N18-3013,N04-1022,0,0.307153,"n mechanism that determines which source word to focus on, informed by A and St−1 . Bahdanau et al. (2015) use recurrent layers to both compute A and the next target word yt . Gehring et al. (2017) use convolutional layers instead, and Vaswani et al. (2017) prescind from GRU or LSTM layers, relying heavily on multi-layered attention mechanisms, stateful only on the translation side. Finally, this function can also represent an ensemble of neural models. Lattice Minimum Bayes Risk decoding computes n-gram posterior probabilities from an evidence space and uses them to score a hypothesis space (Kumar and Byrne, 2004; Tromble et al., 2008; Blackwood et al., 2010). It improves single SMT systems, and also lends itself quite nicely to system combination (Sim et al., 2007; de Gispert et al., 2009). Stahlberg et al. (2017) have recently shown a way to use it with NMT decoding: a traditional SMT system is first used to create an evidence space ϕe , and the NMT space is then scored left-to-right with both the NMT model(s) and the n-gram posteriors gathered from ϕe . More formally: z it turns into a dense matrix with the summation of Θ0 . Both sparse and dense operations can be performed on the GPU. We have foun"
N18-3013,C16-1205,0,0.0245164,"rne}@sdl.com † ‡ Department of Engineering, University of Cambridge, U.K. Abstract reach very high fluency, often sacrificing adequacy (Tu et al., 2017; Zhang et al., 2017; Koehn and Knowles, 2017), and have been reported to behave badly under noisy conditions (Belinkov and Bisk, 2018). Stahlberg et al. (2017) show an effective way to counter these problems by taking advantage of the higher adequacy inherent to SMT systems via Lattice Minimum Bayes Risk (LMBR) decoding (Tromble et al., 2008). This makes the system more robust to pitfalls, such as over- and under-generation (Feng et al., 2016; Meng et al., 2016; Tu et al., 2016) which is important for commercial applications. In this paper, we describe a batched beam decoding algorithm that uses NMT models with LMBR n-gram posterior probabilities (Stahlberg et al., 2017). Batching in NMT beam decoding has been mentioned or assumed in the literature, e.g. (Devlin, 2017; Junczys-Dowmunt et al., 2016), but to the best of our knowledge it has not been formally described, and there are interesting aspects for deployment worth taking into consideration. We also report on the effect of LMBR posteriors on state-of-the-art neural systems, for five translatio"
N18-3013,D17-1151,0,0.0226232,"(2017), an inexpensive way to improve both speed and GPU memory footprint. In the process, both accelerated systems have lost 0.9 BLEU relative to the baseline. As an example, let us break down the effects of accelerating the LNMT system: using only 200-best hypotheses from the phrase-based translation lattice reduces 0.3 BLEU. Replacing the ensemble with a data-free shrunken model reduces another 0.2 BLEU and decreasing the beam size reduces 0.4 BLEU. The impact of reducing the beam size varies from system to system, although often does not result in substantial quality loss for NMT models (Britz et al., 2017). It is worth noting that these two systems share exactly the same neural model and parameter values. However, LNMT runs 4500 words per minute (wpm) slower than FNMT. Figure 1 breaks down the decoding times for both the accelerated FNMT and LNMT systems. The LNMT pipeline also requires a phrase-based decoder and the extra component to compute the n-gram posterior probabil5 The file size of each 3 individual models of the ensemble is 510MB; the size of the shrunken model is 1.2GB. 110 Figure 2: Batch beam decoder speed measured over newstest-2017 test set, using the accelerated FNMT system (25."
N18-3013,Q17-1007,0,0.0470511,"Missing"
N18-3013,P16-1008,0,0.0228724,"partment of Engineering, University of Cambridge, U.K. Abstract reach very high fluency, often sacrificing adequacy (Tu et al., 2017; Zhang et al., 2017; Koehn and Knowles, 2017), and have been reported to behave badly under noisy conditions (Belinkov and Bisk, 2018). Stahlberg et al. (2017) show an effective way to counter these problems by taking advantage of the higher adequacy inherent to SMT systems via Lattice Minimum Bayes Risk (LMBR) decoding (Tromble et al., 2008). This makes the system more robust to pitfalls, such as over- and under-generation (Feng et al., 2016; Meng et al., 2016; Tu et al., 2016) which is important for commercial applications. In this paper, we describe a batched beam decoding algorithm that uses NMT models with LMBR n-gram posterior probabilities (Stahlberg et al., 2017). Batching in NMT beam decoding has been mentioned or assumed in the literature, e.g. (Devlin, 2017; Junczys-Dowmunt et al., 2016), but to the best of our knowledge it has not been formally described, and there are interesting aspects for deployment worth taking into consideration. We also report on the effect of LMBR posteriors on state-of-the-art neural systems, for five translation tasks. Finally,"
N18-3013,W17-3208,0,0.0387818,"Missing"
N18-3013,P03-1021,0,0.0195144,"1 WMT17 eng-ger chi-eng 19.6 15.8 26.1 20.8 26.6 22.0 28.9 24.8 29.2 25.4 28.3 26.4 WAT eng-jpn jpn-eng 33.4 18.0 39.1 25.3 40.4 26.1 44.6 29.4 44.9 30.2 43.3 28.4 Table 1: Quality assessment of our NMT systems with and without LMBR posteriors for GRU-based (FNMT, LNMT) and Transformer models (TNMT, LTNMT). Cased BLEU scores reported on 5 translation tasks.The exact PBMT systems used to compute n-gram posteriors for LNMT and LTNMT systems are also reported. The last row shows scores for the best official submissions to each task. model (Heafield et al., 2013), and is tuned with standard MERT (Och, 2003); n-gram posterior probabilities are computed on-the-fly over rich translation lattices, with size bounded by the PBMT stack and distortion limits. The parameter λ in Equation 2 is set as 0.5 divided by the number of models in the ensemble. Empirically we have found this to be a good setting in many tasks. Unless noted otherwise, the beam size is set to 12 and the NMT beam decoder always batches queries to the neural model. The beam decoder relies on an early preview of ArrayFire 3.6 (Yalamanchili et al., 2015)3 , compiled with CUDA 8.0 libraries. For speed measurements, the decoder uses one s"
N18-3013,P02-1040,0,0.102076,"limits. The parameter λ in Equation 2 is set as 0.5 divided by the number of models in the ensemble. Empirically we have found this to be a good setting in many tasks. Unless noted otherwise, the beam size is set to 12 and the NMT beam decoder always batches queries to the neural model. The beam decoder relies on an early preview of ArrayFire 3.6 (Yalamanchili et al., 2015)3 , compiled with CUDA 8.0 libraries. For speed measurements, the decoder uses one single CPU thread. For hardware, we use an Intel Xeon CPU E5-2640 at 2.60GHz. The GPU is a GeForce GTX 1080Ti. We report cased BLEU scores (Papineni et al., 2002), strictly comparable to the official scores in each task4 . pairs for the WMT17 task, and Japanese-English and English-Japanese for the WAT task. For the German tasks we use news-test2013 as a development set, and news-test2017 as a test set; for Chinese-English, we use news-dev2017 as a development set, and news-test2017 as a test set. For Japanese tasks we use the ASPEC corpus (Nakazawa et al., 2016). We use all available data in each task for training. In addition, for German we use backtranslation data (Sennrich et al., 2016a). All training data for neural models is preprocessed with the"
N18-3013,P16-1009,0,0.374586,"prepare (LMBR-based) NMT systems for deployment, and how our batching algorithm performs in terms of memory and speed. We describe a batched beam decoding algorithm for NMT with LMBR n-gram posteriors, showing that LMBR techniques still yield gains on top of the best recently reported results with Transformers. We also discuss acceleration strategies for deployment, and the effect of the beam size and batching on memory and speed. 1 Introduction The advent of Neural Machine Translation (NMT) has revolutionized the market. Objective improvements (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Gehring et al., 2017; Vaswani et al., 2017) and a fair amount of neural hype have increased the pressure on companies offering Machine Translation services to shift as quickly as possible to this new paradigm. Such a radical change entails non-trivial challenges for deployment; consumers certainly look forward to better translation quality, but do not want to lose all the good features that have been developed over the years along with SMT technology. With NMT, real time decoding is challenging without GPUs, and still an avenue for research (Devlin, 2017). Great speeds have been reported by"
N18-3013,P16-1162,0,0.380163,"prepare (LMBR-based) NMT systems for deployment, and how our batching algorithm performs in terms of memory and speed. We describe a batched beam decoding algorithm for NMT with LMBR n-gram posteriors, showing that LMBR techniques still yield gains on top of the best recently reported results with Transformers. We also discuss acceleration strategies for deployment, and the effect of the beam size and batching on memory and speed. 1 Introduction The advent of Neural Machine Translation (NMT) has revolutionized the market. Objective improvements (Sutskever et al., 2014; Bahdanau et al., 2015; Sennrich et al., 2016b; Gehring et al., 2017; Vaswani et al., 2017) and a fair amount of neural hype have increased the pressure on companies offering Machine Translation services to shift as quickly as possible to this new paradigm. Such a radical change entails non-trivial challenges for deployment; consumers certainly look forward to better translation quality, but do not want to lose all the good features that have been developed over the years along with SMT technology. With NMT, real time decoding is challenging without GPUs, and still an avenue for research (Devlin, 2017). Great speeds have been reported by"
N18-3013,I17-1016,0,0.0328748,"Missing"
N18-3013,D17-1208,1,0.787244,"est quality improvements at real-time speed and with acceptable memory footprint. As an example, we analyse in detail the English-German FNMT and LNMT case and discuss the main trade-offs if one wanted to accelerate them. Although the actual measurements vary across all our productised NMT engines, the trends are similar to the ones reported here. In this particular case we specify a beam width of 0.01 for early pruning (Wu et al., 2016; Delaney et al., 2006) and reduce the beam size to 4. We also shrink the ensemble into one single big model5 using the data-free shrinking method described by Stahlberg and Byrne (2017), an inexpensive way to improve both speed and GPU memory footprint. In the process, both accelerated systems have lost 0.9 BLEU relative to the baseline. As an example, let us break down the effects of accelerating the LNMT system: using only 200-best hypotheses from the phrase-based translation lattice reduces 0.3 BLEU. Replacing the ensemble with a data-free shrunken model reduces another 0.2 BLEU and decreasing the beam size reduces 0.4 BLEU. The impact of reducing the beam size varies from system to system, although often does not result in substantial quality loss for NMT models (Britz e"
N18-3013,E17-2058,1,0.780067,"Missing"
N18-3013,P16-2049,1,0.838436,"es to improve on. In addition, for LNMT systems we tune phrasebased decoder parameters such as the distortion limit, the number of translations per source phrase and the stack limit. To compute n-gram posteriors we now only take a 200-best from the phrasebased translation lattice. Table 2 shows a contrast of our English-German WMT17 research systems versus the respective accelerated ones. 3. Further, applying LMBR posteriors along with the Transformer model yields gains in all tasks (LTNMT vs TNMT), up to +0.8BLEU in Japanese-English. Interestingly, while we find that rescoring PBMT lattices (Stahlberg et al., 2016) with GRU models yields similar improvements to those reported by Stahlberg et al. (2017), we did not find gains when rescoring with the stronger TNMT models instead. 4.3 FNMT LNMT Research BLEU speed 26.1 2207 26.6 263 Accelerated BLEU speed 25.2 9449 25.7 4927 Table 2: Cased BLEU scores for research vs accelerated English-to-German WMT17 systems. Speed reported in words per minute. Accelerating FNMT and LNMT systems for deployment There is no particular constraint on speed for the research systems reported in Table 1. We now address the question of deploying NMT systems so that MT users get"
N18-3013,D08-1065,0,0.240919,"as† William Tambellini† Adri`a De Gispert†‡ Eva Hasler† Bill Byrne†‡ SDL Research {giglesias|wtambellini|agispert|ehasler|bbyrne}@sdl.com † ‡ Department of Engineering, University of Cambridge, U.K. Abstract reach very high fluency, often sacrificing adequacy (Tu et al., 2017; Zhang et al., 2017; Koehn and Knowles, 2017), and have been reported to behave badly under noisy conditions (Belinkov and Bisk, 2018). Stahlberg et al. (2017) show an effective way to counter these problems by taking advantage of the higher adequacy inherent to SMT systems via Lattice Minimum Bayes Risk (LMBR) decoding (Tromble et al., 2008). This makes the system more robust to pitfalls, such as over- and under-generation (Feng et al., 2016; Meng et al., 2016; Tu et al., 2016) which is important for commercial applications. In this paper, we describe a batched beam decoding algorithm that uses NMT models with LMBR n-gram posterior probabilities (Stahlberg et al., 2017). Batching in NMT beam decoding has been mentioned or assumed in the literature, e.g. (Devlin, 2017; Junczys-Dowmunt et al., 2016), but to the best of our knowledge it has not been formally described, and there are interesting aspects for deployment worth taking in"
P05-2012,A00-1031,0,0.048086,"Missing"
P05-2012,carreras-etal-2004-freeling,0,0.147558,"Missing"
P05-2012,corston-oliver-gamon-2004-normalizing,0,0.0122805,"anslation using IBM model 1 in an Arabic – English task can be found in (Lee, 2004). From a processed Arabic text with all prefixes and suffixes separated, the author determines which of them should be linked back to the word and which should not. However, no mapping to base forms is performed, and plurals are still different words than singulars. In (Nießen and Ney, 2004) hierarchical lexicon models including base form and POS information for translation from German into English are introduced, among other morphology-based data transformations. Finally, the same pair of languages is used in (Corston-Oliver and Gamon, 2004), where the inflectional normalization leads to improvements in the perplexity of IBM translation models and reduces alignment errors. However, compound verbs are not mentioned. 7 Conclusion A proposal of linguistically classifying translation phrases to improve statistical machine translation performance has been presented. This classification allows for a better translation modeling and a generalization to unseen forms. A preliminary implementation detecting verbs in an English – Spanish task has been presented. Experiments show a significant improvement in word alignment, and in preliminary"
P05-2012,N04-1033,0,0.114918,"Missing"
P05-2012,N04-4015,0,0.0315114,"ated against the current implementation. 6 Related Work The approach to deal with inflected forms presented in (Ueffing and Ney, 2003) is similar in that it also tackles verbs in an English – Spanish task. However, whereas the authors join personal pronouns and auxiliaries to form extended English units and do not transform the Spanish side, leading to an increased English vocabulary, our proposal aims at reducing both vocabularies by mapping all different verb forms to the base form of the head verb. An improvement in translation using IBM model 1 in an Arabic – English task can be found in (Lee, 2004). From a processed Arabic text with all prefixes and suffixes separated, the author determines which of them should be linked back to the word and which should not. However, no mapping to base forms is performed, and plurals are still different words than singulars. In (Nießen and Ney, 2004) hierarchical lexicon models including base form and POS information for translation from German into English are introduced, among other morphology-based data transformations. Finally, the same pair of languages is used in (Corston-Oliver and Gamon, 2004), where the inflectional normalization leads to impr"
P05-2012,J04-2003,0,0.02938,"extended English units and do not transform the Spanish side, leading to an increased English vocabulary, our proposal aims at reducing both vocabularies by mapping all different verb forms to the base form of the head verb. An improvement in translation using IBM model 1 in an Arabic – English task can be found in (Lee, 2004). From a processed Arabic text with all prefixes and suffixes separated, the author determines which of them should be linked back to the word and which should not. However, no mapping to base forms is performed, and plurals are still different words than singulars. In (Nießen and Ney, 2004) hierarchical lexicon models including base form and POS information for translation from German into English are introduced, among other morphology-based data transformations. Finally, the same pair of languages is used in (Corston-Oliver and Gamon, 2004), where the inflectional normalization leads to improvements in the perplexity of IBM translation models and reduces alignment errors. However, compound verbs are not mentioned. 7 Conclusion A proposal of linguistically classifying translation phrases to improve statistical machine translation performance has been presented. This classificati"
P05-2012,P00-1056,0,0.0489106,"entage of unseen verb forms and lemmas are also shown. In average, detected English verbs contain 1.81 words, whereas Spanish verbs contain 1.08 words. This is explained by the fact that we are including the personal pronouns in English and modals for future, conditionals and other verb tenses. 4.3 Word alignment results In order to assess the quality of the word alignment, we randomly selected from the training corpus 350 sentences, and a manual gold standard alignment has been done with the criterion of Sure and Possible links, in order to compute Alignment Error Rate (AER) as described in (Och and Ney, 2000) and widely used in literature, together with appropriately redefined Recall and Precision measures. Mathematically, they can be expressed thus: recall = |A ∩ S| |A ∩ P | , precision = |S| |A| Verb Phrase Detection/Classification Table 2 shows the number of detected verbs using the detection rules presented in section 3.1, and the 70 AER = 1 − |A ∩ S |+ |A ∩ P | |A |+ |S| where A is the hypothesis alignment and S is the set of Sure links in the gold standard reference, and P includes the set of Possible and Sure links in the gold standard reference. We have aligned our data using GIZA++ (Och,"
P05-2012,J04-4002,0,0.038331,"rkshop, pages 67–72, c Ann Arbor, Michigan, June 2005. 2005 Association for Computational Linguistics 2 Morphosyntactic classification of translation units State-of-the-art SMT systems use a log-linear combination of models to decide the best-scoring target sentence given a source sentence. Among these models, the basic ones are a translation model P r(e|f ) and a target language model P r(e), which can be complemented by reordering models (if the language pairs presents very long alignments in training), word penalty to avoid favoring short sentences, class-based target-language models, etc (Och and Ney, 2004). The translation model is based on phrases; we have a table of the probabilities of translating a certain source phrase f˜j into a certain target phrase e˜k . Several strategies to compute these probabilities have been proposed (Zens et al., 2004; Crego et al., 2004), but none of them takes into account the fact that, when it comes to translation, many different inflected forms of words share the same translation. Furthermore, they try to model the probability of translating certain phrases that contain just auxiliary words that are not directly relevant in translation, but play a secondary r"
P05-2012,E03-1007,0,0.0296615,"he night, nights or during the night would all be mapped to the class ’night’. • Temporal and numeric expressions. As they are usually tackled in a preprocessing stage in current SMT systems, we did not deal with them here. More on a long-term basis, ambiguous linguistic classification could also be allowed and included in the translation model. For this, incorporating statistical classification tools (chunkers, shallow parsers, phrase detectors, etc.) should be considered, and evaluated against the current implementation. 6 Related Work The approach to deal with inflected forms presented in (Ueffing and Ney, 2003) is similar in that it also tackles verbs in an English – Spanish task. However, whereas the authors join personal pronouns and auxiliaries to form extended English units and do not transform the Spanish side, leading to an increased English vocabulary, our proposal aims at reducing both vocabularies by mapping all different verb forms to the base form of the head verb. An improvement in translation using IBM model 1 in an Arabic – English task can be found in (Lee, 2004). From a processed Arabic text with all prefixes and suffixes separated, the author determines which of them should be linke"
P10-2006,N10-1139,0,0.225287,"Missing"
P10-2006,D07-1090,0,0.193351,"alignments are generated using MTTK (Deng and Byrne, 2008) over 150M words of parallel text for the constrained NIST MT08 Arabic→English track. In decoding, a Shallow1 grammar with a single level of rule nesting is used and no pruning is performed in generating first-pass lattices (Iglesias et al., 2009). The first-pass language model is a modified Kneser-Ney (Kneser and Ney, 1995) 4-gram estimated over the English parallel text and an 881M word subset of the GigaWord Third Edition (Graff et al., 2007). Prior to LMBR, the lattices are rescored with large stupid-backoff 5-gram language models (Brants et al., 2007) estimated over more than 6 billion words of English text. The n-gram factors θ0 , . . . , θ4 are set according to Tromble et al. (2008) using unigram precision 3 p = 0.85 and average recall ratio r = 0.74. Our translation decoder and MBR procedures are implemented using OpenFst (Allauzen et al., 2007). 6 LMBR Speed and Performance Lattice MBR decoding performance is shown in Table 1. Compared to the maximum likelihood translation hypotheses (row ML), LMBR gives gains of +0.8 to +1.0 BLEU for newswire data and +0.5 BLEU for newsgroup data (row LMBR). The other rows of Table 1 show the performa"
P10-2006,D08-1065,0,0.283274,"he required statistics. We show that these procedures can be implemented exactly through simple transformations of word sequences to sequences of n-grams. This yields a novel implementation of lattice minimum Bayes-risk decoding which is fast and exact even for very large lattices. ˆ = argmin E E ′ ∈E L(E, E ′ )P (E|F ) (3) E∈E The approximation replaces the sum over all paths in the lattice by a sum over lattice n-grams. Even though a lattice may have many n-grams, it is possible to extract and enumerate them exactly whereas this is often impossible for individual paths. Therefore, while the Tromble et al. (2008) linearisation of the gain function in the decision rule is an approximation, Equation (1) can be computed exactly even over very large lattices. The challenge is to do so efficiently. If the quantity p(u|E) had the form of a conditional expected count 1 Introduction This paper focuses on an exact implementation of the linearised form of lattice minimum Bayesrisk (LMBR) decoding using general purpose weighted finite state transducer (WFST) operations1 . The LMBR decision rule in Tromble et al. (2008) has the form   X ′ ′ ˆ E = argmax θ0 |E |+ θu #u (E )p(u|E) E ′ ∈E X u∈N c(u|E) = (1) where"
P10-2006,P02-1001,0,0.0236584,"hose with an explicit transition. See the OpenFst documentation: http://openfst.org We make use of a trick to count higher-order ngrams. We build transducer Φn to map word se28 ρ:ǫ 0 ǫ:ǫ 1 u1 :u1 ǫ:ǫ ρ:ǫ u2 :u2 More than one final state may gather probabilities for the same u; to compute p(u|E) these probabilities are added. The forward algorithm requires that En ◦ΨR n be topologically sorted; although sorting can be slow, it is still quicker than log semiring ǫ-removal and determinization. The statistics gathered by the forward algorithm could also be gathered under the expectation semiring (Eisner, 2002) with suitably defined features. We take the view that the full complexity of that approach is not needed here, since only one symbol is introduced per path and per exit state. L Unlike En ◦ ΨR n , the composition En ◦ Ψn does not segregate paths by u such that there is a direct association between final states and symbols. The forward algorithm does not readily yield the per-symbol probabilities, although an arc weight vector indexed by symbols could be used to correctly aggregate the required statistics (Riley et al., 2009). For large Nn this would be memory intensive. The association betwee"
P10-2006,N09-1049,1,0.840208,"Missing"
P10-2006,N04-1022,1,0.88609,"tistical Machine Translation Lattices Graeme Blackwood, Adri`a de Gispert, William Byrne Machine Intelligence Laboratory Cambridge University Engineering Department Trumpington Street, CB2 1PZ, U.K. {gwb24|ad465|wjb31}@cam.ac.uk Abstract once. It is the efficient computation of these path posterior n-gram probabilities that is the primary focus of this paper. We will show how general purpose WFST algorithms can be employed to efficiently compute p(u|E) for all u ∈ N . Tromble et al. (2008) use Equation (1) as an approximation to the general form of statistical machine translation MBR decoder (Kumar and Byrne, 2004): This paper presents an efficient implementation of linearised lattice minimum Bayes-risk decoding using weighted finite state transducers. We introduce transducers to efficiently count lattice paths containing n-grams and use these to gather the required statistics. We show that these procedures can be implemented exactly through simple transformations of word sequences to sequences of n-grams. This yields a novel implementation of lattice minimum Bayes-risk decoding which is fast and exact even for very large lattices. ˆ = argmin E E ′ ∈E L(E, E ′ )P (E|F ) (3) E∈E The approximation replace"
P10-2006,P09-1019,0,0.02694,"cases it will be perfectly fine, depending on how closely p(u|E) and c(u|E) agree for higher-order n-grams. Experimentally, Allauzen et al. (2010) find this approximation works well at k = 1 for MBR decoding of statistical machine translation lattices. However, there may be scenarios in which p(u|E) and c(u|E) differ so that Equation (5) is no longer useful in place of the original Tromble et al. (2008) approximation. In the following sections, we present an efficient method for simultaneous calculation of p(u|E) for n-grams of a fixed order. While other fast MBR approximations are possible (Kumar et al., 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. 2 N-gram Mapping Transducer 2 The special composition symbol σ matches any arc; ρ matches any arc other than those with an explicit transition. See the OpenFst documentation: http://openfst.org We make use of a trick to count higher-order ngrams. We build transducer Φn to map word se28 ρ:ǫ 0 ǫ:ǫ 1 u1 :u1 ǫ:ǫ ρ:ǫ u2 :u2 More than one final state may gather probabilities for the same u; to compute p(u|E) these probabilities are added."
P10-2006,N09-4005,0,\N,Missing
P10-2006,P03-1006,0,\N,Missing
P18-2051,P17-2021,0,0.194483,"focus on syntax in ensembles containing multiple sentence representations. We formulate beam search over such ensembles using WFSTs, and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task. 1 Introduction Ensembles of multiple NMT models consistently and significantly improve over single models (Garmash and Monz, 2016). Previous work has observed that NMT models trained to generate target syntax can exhibit improved sentence structure (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) relative to those trained on plain-text, while plain-text models produce shorter sequences and so may encode lexical information more easily (Nadejde et al., 2017). We hypothesize that an NMT ensemble would be strengthened if its component models were complementary in this way. However, ensembling often requires component models to make predictions relating to the same output sequence position at each time step. Models producing different sentence representations are necessarily synchronized to enable this. We propose an approach to decoding ensembles of models generat"
P18-2051,N16-1024,0,0.0329883,"erform NMT with syntax annotation in the form of Combinatory Categorial Grammar (CCG) supertags. Aharoni and Goldberg (2017) translate from source BPE into target linearized parse trees, 319 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 319–325 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics but omit POS tags to reduce sequence length. They demonstrate improved target language reordering when producing syntax. Eriguchi et al. (2017) combine recurrent neural network grammar (RNNG) models (Dyer et al., 2016) with attention-based models to produce well-formed dependency trees. Wu et al. (2017) similarly produce both words and arcstandard algorithm actions (Nivre, 2004). Previous approaches to ensembling diverse models focus on model inputs. Hokamp (2017) shows improvements in the quality estimation task using ensembles of NMT models with multiple input representations which share an output representation. Garmash and Monz (2016) show translation improvements with multi-source-language NMT ensembles. 2 Ensembles of Syntax Models We wish to ensemble using models which generate linearized constituenc"
P18-2051,P17-2012,0,0.26068,"containing multiple sentence representations. We formulate beam search over such ensembles using WFSTs, and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task. 1 Introduction Ensembles of multiple NMT models consistently and significantly improve over single models (Garmash and Monz, 2016). Previous work has observed that NMT models trained to generate target syntax can exhibit improved sentence structure (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) relative to those trained on plain-text, while plain-text models produce shorter sequences and so may encode lexical information more easily (Nadejde et al., 2017). We hypothesize that an NMT ensemble would be strengthened if its component models were complementary in this way. However, ensembling often requires component models to make predictions relating to the same output sequence position at each time step. Models producing different sentence representations are necessarily synchronized to enable this. We propose an approach to decoding ensembles of models generating different representa"
P18-2051,C16-1133,0,0.0721567,"‡ SDL Research, Cambridge, UK Abstract We explore strategies for incorporating target syntax into Neural Machine Translation. We specifically focus on syntax in ensembles containing multiple sentence representations. We formulate beam search over such ensembles using WFSTs, and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task. 1 Introduction Ensembles of multiple NMT models consistently and significantly improve over single models (Garmash and Monz, 2016). Previous work has observed that NMT models trained to generate target syntax can exhibit improved sentence structure (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) relative to those trained on plain-text, while plain-text models produce shorter sequences and so may encode lexical information more easily (Nadejde et al., 2017). We hypothesize that an NMT ensemble would be strengthened if its component models were complementary in this way. However, ensembling often requires component models to make predictions relating to the same output sequence position at each time step. Models produc"
P18-2051,W17-4775,0,0.0250167,"mputational Linguistics (Short Papers), pages 319–325 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics but omit POS tags to reduce sequence length. They demonstrate improved target language reordering when producing syntax. Eriguchi et al. (2017) combine recurrent neural network grammar (RNNG) models (Dyer et al., 2016) with attention-based models to produce well-formed dependency trees. Wu et al. (2017) similarly produce both words and arcstandard algorithm actions (Nivre, 2004). Previous approaches to ensembling diverse models focus on model inputs. Hokamp (2017) shows improvements in the quality estimation task using ensembles of NMT models with multiple input representations which share an output representation. Garmash and Monz (2016) show translation improvements with multi-source-language NMT ensembles. 2 Ensembles of Syntax Models We wish to ensemble using models which generate linearized constituency trees but these representations can be very long and difficult to model. We therefore propose a derivation-based representation which is much more compact than a linearized parse tree (examples in Table 1). Our linearized derivation representation"
P18-2051,P07-2045,0,0.00436258,"ed derivation Linearized derivation 28.8 Linearized tree Plain BPE 28.9 It is also possible to constrain decoding Plain BPE Linearized derivation 28.8 of linearized trees and derivations to wellLinearized derivation Plain BPE 29.4† † formed outputs. However, we found that this POS/BPE Plain BPE 29.3 Plain BPE POS/BPE 29.4† gives little improvement in BLEU over unconstrained decoding although it remains an interTable 5: Ja-En Transformer ensembles: † esting line of research. marks significant improvement on plain BPE baseline shown in Table 4 (p < 0.05 using 4 Conclusions bootstrap resampling (Koehn et al., 2007)). 3 indicates that large batch training is instrumental in this. We find that RNN-based syntax models can equal plain BPE models as in Aharoni and Goldberg (2017); Eriguchi et al. (2017) use syntax for a 1 BLEU improvement on this dataset, but over a much lower baseline. Our plain BPE Transformer outperforms all syntax models except POS/BPE. More compact syntax representations perform better, with POS/BPE outperforming linearized derivations, which outperform linearized trees. Ensembles of two identical models trained with different seeds only slightly improve over the single model (Table 5)."
P18-2051,W17-5706,0,0.014417,"the linear derivation model. It has been suggested that decaying the learning rate can have a similar effect to large batch training (Smith et al., 2017), but reducing the initial learning rate by a factor of 8 alone did not give the same improvements. Representation Plain BPE Linearized derivation Batches / update 1 1 8 1 1 8 Learning rate 0.025 0.2 0.2 0.025 0.2 0.2 Test BLEU 27.5 27.2 28.9 25.6 25.6 28.7 Table 3: Single Transformers trained to convergence on 1M WAT Ja-En, batch size 4096 Our plain BPE baseline (Table 4) outperforms the current best system on WAT Ja-En, an 8-model ensemble (Morishita et al., 2017). Our syntax models achieve similar results despite producing much longer sequences. Table 322 Architecture Representation Dev Test BLEU BLEU 28.4 By ensembling syntax and plain-text we hope to benefit from their complementary Seq2seq Best WAT17 result (8-model (Morishita et al., strengths. To highlight these, we examine hyensemble) 2017) potheses generated by the plain BPE and linPlain BPE 21.6 21.2 Seq2seq earized derivation models. We find that the Linearized derivation 21.9 21.2 Plain BPE 28.0 28.9 syntax model is often more grammatical, even Linearized tree 28.2 28.4 when the plain BPE mo"
P18-2051,W17-4707,0,0.0198566,"especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task. 1 Introduction Ensembles of multiple NMT models consistently and significantly improve over single models (Garmash and Monz, 2016). Previous work has observed that NMT models trained to generate target syntax can exhibit improved sentence structure (Aharoni and Goldberg, 2017; Eriguchi et al., 2017) relative to those trained on plain-text, while plain-text models produce shorter sequences and so may encode lexical information more easily (Nadejde et al., 2017). We hypothesize that an NMT ensemble would be strengthened if its component models were complementary in this way. However, ensembling often requires component models to make predictions relating to the same output sequence position at each time step. Models producing different sentence representations are necessarily synchronized to enable this. We propose an approach to decoding ensembles of models generating different representations, focusing on models generating syntax. As part of our investigation we suggest strategies for practical NMT with very long target sequences. These long sequen"
P18-2051,W17-5708,0,0.0383692,"nal GPU memory. The Tensor2Tensor framework (Vaswani et al., 2018) defines batch size as the number of tokens per batch, so batches will contain fewer sequences if their average length increases. During NMT training, by default, the gradients used to update model parameters are calculated over individual batches. A possible consequence is that batches containing fewer sequences per update may have ‘noisier’ estimated gradients than batches with more sequences. Previous research has used very large batches to improve training convergence while requiring fewer model updates (Smith et al., 2017; Neishi et al., 2017). However, with such large batches the model size may exceed available GPU memory. Training on multiple GPUs is one way to increase the amount of data used to estimate gradients, but it requires significant resources. Our strategy avoids this problem by using delayed SGD updates. We accumulate gradients over a fixed number of batches before using the accumulated gradients to update the model1 . This lets us effectively use very large batch sizes without requiring multiple GPUs. 2.2 Ensembling Representations Table 1 shows several different representations of the same hypothesis. To formulate a"
P18-2051,W04-0308,0,0.0215238,"ized parse trees, 319 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 319–325 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics but omit POS tags to reduce sequence length. They demonstrate improved target language reordering when producing syntax. Eriguchi et al. (2017) combine recurrent neural network grammar (RNNG) models (Dyer et al., 2016) with attention-based models to produce well-formed dependency trees. Wu et al. (2017) similarly produce both words and arcstandard algorithm actions (Nivre, 2004). Previous approaches to ensembling diverse models focus on model inputs. Hokamp (2017) shows improvements in the quality estimation task using ensembles of NMT models with multiple input representations which share an output representation. Garmash and Monz (2016) show translation improvements with multi-source-language NMT ensembles. 2 Ensembles of Syntax Models We wish to ensemble using models which generate linearized constituency trees but these representations can be very long and difficult to model. We therefore propose a derivation-based representation which is much more compact than a"
P18-2051,N15-3009,0,0.0282745,"Missing"
P18-2051,P16-1162,0,0.31982,"Missing"
P18-2051,D17-2005,1,0.518257,"ulated updates every 8 batches. To compare target representations we train Transformer models with target representations (1), (2), (4) and (5) shown in Table 1, using delayed SGD updates every 8 batches. We decode with individual models and two-model ensembles, comparing results for single-representation and multi-representation ensembles. Each multirepresentation ensemble consists of the plain BPE model and one other individual model. All Transformer architectures are Tensor2Tensor’s base Transformer model (Vaswani et al., 2018) with a batch size of 4096. In all cases we decode using SGNMT (Stahlberg et al., 2017) with beam size 4, using the average of the final 20 checkpoints. For comparison with earlier target syntax work, we also train two RNN attention-based seq2seq models (Bahdanau et al., 2015) with normal SGD to produce plain BPE sequences and linearized derivations. For these models we use embedding size 400, a single BiLSTM layer of size 750, and batch size 80. We report all experiments for JapaneseEnglish, using the first 1M training sentences of the Japanese-English ASPEC data (Nakazawa et al., 2016). All models use plain BPE Japanese source sentences. English constituency trees are obtained"
P18-2051,W18-1819,0,0.25904,"ntation ((4) in Table 1) consists of the derivation’s right-hand side tokens with an end-of-rule marker, </R> , marking the last non-terminal in each rule. The original tree can be directly reproduced from the sequence, so that structure information is maintained. We map words to subwords as described in Section 3. 2.1 Delayed SGD Update Training for Long Sequences We suggest a training strategy for the Transformer model (Vaswani et al., 2017) which gives improved performance for long sequences, like syntax representations, without requiring additional GPU memory. The Tensor2Tensor framework (Vaswani et al., 2018) defines batch size as the number of tokens per batch, so batches will contain fewer sequences if their average length increases. During NMT training, by default, the gradients used to update model parameters are calculated over individual batches. A possible consequence is that batches containing fewer sequences per update may have ‘noisier’ estimated gradients than batches with more sequences. Previous research has used very large batches to improve training convergence while requiring fewer model updates (Smith et al., 2017; Neishi et al., 2017). However, with such large batches the model s"
P18-2051,P17-1065,0,0.0188723,"pertags. Aharoni and Goldberg (2017) translate from source BPE into target linearized parse trees, 319 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 319–325 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics but omit POS tags to reduce sequence length. They demonstrate improved target language reordering when producing syntax. Eriguchi et al. (2017) combine recurrent neural network grammar (RNNG) models (Dyer et al., 2016) with attention-based models to produce well-formed dependency trees. Wu et al. (2017) similarly produce both words and arcstandard algorithm actions (Nivre, 2004). Previous approaches to ensembling diverse models focus on model inputs. Hokamp (2017) shows improvements in the quality estimation task using ensembles of NMT models with multiple input representations which share an output representation. Garmash and Monz (2016) show translation improvements with multi-source-language NMT ensembles. 2 Ensembles of Syntax Models We wish to ensemble using models which generate linearized constituency trees but these representations can be very long and difficult to model. We therefor"
P19-1022,D17-1156,0,0.129197,"s t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During inference, Garmash and Monz (2016) use a gating network to learn weights for a multisource NMT ensemble. Freitag and Al-Onaizan (2016) use uniform ensembles of general and noreg fine-tuned models. Figure 1: Adaptively adjusting ensemble model weights Wk,i (Eq. 6) during decoding with BI We propose a simpler approach based on the source language n-gram language models from Eq. 9. We assume that each Gt is also a language m"
P19-1022,L16-1470,0,0.0841703,"Missing"
P19-1022,W18-6319,0,0.024261,"training sentences for minimum three tokens and maximum 120 tokens, and remove sentence pairs with length ratios higher than 4.5:1 or lower than 1:4.5. Table 2 shows filtered training sentence counts. Each language pair uses a 32K-merge source-target BPE vocabulary trained on the general domain (Sennrich et al., 2016b). We implement in Tensor2Tensor (Vaswani et al., 2018) and use its base Transformer model (Vaswani et al., 2017) for all NMT models. At inference time we decode with beam size 4 in SGNMT (Stahlberg et al., 2017) and evaluate with case-sensitive detokenized BLEU using SacreBLEU (Post, 2018). For BI, we use 4-gram KENLM models (Heafield, 2011). λk,t can be interpreted as the probability that task t contains sentences x drawn from domain k as estimated over the Vt . Figure 1 demonstrates this adaptive decoding scheme when weighting a biomedical and a general (news) domain model to produce a biomedical sentence under BI. The model weights Wk,i are even until biomedical-specific vocabulary is produced, at which point the in-domain model dominates. Decoder Uniform IS Identity-BI BI BI+IS Related Work δk (t) Eq. 10 Eq. 10 Table 1: Setting task posterior p(t|x) and domain-task weight λ"
P19-1022,E17-2045,0,0.132289,"for static decoding with an informative source. 1.2.2 T X = PT (9) p(t, yi |hi , x) Wk,i = p(x|t0 )p(t0 ) t=1 t=1 K X p(x|t)p(t) t0 =1 (4) At step i, where hi is history y1:i−1 : T X (8) and decoding proceeds according to Eq. 2. We refer to this as decoding with an informative source (IS). We propose using Gt , an collection of n-gram language models trained on source language sentences from tasks t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During inference, Garmash and Mon"
P19-1022,P16-1009,0,0.17909,"age models to estimate p(t = k|x) in Eq. 8 for static decoding with an informative source. 1.2.2 T X = PT (9) p(t, yi |hi , x) Wk,i = p(x|t0 )p(t0 ) t=1 t=1 K X p(x|t)p(t) t0 =1 (4) At step i, where hi is history y1:i−1 : T X (8) and decoding proceeds according to Eq. 2. We refer to this as decoding with an informative source (IS). We propose using Gt , an collection of n-gram language models trained on source language sentences from tasks t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain ada"
P19-1022,C16-1133,0,0.0161946,"d et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During inference, Garmash and Monz (2016) use a gating network to learn weights for a multisource NMT ensemble. Freitag and Al-Onaizan (2016) use uniform ensembles of general and noreg fine-tuned models. Figure 1: Adaptively adjusting ensemble model weights Wk,i (Eq. 6) during decoding with BI We propose a simpler approach based on the source language n-gram language models from Eq. 9. We assume that each Gt is also a language model P for its corresponding domain k. With Gk,t = x∈Vt Gk (x), we take: 2 Gk,t k0 Gk0 ,t λk,t = P (10) 1.2.3 Summary We summarize our approaches to decoding in Table 1. Static Adaptive p(t|x) λk,t 1 T 1 T δk"
P19-1022,P16-1162,0,0.538171,"age models to estimate p(t = k|x) in Eq. 8 for static decoding with an informative source. 1.2.2 T X = PT (9) p(t, yi |hi , x) Wk,i = p(x|t0 )p(t0 ) t=1 t=1 K X p(x|t)p(t) t0 =1 (4) At step i, where hi is history y1:i−1 : T X (8) and decoding proceeds according to Eq. 2. We refer to this as decoding with an informative source (IS). We propose using Gt , an collection of n-gram language models trained on source language sentences from tasks t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain ada"
P19-1022,W11-2123,0,0.0649947,"maximum 120 tokens, and remove sentence pairs with length ratios higher than 4.5:1 or lower than 1:4.5. Table 2 shows filtered training sentence counts. Each language pair uses a 32K-merge source-target BPE vocabulary trained on the general domain (Sennrich et al., 2016b). We implement in Tensor2Tensor (Vaswani et al., 2018) and use its base Transformer model (Vaswani et al., 2017) for all NMT models. At inference time we decode with beam size 4 in SGNMT (Stahlberg et al., 2017) and evaluate with case-sensitive detokenized BLEU using SacreBLEU (Post, 2018). For BI, we use 4-gram KENLM models (Heafield, 2011). λk,t can be interpreted as the probability that task t contains sentences x drawn from domain k as estimated over the Vt . Figure 1 demonstrates this adaptive decoding scheme when weighting a biomedical and a general (news) domain model to produce a biomedical sentence under BI. The model weights Wk,i are even until biomedical-specific vocabulary is produced, at which point the in-domain model dominates. Decoder Uniform IS Identity-BI BI BI+IS Related Work δk (t) Eq. 10 Eq. 10 Table 1: Setting task posterior p(t|x) and domain-task weight λk,t for T tasks under decoding schemes in this work."
P19-1022,D17-2005,1,0.859979,"al., 2016), and then sequentially to the APE 2017 IT task (Turchi et al., 2017). We filter training sentences for minimum three tokens and maximum 120 tokens, and remove sentence pairs with length ratios higher than 4.5:1 or lower than 1:4.5. Table 2 shows filtered training sentence counts. Each language pair uses a 32K-merge source-target BPE vocabulary trained on the general domain (Sennrich et al., 2016b). We implement in Tensor2Tensor (Vaswani et al., 2018) and use its base Transformer model (Vaswani et al., 2017) for all NMT models. At inference time we decode with beam size 4 in SGNMT (Stahlberg et al., 2017) and evaluate with case-sensitive detokenized BLEU using SacreBLEU (Post, 2018). For BI, we use 4-gram KENLM models (Heafield, 2011). λk,t can be interpreted as the probability that task t contains sentences x drawn from domain k as estimated over the Vt . Figure 1 demonstrates this adaptive decoding scheme when weighting a biomedical and a general (news) domain model to produce a biomedical sentence under BI. The model weights Wk,i are even until biomedical-specific vocabulary is produced, at which point the in-domain model dominates. Decoder Uniform IS Identity-BI BI BI+IS Related Work δk (t"
P19-1022,W18-2705,0,0.15007,"Wk,i = p(x|t0 )p(t0 ) t=1 t=1 K X p(x|t)p(t) t0 =1 (4) At step i, where hi is history y1:i−1 : T X (8) and decoding proceeds according to Eq. 2. We refer to this as decoding with an informative source (IS). We propose using Gt , an collection of n-gram language models trained on source language sentences from tasks t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During inference, Garmash and Monz (2016) use a gating network to learn weights for a multisource NMT ensemble. Freitag"
P19-1022,N19-1209,0,0.257037,"tation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During inference, Garmash and Monz (2016) use a gating network to learn weights for a multisource NMT ensemble. Freitag and Al-Onaizan (2016) use uniform ensembles of general and noreg fine-tuned models. Figure 1: Adaptively adjusting ensemble model weights Wk,i (Eq. 6) during decoding with BI We propose a simpler approach based on the source language n-gram language models from Eq. 9. We assume that each Gt is also a language model P for its corresponding domain k. With Gk,t = x∈Vt Gk (x), we take: 2 Gk,t k0 Gk0 ,t λk,t = P (10) 1."
P19-1022,W17-3204,0,0.0269885,"ing a large generaldomain corpus (Luong and Manning, 2015). The ∗ are fine-tuned on task optimized parameters θA B, a new domain. Without regularization, catastrophic forgetting can occur: performance on task A degrades as parameters adjust to the new objective. A regularized objective is: X ∗ L(θ) = LB (θ) + Λ Fj (θj − θA,j )2 (1) Introduction Neural Machine Translation (NMT) models are effective when trained on broad domains with large datasets, such as news translation (Bojar et al., 2017). However, test data may be drawn from a different domain, on which general models can perform poorly (Koehn and Knowles, 2017). We address the problem of adapting to one or more domains while maintaining good performance across all domains. Crucially, we assume the realistic scenario where the domain is unknown at inference time. One solution is ensembling models trained on different domains (Freitag and Al-Onaizan, 2016). This approach has two main drawbacks. Firstly, obtaining models for each domain is challenging. Training from scratch on each new domain is impractical, while continuing training on a new domain can cause catastrophic forgetting of previous tasks (French, 1999), even in an ensemble (Freitag and Al-"
P19-1022,N18-2080,0,0.0223401,"t=1 K X p(x|t)p(t) t0 =1 (4) At step i, where hi is history y1:i−1 : T X (8) and decoding proceeds according to Eq. 2. We refer to this as decoding with an informative source (IS). We propose using Gt , an collection of n-gram language models trained on source language sentences from tasks t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During inference, Garmash and Monz (2016) use a gating network to learn weights for a multisource NMT ensemble. Freitag and Al-Onaizan"
P19-1022,P17-2089,0,0.0311959,"(t = k|x) in Eq. 8 for static decoding with an informative source. 1.2.2 T X = PT (9) p(t, yi |hi , x) Wk,i = p(x|t0 )p(t0 ) t=1 t=1 K X p(x|t)p(t) t0 =1 (4) At step i, where hi is history y1:i−1 : T X (8) and decoding proceeds according to Eq. 2. We refer to this as decoding with an informative source (IS). We propose using Gt , an collection of n-gram language models trained on source language sentences from tasks t, to estimate p(t|x): k=1 T X Static decoder configurations (7) 223 1.3 Approaches to NMT domain adaptation include training data selection or generation (Sennrich et al., 2016a; Wang et al., 2017; Sajjad et al., 2017) and fine-tuning output distributions (Dakwale and Monz, 2017; Khayrallah et al., 2018). Vilar (2018) regularizes parameters with an importance network, while Thompson et al. (2018) freeze subsets of the model parameters before finetuning. Both observe forgetting with the adapted model on the general domain data in the realistic scenario where the test data domain is unknown. Barone et al. (2017) fine-tune with L2 regularization to reduce forgetting. Concurrently with our work, Thompson et al. (2019) apply EWC to reduce forgetting during NMT domain adaptation. During infe"
W03-1504,W02-2004,1,0.882393,"Missing"
W03-1504,E03-1038,1,0.880696,"Missing"
W03-1504,W99-0613,0,0.348162,"for several languages. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre– existing linguistic resources and/or limited funding possibilities. This is one of the main causes for the recent growing interest on developing language– independent NERC systems, which may be trained from small training sets by taking advantage of unlabelled examples (Collins and Singer, 1999; Abney, 2002), and which are easy to adapt to changing domains (being all these aspects closely related). This work focuses on exploring the construction of a low–cost Named Entity classification (NEC) module for Catalan without making use of large/expensive resources of the language. In doing so, the paper first explores the training of classification models by using only Catalan resources and then proposes a training scheme, in which a Catalan/Spanish bilingual classifier is trained directly from a training set including examples of the two languages. In both cases, the bootstrapping of the"
W03-1504,W03-0419,0,0.0206414,"Missing"
W03-1504,W02-2024,0,0.0121427,"ch may improve the performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifying those units in a text has kept on growing during the last years. Previous work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC competition task. More recent approaches can be found in the proceedings of the shared task at the 2002 and 2003 editions of the Conference on Natural Language Learning (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), where several machine–learning (ML) systems were compared at the NERC task for several languages. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre– existing linguistic resources and/or limited funding possibilities. This is one of the main causes for the recent growing interest on developing language– in"
W03-1504,P02-1046,0,\N,Missing
W05-0823,N04-1033,0,0.265878,"Missing"
W05-0823,2004.iwslt-evaluation.14,1,0.848754,"Missing"
W05-0823,N03-1017,0,0.0598703,"Missing"
W05-0823,P00-1056,0,0.0836903,"e pairs with a word ratio larger than 2.4. As a result of this preprocessing, the number of sentences in each training set was slightly reduced. However, no significant reduction was produced. In the case of French, a re-tokenizing procedure was performed in which all apostrophes appearing alone were attached to their corresponding words. For example, pairs of tokens such as l ’ and qu ’ were reduced to single tokens such as l’ and qu’. 134 Once the training data was preprocessed, a wordto-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). As an approximation to the most probable alignment, the Viterbi alignment was considered. Then, the intersection and union of alignment sets in both directions were computed for each training set. 3.2 Feature Function Computation The considered translation system implements a total of five feature functions. The first of these models is the tuple 3-gram model, which was already described in section 2. Tuples for the translation model were extracted from the union set of alignments as shown in Figure 1. Once tuples had been extracted, the tuple vocabulary was pruned by using histogram pruning"
W05-0823,P02-1038,0,0.0900962,"were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model. 2 Bilingual N-gram Translation Model 1 Introduction During the last decade, statistical machine translation (SMT) systems have evolved from the original word-based approach (Brown et al., 1993) into phrase-based translation systems (Koehn et al., 2003). Similarly, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple models is implemented (Och and Ney, 2002). The SMT approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams. This translation model was developed by de Gispert and Mari˜no (2002), and it differs from the well known phrase-based translation model in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This model is described in section 2. Translation results from the four source languages made available for the shared task ("
W05-0823,P02-1040,0,0.0765659,"ts a beam-search strategy based on dynamic programming and takes into account all the five feature functions described above simultaneously. It also allows for three different pruning methods: threshold pruning, histogram pruning, and hypothesis recombination. For all the results presented in this work the decoder’s monotonic search modality was used. An optimization tool, which is based on a simplex method (Press et al., 2002), was developed and used for computing log-linear weights for each of the feature functions described above. This algorithm adjusts the log-linear weights so that BLEU (Papineni et al., 2002) is maximized over a given development set. One optimization for each language pair was performed by using the 2000-sentence development sets made available for the shared task. 4 Shared Task Results Table 2 presents the BLEU scores obtained for the shared task test data. Each test set consisted of 2000 sentences. The computed BLEU scores were case insensitive and used one translation reference. 135 sufficient and that , in the future , it is necessary to develop the Union better and a different structure... It is evident from these translation outputs that translation quality decreases when m"
W05-0823,J93-2003,0,\N,Missing
W06-3101,P04-1079,0,0.0149782,"sibilities for improvements. 1 Patrik Lambert† Rafael Banchs† 2 Introduction The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al."
W06-3101,W05-0909,0,0.0366343,"lation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nie"
W06-3101,N04-4015,0,0.00900173,"has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic information in combination with the automatic evaluation measures WER and PER in order to get more details about the tran"
W06-3101,2005.iwslt-1.19,1,0.729357,"s. 1 Patrik Lambert† Rafael Banchs† 2 Introduction The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed"
W06-3101,C00-2162,1,0.81769,"Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of"
W06-3101,2001.mtsummit-papers.45,1,0.834325,"05). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic infor"
W06-3101,W01-1407,1,0.840883,"05). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic infor"
W06-3101,niessen-etal-2000-evaluation,1,0.397729,"nt issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, c New York City, June 2006. 2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our kno"
W06-3101,P02-1040,0,0.115841,"t`ecnica de Catalunya (UPC), Barcelona, Spain ⊥ ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy {popovic,ney}@informatik.rwth-aachen.de {gupta,federico}@itc.it {agispert,canton}@gps.tsc.upc.es {lambert,banchs}@gps.tsc.upc.es Abstract A variety of automatic evaluation measures have been proposed and studied over the last years, some of them are shown to be a very useful tool for comparing different systems as well as for evaluating improvements within one system. The most widely used are Word Error Rate (WER), Position Independent Word Error Rate (PER), the BLEU score (Papineni et al., 2002) and the NIST score (Doddington, 2002). However, none of these measures give any details about the nature of translation errors. A relationship between these error measures and the actual errors in the translation outputs is not easy to find. Therefore some analysis of the translation errors is necessary in order to define the main problems and to focus the research efforts. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), but like human evaluation, this is also a time consuming task. The goal of this work is to present a framework for au"
W06-3101,popovic-ney-2004-towards,1,0.340303,"Missing"
W06-3101,popovic-ney-2006-pos,1,0.738548,"Missing"
W06-3101,2005.mtsummit-papers.34,1,0.721067,"173 0.15 0.09 2.7 1.7 840 1094 22774 26917 4081 3958 0.14 0.25 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus). The statistics of the corpora can be seen in Table 1. The statistical machine translation system used in this work is based on a log-linear combination of seven different models. The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used. A more detailed description of the system can be found in (Vilar et al., 2005; Zens et al., 2005). 4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus. In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovi´c and Ney, 2006) is also analysed. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round. Therefore, local reorderings of nouns and ad3 WER 34.5 33.5 41.8 38.9 PER 25.5 25.2 30.7 29.5 BLEU 54.7 56.4 43.2 48.5 English→Spanish full base"
W06-3101,vilar-etal-2006-error,1,0.542481,"Missing"
W06-3101,2005.iwslt-1.20,1,0.479674,"7 840 1094 22774 26917 4081 3958 0.14 0.25 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus). The statistics of the corpora can be seen in Table 1. The statistical machine translation system used in this work is based on a log-linear combination of seven different models. The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used. A more detailed description of the system can be found in (Vilar et al., 2005; Zens et al., 2005). 4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus. In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovi´c and Ney, 2006) is also analysed. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round. Therefore, local reorderings of nouns and ad3 WER 34.5 33.5 41.8 38.9 PER 25.5 25.2 30.7 29.5 BLEU 54.7 56.4 43.2 48.5 English→Spanish full baseline reorder 13k bas"
W06-3101,H05-1085,0,\N,Missing
W06-3120,A00-1031,0,0.0229195,"ese figures in the optimization function. 3 Shared Task Results 3.1 Data The data provided for this shared task corresponds to a subset of the official transcriptions of the European Parliament Plenary Sessions, and it is available through the shared task website at: http://www.statmt.org/wmt06/shared-task/. The development set used to tune the system consists of a subset (500 first sentences) of the official development set made available for the Shared Task. We carried out a morphological analysis of the data. The English POS-tagging has been carried out using freely available T N T tagger (Brants, 2000). In the Spanish case, we have used the F reeling (Carreras et al., 2004) analysis tool which generates the POS-tagging for each input word. 3.2 Systems configurations The baseline system is the same for all tasks and includes the following features functions: cp, pp, lm, ibm1, ibm1−1 , wb, pb. The POStag target language model has been used in those tasks for which the tagger was available. Table 1 shows the reordering configuration used for each task. The Block Reordering (application 2) has been used when the source language belongs to the Romanic family. The length of the block is limited t"
W06-3120,W05-0827,1,0.879283,"Missing"
W06-3120,2005.iwslt-1.23,1,0.907982,"Missing"
W06-3120,W06-3125,1,0.884485,"Missing"
W06-3120,P05-2012,1,0.889176,"Missing"
W06-3120,W05-0831,0,0.0297232,"Full verb forms The morphology of the verbs usually differs in each language. Therefore, it is interesting to classify the verbs in order to address the rich variety of verbal forms. Each verb is reduced into its base form and reduced POS tag as explained in (de Gispert, 2005). This transformation is only done for the alignment, and its goal is to simplify the work of the word alignment improving its quality. Block reordering (br) The difference in word order between two languages is one of the most significant sources of error in SMT. Related works either deal with reordering in general as (Kanthak et al., 2005) or deal with local reordering as (Tillmann and Ney, 2003). We report a local reordering technique, which is implemented as a preprocessing stage, with two applications: (1) to improve only alignment quality, and (2) to improve alignment quality and to infer reordering in translation. Here, we present a short explanation of the algorithm, for further details see Costa-juss`a and Fonollosa (2006). 142 Proceedings of the Workshop on Statistical Machine Translation, pages 142–145, c New York City, June 2006. 2006 Association for Computational Linguistics of the bilingual phrase, and no word on ei"
W06-3120,W06-3114,0,0.0221223,"ts to observe its efficiency in all the pairs used in this evaluation. The rgraph has been applied in those cases where: we do not use br2 (there is no sense in applying them simultaneously); and we have the tagger for the source language model available. In the case of the pair GeEn, we have not experimented any reordering, we left the application of both reordering approaches as future work. 3.3 Discussion Table 2 presents the BLEU scores evaluated on the test set (using TRUECASE) for each configuration. The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006). For both, Es2En and Fr2En tasks, br helps slightly. The improvement of the approach depends on the quality of the alignment. The better alignments allow to extract higher quality Alignment Blocks (Costa-juss`a and Fonollosa, 2006). The En2Es task is improved when adding both br1 and rgraph. Similarly, the En2Fr task seems to perform fairly well when using the rgraph. In this case, the improvement of the approach depends on the quality of the alignment patterns (Crego et al., 2006). However, it has the advantage of delaying the final decision of reordering to the overall search, where all mod"
W06-3120,N03-1017,0,0.00728769,"o infer reordering in translation. Here, we present a short explanation of the algorithm, for further details see Costa-juss`a and Fonollosa (2006). 142 Proceedings of the Workshop on Statistical Machine Translation, pages 142–145, c New York City, June 2006. 2006 Association for Computational Linguistics of the bilingual phrase, and no word on either side of the phrase is aligned to a word out of the phrase. We limit the maximum size of any given phrase to 7. The huge increase in computational and storage cost of including longer phrases does not provide a significant improvement in quality (Koehn et al., 2003) as the probability of reappearance of larger phrases decreases. 2.3 Figure 1: Example of an Alignment Block, i.e. a pair of consecutive blocks whose target translation is swapped This reordering strategy is intended to infer the most probable reordering for sequences of words, which are referred to as blocks, in order to monotonize current data alignments and generalize reordering for unseen pairs of blocks. Given a word alignment, we identify those pairs of consecutive source blocks whose translation is swapped, i.e. those blocks which, if swapped, generate a correct monotone translation. Fi"
W06-3120,J04-4002,0,0.0268059,"created). Based on this information, the source side of the bilingual corpora are reordered. In case of applying the reordering technique for purpose (1), we modify only the source training corpora to realign and then we recover the original order of the training corpora. In case of using Block Reordering for purpose (2), we modify all the source corpora (both training and test), and we use the new training corpora to realign and build the final translation system. 2.2 Phrase Extraction Given a sentence pair and a corresponding word alignment, phrases are extracted following the criterion in Och and Ney (2004). A phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: words are consecutive along both sides 143 Feature functions Conditional and posterior probability (cp, pp) Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions. The target language model (lm) consists of an n-gram model, in which the probability of a translation hypothesis is approximated by the product of word n-gram probabilities. As default language model feature, we use a standard word-base"
W06-3120,carreras-etal-2004-freeling,0,\N,Missing
W06-3120,J03-1005,0,\N,Missing
W06-3120,N04-1033,0,\N,Missing
W06-3125,W05-0823,1,0.872672,"Missing"
W06-3125,A00-1031,0,0.117706,"d over the development set for each of the six translation directions considered. 163 This baseline system is actually very similar to the system used for last year’s shared task “Exploiting Parallel Texts for Statistical Machine Translation” of ACL’05 Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond (Banchs et al., 2005), whose results are available at: http://www.statmt.org/wpt05/ mt-shared-task/. A more detailed description of the system can be found in (2005). The tools used for POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English. All language models were estimated using the SRI language modeling toolkit. Word-to-word alignments were extracted with GIZA++. Improvements in word-toword alignments were achieved through verb group classification as described in (de Gispert, 2005). 3 Reordering Framework In this section we outline the reordering framework used for the experiments (Crego and Mari˜no, 2006). A highly constrained reordered search is performed by means of a set of reordering patterns (linguistically motivated rewrite patterns) which are used to extend the monotone search graph with additional arcs."
W06-3125,carreras-etal-2004-freeling,0,0.0548444,"Missing"
W06-3125,W06-3120,1,0.883993,"Missing"
W06-3125,N04-1033,0,0.0842803,"Missing"
W06-3125,P05-2012,1,0.901179,"Missing"
W06-3125,N03-1017,0,0.00542142,"luation with a tagged target language model (using Part-Of-Speech tags). For both Spanish-English translation directions and the English-to-French translation task, the baseline system allows for linguistically motivated sourceside reorderings. 2 Baseline N-gram-based SMT System 1 Introduction The statistical machine translation approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams (de Gispert and Mari˜no, 2002). This translation model differs from the well known phrase-based translation approach (Koehn et al., 2003) in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This translation approach is described in detail in (Mari˜no et al., 2005). For those translation tasks with Spanish or English as target language, an additional tagged (usAs already mentioned, the translation model used here is based on bilingual n-grams. It actually constitutes a language model of bilingual units, referred to as tuples, which approximates the joint probability between source and target languages by us"
W06-3125,2005.mtsummit-papers.36,1,0.909254,"Missing"
W06-3125,J93-2003,0,\N,Missing
W08-0316,P03-1006,0,0.0144341,"whether each phrase-pair occurred once, twice, or more than twice 1 Available at http://snowball.tartarus.org in the parallel text (Bender et al., 2007). All decoding and minimum error training operations are performed with WFSTs and implemented using the OpenFST libraries (Allauzen et al., 2007). 3.2 English Language Models Separate language models are used when translating the Europarl and News sets. The models are estimated using SRILM (Stolcke, 2002) and converted to WFSTs for use in TTM translation. We use the offline approximation in which failure transitions are replaced with epsilons (Allauzen et al., 2003). The Europarl language model is a KneserNey (Kneser and Ney, 1995) smoothed defaultcutoff 5-gram back-off language model estimated over the concatenation of the Europarl and News language model training data. The News language model is created by optimising the interpolation weights of two component models with respect to the News Commentary development sets since we believe these more closely match the newstest2008 domain. The optimised interpolation weights were 0.44 for the Europarl corpus and 0.56 for the much smaller News Commentary corpus. For our contrast submission, we rescore the fir"
W08-0316,D07-1090,0,0.0220365,"oothed defaultcutoff 5-gram back-off language model estimated over the concatenation of the Europarl and News language model training data. The News language model is created by optimising the interpolation weights of two component models with respect to the News Commentary development sets since we believe these more closely match the newstest2008 domain. The optimised interpolation weights were 0.44 for the Europarl corpus and 0.56 for the much smaller News Commentary corpus. For our contrast submission, we rescore the first-pass translation lattices with a large zero-cutoff stupid-backoff (Brants et al., 2007) language model estimated over approximately five billion words of newswire text. 4 Results and Discussion Table 2 reports lower-cased BLEU scores for the French→English and Spanish→English Europarl and News translation tasks. The NIST scores are also provided in parentheses. The row labelled “TTM+MET” shows results obtained after TTM translation and minimum error training, i.e. our primary submission constrained to use only the data distributed for the task. The row labelled “+5gram” shows translation results obtained after rescoring with the large zero-cutoff 5-gram language model described"
W08-0316,N03-1017,0,0.0267675,"MJ1 reordering model for a pair of phrases x1 and x2 is implemented as a WFST. x:x 1−p / b=0 0 1.30M vocab 124k 106k 140k 106k Table 1: Parallel corpora statistics. All of the training and system tuning was performed using lower-cased data. Word alignments were generated using GIZA++ (Och and Ney, 2003) over a stemmed version of the parallel text. Stems for each language were obtained using the Snowball stemmer1 . After unioning the Viterbi alignments, the stems were replaced with their original words, and phrase-pairs of up to five foreign words in length were extracted in the usual fashion (Koehn et al., 2003). 3.1 x1 : x2 1.33M words 39.9M 36.4M 38.2M 35.7M System Tuning Minimum error training (Och, 2003) under BLEU (Papineni et al., 2001) was used to optimise the feature weights of the decoder with respect to the dev2006 development set. The following features are optimized: p / b=+1 1 1 / b=−1 x2 : x1 Figure 1: The uniform MJ1 reordering transducer. • Language model scale factor 3 System Development • Word and phrase insertion penalties CUED participated in two of the WMT shared task tracks: French→English and Spanish→English. For both tracks, primary and contrast systems were submitted. The pri"
W08-0316,H05-1021,1,0.838438,"em for SpanishEnglish and French-English translation in the ACL 2008 Third Workshop on Statistical Machine Translation Shared Task. The CUED system follows a generative model of translation and is implemented by composition of component models realised as Weighted Finite State Transducers, without the use of a special-purpose decoder. Details of system tuning for both Europarl and News translation tasks are provided. 2 The Transducer Translation Model 1 Introduction The Cambridge University Engineering Department statistical machine translation system follows the Transducer Translation Model (Kumar and Byrne, 2005; Kumar et al., 2006), a phrase-based generative model of translation that applies a series of transformations specified by conditional probability distributions and encoded as Weighted Finite State Transducers (Mohri et al., 2002). The main advantages of this approach are its modularity, which facilitates the development and evaluation of each component individually, and its implementation simplicity which allows us to focus on modeling issues rather than complex decoding and search algorithms. In addition, no special-purpose decoder is required since standard WFST operations can be used to o"
W08-0316,J03-1002,0,0.00340136,"djacent phrases can be swapped; this is the MJ1 reordering model of (Kumar and Byrne, 2005). Although the reordering probability for each pair of phrases could be estimated from word-aligned parallel data, we here assume a uniform reordering probability p tuned as described in section 3.1. Figure 1 shows how the MJ1 reordering model for a pair of phrases x1 and x2 is implemented as a WFST. x:x 1−p / b=0 0 1.30M vocab 124k 106k 140k 106k Table 1: Parallel corpora statistics. All of the training and system tuning was performed using lower-cased data. Word alignments were generated using GIZA++ (Och and Ney, 2003) over a stemmed version of the parallel text. Stems for each language were obtained using the Snowball stemmer1 . After unioning the Viterbi alignments, the stems were replaced with their original words, and phrase-pairs of up to five foreign words in length were extracted in the usual fashion (Koehn et al., 2003). 3.1 x1 : x2 1.33M words 39.9M 36.4M 38.2M 35.7M System Tuning Minimum error training (Och, 2003) under BLEU (Papineni et al., 2001) was used to optimise the feature weights of the decoder with respect to the dev2006 development set. The following features are optimized: p / b=+1 1 1"
W08-0316,P03-1021,0,0.0975282,"Missing"
W08-0316,2001.mtsummit-papers.68,0,0.0784944,"Missing"
W08-0316,P02-1040,0,\N,Missing
W10-1722,P10-2006,1,0.832714,"ata used for the first-pass LMs (as summarised in Tables 3, 4). The Spanish secondpass 5-gram LM includes an additional 1.4 billion words of monolingual data from the Spanish GigaWord Second Edition (Mendonca et al., 2009) and Europarl, which were not included in the first-pass LM (see Table 5). 2.6.2 LMBR Decoding Minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) over the full evidence space of the 5-gram rescored lattices was applied to select the translation hypothesis that maximises the conditional expected gain under the linearised sentence-level BLEU score (Tromble et al., 2008; Blackwood and Byrne, 2010). The unigram precision p and average recall ratio r were set as described in Tromble et al. (2008) using the newstest2008 development set. 2.7 Hypothesis Combination Linearised lattice minimum Bayes-risk decoding (Tromble et al., 2008) can also be used as an effective framework for multiple lattice combination (de Gispert et al., 2010). For the French-English language pair, we used LMBR to combine translation lattices produced by systems trained on alternative data sets. Parameter Optimisation Minimum error rate training (MERT) (Och, 2003) under the BLEU score (Papineni et al., 2001) optimise"
W10-1722,D07-1090,0,0.0248633,"eresting aspect is that HiFST is able to build exact search spaces with this model, i.e. there is no pruning in search that may lead to spurious undergeneration errors. 2.5 indicating whether a rule occurs once, twice, or more than twice in the parallel training data. 2.6 Lattice Rescoring One of the advantages of HiFST is direct generation of large translation lattices encoding many alternative translation hypotheses. These first-pass lattices are rescored with second-pass higher-order LMs prior to LMBR. 2.6.1 5-gram LM Lattice Rescoring We build sentence-specific, zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram LMs estimated over approximately 6.2 billion words for English, 2.3 billion words for French, and 1.4 billion words for Spanish. For the English-French task, the second-pass LM training data is the same monolingual data used for the first-pass LMs (as summarised in Tables 3, 4). The Spanish secondpass 5-gram LM includes an additional 1.4 billion words of monolingual data from the Spanish GigaWord Second Edition (Mendonca et al., 2009) and Europarl, which were not included in the first-pass LM (see Table 5). 2.6.2 LMBR Decoding Minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) o"
W10-1722,A00-1031,0,0.0721921,"it (Deng and Byrne, 2005). In the English-to-French and English-to-Spanish directions, we trained a word-to-phrase HMM model with maximum phrase length of 2. In the French to English and Spanish to English directions, we trained a wordto-phrase HMM Model with a bigram translation table and maximum phrase length of 4. We also trained context-dependent alignment models (Brunning et al., 2009) for the FrenchEnglish medium-size (B) dataset. The context of a word is based on its part-of-speech and the partof-speech tags of the surrounding words. These tags were obtained by applying the TnT Tagger (Brants, 2000) for English and the TreeTagger (Schmid, 1994) for French. Decision tree clustering based on optimisation of the EM auxiliary function was used to group contexts that translate similarly. Unfortunately, time constraints prevented us from training context-dependent models for the larger (C) dataset. 2.3 # Sentences 9.0M 1.3M 12.9M 16.0M 30.4M 62.1M 73.6M 21.4M 48.7M 275.4M # Sentences 9.0M 25.2M 12.7M 15.2M 21.4M 83.5 M # Tokens 282.8 696.0M 300.6M 373.5M 684.4M 2337.3M Table 4: French monolingual training corpora. Corpus NC + News EU + Gigaword (5g) Total # Sentences 4.0M 249.4M 253.4 M # Toke"
W10-1722,N09-1013,1,0.889427,"Missing"
W10-1722,N04-1022,1,0.818246,"ff (Brants et al., 2007) 5-gram LMs estimated over approximately 6.2 billion words for English, 2.3 billion words for French, and 1.4 billion words for Spanish. For the English-French task, the second-pass LM training data is the same monolingual data used for the first-pass LMs (as summarised in Tables 3, 4). The Spanish secondpass 5-gram LM includes an additional 1.4 billion words of monolingual data from the Spanish GigaWord Second Edition (Mendonca et al., 2009) and Europarl, which were not included in the first-pass LM (see Table 5). 2.6.2 LMBR Decoding Minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) over the full evidence space of the 5-gram rescored lattices was applied to select the translation hypothesis that maximises the conditional expected gain under the linearised sentence-level BLEU score (Tromble et al., 2008; Blackwood and Byrne, 2010). The unigram precision p and average recall ratio r were set as described in Tromble et al. (2008) using the newstest2008 development set. 2.7 Hypothesis Combination Linearised lattice minimum Bayes-risk decoding (Tromble et al., 2008) can also be used as an effective framework for multiple lattice combination (de Gispert et al., 2010). For the"
W10-1722,2001.mtsummit-papers.46,0,0.0630332,"+0.8 BLEU improvement). The use of context dependent alignment models gives a small improvement in the French-to-English direction: system (B)CD improves by +0.3 BLEU over system (B) on newstest2009. In the English-to-French direction, there is no improvement nor degradation in performance. 5-gram and LMBR rescoring also give consistent improvement throughout the datasets. Finally, combination between the medium-size system (B)CD and the full-size system (C) gives further small gains in BLEU over LMBR on each individual system. 4 Multi-Source Translation Experiments Multi-source translation (Och and Ney, 2001; Schroeder et al., 2009) is possible whenever multiple translations of the source language input sentence are available. The motivation for multisource translation is that some of the ambiguity that must be resolved in translating between one pair of languages may not be present in a different pair. In the following experiments, multiple LMBR is applied for the first time to the task of multi-source translation. 158 Task SP → EN EN → SP Configuration HiFST (A) +5g+LMBR HiFST (B) HiFST (B2) HiFST (B3) HiFST (A) +5g+LMBR newstest2008 24.6 25.4 23.7 24.3 24.2 23.9 24.7 newstest2009 26.0 27.0 25."
W10-1722,P03-1021,0,0.00637988,"vel BLEU score (Tromble et al., 2008; Blackwood and Byrne, 2010). The unigram precision p and average recall ratio r were set as described in Tromble et al. (2008) using the newstest2008 development set. 2.7 Hypothesis Combination Linearised lattice minimum Bayes-risk decoding (Tromble et al., 2008) can also be used as an effective framework for multiple lattice combination (de Gispert et al., 2010). For the French-English language pair, we used LMBR to combine translation lattices produced by systems trained on alternative data sets. Parameter Optimisation Minimum error rate training (MERT) (Och, 2003) under the BLEU score (Papineni et al., 2001) optimises the weights of the following decoder features with respect to the newstest2008 development set: target LM, number of usages of the glue rule, word and rule insertion penalties, word deletion scale factor, source-to-target and targetto-source translation models, source-to-target and target-to-source lexical models, and three binary rule count features inspired by Bender et al. (2007) 2.8 Post-processing For both Spanish-English and French-English systems, the recasing procedure was performed with the SRILM toolkit. For the Spanish-English"
W10-1722,2001.mtsummit-papers.68,0,0.0152255,"008; Blackwood and Byrne, 2010). The unigram precision p and average recall ratio r were set as described in Tromble et al. (2008) using the newstest2008 development set. 2.7 Hypothesis Combination Linearised lattice minimum Bayes-risk decoding (Tromble et al., 2008) can also be used as an effective framework for multiple lattice combination (de Gispert et al., 2010). For the French-English language pair, we used LMBR to combine translation lattices produced by systems trained on alternative data sets. Parameter Optimisation Minimum error rate training (MERT) (Och, 2003) under the BLEU score (Papineni et al., 2001) optimises the weights of the following decoder features with respect to the newstest2008 development set: target LM, number of usages of the glue rule, word and rule insertion penalties, word deletion scale factor, source-to-target and targetto-source translation models, source-to-target and target-to-source lexical models, and three binary rule count features inspired by Bender et al. (2007) 2.8 Post-processing For both Spanish-English and French-English systems, the recasing procedure was performed with the SRILM toolkit. For the Spanish-English system, we created models from the GigaWord s"
W10-1722,E09-1082,0,0.0209854,"ent). The use of context dependent alignment models gives a small improvement in the French-to-English direction: system (B)CD improves by +0.3 BLEU over system (B) on newstest2009. In the English-to-French direction, there is no improvement nor degradation in performance. 5-gram and LMBR rescoring also give consistent improvement throughout the datasets. Finally, combination between the medium-size system (B)CD and the full-size system (C) gives further small gains in BLEU over LMBR on each individual system. 4 Multi-Source Translation Experiments Multi-source translation (Och and Ney, 2001; Schroeder et al., 2009) is possible whenever multiple translations of the source language input sentence are available. The motivation for multisource translation is that some of the ambiguity that must be resolved in translating between one pair of languages may not be present in a different pair. In the following experiments, multiple LMBR is applied for the first time to the task of multi-source translation. 158 Task SP → EN EN → SP Configuration HiFST (A) +5g+LMBR HiFST (B) HiFST (B2) HiFST (B3) HiFST (A) +5g+LMBR newstest2008 24.6 25.4 23.7 24.3 24.2 23.9 24.7 newstest2009 26.0 27.0 25.4 25.7 25.6 24.5 25.5 new"
W10-1722,J07-2003,0,0.354651,"from pre-processing to postprocessing. Section 3 presents and discusses results and Section 4 describes an additional experiment on multi-source translation. 1 Introduction This paper describes the Cambridge University Engineering Department (CUED) system submission to the ACL 2010 Fifth Workshop on Statistical Machine Translation (WMT10). Our translation system is HiFST (Iglesias et al., 2009a), a hierarchical phrase-based decoder that generates translation lattices directly. Decoding is guided by a CYK parser based on a synchronous contextfree grammar induced from automatic word alignments (Chiang, 2007). The decoder is implemented with Weighted Finite State Transducers (WFSTs) using standard operations available in the OpenFst libraries (Allauzen et al., 2007). The use of WFSTs allows fast and efficient exploration of a vast translation search space, avoiding search errors in decoding. It also allows better integration with other steps in our translation pipeline such as 5-gram language model (LM) rescoring and lattice minimum Bayes-risk (LMBR) decoding. 2 System Development We built three French-English and two SpanishEnglish systems, trained on different portions of the parallel data sets"
W10-1722,D08-1065,0,0.0212387,"the same monolingual data used for the first-pass LMs (as summarised in Tables 3, 4). The Spanish secondpass 5-gram LM includes an additional 1.4 billion words of monolingual data from the Spanish GigaWord Second Edition (Mendonca et al., 2009) and Europarl, which were not included in the first-pass LM (see Table 5). 2.6.2 LMBR Decoding Minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) over the full evidence space of the 5-gram rescored lattices was applied to select the translation hypothesis that maximises the conditional expected gain under the linearised sentence-level BLEU score (Tromble et al., 2008; Blackwood and Byrne, 2010). The unigram precision p and average recall ratio r were set as described in Tromble et al. (2008) using the newstest2008 development set. 2.7 Hypothesis Combination Linearised lattice minimum Bayes-risk decoding (Tromble et al., 2008) can also be used as an effective framework for multiple lattice combination (de Gispert et al., 2010). For the French-English language pair, we used LMBR to combine translation lattices produced by systems trained on alternative data sets. Parameter Optimisation Minimum error rate training (MERT) (Och, 2003) under the BLEU score (Pap"
W10-1722,N09-2019,1,0.890712,"Missing"
W10-1722,J10-3008,1,0.814178,"Missing"
W10-1722,H05-1022,1,0.85111,"Europarl + News-Commentary + UN SP 205.6M 420.8k 6.5M EN 192.0M 402.8k Corpus EU + NC + UN CNA LTW XIN AFP APW NYT Giga News Total Table 2: Parallel data sets used for Spanish-toEnglish experiments. UTF8 token (e.g., replacing “&amp” by “&”) as this interacts with tokenization. Data was then tokenized and lowercased, so mixed case is added as post-processing. 2.2 # Tokens 246.4M 34.8M 298.7M 352.5M 710.6M 1268.6M 1622.5M 573.8M 1128.4M 6236.4M Table 3: English monolingual training corpora. Corpus EU + NC + UN AFP APW News Giga Total Alignments Parallel data was aligned using the MTTK toolkit (Deng and Byrne, 2005). In the English-to-French and English-to-Spanish directions, we trained a word-to-phrase HMM model with maximum phrase length of 2. In the French to English and Spanish to English directions, we trained a wordto-phrase HMM Model with a bigram translation table and maximum phrase length of 4. We also trained context-dependent alignment models (Brunning et al., 2009) for the FrenchEnglish medium-size (B) dataset. The context of a word is based on its part-of-speech and the partof-speech tags of the surrounding words. These tags were obtained by applying the TnT Tagger (Brants, 2000) for English"
W10-1722,N09-1049,1,0.897262,"Missing"
W10-1722,P02-1040,0,\N,Missing
W10-1722,2010.iwslt-keynotes.2,1,\N,Missing
W13-2225,J93-2003,0,0.0654043,"the model parameters are stored on disk in the HFile format (Pino et al., 2012) for fast querying. Rule extraction and feature computation takes about 2h30. The HFile format requires data to be stored in a key-value structure. For the key, we use shared source side of many rules. The value is a list of tuples containing the possible targets for the source key and the associated parameters of the full rule. The query set of keys for the test set is all possible source phrases (including nonterminals) found in the test set. During HFile querying we add other features. These include IBM Model 1 (Brown et al., 1993) lexical probabilities. Loading these models in memory doesn’t fit well with the MapReduce model so lexical features are computed for each # Tokens 652.5M 654.1M 1594.3M 874.1M 1429.3M 66.4M 326.5M 1744.3M 425.3M 7766.9M Table 2: Statistics for English monolingual corpora. 2.5 Decoding For translation, we use the HiFST decoder (Iglesias et al., 2009). HiFST is a hierarchical decoder that builds target word lattices guided by a probabilistic synchronous context-free grammar. Assuming N to be the set of non-terminals and T the set of terminals or words, then we can define the grammar as a set R"
W13-2225,P11-2080,0,0.0174695,"al grammars (de Gispert et al., 2010) in our experiments. This model is constrained enough that the decoder can build exact search spaces, i.e. there is no pruning in search that may lead to spurious undergeneration errors. 2.6 R X E Y 1 pM 1 (ene |rur ) (E + 1)R r=1 e=0 (1) where ru are the terminals in the Russian side of a rule, en are the terminals in the English side of a rule, including the null word, R is the number of Russian terminals, E is the number of English terminals and pM 1 is the IBM Model 1 probability. In addition to these standard features, we also use provenance features (Chiang et al., 2011). The parallel data is divided into four subcorpora: the Common Crawl (CC) corpus, the News Commentary (NC) corpus, the Yandex (Yx) corpus and the Wiki Headlines (Wiki) corpus. For each of these subcorpora, source-to-target and target-to-source translation and lexical scores are computed. This requires computing IBM Model 1 for each subcorpus. In total, there are 28 features, 12 standard features and 16 provenance features. When retrieving relevant rules for a particular test set, various thresholds are applied, such as number of targets per source or translation probability cutoffs. Threshold"
W13-2225,J07-2003,0,0.681038,"ides of the parallel corpus are then lowercased, so mixed case is restored in post-processing. Corpus statistics after filtering and for various segmentations are summarised in Table 1. Introduction This paper describes the University of Cambridge system submission to the ACL 2013 Eighth Workshop on Statistical Machine Translation (WMT13). Our translation system is HiFST (Iglesias et al., 2009), a hierarchical phrase-based decoder that generates translation lattices directly. Decoding is guided by a CYK parser based on a synchronous context-free grammar induced from automatic word alignments (Chiang, 2007). The decoder is implemented with Weighted Finite State Transducers (WFSTs) using standard operations available in the OpenFst libraries (Allauzen et al., 2007). The use of WFSTs allows fast and efficient exploration of a vast translation search space, avoiding search errors in decoding. It also allows better integration with other steps in our translation pipeline such as 5-gram language model (LM) rescoring and lattice minimum Bayes-risk (LMBR) decoding (Blackwood, 2010). We participate in the Russian-English translation shared task in the Russian-English direction. This is the first time we"
W13-2225,N09-2019,1,0.905812,"Missing"
W13-2225,J10-3008,1,0.911969,"Missing"
W13-2225,D07-1090,0,0.0739642,"to-source lexical scores • target word count • rule count • glue rule count 2.7 • deletion rule count (each source unigram, except for OOVs, is allowed to be deleted) Lattice Rescoring The HiFST decoder is set to directly generate large translation lattices encoding many alternative translation hypotheses. These first-pass lattices are rescored with second-pass higher-order LMs prior to LMBR. • binary feature indicating whether a rule is extracted once, twice or more than twice (Bender et al., 2007) 202 2.7.1 5-gram LM Lattice Rescoring We build a sentence-specific, zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram LMs estimated over the data described in section 2.4. Lattices obtained by first-pass decoding are rescored with this 5-gram LM (Blackwood, 2010). We assume for human judgment purposes that it is better to have a non English word in Latin alphabet than in Cyrillic (e.g. uprazdnyayushchie); sometimes, transliteration can also give a correct output (e.g. Movember), especially in the case of proper nouns. 2.7.2 LMBR Decoding Minimum Bayes-risk decoding (Kumar and Byrne, 2004) over the full evidence space of the 5gram rescored lattices is applied to select the translation hypothesis that m"
W13-2225,W08-0333,0,0.0160843,"ction and Retrieval A synchronous context-free grammar (Chiang, 2007) is extracted from the alignments. The constraints are set as in the original publication with the following exceptions: • phrase-based rule maximum number of source words: 9 Corpus EU + NC + UN + CzEng + Yx Giga + CC + Wiki News Crawl afp apw cna + wpb ltw nyt xin Total • maximum number of source element (terminal or nonterminal): 5 • maximum span for nonterminals: 10 Maximum likelihood estimates for the translation probabilities are computed using MapReduce. We use a custom Hadoop-based toolkit which implements method 3 of Dyer et al. (2008). Once computed, the model parameters are stored on disk in the HFile format (Pino et al., 2012) for fast querying. Rule extraction and feature computation takes about 2h30. The HFile format requires data to be stored in a key-value structure. For the key, we use shared source side of many rules. The value is a list of tuples containing the possible targets for the source key and the associated parameters of the full rule. The query set of keys for the test set is all possible source phrases (including nonterminals) found in the test set. During HFile querying we add other features. These incl"
W13-2225,P13-2121,0,0.0522527,"Missing"
W13-2225,N09-1049,1,0.900817,"Missing"
W13-2225,2005.iwslt-1.8,0,0.0447139,"9.3M 66.4M 326.5M 1744.3M 425.3M 7766.9M Table 2: Statistics for English monolingual corpora. 2.5 Decoding For translation, we use the HiFST decoder (Iglesias et al., 2009). HiFST is a hierarchical decoder that builds target word lattices guided by a probabilistic synchronous context-free grammar. Assuming N to be the set of non-terminals and T the set of terminals or words, then we can define the grammar as a set R = {R} of rules R : N → hγ,αi / p, where N ∈ N, γ, α ∈ {N ∪ T}+ and p the rule score. 201 No alignment information is used when computing lexical scores as done in Equation (4) in (Koehn et al., 2005). Instead, the source-to-target lexical score is computed in Equation 1: HiFST translates in three steps. The first step is a variant of the CYK algorithm (Chappelier and Rajman, 1998), in which we apply hypothesis recombination without pruning. Only the source language sentence is parsed using the corresponding source-side context-free grammar with rules N → γ. Each cell in the CYK grid is specified by a non-terminal symbol and position: (N, x, y), spanning sxx+y−1 on the source sentence s1 ...sJ . For the second step, we use a recursive algorithm to construct word lattices with all possible"
W13-2225,N04-1022,1,0.716856,"(Bender et al., 2007) 202 2.7.1 5-gram LM Lattice Rescoring We build a sentence-specific, zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram LMs estimated over the data described in section 2.4. Lattices obtained by first-pass decoding are rescored with this 5-gram LM (Blackwood, 2010). We assume for human judgment purposes that it is better to have a non English word in Latin alphabet than in Cyrillic (e.g. uprazdnyayushchie); sometimes, transliteration can also give a correct output (e.g. Movember), especially in the case of proper nouns. 2.7.2 LMBR Decoding Minimum Bayes-risk decoding (Kumar and Byrne, 2004) over the full evidence space of the 5gram rescored lattices is applied to select the translation hypothesis that maximises the conditional expected gain under the linearised sentence-level BLEU score (Tromble et al., 2008; Blackwood, 2010). The unigram precision p and average recall ratio r are set as described in Tromble et al. (2008) using the newstest2012.tune development set. 3 2.8 Results are reported in Table 3. We use the internationalisation switch for the NIST BLEU scoring script in order to properly lowercase the hypothesis and the reference. This introduces a slight discrepancy wit"
W13-2225,D08-1076,0,0.0967682,"M Model 1 for each subcorpus. In total, there are 28 features, 12 standard features and 16 provenance features. When retrieving relevant rules for a particular test set, various thresholds are applied, such as number of targets per source or translation probability cutoffs. Thresholds involving source-totarget translation scores are applied separately for each provenance and the union of all surviving rules for each provenance is kept. This strategy gives slight gains over using thresholds only for the general translation table. We use an implementation of lattice minimum error rate training (Macherey et al., 2008) to optimise under the BLEU score (Papineni et al., 2001) the feature weights with respect to the odd sentences of the newstest2012 development set (newstest2012.tune). The weights obtained match our expectation, for example, the source-to-target translation feature weight is higher for the NC corpus than for other corpora since we are translating news. s(ru, en) = Features and Parameter Optimisation We use the following standard features: • language model • source-to-target and target-to-source translation scores • source-to-target and target-to-source lexical scores • target word count • rul"
W13-2225,2001.mtsummit-papers.68,0,0.0503054,"ures, 12 standard features and 16 provenance features. When retrieving relevant rules for a particular test set, various thresholds are applied, such as number of targets per source or translation probability cutoffs. Thresholds involving source-totarget translation scores are applied separately for each provenance and the union of all surviving rules for each provenance is kept. This strategy gives slight gains over using thresholds only for the general translation table. We use an implementation of lattice minimum error rate training (Macherey et al., 2008) to optimise under the BLEU score (Papineni et al., 2001) the feature weights with respect to the odd sentences of the newstest2012 development set (newstest2012.tune). The weights obtained match our expectation, for example, the source-to-target translation feature weight is higher for the NC corpus than for other corpora since we are translating news. s(ru, en) = Features and Parameter Optimisation We use the following standard features: • language model • source-to-target and target-to-source translation scores • source-to-target and target-to-source lexical scores • target word count • rule count • glue rule count 2.7 • deletion rule count (each"
W13-2225,D08-1065,0,0.12026,"t-pass decoding are rescored with this 5-gram LM (Blackwood, 2010). We assume for human judgment purposes that it is better to have a non English word in Latin alphabet than in Cyrillic (e.g. uprazdnyayushchie); sometimes, transliteration can also give a correct output (e.g. Movember), especially in the case of proper nouns. 2.7.2 LMBR Decoding Minimum Bayes-risk decoding (Kumar and Byrne, 2004) over the full evidence space of the 5gram rescored lattices is applied to select the translation hypothesis that maximises the conditional expected gain under the linearised sentence-level BLEU score (Tromble et al., 2008; Blackwood, 2010). The unigram precision p and average recall ratio r are set as described in Tromble et al. (2008) using the newstest2012.tune development set. 3 2.8 Results are reported in Table 3. We use the internationalisation switch for the NIST BLEU scoring script in order to properly lowercase the hypothesis and the reference. This introduces a slight discrepancy with official results going into the English language. The newstest2012.test development set consists of even sentences from newstest2012. We observe that the CoreNLP system (A) outperforms the other two systems. The CoreNLP+"
W13-2225,P02-1040,0,\N,Missing
W13-2225,2010.iwslt-keynotes.2,1,\N,Missing
W17-3531,W15-3001,0,0.0185739,"al hypothesis with the product of heuristic estimates of its words. This is especially useful for model combinations since all models are taken into account. We also implement hypothesis recombination to further reduce the number of search errors. More formally, at each time step t our beam search keeps the n best hypotheses according to scoring function S(·) using partial model score s(·) and estimates g(·): S(w1t ) = s(w1t ) − g(w1t ) s(w1t ) = log P (w1t |B OW(w)) X g(w1t ) = log Pˆ (w0 ) (4) w0 ∈w1t 4 Experimental Setup We evaluate using data from the English-German news translation task (Bojar et al., 2015, WMT) and using the English Penn Treebank data (Marcus et al., 1993, PTB). Since additional knowledge sources are often available in practice, such as access to the source sentence in a translation scenario, we also report on bilingual experiments for the WMT task. 4.1 Data and evaluation The WMT parallel training data includes Europarl v7, Common Crawl, and News Commentary v10. We use news-test2013 for tuning model combinations and news-test2015 for testing. All monolingual models for the WMT task were trained on the German news2015 corpus (∼51.3M sentences). For PTB, we use preprocessed dat"
W17-3531,E14-1028,1,0.921133,"Missing"
W17-3531,N15-1012,0,0.061208,"text in tasks such as general natural language generation (Reiter and Dale, 1997), image caption generation (Xu et al., 2015), or machine translation (Bahdanau et al., 2015). Since plausible word order is an essential criterion of output fluency for all of these tasks, progress on the wordordering problem is likely to have a positive impact on these tasks as well. Word ordering has often been addressed as syntactic linearization which is a strategy that involves using syntactic structures or partof-speech and dependency labels (Zhang and Clark, 2011; Zhang et al., 2012; Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016). It has also been addressed as LM-based linearization which relies solely on language models and obtains better Work partially supported by U.K. EPSRC grant EP/L027623/1. scores (de Gispert et al., 2014; Schmaltz et al., 2016). Recently, Schmaltz et al. (2016) showed that recurrent neural network language models (Mikolov et al., 2010, R NNLMs) with long short-term memory (Hochreiter and Schmidhuber, 1997, L STM) cells are very effective for word ordering even without any explicit syntactic information. We continue this line of work and make the following contribution"
W17-3531,J93-2004,0,0.0575649,"This is especially useful for model combinations since all models are taken into account. We also implement hypothesis recombination to further reduce the number of search errors. More formally, at each time step t our beam search keeps the n best hypotheses according to scoring function S(·) using partial model score s(·) and estimates g(·): S(w1t ) = s(w1t ) − g(w1t ) s(w1t ) = log P (w1t |B OW(w)) X g(w1t ) = log Pˆ (w0 ) (4) w0 ∈w1t 4 Experimental Setup We evaluate using data from the English-German news translation task (Bojar et al., 2015, WMT) and using the English Penn Treebank data (Marcus et al., 1993, PTB). Since additional knowledge sources are often available in practice, such as access to the source sentence in a translation scenario, we also report on bilingual experiments for the WMT task. 4.1 Data and evaluation The WMT parallel training data includes Europarl v7, Common Crawl, and News Commentary v10. We use news-test2013 for tuning model combinations and news-test2015 for testing. All monolingual models for the WMT task were trained on the German news2015 corpus (∼51.3M sentences). For PTB, we use preprocessed data by Schmaltz et al. (2016) for a fair comparison (∼40k sentences fo"
W17-3531,N16-1058,0,0.0177344,"as general natural language generation (Reiter and Dale, 1997), image caption generation (Xu et al., 2015), or machine translation (Bahdanau et al., 2015). Since plausible word order is an essential criterion of output fluency for all of these tasks, progress on the wordordering problem is likely to have a positive impact on these tasks as well. Word ordering has often been addressed as syntactic linearization which is a strategy that involves using syntactic structures or partof-speech and dependency labels (Zhang and Clark, 2011; Zhang et al., 2012; Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016). It has also been addressed as LM-based linearization which relies solely on language models and obtains better Work partially supported by U.K. EPSRC grant EP/L027623/1. scores (de Gispert et al., 2014; Schmaltz et al., 2016). Recently, Schmaltz et al. (2016) showed that recurrent neural network language models (Mikolov et al., 2010, R NNLMs) with long short-term memory (Hochreiter and Schmidhuber, 1997, L STM) cells are very effective for word ordering even without any explicit syntactic information. We continue this line of work and make the following contributions. We compare several lang"
W17-3531,D16-1255,0,0.750893,"or all of these tasks, progress on the wordordering problem is likely to have a positive impact on these tasks as well. Word ordering has often been addressed as syntactic linearization which is a strategy that involves using syntactic structures or partof-speech and dependency labels (Zhang and Clark, 2011; Zhang et al., 2012; Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016). It has also been addressed as LM-based linearization which relies solely on language models and obtains better Work partially supported by U.K. EPSRC grant EP/L027623/1. scores (de Gispert et al., 2014; Schmaltz et al., 2016). Recently, Schmaltz et al. (2016) showed that recurrent neural network language models (Mikolov et al., 2010, R NNLMs) with long short-term memory (Hochreiter and Schmidhuber, 1997, L STM) cells are very effective for word ordering even without any explicit syntactic information. We continue this line of work and make the following contributions. We compare several language models on the word-ordering task and propose a bag-to-sequence neural architecture that equips an LSTM decoder with explicit context of the bag-ofwords (B OW) to be ordered. This model performs particularly strongly on WMT"
W17-3531,W16-2323,0,0.0236092,"otations of the same size as the hidden units in the decoder. The R NNLM is based on the “large” setup of Zaremba et al. (2014) which uses an L STM. N PLM, a 5-gram neural feedforward language model, was trained for 10 epochs with a vocabulary size of 100k, 150 input and output units, 750 hidden units and 100 noise samples (Vaswani et al., 2013). The n-gram language model is a 5-gram model estimated with S RILM (Kneser and Ney, 1995). For the bilingual setting, we implemented a seq2seq NMT system following Bahdanau et al. (2015) using a beam size of 12 in line with recent NMT systems for WMT (Sennrich et al., 2016). R NNLM, bag2seq and seq2seq were implemented using TensorFlow (Abadi et al., 2015)2 and we used sgnmt for beam decoding3 . Following Schmaltz et al. (2016), our neural models for PTB have a vocabulary of 16,161 incl. two different unk tokens and the R NNLM is based on the “medium” setup of Zaremba et al. (2014). bag2seq uses 300 dimensional word embeddings and 500 hidden units in the decoder L STM. We also compare to G YRO (de Gispert et al., 2014) which explicitly targets the word-ordering problem. We extracted 1-gram to 5-gram phrase rules from the PTB train1 We thank the authors for help"
W17-3531,D13-1140,0,0.0200824,"odel settings For WMT, the bag2seq parameter settings follow the recent NMT systems trained on WMT data. We use a 50k vocabulary, 620 dimensional word embeddings and 1000 hidden units in the decoder L STM cells. On the encoder side, the input tokens are embedded to form annotations of the same size as the hidden units in the decoder. The R NNLM is based on the “large” setup of Zaremba et al. (2014) which uses an L STM. N PLM, a 5-gram neural feedforward language model, was trained for 10 epochs with a vocabulary size of 100k, 150 input and output units, 750 hidden units and 100 noise samples (Vaswani et al., 2013). The n-gram language model is a 5-gram model estimated with S RILM (Kneser and Ney, 1995). For the bilingual setting, we implemented a seq2seq NMT system following Bahdanau et al. (2015) using a beam size of 12 in line with recent NMT systems for WMT (Sennrich et al., 2016). R NNLM, bag2seq and seq2seq were implemented using TensorFlow (Abadi et al., 2015)2 and we used sgnmt for beam decoding3 . Following Schmaltz et al. (2016), our neural models for PTB have a vocabulary of 16,161 incl. two different unk tokens and the R NNLM is based on the “medium” setup of Zaremba et al. (2014). bag2seq u"
W17-3531,D11-1106,0,0.0982226,"for studying and comparing different kinds of models that produce text in tasks such as general natural language generation (Reiter and Dale, 1997), image caption generation (Xu et al., 2015), or machine translation (Bahdanau et al., 2015). Since plausible word order is an essential criterion of output fluency for all of these tasks, progress on the wordordering problem is likely to have a positive impact on these tasks as well. Word ordering has often been addressed as syntactic linearization which is a strategy that involves using syntactic structures or partof-speech and dependency labels (Zhang and Clark, 2011; Zhang et al., 2012; Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016). It has also been addressed as LM-based linearization which relies solely on language models and obtains better Work partially supported by U.K. EPSRC grant EP/L027623/1. scores (de Gispert et al., 2014; Schmaltz et al., 2016). Recently, Schmaltz et al. (2016) showed that recurrent neural network language models (Mikolov et al., 2010, R NNLMs) with long short-term memory (Hochreiter and Schmidhuber, 1997, L STM) cells are very effective for word ordering even without any explicit syntactic information. We c"
W17-3531,J15-3005,0,0.0725805,"of models that produce text in tasks such as general natural language generation (Reiter and Dale, 1997), image caption generation (Xu et al., 2015), or machine translation (Bahdanau et al., 2015). Since plausible word order is an essential criterion of output fluency for all of these tasks, progress on the wordordering problem is likely to have a positive impact on these tasks as well. Word ordering has often been addressed as syntactic linearization which is a strategy that involves using syntactic structures or partof-speech and dependency labels (Zhang and Clark, 2011; Zhang et al., 2012; Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016). It has also been addressed as LM-based linearization which relies solely on language models and obtains better Work partially supported by U.K. EPSRC grant EP/L027623/1. scores (de Gispert et al., 2014; Schmaltz et al., 2016). Recently, Schmaltz et al. (2016) showed that recurrent neural network language models (Mikolov et al., 2010, R NNLMs) with long short-term memory (Hochreiter and Schmidhuber, 1997, L STM) cells are very effective for word ordering even without any explicit syntactic information. We continue this line of work and make the foll"
W17-3531,E12-1075,0,0.133967,"ing different kinds of models that produce text in tasks such as general natural language generation (Reiter and Dale, 1997), image caption generation (Xu et al., 2015), or machine translation (Bahdanau et al., 2015). Since plausible word order is an essential criterion of output fluency for all of these tasks, progress on the wordordering problem is likely to have a positive impact on these tasks as well. Word ordering has often been addressed as syntactic linearization which is a strategy that involves using syntactic structures or partof-speech and dependency labels (Zhang and Clark, 2011; Zhang et al., 2012; Zhang and Clark, 2015; Liu et al., 2015; Puduppully et al., 2016). It has also been addressed as LM-based linearization which relies solely on language models and obtains better Work partially supported by U.K. EPSRC grant EP/L027623/1. scores (de Gispert et al., 2014; Schmaltz et al., 2016). Recently, Schmaltz et al. (2016) showed that recurrent neural network language models (Mikolov et al., 2010, R NNLMs) with long short-term memory (Hochreiter and Schmidhuber, 1997, L STM) cells are very effective for word ordering even without any explicit syntactic information. We continue this line of"
W18-6427,D17-1148,0,0.0356724,"NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles usually consist of multiple models of the same architecture. In this paper, we compare and combine three very different architectures (recurrent, convolutional, and self-attention based) in two different ways (full posterior and MBR-based), and find that combination with MBR-based n-gram scores is superior. 7 2016. Deeper machine translation and evaluation for German. In Proceedi"
W18-6427,W18-2202,0,0.018873,"nts worse than our best single neural model. We list the performance of our submitted systems on all test sets in Tab. 8. Test set news-test14 news-test15 news-test16 news-test17 news-test18 news-test14 news-test15 news-test16 news-test17 news-test18 news-dev17 news-test17 news-test18 BLEU 31.6 32.6 38.5 31.7 46.6 36.8 36.5 45.1 38.7 48.0 25.7 27.1 27.7 Table 8: BLEU scores of the submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and"
W18-6427,I17-2004,0,0.0390255,"ble 8: BLEU scores of the submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles usually consist of multip"
W18-6427,N18-2046,0,0.033061,"submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles usually consist of multiple models of the same architecture. In t"
W18-6427,W17-3204,0,0.0290138,"e PBMT baseline is usally more than 10 BLEU points worse than our best single neural model. We list the performance of our submitted systems on all test sets in Tab. 8. Test set news-test14 news-test15 news-test16 news-test17 news-test18 news-test14 news-test15 news-test16 news-test17 news-test18 news-dev17 news-test17 news-test18 BLEU 31.6 32.6 38.5 31.7 46.6 36.8 36.5 45.1 38.7 48.0 25.7 27.1 27.7 Table 8: BLEU scores of the submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysD"
W18-6427,N04-1022,0,0.0969931,"n-gram posteriors from the R2L model, reverse them, and use them for system combination as described in Sec. 2. λi log P (yt |y1t−1 , x, Mi ) + q X | gorithm similar to golden-section search (Kiefer, 1953).  (2) where λ1 , . . . , λq are interpolation weights. Eq. 2 also describes how to use beam search in this framework as hypotheses can be built up from left to right due to the outer sum over time steps. The MBR-based models contribute via the probabilt |x, M ) of an n-gram y t ity P (yt−n j t−n given the source sentence x. Posteriors in this form are commonly used for MBR decoding in SMT (Kumar and Byrne, 2004; Tromble et al., 2008), and can be extracted efficiently from translation lattices using counting transducers (Blackwood et al., 2010). For our neural models we run beam search with beam size 15 and compute posteriors over the 15best list. We smooth all n-gram posteriors as suggested by Stahlberg et al. (2017a). Note that our generalization to more than two systems can still be seen as instance of the original scheme from Stahlberg et al. (2017a) by viewing the first group M1 , . . . , Mp as ensemble and the evidence space from the second group Mp+1 , . . . , Mq as mixture model. The performa"
W18-6427,N18-3013,1,0.880265,"Missing"
W18-6427,D13-1054,0,0.0282011,"ission ranks second in terms of BLEU score in the WMT18 evaluation campaign on English-German and German-English, and outperforms all other systems on a variety of linguistic phenomena on German-English (Avramidis et al., 2018). Introduction 2 Encoder-decoder networks (Pollack, 1990; Chris˜ man, 1991; Forcada and Neco, 1997; Kalchbrenner and Blunsom, 2013) are the current prevailing architecture for neural machine translation (NMT). Various architectures have been used in the general framework of encoder and decoder networks such as recursive auto-encoders (Pollack, 1990; Socher et al., 2011; Li et al., 2013), (attentional) recurrent models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Chen et al., 2018), convolutional models (Kalchbrenner and Blunsom, 2013; Kaiser et al., 2017; Gehring et al., 2017), and, most recently, purely (self-)attention-based models (Vaswani et al., 2017; Ahmed et al., 2017; Shaw et al., 2018). In the spirit of Chen et al. (2018) we devoted our WMT18 submission to exploring the three most commonly used architectures: recurrent, convolutional, and self-attentionbased models like the Transformer (Vaswani et al., 2017). Our experiments s"
W18-6427,W16-2316,0,0.025543,"onger LSTM training. We use news-test2017 as development set on all language pairs to tune the model interpolation weights λ (Eq. 2) and the scaling factor for length normalization. We train vanilla phrase-based SMT systems5 and extract 1000-best lists of unique translations candidates, from which n-gram posteriors are calculated. All neural models were trained with the Adam optimizer (Kingma and Ba, 2015), dropout (Srivastava et al., 2014), and label smoothing (Szegedy et al., 2016) using the Tensor2Tensor (Vaswani et al., 2018) library. We decode with the average of the last 40 checkpoints (Junczys-Dowmunt et al., 2016a). We make extensive use of the delayed SGD updates technique we already applied successfully to syntax-based NMT (Saunders et al., 2018). Delaying SGD updates allows to arbitrarily choose the effective batch size even on limited GPU hardware. Large batch training has received some attention in recent research (Smith et al., 2017; Neishi et al., 2017) and has been shown particularly useful for training the Transformer architecture with the Tensor2Tensor framework (Popel and Bojar, 2018). We support these findings in Tab. 4.6 Our technical infrastructure7 allows us to train on four P100 GPUs s"
W18-6427,N16-1046,0,0.0667258,"Missing"
W18-6427,J82-2005,0,0.814358,"Missing"
W18-6427,W16-4602,0,0.0505981,"uperior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles usually consist of multiple models of the same architecture. In this paper, we compare and combine three very different architectures (recurrent, convolutional, and self-attention based) in two different ways (full posterior and MBR-based), and find that combination with MBR-based n-gram scores is superior. 7 2016. Deeper machine translat"
W18-6427,D13-1176,0,0.0621467,"he batch size on limited GPU hardware. Furthermore, we also report gains from MBR-based combination with a phrase-based SMT system. We found this particularly striking as the SMT baselines are often more than 10 BLEU points below our strongest neural models. Our final submission ranks second in terms of BLEU score in the WMT18 evaluation campaign on English-German and German-English, and outperforms all other systems on a variety of linguistic phenomena on German-English (Avramidis et al., 2018). Introduction 2 Encoder-decoder networks (Pollack, 1990; Chris˜ man, 1991; Forcada and Neco, 1997; Kalchbrenner and Blunsom, 2013) are the current prevailing architecture for neural machine translation (NMT). Various architectures have been used in the general framework of encoder and decoder networks such as recursive auto-encoders (Pollack, 1990; Socher et al., 2011; Li et al., 2013), (attentional) recurrent models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Chen et al., 2018), convolutional models (Kalchbrenner and Blunsom, 2013; Kaiser et al., 2017; Gehring et al., 2017), and, most recently, purely (self-)attention-based models (Vaswani et al., 2017; Ahmed et al., 2017; Shaw e"
W18-6427,D15-1166,0,0.0576362,"German-English, and outperforms all other systems on a variety of linguistic phenomena on German-English (Avramidis et al., 2018). Introduction 2 Encoder-decoder networks (Pollack, 1990; Chris˜ man, 1991; Forcada and Neco, 1997; Kalchbrenner and Blunsom, 2013) are the current prevailing architecture for neural machine translation (NMT). Various architectures have been used in the general framework of encoder and decoder networks such as recursive auto-encoders (Pollack, 1990; Socher et al., 2011; Li et al., 2013), (attentional) recurrent models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Chen et al., 2018), convolutional models (Kalchbrenner and Blunsom, 2013; Kaiser et al., 2017; Gehring et al., 2017), and, most recently, purely (self-)attention-based models (Vaswani et al., 2017; Ahmed et al., 2017; Shaw et al., 2018). In the spirit of Chen et al. (2018) we devoted our WMT18 submission to exploring the three most commonly used architectures: recurrent, convolutional, and self-attentionbased models like the Transformer (Vaswani et al., 2017). Our experiments suggest that self-attention is the superior architecture on the tested language pairs, but it can st"
W18-6427,P18-2051,1,0.876707,"Missing"
W18-6427,D08-1076,0,0.0471058,"m search with beam size 15 and compute posteriors over the 15best list. We smooth all n-gram posteriors as suggested by Stahlberg et al. (2017a). Note that our generalization to more than two systems can still be seen as instance of the original scheme from Stahlberg et al. (2017a) by viewing the first group M1 , . . . , Mp as ensemble and the evidence space from the second group Mp+1 , . . . , Mq as mixture model. The performance of our system combinations depends on the correct calibration of the interpolation weights λ1 , . . . , λq . We first tried to use nbest or lattice MERT (Och, 2003; Macherey et al., 2008) to find interpolation weights, but these techniques were not effective in our setting, possibly due to the lack of diversity and depth in nbest lists from standard beam search. Therefore, we tune on the first best translation using Powell’s method (Powell, 1964) with a line search al4 Experimental Setup 4.1 Data Selection We ran language detection (Nakatani, 2010) and gentle length filtering based on the number of characters and words in a sentence on all available monolingual and parallel data in English, German, and Chinese. Due to the high level of noise in the ParaCrawl corpus and its lar"
W18-6427,C16-1160,0,0.0235827,"her small, but we still found them surprising given that the PBMT baseline is usally more than 10 BLEU points worse than our best single neural model. We list the performance of our submitted systems on all test sets in Tab. 8. Test set news-test14 news-test15 news-test16 news-test17 news-test18 news-test14 news-test15 news-test16 news-test17 news-test18 news-dev17 news-test17 news-test18 BLEU 31.6 32.6 38.5 31.7 46.6 36.8 36.5 45.1 38.7 48.0 25.7 27.1 27.7 Table 8: BLEU scores of the submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 201"
W18-6427,W18-1811,0,0.0507167,"large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles usually consist of multiple models of the same architecture. In this paper, we compare and combine three very diff"
W18-6427,W17-4739,0,0.159119,"rior scores log-linearly, and bias the combined score S(y|x) towards low-risk hypotheses with respect to the MBR-based group as suggested by Stahlberg et al. (2017a, Eq. 4):1 S(y|x) = p T X X t=1 |i=1 3 {z } {z } Full posterior j=p+1 λj 4 X n=1 t P (yt−n |x, Mj ) MBR-based n-gram scores Right-to-left Translation Models Standard NMT models generate the translation from left to right on the target side. Recent work has shown that incorporating models which generate the target sentence in reverse order (i.e. from right to left) can improve translation quality (Liu et al., 2016; Li et al., 2017; Sennrich et al., 2017; Hassan et al., 2018). Right-to-left models are often used to rescore n-best lists from left-to-right models. However, we could not find improvements from rescoring in our setting. Instead, we extract n-gram posteriors from the R2L model, reverse them, and use them for system combination as described in Sec. 2. λi log P (yt |y1t−1 , x, Mi ) + q X | gorithm similar to golden-section search (Kiefer, 1953).  (2) where λ1 , . . . , λq are interpolation weights. Eq. 2 also describes how to use beam search in this framework as hypotheses can be built up from left to right due to the outer sum over"
W18-6427,P16-1009,0,0.286025,"Crawl more aggressively with the following rules: • No words contain more than 40 characters. • Sentences must not contain HTML tags. • The minimum sentence length is 4 words. • The character ratio between source and target must not exceed 1:3 or 3:1. • Source and target sentences must be equal after stripping out non-numerical characters. • Sentences must end with punctuation marks. This additional filtering reduced the size of ParaCrawl from originally 36M sentences to 19M sentences after language detection, and to 11M sentences after applying the more aggressive rules. For backtranslation (Sennrich et al., 2016a) we selected 20M sentences from News Crawl 2017. We used a single Transformer (Vaswani et al., 2017) model in Tensor2Tensor’s (Vaswani et al., 2018) transformer base configuration 1 Eq. 2 differs from Eq. 4 of Stahlberg et al. (2017a) in that we do not use a word penalty Θ0 here, and we do not tune weights for different order n-grams separately (Θ1 , . . . Θ4 ). Both did not improve translation quality in our setting. 505 Corpus Common Crawl Europarl v7 News Commentary v13 Rapid 2016 ParaCrawl Synthetic (news-2017) Total Over-sampling 2x 2x 2x 2x 1x 1x Architecture LSTM SliceNet Transformer"
W18-6427,W17-5708,0,0.0244824,"d with the Adam optimizer (Kingma and Ba, 2015), dropout (Srivastava et al., 2014), and label smoothing (Szegedy et al., 2016) using the Tensor2Tensor (Vaswani et al., 2018) library. We decode with the average of the last 40 checkpoints (Junczys-Dowmunt et al., 2016a). We make extensive use of the delayed SGD updates technique we already applied successfully to syntax-based NMT (Saunders et al., 2018). Delaying SGD updates allows to arbitrarily choose the effective batch size even on limited GPU hardware. Large batch training has received some attention in recent research (Smith et al., 2017; Neishi et al., 2017) and has been shown particularly useful for training the Transformer architecture with the Tensor2Tensor framework (Popel and Bojar, 2018). We support these findings in Tab. 4.6 Our technical infrastructure7 allows us to train on four P100 GPUs simultaneously, which limits the number of physical GPUs to g = 4 and the batch size8 to b = 2048 due to the GPU memory. Thus, the maximum possible effective batch size without delaying SGD updates is b0 = 8192. Training with delay factor d accumulates gradients over d batches and applies the optimizer update rule on the accumulated gradients. This allo"
W18-6427,P16-1162,0,0.545743,"Crawl more aggressively with the following rules: • No words contain more than 40 characters. • Sentences must not contain HTML tags. • The minimum sentence length is 4 words. • The character ratio between source and target must not exceed 1:3 or 3:1. • Source and target sentences must be equal after stripping out non-numerical characters. • Sentences must end with punctuation marks. This additional filtering reduced the size of ParaCrawl from originally 36M sentences to 19M sentences after language detection, and to 11M sentences after applying the more aggressive rules. For backtranslation (Sennrich et al., 2016a) we selected 20M sentences from News Crawl 2017. We used a single Transformer (Vaswani et al., 2017) model in Tensor2Tensor’s (Vaswani et al., 2018) transformer base configuration 1 Eq. 2 differs from Eq. 4 of Stahlberg et al. (2017a) in that we do not use a word penalty Θ0 here, and we do not tune weights for different order n-grams separately (Θ1 , . . . Θ4 ). Both did not improve translation quality in our setting. 505 Corpus Common Crawl Europarl v7 News Commentary v13 Rapid 2016 ParaCrawl Synthetic (news-2017) Total Over-sampling 2x 2x 2x 2x 1x 1x Architecture LSTM SliceNet Transformer"
W18-6427,W15-5003,0,0.0426753,".6 36.8 36.5 45.1 38.7 48.0 25.7 27.1 27.7 Table 8: BLEU scores of the submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performa"
W18-6427,N18-2074,0,0.178466,"2013) are the current prevailing architecture for neural machine translation (NMT). Various architectures have been used in the general framework of encoder and decoder networks such as recursive auto-encoders (Pollack, 1990; Socher et al., 2011; Li et al., 2013), (attentional) recurrent models (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016; Chen et al., 2018), convolutional models (Kalchbrenner and Blunsom, 2013; Kaiser et al., 2017; Gehring et al., 2017), and, most recently, purely (self-)attention-based models (Vaswani et al., 2017; Ahmed et al., 2017; Shaw et al., 2018). In the spirit of Chen et al. (2018) we devoted our WMT18 submission to exploring the three most commonly used architectures: recurrent, convolutional, and self-attentionbased models like the Transformer (Vaswani et al., 2017). Our experiments suggest that self-attention is the superior architecture on the tested language pairs, but it can still benefit from model combination with the other two. We show that using large batch sizes is crucial to Transformer training, and that the delayed SGD updates technique (Saunders et al., 2018) is useful to increase System Combination Stahlberg et al. (2"
W18-6427,C16-1172,0,0.0864616,"Missing"
W18-6427,P03-1021,0,0.112904,"we run beam search with beam size 15 and compute posteriors over the 15best list. We smooth all n-gram posteriors as suggested by Stahlberg et al. (2017a). Note that our generalization to more than two systems can still be seen as instance of the original scheme from Stahlberg et al. (2017a) by viewing the first group M1 , . . . , Mp as ensemble and the evidence space from the second group Mp+1 , . . . , Mq as mixture model. The performance of our system combinations depends on the correct calibration of the interpolation weights λ1 , . . . , λq . We first tried to use nbest or lattice MERT (Och, 2003; Macherey et al., 2008) to find interpolation weights, but these techniques were not effective in our setting, possibly due to the lack of diversity and depth in nbest lists from standard beam search. Therefore, we tune on the first best translation using Powell’s method (Powell, 1964) with a line search al4 Experimental Setup 4.1 Data Selection We ran language detection (Nakatani, 2010) and gentle length filtering based on the number of characters and words in a sentence on all available monolingual and parallel data in English, German, and Chinese. Due to the high level of noise in the Para"
W18-6427,D11-1014,0,0.080721,"Missing"
W18-6427,W18-1819,0,0.0616536,"Missing"
W18-6427,E17-2058,1,0.895446,"Missing"
W18-6427,D17-2005,1,0.600459,"Shaw et al., 2018). In the spirit of Chen et al. (2018) we devoted our WMT18 submission to exploring the three most commonly used architectures: recurrent, convolutional, and self-attentionbased models like the Transformer (Vaswani et al., 2017). Our experiments suggest that self-attention is the superior architecture on the tested language pairs, but it can still benefit from model combination with the other two. We show that using large batch sizes is crucial to Transformer training, and that the delayed SGD updates technique (Saunders et al., 2018) is useful to increase System Combination Stahlberg et al. (2017a) combined SMT and NMT in a hybrid system with a minimum Bayes-risk (MBR) formulation which has been proven useful even for practical industry-level MT (Iglesias et al., 2018). Our system combination scheme is a generalization of this approach to more than two systems. Suppose we want to combine q models M1 , . . . , Mq . We first divide the models into two groups by selecting a p with 1 ≤ p ≤ q. We refer to scores from the first group M1 , . . . , Mp as full posterior scores and from the second group Mp+1 , . . . , Mq as MBR-based scores. Full posterior models contribute to the combined scor"
W18-6427,1983.tc-1.13,0,0.701411,"Missing"
W18-6427,P16-2049,1,0.827016,"7 48.0 25.7 27.1 27.7 Table 8: BLEU scores of the submitted systems (row 10 in Tab. 7). 6 Related Work There is a large body of research comparing NMT and SMT (Schnober et al., 2016; Toral and S´anchez-Cartagena, 2017; Koehn and Knowles, 2017; Menacer et al., 2017; Dowling et al., 2018; Bentivogli et al., 2016, 2018). Most studies have found superior overall translation quality of NMT models in most settings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles"
W18-6427,P17-2060,0,0.0274836,"tings, but complementary strengths of both paradigms. Therefore, the literature about hybrid NMT-SMT sys508 tems is also vast, ranging from rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018), MBR-based formalisms (Stahlberg et al., 2017a, 2018; Iglesias et al., 2018), NMT assisting SMT (JunczysDowmunt et al., 2016b; Du and Way, 2017), and SMT assisting NMT (Niehues et al., 2016; He et al., 2016; Long et al., 2016; Wang et al., 2017; Dahlmann et al., 2017; Zhou et al., 2017). We confirm the potential of hybrid systems by reporting gains on top of very strong neural ensembles. Ensembling is a well-known technique in NMT to improve system performance. However, ensembles usually consist of multiple models of the same architecture. In this paper, we compare and combine three very different architectures (recurrent, convolutional, and self-attention based) in two different ways (full posterior and MBR-based), and find that combination with MBR-based n-gram scores is superior. 7 2016. Deeper machine translation and evaluation for German. In Proceedings of the 2nd Deep"
W18-6427,W18-1821,1,0.783781,"Missing"
W18-6427,E17-1100,0,0.0450447,"Missing"
W18-6427,D08-1065,0,0.0405232,"the R2L model, reverse them, and use them for system combination as described in Sec. 2. λi log P (yt |y1t−1 , x, Mi ) + q X | gorithm similar to golden-section search (Kiefer, 1953).  (2) where λ1 , . . . , λq are interpolation weights. Eq. 2 also describes how to use beam search in this framework as hypotheses can be built up from left to right due to the outer sum over time steps. The MBR-based models contribute via the probabilt |x, M ) of an n-gram y t ity P (yt−n j t−n given the source sentence x. Posteriors in this form are commonly used for MBR decoding in SMT (Kumar and Byrne, 2004; Tromble et al., 2008), and can be extracted efficiently from translation lattices using counting transducers (Blackwood et al., 2010). For our neural models we run beam search with beam size 15 and compute posteriors over the 15best list. We smooth all n-gram posteriors as suggested by Stahlberg et al. (2017a). Note that our generalization to more than two systems can still be seen as instance of the original scheme from Stahlberg et al. (2017a) by viewing the first group M1 , . . . , Mp as ensemble and the evidence space from the second group Mp+1 , . . . , Mq as mixture model. The performance of our system combi"
W18-6427,W18-6436,0,\N,Missing
W18-6427,P10-2006,1,\N,Missing
W19-5340,W16-6404,0,0.0223852,"bmissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way, 2017). Wang et al. (2017b, 2018) interpolated NMT posteriors with word recommendations fro"
W19-5340,D17-1156,0,0.0195049,"vs. 6 in Tab. 4). Gains are generally smaller on German-English. 4.3 et al., 2017) and checkpoint averaging (JunczysDowmunt et al., 2016b,a). In our context, both methods aim to avoid catastrophic forgetting5 (Goodfellow et al., 2013; French, 1999) and over-fitting by keeping the adapted model close to the original, and can thus be seen as regularized fine-tuning techniques. Khayrallah et al. (2018); Dakwale and Monz (2017) regularized the output distributions during fine-tuning using techniques inspired by knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014; Kim and Rush, 2016). Barone et al. (2017) applied standard L2 regularization and a variant of dropout to domain adaptation. EWC as generalization of L2 regularization has been used for NMT domain adaptation by Thompson et al. (2019); Saunders et al. (2019). In particular, Saunders et al. (2019) showed that EWC is not only more effective than L2 in reducing catastrophic forgetting but even yields gains on the general domain when used for fine-tuning on a related domain. Language modelling We introduced our new Intra-Inter Transformer architecture for document-level language modelling in Sec. 2. Tab. 5 shows that our architecture achie"
W19-5340,N18-1118,0,0.119742,"Extracting n-gram probabilities from traditional PBSMT lattices as described by Stahlberg et al. (2017a) and using them as source-conditioned n-gram LMs yields gains even on top of our ensembles (row 4). Our document-level Intra-Inter language models improve the ensembles and the single En-De Base model, but hurt performance slightly for the single Big models (row 5). 5 Document-level MT Various techniques have been proposed to provide the translation system with inter-sentential context, for example by initializing encoder or decoder states (Wang et al., 2017a), using multi-source encoders (Bawden et al., 2018; Jean et al., 2017), as additional decoder input (Wang et al., 2017a), with memoryaugmented neural networks (Tu et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2017), hierarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... c"
W19-5340,W18-6478,0,0.304334,"ntences must not contain HTML tags. • The minimum sentence length is 4 words. • The character ratio between source and target must not exceed 1:3 or 3:1. • Source and target sentences must be equal after stripping out non-numerical characters. • Sentences must end with punctuation marks. Tab. 2 indicates that our systems benefit from ParaCrawl even without filtering (rows 1 vs. 2). Our best ‘Base’ model uses both dual and naive filtering. However, the difference between filtering techniques diminishes under stronger ‘Big’ models with back-translation (rows 6 and 7). ParaCrawl Corpus Filtering Junczys-Dowmunt (2018a,b) reported large gains from filtering the ParaCrawl corpus. This year, the WMT organizers made version 3 of the ParaCrawl corpus available. We compared two different filtering approaches on the new data set. First, we implemented dual cross-entropy filtering (Junczys-Dowmunt, 2018a,b), a sophisticated data selection criterion based on neural 4 4.1 Results Back-translation Back-translation (Sennrich et al., 2016b) is a wellestablished technique to use monolingual target language data for NMT. The idea is to automatically generate translations into the source language with an inverse translat"
W19-5340,D17-1148,0,0.0202572,"ure of the core NMT system. German-English Team BLEU MSRA 42.8 Facebook FAIR 40.8 NEU 40.5 UCAM 39.7 RWTH 39.6 MLLP-UPV 39.3 DFKI 38.8 4 more... Table 7: English-German and German-English primary submissions to the WMT19 shared task. Year 2017 2018 2019 Best in competition 28.3 48.3 44.9 This work ∆ 32.8 49.3 43.0 +4.5 +1.0 -1.9 Table 8: Comparison of our English-German system with the winning submissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distan"
W19-5340,W18-6415,0,0.254207,"ntences must not contain HTML tags. • The minimum sentence length is 4 words. • The character ratio between source and target must not exceed 1:3 or 3:1. • Source and target sentences must be equal after stripping out non-numerical characters. • Sentences must end with punctuation marks. Tab. 2 indicates that our systems benefit from ParaCrawl even without filtering (rows 1 vs. 2). Our best ‘Base’ model uses both dual and naive filtering. However, the difference between filtering techniques diminishes under stronger ‘Big’ models with back-translation (rows 6 and 7). ParaCrawl Corpus Filtering Junczys-Dowmunt (2018a,b) reported large gains from filtering the ParaCrawl corpus. This year, the WMT organizers made version 3 of the ParaCrawl corpus available. We compared two different filtering approaches on the new data set. First, we implemented dual cross-entropy filtering (Junczys-Dowmunt, 2018a,b), a sophisticated data selection criterion based on neural 4 4.1 Results Back-translation Back-translation (Sennrich et al., 2016b) is a wellestablished technique to use monolingual target language data for NMT. The idea is to automatically generate translations into the source language with an inverse translat"
W19-5340,J82-2005,0,0.725998,"Missing"
W19-5340,W16-2316,0,0.0565057,"Missing"
W19-5340,D18-1045,0,0.127109,"itional cross-entropy filtering (JunczysDowmunt, 2018a,b). • Elastic weight consolidation (Kirkpatrick et al., 2017, EWC) is a domain adaptation technique that aims to avoid degradation in performance on the original domain. We report large gains from fine-tuning our models on former English-German WMT test sets with EWC. We find that combining finetuning with checkpoint averaging (JunczysDowmunt et al., 2016b,a) yields further significant gains. Fine-tuning is less effective for German-English. • We confirm the effectiveness of source-side noise for scaling up back-translation as proposed by Edunov et al. (2018). 364 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 364–373 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2 Document-level Language Modelling MT systems usually translate sentences in isolation. However, there is evidence that humans also take context into account, and judge translations from humans with access to the full document higher than the output of a state-of-the-art sentence-level machine translation system (L¨aubli et al., 2018). Common examples of ambiguity which can be resolved wi"
W19-5340,2015.iwslt-evaluation.11,0,0.128742,"rate document-level context in a light-weight fashion, we propose a modification to the Transformer (Vaswani et al., 2017) that has separate attention layers for inter- and intra-sentential context. We report large perplexity reductions compared to sentence-level LMs under the new architecture. Our document-level LM yields small BLEU gains on top of strong NMT ensembles, and we hope to benefit even more from it in document-level human evaluation. Introduction Both fine-tuning and language modelling are techniques widely used for NMT. Fine-tuning is often used to adapt a model to a new domain (Luong and Manning, 2015), while ensembling neural machine translation (NMT) with neural language models (LMs) is an effective way to leverage monolingual data (Gulcehre et al., 2015, 2017; Stahlberg et al., 2018a). Our submission to the WMT19 news shared task relies on ideas from these two lines of research, but applies and combines them in novel ways. Our contributions are: • Even though the performance gap between NMT and traditional statistical machine translation (SMT) is growing rapidly on the task at hand, SMT can still improve very strong NMT ensembles. To combine NMT and SMT we follow Stahlberg et al. (2017a,"
W19-5340,I17-2004,0,0.0190888,"le 8: Comparison of our English-German system with the winning submissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way, 2017). Wang et al. (2017b,"
W19-5340,W18-1811,0,0.0432,"two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way, 2017). Wang et al. (2017b, 2018) interpolated NMT posteriors with word recommendations from SMT and jointly traine"
W19-5340,W18-2705,0,0.0205868,"Fi = E ∇2 LA (θi ) is an estimate of task A Fisher information, which represents the importance of parameter θi to A. On English-German, fine-tuning with EWC and checkpoint averaging yields an 1.1 BLEU improvement (rows 1 vs. 6 in Tab. 4). Gains are generally smaller on German-English. 4.3 et al., 2017) and checkpoint averaging (JunczysDowmunt et al., 2016b,a). In our context, both methods aim to avoid catastrophic forgetting5 (Goodfellow et al., 2013; French, 1999) and over-fitting by keeping the adapted model close to the original, and can thus be seen as regularized fine-tuning techniques. Khayrallah et al. (2018); Dakwale and Monz (2017) regularized the output distributions during fine-tuning using techniques inspired by knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014; Kim and Rush, 2016). Barone et al. (2017) applied standard L2 regularization and a variant of dropout to domain adaptation. EWC as generalization of L2 regularization has been used for NMT domain adaptation by Thompson et al. (2019); Saunders et al. (2019). In particular, Saunders et al. (2019) showed that EWC is not only more effective than L2 in reducing catastrophic forgetting but even yields gains on the general dom"
W19-5340,P18-1118,0,0.0256459,"ram LMs yields gains even on top of our ensembles (row 4). Our document-level Intra-Inter language models improve the ensembles and the single En-De Base model, but hurt performance slightly for the single Big models (row 5). 5 Document-level MT Various techniques have been proposed to provide the translation system with inter-sentential context, for example by initializing encoder or decoder states (Wang et al., 2017a), using multi-source encoders (Bawden et al., 2018; Jean et al., 2017), as additional decoder input (Wang et al., 2017a), with memoryaugmented neural networks (Tu et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2017), hierarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... chical attention (Miculicich et al., 2018; Maruf et al., 2019), deliberation networks (Xiong et al., 2018), or by simply concatenating multiple source"
W19-5340,D16-1139,0,0.0312813,"improvement (rows 1 vs. 6 in Tab. 4). Gains are generally smaller on German-English. 4.3 et al., 2017) and checkpoint averaging (JunczysDowmunt et al., 2016b,a). In our context, both methods aim to avoid catastrophic forgetting5 (Goodfellow et al., 2013; French, 1999) and over-fitting by keeping the adapted model close to the original, and can thus be seen as regularized fine-tuning techniques. Khayrallah et al. (2018); Dakwale and Monz (2017) regularized the output distributions during fine-tuning using techniques inspired by knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014; Kim and Rush, 2016). Barone et al. (2017) applied standard L2 regularization and a variant of dropout to domain adaptation. EWC as generalization of L2 regularization has been used for NMT domain adaptation by Thompson et al. (2019); Saunders et al. (2019). In particular, Saunders et al. (2019) showed that EWC is not only more effective than L2 in reducing catastrophic forgetting but even yields gains on the general domain when used for fine-tuning on a related domain. Language modelling We introduced our new Intra-Inter Transformer architecture for document-level language modelling in Sec. 2. Tab. 5 shows that"
W19-5340,N19-1313,0,0.0123114,"r input (Wang et al., 2017a), with memoryaugmented neural networks (Tu et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2017), hierarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... chical attention (Miculicich et al., 2018; Maruf et al., 2019), deliberation networks (Xiong et al., 2018), or by simply concatenating multiple source and/or target sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018). Context-aware extensions to Transformer encoders have been proposed by Voita et al. (2018); Zhang et al. (2018). Techniques also differ in whether they use source context only (Jean et al., 2017; Wang et al., 2017a; Voita et al., 2018; Zhang et al., 2018), target context only (Tu et al., 2018; Kuang et al., 2017), or both (Bawden et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Tiedemann and Scherrer, 2017; Maruf et"
W19-5340,D18-1325,0,0.0205331,"17), as additional decoder input (Wang et al., 2017a), with memoryaugmented neural networks (Tu et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2017), hierarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... chical attention (Miculicich et al., 2018; Maruf et al., 2019), deliberation networks (Xiong et al., 2018), or by simply concatenating multiple source and/or target sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018). Context-aware extensions to Transformer encoders have been proposed by Voita et al. (2018); Zhang et al. (2018). Techniques also differ in whether they use source context only (Jean et al., 2017; Wang et al., 2017a; Voita et al., 2018; Zhang et al., 2018), target context only (Tu et al., 2018; Kuang et al., 2017), or both (Bawden et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Tiedemann and Sch"
W19-5340,W18-6417,0,0.0924966,"18a). Our submission to the WMT19 news shared task relies on ideas from these two lines of research, but applies and combines them in novel ways. Our contributions are: • Even though the performance gap between NMT and traditional statistical machine translation (SMT) is growing rapidly on the task at hand, SMT can still improve very strong NMT ensembles. To combine NMT and SMT we follow Stahlberg et al. (2017a, 2018b) and build a specialized n-gram LM for each sentence that computes the risk of hypotheses relative to SMT lattices. • While data filtering was central in last year’s evaluation (Koehn et al., 2018b; JunczysDowmunt, 2018b), in our experiments this year we found that a very simple filtering approach based on a small number of crude heuristics can perform as well as dual conditional cross-entropy filtering (JunczysDowmunt, 2018a,b). • Elastic weight consolidation (Kirkpatrick et al., 2017, EWC) is a domain adaptation technique that aims to avoid degradation in performance on the original domain. We report large gains from fine-tuning our models on former English-German WMT test sets with EWC. We find that combining finetuning with checkpoint averaging (JunczysDowmunt et al., 2016b,a) yiel"
W19-5340,W18-6307,0,0.0420208,"Missing"
W19-5340,W15-5003,0,0.0306058,"This work ∆ 32.8 49.3 43.0 +4.5 +1.0 -1.9 Table 8: Comparison of our English-German system with the winning submissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or"
W19-5340,D18-1512,0,0.0370169,"Missing"
W19-5340,C16-1172,0,0.0344343,"Missing"
W19-5340,W16-4602,0,0.027896,"ations from a sentence-level MT system. Our method is light-weight as, similarly to Tiedemann and Scherrer (2017), we do not modify the architecture of the core NMT system. German-English Team BLEU MSRA 42.8 Facebook FAIR 40.8 NEU 40.5 UCAM 39.7 RWTH 39.6 MLLP-UPV 39.3 DFKI 38.8 4 more... Table 7: English-German and German-English primary submissions to the WMT19 shared task. Year 2017 2018 2019 Best in competition 28.3 48.3 44.9 This work ∆ 32.8 49.3 43.0 +4.5 +1.0 -1.9 Table 8: Comparison of our English-German system with the winning submissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), altho"
W19-5340,P18-2051,1,0.899395,"Missing"
W19-5340,W16-2324,1,0.84347,"3 43.0 +4.5 +1.0 -1.9 Table 8: Comparison of our English-German system with the winning submissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way,"
W19-5340,P19-1022,1,0.888492,"Missing"
W19-5340,D17-2005,1,0.912788,"Luong and Manning, 2015), while ensembling neural machine translation (NMT) with neural language models (LMs) is an effective way to leverage monolingual data (Gulcehre et al., 2015, 2017; Stahlberg et al., 2018a). Our submission to the WMT19 news shared task relies on ideas from these two lines of research, but applies and combines them in novel ways. Our contributions are: • Even though the performance gap between NMT and traditional statistical machine translation (SMT) is growing rapidly on the task at hand, SMT can still improve very strong NMT ensembles. To combine NMT and SMT we follow Stahlberg et al. (2017a, 2018b) and build a specialized n-gram LM for each sentence that computes the risk of hypotheses relative to SMT lattices. • While data filtering was central in last year’s evaluation (Koehn et al., 2018b; JunczysDowmunt, 2018b), in our experiments this year we found that a very simple filtering approach based on a small number of crude heuristics can perform as well as dual conditional cross-entropy filtering (JunczysDowmunt, 2018a,b). • Elastic weight consolidation (Kirkpatrick et al., 2017, EWC) is a domain adaptation technique that aims to avoid degradation in performance on the original"
W19-5340,W18-6426,0,0.0220852,"37.3 30.3 test18 43.8 44.3 43.2 43.1 44.8 44.7 44.4 45.2 45.2 Table 3: Using different corpora for back-translation. We back-translated with a ‘base’ model for news-2017 and the big single Transformer model of Stahlberg et al. (2018b) for news-2016 and news-2018. Fine-tuning 1 2 3 4 5 6 No No Cont’d train. Cont’d train. EWC EWC Checkpoint averaging X X X BLEU (test18) En-De De-En 46.7 46.5 46.6 46.4 47.1 46.6 47.3 46.8 47.1 46.4 47.8 46.8 until it converges on a training corpus A, and then continues training on a usually much smaller corpus B which is close to the target domain. Similarly to Schamper et al. (2018); Koehn et al. (2018a), we fine-tune our models on former WMT test sets (2008-2016) to adapt them to the target domain of high-quality news translations. Due to the very small size of corpus B, much care has to be taken to avoid over-fitting. We experimented with different techniques that keep the model parameters in the fine-tuning phase close to the original ones. First, we fine-tuned our models for about 1K-2K iterations (depending on the performance on the news-test2017 dev set) and dumped checkpoints every 500 steps. Averaging all fine-tuning checkpoints together with the last unadapted c"
W19-5340,P16-2049,1,0.835571,"3 43.0 +4.5 +1.0 -1.9 Table 8: Comparison of our English-German system with the winning submissions over the past two years. 2016b). In contrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way,"
W19-5340,W18-1821,1,0.879725,"context. We report large perplexity reductions compared to sentence-level LMs under the new architecture. Our document-level LM yields small BLEU gains on top of strong NMT ensembles, and we hope to benefit even more from it in document-level human evaluation. Introduction Both fine-tuning and language modelling are techniques widely used for NMT. Fine-tuning is often used to adapt a model to a new domain (Luong and Manning, 2015), while ensembling neural machine translation (NMT) with neural language models (LMs) is an effective way to leverage monolingual data (Gulcehre et al., 2015, 2017; Stahlberg et al., 2018a). Our submission to the WMT19 news shared task relies on ideas from these two lines of research, but applies and combines them in novel ways. Our contributions are: • Even though the performance gap between NMT and traditional statistical machine translation (SMT) is growing rapidly on the task at hand, SMT can still improve very strong NMT ensembles. To combine NMT and SMT we follow Stahlberg et al. (2017a, 2018b) and build a specialized n-gram LM for each sentence that computes the risk of hypotheses relative to SMT lattices. • While data filtering was central in last year’s evaluation (Ko"
W19-5340,W16-2323,0,0.363162,"ra-sentential and inter-sentential attention masks for an English example from news-test2017. Document-level context helps to predict the next word (‘vinyl’). We stop when the translation score difference to the first-best translation exceeds a threshold.1 3 T2T HParams set # physical GPUs Batch size SGD delay factor # training iterations Beam size Experimental Setup Our experimental setup is essentially the same as last year (Stahlberg et al., 2018b): Our preprocessing includes Moses tokenization, punctuation normalization, truecasing, and joint subword segmentation using byte pair encoding (Sennrich et al., 2016c) with 32K merge operations. We compute cased BLEU scores with mteval-v13a.pl that are directly comparable with the official WMT scores.2 Our models are trained with the TensorFlow (Abadi et al., 2016) based Tensor2Tensor (Vaswani et al., 2018) library and decoded with our SGNMT framework (Stahlberg et al., 2017b, 2018c). We delay SGD updates (Saunders et al., 2018) to use larger training batch sizes than our technical infrastructure3 would normally allow with vanilla SGD by using the MultistepAdam optimizer in Tensor2Tensor. We use Transformer (Vaswani et al., 2017) models in two configurati"
W19-5340,P16-1009,0,0.547142,"ra-sentential and inter-sentential attention masks for an English example from news-test2017. Document-level context helps to predict the next word (‘vinyl’). We stop when the translation score difference to the first-best translation exceeds a threshold.1 3 T2T HParams set # physical GPUs Batch size SGD delay factor # training iterations Beam size Experimental Setup Our experimental setup is essentially the same as last year (Stahlberg et al., 2018b): Our preprocessing includes Moses tokenization, punctuation normalization, truecasing, and joint subword segmentation using byte pair encoding (Sennrich et al., 2016c) with 32K merge operations. We compute cased BLEU scores with mteval-v13a.pl that are directly comparable with the official WMT scores.2 Our models are trained with the TensorFlow (Abadi et al., 2016) based Tensor2Tensor (Vaswani et al., 2018) library and decoded with our SGNMT framework (Stahlberg et al., 2017b, 2018c). We delay SGD updates (Saunders et al., 2018) to use larger training batch sizes than our technical infrastructure3 would normally allow with vanilla SGD by using the MultistepAdam optimizer in Tensor2Tensor. We use Transformer (Vaswani et al., 2017) models in two configurati"
W19-5340,N19-1209,0,0.0173239,"trophic forgetting5 (Goodfellow et al., 2013; French, 1999) and over-fitting by keeping the adapted model close to the original, and can thus be seen as regularized fine-tuning techniques. Khayrallah et al. (2018); Dakwale and Monz (2017) regularized the output distributions during fine-tuning using techniques inspired by knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014; Kim and Rush, 2016). Barone et al. (2017) applied standard L2 regularization and a variant of dropout to domain adaptation. EWC as generalization of L2 regularization has been used for NMT domain adaptation by Thompson et al. (2019); Saunders et al. (2019). In particular, Saunders et al. (2019) showed that EWC is not only more effective than L2 in reducing catastrophic forgetting but even yields gains on the general domain when used for fine-tuning on a related domain. Language modelling We introduced our new Intra-Inter Transformer architecture for document-level language modelling in Sec. 2. Tab. 5 shows that our architecture achieves much better perplexity than both a sentence-level language model and a documentlevel vanilla Transformer model. Tab. 6 summarizes our translation results with various kinds of language mo"
W19-5340,P16-1162,0,0.694172,"ra-sentential and inter-sentential attention masks for an English example from news-test2017. Document-level context helps to predict the next word (‘vinyl’). We stop when the translation score difference to the first-best translation exceeds a threshold.1 3 T2T HParams set # physical GPUs Batch size SGD delay factor # training iterations Beam size Experimental Setup Our experimental setup is essentially the same as last year (Stahlberg et al., 2018b): Our preprocessing includes Moses tokenization, punctuation normalization, truecasing, and joint subword segmentation using byte pair encoding (Sennrich et al., 2016c) with 32K merge operations. We compute cased BLEU scores with mteval-v13a.pl that are directly comparable with the official WMT scores.2 Our models are trained with the TensorFlow (Abadi et al., 2016) based Tensor2Tensor (Vaswani et al., 2018) library and decoded with our SGNMT framework (Stahlberg et al., 2017b, 2018c). We delay SGD updates (Saunders et al., 2018) to use larger training batch sizes than our technical infrastructure3 would normally allow with vanilla SGD by using the MultistepAdam optimizer in Tensor2Tensor. We use Transformer (Vaswani et al., 2017) models in two configurati"
W19-5340,W17-4811,0,0.0278791,"erarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... chical attention (Miculicich et al., 2018; Maruf et al., 2019), deliberation networks (Xiong et al., 2018), or by simply concatenating multiple source and/or target sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018). Context-aware extensions to Transformer encoders have been proposed by Voita et al. (2018); Zhang et al. (2018). Techniques also differ in whether they use source context only (Jean et al., 2017; Wang et al., 2017a; Voita et al., 2018; Zhang et al., 2018), target context only (Tu et al., 2018; Kuang et al., 2017), or both (Bawden et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Tiedemann and Scherrer, 2017; Maruf et al., 2019). Several studies on document-level NMT indicate that automatic and human sentence-level evaluation metrics often do not correlate"
W19-5340,W18-6321,1,0.908682,"context. We report large perplexity reductions compared to sentence-level LMs under the new architecture. Our document-level LM yields small BLEU gains on top of strong NMT ensembles, and we hope to benefit even more from it in document-level human evaluation. Introduction Both fine-tuning and language modelling are techniques widely used for NMT. Fine-tuning is often used to adapt a model to a new domain (Luong and Manning, 2015), while ensembling neural machine translation (NMT) with neural language models (LMs) is an effective way to leverage monolingual data (Gulcehre et al., 2015, 2017; Stahlberg et al., 2018a). Our submission to the WMT19 news shared task relies on ideas from these two lines of research, but applies and combines them in novel ways. Our contributions are: • Even though the performance gap between NMT and traditional statistical machine translation (SMT) is growing rapidly on the task at hand, SMT can still improve very strong NMT ensembles. To combine NMT and SMT we follow Stahlberg et al. (2017a, 2018b) and build a specialized n-gram LM for each sentence that computes the risk of hypotheses relative to SMT lattices. • While data filtering was central in last year’s evaluation (Ko"
W19-5340,Q18-1029,0,0.0205767,"e-conditioned n-gram LMs yields gains even on top of our ensembles (row 4). Our document-level Intra-Inter language models improve the ensembles and the single En-De Base model, but hurt performance slightly for the single Big models (row 5). 5 Document-level MT Various techniques have been proposed to provide the translation system with inter-sentential context, for example by initializing encoder or decoder states (Wang et al., 2017a), using multi-source encoders (Bawden et al., 2018; Jean et al., 2017), as additional decoder input (Wang et al., 2017a), with memoryaugmented neural networks (Tu et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2017), hierarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... chical attention (Miculicich et al., 2018; Maruf et al., 2019), deliberation networks (Xiong et al., 2018), or by simply conca"
W19-5340,W18-6427,1,0.864639,"Missing"
W19-5340,W18-1819,0,0.0235889,"threshold.1 3 T2T HParams set # physical GPUs Batch size SGD delay factor # training iterations Beam size Experimental Setup Our experimental setup is essentially the same as last year (Stahlberg et al., 2018b): Our preprocessing includes Moses tokenization, punctuation normalization, truecasing, and joint subword segmentation using byte pair encoding (Sennrich et al., 2016c) with 32K merge operations. We compute cased BLEU scores with mteval-v13a.pl that are directly comparable with the official WMT scores.2 Our models are trained with the TensorFlow (Abadi et al., 2016) based Tensor2Tensor (Vaswani et al., 2018) library and decoded with our SGNMT framework (Stahlberg et al., 2017b, 2018c). We delay SGD updates (Saunders et al., 2018) to use larger training batch sizes than our technical infrastructure3 would normally allow with vanilla SGD by using the MultistepAdam optimizer in Tensor2Tensor. We use Transformer (Vaswani et al., 2017) models in two configurations (Tab. 1). Preliminary experiments are carried out with the ‘Base’ configuration while we use the ‘Big’ models for our final system. We use news-test2017 as development set to tune model weights and select checkpoints and news-test2018 as tes"
W19-5340,E17-2058,1,0.902934,"Missing"
W19-5340,P18-1117,0,0.019973,", but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Facebook FAIR 42.7 JHU 42.5 eTranslation 41.9 8 more... chical attention (Miculicich et al., 2018; Maruf et al., 2019), deliberation networks (Xiong et al., 2018), or by simply concatenating multiple source and/or target sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018). Context-aware extensions to Transformer encoders have been proposed by Voita et al. (2018); Zhang et al. (2018). Techniques also differ in whether they use source context only (Jean et al., 2017; Wang et al., 2017a; Voita et al., 2018; Zhang et al., 2018), target context only (Tu et al., 2018; Kuang et al., 2017), or both (Bawden et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Tiedemann and Scherrer, 2017; Maruf et al., 2019). Several studies on document-level NMT indicate that automatic and human sentence-level evaluation metrics often do not correlate well with improvements in discourse level phenomena (Bawden et al., 2018; L¨aubli et al., 2018; M¨uller et al., 20"
W19-5340,D17-1301,0,0.162518,"of) Big models with backtranslation (row 2 vs. 3). Extracting n-gram probabilities from traditional PBSMT lattices as described by Stahlberg et al. (2017a) and using them as source-conditioned n-gram LMs yields gains even on top of our ensembles (row 4). Our document-level Intra-Inter language models improve the ensembles and the single En-De Base model, but hurt performance slightly for the single Big models (row 5). 5 Document-level MT Various techniques have been proposed to provide the translation system with inter-sentential context, for example by initializing encoder or decoder states (Wang et al., 2017a), using multi-source encoders (Bawden et al., 2018; Jean et al., 2017), as additional decoder input (Wang et al., 2017a), with memoryaugmented neural networks (Tu et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2017), hierarRelated Work 5 Catastrophic forgetting occurs when the performance on the specific domain is improved after fine-tuning, but the performance of the model on the general domain has decreased drastically. Regularized fine-tuning Our approach to finetuning is a combination of EWC (Kirkpatrick 368 English-German Team BLEU MSRA 44.9 Microsoft 43.9 NEU 43.5 UCAM 43.0 Faceb"
W19-5340,D18-1049,0,0.0368862,"Missing"
W19-5340,I17-1016,0,0.0168159,"ntrast, Long et al. (2016) translated most of the sentence with an NMT system, and just used SMT to translate technical terms in a post-processing step. Dahlmann et al. (2017) proposed a hybrid search algorithm in which the neural decoder expands hypotheses with phrases from an SMT system. 6 NMT-SMT hybrid systems Popular examples of combining a fully trained SMT system with independently trained NMT are rescoring and reranking methods (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way, 2017). Wang et al. (2017b, 2018) interpolated NMT posteriors with word recommendations from SMT and jointly trained NMT together with a"
W19-5340,P17-2060,0,0.0229361,"s (Neubig et al., 2015; Stahlberg et al., 2016b; Khayrallah et al., 2017; Grundkiewicz and Junczys-Dowmunt, 2018; Avramidis et al., 2016; Marie and Fujita, 2018; Zhang et al., 2017), although these models may be too constraining if the neural system is much stronger than the SMT system. Loose combination schemes include the edit-distance-based system of Stahlberg et al. (2016a) or the minimum Bayes-risk approach of Stahlberg et al. (2017a) we adopted in this work. NMT and SMT can also be combined in a cascade, with SMT providing the input to a post-processing NMT system (Niehues et al., 2016; Zhou et al., 2017) or vice versa (Du and Way, 2017). Wang et al. (2017b, 2018) interpolated NMT posteriors with word recommendations from SMT and jointly trained NMT together with a gating function which assigns the weight between SMT and NMT scores dynamically. The AMU-UEDIN submission to WMT16 let SMT take the lead and used NMT as a feature in phrase-based MT (Junczys-Dowmunt et al., Conclusion Our WMT19 submission focused on regularized fine-tuning and language modelling. With our novel Intra-Inter Transformer architecture for document-level LMs we achieved significant reductions in perplexity and minor impr"
W19-5340,W16-2310,0,\N,Missing
W19-5340,W18-6453,0,\N,Missing
