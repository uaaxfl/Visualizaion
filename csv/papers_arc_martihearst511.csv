2021.naacl-main.255,News Headline Grouping as a Challenging {NLU} Task,2021,-1,-1,3,1,4008,philippe laban,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Recent progress in Natural Language Understanding (NLU) has seen the latest models outperform human performance on many standard tasks. These impressive results have led the community to introspect on dataset limitations, and iterate on more nuanced challenges. In this paper, we introduce the task of HeadLine Grouping (HLG) and a corresponding dataset (HLGD) consisting of 20,056 pairs of news headlines, each labeled with a binary judgement as to whether the pair belongs within the same group. On HLGD, human annotators achieve high performance of around 0.9 F-1, while current state-of-the art Transformer models only reach 0.75 F-1, opening the path for further improvements. We further propose a novel unsupervised Headline Generator Swap model for the task of HeadLine Grouping that achieves within 3 F-1 of the best supervised model. Finally, we analyze high-performing models with consistency tests, and find that models are not consistent in their predictions, revealing modeling limits of current architectures."
2021.findings-emnlp.266,Modeling Mathematical Notation Semantics in Academic Papers,2021,-1,-1,4,0,6893,hwiyeol jo,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Natural language models often fall short when understanding and generating mathematical notation. What is not clear is whether these shortcomings are due to fundamental limitations of the models, or the absence of appropriate tasks. In this paper, we explore the extent to which natural language models can learn semantics between mathematical notation and their surrounding text. We propose two notation prediction tasks, and train a model that selectively masks notation tokens and encodes left and/or right sentences as context. Compared to baseline models trained by masked language modeling, our method achieved significantly better performance at the two tasks, showing that this approach is a good first step towards modeling mathematical texts. However, the current models rarely predict unseen symbols correctly, and token-level predictions are more accurate than symbol-level predictions, indicating more work is needed to represent structural patterns. Based on the results, we suggest future works toward modeling mathematical texts."
2021.bea-1.17,Automatically Generating Cause-and-Effect Questions from Passages,2021,-1,-1,5,1,12240,katherine stasaski,Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications,0,"Automated question generation has the potential to greatly aid in education applications, such as online study aids to check understanding of readings. The state-of-the-art in neural question generation has advanced greatly, due in part to the availability of large datasets of question-answer pairs. However, the questions generated are often surface-level and not challenging for a human to answer. To develop more challenging questions, we propose the novel task of cause-and-effect question generation. We build a pipeline that extracts causal relations from passages of input text, and feeds these as input to a state-of-the-art neural question generator. The extractor is based on prior work that classifies causal relations by linguistic category (Cao et al., 2016; Altenberg, 1984). This work results in a new, publicly available collection of cause-and-effect questions. We evaluate via both automatic and manual metrics and find performance improves for both question generation and question answering when we utilize a small auxiliary data source of cause-and-effect questions for fine-tuning. Our approach can be easily applied to generate cause-and-effect questions from other text collections and educational material, allowing for adaptable large-scale generation of cause-and-effect questions."
2021.acl-short.134,Can Transformer Models Measure Coherence In Text: Re-Thinking the Shuffle Test,2021,-1,-1,4,1,4008,philippe laban,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"The Shuffle Test is the most common task to evaluate whether NLP models can measure coherence in text. Most recent work uses direct supervision on the task; we show that by simply finetuning a RoBERTa model, we can achieve a near perfect accuracy of 97.8{\%}, a state-of-the-art. We argue that this outstanding performance is unlikely to lead to a good model of text coherence, and suggest that the Shuffle Test should be approached in a Zero-Shot setting: models should be evaluated without being trained on the task itself. We evaluate common models in this setting, such as Generative and Bi-directional Transformers, and find that larger architectures achieve high-performance out-of-the-box. Finally, we suggest the k-Block Shuffle Test, a modification of the original by increasing the size of blocks shuffled. Even though human reader performance remains high (around 95{\%} accuracy), model performance drops from 94{\%} to 78{\%} as block size increases, creating a conceptually simple challenge to benchmark NLP models."
2021.acl-long.498,Keep It Simple: Unsupervised Simplification of Multi-Paragraph Text,2021,-1,-1,4,1,4008,philippe laban,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"This work presents Keep it Simple (KiS), a new approach to unsupervised text simplification which learns to balance a reward across three properties: fluency, salience and simplicity. We train the model with a novel algorithm to optimize the reward (k-SCST), in which the model proposes several candidate simplifications, computes each candidate{'}s reward, and encourages candidates that outperform the mean reward. Finally, we propose a realistic text comprehension task as an evaluation method for text simplification. When tested on the English news domain, the KiS model outperforms strong supervised baselines by more than 4 SARI points, and can help people complete a comprehension task an average of 18{\%} faster while retaining accuracy, when compared to the original text."
2020.sdp-1.22,"Document-Level Definition Detection in Scholarly Documents: Existing Models, Error Analyses, and Future Directions",2020,-1,-1,6,0.42214,7072,dongyeop kang,Proceedings of the First Workshop on Scholarly Document Processing,0,"The task of definition detection is important for scholarly papers, because papers often make use of technical terminology that may be unfamiliar to readers. Despite prior work on definition detection, current approaches are far from being accurate enough to use in realworld applications. In this paper, we first perform in-depth error analysis of the current best performing definition detection system and discover major causes of errors. Based on this analysis, we develop a new definition detection system, HEDDEx, that utilizes syntactic features, transformer encoders, and heuristic filters, and evaluate it on a standard sentence-level benchmark. Because current benchmarks evaluate randomly sampled sentences, we propose an alternative evaluation that assesses every sentence within a document. This allows for evaluating recall in addition to precision. HEDDEx outperforms the leading system on both the sentence-level and the document-level tasks, by 12.7 F1 points and 14.4 F1 points, respectively. We note that performance on the high-recall document-level task is much lower than in the standard evaluation approach, due to the necessity of incorporation of document structure as features. We discuss remaining challenges in document-level definition detection, ideas for improvements, and potential issues for the development of reading aid applications."
2020.emnlp-demos.18,{S}ci{S}ight: Combining faceted navigation and research group detection for {COVID}-19 exploratory scientific search,2020,-1,-1,7,0,4307,tom hope,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"The COVID-19 pandemic has sparked unprecedented mobilization of scientists, generating a deluge of papers that makes it hard for researchers to keep track and explore new directions. Search engines are designed for targeted queries, not for discovery of connections across a corpus. In this paper, we present SciSight, a system for exploratory search of COVID-19 research integrating two key capabilities: first, exploring associations between biomedical facets automatically extracted from papers (e.g., genes, drugs, diseases, patient outcomes); second, combining textual and network information to search and visualize groups of researchers and their ties. SciSight has so far served over 15K users with over 42K page views and 13{\%} returns."
2020.bea-1.5,{CIMA}: A Large Open Access Dialogue Dataset for Tutoring,2020,-1,-1,3,1,12240,katherine stasaski,Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"One-to-one tutoring is often an effective means to help students learn, and recent experiments with neural conversation systems are promising. However, large open datasets of tutoring conversations are lacking. To remedy this, we propose a novel asynchronous method for collecting tutoring dialogue via crowdworkers that is both amenable to the needs of deep learning algorithms and reflective of pedagogical concerns. In this approach, extended conversations are obtained between crowdworkers role-playing as both students and tutors. The CIMA collection, which we make publicly available, is novel in that students are exposed to overlapping grounded concepts between exercises and multiple relevant tutoring responses are collected for the same input. CIMA contains several compelling properties from an educational perspective: student role-players complete exercises in fewer turns during the course of the conversation and tutor players adopt strategies that conform with some educational conversational norms, such as providing hints versus asking questions in appropriate contexts. The dataset enables a model to be trained to generate the next tutoring utterance in a conversation, conditioned on a provided action strategy."
2020.acl-main.446,More Diverse Dialogue Datasets via Diversity-Informed Data Collection,2020,-1,-1,3,1,12240,katherine stasaski,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Automated generation of conversational dialogue using modern neural architectures has made notable advances. However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem. We introduce a new strategy to address this problem, called Diversity-Informed Data Collection. Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpus-level statistics to determine which conversational participants to collect data from. Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation. This method is generalizable and can be used with other corpus-level metrics."
2020.acl-main.460,The Summary Loop: Learning to Write Abstractive Summaries Without Examples,2020,-1,-1,4,1,4008,philippe laban,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary. A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score summaries. When tested on popular news summarization datasets, the method outperforms previous unsupervised methods by more than 2 R-1 points, and approaches results of competitive supervised methods. Our model attains higher levels of abstraction with copied passages roughly two times shorter than prior work, and learns to compress and merge sentences without supervision."
2020.acl-demos.43,What{'}s The Latest? A Question-driven News Chatbot,2020,-1,-1,3,1,4008,philippe laban,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This work describes an automatic news chatbot that draws content from a diverse set of news articles and creates conversations with a user about the news. Key components of the system include the automatic organization of news articles into topical chatrooms, integration of automatically generated questions into the conversation, and a novel method for choosing which questions to present which avoids repetitive suggestions. We describe the algorithmic framework and present the results of a usability study that shows that news readers using the system successfully engage in multi-turn conversations about specific news stories."
W19-3001,Towards augmenting crisis counselor training by improving message retrieval,2019,0,0,2,0,13093,orianna demasi,Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology,0,"A fundamental challenge when training counselors is presenting novices with the opportunity to practice counseling distressed individuals without exacerbating a situation. Rather than replacing human empathy with an automated counselor, we propose simulating an individual in crisis so that human counselors in training can practice crisis counseling in a low-risk environment. Towards this end, we collect a dataset of suicide prevention counselor role-play transcripts and make initial steps towards constructing a CRISISbot for humans to counsel while in training. In this data-constrained setting, we evaluate the potential for message retrieval to construct a coherent chat agent in light of recent advances with text embedding methods. Our results show that embeddings can considerably improve retrieval approaches to make them competitive with generative models. By coherently retrieving messages, we can help counselors practice chatting in a low-risk environment."
W17-5034,Multiple Choice Question Generation Utilizing An Ontology,2017,19,9,2,1,12240,katherine stasaski,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Ontologies provide a structured representation of concepts and the relationships which connect them. This work investigates how a pre-existing educational Biology ontology can be used to generate useful practice questions for students by using the connectivity structure in a novel way. It also introduces a novel way to generate multiple-choice distractors from the ontology, and compares this to a baseline of using embedding representations of nodes. An assessment by an experienced science teacher shows a significant advantage over a baseline when using the ontology for distractor generation. A subsequent study with three science teachers on the results of a modified question generation algorithm finds significant improvements. An in-depth analysis of the teachers{'} comments yields useful insights for any researcher working on automated question generation for educational applications."
W17-2701,news{L}ens: building and visualizing long-ranging news stories,2017,10,1,2,1,4008,philippe laban,Proceedings of the Events and Stories in the News Workshop,0,"We propose a method to aggregate and organize a large, multi-source dataset of news articles into a collection of major stories, and automatically name and visualize these stories in a working system. The approach is able to run online, as new articles are added, processing 4 million news articles from 20 news sources, and extracting 80000 major stories, some of which span several years. The visual interface consists of lanes of timelines, each annotated with information that is deemed important for the story, including extracted quotations. The working system allows a user to search and navigate 8 years of story information."
W16-0526,Augmenting Course Material with Open Access Textbooks,2016,6,1,2,0,34081,smitha milli,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,None
W16-0203,Intersecting Word Vectors to Take Figurative Language to New Heights,2016,13,5,4,0,34145,andrea gagliano,Proceedings of the Fifth Workshop on Computational Linguistics for Literature,0,None
N16-1134,Patterns of Wisdom: Discourse-Level Style in Multi-Sentence Quotations,2016,15,3,2,0,34147,kyle booten,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
P15-1120,Can Natural Language Processing Become Natural Language Coaching?,2015,44,5,1,1,4010,marti hearst,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"How we teach and learn is undergoing a revolution, due to changes in technology and connectivity. Education may be one of the best application areas for advanced NLP techniques, and NLP researchers have much to contribute to this problem, especially in the areas of learning to write, mastery learning, and peer learning. In this paper I consider what happens when we convert natural language processors into natural language coaches. 1 Why Should You Care, NLP Researcher? There is a revolution in learning underway. Students are taking Massive Open Online Courses as well as online tutorials and paid online courses. Technology and connectivity makes it possible for students to learn from anywhere in the world, at any time, to fit their schedules. And in todayxe2x80x99s knowledge-based economy, going to school only in onexe2x80x99s early years is no longer enough; in future most people are going to need continuous, lifelong education. Students are changing too xe2x80x94 they expect to interact with information and technology. Fortunately, pedagogical research shows significant benefits of active learning over passive methods. The modern view of teaching means students work actively in class, talk with peers, and are coached more than graded by their instructors. In this new world of education, there is a great need for NLP research to step in and help. I hope in this paper to excite colleagues about the possibilities and suggest a few new ways of looking at them. I do not attempt to cover the field of language and learning comprehensively, nor do I claim there is no work in the field. In fact there is quite a bit, such as a recent special issue on language learning resources (Sharoff et al., 2014), the long running ACL workshops on Building Educational Applications using NLP (Tetreault et al., 2015), and a recent shared task competition on grammatical error detection for second language learners (Ng et al., 2014). But I hope I am casting a few interesting thoughts in this direction for those colleagues who are not focused on this particular topic."
P14-2045,Improving the Recognizability of Syntactic Relations Using Contextualized Examples,2014,21,0,2,0,39136,aditi muralidharan,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"A common task in qualitative data analysis is to characterize the usage of a linguistic entity by issuing queries over syntactic relations between words. Previous interfaces for searching over syntactic structures require programming-style queries. User interface research suggests that it is easier to recognize a pattern than to compose it from scratch; therefore, interfaces for non-experts should show previews of syntactic relations. What these previews should look like is an open question that we explored with a 400-participant Mechanical Turk experiment. We found that syntactic relations are recognized with 34% higher accuracy when contextual examples are shown than a baseline of naming the relations alone. This suggests that user interfaces should display contextual examples of syntactic relations to help users choose between different relations."
W09-3608,{NLP} Support for Faceted Navigation in Scholarly Collection,2009,-1,-1,1,1,4010,marti hearst,Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries ({NLPIR}4{DL}),0,None
P08-1052,Solving Relational Similarity Problems Using the Web as a Corpus,2008,26,58,2,1,1636,preslav nakov,Proceedings of ACL-08: HLT,1,"We present a simple linguistically-motivated method for characterizing the semantic relations that hold between two nouns. The approach leverages the vast size of the Web in order to build lexically-specific features. The main idea is to look for verbs, prepositions, and coordinating conjunctions that can help make explicit the hidden relations between the target nouns. Using these features in instance-based classifiers, we demonstrate state-of-the-art results on various relational similarity problems, including mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER, characterizing noun-noun compounds in terms of abstract linguistic predicates like CAUSE, USE, and FROM, classifying the relations between nominals in context, and solving SAT verbal analogy problems. In essence, the approach puts together some existing ideas, showing that they apply generally to various semantic tasks, finding that verbs are especially useful features."
P08-1080,Improving Search Results Quality by Customizing Summary Lengths,2008,19,49,2,0,43582,michael kaisser,Proceedings of ACL-08: HLT,1,"Web search engines today typically show results as a list of titles and short snippets that summarize how the retrieved documents are related to the query. However, recent research suggests that longer summaries can be preferable for certain types of queries. This paper presents empirical evidence that judges can predict appropriate search result summary lengths, and that perceptions of search result quality can be affected by varying these result lengths. These findings have important implications for search results presentation, especially for natural language queries."
W07-1010,Exploring the Efficacy of Caption Search for Bioscience Journal Search Interfaces,2007,20,16,1,1,4010,marti hearst,"Biological, translational, and clinical language processing",0,"This paper presents the results of a pilot usability study of a novel approach to search user interfaces for bioscience journal articles. The main idea is to support search over figure captions explicitly, and show the corresponding figures directly within the search results. Participants in a pilot study expressed surprise at the idea, noting that they had never thought of search in this way. They also reported strong positive reactions to the idea: 7 out of 8 said they would use a search system with this kind of feature, suggesting that this is a promising idea for journal article search."
W07-0730,{UCB} System Description for the {WMT} 2007 Shared Task,2007,8,9,2,1,1636,preslav nakov,Proceedings of the Second Workshop on Statistical Machine Translation,0,"For the WMT 2007 shared task, the UC Berkeley team employed three techniques of interest. First, we used monolingual syntactic paraphrases to provide syntactic variety to the source training set sentences. Second, we trained two language models: a small in-domain model and a large out-of-domain model. Finally, we made use of results from prior research that shows that cognate pairs can improve word alignments. We contributed runs translating English to Spanish, French, and German using various combinations of these techniques."
S07-1080,{UCB}: System Description for {S}em{E}val Task {\\#}4,2007,13,13,2,1,1636,preslav nakov,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"The UC Berkeley team participated in the SemEval 2007 Task #4, with an approach that leverages the vast size of the Web in order to build lexically-specific features. The idea is to determine which verbs, prepositions, and conjunctions are used in sentences containing a target word pair, and to compare those to features extracted for other word pairs in order to determine which are most similar. By combining these Web features with words from the sentence context, our team was able to achieve the best results for systems of category C and third best for systems of category A."
N07-1031,Automating Creation of Hierarchical Faceted Metadata Structures,2007,18,108,2,1,46862,emilia stoica,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"We describe Castanet, an algorithm for automatically generating hierarchical faceted metadata from textual descriptions of items, to be incorporated into browsing and navigation interfaces for large information collections. From an existing lexical database (such as WordNet), Castanet carves out a structure that reflects the contents of the target information collection; moderate manual modifications improve the outcome. The algorithm is simple yet effective: a study conducted with 34 information architects finds that Castanet achieves higher quality results than other automated category creation algorithms, and 85% of the study participants said they would like to use the system for their work."
D07-1089,Multiple Alignment of Citation Sentences with Conditional Random Fields and Posterior Decoding,2007,29,7,3,1,49358,ariel schwartz,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"In scientific literature, sentences that cite related work can be a valuable resource for applications such as summarization, synonym identification, and entity extraction. In order to determine which equivalent entities are discussed in the various citation sentences, we propose aligning the words within thesesentences accordingto semantic similarity. This problem is partly analogous to the problem of multiple sequence alignment in the biosciences, and is also closely related to the word alignment problem in statistical machine translation. In this paper we address the problem of multiple citation concept alignment by combining and modifying the CRF based pairwise word alignment system of Blunsom & Cohn (2006) and a posterior decoding based multiple sequence alignment algorithm of Schwartz & Pachter (2007). We evaluate the algorithm on hand-labeled data, achieving results that improve on a baseline."
W06-3326,Summarizing Key Concepts using Citation Sentences,2006,4,15,2,1,49358,ariel schwartz,Proceedings of the {HLT}-{NAACL} {B}io{NLP} Workshop on Linking Natural Language and Biology,0,"Citations have great potential to be a valuable resource in mining the bioscience literature (Nakov et al., 2004). The text around citations (or citances) tends to state biological facts with reference to the original papers that discovered them. The cited facts are typically stated in a more concise way in the citing papers than in the original. We hypothesize that in many cases, as time goes by, the citation sentences can more accurately indicate the most important contributions of a paper than its original abstract."
W05-0603,Search Engine Statistics Beyond the n-Gram: Application to Noun Compound Bracketing,2005,10,78,2,1,1636,preslav nakov,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"In order to achieve the long-range goal of semantic interpretation of noun compounds, it is often necessary to first determine their syntactic structure. This paper describes an unsupervised method for noun compound bracketing which extracts statistics from Web search engines using a X2 measure, a new set of surface features, and paraphrases. On a gold standard, the system achieves results of 89.34% (baseline 66.80%), which is a sizable improvement over the state of the art (80.70%)."
W05-0101,Teaching Applied Natural Language Processing: Triumphs and Tribulations,2005,8,7,1,1,4010,marti hearst,Proceedings of the Second {ACL} Workshop on Effective Tools and Methodologies for Teaching {NLP} and {CL},0,"In Fall 2004 I introduced a new course called Applied Natural Language Processing, in which students acquire an understanding of which text analysis techniques are currently feasible for practical applications. The class was intended for interdisciplinary students with a somewhat technical background. This paper describes the topics covered and the programming exercises, emphasizing which aspects were successful and which problematic, and makes recommendations for future versions of the course."
P05-3017,Supporting Annotation Layers for Natural Language Processing,2005,17,8,4,1,1636,preslav nakov,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"We demonstrate a system for flexible querying against text that has been annotated with the results of NLP processing. The system supports self-overlapping and parallel layers, integration of syntactic and ontological hierarchies, flexibility in the format of returned results, and tight integration with SQL. We present a query language and its use on examples taken from the NLP literature."
H05-1092,Multi-way Relation Classification: Application to Protein-Protein Interactions,2005,16,60,2,1,44606,barbara rosario,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We address the problem of multi-way relation classification, applied to identification of the interactions between proteins in bioscience text. A major impediment to such work is the acquisition of appropriately labeled training data; for our experiments we have identified a database that serves as a proxy for training data. We use two graphical models and a neural net for the classification of the interactions, achieving an accuracy of 64% for a 10-way distinction between relation types. We also provide evidence that the exploitation of the sentences surrounding a citation to a paper can yield higher accuracy than other sentences."
H05-1105,Using the Web as an Implicit Training Set: Application to Structural Ambiguity Resolution,2005,26,68,2,1,1636,preslav nakov,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Recent work has shown that very large corpora can act as training data for NLP algorithms even without explicit labels. In this paper we show how the use of surface features and paraphrases in queries against search engines can be used to infer labels for structural ambiguity resolution tasks. Using unsupervised algorithms, we achieve 84% precision on PP-attachment and 80% on noun compound coordination."
P04-1055,Classifying Semantic Relations in Bioscience Texts,2004,19,211,2,1,44606,barbara rosario,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"A crucial step toward the goal of automatic extraction of propositional information from natural language text is the identification of semantic relations between constituents in sentences. We examine the problem of distinguishing among seven relation types that can occur between the entities treatment and disease in bioscience text, and the problem of identifying such entities. We compare five generative graphical models and a neural network, using lexical, syntactic, and semantic features, finding that the latter help achieve high classification accuracy."
N04-4030,Nearly-Automated Metadata Hierarchy Creation,2004,16,44,2,1,46862,emilia stoica,Proceedings of {HLT}-{NAACL} 2004: Short Papers,0,"Currently, information architects create metadata category hierarchies manually. We present a nearly-automated approach for deriving such hierarchies, by converting the lexical hierarchy WordNet into a format that reflects the contents of a target information collection. We use the term nearly-automated because an information architect should have to make only small adjustments to produce an acceptable metadata structure. We contrast the results with an algorithm that uses lexical co-occurrence statistics."
N03-2023,Category-based Pseudowords,2003,5,20,2,1,1636,preslav nakov,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,"A pseudoword is a composite comprised of two or more words chosen at random; the individual occurrences of the original words within a text are replaced by their conflation. Pseudowords are a useful mechanism for evaluating the impact of word sense ambiguity in many NLP applications. However, the standard method for constructing pseudowords has some drawbacks. Because the constituent words are chosen at random, the word contexts that surround pseudowords do not necessarily reflect the contexts that real ambiguous words occur in. This in turn leads to an optimistic upper bound on algorithm performance. To address these drawbacks, we propose the use of lexical categories to create more realistic pseudowords, and evaluate the results of different variations of this idea against the standard approach."
P02-1032,"The Descent of Hierarchy, and Selection in Relational Semantics",2002,20,64,2,1,44606,barbara rosario,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"In many types of technical texts, meaning is embedded in noun compounds. A language understanding program needs to be able to interpret these in order to ascertain sentence meaning. We explore the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories, and then using this category membership to determine the relation that holds between the nouns. In this paper we present the results of an analysis of this method on two-word noun compounds from the biomedical domain, obtaining classification accuracy of approximately 90%. Since lexical hierarchies are not necessarily ideally suited for this task, we also pose the question: how far down the hierarchy must the algorithm descend before all the terms within the subhierarchy behave uniformly with respect to the semantic relation in question? We find that the topmost levels of the hierarchy yield an accurate classification, thus providing an economic way of assigning relations to noun compounds."
J02-1002,A Critique and Improvement of an Evaluation Metric for Text Segmentation,2002,0,0,2,0,53579,lev pevzner,Computational Linguistics,0,"The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms. However, a theoretical analysis ..."
W01-0511,Classifying the Semantic Relations in Noun Compounds via a Domain-Specific Lexical Hierarchy,2001,24,141,2,1,44606,barbara rosario,Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,0,"We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves."
P99-1001,Untangling Text Data Mining,1999,34,616,1,1,4010,marti hearst,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"The possibilities for data mining from large text collections are virtually untapped. Text expresses a vast, rich range of information, but encodes this information in a form that is difficult to decipher automatically. Perhaps for this reason, there has been little work in text data mining to date, and most people who have talked about it have either conflated it with information access or have not made use of text directly to discover heretofore unknown information.n n In this paper I will first define data mining, information access, and corpus-based computational linguistics, and then discuss the relationship of these to text data mining. The intent behind these contrasts is to draw attention to exciting new kinds of problems for computational linguists. I describe examples of what I consider to be real text data mining efforts and briefly outline recent ideas about how to pursue exploratory data analysis over text."
J97-2002,Adaptive Multilingual Sentence Boundary Disambiguation,1997,35,133,2,0.833333,51834,david palmer,Computational Linguistics,0,"The sentence is a standard textual unit in natual language processing applications. In many language the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection.As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on Franch and German."
J97-1003,Text Tiling: Segmenting Text into Multi-paragraph Subtopic Passages,1997,70,1019,1,1,4010,marti hearst,Computational Linguistics,0,"TextTiling is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics. The discourse cues for identifying major subtopic shifts are patterns of lexical co-occurrence and distribution. The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts. Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including information retrieval and summarization."
P94-1002,Multi-Paragraph Segmentation Expository Text,1994,27,484,1,1,4010,marti hearst,32nd Annual Meeting of the Association for Computational Linguistics,1,"This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts. The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes. Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts."
A94-1013,Adaptive Sentence Boundary Disambiguation,1994,10,54,2,0.833333,51834,david palmer,Fourth Conference on Applied Natural Language Processing,0,"Labeling of sentence boundaries is a necessary prerequisite for many natural language processing tasks, including part-of-speech tagging and sentence alignment. End-of-sentence punctuation marks are ambiguous; to disambiguate them most systems use brittle, special-purpose regular expression grammars and exception rules. As an alternative, we have developed an efficient, trainable algorithm that uses a lexicon with part-of-speech probabilities and a feed-forward neural network. This work demonstrates the feasibility of using prior probabilities of part-of-speech assignments, as opposed to words or definite part-of-speech assignments, as contextual information. After training for less than one minute, the method correctly labels over 98.5% of sentence boundaries in a corpus of over 27,000 sentence-boundary marks. We show the method to be efficient and easily adaptable to different text genres, including single-case texts."
W93-0307,Structural Ambiguity and Conceptual Relations,1993,16,32,2,0,8279,philip resnik,{V}ery {L}arge {C}orpora: Academic and Industrial Perspectives,0,"Lexical co-occurrence statistics are becoming widely used in the syntactic analysis of unconstrained text. However, analyses based solely on lexical relationships suffer from sparseness of data: it is sometimes necessary to use a less informed model in order to reliably estimate statistical parameters. For example, the lexical association strategy for resolving ambiguous prepositional phrase attachments [Hindle and Rooth. 1991] takes into account only the attachment site (a verb or its direct object) and the preposition, ignoring the object of the preposition. We investigated an extension of the lexical association strategy to make use of noun class information, thus permitting a disambiguation strategy to take more information into account. Although in preliminary experiments the extended strategy did not yield improved performance over lexical association alone. a qualitative analysis of the results suggests that the problem lies not in the noun class information, but rather in the multiplicity of classes available for each noun in the absence of sense disambiguation. This suggests several possible revisions of our proposal. 1. P r e f e r e n c e S t r a t e g i e s Prepositional phrase attachment is a paradigmatic case of the structural ambiguity problems faced by natural language parsing systems. Most models of grammar will not constrain the analysis of such attachments in examples like (1): the grammar simply specifies that a prepositional phrase such as on computer theft can be attached in several ways, and leaves the problem of selecting the correct choice to some other process. (1) a. Eventually, Mr. Stoll was invited to both the CIA and NSA to brief high-ranking officers on computer theft. b. Eventually, Mr. Stoll was invited to both the ClA and NSA [to brief [high-ranking officers on computer theft]]. c. Eventually, Mr. Stoll was invited to both the CIA and NSA [to brief [high-ranking ollicers] [on computer theft]]. As [Church and Patil, 1982] point out, the number of analyses given combinations of such all ways ambiguous constructions grows rapidly even for sentences of quite Marti A. Hearst Computer Science Division 465 Evans Hall University of California, Berkeley Berkeley, CA 94720 USA mar t i @ c s . b e r k e l e y . e d u reasonable length, so this other process has an important role to play. Discussions of sentence processing have focused primarily on structurally-based preference strategies such as right association and minimal attachment [Kimball, 1973; Frazier, 1979; Ford et al., 1982]; [Hobbs and Bear, 1990], while acknowledging the importance of semantics and pragmatics in attachment decisions, propose two syntactically-based attachment rules that are meant to be generalizations of those structural strategies. Others, however, have argued that syntactic considerations alone are insumcient for determining prepositional phrase attachments, suggesting instead that preference relationships among lexical items are the crucial factor. For example: [Wilks et aL, 1985] argue that the right attachment rules posited by [Frazier, 1979] are incorrect for phrases in general, and supply counterexarnples. They further argue that lexical preferences alone as suggested by [Ford et al., 1982] are too simplistic, and suggest instad the use of preference semantics. In the preference semantics framework, attachment relations of phrases are determined by comparing the preferences emanating from all the entities involved in the attachment, until the best mutual fit is found. Their CASSEX system represents the various meanings of the preposition in terms of (a) the preferred semantic class of the noun or verb that proceeds the preposition (e.g., move, be, strike), (b) the case of the preposition (e.g., instrument, time, loc.static), and (c) the preferred semantic class of the head noun of the prepositional phrase (e.g., physob, event). The difficult part of this method is the identification of preference relationships and particularly determining the strengths of the preferences and how they should interact. (See also discussion in [Schubert, 19841.) lDahlgren and McDowell, 1986] also suggests using preferences based on hand-built knowledge about the prepositions and their objects, specifying a simpler set of rules than those of [Wilks et al., 1985]."
W93-0106,Customizing a Lexicon to Better Suit a Computational Task,1993,-1,-1,1,1,4010,marti hearst,Acquisition of Lexical Knowledge from Text,0,None
C92-2082,Automatic Acquisition of Hyponyms from Large Text Corpora,1992,17,2618,1,1,4010,marti hearst,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested."
