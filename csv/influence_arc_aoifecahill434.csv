2020.bea-1.19,P18-1041,0,0.0294585,"ote that system 2 does not take any explicit math features into account, and the mathematical expressions are assumed to be captured through character level features. System 3 (SVRmsw ) is a feature-based SVR taking into account both textual context (through word-ngrams and syntactic dependencies) as well as explicit math features, but no character-level ngrams. Our final system is a recurrant neural network (RNN) system. The RNN model uses pre-trained word embeddings encoded by a bidirectional gated recurrent unit (GRU). The hidden states of the GRU are aggregated by a max pooling mechanism (Shen et al., 2018). The output of the encoder is aggregated in a fully-connected feedforward layer with sigmoid activation to predict the score of the response. This architecture has achieved state-of-the-art performance on the ASAP-SAS benchmark dataset (Riordan et al., 2019). Additional information about steps to replicate the system can be found in the Appendix. 4 Experiments We conduct a set of experiments to answer the following research questions: 188 1. How important is textual context for responses involving mathematical expressions with respect to automated scoring? (Comparing Exp 0 and Exp 1) 2. Do ch"
2020.bea-1.2,P11-1067,0,0.0375319,"onfirming previous findings that automated scoring systems are likely to be robust to random noise in the data. At the same time, the choice of evaluation set led to very different estimates of system performance regardless of what data was used to train the system. Related work The effect of noisy labels on machine learning algorithms has been extensively studied in terms of their effect on system training in both general machine learning literature (see, for example, Frénay and Verleysen (2014) for a comprehensive review), NLP (Reidsma and Carletta, 2008; Beigman Klebanov and Beigman, 2009; Schwartz et al., 2011; Plank et al., 2014; Martínez Alonso et al., 2015; Jamison and Gurevych, 2015) and automated scoring (Horbach et al., 2014; Zesch et al., 2015). One key insight that emerged from such work is that the nature of the noise is extremely important for the system performance. Machine learning algorithms are greatly affected by systematic noise but are less sensitive to random noise (Reidsma and Carletta, 2008; Reidsma and op den Akker, 2008). A typical case of random noise is when the labeling is done by multiple annotators which minimizes the individual bias introduced by any single annotator. Fo"
2020.bea-1.2,W18-0501,1,0.826751,"011) and the inter-rater agreement can vary substantially across prompts as well as across applications. For example, in the ASAP-AES data (Shermis, 2014), the agreement varies from Pearson’s r=0.63 to r=0.85 across “essay sets” (writing prompts) . In many automated scoring studies, the data for training and evaluating the system are randomly sampled from the same dataset, which means that the quality of human labels may affect both system training and evaluation. Notably, the effect of label quality on training and evaluation may not be the same. Previous studies (Reidsma and Carletta, 2008; Loukina et al., 2018) suggest that when annotation noise is relatively random, a system trained on noisier annotations may perform as well as a system trained on clean annotations. On the other hand, noise in the human labels used for evaluation 18 th Proceedings of the 15 Workshop on Innovative Use of NLP for Building Educational Applications, pages 18–29 c July 10, 2020. 2020 Association for Computational Linguistics and machine-human agreement. We then show that one possible solution is to use proportional reduction in mean squared error (PRMSE) (Haberman, 2008), a metric developed in the educational measuremen"
2020.bea-1.2,D08-1027,0,0.339019,"Missing"
2020.bea-1.2,W17-5052,1,0.847526,"uman labels to adjust 19 performance estimates. We further explore how agreement between human raters affects the evaluation of automated scoring systems. We focus on a specific case where the human rating process is organized in such a way that annotator bias is minimized. In other words, the label noise can be considered random. We also assume that the scores produced by an automated scoring system are on a continuous scale. This is typical for many automated scoring contexts including essay scoring (Shermis, 2014), speech scoring (Zechner et al., 2009) and, to some extent, content scoring (Madnani et al., 2017a; Riordan et al., 2019) but, of course, not for all possible contexts: for example, some of the SemEval 2013 shared tasks on short answer scoring (Dzikovska et al., 2016) use a different scoring approach. 3 each rater, its score for a response was modeled as the gold-standard score for the response plus a random error. We model different groups of raters: with low (inter-rater correlation r=0.4), moderate (r=0.55), average (r=0.65) and high (r=0.8) agreement. The correlations for different categories were informed by correlations we have observed in empirical data from various studies. The er"
2020.bea-1.2,W17-1605,1,0.909232,"Missing"
2020.bea-1.2,W15-0625,0,0.0211173,"Reidsma and Carletta (2008) used simulated data to explore the effect of noisy labels on classifier performance. They showed that the performance of the model, measured using Cohen’s Kappa, when evaluated against the ‘real’ (or gold-standard) labels was higher than the performance when evaluated against the ‘observed’ labels with added random noise. This is because for some instances, the classifier’s predictions were correct, but the ‘observed’ Metrics such as Pearson’s correlation or quadratically-weighted kappa, commonly used to evaluate automated scoring systems (Williamson et al., 2012; Yannakoudakis and Cummins, 2015; Haberman, 2019), compare automated scores to observed human scores without correcting for any errors in human scores. In order to account for differences in human-human agreement, these are then compared to the same metrics computed for the human raters using measures such as “degradation”: the difference between human-human and humanmachine agreement (Williamson et al., 2012). In this paper, we build on findings from the educational measurement community to explore an alternative approach where estimates of system performance are corrected for measurement error in the human labels. Classica"
2020.bea-1.2,N15-1152,0,0.0210417,"ystems are likely to be robust to random noise in the data. At the same time, the choice of evaluation set led to very different estimates of system performance regardless of what data was used to train the system. Related work The effect of noisy labels on machine learning algorithms has been extensively studied in terms of their effect on system training in both general machine learning literature (see, for example, Frénay and Verleysen (2014) for a comprehensive review), NLP (Reidsma and Carletta, 2008; Beigman Klebanov and Beigman, 2009; Schwartz et al., 2011; Plank et al., 2014; Martínez Alonso et al., 2015; Jamison and Gurevych, 2015) and automated scoring (Horbach et al., 2014; Zesch et al., 2015). One key insight that emerged from such work is that the nature of the noise is extremely important for the system performance. Machine learning algorithms are greatly affected by systematic noise but are less sensitive to random noise (Reidsma and Carletta, 2008; Reidsma and op den Akker, 2008). A typical case of random noise is when the labeling is done by multiple annotators which minimizes the individual bias introduced by any single annotator. For example, in a study on crowdsourcing NLP tasks,"
2020.coling-main.76,S13-2046,0,0.0643111,"Missing"
2020.coling-main.76,W17-5908,1,0.884272,"Missing"
2020.coling-main.76,N18-1170,0,0.025134,"(Ebrahimi et al., 2017) generates textual adversarial examples by swapping one character with another by gradient computation, and Textbugger (Li et al., 2018) by inserting spaces or swapping random letters. These methods are quite likely to leave the semantics of the underlying text unchanged, which is much harder to achieve when texts are manipulated on the token level. A common strategy is to replace single words (usually nouns) with near-synonyms chosen by humans (Kuleshov et al., 2018) or as nearest neighbors in an embedding space (Pennington et al., 2014). At the sentence level, SCPNs (Iyyer et al., 2018) produce a paraphrase of a given sentence without changing the original meaning. In the educational domain, this is comparable to unusual, atypical or creative correct answers that run the risk of being classified as incorrect (Yoon et al., 2018). However, none of the generation methods above is suitable to our research. If we use a real, correct answer as starting point for transformation into an adversarial one and leave the semantic meaning unchanged, we generate a correct answer. But we need answers which are definitely wrong in content as adversarial answers, in order to test the system’s"
2020.coling-main.76,D14-1162,0,0.0833981,"owing the similar principle in NLP applications, HotFlip (Ebrahimi et al., 2017) generates textual adversarial examples by swapping one character with another by gradient computation, and Textbugger (Li et al., 2018) by inserting spaces or swapping random letters. These methods are quite likely to leave the semantics of the underlying text unchanged, which is much harder to achieve when texts are manipulated on the token level. A common strategy is to replace single words (usually nouns) with near-synonyms chosen by humans (Kuleshov et al., 2018) or as nearest neighbors in an embedding space (Pennington et al., 2014). At the sentence level, SCPNs (Iyyer et al., 2018) produce a paraphrase of a given sentence without changing the original meaning. In the educational domain, this is comparable to unusual, atypical or creative correct answers that run the risk of being classified as incorrect (Yoon et al., 2018). However, none of the generation methods above is suitable to our research. If we use a real, correct answer as starting point for transformation into an adversarial one and leave the semantic meaning unchanged, we generate a correct answer. But we need answers which are definitely wrong in content as"
2020.coling-main.76,W19-4411,1,0.822592,"ems that represent what is usually applied in practice – so we can find typical problems – instead of highly optimized systems whose vulnerabilities to adversarial input might be highly idiosyncratic. As a representative state-of-the-art shallow system, we selected the ESCRITO scoring toolkit (Zesch and Horbach, 2018) with an SVM classifier (Cortes and Vapnik, 1995), as implemented in Weka using the default PolyKernel. As features we used the top 10000 character 2-5 grams, the top 10000 word 1-5 grams, and answer length. As a deep learning system, we employed the RNN-based system described in Riordan et al. (2019). It uses pretrained word embeddings encoded by a single layer 250-dimensional bidirectional GRU. The hidden states of the GRU are aggregated by a max pooling mechanism. The output of the encoder is aggregated in a fully-connected feedforward layer with sigmoid activation that computes a scalar output for the predicted score. Characters are encoded with a sequence of 25-dimensional character embeddings (randomly initialized) followed by a convolutional neural network (100 filters and filter sizes of (3,4,5)). The character embeddings are concatenated with the word embeddings prior to the word-"
2020.coling-main.76,N18-3008,1,0.834324,"e the semantics of the underlying text unchanged, which is much harder to achieve when texts are manipulated on the token level. A common strategy is to replace single words (usually nouns) with near-synonyms chosen by humans (Kuleshov et al., 2018) or as nearest neighbors in an embedding space (Pennington et al., 2014). At the sentence level, SCPNs (Iyyer et al., 2018) produce a paraphrase of a given sentence without changing the original meaning. In the educational domain, this is comparable to unusual, atypical or creative correct answers that run the risk of being classified as incorrect (Yoon et al., 2018). However, none of the generation methods above is suitable to our research. If we use a real, correct answer as starting point for transformation into an adversarial one and leave the semantic meaning unchanged, we generate a correct answer. But we need answers which are definitely wrong in content as adversarial answers, in order to test the system’s ability of rejecting cheating behaviour. We start with basic methods like uniform selection of characters and words from a generic corpus, but also use methods that are specific to the educational task like shuffling words in correct answers. Th"
2020.coling-main.76,L18-1365,1,0.847326,"ersarial examples for this investigation. 3 https://github.com/graykode/gpt-2-Pytorch 885 as described in the previous section. 3.1 Scoring Systems Automatic scoring systems can be categorized into shallow and deep learning systems (Collobert and Weston, 2008). For our study, we focused on ‘typical’ systems that represent what is usually applied in practice – so we can find typical problems – instead of highly optimized systems whose vulnerabilities to adversarial input might be highly idiosyncratic. As a representative state-of-the-art shallow system, we selected the ESCRITO scoring toolkit (Zesch and Horbach, 2018) with an SVM classifier (Cortes and Vapnik, 1995), as implemented in Weka using the default PolyKernel. As features we used the top 10000 character 2-5 grams, the top 10000 word 1-5 grams, and answer length. As a deep learning system, we employed the RNN-based system described in Riordan et al. (2019). It uses pretrained word embeddings encoded by a single layer 250-dimensional bidirectional GRU. The hidden states of the GRU are aggregated by a max pooling mechanism. The output of the encoder is aggregated in a fully-connected feedforward layer with sigmoid activation that computes a scalar ou"
2020.coling-main.76,W15-0615,1,0.890875,"Missing"
2020.coling-main.76,W12-2022,0,0.0509054,"Missing"
2021.naacl-demos.14,C18-2025,1,0.920614,"/es. html contains information about the tool, links to the download page, a video describing the main features of the tool, as well as an FAQ section. The tool is implemented as a Google Doc add-on (front-end), freely available to download from the app store, with a server-based 7 https://en.wikipedia.org/wiki/Spanish_ language 116 Proceedings of NAACL-HLT 2021: Demonstrations, pages 116–124 June 6–11, 2021. ©2021 Association for Computational Linguistics back-end processing student texts and computing feedback. It is an extension of the original work done for English (Burstein et al., 2018; Madnani et al., 2018a) and the add-on allows users to select either English or Spanish on a per-document basis. Figure 1 shows the language selection screen when the add-on is first started. front-end of the tool for display to the user. The design of the back-end feedback engine was based on the corresponding English one. However, in terms of the implementation, much of the functionality naturally differs in order to account for the language differences. Furthermore, we introduce some new functionality – most notably the section on well-organized writing – that could potentially also be made available for the En"
2021.naacl-demos.14,P14-5010,0,0.0038554,"he user. The design of the back-end feedback engine was based on the corresponding English one. However, in terms of the implementation, much of the functionality naturally differs in order to account for the language differences. Furthermore, we introduce some new functionality – most notably the section on well-organized writing – that could potentially also be made available for the English version of the tool. We take advantage of a number of publicly available tools to build our feedback components. We use the Spanish Stanford Core NLP9 for tokenization, tagging and constituency parsing (Manning et al., 2014). We use the Spacy10 Spanish dependency parser (Honnibal et al., 2020), aligning the dependency relations to the tokenization provided by the Stanford tools. We use the standalone version of Spanish LanguageTool11 to compute a subset of the feedback relating to writing conventions (spelling and grammatical errors). 3.1 Feedback Components Spanish Writing Mentor gives feedback on four broad areas of writing: topic development, coherence, writing conventions, and essay organization. Figure 2 shows the tool when the user loads the app on an open document. Figure 1: Writing Mentor offers feedback"
2021.naacl-demos.14,C10-2103,0,0.0417733,"eTool5 or SpanishChecker6 ). However, there are no tools for Spanish that offer the kind of comprehensive writing feedback that tools such as Writing Mentor and Grammarly offer. There is a huge native Spanish-speaking population (almost 500 million globally according 1 www.grammarly.com mentormywriting.org 3 www.gingersoftware.com 4 www.revisionassistant.com/ 5 https://languagetool.org 6 https://spanishchecker.com/ 2 Related Work in Automated Feedback Studies have shown that automated feedback on student writing can have a positive impact on their learning (Attali, 2004; Shermis et al., 2004; Nagata and Nakatani, 2010; Cotos, 2011; Roscoe et al., 2014). The NLP technologies used to provide feedback on writing have often gone hand-in-hand with the development of automated scoring systems. The intuition is that if the system is “measuring” some aspect of writing in order to be able to grade it, it could also use that same measurement in order to give feedback. However, there are also studies with mixed, or less favourable outcomes when students use tools that provide automated feedback on their writing (Choi, 2010; Bai and Hu, 2017; Ranalli et al., 2017). This is an active area of research, and one that requ"
C10-2163,W02-1503,1,0.801455,"tween a verbal, predicative or adverbial use. 2.1 Participles in the German LFG In order to account for sentences like (1-c), an intuitive approach would be to generally allow for adverb conversion of participles in the grammar. However, in Zarrieß et al. (2010), we show that such a rule can have a strong negative effect on the overall performance of the parsing system, despite the fact that it produces the desired syntactic and semantic analysis for specific sentences. This problem was illustrated using a German LFG grammar (Rohrer and Forst, 2006) constructed as part of the ParGram project (Butt et al., 2002). The grammar is implemented in the XLE, a grammar development environment which includes a very efficient LFG parser and a stochastic disambiguation component which is based on a loglinear probability model (Riezler et al., 2002). In Zarrieß et al. (2010), we found that the naive implementation of adverbial participles in the German LFG, i.e. in terms of a general grammar rule that allows for participles-adverb conversion, leads to spurious ambiguities that mislead the disambiguation component of the grammar. Moreover, the rule increases the number of timeouts, i.e. sentences that cannot be p"
C10-2163,H01-1035,0,0.0430209,"se study on cross-lingual induction of lexical resources for deep, broad-coverage syntactic analysis of German. We use a parallel corpus to induce a classifier for German participles which can predict their syntactic category. By means of this classifier, we induce a resource of adverbial participles from a huge monolingual corpus of German. We integrate the resource into a German LFG grammar and show that it improves parsing coverage while maintaining accuracy. 1 Introduction Parallel corpora are currently exploited in a wide range of induction scenarios, including projection of morphologic (Yarowsky et al., 2001), syntactic (Hwa et al., 2005) and semantic (Pad´o and Lapata, 2009) resources. In this paper, we use crosslingual data to learn to predict whether a lexical item belongs to a specific syntactic category that cannot easily be learned from monolingual resources. In an application test scenario, we show that this prediction method can be used to obtain a lexical resource that improves deep, grammarbased parsing. The general idea of cross-lingual induction is that linguistic annotations or structures, which are not available or explicit in a given language, can be inferred from another language w"
C10-2163,W10-2106,1,0.214349,"However, this latter perspective has been less prominent in the NLP community so far. This paper investigates a cross-lingual induction method based on an exemplary problem arising in the deep syntactic analysis of German. This showcase is the syntactic flexibility of German participles, being morphologically ambiguous between verbal, adjectival and adverbial readings, and it is instructive for several reasons: first, the phenomenon is a notorious problem for linguistic analysis and annotation of German, such that standard German resources do not represent the underlying analysis. Second, in Zarrieß et al. (2010), we showed that integrating the phenomenon of adverbial participles in a naive way into a broadcoverage grammar of German leads to significant parsing problems, due to spurious ambiguities. Third, it is completely straightforward to detect adverbial participles in cross-lingual data since in other languages, e.g. English or French, adverbs are often morphologically marked. In this paper, we use instances of adverbially translated participles in a parallel corpus to bootstrap a classifier that is able to identify an adverbially used participle based on its monolingual syntactic context. In con"
C10-2163,W02-2018,0,0.0205207,"To do this, we use the filtering mechanisms already proposed in Zarrieß et al. (2010). These filters apply on the type level, such that we first identify the positive types (46 total) and then use all instances of these types in the 4891 sentences as positive instances of adverbial participles (1978 instances). The remaining sentences are used as negative instances. For the training of the classifier, we use maximum-entropy classification, which is also commonly used for the general task of tagging (Ratnaparkhi, 1996). In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002). The tags of the words surrounding the participles are used as features in the classification task. We explore different sizes of the context window, where the trigram window is the most succesful (see Table 1). Beyond the trigram window, the results of the classifier start decreasing again, probably because of too many misleading features. Generally, this experiment shows that the grammar-based identification is more precise, but that the classifier still performs surprisingly well. Compared to the results from the grammar-based identification, the high accuracy of the classifier suggests th"
C10-2163,P07-1123,0,0.0605871,"Missing"
C10-2163,W96-0213,0,0.182566,"nces from this training set, and then divide it into a set of positive and negative instances. To do this, we use the filtering mechanisms already proposed in Zarrieß et al. (2010). These filters apply on the type level, such that we first identify the positive types (46 total) and then use all instances of these types in the 4891 sentences as positive instances of adverbial participles (1978 instances). The remaining sentences are used as negative instances. For the training of the classifier, we use maximum-entropy classification, which is also commonly used for the general task of tagging (Ratnaparkhi, 1996). In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002). The tags of the words surrounding the participles are used as features in the classification task. We explore different sizes of the context window, where the trigram window is the most succesful (see Table 1). Beyond the trigram window, the results of the classifier start decreasing again, probably because of too many misleading features. Generally, this experiment shows that the grammar-based identification is more precise, but that the classifier still performs surprisingly well. Compared to the resul"
C10-2163,P02-1035,0,0.0239769,". However, in Zarrieß et al. (2010), we show that such a rule can have a strong negative effect on the overall performance of the parsing system, despite the fact that it produces the desired syntactic and semantic analysis for specific sentences. This problem was illustrated using a German LFG grammar (Rohrer and Forst, 2006) constructed as part of the ParGram project (Butt et al., 2002). The grammar is implemented in the XLE, a grammar development environment which includes a very efficient LFG parser and a stochastic disambiguation component which is based on a loglinear probability model (Riezler et al., 2002). In Zarrieß et al. (2010), we found that the naive implementation of adverbial participles in the German LFG, i.e. in terms of a general grammar rule that allows for participles-adverb conversion, leads to spurious ambiguities that mislead the disambiguation component of the grammar. Moreover, the rule increases the number of timeouts, i.e. sentences that cannot be parsed in a predefined amount of time (20 seconds). Therefore, we observe a drop in parsing accuracy although grammar coverage is improved. As a solution, we induced a lexical resource of adverbial participles based on their adverb"
C10-2163,rohrer-forst-2006-improving,1,0.844851,"t inflected, the surface form of a German participle is ambiguous between a verbal, predicative or adverbial use. 2.1 Participles in the German LFG In order to account for sentences like (1-c), an intuitive approach would be to generally allow for adverb conversion of participles in the grammar. However, in Zarrieß et al. (2010), we show that such a rule can have a strong negative effect on the overall performance of the parsing system, despite the fact that it produces the desired syntactic and semantic analysis for specific sentences. This problem was illustrated using a German LFG grammar (Rohrer and Forst, 2006) constructed as part of the ParGram project (Butt et al., 2002). The grammar is implemented in the XLE, a grammar development environment which includes a very efficient LFG parser and a stochastic disambiguation component which is based on a loglinear probability model (Riezler et al., 2002). In Zarrieß et al. (2010), we found that the naive implementation of adverbial participles in the German LFG, i.e. in terms of a general grammar rule that allows for participles-adverb conversion, leads to spurious ambiguities that mislead the disambiguation component of the grammar. Moreover, the rule in"
C12-1158,N12-1033,0,0.0956259,"Missing"
C12-1158,brooke-hirst-2012-measuring,0,0.126261,"ich features actually perform best. A second problem is that there is no consensus on the scope of the evaluation. The ICLE contains English essays written by native speakers of 16 languages. Typically a subset of 7 languages is used in the evaluations, although more recently some work has reported results for a larger set. Moreover, when researchers report results for 7 languages, they are not always reporting on the same 7 languages. For example, in the work of Wong and Dras (2011) the 7 native languages (L1s) are Bulgarian, Chinese, Czech, French, Japanese, Russian, and Spanish. Whereas in Brooke and Hirst (2012), Italian and Polish are used instead of Bulgarian and Czech. In addition, different researchers have split the corpus in different ways when training and evaluating their systems, making it even more difficult to compare results across experiments. 1 2 Note that Kochmar (2011) used a subsection of the Cambridge Learner Corpus. Throughout this paper, we will refer to ICLE version 2 as ICLE. In this paper, we first provide an automatic method for extracting data from the ICLE corpus to remove some of the corpus-specific idiosyncracies that automatic Native Language Identification classifiers cu"
C12-1158,D11-1010,0,0.020331,"sification, Corpora. 1 Introduction One growing NLP field is that of Native Language Identification (NLI), which is the task of automatically identifying a speaker’s first language based solely on the speaker’s writing in another language. NLI can be useful for a number of applications. Native language is often used as a feature in machine learning approaches to authorship profiling (Estival et al., 2007), which is frequently used in forensic linguistics. NLI can also be used in educational settings to provide more targeted feedback to language learners about their errors (Chang et al., 2008; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011). It is well known that speakers of different languages make different kinds of errors when learning a language (Swan and Smith, 2001). For example, a French speaker learning English might write sentence (1), which contains a verb tense error. On the other hand, a Japanese speaker learning English might make the verb tense error shown in (2). A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achi"
C12-1158,de-marneffe-etal-2006-generating,0,0.0145892,"Missing"
C12-1158,P11-2038,0,0.0920066,"Missing"
C12-1158,P09-2012,0,0.0654302,"Missing"
C12-1158,P11-1093,0,0.0289759,"ntroduction One growing NLP field is that of Native Language Identification (NLI), which is the task of automatically identifying a speaker’s first language based solely on the speaker’s writing in another language. NLI can be useful for a number of applications. Native language is often used as a feature in machine learning approaches to authorship profiling (Estival et al., 2007), which is frequently used in forensic linguistics. NLI can also be used in educational settings to provide more targeted feedback to language learners about their errors (Chang et al., 2008; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011). It is well known that speakers of different languages make different kinds of errors when learning a language (Swan and Smith, 2001). For example, a French speaker learning English might write sentence (1), which contains a verb tense error. On the other hand, a Japanese speaker learning English might make the verb tense error shown in (2). A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achieve it completely. (2) They"
C12-1158,1996.amta-1.36,0,0.411296,"Missing"
C12-1158,P12-2038,0,0.285243,"the verb tense error shown in (2). A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achieve it completely. (2) They also said to have great curiosity. There has been a great deal of work on NLI in recent years. The methods employed have ranged from some combination of lexical, part-of-speech and n-gram features (Koppel et al., 2005), to syntactic features (Wong and Dras, 2011) including Tree Substitution Grammars (TSGs) (Swanson and Charniak, 2012), to topic models (Wong et al., 2011). Despite these research efforts, it has been somewhat hard to compare different approaches for a number of reasons. The first difficulty is with the evaluation data set. Evaluating an NLI system requires a corpus containing texts in a language other than the native language of the writer. Because of a scarcity of such corpora, most work1 has used the ICLEv22 for training and evaluation since it contains several hundred essays written by college-level English language learners. However, this corpus is quite small for training and testing statistical systems"
C12-1158,C08-1109,1,0.893112,"Missing"
C12-1158,N03-1033,0,0.070628,"Missing"
C12-1158,W07-0602,0,0.367516,"Missing"
C12-1158,U09-1008,0,0.299281,"Missing"
C12-1158,D11-1148,0,0.390776,"On the other hand, a Japanese speaker learning English might make the verb tense error shown in (2). A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achieve it completely. (2) They also said to have great curiosity. There has been a great deal of work on NLI in recent years. The methods employed have ranged from some combination of lexical, part-of-speech and n-gram features (Koppel et al., 2005), to syntactic features (Wong and Dras, 2011) including Tree Substitution Grammars (TSGs) (Swanson and Charniak, 2012), to topic models (Wong et al., 2011). Despite these research efforts, it has been somewhat hard to compare different approaches for a number of reasons. The first difficulty is with the evaluation data set. Evaluating an NLI system requires a corpus containing texts in a language other than the native language of the writer. Because of a scarcity of such corpora, most work1 has used the ICLEv22 for training and evaluation since it contains several hundred essays written by college-level English language learners. However"
C12-1158,U11-1015,0,0.0665164,"tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achieve it completely. (2) They also said to have great curiosity. There has been a great deal of work on NLI in recent years. The methods employed have ranged from some combination of lexical, part-of-speech and n-gram features (Koppel et al., 2005), to syntactic features (Wong and Dras, 2011) including Tree Substitution Grammars (TSGs) (Swanson and Charniak, 2012), to topic models (Wong et al., 2011). Despite these research efforts, it has been somewhat hard to compare different approaches for a number of reasons. The first difficulty is with the evaluation data set. Evaluating an NLI system requires a corpus containing texts in a language other than the native language of the writer. Because of a scarcity of such corpora, most work1 has used the ICLEv22 for training and evaluation since it contains several hundred essays written by college-level English language learners. However, this corpus is quite small for training and testing statistical systems which makes it difficult to tell whe"
C12-1158,D12-1064,0,0.18574,"Missing"
C18-1094,Q13-1032,0,0.0260656,"ture and/or organization of the response, (iii) Relevance of the response to the question that was asked. 2 Motivation Over the last few years, there has been a significant increase in the number of NLP conference and workshop publications on the task of automated scoring of student responses (Burrows et al., 2015; Zesch et al., 2015; Sultan et al., 2016). Much of this increase in interest stems from the public availability of fairly large datasets containing scored human responses as part of shared tasks and public contests (the ASAP1 and ASAP22 Kaggle shared tasks, the Powergrading dataset (Basu et al., 2013), and the SemEval This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 https://www.kaggle.com/c/asap-aes 2 https://www.kaggle.com/c/asap-sas 1099 Proceedings of the 27th International Conference on Computational Linguistics, pages 1099–1109 Santa Fe, New Mexico, USA, August 20-26, 2018. 2013 Shared Task (Dzikovska et al., 2013)). Although this increase in interest is well-motivated and has led to useful technical advances, it has also propagated the impression that the task of automated scoring can"
C18-1094,W16-0522,0,0.0138923,"community, our goal is to bring them to the attention of the mainstream NLP audience as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before we describe our position, we describe the various entities who are likely to be"
C18-1094,P98-1032,0,0.08571,"s NLP researchers to understand and incorporate these perspectives into our research and work towards a mutually satisfactory solution in order to build automated scoring systems that are accurate, fair, unbiased, and useful. 1 What is Automated Scoring? Automated scoring is an NLP application usually deployed in the educational domain. It involves automatically analyzing a student’s response to a question and generating either (a) a score in order to assess the student’s knowledge and/or other skills and/or (b) some actionable feedback on how the student can improve the response (Page, 1966; Burstein et al., 1998; Burstein et al., 2004; Zechner et al., 2009; Bernstein et al., 2010). It is considered an NLP application since typically the core technology behind the automated analysis of the student response enlists NLP techniques. The student responses can include essays, short answers, or spoken responses and the two most common kinds of automated scoring are the automated evaluation of writing quality and content knowledge. Both the scores and feedback are usually based on linguistic characteristics of the responses including but not limited to: (i) Lower-level errors in the response (e.g., pronuncia"
C18-1094,K17-1017,0,0.0135374,"Proceedings of the 27th International Conference on Computational Linguistics, pages 1099–1109 Santa Fe, New Mexico, USA, August 20-26, 2018. 2013 Shared Task (Dzikovska et al., 2013)). Although this increase in interest is well-motivated and has led to useful technical advances, it has also propagated the impression that the task of automated scoring can be a self-contained, moderately simple machine learning task requiring “only” sophisticated feature engineering (Somasundaran et al., 2015; Ghosh et al., 2016) or, more recently, complex neural network architectures (Taghipour and Ng, 2016; Dong et al., 2017; Riordan et al., 2017; Tay et al., 2017). Our motivation for writing this paper is to highlight the differences between automated scoring as a shared task and automated scoring as an area of NLP research that serves the end goal of building and deploying accurate and unbiased scoring models for use in actual assessments or classrooms. For the latter, we argue that it is essential for NLP researchers to interact with and, indeed, collaborate with additional parties who are impacted by and contribute to automated scoring.3 To be clear, the issues we highlight are not simply a result of “operati"
C18-1094,P16-2089,0,0.0303005,"ons.org/licenses/by/4.0/ 1 https://www.kaggle.com/c/asap-aes 2 https://www.kaggle.com/c/asap-sas 1099 Proceedings of the 27th International Conference on Computational Linguistics, pages 1099–1109 Santa Fe, New Mexico, USA, August 20-26, 2018. 2013 Shared Task (Dzikovska et al., 2013)). Although this increase in interest is well-motivated and has led to useful technical advances, it has also propagated the impression that the task of automated scoring can be a self-contained, moderately simple machine learning task requiring “only” sophisticated feature engineering (Somasundaran et al., 2015; Ghosh et al., 2016) or, more recently, complex neural network architectures (Taghipour and Ng, 2016; Dong et al., 2017; Riordan et al., 2017; Tay et al., 2017). Our motivation for writing this paper is to highlight the differences between automated scoring as a shared task and automated scoring as an area of NLP research that serves the end goal of building and deploying accurate and unbiased scoring models for use in actual assessments or classrooms. For the latter, we argue that it is essential for NLP researchers to interact with and, indeed, collaborate with additional parties who are impacted by and contrib"
C18-1094,W17-5006,0,0.0225998,"ttention of the mainstream NLP audience as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before we describe our position, we describe the various entities who are likely to be affected by automated scoring systems, and the"
C18-1094,P16-4014,1,0.785195,"king human scorers how they mentally translate the rubric into specific scoring decisions, humans are not as interpretable as one might think (Lipton, 2016). (d) vs. Teachers. To make sure that automated scoring (or feedback) systems behave as expected if deployed for in-classroom use, NLP researchers would like to conduct research studies with such systems in real classrooms in order to collect useful data, e.g., written or spoken responses, student behavior, and indicators of engagement which can then be used to improve the system further (Burstein et al., 2016; Burstein and Sabatini, 2016; Madnani et al., 2016). However, teachers want to ensure that such systems are sufficiently nuanced — and not too primitive — to handle interactions with students and do not lead to students being distracted instead of learning. Furthermore, it takes time to build up a level of trust between the teachers and NLP researchers as a system is being fine-tuned and developed. It is evident from the above discussion that trying to cater to everyone is akin to solving a difficult constraint satisfaction problem. For example, if NLP researchers want to build a more interpretable automated scoring system that can provide mor"
C18-1094,W17-1605,1,0.891136,"Missing"
C18-1094,W15-0629,0,0.0290828,"owever, since there are limited opportunities for that community to interact with the NLP community, our goal is to bring them to the attention of the mainstream NLP audience as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before"
C18-1094,W17-5001,0,0.0131632,"m NLP audience as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before we describe our position, we describe the various entities who are likely to be affected by automated scoring systems, and their correspon"
C18-1094,N16-3020,0,0.0238297,"ain before deployment. (b) vs. Score Users. As described above, one of the most important considerations for test-takers is a reasonably clear explanation of why they received the particular scores that they did. The NLP 1102 researchers optimizing for agreement with human scores might lean towards using more sophisticated machine learning models such as SVMs with non-linear kernels and deep neural networks. However, such models do not really lend themselves to post-hoc interpretability (Lipton, 2016). Although interpretability is an active area of research in the machine learning literature (Ribeiro et al., 2016; Koh and Liang, 2017; Doshi-Velez and Kim, 2017), it currently lags far behind the research on machine learning methods. Ensuring that there are no biases in automated scores – important for institutions using test scores to make decisions – is a topic that sees little discussion in the NLP literature. This is partly driven by a lack of demographic data available in publicly available datasets, as well as perhaps a focus on empirical accuracy. (c) vs. Subject-matter Experts. Subject-matter experts or assessment developers want to ensure that all the hard work that has been done on their end t"
C18-1094,W17-5017,1,0.852987,"27th International Conference on Computational Linguistics, pages 1099–1109 Santa Fe, New Mexico, USA, August 20-26, 2018. 2013 Shared Task (Dzikovska et al., 2013)). Although this increase in interest is well-motivated and has led to useful technical advances, it has also propagated the impression that the task of automated scoring can be a self-contained, moderately simple machine learning task requiring “only” sophisticated feature engineering (Somasundaran et al., 2015; Ghosh et al., 2016) or, more recently, complex neural network architectures (Taghipour and Ng, 2016; Dong et al., 2017; Riordan et al., 2017; Tay et al., 2017). Our motivation for writing this paper is to highlight the differences between automated scoring as a shared task and automated scoring as an area of NLP research that serves the end goal of building and deploying accurate and unbiased scoring models for use in actual assessments or classrooms. For the latter, we argue that it is essential for NLP researchers to interact with and, indeed, collaborate with additional parties who are impacted by and contribute to automated scoring.3 To be clear, the issues we highlight are not simply a result of “operationalizing” NLP researc"
C18-1094,W15-0605,0,0.0225044,"tails: http:// creativecommons.org/licenses/by/4.0/ 1 https://www.kaggle.com/c/asap-aes 2 https://www.kaggle.com/c/asap-sas 1099 Proceedings of the 27th International Conference on Computational Linguistics, pages 1099–1109 Santa Fe, New Mexico, USA, August 20-26, 2018. 2013 Shared Task (Dzikovska et al., 2013)). Although this increase in interest is well-motivated and has led to useful technical advances, it has also propagated the impression that the task of automated scoring can be a self-contained, moderately simple machine learning task requiring “only” sophisticated feature engineering (Somasundaran et al., 2015; Ghosh et al., 2016) or, more recently, complex neural network architectures (Taghipour and Ng, 2016; Dong et al., 2017; Riordan et al., 2017; Tay et al., 2017). Our motivation for writing this paper is to highlight the differences between automated scoring as a shared task and automated scoring as an area of NLP research that serves the end goal of building and deploying accurate and unbiased scoring models for use in actual assessments or classrooms. For the latter, we argue that it is essential for NLP researchers to interact with and, indeed, collaborate with additional parties who are im"
C18-1094,N16-1123,0,0.0785039,"k are usually based on linguistic characteristics of the responses including but not limited to: (i) Lower-level errors in the response (e.g., pronunciation errors in spoken responses and grammatical/spelling errors in written responses), (ii) The discourse structure and/or organization of the response, (iii) Relevance of the response to the question that was asked. 2 Motivation Over the last few years, there has been a significant increase in the number of NLP conference and workshop publications on the task of automated scoring of student responses (Burrows et al., 2015; Zesch et al., 2015; Sultan et al., 2016). Much of this increase in interest stems from the public availability of fairly large datasets containing scored human responses as part of shared tasks and public contests (the ASAP1 and ASAP22 Kaggle shared tasks, the Powergrading dataset (Basu et al., 2013), and the SemEval This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 https://www.kaggle.com/c/asap-aes 2 https://www.kaggle.com/c/asap-sas 1099 Proceedings of the 27th International Conference on Computational Linguistics, pages 1099–1109 S"
C18-1094,D16-1193,0,0.0453309,"ggle.com/c/asap-sas 1099 Proceedings of the 27th International Conference on Computational Linguistics, pages 1099–1109 Santa Fe, New Mexico, USA, August 20-26, 2018. 2013 Shared Task (Dzikovska et al., 2013)). Although this increase in interest is well-motivated and has led to useful technical advances, it has also propagated the impression that the task of automated scoring can be a self-contained, moderately simple machine learning task requiring “only” sophisticated feature engineering (Somasundaran et al., 2015; Ghosh et al., 2016) or, more recently, complex neural network architectures (Taghipour and Ng, 2016; Dong et al., 2017; Riordan et al., 2017; Tay et al., 2017). Our motivation for writing this paper is to highlight the differences between automated scoring as a shared task and automated scoring as an area of NLP research that serves the end goal of building and deploying accurate and unbiased scoring models for use in actual assessments or classrooms. For the latter, we argue that it is essential for NLP researchers to interact with and, indeed, collaborate with additional parties who are impacted by and contribute to automated scoring.3 To be clear, the issues we highlight are not simply a"
C18-1094,W15-0621,0,0.0201929,"portunities for that community to interact with the NLP community, our goal is to bring them to the attention of the mainstream NLP audience as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before we describe our position,"
C18-1094,W17-5013,0,0.0213995,"ce as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before we describe our position, we describe the various entities who are likely to be affected by automated scoring systems, and their corresponding perspectives on t"
C18-1094,W16-4916,0,0.0144614,"to bring them to the attention of the mainstream NLP audience as a first step towards the cross-discipline collaboration that we argue for in this paper. In addition, there have also been efforts in the NLP community itself to highlight how NLP research on automated scoring can be situated in cross-disciplinary contexts (e.g. the workshops on Innovative Use of NLP for Building Educational Applications (BEA) and the workshops on NLP Techniques for Educational Applications (NLPTEA)) (Napoles and Callison-Burch, 2015; Wilson and Martin, 2015; Burstein et al., 2016; Beigman Klebanov et al., 2016; Zalmout et al., 2016; Lugini and Litman, 2017; Pado, 2017; Yaneva et al., 2017). However, the topics of these workshops tend not to always make it into the mainstream NLP conference discussions. Yet, at the same time there has been an increasing interest in automated scoring techniques at main NLP conferences. It is important not to lose the link between developing new and improved NLP techniques for automated scoring and the contexts in which they are generally applied. 4 Perspectives on Automated Scoring Before we describe our position, we describe the various entities who are likely to be affected by automated"
C18-1094,W15-0626,0,0.0802712,"e scores and feedback are usually based on linguistic characteristics of the responses including but not limited to: (i) Lower-level errors in the response (e.g., pronunciation errors in spoken responses and grammatical/spelling errors in written responses), (ii) The discourse structure and/or organization of the response, (iii) Relevance of the response to the question that was asked. 2 Motivation Over the last few years, there has been a significant increase in the number of NLP conference and workshop publications on the task of automated scoring of student responses (Burrows et al., 2015; Zesch et al., 2015; Sultan et al., 2016). Much of this increase in interest stems from the public availability of fairly large datasets containing scored human responses as part of shared tasks and public contests (the ASAP1 and ASAP22 Kaggle shared tasks, the Powergrading dataset (Basu et al., 2013), and the SemEval This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 https://www.kaggle.com/c/asap-aes 2 https://www.kaggle.com/c/asap-sas 1099 Proceedings of the 27th International Conference on Computational Linguist"
cahill-van-genabith-2002-tts,J93-2004,0,\N,Missing
cahill-van-genabith-2002-tts,J03-4003,0,\N,Missing
cahill-van-genabith-2002-tts,P96-1025,0,\N,Missing
D07-1028,C00-1007,0,0.285233,"describe the source of the multi-word units (MWU) used in our experiments and the various techniques we employ to make use of these MWUs in the generation process. Section 6 gives experimental details and results. 2 Related Work on Statistical Generation In (statistical) generators, sentences are generated from an abstract linguistic encoding via the application of grammar rules. These rules can be handcrafted grammar rules, such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005), created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al., 2005; Cahill and van Genabith, 2006). Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licen"
D07-1028,N07-1021,0,0.179772,"how the new history-based model improves over the baseline. In Section 5 we describe the source of the multi-word units (MWU) used in our experiments and the various techniques we employ to make use of these MWUs in the generation process. Section 6 gives experimental details and results. 2 Related Work on Statistical Generation In (statistical) generators, sentences are generated from an abstract linguistic encoding via the application of grammar rules. These rules can be handcrafted grammar rules, such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005), created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al., 2005; Cahill and van Genabith, 2006). Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistical generators is the probability model us"
D07-1028,H92-1026,0,0.0464316,"story (context) to guide local generation decisions. The new history-based probabilistic generation model is defined as: P (T ree|F-Str) := Y P (X → Y |X, F eats, GF) (3) X → Y in T ree F eats = {ai |∃vj (φ(X))ai = vj } (φ(M(X)))GF = φ(X) Note that the new conditioning feature, the fstructure mother grammatical function, GF, is available from structure previously generated in the cstructure tree. As such, it is part of the history of the tree, i.e. it has already been generated in the topdown derivation of the tree. In this way, the generation model resembles history-based models for parsing (Black et al., 1992; Collins, 1999; Charniak, 2000). Unlike, say, the parent annotation for parsing of (Johnson, 1998) the parent GF feature for a particular node expansion is not merely extracted from the parent node in the c-structure tree, but is sometimes extracted from an ancestor node further up the c-structure tree via intervening ↑=↓ functional annotations. Section 6 provides evaluation results for the new model on section 23 of the Penn treebank. 5 Multi-Word Units In another effort to improve generator accuracy over the baseline model we explored the use of multiword units in generation. We expect that"
D07-1028,P06-1130,1,0.832771,"Missing"
D07-1028,P04-1041,1,0.897415,"Missing"
D07-1028,I05-1015,0,0.124868,"the baseline generation model and in Section 4 we show how the new history-based model improves over the baseline. In Section 5 we describe the source of the multi-word units (MWU) used in our experiments and the various techniques we employ to make use of these MWUs in the generation process. Section 6 gives experimental details and results. 2 Related Work on Statistical Generation In (statistical) generators, sentences are generated from an abstract linguistic encoding via the application of grammar rules. These rules can be handcrafted grammar rules, such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005), created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al., 2005; Cahill and van Genabith, 2006). Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistic"
D07-1028,A00-2018,0,0.0452756,"Azenbergstrae 12, D-70174 Stuttgart, Germany. aoife.cahill@ims.uni-stuttgart.de and van Genabith (2006), which do not rely on handcrafted grammars and thus can easily be ported to new languages. This paper is concerned with sentence generation from Lexical-Functional Grammar (LFG) fstructures (Kaplan, 1995). We present improvements in previous LFG-based generation models firstly by breaking down PCFG independence assumptions so that more f-structure conditioning context is included when predicting grammar rule expansions. This history-based approach has worked well in parsing (Collins, 1999; Charniak, 2000) and we show that it also improves PCFG-based generation. We also present work on utilising named entities and other multi-word units to improve generation results for both accuracy and coverage. There has been a limited amount of exploration into the use of multi-word units in probabilistic parsing, for example in (Kaplan and King, 2003) (LFG parsing) and (Nivre and Nilsson, 2004) (dependency parsing). We are not aware of any similar work on generation. In the LFG-based generation algorithm presented by Cahill and van Genabith (2006) complex named entities (i.e. those consisting of more than"
D07-1028,W03-0423,0,0.0124289,"k on chart generation, already contains the hard constraint that when combining two chart edges they must cover disjoint sets of words. We added an additional constraint which prevents edges from being combined if this would result in the generation of a string which contained a named entity which was either incomplete or where the words in the named entity were generated in the wrong order. 5.2 Types of MWUs used in Experiments We carry out experiments with multi-word units from three different sources. First, we use the output of the maximum entropy-based named entity recognition system of (Chieu and Ng, 2003). This system identifies four types of named entity: person, organisation, location, and miscellaneous. Additionally we use a dictionary of candidate multi-word expressions based on a list from the Stanford Multiword Expression Project4 . Finally, we also carry out experiments with multi-word units extracted from the BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005). This supplements the Penn WSJ treebank’s one million words of syntax-annotated Wall Street Journal text with additional annotations of 23 named entity types, including nominal-type named entities such"
D07-1028,W01-0812,0,0.0197203,"shi et al. (2005) describe a treebankextracted HPSG-based chart generator. Importing techniques developed for HPSG parsing, they apply a log linear model to a packed representation of all alternative derivation trees for a given input. They found that a model which included syntactic information outperformed a bigram model as well as a combination of bigram and syntax model. The probability model described in this paper also incorporates syntactic information, however, unlike the discriminative HPSG models just described, it is a generative history- and PCFG-based model. While Belz (2007) and Humphreys et al. (2001) mention the use of contextual features for the rules in their generation models, they do not provide details nor do they provide a formal probability model. To the best of our knowledge this is the first paper providing a probabilistic generative, history-based generation model. 3 Surface Realisation from f-Structures Cahill and van Genabith (2006) present a probabilistic surface generation model for LFG (Kaplan, 1995). LFG is a constraint-based theory of grammar, which analyses strings in terms of c(onstituency)-structure and f(unctional)-structure (Figure 1). C-structure is defined in terms"
D07-1028,J98-4004,0,0.0234125,"el is defined as: P (T ree|F-Str) := Y P (X → Y |X, F eats, GF) (3) X → Y in T ree F eats = {ai |∃vj (φ(X))ai = vj } (φ(M(X)))GF = φ(X) Note that the new conditioning feature, the fstructure mother grammatical function, GF, is available from structure previously generated in the cstructure tree. As such, it is part of the history of the tree, i.e. it has already been generated in the topdown derivation of the tree. In this way, the generation model resembles history-based models for parsing (Black et al., 1992; Collins, 1999; Charniak, 2000). Unlike, say, the parent annotation for parsing of (Johnson, 1998) the parent GF feature for a particular node expansion is not merely extracted from the parent node in the c-structure tree, but is sometimes extracted from an ancestor node further up the c-structure tree via intervening ↑=↓ functional annotations. Section 6 provides evaluation results for the new model on section 23 of the Penn treebank. 5 Multi-Word Units In another effort to improve generator accuracy over the baseline model we explored the use of multiword units in generation. We expect that the identification of MWUs may be useful in imposing wordorder constraints and reducing the comple"
D07-1028,P96-1027,0,0.139347,"ses the generation of sequences of words which violate the internal word order of named entities. The input is marked-up in such a way that, although named entities are no longer chunked together to form single words, the algorithm can read which items are part of named entities. See the rightmost f-structure in Figure 5 for an example of an f-structure markedup in this way. The tag NE1 1, for example, indicates that the sub-f-structure is part of a named identity with id number 1 and that the item corresponds to the first word of the named entity. The baseline generation algorithm, following Kay (1996)’s work on chart generation, already contains the hard constraint that when combining two chart edges they must cover disjoint sets of words. We added an additional constraint which prevents edges from being combined if this would result in the generation of a string which contained a named entity which was either incomplete or where the words in the named entity were generated in the wrong order. 5.2 Types of MWUs used in Experiments We carry out experiments with multi-word units from three different sources. First, we use the output of the maximum entropy-based named entity recognition syste"
D07-1028,P98-1116,0,0.0680725,"ned and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licensed by the grammar. One generation technique is to first generate all possible sentences, storing them in a word lattice (Langkilde and Knight, 1998) or, alternatively, a generation forest, a packed represention of alternate trees proposed by the generator (Langkilde, 2000), and then select the most probable sequence of words via an n-gram language model. Increasingly syntax-based information is being incorporated directly into the generation model. For example, Carroll and Oepen (2005) describe a sen268 tence realisation process which uses a hand-crafted HPSG grammar to generate a generation forest. A selective unpacking algorithm allows the extraction of an n-best list of realisations where realisation ranking is based on a maximum entro"
D07-1028,W02-2103,0,0.555453,"al) generators, sentences are generated from an abstract linguistic encoding via the application of grammar rules. These rules can be handcrafted grammar rules, such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005), created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al., 2005; Cahill and van Genabith, 2006). Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licensed by the grammar. One generation technique is to first generate all possible sentences, storing them in a word lattice (Langkilde and Knight, 1998) or, alternatively, a generation forest, a packed represention of alternate trees proposed by the generator (Langk"
D07-1028,A00-2023,0,0.284489,"2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licensed by the grammar. One generation technique is to first generate all possible sentences, storing them in a word lattice (Langkilde and Knight, 1998) or, alternatively, a generation forest, a packed represention of alternate trees proposed by the generator (Langkilde, 2000), and then select the most probable sequence of words via an n-gram language model. Increasingly syntax-based information is being incorporated directly into the generation model. For example, Carroll and Oepen (2005) describe a sen268 tence realisation process which uses a hand-crafted HPSG grammar to generate a generation forest. A selective unpacking algorithm allows the extraction of an n-best list of realisations where realisation ranking is based on a maximum entropy model. This unpacking algorithm is used in (Velldal and Oepen, 2005) to rank realisations with features defined over HPSG"
D07-1028,J93-2004,0,0.0281111,"Missing"
D07-1028,W05-1510,0,0.73623,"surface realisation, is the task of generating meaningful, grammatically correct and fluent text from some abstract semantic or syntactic representation of the sentence. It is an important and growing field of natural language processing with applications in areas such as transferbased machine translation (Riezler and Maxwell, 2006) and sentence condensation (Riezler et al., 2003). While recent work on generation in restricted domains, such as (Belz, 2007), has shown promising results there remains much room for improvement particularly for broad coverage and robust generators, like those of Nakanishi et al. (2005) and Cahill ∗ Now at the Institut f¨ur Maschinelle Sprachverarbeitung, Universit¨at Stuttgart, Azenbergstrae 12, D-70174 Stuttgart, Germany. aoife.cahill@ims.uni-stuttgart.de and van Genabith (2006), which do not rely on handcrafted grammars and thus can easily be ported to new languages. This paper is concerned with sentence generation from Lexical-Functional Grammar (LFG) fstructures (Kaplan, 1995). We present improvements in previous LFG-based generation models firstly by breaking down PCFG independence assumptions so that more f-structure conditioning context is included when predicting gr"
D07-1028,A00-2026,0,0.030549,"such as those of (LangkildeGeary, 2002; Carroll and Oepen, 2005), created semi-automatically (Belz, 2007) or, alternatively, extracted fully automatically from treebanks (Bangalore and Rambow, 2000; Nakanishi et al., 2005; Cahill and van Genabith, 2006). Insofar as it is a broad coverage generator, which has been trained and tested on sections of the WSJ corpus, our generator is closer to the generators of (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Nakanishi et al., 2005) than to those designed for more restricted domains such as weather forecast (Belz, 2007) and air travel domains (Ratnaparkhi, 2000). Another feature which characterises statistical generators is the probability model used to select the most probable sentence from among the space of all possible sentences licensed by the grammar. One generation technique is to first generate all possible sentences, storing them in a word lattice (Langkilde and Knight, 1998) or, alternatively, a generation forest, a packed represention of alternate trees proposed by the generator (Langkilde, 2000), and then select the most probable sequence of words via an n-gram language model. Increasingly syntax-based information is being incorporated di"
D07-1028,N06-1032,0,0.05047,"Missing"
D07-1028,N03-1026,0,0.0475794,"Missing"
D07-1028,2005.mtsummit-papers.15,0,0.204767,"ed represention of alternate trees proposed by the generator (Langkilde, 2000), and then select the most probable sequence of words via an n-gram language model. Increasingly syntax-based information is being incorporated directly into the generation model. For example, Carroll and Oepen (2005) describe a sen268 tence realisation process which uses a hand-crafted HPSG grammar to generate a generation forest. A selective unpacking algorithm allows the extraction of an n-best list of realisations where realisation ranking is based on a maximum entropy model. This unpacking algorithm is used in (Velldal and Oepen, 2005) to rank realisations with features defined over HPSG derivation trees. They achieved the best results when combining the tree-based model with an n-gram language model. Nakanishi et al. (2005) describe a treebankextracted HPSG-based chart generator. Importing techniques developed for HPSG parsing, they apply a log linear model to a packed representation of all alternative derivation trees for a given input. They found that a model which included syntactic information outperformed a bigram model as well as a combination of bigram and syntax model. The probability model described in this paper"
D07-1028,J03-4003,0,\N,Missing
D07-1028,C98-1112,0,\N,Missing
D07-1028,P93-1005,0,\N,Missing
D14-1142,C12-1025,0,0.0525928,"etreault et al. (2012). A summary of the corpora is given in Table 1. Corpus ICLE TOEFL11 TOEFL11-Big Languages 7 11 11 Documents 770 12, 100 87, 502 Table 1: Summary of corpora used in the experiments. The ICLEv2 is a corpus of essays written by highly-proficient non-native college-level students of English. For many years this was the standard corpus used in the task of native language identification. However, the corpus was originally collected for the purpose of corpus linguistic investigations, and because of this contains some idiosyncrasies that make it problematic for the task of NLI (Brooke and Hirst, 2012). Therefore, a modified version of the corpus that has been normalized as much as possible for topic and character encoding (Tetreault et al., 2012) is used. This version of the corpus contains 110 essays each for 7 native languages: Bulgarian, Chinese, Czech, French, Japanese, Russian and Spanish. The ETS Corpus of Non-Native Written English (TOEFL11) was first introduced by Tetreault et al. (2012) and extended for the 2013 Native Language Identification Shared Task (Tetreault et al., 2013). It was designed to overcome many of the shortcomings identified with using the ICLEv2 corpus for this"
D14-1142,C12-1027,0,0.0462149,"ce the best results. Therefore, they are selected for the remaining experiments. Another set of preliminary experiments were performed to determine the range of n-grams that gives the most accurate results on a 10-fold crossvalidation procedure carried out on the TOEFL11 training set. All the n-grams in the range 2-10 were evaluated. Furthermore, experiments with different blended kernels were conducted to see whether combining n-grams of different lengths could improve the accuracy. The best results were obtained when all the n-grams with the length in the range 5-8 were used. Other authors (Bykh and Meurers, 2012; Popescu and Ionescu, 2013) also report better results by using n-grams with the length in a range, rather than using n-grams of fixed length. Consequently, the results reported in this work are based on blended string kernels based on 5-8 n-grams. Some preliminary experiments were also performed to establish the type of kernel to be used, namely the blended p-spectrum kernel (kˆ5−8 ), the 0/1 blended p-grams presence bits kernel (kˆ5−8 ), the ∩ ), or the blended p-grams intersection kernel (kˆ5−8 LRD .). These different kernel based on LRD (kˆ5−8 kernel representations are obtained from the"
D14-1142,W13-1714,0,0.601215,"these features. It is important to note that this approach is also linguistic theory neutral, since it disregards any features of natural language such as words, phrases, or meaning. On the other hand, a method that considers words as features cannot be completely language independent, since the definition of a word is necessarily languagespecific. For example, a method that uses only function words as features is not completely language independent because it needs a list of function words which is specific to a language. When features such as part-of-speech tags are used, as in the work of Jarvis et al. (2013), the method relies on a part-of-speech tagger which might not be available (yet) for some languages. Furthermore, a way to segment a text into words is not an easy task for some languages, such as Chinese. Character n-grams are used by some of the systems developed for native language identification. 1364 In work where feature ablation results have been reported, the performance with only character ngram features was modest compared to other types of features (Tetreault et al., 2012). Initially, most work limited the character features to unigrams, bigrams and trigrams, perhaps because longer"
D14-1142,W13-1706,1,0.430982,"gations, and because of this contains some idiosyncrasies that make it problematic for the task of NLI (Brooke and Hirst, 2012). Therefore, a modified version of the corpus that has been normalized as much as possible for topic and character encoding (Tetreault et al., 2012) is used. This version of the corpus contains 110 essays each for 7 native languages: Bulgarian, Chinese, Czech, French, Japanese, Russian and Spanish. The ETS Corpus of Non-Native Written English (TOEFL11) was first introduced by Tetreault et al. (2012) and extended for the 2013 Native Language Identification Shared Task (Tetreault et al., 2013). It was designed to overcome many of the shortcomings identified with using the ICLEv2 corpus for this task. The TOEFL11 corpus contains a balanced distribution of essays per prompt (topic) per native language. It also contains information about the language proficiency of each writer. The corpus contains essays written by speakers of the following 11 languages: Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu and Turkish. For the shared task, the 12, 100 essays were split into 9, 900 for training, 1, 100 for development and 1, 100 for testing. Tetreault et a"
D14-1142,E14-4019,0,0.178194,"Missing"
D14-1142,W13-1735,1,0.731833,"ms are used by some of the systems developed for native language identification. 1364 In work where feature ablation results have been reported, the performance with only character ngram features was modest compared to other types of features (Tetreault et al., 2012). Initially, most work limited the character features to unigrams, bigrams and trigrams, perhaps because longer ngrams were considered too expensive to compute or unlikely to improve performance. However, some of the top systems in the 2013 NLI Shared Task were based on longer character n-grams, up to 9-grams (Jarvis et al., 2013; Popescu and Ionescu, 2013). The results presented in this work are obtained using a range of 5–8 n-grams. Combining all 5–8 n-grams would generate millions of features, which are indeed expensive to compute and represent. The key to avoiding the computation of such a large number of features lies in using the dual representation provided by the string kernel. String kernel similarity matrices can be computed much faster and are extremely useful when the number of samples is much lower than the number of features. 3 3.1 Similarity Measures for Strings fined by this kernel associates a vector of dimension |Σ|p containing"
D14-1142,R11-1091,1,0.919163,"Missing"
D14-1142,W06-1657,0,0.0774396,"Missing"
D14-1142,C12-1158,1,0.566766,"tion words which is specific to a language. When features such as part-of-speech tags are used, as in the work of Jarvis et al. (2013), the method relies on a part-of-speech tagger which might not be available (yet) for some languages. Furthermore, a way to segment a text into words is not an easy task for some languages, such as Chinese. Character n-grams are used by some of the systems developed for native language identification. 1364 In work where feature ablation results have been reported, the performance with only character ngram features was modest compared to other types of features (Tetreault et al., 2012). Initially, most work limited the character features to unigrams, bigrams and trigrams, perhaps because longer ngrams were considered too expensive to compute or unlikely to improve performance. However, some of the top systems in the 2013 NLI Shared Task were based on longer character n-grams, up to 9-grams (Jarvis et al., 2013; Popescu and Ionescu, 2013). The results presented in this work are obtained using a range of 5–8 n-grams. Combining all 5–8 n-grams would generate millions of features, which are indeed expensive to compute and represent. The key to avoiding the computation of such a"
E09-1014,W00-1401,0,0.041558,"compare across systems. There is consensus that exact match with respect to an actually observed corpus sentence is too strict a metric and that BLEU score measured against corpus sentences can only give a rough impression of the quality of the system output. It is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so that, as a result, there have been a range of automatic metrics applied including inter alia exact match, string edit distance, NIST SSA, BLEU, NIST, ROUGE, generation string accuracy, generation tree accuracy, word accuracy (Bangalore et al., 2000; Callaway, 2003; Nakanishi et al., 2005; Velldal and Oepen, 2006; Belz and Reiter, 2006). It is not always clear how appropriate these metrics are, especially at the level of individual sentences. Using automatic evaluation metrics cannot be avoided, but ideally, a metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the Martin Forst Palo Alto Research Center 3333 Coyote Hill Road Palo Alto, CA 94304, USA mforst@parc.com language for which the surface realisation system is developed, and not only globally, but also at the l"
E09-1014,E06-1040,0,0.16842,"observed corpus sentence is too strict a metric and that BLEU score measured against corpus sentences can only give a rough impression of the quality of the system output. It is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so that, as a result, there have been a range of automatic metrics applied including inter alia exact match, string edit distance, NIST SSA, BLEU, NIST, ROUGE, generation string accuracy, generation tree accuracy, word accuracy (Bangalore et al., 2000; Callaway, 2003; Nakanishi et al., 2005; Velldal and Oepen, 2006; Belz and Reiter, 2006). It is not always clear how appropriate these metrics are, especially at the level of individual sentences. Using automatic evaluation metrics cannot be avoided, but ideally, a metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the Martin Forst Palo Alto Research Center 3333 Coyote Hill Road Palo Alto, CA 94304, USA mforst@parc.com language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences. Another major consideration in evaluation is what to take a"
E09-1014,W07-2303,1,0.949024,"ce realisation ranking model and (iv) to establish what effect preceding context has on the choice of realisation. In this paper, we concentrate on points (i) and (iv). The remainder of the paper is structured as follows: In Section 2 we outline the realisation ranking system that provided the data for the experiment. In Section 3 we outline the design of the experiment and in Section 4 we present our findings. In Section 5 we relate this to other work and finally we conclude in Section 6. 2 A Realisation Ranking System for German We take the realisation ranking system for German described in Cahill et al. (2007) and present the output to human judges. One goal of this series of experiments is to examine whether the results Proceedings of the 12th Conference of the European Chapter of the ACL, pages 112–120, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 112 based on automatic evaluation metrics published in that paper are confirmed in an evaluation by humans. Another goal is to collect data that will allow us and other researchers1 to explore more finegrained and reliable automatic evaluation metrics for realisation ranking. The system presented by Cahill et"
E09-1014,W05-1510,0,0.11548,"sus that exact match with respect to an actually observed corpus sentence is too strict a metric and that BLEU score measured against corpus sentences can only give a rough impression of the quality of the system output. It is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so that, as a result, there have been a range of automatic metrics applied including inter alia exact match, string edit distance, NIST SSA, BLEU, NIST, ROUGE, generation string accuracy, generation tree accuracy, word accuracy (Bangalore et al., 2000; Callaway, 2003; Nakanishi et al., 2005; Velldal and Oepen, 2006; Belz and Reiter, 2006). It is not always clear how appropriate these metrics are, especially at the level of individual sentences. Using automatic evaluation metrics cannot be avoided, but ideally, a metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the Martin Forst Palo Alto Research Center 3333 Coyote Hill Road Palo Alto, CA 94304, USA mforst@parc.com language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences. Another ma"
E09-1014,W02-2113,0,0.127994,"in the same way as native speakers of the Martin Forst Palo Alto Research Center 3333 Coyote Hill Road Palo Alto, CA 94304, USA mforst@parc.com language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences. Another major consideration in evaluation is what to take as the gold standard. The easiest option is to take the original corpus string that was used to produce the abstract representation from which we generate. However, there may well be other realisations of the same input that are as suitable in the given context. Reiter and Sripada (2002) argue that while we should take advantage of large corpora in NLG, we also need to take care that we do not introduce errors by learning from incorrect data present in corpora. In order to better understand what makes good evaluation data (and metrics), we designed and implemented an experiment in which human judges evaluated German string realisations. The main aims of this experiment were: (i) to establish how much variation in German word order is acceptable for human judges, (ii) to find an automatic evaluation metric that mirrors the findings of the human evaluation, (iii) to provide det"
E09-1014,rohrer-forst-2006-improving,1,0.894161,"Missing"
E09-1014,W06-1661,0,0.0237396,"h respect to an actually observed corpus sentence is too strict a metric and that BLEU score measured against corpus sentences can only give a rough impression of the quality of the system output. It is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so that, as a result, there have been a range of automatic metrics applied including inter alia exact match, string edit distance, NIST SSA, BLEU, NIST, ROUGE, generation string accuracy, generation tree accuracy, word accuracy (Bangalore et al., 2000; Callaway, 2003; Nakanishi et al., 2005; Velldal and Oepen, 2006; Belz and Reiter, 2006). It is not always clear how appropriate these metrics are, especially at the level of individual sentences. Using automatic evaluation metrics cannot be avoided, but ideally, a metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the Martin Forst Palo Alto Research Center 3333 Coyote Hill Road Palo Alto, CA 94304, USA mforst@parc.com language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences. Another major consideration in eval"
E09-1014,N03-2021,0,\N,Missing
E09-1014,W08-1007,0,\N,Missing
E09-1014,P02-1040,0,\N,Missing
E09-1014,P09-1092,1,\N,Missing
E09-1014,P07-1041,0,\N,Missing
E09-1014,P09-1022,0,\N,Missing
E09-1014,W04-1013,0,\N,Missing
E12-1068,P08-1087,0,0.108597,"which is a useful approach for dealing with issues of word-formation. However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing. Much previous work looks at the impact of using source side information (i.e., feature functions on the aligned English), such as those of Avramidis and Koehn (2008), Yeniterzi and Oflazer (2010) and others. Toutanova et. al.’s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance. For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags Sty"
E12-1068,P08-2039,0,0.0493127,"problematic as this requires the use of morphologically-aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation. Despite this, our research clearly shows that the feature-based approach is superior for English-to-German SMT. This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing. As parsing performance improves, the performance of linguistic-feature-based approaches will increase. Virpioja et al. (2007), Badr et al. (2008), Luong et al. (2010), Clifton and Sarkar (2011), and others are primarily concerned with using morpheme segmentation in SMT, which is a useful approach for dealing with issues of word-formation. However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing. Much previous"
E12-1068,W10-1705,0,0.154895,"cessing and resynthesize in postprocessing” approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Mari˜no (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the 671 agreement phenomena that we model. Our CRF framework allows us to use more complex context features. We have directly addressed the question as to whether inflection should be predicted using surface forms as the target of the prediction, or whether linguistic features should be predicted, along with the use of a subsequent generation step. The d"
E12-1068,P11-1004,0,0.0914439,"morphologically-aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation. Despite this, our research clearly shows that the feature-based approach is superior for English-to-German SMT. This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing. As parsing performance improves, the performance of linguistic-feature-based approaches will increase. Virpioja et al. (2007), Badr et al. (2008), Luong et al. (2010), Clifton and Sarkar (2011), and others are primarily concerned with using morpheme segmentation in SMT, which is a useful approach for dealing with issues of word-formation. However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing. Much previous work looks at the impact of using source side i"
E12-1068,W09-0420,1,0.874006,"zation. We view this issue as a different sort of problem entirely, one of word-formation (rather than inflection). We apply a “split in preprocessing and resynthesize in postprocessing” approach to these phenomena, combined with inflection prediction that is similar to that of Toutanova et. al. The only work that we are aware of which deals with both issues is the work of de Gispert and Mari˜no (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the 671 agreement phenomena that we model. Our CRF framework allows us to use more complex context features. We have directly addressed the question as to whether inflection should be predicted using surfac"
E12-1068,W10-1734,1,0.864458,"productive in German and lead to data sparsity. We split the German compounds in the training data, so that our stem translation system can now work with the individual words in the compounds. After we have translated to a split/stemmed representation, we determine whether to merge words together to form a compound. Then we merge them to create stems in the same representation as before and we perform inflection and portmanteau merging exactly as previously discussed. 8.1 Details of Splitting Process We prepare the training data by splitting compounds in two steps, following the technique of Fritzinger and Fraser (2010). First, possible split points are extracted using SMOR, and second, the best split points are selected using the geometric mean of word part frequencies. compound Inflationsrate auszubrechen word parts Inflation Rate aus zu brechen gloss inflation rate out to break (to break out) Training data is then stemmed as described in Section 2.3. The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb. In these cases, f"
E12-1068,D07-1091,0,0.685969,"this moves some of the difficulty in predicting case from the inflection prediction step to the stem translation step. Since the choice of case in a PP is often determined by the PP’s meaning (and there are often different meanings possible given different case choices), it seems reasonable to make this decision during stem translation. Verbs are represented using their inflected surface form. Having access to inflected verb forms has a positive influence on case prediction in the second 2 We use an additional target factor to obtain the coarse POS for each stem, applying a 7-gram POS model. Koehn and Hoang (2007) showed that the use of a POS factor only results in negligible BLEU improvements, but we need access to the POS in our inflection prediction models. 665 input decoder output inflected in<APPR&gt;<Dat&gt; in in die<+ART&gt;<Def&gt; dem contrast Gegensatz<+NN&gt;<Masc&gt;<Sg&gt; Gegensatz to zu<APPR&gt;<Dat&gt; zu the die<+ART&gt;<Def&gt; der animated lebhaft<+ADJ&gt;<Pos&gt; lebhaften debate Debatte<+NN&gt;<Fem&gt;<Sg&gt; Debatte merged im Gegensatz zur lebhaften Debatte Table 1: Re-merging of prepositions and articles after inflection to form portmanteaus, in dem means in the. step through subject-verb agreement. Articles are reduced to th"
E12-1068,E03-1076,0,0.321449,"va et. al.’s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance. For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008)) or are (almost) knowledge-free (e.g., Koehn and Knight (2003)). Compound merging is less well studied. Popovic et al. (2006) used a simple, list-based merging approach, merging all consecutive words included in a merging list. This approach resulted in too many compounds. We follow Stymne and Cancedda (2011), for compound merging. We trained a CRF using (nearly all) of the features they used and found their approach to be effective (when combined with inflection and portmanteau merging) on one of our two test sets. 10 Conclusion We have shown that both the prediction of surface forms and the prediction of linguistic features are of interest for improvin"
E12-1068,P10-1052,0,0.234392,"Missing"
E12-1068,D10-1015,0,0.0193011,"requires the use of morphologically-aware syntactic parsers to annotate the training data with such features, and additionally depends on the coverage of morphological analysis and generation. Despite this, our research clearly shows that the feature-based approach is superior for English-to-German SMT. This is a striking result considering state-of-theart performance of German parsing is poor compared with the best performance on English parsing. As parsing performance improves, the performance of linguistic-feature-based approaches will increase. Virpioja et al. (2007), Badr et al. (2008), Luong et al. (2010), Clifton and Sarkar (2011), and others are primarily concerned with using morpheme segmentation in SMT, which is a useful approach for dealing with issues of word-formation. However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing. Much previous work looks at the im"
E12-1068,schmid-etal-2004-smor,0,0.877102,"n, including the inflection-dependent phenomenon of portmanteaus. Later, after performing an extensive analysis of this system, we will extend it 664 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 664–674, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics to model compounds, a highly productive phenomenon in German (see Section 8). The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR, a morphological analyzer/generator of German (Schmid et al., 2004) and the BitPar parser, which is a state-of-the-art parser of German (Schmid, 2004). 2.1 Issues of inflection prediction In order to ensure coherent German NPs, we model linguistic features of each word in an NP. We model case, gender, and number agreement and whether or not the word is in the scope of a determiner (such as a definite article), which we label in-weak-context (this linguistic feature is necessary to determine the type of inflection of adjectives and other words: strong, weak, mixed). This is a diverse group of features. The number of a German noun can often be determined given"
E12-1068,C04-1024,0,0.286356,"an extensive analysis of this system, we will extend it 664 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 664–674, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics to model compounds, a highly productive phenomenon in German (see Section 8). The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR, a morphological analyzer/generator of German (Schmid et al., 2004) and the BitPar parser, which is a state-of-the-art parser of German (Schmid, 2004). 2.1 Issues of inflection prediction In order to ensure coherent German NPs, we model linguistic features of each word in an NP. We model case, gender, and number agreement and whether or not the word is in the scope of a determiner (such as a definite article), which we label in-weak-context (this linguistic feature is necessary to determine the type of inflection of adjectives and other words: strong, weak, mixed). This is a diverse group of features. The number of a German noun can often be determined given only the English source word. The gender of a German noun is innate and often diffi"
E12-1068,W11-2129,0,0.446157,"3. The formerly modifying words of the compound (in our example the words to the left of the rightmost word) do not have a stem markup assigned, except for two cases: i) they are nouns themselves or ii) they are particles separated from a verb. In these cases, former modifiers are represented identically to their individual occurring counterparts, which helps generalization. 8.2 Model for Compound Merging After translation, compound parts have to be resynthesized into compounds before inflection. Two decisions have to be taken: i) where to 670 merge and ii) how to merge. Following the work of Stymne and Cancedda (2011), we implement a linear-chain CRF merging system using the following features: stemmed (separated) surface form, part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word+1, word as true prefix, word+1 as true suffix, plus frequency comparisons of these. The CRF is trained on the split monolingual data. It only proposes merging decisions, merging itself uses a list extracted from the monolingual data (Popovic et al., 2006). 8.3 Experiments We evaluated the end-to-end inflection system with the addition of compounds.15 As in the inflection experiments descri"
E12-1068,P08-1059,0,0.455719,"M) and has a different input representation based on the previously described markup. The input consists of a sequence of coarse POS tags, and for those stems that are marked up with the relevant feature, this feature value. Finally, we combine the predicted features together to produce the same final output as the single joint sequence model, and then generate each surface form using SMOR. 5. Using four CRFs (one for each linguistic feature). The sequence models already presented are limited to the n-gram feature space, and those that predict linguistic features are not strongly lexicalized. Toutanova et al. (2008) uses an MEMM which allows the integration of a wide variety of feature functions. We also wanted to experiment with additional feature functions, and so we train 4 separate linear chain CRF6 models on our data (one for each linguistic feature we want to predict). We chose CRFs over MEMMs to avoid the label bias problem (Lafferty et al., 2001). The CRF feature functions, for each German word wi , are in Table 3. The common feature functions are used in all models, while each of the 4 separate models (one for each linguistic feature) includes the context of only that linguistic feature. We use"
E12-1068,2007.mtsummit-papers.65,0,0.019067,"Missing"
E12-1068,W11-2126,0,0.152964,"f which deals with both issues is the work of de Gispert and Mari˜no (2008), which deals with verbal morphology and attached pronouns. There has been other work on solving inflection. Koehn and Hoang (2007) introduced factored SMT. We use more complex context features. Fraser (2009) tried to solve the inflection prediction problem by simply building an SMT system for translating from stems to inflected forms. Bojar and Kos (2010) improved on this by marking prepositions with the case they mark (one of the most important markups in our system). Both efforts were ineffective on large data sets. Williams and Koehn (2011) used unification in an SMT system to model some of the 671 agreement phenomena that we model. Our CRF framework allows us to use more complex context features. We have directly addressed the question as to whether inflection should be predicted using surface forms as the target of the prediction, or whether linguistic features should be predicted, along with the use of a subsequent generation step. The direct prediction of surface forms is limited to those forms observed in the training data, which is a significant limitation. However, it is reasonable to expect that the use of features (and"
E12-1068,P10-1047,0,0.0195547,"or dealing with issues of word-formation. However, this does not deal directly with linguistic features marked by inflection. In German these linguistic features are marked very irregularly and there is widespread syncretism, making it difficult to split off morphemes specifying these features. So it is questionable as to whether morpheme segmentation techniques are sufficient to solve the inflectional problem we are addressing. Much previous work looks at the impact of using source side information (i.e., feature functions on the aligned English), such as those of Avramidis and Koehn (2008), Yeniterzi and Oflazer (2010) and others. Toutanova et. al.’s work showed that it is most important to model target side coherence and our stem markup also allows us to access source side information. Using additional source side information beyond the markup did not produce a gain in performance. For compound splitting, we follow Fritzinger and Fraser (2010), using linguistic knowledge encoded in a rule-based morphological analyser and then selecting the best analysis based on the geometric mean of word part frequencies. Other approaches use less deep linguistic resources (e.g., POS-tags Stymne (2008)) or are (almost) kn"
E12-1078,J08-1001,0,0.0278889,"on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfeld (or “prefield”). Filippova and Strube (2007) show that once the Vorfeld (i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the Mittelfeld (i.e. the constituents that follow the finite verb) is very easy. Cheung and Penn (2010) extend the approach of Filippova and Strube (2007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 2 3 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and LaMotivation While there would be many ways to const"
E12-1078,N04-1015,0,0.0247948,"i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the Mittelfeld (i.e. the constituents that follow the finite verb) is very easy. Cheung and Penn (2010) extend the approach of Filippova and Strube (2007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 2 3 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and LaMotivation While there would be many ways to construe or represent discourse context (e.g. in terms of the global discourse or information structure), we concentrate on capturing local coherence through the distribution of discourse referents in"
E12-1078,E06-1040,0,0.021211,"gests that sentenceinternal realisation implicitly carries a lot of imformation about discourse context. On average, the morphosyntactic properties of constituents in a text are better approximates of their discourse status than actual coreference relations. This result feeds into a number of research questions concerning the representation of discourse and its application in generation systems. Although we should certainly not expect a computational model to achieve a perfect accuracy in the constituent ordering task – even humans only agree to a certain extent in rating word order variants (Belz and Reiter, 2006; Cahill, 2009) – the average accuracy in the 60’s for prediction of Vorfeld occupance is still moderate. An obvious direction would be to further investigate more complex representations of discourse that take into account the relations between utterances, such as topic shifts. Moreover, it is not clear whether the effects we find for linearisation in this paper carry over to other levels of generation such as tactical generation where syntactic functions are not fully specified. In a broader perspective, our results underline the need for better formalisations of discourse that can be transl"
E12-1078,W10-1834,0,0.0221106,"Missing"
E12-1078,P09-1092,1,0.943535,"actual lexical choice, including functional categories such as determiners, is fully fixed (which is of course not always the case), one can take advantage of 767 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 767–776, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics these reflexes. This explains in part the fairly high baseline performance of n-gram language models in the surface realization task. And the effect can indeed be taken much further: the discriminative training experiments of Cahill and Riester (2009) show how effective it is to systematically take advantage of asymmetry patterns in the morphosyntactic reflexes of the discourse notion of information status (i.e., using a feature set with well-chosen purely sentence-bound features). These observations give rise to the question: in the light of the difficulty in obtaining reliable discourse information on the one hand and the effectiveness of exploiting the reflexes of discourse in the sentence-internal material on the other – can we nevertheless expect to gain something from adding sentence-external feature information? We propose two scena"
E12-1078,W07-2303,1,0.935476,"pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfe"
E12-1078,P09-2025,1,0.85901,"rnal realisation implicitly carries a lot of imformation about discourse context. On average, the morphosyntactic properties of constituents in a text are better approximates of their discourse status than actual coreference relations. This result feeds into a number of research questions concerning the representation of discourse and its application in generation systems. Although we should certainly not expect a computational model to achieve a perfect accuracy in the constituent ordering task – even humans only agree to a certain extent in rating word order variants (Belz and Reiter, 2006; Cahill, 2009) – the average accuracy in the 60’s for prediction of Vorfeld occupance is still moderate. An obvious direction would be to further investigate more complex representations of discourse that take into account the relations between utterances, such as topic shifts. Moreover, it is not clear whether the effects we find for linearisation in this paper carry over to other levels of generation such as tactical generation where syntactic functions are not fully specified. In a broader perspective, our results underline the need for better formalisations of discourse that can be translated into featu"
E12-1078,P10-1020,0,0.0149304,"ited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfeld (or “prefield”). Filippova and Strube (2007) show that once the Vorfeld (i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the Mittelfeld (i.e. the constituents that follow the finite verb) is very easy. Cheung and Penn (2010) extend the approach of Filippova and Strube (2007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 2 3 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of r"
E12-1078,J10-3005,0,0.0482,"Missing"
E12-1078,N09-2057,0,0.0249541,"btained from coreference and lexical chain relations. pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial posit"
E12-1078,J95-2003,0,0.940218,"ference relations. As we get the same effect in both setups – the sentenceexternal features do not improve over a baseline that captures basic morphosyntactic properties of the constituents – we conclude that sentenceinternal realisation is actually a relatively accurate predictor of discourse context, even more accurate than information that can be obtained from coreference and lexical chain relations. pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries i"
E12-1078,J09-1003,0,0.017767,"007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 2 3 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and LaMotivation While there would be many ways to construe or represent discourse context (e.g. in terms of the global discourse or information structure), we concentrate on capturing local coherence through the distribution of discourse referents in a text. These discourse referents basically correspond to the constituents that our surface realisation model has to put in the right order. As the order of referents or constituents is arguably influenced by the information structure of a sentence given th"
E12-1078,W05-0311,0,0.0198734,"nd Zinsmeister, 2009) have made some good progress towards a coherenceoriented account of at least the left edge of the German clause structure, the Vorfeld constituent. What makes the technological application of theoretical insights even harder is that for most relevant factors, automatic recognition cannot be performed with high accuracy (e.g., a coreference accuracy in the 70’s means there is a good deal of noise) and for the higher-level notions such as the information-structural focus, interannotator agreement on real corpus data tends to be much lower than for core-grammatical notions (Poesio and Artstein, 2005; Ritz et al., 2008). On the other hand, many of the relevant discourse factors are reflected indirectly in properties of the sentence-internal material. Most notably, knowing the shape of referring expressions narrows down many aspects of givenness and salience of its referent; pronominal realizations indicate givenness, and in German there are even two variants of the personal pronoun (er and der) for distinguishing salience. So, if the generation task is set in such a way that the actual lexical choice, including functional categories such as determiners, is fully fixed (which is of course"
E12-1078,J04-3003,0,0.0644019,"Missing"
E12-1078,C04-1097,0,0.267231,"ormation that can be obtained from coreference and lexical chain relations. pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted t"
E12-1078,ritz-etal-2008-annotation,0,0.025282,"Missing"
E12-1078,rohrer-forst-2006-improving,0,0.0276056,"nt that investigates sentence-external context in a surface realisation task. The sentence-external context is represented in terms of lexical chain features and compared to sentence-internal models which are based on morphosyntactic features. The experiment thus targets a generation scenario where no coreference information is available and aims at assessing whether relatively naive context information is also useful. 4.1 System Description We carry out our first experiment in a regeneration set-up with two components: a) a largescale hand-crafted Lexical Functional Grammar (LFG) for German (Rohrer and Forst, 2006), used to parse and regenerate a corpus sentence, b) a stochastic ranker that selects the most appropriate regenerated sentence in context according to an underlying, linguistically motivated feature model. In contrast to fully statistical linearisation methods, our system first generates the full set of sentences that correspond to the grammatically well-formed realisations of the intermediate syntactic representation.1 This representation is an f-structure, which underspecifies the order of constituents and, to some extent, their morphological realisation, such that the output sentences cont"
E12-1078,2005.mtsummit-papers.15,0,0.019652,"lexical chain relations. pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentenc"
E12-1078,J91-1002,0,\N,Missing
E14-1061,N12-1047,0,0.0368869,"Models were trained with the default settings of the Moses SMT toolkit, v1.0 (Koehn et al., 2007) using the data from the EACL 2009 workshop on statistical machine translation. All compound processing systems are trained and tuned identically, except using different C RF models for compound prediction. All training data was split and reduced to the underspecified representation described in Section 4. We used KenLM (Heafield, 2011) with SRILM (Stolcke, 2002) to train a 5-gram language model based on all available target language training data. For tuning, we used batch-mira with ‘safe-hope’ (Cherry and Foster, 2012) and ran it separately for every experiment. We integrated the 9 We used pair-wise bootstrap resampling with sample size 1000 and p-value 0.05, from: http://www.ark.cs.cmu.edu/MT 584 lexically matches the reference correct translation wrong translation group ID 1a: perfect match 1b: inflection wrong 2a: merging wrong 2b: no merging 3a: compound 3b: no compound 4a: compound 4b: no compound example reference english Inflationsrate Inflationsrate inflation rate Rohstoffpreisen Rohstoffpreise raw material prices Anwaltsbewegung Anw¨altebewegung lawyers movement Polizei Chef Polizeichef police chie"
E14-1061,schmid-etal-2004-smor,0,0.288925,"e all of the morphological information stripped from the underspecified representation. Note that erroneous over-splitting might make the correct merging of compounds difficult3 (or even impossible), due to the number of correct decisions required. For example, it requires only 1 correct prediction to recombine “Niederschlag|Menge” into “Niederschlagsmenge” (“amount of precipitation”) but 3 for the wrong split into “nie|der|Schlag|Menge” (“never|the|hit|amount”). We use the compound splitter of Fritzinger and Fraser (2010), who have shown that using a rule-based morphological analyser (S MOR, Schmid et al. (2004)) drastically reduced the number of erroneous splits when compared to the frequency-based approach of Koehn and Knight (2003). However, we adapted it to work on tokens: some words can, depending on their context, either be interpreted as named entities or common nouns, e.g., “Dinkelacker” (a German beer brand or “spelt|field”).4 We parsed the training data and use the parser’s decisions to identify proper names, see “Baumeister” in Figure 2. After splitting, we use S MOR to reduce words to lemmas, keeping morphological features like gender or number, and stripping features like case, as ¨ illu"
E14-1061,E12-1068,1,0.889999,"rred in the parallel training data. As parallel training data is limited, it is desirable to extract as much information from it as possible. We present an approach for compound processing in SMT, translating from English to German, that splits compounds prior to training (in order to access the individual words which together form the compound) and recombines them after translation. While compound splitting is a well-studied task, compound merging has not received as much attention in the past. We start from Stymne and Cancedda (2011), who used sequence models to predict compound merging and Fraser et al. (2012) who, in addition, generalise over German inflection. Our new contributions are: (i) We project 2 Dealing with Compounds in SMT In German, two (or more) single words (usually nouns or adjectives) are combined to form a compound which is considered a semantic unit. The rightmost part is referred to as the head while all other parts are called modifiers. E XAMPLE (1) lists different ways of joining simple words into compounds: mostly, no modification is required (A) or a filler letter is introduced (B). More rarely, a letter is deleted (C), or transformed (D). 579 Proceedings of the 14th Confere"
E14-1061,W10-1734,1,0.959617,"(D) Kriterium+Liste = Kriterienliste (“criteria list”) In the past, there have been numerous attempts to split compounds, all improving translation quality when translating from a compounding to a noncompounding language. Several compound splitting approaches make use of substring corpus frequencies in order to find the optimal split points of a compound (e.g. Koehn and Knight (2003), who allowed only “(e)s” as filler letters). Stymne et al. (2008) use Koehn and Knight’s technique, include a larger list of possible modifier transformations and apply P OS restrictions on the substrings, while Fritzinger and Fraser (2010) use a morphological analyser to find only linguistically motivated substrings. In contrast, Dyer (2010) presents a latticebased approach to encode different segmentations of words (instead of finding the one-best split). More recently, Macherey et al. (2011) presented a language-independent unsupervised approach in which filler letters and a list of words not to be split (e.g., named entities) are learned using phrase tables and Levenshtein distance. German compounds are highly productive,1 and traditional SMT approaches often fail in the face of such productivity. Therefore, special processi"
E14-1061,W11-2129,0,0.322661,"translation (SMT) approaches, because words can only be translated as they have occurred in the parallel training data. As parallel training data is limited, it is desirable to extract as much information from it as possible. We present an approach for compound processing in SMT, translating from English to German, that splits compounds prior to training (in order to access the individual words which together form the compound) and recombines them after translation. While compound splitting is a well-studied task, compound merging has not received as much attention in the past. We start from Stymne and Cancedda (2011), who used sequence models to predict compound merging and Fraser et al. (2012) who, in addition, generalise over German inflection. Our new contributions are: (i) We project 2 Dealing with Compounds in SMT In German, two (or more) single words (usually nouns or adjectives) are combined to form a compound which is considered a semantic unit. The rightmost part is referred to as the head while all other parts are called modifiers. E XAMPLE (1) lists different ways of joining simple words into compounds: mostly, no modification is required (A) or a filler letter is introduced (B). More rarely, a"
E14-1061,W11-2123,0,0.0223943,"R (14.61) and the U NSPLIT baseline (14.74) is not statistically significant. Translation Performance We integrated our compound processing pipeline into an end-to-end SMT system. Models were trained with the default settings of the Moses SMT toolkit, v1.0 (Koehn et al., 2007) using the data from the EACL 2009 workshop on statistical machine translation. All compound processing systems are trained and tuned identically, except using different C RF models for compound prediction. All training data was split and reduced to the underspecified representation described in Section 4. We used KenLM (Heafield, 2011) with SRILM (Stolcke, 2002) to train a 5-gram language model based on all available target language training data. For tuning, we used batch-mira with ‘safe-hope’ (Cherry and Foster, 2012) and ran it separately for every experiment. We integrated the 9 We used pair-wise bootstrap resampling with sample size 1000 and p-value 0.05, from: http://www.ark.cs.cmu.edu/MT 584 lexically matches the reference correct translation wrong translation group ID 1a: perfect match 1b: inflection wrong 2a: merging wrong 2b: no merging 3a: compound 3b: no compound 4a: compound 4b: no compound example reference en"
E14-1061,E03-1076,0,0.675514,"training data. ting and merging, we thus report on previous approaches for both of these tasks. E XAMPLE (1) (A) Haus+Boot = Hausboot (“house boat”) (B) Ort+s+Zeit = Ortszeit (“local time”) (C) Kirche-e+Turm = Kirchturm (“church tower”) (D) Kriterium+Liste = Kriterienliste (“criteria list”) In the past, there have been numerous attempts to split compounds, all improving translation quality when translating from a compounding to a noncompounding language. Several compound splitting approaches make use of substring corpus frequencies in order to find the optimal split points of a compound (e.g. Koehn and Knight (2003), who allowed only “(e)s” as filler letters). Stymne et al. (2008) use Koehn and Knight’s technique, include a larger list of possible modifier transformations and apply P OS restrictions on the substrings, while Fritzinger and Fraser (2010) use a morphological analyser to find only linguistically motivated substrings. In contrast, Dyer (2010) presents a latticebased approach to encode different segmentations of words (instead of finding the one-best split). More recently, Macherey et al. (2011) presented a language-independent unsupervised approach in which filler letters and a list of words"
E14-1061,W08-0317,0,0.513437,"s for both of these tasks. E XAMPLE (1) (A) Haus+Boot = Hausboot (“house boat”) (B) Ort+s+Zeit = Ortszeit (“local time”) (C) Kirche-e+Turm = Kirchturm (“church tower”) (D) Kriterium+Liste = Kriterienliste (“criteria list”) In the past, there have been numerous attempts to split compounds, all improving translation quality when translating from a compounding to a noncompounding language. Several compound splitting approaches make use of substring corpus frequencies in order to find the optimal split points of a compound (e.g. Koehn and Knight (2003), who allowed only “(e)s” as filler letters). Stymne et al. (2008) use Koehn and Knight’s technique, include a larger list of possible modifier transformations and apply P OS restrictions on the substrings, while Fritzinger and Fraser (2010) use a morphological analyser to find only linguistically motivated substrings. In contrast, Dyer (2010) presents a latticebased approach to encode different segmentations of words (instead of finding the one-best split). More recently, Macherey et al. (2011) presented a language-independent unsupervised approach in which filler letters and a list of words not to be split (e.g., named entities) are learned using phrase ta"
E14-1061,P07-2045,0,0.00594032,"but without compound processing (U NSPLIT). Table 4 shows that only U NSPLIT and S TR (source language and a reduced set of target language features) are significantly9 improving over the R AW baseline. They also significantly outperform all other systems, except S T (full source and target language feature set). The difference between S TR (14.61) and the U NSPLIT baseline (14.74) is not statistically significant. Translation Performance We integrated our compound processing pipeline into an end-to-end SMT system. Models were trained with the default settings of the Moses SMT toolkit, v1.0 (Koehn et al., 2007) using the data from the EACL 2009 workshop on statistical machine translation. All compound processing systems are trained and tuned identically, except using different C RF models for compound prediction. All training data was split and reduced to the underspecified representation described in Section 4. We used KenLM (Heafield, 2011) with SRILM (Stolcke, 2002) to train a 5-gram language model based on all available target language training data. For tuning, we used batch-mira with ‘safe-hope’ (Cherry and Foster, 2012) and ran it separately for every experiment. We integrated the 9 We used p"
E14-1061,E09-3008,0,0.0820407,"Missing"
E14-1061,P08-1059,0,0.139495,"d representation we are using allows for maximal generalisation over word parts independent of their position of occurrence or inflectional realisations. Moreover, their experiments were limited to predicting compounds on held-out data; no results were reported for using their approach in translation. In Fraser et al. (2012) we re-implemented the approach of Stymne and Cancedda (2011), combined it with inflection prediction and applied it to a translation task. However, compound merging was restricted to a list of compounds and parts. Our present work facilitates more independent combination. Toutanova et al. (2008) and Weller et al. (2013) used source language features for target language inflection, but to our knowledge, none of these works applied source language features for compound merging. 4 Step 1: Underspecified Representation 3 In contrast, they may not hurt translation quality in the other direction, where phrase-based SMT is likely to learn the split words as a phrase and thus recover from that error. 4 Note that Macherey et al. (2011) blocked splitting of words which can be used as named entities, independent of context, which is less general than our solution. In order to enhance translatio"
E14-1061,P13-1058,1,0.831988,"g allows for maximal generalisation over word parts independent of their position of occurrence or inflectional realisations. Moreover, their experiments were limited to predicting compounds on held-out data; no results were reported for using their approach in translation. In Fraser et al. (2012) we re-implemented the approach of Stymne and Cancedda (2011), combined it with inflection prediction and applied it to a translation task. However, compound merging was restricted to a list of compounds and parts. Our present work facilitates more independent combination. Toutanova et al. (2008) and Weller et al. (2013) used source language features for target language inflection, but to our knowledge, none of these works applied source language features for compound merging. 4 Step 1: Underspecified Representation 3 In contrast, they may not hurt translation quality in the other direction, where phrase-based SMT is likely to learn the split words as a phrase and thus recover from that error. 4 Note that Macherey et al. (2011) blocked splitting of words which can be used as named entities, independent of context, which is less general than our solution. In order to enhance translation model accuracy, it is r"
E14-1061,P11-1140,0,0.229461,"use of substring corpus frequencies in order to find the optimal split points of a compound (e.g. Koehn and Knight (2003), who allowed only “(e)s” as filler letters). Stymne et al. (2008) use Koehn and Knight’s technique, include a larger list of possible modifier transformations and apply P OS restrictions on the substrings, while Fritzinger and Fraser (2010) use a morphological analyser to find only linguistically motivated substrings. In contrast, Dyer (2010) presents a latticebased approach to encode different segmentations of words (instead of finding the one-best split). More recently, Macherey et al. (2011) presented a language-independent unsupervised approach in which filler letters and a list of words not to be split (e.g., named entities) are learned using phrase tables and Levenshtein distance. German compounds are highly productive,1 and traditional SMT approaches often fail in the face of such productivity. Therefore, special processing of compounds is required for translation into German, as many compounds will not (e.g. Hausboot, “house boat”) or only rarely have been seen in the training data.2 In contrast, most compounds consist of two (or more) simple words that occur more frequently"
E14-1061,P02-1040,0,\N,Missing
J05-3003,P87-1027,0,0.109086,"and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb classes (based on Levin [1993]) for obtaining more accurate back-off estimates for hypothesis selection. Carroll and Rooth (1998) use a handwritten head-lexicalized, context-free grammar"
J05-3003,J93-2002,0,0.800674,"ter detail in Section 6, in which we compare our results with those reported elsewhere in the literature. We will divide more-general approaches to subcategorization frame acquisition into two groups: those which extract information from raw text and those which use preparsed and hand-corrected treebank data as their input. Typically in the approaches based on raw text, a number of subcategorization patterns are predefined, a set of verb subcategorization frame associations are hypothesized from the data, and statistical methods are applied to reliably select hypotheses for the final lexicon. Brent (1993) relies on morphosyntactic cues in the untagged Brown corpus as indicators of six predefined subcategorization frames. The frames do not include details of specific prepositions. Brent used hypothesis testing on binomial frequency data to statistically filter the induced frames. Ushioda et al. (1993) run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes. The experiment is limited by the fact that all prepositional phrases are treated as adjuncts. Ushioda et al. (1993) employ an additional statistical method based"
J05-3003,A97-1052,0,0.911163,"and the constituents with which they co-occur. He assumes 19 different subcategorization 334 O’Donovan et al. Large-Scale Induction and Evaluation of Lexical Resources frame definitions, and the extracted frames include details of specific prepositions. The extracted frames are noisy as a result of parser errors and so are filtered using the binomial hypothesis theory (BHT), following Brent (1993). Applying his technique to approximately four million words of New York Times newswire, Manning acquired 4,900 verb-subcategorization frame pairs for 3,104 verbs, an average of 1.6 frames per verb. Briscoe and Carroll (1997) predefine 163 verbal subcategorization frames, obtained by manually merging the classes exemplified in the COMLEX (MacLeod, Grishman, and Meyers 1994) and ANLT (Boguraev et al. 1987) dictionaries and adding around 30 frames found by manual inspection. The frames incorporate control information and details of specific prepositions. Briscoe and Carroll (1997) refine the BHT with a priori information about the probabilities of subcategorization frame membership and use it to filter the induced frames. Recent work by Korhonen (2002) on the filtering phase of this approach uses linguistic verb cla"
J05-3003,Y04-1016,1,0.884078,"Missing"
J05-3003,P04-1041,1,0.777592,"Missing"
J05-3003,2000.iwpt-1.9,0,0.104136,"e (Carroll and Rooth 1998). Manning (1993) argues that, aside from missing domain-specific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature. Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue. Aside from the extraction of theory-neutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG, CCG, and HPSG (Chen and Vijay-Shanker 2000; Xia 1999; Hockenmaier, Bierner, and Baldridge 2004; Nakanishi, Miyao, and Tsujii 2004). In this article we present an approach to automating the process of lexical acquisition for LFG (i.e., grammatical-function-based systems). However, our approach also generalizes to CFG category-based approaches. In LFG, subcategorization requirements are enforced through semantic forms specifying which grammatical functions are required by a particular predicate. Our approach is based on earlier work on LFG semantic form extraction (van Genabith, Sadler, and Way 1999) and recent progress in automatically"
J05-3003,P97-1003,0,0.14526,"n of Collins’s (1997) approach to the differentiation between complement and adjunct. This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the approaches of Magerman (1994) and Collins (1997). Then the elementary trees are read off in a quite straightforward manner. Finally any invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics. The number of frame types extracted by Xia (1999) ranged from 3,014 to 6,099. Hockenmaier, Bierner, and Baldridge (2004) outline a method for the automatic extraction of a large syntactic CCG lexicon from the Penn-II Treebank. For each tree, the algorithm annotates the nodes with CCG categories in a top-down recursive manner. The first step is to label each node as either a head,"
J05-3003,C94-1042,0,0.276371,"Missing"
J05-3003,J93-1005,0,0.22634,"Missing"
J05-3003,W03-2401,0,0.0288161,"Missing"
J05-3003,kingsbury-palmer-2002-treebank,0,0.025675,"Missing"
J05-3003,kinyon-prolo-2002-identifying,0,0.479907,"Missing"
J05-3003,P98-1115,0,0.0235922,"Missing"
J05-3003,H94-1003,0,0.0569859,"Missing"
J05-3003,P95-1037,0,0.00938586,"has been carried out on the extraction of formalism-specific lexical resources from the Penn-II Treebank, in particular TAG, CCG, and HPSG. As these formalisms are fully lexicalized with an invariant (LTAG and CCG) or limited (HPSG) rule component, the extraction of a lexicon essentially amounts to the creation of a grammar. Chen and Vijay-Shanker (2000) explore a number of related approaches to the extraction of a lexicalized TAG from the Penn-II Treebank with the aim of constructing a statistical model for parsing. The extraction procedure utilizes a head percolation table as introduced by Magerman (1995) in combination with a variation of Collins’s (1997) approach to the differentiation between complement and adjunct. This results in the construction of a set of lexically anchored elementary trees which make up the TAG in question. The number of frame types extracted (i.e., an elementary tree without a specific lexical anchor) ranged from 2,366 to 8,996. Xia (1999) also presents a similar method for the extraction of a TAG from the Penn Treebank. The extraction procedure consists of three steps: First, the bracketing of the trees in the Penn Treebank is corrected and extended based on the app"
J05-3003,P93-1032,0,0.746649,"is the subcategorization requirements of an entry (i.e., the arguments a predicate must take in order to form a grammatical construction). Lexicons, including subcategorization details, were traditionally produced by hand. However, as the manual construction of lexical resources is time consuming, error prone, expensive, and rarely ever complete, it is often the case that the limitations of NLP systems based on lexicalized approaches are due to bottlenecks in the lexicon component. In addition, subcategorization requirements may vary across linguistic domain or genre (Carroll and Rooth 1998). Manning (1993) argues that, aside from missing domain-specific complementation trends, dictionaries produced by hand will tend to lag behind real language use because of their static nature. Given these facts, research on automating acquisition of dictionaries for lexically based NLP systems is a particularly important issue. Aside from the extraction of theory-neutral subcategorization lexicons, there has also been work in the automatic construction of lexical resources which comply with the principles of particular linguistic theories such as LTAG, CCG, and HPSG (Chen and Vijay-Shanker 2000; Xia 1999; Hoc"
J05-3003,H94-1020,0,0.0194078,"-structure information. F-structures are attribute–value structures which represent abstract syntactic information, approximating to basic predicate–argument–modifier structures. Most of the early work on automatic f-structure annotation (e.g., van Genabith, Way, and Sadler 1999; Frank 2000; Sadler, van Genabith, and Way 2000) was applied only to small data sets (fewer than 200 sentences) and was largely proof of concept. However, more recent work (Cahill et al. 2002; Cahill, McCarthy, et al. 2004) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank (Marcus et al. 1994), containing more than 1,000,000 words and 49,000 sentences. We utilize the automatic annotation algorithm of Cahill et al. (2002) and Cahill, McCarthy, et al. (2004) to derive a version of Penn-II in which each node in each tree is annotated with LFG functional annotations in the form of attribute-value structure equations. The algorithm uses categorial, configurational, local head, and Penn-II functional and trace information. The annotation procedure is dependent on locating the head daughter, for which an amended version of Magerman (1994) is used. The head is annotated with the LFG equati"
J05-3003,P04-1047,1,0.624486,"Missing"
J05-3003,P98-2184,0,0.0423901,"in higher recall scores than those achieved when we (effectively) reversed the mapping (COMLEX-LFG Mapping II [Section 6.3]). The first mapping is essentially a conflation of our more fine-grained LFG grammatical functions with the more generic COMLEX functions, while the second mapping tries to maintain as many distinctions as possible. Another drawback to using an existing external gold standard such as COMLEX to evaluate an automatically induced subcategorization lexicon is that the resources are not necessarily constructed from the same source data. As noted above, it is well documented (Roland and Jurafsky 1998) that subcategorization frames (and their frequencies) vary across domains. We have extracted frames from two sources (the WSJ and the Brown corpus), whereas COMLEX was built using examples from the San Jose Mercury News, the Brown corpus, several literary works from the Library of America, scientific abstracts from the U.S. Department of Energy, and the WSJ. For this reason, it is likely to contain a greater variety of subcategorization frames than our induced lexicon. It is also possible that because of human error, COMLEX contains subcategorization frames the validity of which are in doubt,"
J05-3003,C00-2100,0,0.138208,"ts of verb occurrences in the Penn-II Treebank. This is made possible by manual examination of more than 150 different sequences of syntactic and functional tags in the treebank. Each of these sequences was categorized as a modifier or argument. Arguments were then mapped to traditional syntactic functions. For example, the tag sequence NP-SBJ denotes a mandatory argument, and its syntactic function is subject. In general, argumenthood was preferred over adjuncthoood. As Kinyon and Prolo (2002) does not include an evaluation, currently it is impossible to say how effective their technique is. Sarkar and Zeman (2000) present an approach to learn previously unknown frames for Czech from the Prague Dependency Bank (Hajic 335 Computational Linguistics Volume 31, Number 3 1998). Czech is a language with a freer word order than English and so configurational information cannot be relied upon. In a dependency tree, the set of all dependents of the verb make up a so-called observed frame, whereas a subcategorization frame contains a subset of the dependents in the observed frame. Finding subcategorization frames involves filtering adjuncts from the observed frame. This is achieved using three different hypothesi"
J05-3003,schulte-im-walde-2002-subcategorisation,0,0.146073,"Missing"
J05-3003,C02-1145,0,0.0658598,"Missing"
J05-3003,W98-1505,0,\N,Missing
J05-3003,C98-2179,0,\N,Missing
J05-3003,C98-1111,0,\N,Missing
J08-1003,J97-4005,0,0.0398419,"Missing"
J08-1003,baldwin-etal-2004-road,0,0.0160361,"btain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive effo"
J08-1003,H91-1060,0,0.0647814,"Missing"
J08-1003,E03-1005,0,0.0139868,"tensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage"
J08-1003,J93-1002,0,0.305911,"0], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 howev"
J08-1003,P06-2006,0,0.0594671,"Missing"
J08-1003,W02-1503,0,0.0261788,"uages do not always interpret linguistic material locally where the material is encountered in the string (or tree). In order to obtain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf"
J08-1003,P04-1041,1,0.839632,"Missing"
J08-1003,C02-1013,0,0.117113,"al locally where the material is encountered in the string (or tree). In order to obtain accurate and complete predicate–argument, dependency, or logical form representations, a hallmark of deep grammars is that they usually involve a long-distance dependency (LDD) resolution mechanism. Traditionally, deep grammars are hand-crafted (cf. the ALVEY Natural Language Tools [Briscoe et al. 1987], the Core Language Engine [Alshawi and Pulman 1992], the Alpino Dutch dependency parser [Bouma, van Noord, and Malouf 2000], the Xerox Linguistic Environment [Butt et al. 2002], the RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been success"
J08-1003,A00-2018,0,0.01612,"Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically"
J08-1003,P04-1014,0,0.0567308,"Missing"
J08-1003,P07-1032,0,0.0390822,"Missing"
J08-1003,P97-1003,0,0.151407,"arsers). The article also reports on a task-based evaluation experiment to rank the parsers using the grammatical relations as input to an anaphora resolution system. Preiss concluded that parser ranking using grammatical relations reﬂected the absolute ranking (between treebank-induced parsers) using traditional tree-based metrics, but that the difference between the performance of the parsing algorithms narrowed when they carried out the anaphora resolution task. Her results show that the hand-crafted deep uniﬁcation parser (Briscoe and Carroll 1993) outperforms the machine-learned parsers (Collins 1997; Charniak 2000) on the f-score derived from weighted precision and recall on grammatical relations.4 Kaplan et al. (2004) compare their deep, handcrafted, LFG-based XLE parsing system (Riezler et al. 2002) with Collins’s (1999) model 3 using a simple conversion-based approach, capturing dependencies from the tree output of the machine-learned parser, and evaluating both parsers against the PARC 700 Dependency Bank (King et al. 2003). They conclude that the hand-crafted, deep grammar outperforms the state-of-the-art treebank-based shallow parser on the level of dependency representation, at th"
J08-1003,W03-1005,0,0.0353103,"Missing"
J08-1003,C86-1129,0,0.453477,"ion, as well as traces and the Penn-II functional tag annotations (Table 1) to automatically associate Penn-II CFG trees with LFG f-structure information. Given a tree, such as the Penn-II-style tree in Figure 2, the algorithm will traverse the tree and deterministically add f-structure equations to the phrasal and leaf nodes of the tree, resulting in an f-structure annotated version of the tree. The annotations are then collected and passed on to a constraint solver which generates an f-structure (if the constraints are satisﬁable). We use a simple graph-uniﬁcation-based constraint solver ¨ (Eisele and Dorre 1986), extended to handle path, set-valued, disjunctive, and existential constraints. Given parser output without Penn-II style annotations and traces, the same algorithm is used to assign annotations to each node in the tree, whereas a separate module is applied at the level of f-structure to resolve any long-distance dependencies (see Section 2.3). 5 van Genabith and Crouch (1996, 1997) provide translations between f-structures, Quasi-Logical Forms (QLFs), and Underspeciﬁed Discourse Representation Structures (UDRSs). 86 Cahill et al. Statistical Parsing Using Automatic Dependency Structures Tabl"
J08-1003,W03-1008,0,0.0100689,"resent research on inducing Penn-II treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced CCG-based models including LDD resolution. It would be interesting to conduct a comparative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive comparison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RASP parsing systems. 117 Computational Linguistics Volume 34, Number 1 Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automatically induced CCG and HPSG resources. PropBank-based evaluations provide valuable information to rank parsing systems. Currently, however, PropBank-based evaluations are somewhat partial: They only represent (and hence score) verbal arguments and disregard a raft of other semantically important dependencies (e.g., temporal and aspectual information, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS 500 Dependency Banks.31 7. Conclusions Parser compa"
J08-1003,P03-1046,0,0.0243758,"results show that machine-learning-based resources can outperform deep, state-of-the-art hand-crafted resources with respect to the quality of dependencies generated. Treebank-based, deep and wide-coverage constraint-based grammar acquisition has become an important research topic: Starting with the seminal TAG-based work of Xia (1999), there are now also HPSG-, CCG- and LFG-based approaches. Miyao, Ninomiya, and Tsujii (2003) and Tsuruoka, Miyao, and Tsujii (2004) present research on inducing Penn-II treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced CCG-based models including LDD resolution. It would be interesting to conduct a comparative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive comparison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RASP parsing systems. 117 Computational Linguistics Volume 34, Number 1 Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automati"
J08-1003,P02-1043,0,0.347852,"and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage deep grammars from treebank resources (TAG [Xia 1999], CCG [Hockenmaier and Steedman 2002], HPSG [Miyao, Ninomiya, and Tsujii 2003], LFG [Cahill et al. 2002b, 2004]). To a ﬁrst approximation, these approaches can be classiﬁed as “conversion”- or “annotation”-based. TAG-based approaches convert 1 Our use of the terms “shallow” and “deep” parsers/grammars follows Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often ﬁnite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations"
J08-1003,P02-1018,0,0.00917585,"s Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often ﬁnite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations we mean deep, ﬁne-grained, labeled dependencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume 34, Number 1 treebank trees into (lexicalized) ele"
J08-1003,N04-1013,1,0.397922,"is (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep grammars relate strings to informatio"
J08-1003,W03-2401,1,0.923661,"ular style of linguistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep gram"
J08-1003,P03-1054,0,0.0412189,"ime-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are shallow: They usually do not attempt to resolve LDDs nor do they associate strings with meaning representations. Over the last few years, addressing the knowledge acquisition bottleneck in deep constraint-based grammar development, a growing body of research has emerged to automatically acquire wide-coverage deep grammars from treeba"
J08-1003,P04-1042,0,0.0169367,"pendencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume 34, Number 1 treebank trees into (lexicalized) elementary or adjunct trees. CCG-based approaches convert trees into CCG derivations from which CCG categories can be extracted. HPSGand LFG-based grammar induction methods automatically annotate treebank trees with (typed) attribute-value structure information for the extraction of constraint-based grammars and lexical resources. Two recent papers (Preiss 2003; Kaplan et al"
J08-1003,H94-1020,0,0.0187269,"of (related) drawbacks: 1. Bracketed trees do not always provide NLP applications with enough information to carry out the required tasks: Many applications involve a deeper analysis of the input in the form of semantically motivated information such as deep dependency relations, predicate–argument structures, or simple logical forms. 2. A number of alternative, but equally valid tree representations can potentially be given for the same input. To give just a few examples: In English, VPs containing modals and auxiliaries can be analyzed using (predominantly) binary branching rules (Penn-II [Marcus et al. 1994]), or employ ﬂatter analyses where modals and auxiliaries are sisters of the main verb (AP treebank [Leech and Garside 1991]), or indeed do without a designated VP constituent at all (SUSANNE [Sampson 1995]). Treebank bracketing guidelines can use “traditional” CFG categories such as S, NP, and so on (Penn-II) or a maximal projection-inspired analysis with IPs and DPs (Chinese Penn Treebank [Xue et al. 2004]). 3. Because a tree-based gold standard for parser evaluation must adopt a particular style of linguistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), e"
J08-1003,E06-1011,0,0.0298537,"use of the terms “shallow” and “deep” parsers/grammars follows Kaplan et al. (2004) where a “shallow parser” does not relate strings to meaning representations. This deviates from a more common use of the terms where, for example, a “shallow parser” refers to (often ﬁnite-state-based) parsers (or chunkers) that may produce partial bracketings of input strings. 2 By dependency relations we mean deep, ﬁne-grained, labeled dependencies that encode long-distance dependencies and passive information, for example. These differ from the types of unlabeled dependency relations in other work such as (McDonald and Pereira 2006). 3 Both Collins Model 3 (1999) and Johnson (2002) output CFG tree representations with traces. Collins Model 3 performs LDD resolution for wh-relative clause constructions, Johnson (2002) for a wide range of LDD phenomena in a post-processing approach based on Penn-II tree fragments linking displaced material with where it is to be interpreted semantically. The work of Dienes and Dubey (2003) and Levy and Manning (2004) is similar to that of Johnson (2002), recovering empty categories on top of CFG-based parsers. None of them map strings into dependencies. 83 Computational Linguistics Volume"
J08-1003,C04-1204,0,0.0126897,"I treebank-based HPSGs with log-linear probability models. Hockenmaier and Steedman (2002) and Hockenmaier (2003) provide treebank-induced CCG-based models including LDD resolution. It would be interesting to conduct a comparative evaluation involving treebank-based HPSG, CCG, and LFG resources against the PARC 700 and CBS 500 Dependency Banks to enable more comprehensive comparison of machine-learning-based with hand-crafted, deep, wide-coverage resources such as those used in the XLE or RASP parsing systems. 117 Computational Linguistics Volume 34, Number 1 Gildea and Hockenmaier (2003) and Miyao and Tsujii (2004) present PropBank (Kingsbury, Palmer, and Marcus 2002)-based evaluations of their automatically induced CCG and HPSG resources. PropBank-based evaluations provide valuable information to rank parsing systems. Currently, however, PropBank-based evaluations are somewhat partial: They only represent (and hence score) verbal arguments and disregard a raft of other semantically important dependencies (e.g., temporal and aspectual information, dependencies within nominal clusters, etc.) as captured in the PARC 700 or CBS 500 Dependency Banks.31 7. Conclusions Parser comparison is a non-trivial and t"
J08-1003,P04-1047,1,0.924795,"Missing"
J08-1003,E03-1025,0,0.233427,"uistic analysis (reﬂected in the geometry and nomenclature of the nodes in the trees), evaluation of statistical parsers and grammars that are derived from particular treebank resources (as well as hand-crafted grammars/parsers) can suffer unduly if the gold standard deviates systematically from the (possibly) equally valid style of linguistic analysis provided by the parser. Problems such as these have motivated research on more abstract, dependencybased parser evaluation (e.g., Lin 1995; Carroll, Briscoe, and Sanﬁlippo 1998; Carroll et al. 2002; Clark and Hockenmaier 2002; King et al. 2003; Preiss 2003; Kaplan et al. 2004; Miyao and Tsujii 2004). Dependency-based linguistic representations are approximations of abstract predicate-argument-adjunct (or more basic head-dependent) 82 Cahill et al. Statistical Parsing Using Automatic Dependency Structures structures, providing a more normalized representation abstracting away from the particulars of surface realization or CFG-tree representation, which enables meaningful cross-parser evaluation. A related contrast holds between shallow and deep grammars and parsers.1 In addition to deﬁning a language (as a set of strings), deep grammars relate s"
J08-1003,P02-1035,1,0.159909,"RASP dependency parser [Carroll and Briscoe 2002] and the LinGO English Resource Grammar [Flickinger 2000; Baldwin et al. 2004]). Wide-coverage, deep-grammar development, particularly in rich formalisms such as LFG (Kaplan and Bresnan 1982; Bresnan 2001; Dalrymple 2001) and HPSG (Pollard and Sag 1994), is knowledge-intensive, time-consuming and expensive, constituting an instance of the (in-)famous “knowledge acquisition bottleneck” familiar from other areas in traditional, rule-based AI and NLP. Very few hand-crafted deep grammars (Briscoe and Carroll 1993; Bouma, van Noord, and Malouf 2000; Riezler et al. 2002) have, in fact, been successfully scaled to unrestricted input. The last 15 years have seen extensive efforts on treebank-based automatic grammar acquisition using a variety of machine-learning techniques (e.g., Gaizauskas 1995; Charniak 1996; Collins 1999; Johnson 1999; Charniak 2000; Bikel 2002; Bod 2003; Klein and Manning 2003). These grammars are wide-coverage and robust and in contrast to manual grammar development, machine-learning-based grammar acquisition incurs relatively low development cost. With few notable exceptions,3 however, these treebank-induced wide-coverage grammars are sha"
J08-1003,C96-1045,1,0.742182,"Missing"
J08-1003,P97-1052,1,0.798137,"Missing"
J08-1003,C00-2137,0,0.0191341,"able tuples between the two systems. Approximate randomization produces shufﬂes by random assignments instead of evaluating all 2S possible assignments. Signiﬁcance levels are computed as the percentage of trials where the pseudo statistic, that is the test statistic computed on the shufﬂed data, is greater than or equal to the actual statistic, that is the test statistic computed on the test data. A sketch of an algorithm for approximate randomization testing is given in Figure 12. 21 Applications of this test to natural language processing problems can be found in Chinchor et al. (1993) and Yeh (2000). 104 Cahill et al. Statistical Parsing Using Automatic Dependency Structures Table 10 Comparing parsers evaluated against Section 22 dependencies (preds-only): p-values for approximate randomization test for 10,000,000 randomizations. PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags PCFG Parent-PCFG Collins M3 Charniak Bikel Bikel+Tags <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 <.0001 .0003 <.0001 <.0001 - Table 10 gives the p-values (the smallest ﬁxed level at which the null hypothesis can be rejected) for comparing each parser against all of the other"
J08-1003,J98-4004,0,\N,Missing
J08-1003,W96-0213,0,\N,Missing
J08-1003,J03-4003,0,\N,Missing
J08-1003,J93-3001,0,\N,Missing
J16-3005,C12-1027,0,0.0567206,"Missing"
J16-3005,C14-1185,0,0.0615309,"Missing"
J16-3005,P11-1030,0,0.598712,"logical differences. The goal of the current study is to give a full view of the string kernels approach and shed some light on why this approach works so well. 1. Introduction Using words as basic units is natural in textual analysis tasks such as text categorization, authorship identification, or plagiarism detection. Perhaps surprisingly, recent results indicate that methods handling the text at the character level can also be very effective (Lodhi et al. 2002; Kate and Mooney 2006; Sanderson and Guenter 2006; Popescu and Dinu 2007; Grozea, Gehl, and Popescu 2009; Escalante, Solorio, and ´ Montes-y-Gomez 2011; Popescu 2011; Popescu and Grozea 2012). By avoiding to explicitly consider features of natural language such as words, phrases, or meaning, an approach that works at the character level has an important advantage in that it is language independent and linguistic theory neutral. In this context, a state-of-the-art machine learning system for native language identification (NLI) that works at the character level is presented in this article. NLI is the task of identifying the native language of a writer, based on a text they have written in a language other than their mother tongue. This is an"
J16-3005,W13-1728,0,0.0179868,"Missing"
J16-3005,W13-1713,0,0.139191,"Missing"
J16-3005,D14-1142,1,0.738108,"Missing"
J16-3005,W13-1714,0,0.0566096,"Missing"
J16-3005,P06-1115,0,0.67215,"atures, this article offers insights into two kinds of language transfer effects, namely, word choice (lexical transfer) and morphological differences. The goal of the current study is to give a full view of the string kernels approach and shed some light on why this approach works so well. 1. Introduction Using words as basic units is natural in textual analysis tasks such as text categorization, authorship identification, or plagiarism detection. Perhaps surprisingly, recent results indicate that methods handling the text at the character level can also be very effective (Lodhi et al. 2002; Kate and Mooney 2006; Sanderson and Guenter 2006; Popescu and Dinu 2007; Grozea, Gehl, and Popescu 2009; Escalante, Solorio, and ´ Montes-y-Gomez 2011; Popescu 2011; Popescu and Grozea 2012). By avoiding to explicitly consider features of natural language such as words, phrases, or meaning, an approach that works at the character level has an important advantage in that it is language independent and linguistic theory neutral. In this context, a state-of-the-art machine learning system for native language identification (NLI) that works at the character level is presented in this article. NLI is the task of ident"
J16-3005,W14-3625,0,0.150686,"Missing"
J16-3005,E14-4019,0,0.446583,"proach that works well across languages could be useful, for example, in a streamlined intelligence application that is required to work efficiently on a wide range of languages for which NLP tools or language experts might not be readily available, or where the user does not desire a complex customization for each language. On the other hand, kernels based on a different kind of information (for example, syntactic and semantic information) can be combined with string kernels via MKL to improve accuracy in some specific situations. Indeed, others (Tetreault et al. 2012; Bykh and Meurers 2014; Malmasi and Dras 2014a) have obtained better NLI results by using ensemble models based on several features such as context-free grammar production rules, Stanford dependencies, function words, and part-of-speech p-grams, than using models based on each of these features alone. Combing string kernels with other kind of information is not covered in the present work. The second goal of this work is to investigate which properties of the simple kernelbased approach are the main driver behind the higher performance than more complex approaches that take words, lemmas, syntactic information, or even semantics into acc"
J16-3005,D14-1144,0,0.0256398,"Missing"
J16-3005,W13-1716,0,0.0268191,"Missing"
J16-3005,W13-1717,0,0.0360714,"Missing"
J16-3005,D09-1012,0,0.078541,"Missing"
J16-3005,W10-2926,0,0.0574653,"Missing"
J16-3005,R11-1091,1,0.871879,"The goal of the current study is to give a full view of the string kernels approach and shed some light on why this approach works so well. 1. Introduction Using words as basic units is natural in textual analysis tasks such as text categorization, authorship identification, or plagiarism detection. Perhaps surprisingly, recent results indicate that methods handling the text at the character level can also be very effective (Lodhi et al. 2002; Kate and Mooney 2006; Sanderson and Guenter 2006; Popescu and Dinu 2007; Grozea, Gehl, and Popescu 2009; Escalante, Solorio, and ´ Montes-y-Gomez 2011; Popescu 2011; Popescu and Grozea 2012). By avoiding to explicitly consider features of natural language such as words, phrases, or meaning, an approach that works at the character level has an important advantage in that it is language independent and linguistic theory neutral. In this context, a state-of-the-art machine learning system for native language identification (NLI) that works at the character level is presented in this article. NLI is the task of identifying the native language of a writer, based on a text they have written in a language other than their mother tongue. This is an interesting s"
J16-3005,W13-1735,1,0.783229,"thorship identification, where the native language of an author is just one piece of the puzzle (Estival et al. 2007). NLI can also play a key role in second language acquisition applications where NLI techniques are used to identify language transfer patterns that help teachers and students focus feedback and learning on particular areas of interest (Rozovskaya and Roth 2010; Jarvis and Crossley 2012). The system based on string kernels described in this article was initially designed to participate in the 2013 NLI Shared Task (Tetreault, Blanchard, and Cahill 2013) and obtained third place (Popescu and Ionescu 2013). The approach was further extended by Ionescu, Popescu, and Cahill (2014) to combine several string kernels via multiple kernel learning (MKL) (Gonen and Alpaydin 2011). The system revealed some interesting facts about the kinds of string kernels and kernel learning methods that worked well for NLI. For instance, one of the best performing kernels is the (histogram) intersection kernel, which is inspired from computer vision (Maji, Berg, and Malik 2008). Another kernel based on Local Rank Distance is inspired from biology (Ionescu 2013; Dinu, Ionescu, and Tomescu 2014). Two kernel classifiers"
J16-3005,D10-1094,0,0.012895,"LI is the task of identifying the native language of a writer, based on a text they have written in a language other than their mother tongue. This is an interesting sub-task in forensic linguistic applications such as plagiarism detection and authorship identification, where the native language of an author is just one piece of the puzzle (Estival et al. 2007). NLI can also play a key role in second language acquisition applications where NLI techniques are used to identify language transfer patterns that help teachers and students focus feedback and learning on particular areas of interest (Rozovskaya and Roth 2010; Jarvis and Crossley 2012). The system based on string kernels described in this article was initially designed to participate in the 2013 NLI Shared Task (Tetreault, Blanchard, and Cahill 2013) and obtained third place (Popescu and Ionescu 2013). The approach was further extended by Ionescu, Popescu, and Cahill (2014) to combine several string kernels via multiple kernel learning (MKL) (Gonen and Alpaydin 2011). The system revealed some interesting facts about the kinds of string kernels and kernel learning methods that worked well for NLI. For instance, one of the best performing kernels is"
J16-3005,W06-1657,0,0.890633,"Missing"
J16-3005,E14-4033,0,0.0890383,"Missing"
J16-3005,tenfjord-etal-2006-ask,0,0.207953,"Missing"
J16-3005,W13-1706,1,0.788496,"Missing"
J16-3005,C12-1158,1,0.892005,"Missing"
J16-3005,N01-1031,0,0.17771,"Missing"
J16-3005,W07-0602,0,0.033833,"Missing"
J16-3005,W13-1736,0,0.0509759,"Missing"
J16-3005,W13-1710,0,\N,Missing
N13-1055,C12-1038,1,0.818668,"ion corrections Total number of corrections suggested Number of correct preposition corrections Total number of corrections in test set Note that due to the high volume of unchanged prepositions in the test corpus, we obtain very high accuracies, which are not indicative of true performance, and are not included in our results. The results of our experiments are presented in Table 2.12 The first part of the table shows the fscores of preposition error correction systems that 10 We use liblinear (Fan et al., 2008) with the L1-regularized logistic regression solver and default parameters. 11 As Chodorow et al. (2012) note, it is not clear how to handle cases where the system predicts a preposition that is neither the same as the writer preposition nor the correct preposition. We count these cases as false positives. 12 No thresholds were used in the systems that were trained on well-edited text. Traditionally, thresholds are applied so as to only predict a correction when the system is highly confident. This has the effect of increasing precision at the cost of recall, and sometimes leads to an overall improved f-score. Here we take the prediction of the system, regardless of the confidence, reflecting a"
N13-1055,P11-1092,0,0.466589,"he usage of prepositions and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and"
N13-1055,W11-2838,0,0.0132963,"on either side of the preposition (∼clean). The third contains all corrections regardless of any other changes in the surrounding context (all). Lang-8 The Lang-8 website contains journals written by language learners, where native speakers highlight and correct errors on a sentence-bysentence basis. As a result, it contains typical grammatical mistakes made by language learners, which can be easily downloaded. We automatically extract 75,622 sentences with preposition errors and corrections from the first million journal entries.7 HOO 2011 We take the test set from the HOO 2011 shared task (Dale and Kilgarriff, 2011) and extract all examples of preposition selection errors. The texts are fragments of ACL papers that have been manually annotated for grammatical errors.8 It is important to note that the three test sets we use are from entirely different domains: exam scripts from non-native English speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault"
N13-1055,W12-2006,0,0.0438256,"and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction"
N13-1055,P11-4017,0,0.0264218,"al., 2012). To our knowledge, no one has previously extracted data for training a grammatical error detection system from Wikipedia revisions. 3.2 Extracting Preposition Correction Data from Wikipedia Revisions As the source of our Wikipedia revisions, we used an XML snapshot of Wikipedia generated in July 2011 containing 8,735,890 articles and 288,583,063 revisions.1 We then used the following process to extract preposition errors and their corresponding corrections from this snapshot: Step 1: Extract the plain text versions of all revisions of all articles using the Java Wikipedia Library (Ferschke et al., 2011). Step 2: For each Wikipedia article, compare each revision with the revision immediately preceding it using an efficient diff algorithm.2 Step 3: Compute all 1-word edit chains for the article, i.e., sequences of related edits derived from all revisions of the same article. For example, say revision 10 of an article inserts the preposition of into a sentence and revision 12 changes that preposition to on. Assuming that no other revisions change this sentence, the corresponding edit chain would contain the following 3 elements: →of→on. The extracted chains contain the full context on either s"
N13-1055,W09-2112,0,0.633063,"Missing"
N13-1055,I08-1059,0,0.0273622,"n welledited text? 3. What is the impact of having a mismatch in the error distributions of the training and test sets? 2 Related Work In this section, we only review work in preposition error correction in terms of the three training paradigms and refer the reader to Leacock et al. (2010) for a more comprehensive review of the field. 2.1 Training on Well-Edited Text Early approaches to error detection and correction did not have access to large amounts of errorannotated data to train statistical models and thus, systems were trained on millions of well-edited examples from news text instead (Gamon et al., 2008; Tetreault and Chodorow, 2008; De Felice and Pulman, 2009). Feature sets usually consisted of ngrams around the preposition, POS sequences, syntactic features and semantic information. Since the model only had knowledge of correct usage, an error was flagged if the system’s prediction for a particular preposition context differed from the preposition the writer used. 2.2 Artificial Errors The issue with training solely on correct usage was that the systems had no knowledge of typical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for ma"
N13-1055,N10-1019,0,0.0565523,"dressing domain adaptation for the task of Japanese particle correction. 2.3 Error-Annotated Learner Corpora Recently, error-annotated learner data has become more readily and publicly available allowing models to be trained on both examples of correct usage as well typical learner errors. Han et al. (2010) showed that a preposition error detection and correction system trained on 100,000 annotated preposition errors from the Chungdahm Corpus of Korean Learner English (in addition to 1 million examples of correct usage) outperformed a model trained only on 5 million examples of correct usage. Gamon (2010) and Dahlmeier and Ng (2011) showed that combining models trained separately on examples of correct and incorrect usage could also improve the performance of a preposition error correction system. 3 3.1 Mining Wikipedia Revisions for Grammatical Error Corrections Related Work Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types"
N13-1055,P12-2076,0,0.0554385,"rror distributions derived from the error-annotated learner corpora available at the time. Izumi et al. (2003) was the first to evaluate a model trained on incorrect usage as well as artificial errors for the task of correcting several different 508 error types, including prepositions. However, with limited training data, system performance was quite poor. Rozovskaya and Roth (2010) evaluated different ways of generating artificial errors and found that a system trained on artificial errors could outperform the more traditional training paradigm of using only well-edited texts. Most recently, Imamura et al. (2012) showed that performance could be improved by training a model on artificial errors and addressing domain adaptation for the task of Japanese particle correction. 2.3 Error-Annotated Learner Corpora Recently, error-annotated learner data has become more readily and publicly available allowing models to be trained on both examples of correct usage as well typical learner errors. Han et al. (2010) showed that a preposition error detection and correction system trained on 100,000 annotated preposition errors from the Chungdahm Corpus of Korean Learner English (in addition to 1 million examples of"
N13-1055,P03-2026,0,0.440635,"of correct usage, an error was flagged if the system’s prediction for a particular preposition context differed from the preposition the writer used. 2.2 Artificial Errors The issue with training solely on correct usage was that the systems had no knowledge of typical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for many years, such error-annotated corpora were not available. Instead, several researchers generated artificial errors based on the error distributions derived from the error-annotated learner corpora available at the time. Izumi et al. (2003) was the first to evaluate a model trained on incorrect usage as well as artificial errors for the task of correcting several different 508 error types, including prepositions. However, with limited training data, system performance was quite poor. Rozovskaya and Roth (2010) evaluated different ways of generating artificial errors and found that a system trained on artificial errors could outperform the more traditional training paradigm of using only well-edited texts. Most recently, Imamura et al. (2012) showed that performance could be improved by training a model on artificial errors and a"
N13-1055,max-wisniewski-2010-mining,0,0.390095,"ect usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction (Zesch, 2012), this resource has not been used for the task of grammatical error correction. To evaluate the usefulness of Wikipedia revision history for grammatical error correction, we address the task of correcting errors in preposition selection (i.e., where the context licenses the use of a preposition, but the writer selects the wrong one). We first train a model directly on instances of correct and incorrect preposition usage extracted from the Wikipedia revision data. We also generate artificial errors using the confusion distributio"
N13-1055,D10-1094,0,0.513155,"using rules and regular expressions (Leacock et al., 2010). On the other hand, errors involving the usage of prepositions and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP ta"
N13-1055,P12-2039,0,0.0232089,"ish speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault and Chodorow (2008)9 to evaluate the many ways of using Wikipedia error corrections as described in the Section 4. We use this system since it has been recreated for other work (Dahlmeier and Ng, 2011; Tetreault et al., 2010) and is similar in methodology to Gamon et al. (2008) 7 Tajiri et al. (2012) extract a corpus of English verb phrases corrected for tense/aspect errors from Lang-8. They kindly provided us with their scripts to carry out the scraping of Lang-8. 8 The results of the HOO 2011 shared task were not reported at level of preposition selection error, therefore it is not possible to compare the results presented in this paper with those results. 9 Note that in that work, the model was evaluated in terms of preposition error detection rather than correction, however the model itself does not change. 511 and De Felice and Pulman (2009). In short, the method models the problem o"
N13-1055,C08-1109,1,0.960607,". What is the impact of having a mismatch in the error distributions of the training and test sets? 2 Related Work In this section, we only review work in preposition error correction in terms of the three training paradigms and refer the reader to Leacock et al. (2010) for a more comprehensive review of the field. 2.1 Training on Well-Edited Text Early approaches to error detection and correction did not have access to large amounts of errorannotated data to train statistical models and thus, systems were trained on millions of well-edited examples from news text instead (Gamon et al., 2008; Tetreault and Chodorow, 2008; De Felice and Pulman, 2009). Feature sets usually consisted of ngrams around the preposition, POS sequences, syntactic features and semantic information. Since the model only had knowledge of correct usage, an error was flagged if the system’s prediction for a particular preposition context differed from the preposition the writer used. 2.2 Artificial Errors The issue with training solely on correct usage was that the systems had no knowledge of typical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for many years, such error-annotated"
N13-1055,P10-2065,1,0.572162,"s we use are from entirely different domains: exam scripts from non-native English speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault and Chodorow (2008)9 to evaluate the many ways of using Wikipedia error corrections as described in the Section 4. We use this system since it has been recreated for other work (Dahlmeier and Ng, 2011; Tetreault et al., 2010) and is similar in methodology to Gamon et al. (2008) 7 Tajiri et al. (2012) extract a corpus of English verb phrases corrected for tense/aspect errors from Lang-8. They kindly provided us with their scripts to carry out the scraping of Lang-8. 8 The results of the HOO 2011 shared task were not reported at level of preposition selection error, therefore it is not possible to compare the results presented in this paper with those results. 9 Note that in that work, the model was evaluated in terms of preposition error detection rather than correction, however the model itself does not change. 51"
N13-1055,N03-1033,0,0.00614282,"here a preposition is replaced with another preposition. If the preposition edit is the only edit in the sentence, we convert the chain into a sentence pair and label it clean. If there are other 1-word edits but not within 5 words of the preposition edit on either side, we label the sentence somewhat clean. Otherwise, we label it dirty. The motivation is that the presence of other nearby edits make the preposition correction less reliable when used in isolation, due to the possible dependencies between corrections. All extracted sentences were part-of-speech tagged using the Stanford Tagger (Toutanova et al., 2003). Using the above process, we are able to extract approximately 2 million sentences containing prepositions errors and their corrections. Some examples of the sentences we extracted are given in Figure 1. Example (4) shows an example of a bad correction. 4 Corpora We use several corpora for training and testing our preposition error correction system. The properties of each are outlined in Table 1, organized by paradigm. For each corpus we report the total number of prepositions used for training, as well as the number and percentage of preposition corrections. 4.1 Well-edited Text We train ou"
N13-1055,P08-2035,0,0.019275,"Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplification (Yatskar et al., 2010) and entailment (Zanzotto and Pennacchiotti, 2010; (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at → in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of → on) those dates supports Ranneft’s claims. (3) [Wiki dirty] . . . cirque has a permanent production (to → at) the Mirage, love. (4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his performances for French touris"
N13-1055,P11-1019,0,0.0895178,"hanged. 4.3 Naturally Occurring Errors We have a number of corpora that contain annotated preposition errors. Note that we are only considering incorrectly selected prepositions, we do not consider missing or extraneous. NUCLE The NUS Corpus of Learner English (NUCLE)5 contains one million words of learner essay text, manually annotated with error tags and corrections. We use the same training, dev and test splits as Dahlmeier and Ng (2011). FCE The CLC FCE Dataset6 is a collection of 1,244 exam scripts written by learners of English as part of the Cambridge ESOL First Certificate in English (Yannakoudakis et al., 2011). It includes demographic metadata about the candidate, a grade for each essay and manuallyannotated error corrections. Wikipedia We use three versions of the preposition errors extracted from the Wikipedia revisions as described in Section 3.2. The first includes corrections where the preposition was the only word corrected in the entire sentence 5 6 http://bit.ly/nuclecorpus http://ilexir.co.uk/applications/clc-fce-dataset/ (clean). The second contains all clean corrections, as well as all corrections where there were no other edits within a five-word span on either side of the preposition ("
N13-1055,N10-1056,0,0.0707676,"ine a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplification (Yatskar et al., 2010) and entailment (Zanzotto and Pennacchiotti, 2010; (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at → in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of → on) those dates supports Ranneft’s claims. (3) [Wiki dirty] . . . cirque has a permanent production (to → at) the Mirage, love. (4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his performances for French tourists (in → to) Petersburg. Figure 1: Example sentences with preposition errors extracted from Wikipedia revisions. The second"
N13-1055,E12-1054,0,0.192817,"The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction (Zesch, 2012), this resource has not been used for the task of grammatical error correction. To evaluate the usefulness of Wikipedia revision history for grammatical error correction, we address the task of correcting errors in preposition selection (i.e., where the context licenses the use of a preposition, but the writer selects the wrong one). We first train a model directly on instances of correct and incorrect preposition usage extracted from the Wikipedia revision data. We also generate artificial errors using the confusion distributions derived from this data. We compare both of these approaches to"
N13-1055,han-etal-2010-using,1,\N,Missing
N13-1055,W10-3504,0,\N,Missing
N18-3008,W10-1013,0,0.0349693,"peech processing and NLP techniques have also been used to detect other types of non-scorable responses: language identification technology for non-English detection (Yoon and Higgins, 2011) and speaker recognition technology for automated impostor detection (Qian et al., 2016). “Banging on the keyboard” can be identified by analyzing part-ofspeech sequences and looking for ill-formed sequences (Higgins et al., 2006). have been developed based on question-specific content models, such as a standard vector space model (VSM) built for each question (Bernstein et al., 2000; Higgins et al., 2006; Louis and Higgins, 2010). For speaking tests eliciting highly or moderately restricted speech, filtering models based on features derived from ASR systems such as normalized confidence scores and language model (LM) scores can achieve good performance in identifying topic-related non-scorable responses (van Doremalen et al., 2009; Lo et al., 2010; Cheng and Shen, 2011). However, this approach is not appropriate for a speaking test that elicits unconstrained spontaneous speech. More recently, similar to techniques that have been applied in essay scoring, systems based on document similarity measures and topic detectio"
N18-3008,W11-1420,1,0.808429,"overall architecture of a generic automated scoring pipeline. Above the dotted line are the key stages in automated scoring. Below the dotted line are the possible additions to the pipeline to handle atypical inputs using filtering models (FMs). task on plagiarism detection. Wang et al. (2016) developed a spoken canned response detection system using similar techniques applied in essay plagiarism detection. In addition, various speech processing and NLP techniques have also been used to detect other types of non-scorable responses: language identification technology for non-English detection (Yoon and Higgins, 2011) and speaker recognition technology for automated impostor detection (Qian et al., 2016). “Banging on the keyboard” can be identified by analyzing part-ofspeech sequences and looking for ill-formed sequences (Higgins et al., 2006). have been developed based on question-specific content models, such as a standard vector space model (VSM) built for each question (Bernstein et al., 2000; Higgins et al., 2006; Louis and Higgins, 2010). For speaking tests eliciting highly or moderately restricted speech, filtering models based on features derived from ASR systems such as normalized confidence score"
P04-1041,J97-4005,0,0.150696,"Missing"
P04-1041,P03-1046,0,0.0771796,"the construction of parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, state-of-the-art parsing technology: grammars (or grammatical information) automatically extracted from treebank resources provide the backbone of many state-of-the-art probabilistic parsing approaches (Charniak, 1996; Collins, 1999; Charniak, 1999; Hockenmaier, 2003; Klein and Manning, 2003). Such approaches are attractive as they achieve robustness, coverage and performance while incurring very low grammar development cost. However, with few notable exceptions (e.g. Collins’ Model 3, (Johnson, 2002), (Hockenmaier, 2003) ), treebank-based probabilistic parsers return fairly simple “surfacey” CFG trees, without deep syntactic or semantic information. The grammars used by such systems are sometimes re2 Or dependency banks. ferred to as “half” (or “shallow”) grammars (Johnson, 2002), i.e. they do not resolve LDDs but interpret linguistic material purely loc"
P04-1041,P02-1018,0,0.541974,"lative clauses and interrogative sentences, however, there is an important difference between the location of the (surface) realisation of linguistic material and the location where this material should be interpreted semantically. Resolution of such long-distance dependencies (LDDs) is therefore crucial in the determination of accurate predicate-argument struc1 Manually constructed f-structures for 105 randomly selected trees from Section 23 of the WSJ section of the Penn-II Treebank ture, deep dependency relations and the construction of proper meaning representations such as logical forms (Johnson, 2002). Modern unification/constraint-based grammars such as LFG or HPSG capture deep linguistic information including LDDs, predicate-argument structure, or logical form. Manually scaling rich unification grammars to naturally occurring free text, however, is extremely time-consuming, expensive and requires considerable linguistic and computational expertise. Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-crafted LFG is"
P04-1041,N04-1013,0,0.0502222,"105 (Cahill et al., 2002) 2. The full 2,416 f-structures automatically generated by the f-structure annotation algorithm for the original Penn-II trees, in a CCG-style (Hockenmaier, 2003) evaluation experiment # Parses Lab. F-Score Unlab. F-Score All GFs F-Score (before LDD resolution) All GFs F-Score (after LDD resolution) Preds only F-Score (before LDD resolution) Preds only F-Score (after LDD resolution) All GFs F-Score (before LDD resolution) All GFs F-Score (after LDD resolution) Preds only F-Score (before LDD resolution) Preds only F-Score (after LDD resolution) Subset of GFs following (Kaplan et al., 2004) Pipeline Integrated PCFG P-PCFG A-PCFG PA-PCFG 2416 Section 23 trees 2416 2416 2416 2414 75.83 80.80 79.17 81.32 78.28 82.70 81.49 83.28 DCU 105 F-Strs 79.82 79.24 81.12 81.20 83.79 84.59 86.30 87.04 70.00 71.57 73.45 74.61 73.78 77.43 78.76 80.97 2416 F-Strs 81.98 81.49 83.32 82.78 84.16 84.37 86.45 86.00 72.00 73.23 75.22 75.10 74.07 76.12 78.36 78.40 PARC 700 Dependency Bank 77.86 80.24 77.68 78.60 Table 7: Parser Evaluation 3. A subset of 560 dependency structures of the PARC 700 Dependency Bank following (Kaplan et al., 2004) The results are given in Table 7. The parenttransformed gramma"
P04-1041,P03-1054,0,0.116149,"f parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, state-of-the-art parsing technology: grammars (or grammatical information) automatically extracted from treebank resources provide the backbone of many state-of-the-art probabilistic parsing approaches (Charniak, 1996; Collins, 1999; Charniak, 1999; Hockenmaier, 2003; Klein and Manning, 2003). Such approaches are attractive as they achieve robustness, coverage and performance while incurring very low grammar development cost. However, with few notable exceptions (e.g. Collins’ Model 3, (Johnson, 2002), (Hockenmaier, 2003) ), treebank-based probabilistic parsers return fairly simple “surfacey” CFG trees, without deep syntactic or semantic information. The grammars used by such systems are sometimes re2 Or dependency banks. ferred to as “half” (or “shallow”) grammars (Johnson, 2002), i.e. they do not resolve LDDs but interpret linguistic material purely locally where it occurs in th"
P04-1041,H94-1003,0,0.210445,"Missing"
P04-1041,H94-1020,0,0.0697514,"as LFG or HPSG capture deep linguistic information including LDDs, predicate-argument structure, or logical form. Manually scaling rich unification grammars to naturally occurring free text, however, is extremely time-consuming, expensive and requires considerable linguistic and computational expertise. Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-crafted LFG is successfully scaled to parse the Penn-II treebank (Marcus et al., 1994) with discriminative (loglinear) parameter estimation techniques. The last 20 years have seen continuously increasing efforts in the construction of parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, state-of-the-art parsing technology: grammars (or grammatical information) automatically extracted from treebank resources"
P04-1041,P04-1047,1,0.744247,"Missing"
P04-1041,P02-1035,0,0.0980593,"proper meaning representations such as logical forms (Johnson, 2002). Modern unification/constraint-based grammars such as LFG or HPSG capture deep linguistic information including LDDs, predicate-argument structure, or logical form. Manually scaling rich unification grammars to naturally occurring free text, however, is extremely time-consuming, expensive and requires considerable linguistic and computational expertise. Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-crafted LFG is successfully scaled to parse the Penn-II treebank (Marcus et al., 1994) with discriminative (loglinear) parameter estimation techniques. The last 20 years have seen continuously increasing efforts in the construction of parse-annotated corpora. Substantial treebanks2 are now available for many languages (including English, Japanese, Chinese, German, French, Czech, Turkish), others are currently under construction (Arabic, Bulgarian) or near completion (Spanish, Catalan). Treebanks have been enormously influential in the development of robust, sta"
P04-1041,W98-0141,0,0.0579954,"Missing"
P04-1041,C96-1045,1,0.860707,"Missing"
P04-1041,A00-2018,0,\N,Missing
P04-1041,J98-4004,0,\N,Missing
P04-1041,J03-4003,0,\N,Missing
P04-1047,J93-2002,0,0.193808,"Missing"
P04-1047,P04-1041,1,0.781736,"Missing"
P04-1047,W98-1505,0,0.100785,"Missing"
P04-1047,2000.iwpt-1.9,0,0.252147,"Missing"
P04-1047,P97-1003,0,0.0933422,"Missing"
P04-1047,kinyon-prolo-2002-identifying,0,0.119169,"Missing"
P04-1047,P98-1115,0,0.0908158,"Missing"
P04-1047,H94-1003,0,0.925761,"ticle verbs. Our method does not predefine the frames to be extracted. In contrast to many other approaches, it discriminates between active and passive frames, properly reflects long distance dependencies and assigns conditional probabilities to the semantic forms associated with each predicate. Section 2 reviews related work in the area of automatic subcategorisation frame extraction. Our methodology and its implementation are presented in Section 3. Section 4 presents the results of our lexical extraction. In Section 5 we evaluate the complete extracted lexicon against the COMLEX resource (MacLeod et al., 1994). To our knowledge, this is the largest evaluation of subcategorisation frames for English. In Section 6, we conclude and give suggestions for future work. 2 Related Work Creating a (subcategorisation) lexicon by hand is time-consuming, error-prone, requires considerable linguistic expertise and is rarely, if ever, complete. In addition, a system incorporating a manually constructed lexicon cannot easily be adapted to specific domains. Accordingly, many researchers have attempted to construct lexicons automatically, especially for English. (Brent, 1993) relies on local morphosyntactic cues (su"
P04-1047,P93-1032,0,0.217958,"Missing"
P04-1047,W93-0109,0,0.205245,"Missing"
P04-1047,W00-1307,0,0.0866381,"Missing"
P04-1047,A97-1052,0,\N,Missing
P04-1047,C98-1111,0,\N,Missing
P06-1063,P04-1041,1,0.548322,"Missing"
P06-1063,W04-3215,0,0.0453483,"Missing"
P06-1063,H90-1021,0,0.0594825,"Missing"
P06-1063,P02-1018,0,0.0130775,"the range 82.36 to 82.46, which is comparable to the baseline score. Figure 7 graphs the results for the third ablation experiment. In this case the training set is a fixed amount of the question training set described above (all 3600 questions) and a reducing amount of data from Sections 02-21 of the Penn Treebank. WHNP-1 NP killed Harvey Oswald (a) SQ WP Who AUX did NP VP Harvey Oswald VB (b) kill NP *T*-1 Figure 8: LDD resolved treebank style trees With few exceptions5 the trees produced by current treebank-based probabilistic parsers do not represent long distance dependencies (Figure 9). Johnson (2002) presents a tree-based method for reconstructing LDD dependencies in PennII trained parser output trees. Cahill et al. (2004) present a method for resolving LDDs 5 Collins’ Model 3 computes a limited number of whdependencies in relative clause constructions. 502 SBARQ SBARQ WHNP SQ WP VP Who VBD SBARQ AUX did WHNP =↓ SQ ↑=↓ WP ↑=↓ VP ↑=↓ Who Harvey Oswald (a) VBD ↑=↓ ↑ NP =↓ OBJ killed SQ  WP Who FOCUS NP killed WHNP ↑ NP   VP Harvey Oswald VB (b) FOCUS PRED OBJ SUBJ kill Harvey Oswald (a)  PRED who 1 ’killh  SUBJ OBJi’ PRED ’Harvey Oswald’   ’who’ (b) PRED  1     Figure 10: Annota"
P06-1063,C02-1150,0,0.0122122,"d the question treebank. These 2000 questions consist of the test questions for the first three years of the TREC QA track (1893 questions) and 107 questions from the 2003 TREC test set. 3.2 CCG Group Questions The CCG provide a number of resources for developing QA systems. One of these resources is a set of 5500 questions and their answer types for use in training question classifiers. The 5500 questions were stripped of answer type annotation, duplicated TREC questions were removed and 2000 questions were used for the question treebank. The CCG 5500 questions come from a number of sources (Li and Roth, 2002) and some of these questions contain minor grammatical mistakes so that, in essence, this corpus is more representative of genuine questions that would be put to a working QA system. A number of changes in tokenisation were corrected (eg. separating contractions), but the minor grammatical errors were left unchanged because we believe that it is necessary for a parser for question analysis to be able to cope with this sort of data if it is to be used in a working QA system. 4 Creating the Treebank 4.1 Bootstrapping a Question Treebank The algorithm used to generate the question treebank is an"
P06-1063,J93-2004,0,0.030621,"Missing"
P06-1063,W01-0521,0,\N,Missing
P06-1063,J03-4003,0,\N,Missing
P06-1130,J97-4005,0,0.255096,"parsing architectures: the pipeline and the integrated parsing architecture. In the pipeline architecture, a PCFG (or a history-based lexicalised generative parser) is extracted from the treebank and used to parse unseen text into trees, the resulting trees are annotated with f-structure equations by the f-structure annotation algorithm and a constraint solver produces an f-structure. In the in1 The resources are approximations in that (i) they do not enforce LFG completeness and coherence constraints and (ii) PCFG-based models can only approximate LFG and similar constraint-based formalisms (Abney, 1997). tegrated architecture, first the treebank trees are automatically annotated with f-structure information, f-structure annotated PCFGs with rules of the form NP(↑OBJ=↓)→DT(↑=↓) NN(↑=↓) are extracted, syntactic categories followed by equations are treated as monadic CFG categories during grammar extraction and parsing, unseen text is parsed into trees with f-structure annotations, the annotations are collected and a constraint solver produces an f-structure. The generation architecture presented here builds on the integrated parsing architecture resources of Cahill et al. (2004). The generatio"
P06-1130,C00-1007,0,0.596076,"Missing"
P06-1130,W01-0520,0,0.0293862,"Missing"
P06-1130,W05-1601,0,0.0900512,"Missing"
P06-1130,Y04-1016,1,0.880023,"Missing"
P06-1130,P04-1041,1,0.721752,"Missing"
P06-1130,I05-1015,0,0.346325,"rule (and the associated set of features), divided by the number of occurrences of rules with the same LHS and set of features. Table 1 gives example VP rule expansions with their probabilities when we train a grammar from Sections 02–21 of the Penn Treebank. 3.1 Chart Generation Algorithm The generation algorithm is based on chart generation as first introduced by Kay (1996) with Viterbi-pruning. The generation grammar is first converted into Chomsky Normal Form (CNF). We recursively build a chart-like data structure in a bottom-up fashion. In contrast to packing of locally equivalent edges (Carroll and Oepen, 2005), in our approach if two chart items have equivalent rule left-hand sides and lexical coverage, only the most probable one is kept. Each grammatical function-labelled (sub-)f-structure in the overall fstructure indexes a (sub-)chart. The chart for each f-structure generates the most probable tree for that f-structure, given the internal set of conditioning f-structure features and its grammatical function label. At each level, grammatical function indexed charts are initially unordered. Charts are linearised by generation grammar rules once the charts themselves have produced the most probable"
P06-1130,C00-1062,0,0.121854,"icient implementation that scales to wide-coverage treebankbased resources. An improved model would maximise the probability of a string given an fstructure by summing over trees with the same yield. More research is required to implement such a model efficiently using packed representations (Carroll and Oepen, 2005). Simple PCFGbased models, while effective and computationally efficient, can only provide approximations to LFG and similar constraint-based formalisms (Abney, 1997). Research on discriminative disambiguation methods (Valldal and Oepen, 2005; Nakanishi et al., 2005) is important. Kaplan and Wedekind (2000) show that for certain linguistically interesting classes of LFG (and PATR etc.) grammars, generation from f-structures yields a context free language. Their proof involves the notion of a 1039 “refinement” grammar where f-structure information is compiled into CFG rules. Our probabilistic generation grammars bear a conceptual similarity to Kaplan and Wedekind’s “refinement” grammars. It would be interesting to explore possible connections between the treebank-based empirical work presented here and the theoretical constructs in Kaplan and Wedekind’s proofs. We presented a full set of generati"
P06-1130,P96-1027,0,0.4731,"resented as arrows), we automatically extract the rule S(↑=↓) → NP(↑SUBJ=↓) VP(↑=↓) conditioned on the feature set {PRED,SUBJ,COMP,TENSE}. The probability of the rule is then calculated by counting the number of occurrences of that rule (and the associated set of features), divided by the number of occurrences of rules with the same LHS and set of features. Table 1 gives example VP rule expansions with their probabilities when we train a grammar from Sections 02–21 of the Penn Treebank. 3.1 Chart Generation Algorithm The generation algorithm is based on chart generation as first introduced by Kay (1996) with Viterbi-pruning. The generation grammar is first converted into Chomsky Normal Form (CNF). We recursively build a chart-like data structure in a bottom-up fashion. In contrast to packing of locally equivalent edges (Carroll and Oepen, 2005), in our approach if two chart items have equivalent rule left-hand sides and lexical coverage, only the most probable one is kept. Each grammatical function-labelled (sub-)f-structure in the overall fstructure indexes a (sub-)chart. The chart for each f-structure generates the most probable tree for that f-structure, given the internal set of conditio"
P06-1130,W02-2103,0,0.388037,"Missing"
P06-1130,A00-2023,0,0.473928,"Missing"
P06-1130,H94-1020,0,0.028187,"Missing"
P06-1130,W05-1510,0,0.733302,"aluate the output of our generation system against the raw strings of Section 23 using the Simple String Accuracy and BLEU (Papineni et al., 2002) evaluation metrics. Simple String Accuracy is based on the string edit distance between the output of the generation system and the gold standard sentence. BLEU is the weighted average of n-gram precision against the gold standard sentences. We also measure coverage as the percentage of input f-structures that generate a string. For evaluation, we automatically expand all contracted words. We only evaluate strings produced by the system (similar to Nakanishi et al. (2005)). We conduct a total of four experiments. The parameters we investigate are lexical smoothing (Section 3.3) and partial output. Partial output is a robustness feature for cases where a sub-fstructure component fails to generate a string and the system outputs a concatenation of the strings generated by the remaining components, rather than fail completely. 1037 Sentence length of Training Data ≤ 20 ≤ 25 ≤ 30 ≤ 40 all Evaluation Metric BLEU String Accuracy Coverage BLEU String Accuracy Coverage BLEU String Accuracy Coverage BLEU String Accuracy Coverage BLEU String Accuracy Coverage ≤ 20 0.681"
P06-1130,P02-1040,0,0.100966,"nal grammar used in parsing, our generation system was not able to distinguish nominative from accusative contexts. The solution we implemented was to carry out a grammar transformation in a pre-processing step, to automatically annotate personal pronouns with their case information. This resulted in phrasal and lexical rules such as NP(↑SUBJ) → PRPˆnom(↑=↓) and PRPˆnom(↑=↓) → I and greatly improved the accuracy of the pronouns generated. 4.2 String-Based Evaluation We evaluate the output of our generation system against the raw strings of Section 23 using the Simple String Accuracy and BLEU (Papineni et al., 2002) evaluation metrics. Simple String Accuracy is based on the string edit distance between the output of the generation system and the gold standard sentence. BLEU is the weighted average of n-gram precision against the gold standard sentences. We also measure coverage as the percentage of input f-structures that generate a string. For evaluation, we automatically expand all contracted words. We only evaluate strings produced by the system (similar to Nakanishi et al. (2005)). We conduct a total of four experiments. The parameters we investigate are lexical smoothing (Section 3.3) and partial ou"
P06-1130,A00-2026,0,0.184462,"Missing"
P06-1130,J96-2001,0,\N,Missing
P09-1092,P96-1039,0,0.30596,"eme is language-independent, and so all one needs to be able to apply this to another language is a corpus annotated with IS categories. We extracted our IS asymmetry patterns from a small corpus of spoken news items. This corpus contains text of a similar domain to the TIGER treebank. Further experiments are required to determine how domain specific the asymmetries are. Much related work on incorporating information status (or information structure) into language generation has been on spoken text, since information structure is often encoded by means of prosody. In a limited domain setting, Prevost (1996) describes a two-tiered information structure representation. During the high level planning stage of generation, using a small knowledge base, elements in the discourse are automatically marked as new or given. Contrast and focus are also assigned automatically. These markings influence the final string generated. We are focusing on a broad-coverage system, and do not use any external world-knowledge resources. Van Deemter and Odijk (1997) annotate the syntactic component from which they are generating with information about givenness. This information is determined by detecting contradiction"
P09-1092,W07-2303,1,0.832392,"y annotate text with IS labels and learn from this data. Unfortunately, however, to date, there has been little success in automatically annotating text with IS. 2 Generation Ranking The task we are considering is generation ranking. In generation (or more specifically, surface realisation) ranking, we take an abstract representation of a sentence (for example, as produced by a machine translation or automatic summarisation system), produce a number of alternative string realisations corresponding to that input and use some model to choose the most likely string. We take the model outlined in Cahill et al. (2007), a log-linear model based on the Lexical Functional Grammar (LFG) Framework (Kaplan and Bresnan, 1982). LFG has two main levels of represen1 We take information status to be a subarea of information structure; the one dealing with varieties of givenness but not with contrast and focus in the strictest sense. 817 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 817–825, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP ROOT:1458 CS 1: PERIOD:397 CProot[std]:1451 DP[std]:906 Cbar:1448 .:389 DPx[std]:903 D[std]:593 Cbar-flat:1436 NP:738 V[v,fin]:976"
P09-1092,W00-1421,0,0.0797093,"Missing"
P09-1092,rohrer-forst-2006-improving,0,0.150923,"tation, C(onstituent)-Structure and F(unctional)Structure. C-Structure is a context-free tree representation that captures characteristics of the surface string while F-Structure is an abstract representation of the basic predicate-argument structure of the string. An example C- and F-Structure pair for the sentence in (1) is given in Figure 1. (1) (2) Die Beh¨orden warnten vor m¨oglichen Nachbeben. the authorities warned of possible aftershocks ‘The authorities warned of possible aftershocks.’ The input to the generation system is an FStructure. A hand-crafted, bi-directional LFG of German (Rohrer and Forst, 2006) is used to generate all possible strings (licensed by the grammar) for this input. As the grammar is hand-crafted, it is designed only to parse (and therefore) generate grammatical strings.2 The task of the realisation ranking system is then to choose the most likely string. Cahill et al. (2007) describe a loglinear model that uses linguistically motivated features and improves over a simple tri-gram language model baseline. We take this log-linear model as our starting point.3 Denn ausdr¨ucklich ist darin der rechtliche Maßstab der Vorinstanz, des S¨achsischen Oberverwaltungsgerichtes, best¨"
P09-1092,W04-3250,0,0.0133495,"ERS PRON 39 DA PRON 25 DEMON PRON 19 GENERIC PRON 11 D - GIVEN - REFLEXIVE REFL PRON 54 Label 2 (+ features) INDEF - REL INDEF ATTR 23 SIMPLE INDEF 17 B/A 0 Total 19 D - GIVEN - CATAPHOR SIMPLE DEF 13 DA PRON 10 0.1 11 0.11 31 NEW SIMPLE INDEF INDEF ATTR INDEF NUM INDEF PPADJ INDEF GEN ... 113 53 32 26 25 Table 6: IS asymmetric pairs augmented with syntactic characteristics 822 2002). We achieve an improvement of 0.0168 BLEU points and 1.91 percentage points in exact match. The improvement in BLEU is statistically significant (p &lt; 0.01) using the paired bootstrap resampling significance test (Koehn, 2004). Going back to Example (3), the new model chooses a “better” string than the Cahill et al. (2007) model. The new model chooses the original string. While the string chosen by the Cahill et al. (2007) system is also a perfectly valid sentence, our empirical findings from the news corpus were that the default order of generic pronoun before definite NP were more frequent. The system with the new features helped to choose the original string, as it had learnt this asymmetry. tences. The results are given in Table 9. Finally, we combine the two lists of features and evaluate, these results are al"
P09-1092,P02-1040,0,0.0769565,"Missing"
P09-2025,W07-2303,1,0.847546,"nts on automatically generated German text against automatic evaluation metrics. We look at a number of metrics from the MT and Summarisation communities and find that for a relative ranking task, most automatic metrics perform equally well and have fairly strong correlations to the human judgements. In contrast, on a naturalness judgement task, the General Text Matcher (GTM) tool correlates best overall, although in general, correlation between the human judgements and the automatic metrics was quite weak. 1 2 Human Evaluation Experiments The data used in our experiments is the output of the Cahill et al. (2007) German realisation ranking system. That system is couched within the Lexical Functional Grammar (LFG) grammatical framework. LFG has two levels of representation, C(onstituent)-Structure which is a contextfree tree representation and F(unctional)-Structure which is a recursive attribute-value matrix capturing basic predicate-argument-adjunct relations. Cahill et al. (2007) use a large-scale handcrafted grammar (Rohrer and Forst, 2006) to generate a number of (almost always) grammatical sentences given an input F-Structure. They show that a linguistically-inspired log-linear ranking model outp"
P09-2025,W08-1007,0,0.0398316,"Missing"
P09-2025,W04-1013,0,0.0827597,"Missing"
P09-2025,N03-2021,0,0.149081,"entence, the ROUGE - L metric correlates best with the human judgements. The GTM metric correlates at about the same level, but in general there seems to be little difference between the metrics. For the Experiment B data we use the Pearson correlation coefficient to measure the correlation between the human judgements and the automatic BLEU SED 0.67 0.85 0.55 0.54 48.04 0.16 82.60 0.70 Table 2: Correlation between human judgements for experiment A (rank 1–3) and automatic metrics We examine the correlation between the human judgements and a number of automatic metrics: General Text Matching (Melamed et al., 2003) calculates word overlap between a reference and a solution, without double counting duplicate words. It places less importance on word order than BLEU. 1.0 1.0 1.0 1.0 0.0 0.0 100 1.0 Experiment B LL Table 1: Average scores of each metric for Experiment A data Correlation with Automatic Metrics GTM Experiment A GOLD LM LL 1.4 2.55 2.05 Word Error Rate Translation Error Rate (Snover et al., 2006) computes the number of insertions, deletions, substitutions and shifts needed to match a solution to a reference. Most of these metrics come from the Machine Translation field, where the task is argua"
P09-2025,P09-1022,0,0.0131053,"ey are different tasks. Experiment A ranks 3 strings relative to one another, while Experiment B measures the naturalness of the string. We would expect automatic metrics to be better at the first task than the second, as it is easier to rank systems relative to each other than to give a system an absolute score. Disappointingly, the correlation between the dependency parsing metric and the human judgements was no higher than the simple GTM stringbased metric (although it did outperform all other automatic metrics). This does not correspond to related work on English Summarisation evaluation (Owczarzak, 2009) which shows that a metric based on an automatically induced LFG parser for English achieves comparable or higher correlation with human judgements than ROUGE and Basic Elements (BE).4 Parsers of German typically do not achieve as high performance as their English counterparts, and further experiments including alternative parsers are needed to see if we can improve performance of this metric. The data used in our experiments was almost always grammatically correct. Therefore the task 3 This is a 1-1 mapping, and the indexing ensures that duplicate words in a sentence are not confused. 4 99 Th"
P09-2025,2001.mtsummit-papers.68,0,0.0789877,"Missing"
P09-2025,J09-4008,0,0.0753317,"from 2 of those experiments since the remaining experiments would not provide any Introduction During the development of a surface realisation system, it is important to be able to quickly and automatically evaluate its performance. The evaluation of a string realisation system usually involves string comparisons between the output of the system and some gold standard set of strings. Typically automatic metrics from the fields of Machine Translation (e.g. BLEU) or Summarisation (e.g. ROUGE) are used, but it is not clear how successful or even appropriate these are. Belz and Reiter (2006) and Reiter and Belz (2009) describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts). Their findings show that the NIST metric correlates best with the human judgements, and all automatic metrics favour systems that generate based on frequency. They conclude that automatic evaluations should be accompanied by human evaluations where possible. Stent et al. (2005) investigate a number of automatic evaluation methods for generation in terms of adequacy 1 In all cases, the three strings were different. 97 Proce"
P09-2025,rohrer-forst-2006-improving,0,0.0327812,"ation between the human judgements and the automatic metrics was quite weak. 1 2 Human Evaluation Experiments The data used in our experiments is the output of the Cahill et al. (2007) German realisation ranking system. That system is couched within the Lexical Functional Grammar (LFG) grammatical framework. LFG has two levels of representation, C(onstituent)-Structure which is a contextfree tree representation and F(unctional)-Structure which is a recursive attribute-value matrix capturing basic predicate-argument-adjunct relations. Cahill et al. (2007) use a large-scale handcrafted grammar (Rohrer and Forst, 2006) to generate a number of (almost always) grammatical sentences given an input F-Structure. They show that a linguistically-inspired log-linear ranking model outperforms a simple baseline tri-gram language model trained on the Huge German Corpus (HGC), a corpus of 200 million words of newspaper and other text. Cahill and Forst (2009) describe a number of experiments where they collect judgements from native speakers about the three systems compared in Cahill et al. (2007): (i) the original corpus string, (ii) the string chosen by the language model, and (iii) the string chosen by the linguistic"
P09-2025,2006.amta-papers.25,0,0.112785,"Missing"
P09-2025,E06-1040,0,0.0359412,"del.1 We only take the data from 2 of those experiments since the remaining experiments would not provide any Introduction During the development of a surface realisation system, it is important to be able to quickly and automatically evaluate its performance. The evaluation of a string realisation system usually involves string comparisons between the output of the system and some gold standard set of strings. Typically automatic metrics from the fields of Machine Translation (e.g. BLEU) or Summarisation (e.g. ROUGE) are used, but it is not clear how successful or even appropriate these are. Belz and Reiter (2006) and Reiter and Belz (2009) describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts). Their findings show that the NIST metric correlates best with the human judgements, and all automatic metrics favour systems that generate based on frequency. They conclude that automatic evaluations should be accompanied by human evaluations where possible. Stent et al. (2005) investigate a number of automatic evaluation methods for generation in terms of adequacy 1 In all cases, the three strin"
P09-2025,E09-1014,1,0.649907,"presentation, C(onstituent)-Structure which is a contextfree tree representation and F(unctional)-Structure which is a recursive attribute-value matrix capturing basic predicate-argument-adjunct relations. Cahill et al. (2007) use a large-scale handcrafted grammar (Rohrer and Forst, 2006) to generate a number of (almost always) grammatical sentences given an input F-Structure. They show that a linguistically-inspired log-linear ranking model outperforms a simple baseline tri-gram language model trained on the Huge German Corpus (HGC), a corpus of 200 million words of newspaper and other text. Cahill and Forst (2009) describe a number of experiments where they collect judgements from native speakers about the three systems compared in Cahill et al. (2007): (i) the original corpus string, (ii) the string chosen by the language model, and (iii) the string chosen by the linguistically-inspired log-linear model.1 We only take the data from 2 of those experiments since the remaining experiments would not provide any Introduction During the development of a surface realisation system, it is important to be able to quickly and automatically evaluate its performance. The evaluation of a string realisation system"
P09-2025,P02-1040,0,\N,Missing
P11-1101,W10-4201,0,0.0187591,"no positive effect on the voice and precedence accuracy. The n-best evaluations even suggest that the LM scores negatively impact the ranker: the accuracy for the top 3 sentences increases much less as compared to the model that does not integrate LM scores.6 The n-best performance of a realisation ranker is practically relevant for re-ranking applications such as Velldal (2008). We think that it is also conceptually interesting. Previous evaluation studies suggest that the original corpus sentence is not always the only optimal realisation of a given linguistic input (Cahill and Forst, 2010; Belz and Kow, 2010). Humans seem to have varying preferences for word order contrasts in certain contexts. The n-best evaluation could reflect the behaviour of a ranking model with respect to the range of variations encountered in real discourse. The pilot human evaluation in the next Section deals with this question. 6 Human Evaluation Our experiment in Section 5.3 has shown that the accuracy of our linguistically informed ranking model dramatically increases when we consider the three 6 (Nakanishi et al., 2005) also note a negative effect of including LM scores in their model, pointing out that the LM was not"
P11-1101,W10-4237,0,0.0289516,"Missing"
P11-1101,W05-1601,0,0.202581,"ecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 1008 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007)."
P11-1101,C10-1012,0,0.0336633,"in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However, they note that these annotations are not suitable for full generation since they are often incomplete. Thus, it is not clear to which degree these annotations are actually underspecified for certain paraphrases. 2.2 Linguistic Background In competition-based linguistic theories (Optimality Theory and related frameworks), the use of argument alternations is construed as an effect of markedness hierarchies (Aissen, 1999; Aissen, 2003). Argument functions (subject, object, . . . ) on the one hand and th"
P11-1101,P09-1092,1,0.897952,"grammatically possible. Bresnan et al. (2007) correlate the use of the English dative alternation to a number of features such as givenness, pronominalisation, definiteness, constituent length, animacy of the involved verb arguments. These features are assumed to reflect the discourse acessibility of the arguments. Interestingly, the properties that have been used to model argument alternations in strict word order languages like English have been identified as factors that influence word order in free word order languages like German, see Filippova and Strube (2007) for a number of pointers. Cahill and Riester (2009) implement a model for German word order variation that approximates the information status of constituents through morphological features like definiteness, pronominalisation etc. We are not aware of any corpus-based generation studies investigating how these properties relate to argument alternations in free word order languages. 3 Generation Architecture Our data-driven methodology for investigating factors relevant to surface realisation uses a regeneration set-up2 with two main components: a) a grammar-based component used to parse a corpus sentence and map it to all its meaning-equivalen"
P11-1101,W07-2303,1,0.906034,"rube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However, they note that these"
P11-1101,D09-1046,0,0.0256955,"Missing"
P11-1101,P07-1041,0,0.351701,"on the person scale than the agent, but an active is grammatically possible. Bresnan et al. (2007) correlate the use of the English dative alternation to a number of features such as givenness, pronominalisation, definiteness, constituent length, animacy of the involved verb arguments. These features are assumed to reflect the discourse acessibility of the arguments. Interestingly, the properties that have been used to model argument alternations in strict word order languages like English have been identified as factors that influence word order in free word order languages like German, see Filippova and Strube (2007) for a number of pointers. Cahill and Riester (2009) implement a model for German word order variation that approximates the information status of constituents through morphological features like definiteness, pronominalisation etc. We are not aware of any corpus-based generation studies investigating how these properties relate to argument alternations in free word order languages. 3 Generation Architecture Our data-driven methodology for investigating factors relevant to surface realisation uses a regeneration set-up2 with two main components: a) a grammar-based component used to parse a cor"
P11-1101,N09-2057,0,0.0127313,"valuation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related"
P11-1101,W07-1203,0,0.135806,"g-equivalent surface realisations, b) a statistical ranking component used to select the correct, i.e. contextually most appropriate surface realisation. Two variants of this set-up that we use are sketched in Figure 1. We generally use a hand-crafted, broad-coverage LFG for German (Rohrer and Forst, 2006) to parse a corpus sentence into a f(unctional) structure3 and generate all surface realisations from a given 2 Compare the bidirectional competition set-up in some Optimality-Theoretic work, e.g., (Kuhn, 2003). 3 The choice among alternative f-structures is done with a discriminative model (Forst, 2007). 1009 Sntx Snty SVM Ranker SVM Ranker Snta1 Snta2 ... Sntam LFG grammar Sntb1 Snta1 Snta2 ... Sntbn LFG Grammar FSa FSb Reverse Sem. Rules SEM FSa LFG grammar Sem. Rules FS1 LFG Grammar Snti Snti Figure 1: Generation pipelines f-structure, following the generation approach of Cahill et al. (2007). F-structures are attributevalue matrices representing grammatical functions and morphosyntactic features; their theoretical motivation lies in the abstraction over details of surface realisation. The grammar is implemented in the XLE framework (Crouch et al., 2006), which allows for reversible use o"
P11-1101,P10-1160,0,0.0208723,"ntation for Sentence (4) in Figure 2, the realiser will not generate an active realisation since the agent role cannot be instantiated by any phrase in the grammar. However, depending on the exact context there are typically options for realising the subject phrase in an active with very little descriptive content. Ideally, one would like to account for these phenomena in a meaning representation that underspecifies the lexicalisation of discourse referents, and also captures the reference of implicit arguments. Especially the latter task has hardly been addressed in NLP applications (but see Gerber and Chai (2010)). In order to work around that problem, we implemented some simple heuristics which underspecify the realisation of certain verb arguments. These rules define: 1. a set of pronouns (generic and neutral pronouns, universal quantifiers) that correspond to “trivial” agents in active and implicit agents 1010 in passive sentences; 2. a set of prepositional adjuncts in passive sentences that correspond to subjects in active sentence (e.g. causative and instrumental prepositions like durch “by means of”); 3. certain syntactic contexts where special underspecification devices are needed, e.g. coordin"
P11-1101,J95-2003,0,0.192951,"nt” features (ScalAl.): combinations of voice and role properties with morphological properties, e.g. “subject is singular”, “agent is 3rd person in active voice” (these are surface-independent, identical for each alternation candidate). The model for which we present our results is based on sentence-internal features only; as Cahill and Riester (2009) showed, these feature carry a considerable amount of implicit information about the discourse context (e.g. in the shape of referring expressions). We also implemented a set of explicitly inter-sentential features, inspired by Centering Theory (Grosz et al., 1995). This model did not improve over the intra-sentential model. Evaluation Measures In order to assess the general quality of our generation ranking models, we 4 The language model is trained on the German data release for the 2009 ACL Workshop on Machine Translation shared task, 11,991,277 total sentences. LM Ling. Model Match BLEU NIST Match BLEU NIST SEMn 68.2 10.72 15.04 0.68 12.95 27.66 0.759 13.14 SEMh 75.8 7.28 11.89 0.65 12.69 26.38 0.747 13.01 Table 2: Evaluation of Experiment 1 use several standard measures: a) exact match: how often does the model select the original corpus sentence,"
P11-1101,P98-1116,0,0.0627281,"king the risk of occasional overgeneration. The paper is structured as follows: Section 2 situates our methodology with respect to other work on surface realisation and briefly summarises the relevant theoretical linguistic background. In Section 3, we present our generation architecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 1008 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich lingui"
P11-1101,W05-1510,0,0.0227506,"sentence is not always the only optimal realisation of a given linguistic input (Cahill and Forst, 2010; Belz and Kow, 2010). Humans seem to have varying preferences for word order contrasts in certain contexts. The n-best evaluation could reflect the behaviour of a ranking model with respect to the range of variations encountered in real discourse. The pilot human evaluation in the next Section deals with this question. 6 Human Evaluation Our experiment in Section 5.3 has shown that the accuracy of our linguistically informed ranking model dramatically increases when we consider the three 6 (Nakanishi et al., 2005) also note a negative effect of including LM scores in their model, pointing out that the LM was not trained on enough data. The corpus used for training our LM might also have been too small or distinct in genre. 1014 best sentences rather than only the top-ranked sentence. This means that the model sometimes predicts almost equal naturalness for different voice realisations. Moreover, in the case of word order, we know from previous evaluation studies, that humans sometimes prefer different realisations than the original corpus sentences. This Section investigates agreement in human judgemen"
P11-1101,A00-2026,0,0.142822,"In Section 3, we present our generation architecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 1008 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal"
P11-1101,P00-1061,1,0.519387,"rd lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not direc"
P11-1101,P02-1035,0,0.0210818,"e realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In mult"
P11-1101,C04-1097,0,0.120095,"sults from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work"
P11-1101,rohrer-forst-2006-improving,0,0.198238,"elate to argument alternations in free word order languages. 3 Generation Architecture Our data-driven methodology for investigating factors relevant to surface realisation uses a regeneration set-up2 with two main components: a) a grammar-based component used to parse a corpus sentence and map it to all its meaning-equivalent surface realisations, b) a statistical ranking component used to select the correct, i.e. contextually most appropriate surface realisation. Two variants of this set-up that we use are sketched in Figure 1. We generally use a hand-crafted, broad-coverage LFG for German (Rohrer and Forst, 2006) to parse a corpus sentence into a f(unctional) structure3 and generate all surface realisations from a given 2 Compare the bidirectional competition set-up in some Optimality-Theoretic work, e.g., (Kuhn, 2003). 3 The choice among alternative f-structures is done with a discriminative model (Forst, 2007). 1009 Sntx Snty SVM Ranker SVM Ranker Snta1 Snta2 ... Sntam LFG grammar Sntb1 Snta1 Snta2 ... Sntbn LFG Grammar FSa FSb Reverse Sem. Rules SEM FSa LFG grammar Sem. Rules FS1 LFG Grammar Snti Snti Figure 1: Generation pipelines f-structure, following the generation approach of Cahill et al. (20"
P11-1101,W06-1661,0,0.236129,"i, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However"
P11-1101,W09-2602,1,0.842156,"oach of Cahill et al. (2007). F-structures are attributevalue matrices representing grammatical functions and morphosyntactic features; their theoretical motivation lies in the abstraction over details of surface realisation. The grammar is implemented in the XLE framework (Crouch et al., 2006), which allows for reversible use of the same declarative grammar in the parsing and generation direction. To obtain a more abstract underlying representation (in the pipeline on the right-hand side of Figure 1), the present work uses an additional semantic construction component (Crouch and King, 2006; Zarrieß, 2009) to map LFG f-structures to meaning representations. For the reverse direction, the meaning representations are mapped to f-structures which can then be mapped to surface strings by the XLE generator (Zarrieß and Kuhn, 2010). For the final realisation ranking step in both pipelines, we used SVMrank, a Support Vector Machine-based learning tool (Joachims, 1996). The ranking step is thus technically independent from the LFG-based component. However, the grammar is used to produce the training data, pairs of corpus sentences and the possible alternations. The two pipelines allow us to vary the de"
P11-1101,C98-1112,0,\N,Missing
P13-4025,W12-3134,0,0.0241482,"Missing"
P13-4025,J10-3003,1,0.84075,"ocessing (NLP) tasks, such as information retrieval, question answering, recognizing textual entailment, text simplification etc. For example, a question answering system facing a question “Who invented bifocals and lightning rods?” could retrieve the correct answer from the text “Benjamin Franklin invented strike termination devices and bifocal reading glasses” given the information that “bifocal reading glasses” is a paraphrase of “bifocals” and “strike termination devices” is a paraphrase of “lightning rods”. There are numerous approaches for automatically extracting paraphrases from text (Madnani and Dorr, 2010). We focus on generating paraphrases by pivoting on bilingual parallel corpora as originally suggested by Bannard and CallisonBurch (2005). This technique operates by attempting to infer semantic equivalence between phrases in the same language by using a second language as a bridge. It builds on one of the initial steps used to train a phrase-based statistical machine translation system. Such systems rely on phrase tables – 2 ParaQuery In this section we first briefly describe how to set up ParaQuery (§2.1) and then demonstrate its use in detail for interactively exploring and characterizing"
P13-4025,P07-1059,0,0.0293967,"mply by using the phrases in the other language as pivots, e.g., if both “man” and “person” correspond to “personne” in French, then they can be considered paraphrases. Each paraphrase pair (rule) in a pivoted paraphrase collection is defined by a source phrase e1 , the target phrase e2 that has been inferred as its paraphrase, and a probability score p(e2 |e1 ) obtained from the probability values in the bilingual phrase table.1 Pivoted paraphrase collections have been successfully used in different NLP tasks including automated document summarization (Zhou et al., 2006), question answering (Riezler et al., 2007), and machine translation (Madnani, 2010). Yet, it is still difficult to get an estimate of the intrinsic quality and coverage of the paraphrases contained in these collections. To remedy this, we propose ParaQuery – a tool that can help explore and analyze pivoted paraphrase collections. Pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition. Although such pivoted paraphrase collections have been successfully used to improve the performance of several different NLP applications, it is still difficult to get an intrinsic estimate of the quality and coverage of"
P13-4025,N06-1057,0,0.0264827,"rases in one language may be inferred simply by using the phrases in the other language as pivots, e.g., if both “man” and “person” correspond to “personne” in French, then they can be considered paraphrases. Each paraphrase pair (rule) in a pivoted paraphrase collection is defined by a source phrase e1 , the target phrase e2 that has been inferred as its paraphrase, and a probability score p(e2 |e1 ) obtained from the probability values in the bilingual phrase table.1 Pivoted paraphrase collections have been successfully used in different NLP tasks including automated document summarization (Zhou et al., 2006), question answering (Riezler et al., 2007), and machine translation (Madnani, 2010). Yet, it is still difficult to get an estimate of the intrinsic quality and coverage of the paraphrases contained in these collections. To remedy this, we propose ParaQuery – a tool that can help explore and analyze pivoted paraphrase collections. Pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition. Although such pivoted paraphrase collections have been successfully used to improve the performance of several different NLP applications, it is still difficult to get an intrins"
P13-4025,P05-1074,0,0.0693368,"Missing"
P13-4025,P09-1051,0,\N,Missing
P14-2029,P03-1054,0,0.0255583,"tem for the binary task by binarizing its predictions.12 We compare our work to a modified version of the publicly available13 system from Post (2011), which performed very well on an artificial dataset. To our knowledge, it is the only publicly available system for grammaticality prediction. It is very Table 1: Pearson’s r on the development set, for our full system and variations excluding each feature type. “− X” indicates the full model without the “X” features. 3.2.4 PCFG Parsing Features We find phrase structure trees and basic dependencies with the Stanford Parser’s English PCFG model (Klein and Manning, 2003; de Marneffe et al., 2006).11 We then compute the following: • the parse score as provided by the Stanford PCFG Parser, normalized for sentence length, later referred to as parse prob • a binary feature that captures whether the top node of the tree is sentential or not (i.e. the assumption is that if the top node is nonsentential, then the sentence is a fragment) • features binning the number of dep relations returned by the dependency conversion. These dep relations are underspecified for function and indicate that the parser was unable to find a standard relation such as subj, possibly ind"
P14-2029,N07-2024,0,0.0193024,"eilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew Mulholland Educational Testing Service Princeton, NJ, USA {mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org Joel Tetreault Yahoo! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract ity of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our syste"
P14-2029,W11-2111,1,0.923297,"Missing"
P14-2029,copestake-flickinger-2000-open,0,0.0167849,"iss + 1) as features. To identify misspellings, we use a freely available spelling dictionary for U.S. English.5 3.2.3 Precision Grammar Features Following Wagner et al. (2007) and Wagner et al. (2009), we use features extracted from precision grammar parsers. These grammars have been hand-crafted and designed to only provide complete syntactic analyses for grammatically correct sentences. This is in contrast to treebanktrained grammars, which will generally provide some analysis regardless of grammaticality. Here, we use (1) the Link Grammar Parser8 and (2) the HPSG English Resource Grammar (Copestake and Flickinger, 2000) and PET parser.9 We use a binary feature, complete link, from the Link grammar that indicates whether at least one complete linkage can be found for a sentence. We also extract several features from the HPSG analyses.10 They mostly reflect information about unification success or failure and the associated costs. In each instance, we use the logarithm of one plus the frequency. 3.2.2 n-gram Count and Language Model Features Given each sentence, the model obtains the counts of n-grams (n = 1 . . . 3) from English Gigaword and computes the following features:6 • of to X log(count(s) + 1) kSn k"
P14-2029,2003.mtsummit-papers.9,0,0.0574979,"Missing"
P14-2029,P11-2038,0,0.180454,"sa Lopez Matthew Mulholland Educational Testing Service Princeton, NJ, USA {mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org Joel Tetreault Yahoo! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract ity of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our system to the one from Post (2011) and"
P14-2029,D11-1010,0,0.0137361,"resent a dataset of learner sentences rated for grammaticality. Such a system could be used, for example, to check or to rank outputs from systems for text summarization, natural language generation, or machine translation. It could also be used in educational applications such as essay scoring. Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions (Tetreault and Chodorow, 2008), articles (Han et al., 2006), and collocations (Dahlmeier and Ng, 2011). While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammatical2 Dataset Description We created a dataset consisting of 3,129 sentences randomly selected from essays written by nonnative speakers of English as part of a test of English language proficiency. We oversampled lower-scoring essays to increase the chances of finding ungrammatical sentences. Two of the a"
P14-2029,P07-1011,0,0.0284516,"al Scale Michael Heilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew Mulholland Educational Testing Service Princeton, NJ, USA {mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org Joel Tetreault Yahoo! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract ity of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we"
P14-2029,de-marneffe-etal-2006-generating,0,0.0433313,"Missing"
P14-2029,1993.eamt-1.1,0,0.520675,"Missing"
P14-2029,C08-1109,1,0.271581,"lop a system for the task of predicting the grammaticality of sentences, and present a dataset of learner sentences rated for grammaticality. Such a system could be used, for example, to check or to rank outputs from systems for text summarization, natural language generation, or machine translation. It could also be used in educational applications such as essay scoring. Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions (Tetreault and Chodorow, 2008), articles (Han et al., 2006), and collocations (Dahlmeier and Ng, 2011). While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammatical2 Dataset Description We created a dataset consisting of 3,129 sentences randomly selected from essays written by nonnative speakers of English as part of a test of English language proficiency. We oversampled lower-scoring essays"
P14-2029,2005.eamt-1.15,0,0.0187352,"Missing"
P14-2029,D07-1012,0,0.257114,"Missing"
P14-2029,U10-1011,0,0.017105,"l Nitin Madnani Melissa Lopez Matthew Mulholland Educational Testing Service Princeton, NJ, USA {mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org Joel Tetreault Yahoo! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract ity of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our system to the one from Pos"
P14-2029,W13-2201,0,\N,Missing
W07-1209,W02-1503,1,0.870416,"or dependency relations. Figure 1 shows the c- and f-structure for the sentence “Growth is slower.”. Parser Output: (S1 (S (NP (NN Growth)) (VP (AUX is) (ADJP (JJR slower))) (. Labeled: [S1 [S Growth [VP is [ADJP slower] ].] ] Unlabeled:[ [ Growth [ is [ slower] ].] ] .))) Figure 2: Example of retained brackets from parser output to constrain the XLE parser 2.2 3 Baseline experiments The XLE Parsing System The XLE parsing system is a deep-grammar-based parsing system. The experiments reported in this paper use the English LFG grammar constructed as part of the ParGram project (Butt et al., 2002). This system incorporates sophisticated ambiguitymanagement technology so that all possible syntactic analyses of a sentence are computed in an efficient, packed representation (Maxwell and Kaplan, 1993). In accordance with LFG theory, the output includes not only standard contextfree phrase-structure trees (c-structures) but also attribute-value matrices (f-structures) that explicitly encode predicate-argument relations and other meaningful properties. The f-structures can be deterministically mapped to dependency triples without any loss of information, using the built-in ordered rewrite sy"
W07-1209,P05-1022,0,0.396589,"remaining constituent after a time threshold has passed). For the experiments reported here, we did not fine-tune these parameters due to time constraints; so default values were arbitrarily set and the same values used for all parsing experiments. 67 We carried out a baseline experiment with two state-of-the-art parsers to establish what effect prebracketing the input to the XLE system has on the quality and number of the solutions produced. We used the Bikel () multi-threaded, head-driven chartparsing engine developed at the University of Pennsylvania. The second parser is that described in Charniak and Johnson (2005). This parser uses a discriminative reranker that selects the most probable parse from the 50-best parses returned by a generative parser based on Charniak (2000). We evaluated against the PARC 700 Dependency Bank (King et al., 2003) which provides goldstandard analyses for 700 sentences chosen at random from Section 23 of the Penn-II Treebank. The Dependency Bank was bootstrapped by parsing the 700 sentences with the XLE English grammar, and then manually correcting the output. The data is divided into two sets, a 140-sentence development set and a test set of 560 sentences (Kaplan et al., 20"
W07-1209,A00-2018,0,0.0806734,"were arbitrarily set and the same values used for all parsing experiments. 67 We carried out a baseline experiment with two state-of-the-art parsers to establish what effect prebracketing the input to the XLE system has on the quality and number of the solutions produced. We used the Bikel () multi-threaded, head-driven chartparsing engine developed at the University of Pennsylvania. The second parser is that described in Charniak and Johnson (2005). This parser uses a discriminative reranker that selects the most probable parse from the 50-best parses returned by a generative parser based on Charniak (2000). We evaluated against the PARC 700 Dependency Bank (King et al., 2003) which provides goldstandard analyses for 700 sentences chosen at random from Section 23 of the Penn-II Treebank. The Dependency Bank was bootstrapped by parsing the 700 sentences with the XLE English grammar, and then manually correcting the output. The data is divided into two sets, a 140-sentence development set and a test set of 560 sentences (Kaplan et al., 2004). We took the raw strings from the 140-sentence development set and parsed them with each of the state-of-the-art probabilistic parsers. As an upper bound for"
W07-1209,C04-1041,0,0.0521989,"Missing"
W07-1209,N04-1013,1,0.845971,"nd Johnson (2005). This parser uses a discriminative reranker that selects the most probable parse from the 50-best parses returned by a generative parser based on Charniak (2000). We evaluated against the PARC 700 Dependency Bank (King et al., 2003) which provides goldstandard analyses for 700 sentences chosen at random from Section 23 of the Penn-II Treebank. The Dependency Bank was bootstrapped by parsing the 700 sentences with the XLE English grammar, and then manually correcting the output. The data is divided into two sets, a 140-sentence development set and a test set of 560 sentences (Kaplan et al., 2004). We took the raw strings from the 140-sentence development set and parsed them with each of the state-of-the-art probabilistic parsers. As an upper bound for the baseline experiment, we use the brackets in the original Penn-II treebank trees for the 140 development set. We then used the brackets from each parser output (or original treebank trees) to constrain the XLE parser. If the input to the XLE parser is bracketed, the parser will only generate c-structures that respect these brackets (i.e., only c-structures with brackets that are compatible with the input brackets are considered during"
W07-1209,W03-2401,1,0.890697,"ments. 67 We carried out a baseline experiment with two state-of-the-art parsers to establish what effect prebracketing the input to the XLE system has on the quality and number of the solutions produced. We used the Bikel () multi-threaded, head-driven chartparsing engine developed at the University of Pennsylvania. The second parser is that described in Charniak and Johnson (2005). This parser uses a discriminative reranker that selects the most probable parse from the 50-best parses returned by a generative parser based on Charniak (2000). We evaluated against the PARC 700 Dependency Bank (King et al., 2003) which provides goldstandard analyses for 700 sentences chosen at random from Section 23 of the Penn-II Treebank. The Dependency Bank was bootstrapped by parsing the 700 sentences with the XLE English grammar, and then manually correcting the output. The data is divided into two sets, a 140-sentence development set and a test set of 560 sentences (Kaplan et al., 2004). We took the raw strings from the 140-sentence development set and parsed them with each of the state-of-the-art probabilistic parsers. As an upper bound for the baseline experiment, we use the brackets in the original Penn-II tr"
W07-1209,C00-1065,0,0.0511611,"Missing"
W07-1209,J93-4001,0,0.0375553,"owth [VP is [ADJP slower] ].] ] Unlabeled:[ [ Growth [ is [ slower] ].] ] .))) Figure 2: Example of retained brackets from parser output to constrain the XLE parser 2.2 3 Baseline experiments The XLE Parsing System The XLE parsing system is a deep-grammar-based parsing system. The experiments reported in this paper use the English LFG grammar constructed as part of the ParGram project (Butt et al., 2002). This system incorporates sophisticated ambiguitymanagement technology so that all possible syntactic analyses of a sentence are computed in an efficient, packed representation (Maxwell and Kaplan, 1993). In accordance with LFG theory, the output includes not only standard contextfree phrase-structure trees (c-structures) but also attribute-value matrices (f-structures) that explicitly encode predicate-argument relations and other meaningful properties. The f-structures can be deterministically mapped to dependency triples without any loss of information, using the built-in ordered rewrite system (Crouch et al., 2002). XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear probability model (Riezl"
W07-1209,W06-1619,0,0.025083,"Missing"
W07-1209,P02-1035,1,0.830097,"1993). In accordance with LFG theory, the output includes not only standard contextfree phrase-structure trees (c-structures) but also attribute-value matrices (f-structures) that explicitly encode predicate-argument relations and other meaningful properties. The f-structures can be deterministically mapped to dependency triples without any loss of information, using the built-in ordered rewrite system (Crouch et al., 2002). XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear probability model (Riezler et al., 2002) that works on the packed representations. The underlying parsing system also has built-in robustness mechanisms that allow it to parse strings that are outside the scope of the grammar as a list of fewest well-formed “fragments”. Furthermore, performance parameters that bound parsing and disambiguation can be tuned for efficient but accurate operation. These parameters include at which point to timeout and return an error, the amount of stack memory to allocate, the number of new edges to add to the chart and at which point to start skimming (a process that guarantees XLE will finish processi"
W07-1209,J99-2004,0,\N,Missing
W07-2303,C00-1007,0,0.324533,"Missing"
W07-2303,E06-1040,0,0.0496239,"hope that results will improve. In the work of Callaway (2003; 2004), a purely symbolic system is presented. Our system makes use of a symbolic grammar, but we apply a statistical ranking model to the output. We believe that this gives us the power to restrict the output of the generator without under-generating. The symbolic grammar enforces hard constraints and the statistical ranking models (possibly conflicting) soft constraints to weight the alternatives. Satisfactory methods for the automatic evaluation of surface realisation (as well as machine translation) have not yet been developed (Belz and Reiter, 2006). The shortcomings of current methods, particularly BLEU score, seem to be even more pronounced for German and other relatively free word-order languages. Given that all of the sentences generated by our system are syntactically valid, one would expect much higher results. A human inspection of the results and investigation of other evaluation metrics such as NIST will be carried out to get a clearer idea of the quality of the ranking. There is a potential bias in the learning algorithm, in that the average length of the training data sentences is lower than the average length of the test data"
W07-2303,W05-1601,0,0.250698,"Missing"
W07-2303,P06-1130,1,0.907656,"Missing"
W07-2303,P04-3009,0,0.0341804,"Missing"
W07-2303,P02-1004,0,0.0285839,"Missing"
W07-2303,W01-0810,0,0.0228062,"in which this sentence is uttered, we have no way of telling where the emphasis of the sentence is. The work described in this paper is part of a much larger project, and future research is already planned to integrate information structure into the surface realisation process. Placement of adjuncts Currently, there is no feature that captures the (relative) location of particular types of adjuncts. In German, there is a strong tendency for temporal adjuncts 22 6 Discussion and future work stituents. Information structure is an important factor when generating the correct surface realisation (Kruijff et al., 2001). German is a language in which word order is largely driven by information structure rather than grammatical function, which is often marked morphologically. In future work, we plan to integrate information structure features into the log-linear model and hope that results will improve. In the work of Callaway (2003; 2004), a purely symbolic system is presented. Our system makes use of a symbolic grammar, but we apply a statistical ranking model to the output. We believe that this gives us the power to restrict the output of the generator without under-generating. The symbolic grammar enforce"
W07-2303,W02-2103,0,0.34425,"Missing"
W07-2303,A00-2023,0,0.536614,"Missing"
W07-2303,W05-1510,0,0.65572,"res. Recently, the grammar has been complemented with a stochastic disambiguation module along the lines of Riezler et al. (2002), consisting of a log-linear model based on structural features (Forst, 2007). This module makes it possible to determine one c-/f-structure pair as the most probable analysis of any given sentence. XLE both for the parsing and the generation direction. For ranking string realisations on the basis of ‘soft’ and potentially contradictory constraints, however, the stochastic approach based on a log-linear model, as it has previously been implemented for English HPSGs (Nakanishi et al., 2005; Velldal and Oepen, 2005), seems more adequate. 2.3 Surface realisation As XLE comes with a fully-fledged generator, the grammar can be used both for parsing and for surface realisation. Figure 3 shows an excerpt of the set of strings (and their LM ranking) that are generated from the f-structure in Figure 2. Note that all of these (as well as the the remaining 139 strings in the set) are grammatical; however, some of them are clearly more likely or unmarked than others. 3.1 Data We use the TIGER Treebank (Brants et al., 2002) to train and test our model. It consists of just over 50,000 annot"
W07-2303,P02-1040,0,0.0743752,"Missing"
W07-2303,P02-1035,0,0.225962,"Missing"
W07-2303,rohrer-forst-2006-improving,1,0.602575,"Missing"
W07-2303,2005.mtsummit-papers.15,0,0.292674,"anese LFG parsing grammar in order to enforce canonical word order by symbolic means. In another purely symbolic approach, Callaway (2003; 2004) describes a wide-coverage system and the author argues that there are several advantages to a symbolic system over a statistical one. We argue that a reversible symbolic system, which is desirable for maintainability and modularity reasons, augmented with a statistical ranking component can produce systematically ranked, high quality surface realisations while maintaining the flexibility associated with hand-crafted systems. Velldal et al. (2004) and Velldal and Oepen (2005) present discriminative disambiguation models using a hand-crafted HPSG grammar for generation from MRS (Minimal Recursion Semantics) structures. They describe three statistical models for realization ranking: The first is a simple n-gram language model, the second uses structural features in a maximum entropy model for disambiguation and a third uses a combination of the two models. Their results show that the third model where the n-gram language model is combined with the structural features in the maximum entropy disambiguation model performs best. Nakanishi et al. (2005) present similar p"
W08-1705,W04-1905,0,0.0513135,"Missing"
W08-1705,W03-2401,0,0.0330184,"he standard sections of Penn Treebank Wall Street Journal Text (Marcus et al., 1994). The training data consists of the original WSJ strings, marked up with some of the Penn 34 Pruning Level Total Time Most Probable F-Score Oracle F-Score Random F-Score # Fragment Parses # Time Outs # Skimmed Sents Treebank constituent information. We marked up NPs and SBARs as well as adjective and verbal POS categories. This is meant to guide the training process, so that it does learn from parses that are not compatible with the original treebank analysis. We evaluated against the PARC 700 Dependency Bank (King et al., 2003), splitting it into 140 sentences as development data and the remaining unseen 560 for final testing (as in Kaplan et al. (2004)). We experimented with different values of the pruning cutoff on the development set; the results are given in Table 1. None 1204 79.93 84.75 75.47 96 1 33 5 392 82.83 87.79 74.31 91 0 0 Table 2: Results of c-structure pruning experiments on English test data 4 Experiments on German We carried out a similar set of experiments on German data to test whether the methodology described above ported to a language other than English. In the case of German, the typical time"
W08-1705,W05-1511,0,0.0628636,"o her.’ (9) Ihr Auto f¨ahrt. her/their car drives ‘Her/Their car drives. (10) Ihr kommt. you(pl) come ‘You come.’ (11) 7.4 pruning. We automatically extracted a specialized corpus of 31,845 sentences from the Huge German Corpus. This corpus is a collection of 200 million words of newspaper and other text. The sentences we extracted all contained examples of proper noun coordination and had been automatically chunked. Training on this sub-corpus as well as the original TIGER training data did have the desired effect of now parsing example (13) with c-structure pruning activated. 8 Related Work Ninomiya et al. (2005) investigate beam thresholding based on the local width to improve the speed of a probabilistic HPSG parser. In each cell of a CYK chart, the method keeps only a portion of the edges which have higher figure of merits compared to the other edges in the same cell. In particular, each cell keeps the edges whose figure of merit is greater than αmax - δ, where αmax is the highest figure of merit among the edges in the chart. The term “beam thresholding” is a little confusing, since a beam search is not necessary – instead, the CYK chart is pruned directly. For this reason, we prefer the term “char"
W08-1705,rohrer-forst-2006-improving,1,0.878825,"ore increases significantly from 79.93 to 82.83. The oracle f-score also increases, while there is a decrease in the random fscore. This shows that we are throwing away good solutions during pruning, but that overall the results improve. Part of this again is due to the fact that with no pruning, skimming is triggered much more often. With a pruning factor of 5, there are no skimmed sentences. There is also one sentence that timed out with no pruning, which also lowers the most probable and oracle f-scores. 1 The reason there are more fragment parses than, for example, the results reported in Rohrer and Forst (2006) is that the bracketed input constrains the parser to only return parses compatible with the bracketed input. If there is no solution compatible with the brackets, then a fragment parse is returned. 35 Pruning Level Oracle F-Score Time (CPU seconds) # Time Outs # Fragments # Skimmed Sents None 83.07 288 0 23 8 4 84.50 100 0 39 0 5 85.47 109 36 0 6 85.75 123 0 31 1 7 85.57 132 0 29 1 8 85.57 151 0 27 1 9 85.02 156 0 27 1 10 84.10 182 0 24 3 Table 1: Results of c-structure pruning experiments on English development data Pruning Level Oracle F-Score Time (CPU seconds) # Time Outs # Fragments None"
W08-1705,W07-1209,1,0.872894,"Missing"
W08-1705,W07-2207,0,0.0342055,"tead. Clark and Curran (2007) describe the use of a supertagger with a CCG parser. A supertagger is like a tagger but with subcategorization information included. Chart pruners and supertaggers are conceptually complementary, since chart pruners prune edges with the same span and the same category, whereas supertaggers prune (lexical) edges with the same span and different categories. Ninomiya et al. (2005) showed that combining a chunk parser with beam thresholding produced better results than either technique alone. So adding a supertagger should improve the results described in this paper. Zhang et al. (2007) describe a technique to selectively unpack an HPSG parse forest to apply maximum entropy features and get the n-best parses. XLE already does something similar when it applies maximum entropy features to get the n-best feature structures after having obtained a packed representation of all of the valid feature structures. The current paper shows that pruning the c-structure chart before doing (packed) unification speeds up the process of getting a packed representation of all the valid feature structures (except the ones that may have been pruned). Er vertraut ihr. he trusts her ‘He trusts he"
W08-1705,P05-1022,0,0.0210722,"articular cell in the chart are pruned if their probabilities are not higher than a certain threshold. The chart pruner uses a simple stochastic CFG model. The probability of a tree is the product of the probabilities of each of the rules used to form the tree, including the rules that lead to lexical items (such as N → dog). The probability of a rule is basically the number of times that that particular form of the rule occurs in the training data divided by the number of times the rule’s category occurs in the training data, plus a smoothing term. This is similar to the pruning described in Charniak and Johnson (2005) where edges in a coarse-grained parse forest are pruned to allow full evaluation with finegrained categories. The pruner prunes at the level of individual constituents in the chart. It calculates the probabilities of each of the subtrees of a constituent and compares them. The probability of each subtree is compared with the best subtree probability for that constituent. If a subtree’s probability is lower than the best probability by a given factor, then the subtree is pruned. In practice, the threshold is the natural logarithm of the factor used. So a value of 5 means that a subtree will be"
W08-1705,J93-2004,0,\N,Missing
W08-1705,J07-4004,0,\N,Missing
W10-2106,rohrer-forst-2006-improving,1,0.942795,"Missing"
W10-2106,H01-1035,0,0.15152,"3). parallel corpora and cross-lingual NLP induction techniques. Since adverbs are often overtly marked in other languages (i.e. the ly-suffix in English), adverbial participles can be straightforwadly detected on word-aligned parallel text. We describe the ingretation of the automatically induced resource of adverbial participles into the German LFG, and provide a detailed evaluation of its effect on the grammar, see Section 5. While the use of parallel resources is rather familiar in a wide range of NLP domains, such as statistical machine translation (Koehn, 2005) or annotation projection (Yarowsky et al., 2001), our work shows that they can be exploited for very specific problems that arise in deep linguistic analysis (see Section 4). In this way, highprecision, data-oriented induction techniques can clearly improve rule-based system development through combining the benefits of high empirical accuracy and little manual effort. 2 3 Participles in the German LFG 3.1 Analysis The morphosyntactic ambiguity of German participles presents a notorious difficulty for theoretical and computational analysis. The reason is that adjectives (i.e. adjectival participles) do not only occur as attributive modifier"
W10-2106,W02-1503,1,0.901049,"ller et al., 1995) assigns the tag “ADJD” to predicative adjectives as well as adverbs. A Broad-Coverage LFG for German Lexical Functional Grammar (LFG) (Bresnan, 2000) is a constraint-based theory of grammar. It posits two levels of representation, c(onstituent)structure and f(unctional)- structure. C-structure is represented by contextfree phrase-structure trees, and captures surface grammatical configurations. F-structures approximate basic predicateargument and adjunct structures. The experiments reported in this paper use the German LFG grammar constructed as part of the ParGram project (Butt et al., 2002). The grammar is implemented in the XLE, a grammar development environment which includes a very efficient LFG parser. Within the spectrum of appraoches to natural language parsing, XLE can be considered a hybrid system combining a hand-crafted grammar with a number of automatic ambiguity management techniques: (i) c-structure pruning where, based on information from statstically obtained parses, some trees are ruled out before fstructure unification (Cahill et al., 2007), (ii) an Optimaly Theory-style constraint mechanism for filtering and ranking competing analyses (Frank et al., 2001), and"
W10-2106,2005.mtsummit-papers.11,0,0.00413936,"ic morphological tags, see (Dipper, 2003). parallel corpora and cross-lingual NLP induction techniques. Since adverbs are often overtly marked in other languages (i.e. the ly-suffix in English), adverbial participles can be straightforwadly detected on word-aligned parallel text. We describe the ingretation of the automatically induced resource of adverbial participles into the German LFG, and provide a detailed evaluation of its effect on the grammar, see Section 5. While the use of parallel resources is rather familiar in a wide range of NLP domains, such as statistical machine translation (Koehn, 2005) or annotation projection (Yarowsky et al., 2001), our work shows that they can be exploited for very specific problems that arise in deep linguistic analysis (see Section 4). In this way, highprecision, data-oriented induction techniques can clearly improve rule-based system development through combining the benefits of high empirical accuracy and little manual effort. 2 3 Participles in the German LFG 3.1 Analysis The morphosyntactic ambiguity of German participles presents a notorious difficulty for theoretical and computational analysis. The reason is that adjectives (i.e. adjectival parti"
W10-2106,nivre-etal-2006-maltparser,0,0.107237,"Missing"
W10-2106,J03-1002,0,0.00381894,"erbial translation for other reasons. A typical configuration is exemplified in (6) where the German main verb vorlegen is translated as the verb-adverb combination put forward. c. Apr`es l’ e´ largissement a` l’ Est, la tendance sera davantage a` la lib´eralisation. In the following, we describe experiments on Europarl where we automatically extract and filter adverbially translated German participles. 4.1 Data We base our experiments on the German, English, French and Dutch part of the Europarl corpus. We automatically word-aligned the German part to each of the others with the GIZA++ tool (Och and Ney, 2003). Note that, due to divergences in sentence alignment and tokenisation, the three word-alignments are not completely synchronised. Moreover, each of the 4 languages has been automatically PoS tagged using the TreeTagger (Schmid, 1994). In addition, the German and English parts have been parsed with MaltParser (Nivre et al., 2006). Since we want to limit our investigation to those participles that are not already recorded as lexicalised adjective or adverb in the DMOR morphology, we first have to generate the set of participle candidates from the tagged Europarl data. We extract all distinct wo"
W10-2106,P02-1035,0,0.169128,"ry efficient LFG parser. Within the spectrum of appraoches to natural language parsing, XLE can be considered a hybrid system combining a hand-crafted grammar with a number of automatic ambiguity management techniques: (i) c-structure pruning where, based on information from statstically obtained parses, some trees are ruled out before fstructure unification (Cahill et al., 2007), (ii) an Optimaly Theory-style constraint mechanism for filtering and ranking competing analyses (Frank et al., 2001), and (iii) a stochastic disambiguation component which is based on a log-linear probability model (Riezler et al., 2002) and works on the packed representations. The German LFG grammar integrates a morphological component which is a variant of (2) a. Das Experiment hat ihn begeistert. ‘The experiment has enthused him.’ b. Er scheint von dem Experiment begeistert. ‘He seems enthusiastic about the experiment.’ c. Er hat begeistert experimentiert. ‘He has experimented in an enthusiastic way’ or: ‘He was enthusiastic when he experimented.’ For performance reasons, the German LFG does not cover free predicatives at the moment. In the context of our crosslingual induction approach, the distinction between predicative"
W10-2106,W08-1705,1,\N,Missing
W12-1632,P99-1048,0,0.031733,"Information structure is usually taken to describe clauseinternal divisions into focus-background, topic-comment, or theme-rheme, which are in turn defined in terms of contextual factors such as given-new information, salience, contrast and alternatives, cf. Steedman and Kruijff-Korbayov´a (2003), Krifka (2007). Information status is the subfield of information structure which exclusively deals with the given-new distinction and which is normally confined to referring expressions. 2 Learning information status A simpler variant of the task is anaphoricity detection (discourse-new detection) (Bean and Riloff, 1999; Ng and Cardie, 2002; Uryupina, 2003; Denis and Baldridge, 2007; Zhou and Kong, 2011), which divides discourse entities into anaphoric (given) and new. Identifying discourse-new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities. In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications. Nissim (2006) and Rahman and Ng (2011) developed methods to automati"
W12-1632,P09-1092,1,0.840954,"has been handannotated with information status labels. We choose a selection of 6668 annotated phrases (1420 sentences). This is an order of magnitude smaller than the annotated Switchboard corpus of Calhoun et al. (2010). We parse each sentence with the German Lexical Functional Grammar of Rohrer and Forst (2006) using the XLE parser in order to automati3 Note that in coreference annotation it is an open question whether two identical generic terms should count as coreferent. 234 cally extract (morpho-)syntactic and functional features for our model. 5 Prediction Model for Information Status Cahill and Riester (2009) show that there are asymmetries between pairs of information status labels contained in sentences, i.e. certain classes of expressions tend to precede certain other classes. We therefore treat the prediction of IS labels as a sequence labeling task.4 We train a CRF using wapiti (Lavergne et al., 2010), with the features outlined in Table 1. We also include a basic “coreference” feature, similar to the lexical features of Rahman and Ng (2011), that fires if there is some lexical overlap of nouns (or compound nouns) in the preceding 10 sentences. The original label set described in Riester et a"
W12-1632,N07-1030,0,0.0270717,"ternal divisions into focus-background, topic-comment, or theme-rheme, which are in turn defined in terms of contextual factors such as given-new information, salience, contrast and alternatives, cf. Steedman and Kruijff-Korbayov´a (2003), Krifka (2007). Information status is the subfield of information structure which exclusively deals with the given-new distinction and which is normally confined to referring expressions. 2 Learning information status A simpler variant of the task is anaphoricity detection (discourse-new detection) (Bean and Riloff, 1999; Ng and Cardie, 2002; Uryupina, 2003; Denis and Baldridge, 2007; Zhou and Kong, 2011), which divides discourse entities into anaphoric (given) and new. Identifying discourse-new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities. In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications. Nissim (2006) and Rahman and Ng (2011) developed methods to automatically identify three different classes: OLD , MEDIATED and NEW e"
W12-1632,finthammer-cramer-2008-exploring,0,0.023776,"Missing"
W12-1632,W97-0802,0,0.151917,"Missing"
W12-1632,P10-1052,0,0.0842727,"Missing"
W12-1632,H05-1031,0,0.0247588,"emonstrative Syntactic shape, e.g. apposition with a determiner and attributive modifier Head noun type, e.g. common * Head noun number, e.g. singular Table 1: Features of the CRF prediction model (* indicates feature used in baseline model) CDU parliamentary group may be known to parts of a German audience but not to other people. We address this by collecting both hearer-known and hearer-unknown definite expressions into one class UNUSED. This does not rule out further subclassification (known/unknown) or the possibility of using machine learning techniques to identify this distinction, see Nenkova et al. (2005). The fact that Rahman and Ng (2011) report the highest confusion rate between NEW and MEDIATED entities may have its roots in this issue. New. Only (specific) indefinites are labeled NEW. Generic. An issue which is not dealt with in Nissim et al. (2004) are GENERIC expressions as in Lions have manes. Reiter and Frank (2010) discuss the task of identifying generic items in a manner similar to the learning tasks presented above, using a Bayesian network. We believe it makes sense to integrate genericity detection into information-status prediction.3 4 German data Our work is based on the DIRNDL"
W12-1632,C02-1139,0,0.0282763,"is usually taken to describe clauseinternal divisions into focus-background, topic-comment, or theme-rheme, which are in turn defined in terms of contextual factors such as given-new information, salience, contrast and alternatives, cf. Steedman and Kruijff-Korbayov´a (2003), Krifka (2007). Information status is the subfield of information structure which exclusively deals with the given-new distinction and which is normally confined to referring expressions. 2 Learning information status A simpler variant of the task is anaphoricity detection (discourse-new detection) (Bean and Riloff, 1999; Ng and Cardie, 2002; Uryupina, 2003; Denis and Baldridge, 2007; Zhou and Kong, 2011), which divides discourse entities into anaphoric (given) and new. Identifying discourse-new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities. In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications. Nissim (2006) and Rahman and Ng (2011) developed methods to automatically identify three"
W12-1632,nissim-etal-2004-annotation,0,0.321473,"s into anaphoric (given) and new. Identifying discourse-new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities. In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications. Nissim (2006) and Rahman and Ng (2011) developed methods to automatically identify three different classes: OLD , MEDIATED and NEW expressions. This classification, which is described in Nissim et al. (2004), has been used for annotating the Switchboard dialog corpus (Calhoun et al., 2010), on which both studies are based. Most recently, Rahman and Ng (2012) extend their automatic prediction system to a more fine-grained set of 16 subtypes. 232 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 232–236, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics Old. The class of OLD entities in Nissim et al. (2004) is not limited to full-fledged anaphors like in Example (1a) but also includes cases of generic an"
W12-1632,W06-1612,0,0.668361,"phoricity detection (discourse-new detection) (Bean and Riloff, 1999; Ng and Cardie, 2002; Uryupina, 2003; Denis and Baldridge, 2007; Zhou and Kong, 2011), which divides discourse entities into anaphoric (given) and new. Identifying discourse-new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities. In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications. Nissim (2006) and Rahman and Ng (2011) developed methods to automatically identify three different classes: OLD , MEDIATED and NEW expressions. This classification, which is described in Nissim et al. (2004), has been used for annotating the Switchboard dialog corpus (Calhoun et al., 2010), on which both studies are based. Most recently, Rahman and Ng (2012) extend their automatic prediction system to a more fine-grained set of 16 subtypes. 232 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 232–236, c Seoul, South Korea, 5-6 July 2012. 2012 A"
W12-1632,J98-2001,0,0.878421,"issim et al. (2004) is not limited to full-fledged anaphors like in Example (1a) but also includes cases of generic and first/second person pronouns like in (1b), which may or may not possess a previous mention. (1) a. b. Shares in General Electric rose as investors bet that the US company would take more lucrative engine orders for the A380. I wonder where this comes from. Mediated. The group of MEDIATED entities mainly has two subtypes: (2a) shows an expression which has not been mentioned before but which is dependent on previous context. Such items have also been called bridging anaphors (Poesio and Vieira, 1998). (2b) contains a phrase which is generally known but does not depend on the discourse context. (2) a. b. Tomorrow, the Shenzhou 8 spacecraft will be in a position to attempt the docking. They hope that he will be given the right to remain in the Netherlands. New. The label NEW, following Nissim et al. (2004: 1024), applies “to entities that have not yet been introduced in the dialog and that the hearer cannot infer from previously mentioned entities.”2 Two kinds of expressions which fall into this category are unfamiliar definites (3a) and (specific) indefinites (3b). (3) a. b. The man who sh"
W12-1632,D11-1099,0,0.545222,"n (discourse-new detection) (Bean and Riloff, 1999; Ng and Cardie, 2002; Uryupina, 2003; Denis and Baldridge, 2007; Zhou and Kong, 2011), which divides discourse entities into anaphoric (given) and new. Identifying discourse-new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities. In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications. Nissim (2006) and Rahman and Ng (2011) developed methods to automatically identify three different classes: OLD , MEDIATED and NEW expressions. This classification, which is described in Nissim et al. (2004), has been used for annotating the Switchboard dialog corpus (Calhoun et al., 2010), on which both studies are based. Most recently, Rahman and Ng (2012) extend their automatic prediction system to a more fine-grained set of 16 subtypes. 232 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 232–236, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computatio"
W12-1632,E12-1081,0,0.451142,"n, there is no need to identify antecedents for new entities. In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications. Nissim (2006) and Rahman and Ng (2011) developed methods to automatically identify three different classes: OLD , MEDIATED and NEW expressions. This classification, which is described in Nissim et al. (2004), has been used for annotating the Switchboard dialog corpus (Calhoun et al., 2010), on which both studies are based. Most recently, Rahman and Ng (2012) extend their automatic prediction system to a more fine-grained set of 16 subtypes. 232 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 232–236, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Linguistics Old. The class of OLD entities in Nissim et al. (2004) is not limited to full-fledged anaphors like in Example (1a) but also includes cases of generic and first/second person pronouns like in (1b), which may or may not possess a previous mention. (1) a. b. Shares in General Electric rose as investors bet"
W12-1632,P10-1005,0,0.0593018,"r people. We address this by collecting both hearer-known and hearer-unknown definite expressions into one class UNUSED. This does not rule out further subclassification (known/unknown) or the possibility of using machine learning techniques to identify this distinction, see Nenkova et al. (2005). The fact that Rahman and Ng (2011) report the highest confusion rate between NEW and MEDIATED entities may have its roots in this issue. New. Only (specific) indefinites are labeled NEW. Generic. An issue which is not dealt with in Nissim et al. (2004) are GENERIC expressions as in Lions have manes. Reiter and Frank (2010) discuss the task of identifying generic items in a manner similar to the learning tasks presented above, using a Bayesian network. We believe it makes sense to integrate genericity detection into information-status prediction.3 4 German data Our work is based on the DIRNDL radio news corpus of Eckart et al. (2012) which has been handannotated with information status labels. We choose a selection of 6668 annotated phrases (1420 sentences). This is an order of magnitude smaller than the annotated Switchboard corpus of Calhoun et al. (2010). We parse each sentence with the German Lexical Functio"
W12-1632,riester-etal-2010-recursive,1,0.816419,"ester (2009) show that there are asymmetries between pairs of information status labels contained in sentences, i.e. certain classes of expressions tend to precede certain other classes. We therefore treat the prediction of IS labels as a sequence labeling task.4 We train a CRF using wapiti (Lavergne et al., 2010), with the features outlined in Table 1. We also include a basic “coreference” feature, similar to the lexical features of Rahman and Ng (2011), that fires if there is some lexical overlap of nouns (or compound nouns) in the preceding 10 sentences. The original label set described in Riester et al. (2010) contains 21 labels. Here we work with a subset of maximally 12 labels, but also consider smaller subsets of labels and carry out a mapping to the Nissim (2006) label set (Table 2).5 We run a 10fold cross-validation experiment and report average prediction accuracy. The results are given in Table 3a. As an informed baseline, we run the same crossvalidation experiment with a subset of features that roughly correspond to the features of Nissim (2006). Our models perform statistically significantly better than the baseline (p &lt; 0.001, using the approximate randomization test) for all label sets."
W12-1632,rohrer-forst-2006-improving,0,0.032475,"sk of identifying generic items in a manner similar to the learning tasks presented above, using a Bayesian network. We believe it makes sense to integrate genericity detection into information-status prediction.3 4 German data Our work is based on the DIRNDL radio news corpus of Eckart et al. (2012) which has been handannotated with information status labels. We choose a selection of 6668 annotated phrases (1420 sentences). This is an order of magnitude smaller than the annotated Switchboard corpus of Calhoun et al. (2010). We parse each sentence with the German Lexical Functional Grammar of Rohrer and Forst (2006) using the XLE parser in order to automati3 Note that in coreference annotation it is an open question whether two identical generic terms should count as coreferent. 234 cally extract (morpho-)syntactic and functional features for our model. 5 Prediction Model for Information Status Cahill and Riester (2009) show that there are asymmetries between pairs of information status labels contained in sentences, i.e. certain classes of expressions tend to precede certain other classes. We therefore treat the prediction of IS labels as a sequence labeling task.4 We train a CRF using wapiti (Lavergne"
W12-1632,P03-2012,0,0.0205037,"escribe clauseinternal divisions into focus-background, topic-comment, or theme-rheme, which are in turn defined in terms of contextual factors such as given-new information, salience, contrast and alternatives, cf. Steedman and Kruijff-Korbayov´a (2003), Krifka (2007). Information status is the subfield of information structure which exclusively deals with the given-new distinction and which is normally confined to referring expressions. 2 Learning information status A simpler variant of the task is anaphoricity detection (discourse-new detection) (Bean and Riloff, 1999; Ng and Cardie, 2002; Uryupina, 2003; Denis and Baldridge, 2007; Zhou and Kong, 2011), which divides discourse entities into anaphoric (given) and new. Identifying discourse-new expressions in texts is helpful as a precursor to coreference resolution, since, by definition, there is no need to identify antecedents for new entities. In the linguistic literature, referring expressions have been distinguished in much more detail, and there is reason to believe that this could also provide useful information for NLP applications. Nissim (2006) and Rahman and Ng (2011) developed methods to automatically identify three different classe"
W12-2027,A00-2019,0,0.308125,"the HOO 2012 format did not account for this, which may have decreased recognition performance slightly. 234 Base System As our base system, we repurpose a complex system designed to automatically score student essays (both native and non-native and across a wide range of competency levels). The system is also used to give feedback to essay writers, so precision is favored over recall. There are three main modules in the essay-scoring system whose purpose it is to detect preposition and determiner errors (as they are defined in that system). Many of the details have been reported previously (Chodorow and Leacock, 2000; Han et al., 2004; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008), so here we will only give brief summaries of these modules. It is important to note that this system was run without modification. That is, no training of new models or tuning was carried out specifically for the shared task. In addition, for the two statistical modules, we only had access to the final, boolean decisions about whether an error is present or not at a particular location in text. That is, we did not have access to confidence scores, and so task-specific tuning for F-score was not an optio"
W12-2027,W07-1604,1,0.823641,"creased recognition performance slightly. 234 Base System As our base system, we repurpose a complex system designed to automatically score student essays (both native and non-native and across a wide range of competency levels). The system is also used to give feedback to essay writers, so precision is favored over recall. There are three main modules in the essay-scoring system whose purpose it is to detect preposition and determiner errors (as they are defined in that system). Many of the details have been reported previously (Chodorow and Leacock, 2000; Han et al., 2004; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008), so here we will only give brief summaries of these modules. It is important to note that this system was run without modification. That is, no training of new models or tuning was carried out specifically for the shared task. In addition, for the two statistical modules, we only had access to the final, boolean decisions about whether an error is present or not at a particular location in text. That is, we did not have access to confidence scores, and so task-specific tuning for F-score was not an option. 3.1 Preposition Error Detection The base system detects"
W12-2027,W12-2006,0,0.04109,"tric used in the HOO 2012 Shared Task: balanced F-score, or F1 (§6). We find that the tuned hybrid system improves upon the recall and F-score of the base system. Also, in the HOO 2012 Shared Task, the hybrid system achieved results that were competitive with other submitted grammatical error detection systems (§7). 233 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 233–241, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics 2 Task Definition 3 In this section, we provide a brief overview of the HOO 2012 Shared Task (Dale et al., 2012). The task focuses on prepositions and determiners only, distinguishing the following error types: preposition selection errors (coded “RT” in the data), extraneous prepositions (“UT”), missing prepositions (“MT”), determiner selection errors (“RD”), extraneous determiners (“UD”), and missing determiners (“MD”). For training and testing data, the shared task uses short essays from an examination for speakers of English as a foreign language. The data includes gold standard human annotations identifying preposition and determiner errors. These errors are represented as edits that transform an u"
W12-2027,N10-1019,0,0.107838,"different types. In this section, we describe how we filter edits using their scores and how we combine them with edits from the base system (§3). As described above, for an alternative v to be considered as a candidate edit, the value of r(w, i, v) in Eq. 2 must be greater than a threshold of 1, indicating that the alternative scores higher than the original word. However, we observed low precision during development when including all candidate edits and decided to penalize the ratios. Bergsma et al. (2009) discuss raising the threshold, which has 7 The heuristics are based on those used in Gamon (2010) (personal communication). 237 a similar effect. Preliminary experiments indicated that different edits (e.g., extraneous preposition edits and preposition selection edits) should have different penalties, and we also want to avoid edits with overlapping spans. Thus, for each location with one or more candidate edits, we select the best according to Equation 3 and filter out the rest. v ∗ = arg max v r(w, i, v) − penalty(wi , v) (3) penalty(wi , v) is a function that takes the current word wi and the alternative v and returns one of 6 values: qRT for preposition selection, qU T for extraneous"
W12-2027,han-etal-2004-detecting,0,0.20828,"account for this, which may have decreased recognition performance slightly. 234 Base System As our base system, we repurpose a complex system designed to automatically score student essays (both native and non-native and across a wide range of competency levels). The system is also used to give feedback to essay writers, so precision is favored over recall. There are three main modules in the essay-scoring system whose purpose it is to detect preposition and determiner errors (as they are defined in that system). Many of the details have been reported previously (Chodorow and Leacock, 2000; Han et al., 2004; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008), so here we will only give brief summaries of these modules. It is important to note that this system was run without modification. That is, no training of new models or tuning was carried out specifically for the shared task. In addition, for the two statistical modules, we only had access to the final, boolean decisions about whether an error is present or not at a particular location in text. That is, we did not have access to confidence scores, and so task-specific tuning for F-score was not an option. 3.1 Preposition"
W12-2027,C08-1109,1,0.913083,"formance slightly. 234 Base System As our base system, we repurpose a complex system designed to automatically score student essays (both native and non-native and across a wide range of competency levels). The system is also used to give feedback to essay writers, so precision is favored over recall. There are three main modules in the essay-scoring system whose purpose it is to detect preposition and determiner errors (as they are defined in that system). Many of the details have been reported previously (Chodorow and Leacock, 2000; Han et al., 2004; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008), so here we will only give brief summaries of these modules. It is important to note that this system was run without modification. That is, no training of new models or tuning was carried out specifically for the shared task. In addition, for the two statistical modules, we only had access to the final, boolean decisions about whether an error is present or not at a particular location in text. That is, we did not have access to confidence scores, and so task-specific tuning for F-score was not an option. 3.1 Preposition Error Detection The base system detects incorrect and extraneous prepos"
W13-1706,C12-1025,0,0.625905,"e and we expect that the data will become available through the Linguistic Data Consortium in 2013. For future editions of the NLI shared task, we think it would be interesting to expand the scope of NLI from identifying the L1 of student essays to be able to identify the L1 of any piece of writing. The ICLE and TOEFL 11 corpora are both collections of academic writing and thus it may be the case that certain features or methodologies generalize better to other writing genres and domains. For those interested in robust NLI approaches, please refer to the TOR team shared task report as well as Brooke and Hirst (2012). In addition, since the TOEFL 11 data contains proficiency level one could include an evaluation by proficiency level as language learners make different types of errors and may even have stylistic differences in their writing as their proficiency progresses. Finally, while this may be in the periphery of the scope of an NLI shared task, one interesting evaluation is to see how well human raters can fare on this task. This would of course involve knowledgeable language instructors who have years of experience in teaching students from different L1s. Our thinking is that NLI might be one task"
W13-1706,P12-2038,0,0.551993,") have figured prominently in prior work. Not only are they easy to compute, but they can be quite predictive. However, there are many variations on the features. Past reseach efforts have explored different n-gram windows (though most tend to focus on unigrams and bigrams), different thresholds for how many ngrams to include as well as whether to encode the feature as binary (presence or absence of the particular n-gram) or as a normalized count. The inclusion of syntactic features has been a focus in recent work. Wong and Dras (2011) explored the use of production rules from two parsers and Swanson and Charniak (2012) explored the use of Tree Substitution Grammars (TSGs). Tetreault et al. (2012) also investigated the use of TSGs as well as dependency features extracted from the Stanford parser. Other approaches to NLI have included the use of Latent Dirichlet Analysis to cluster features (Wong et al., 2011), adaptor grammars (Wong et al., 2012), and language models (Tetreault et al., 2012). Additionally, there has been research into the effects of training and testing on different corpora (Brooke and Hirst, 2011). Much of the aforementioned work takes the perspective of optimizing for the task of Native La"
W13-1706,C12-1158,1,0.829245,"red essays written by college-level English language learners. However, this corpus is quite small for training and testing statistical systems which makes it difficult to tell whether the systems that are developed can scale well to larger data sets or to different domains. Since the ICLE corpus was not designed with the task of NLI in mind, the usability of the corpus for this task is further compromised by idiosyncrasies in the data such as topic bias (as shown by Brooke and Hirst (2011)) and the occurrence of characters which only appear in essays written by speakers of certain languages (Tetreault et al., 2012). As a result, it is hard to draw conclusions about which features 48 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 48–57, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics actually perform best. The second issue is that there has been little consistency in the field in the use of cross-validation, the number of L1s, and which L1s are used. As a result, comparing one approach to another has been extremely difficult. The first Shared Task in Native Language Identification is intended to better unify this c"
W13-1706,D11-1148,0,0.502312,"f the ICLE corpus consisting of 5 L1s. N-gram features (word, character and POS) have figured prominently in prior work. Not only are they easy to compute, but they can be quite predictive. However, there are many variations on the features. Past reseach efforts have explored different n-gram windows (though most tend to focus on unigrams and bigrams), different thresholds for how many ngrams to include as well as whether to encode the feature as binary (presence or absence of the particular n-gram) or as a normalized count. The inclusion of syntactic features has been a focus in recent work. Wong and Dras (2011) explored the use of production rules from two parsers and Swanson and Charniak (2012) explored the use of Tree Substitution Grammars (TSGs). Tetreault et al. (2012) also investigated the use of TSGs as well as dependency features extracted from the Stanford parser. Other approaches to NLI have included the use of Latent Dirichlet Analysis to cluster features (Wong et al., 2011), adaptor grammars (Wong et al., 2012), and language models (Tetreault et al., 2012). Additionally, there has been research into the effects of training and testing on different corpora (Brooke and Hirst, 2011). Much of"
W13-1706,U11-1015,0,0.044661,"many ngrams to include as well as whether to encode the feature as binary (presence or absence of the particular n-gram) or as a normalized count. The inclusion of syntactic features has been a focus in recent work. Wong and Dras (2011) explored the use of production rules from two parsers and Swanson and Charniak (2012) explored the use of Tree Substitution Grammars (TSGs). Tetreault et al. (2012) also investigated the use of TSGs as well as dependency features extracted from the Stanford parser. Other approaches to NLI have included the use of Latent Dirichlet Analysis to cluster features (Wong et al., 2011), adaptor grammars (Wong et al., 2012), and language models (Tetreault et al., 2012). Additionally, there has been research into the effects of training and testing on different corpora (Brooke and Hirst, 2011). Much of the aforementioned work takes the perspective of optimizing for the task of Native Language Identification, that is, what is the best way of modeling the problem to get the highest system accuracy? The problem of Native Language Identifica1 http://nlisharedtask2013.org/bibliography-of-relatedwork-in-nli tion is also of interest to researchers in Second Language Acquisition wher"
W13-1706,D12-1064,0,0.294674,"ther to encode the feature as binary (presence or absence of the particular n-gram) or as a normalized count. The inclusion of syntactic features has been a focus in recent work. Wong and Dras (2011) explored the use of production rules from two parsers and Swanson and Charniak (2012) explored the use of Tree Substitution Grammars (TSGs). Tetreault et al. (2012) also investigated the use of TSGs as well as dependency features extracted from the Stanford parser. Other approaches to NLI have included the use of Latent Dirichlet Analysis to cluster features (Wong et al., 2011), adaptor grammars (Wong et al., 2012), and language models (Tetreault et al., 2012). Additionally, there has been research into the effects of training and testing on different corpora (Brooke and Hirst, 2011). Much of the aforementioned work takes the perspective of optimizing for the task of Native Language Identification, that is, what is the best way of modeling the problem to get the highest system accuracy? The problem of Native Language Identifica1 http://nlisharedtask2013.org/bibliography-of-relatedwork-in-nli tion is also of interest to researchers in Second Language Acquisition where they seek to explain syntactic trans"
W13-1706,P11-1019,0,0.0387459,"uebingen Ualberta UKP Unibuc UNT UTD VTEX Abbreviation BOB CHO HAI CN COR CUN CYW DAR EUR HAU ITA JAR KYL LIM HYD MIC CAR MQ NAI NRC OSL TOR TUE UAB UKP BUC UNT UTD VTX missions. Table 5 shows the results for the third subtask “Open-2”. Four teams competed in this task for a total of 15 submissions. The challenge for those competing in the Open tasks was finding enough non-TOEFL 11 data for each L1 to train a classifier. External corpora commonly used in the competition included the: • ICLE: which covered all L1s except for Arabic, Hindi and Telugu; • FCE: First Certificate in English Corpus (Yannakoudakis et al., 2011): a collection of essay written for an English assessment exam, which covered all L1s except for Arabic, Hindi and Telugu • ICNALE: International Corpus Network of Asian Learners of English (Ishikawa, 2011): a collection of essays written by Chinese, Japanese and Korean learners of English along with 7 other L1s with Asian backgrounds. Table 2: Participating Teams and Team Abbreviations top submission for each team and its performance by overall accuracy and by L1.2 Table 3 shows results for the Closed sub-task where teams developed systems that were trained solely on TOEFL 11- TRAIN and TOEFL"
W13-1739,N13-1055,1,0.88711,"Missing"
W13-1739,W11-2838,0,0.0291019,"age learners. Therefore, we place more importance on the precision of the system than recall. We train our model on features that take the context of a pair of words into account, as well as other discriminative features. We present a number of evaluations on both artificially generated errors and naturally occurring learner errors and show that our classifiers achieve high precision and reasonable recall. 2 Related Work The task of detecting missing hyphens is related to previous work on detecting punctuation errors. One of the classes of errors in the Helping Our Own (HOO) 2011 shared task (Dale and Kilgarriff, 2011) was punctuation. Comma errors are the most frequent kind of punctuation error made by learners. Israel et al. (2012) present a model for detecting these kinds of errors in learner texts. They train CRF models on sentences from unedited essays written by high-level college students and show that they performs well on detecting errors in learner text. As 300 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 300–305, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics far as we are aware, the HOO 2011 system desc"
W13-1739,N12-1029,1,0.854062,"Missing"
W13-1739,W11-2843,0,0.105521,"ation. Comma errors are the most frequent kind of punctuation error made by learners. Israel et al. (2012) present a model for detecting these kinds of errors in learner texts. They train CRF models on sentences from unedited essays written by high-level college students and show that they performs well on detecting errors in learner text. As 300 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 300–305, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics far as we are aware, the HOO 2011 system description of Rozovskaya et al. (2011) is the only work to specifically reference hyphen errors. They use rules derived from frequencies in the training corpus to determine whether a hyphen was required between two words separated by white space. The task of detecting missing hyphens is related to the task of inserting punctuation into the output of unpunctuated text (for example, the output of speech recognition, automatic generation, machine translation, etc.). Systems that are built on the output of speech recognition can obviously take features like prosody into account. In our case, we are dealing only with written text. Grav"
W13-1739,P11-1019,0,0.0559877,"uating on the Brown Corpus with hyphens removed combined news and Wikipedia revision text achieve the highest overall f-score. Figure (1a) shows the Precision Recall curves for the Wikipedia baselines and the three classifiers. The curves mirror the results in the table, showing that the classifier trained on the newswire text, and the classifier trained on the combined data perform best. The Wikipedia counts baseline performs worst. 6 Evaluating on Learner Text We carry out two evaluations of our system on learner text. We first evaluate on the missing hyphen errors contained in the CLC-FCE (Yannakoudakis et al., 2011). This corpus contains 1,244 exam scripts written by learners of English as part of the Cambridge ESOL First Certificate in English. In total, there are 173 instances of missing hyphen errors. The results are given in Table 4, and the precision recall curves are displayed in Figure (1b). The results show that the classifiers consistently achieve high precision on this data set. This is as expected, given the high threshold set. Looking at the curves, it seems that a slightly lower threshold in this case may lead to better results. The curves show that the combined classifier is performing slig"
W14-1810,N04-1043,0,0.00986009,"a revisions. Acknowledgments We would like to thank Beata Beigman Klebanov, Michael Heilman, Jill Burstein, and the anonymous reviewers for their helpful comments about the paper. We also thank Ani Nenkova, Chris Callison-Burch, Lyle Ungar and their students at the University of Pennsylvania for their feedback on this work. • Using other types of expansions. In this paper, we used a very simple method of generating query expansions – a distributional thesaurus. However, in the future, it may be worth exploring other distributional similarity methods such as Brown clusters (Brown et al., 1992; Miller et al., 2004; Liang, 2005) or word2vec (Mikolov et al., 2013). 8 References Yigal Attali. 2004. Exploring the Feedback and Revision Features of Criterion. Paper presented at the National Council on Measurement in Education (NCME), Educational Testing Service, Princeton, NJ. Conclusions In this paper, we presented our work on building a proof-of-concept tool that can provide automated explicit feedback for preposition errors. We used an existing, error-annotated preposition corpus produced by mining Wikipedia revisions Douglas Biber, Tatiana Nekrasova, and Brad Horn. 2011. The Effectiveness of Feedback for"
W14-1810,J92-4003,0,0.105481,"Missing"
W14-1810,W10-0405,0,0.0265754,"ol. §4 outlines the core system 79 Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 79–88, c Baltimore, Maryland USA, June 26, 2014. 2014 Association for Computational Linguistics tions from a very large corpus of annotated errors, rather than performing a web search on all possible alternatives in the context. The advantage of using an error-annotated corpus is that it contains implicit information about frequent confusion pairs (e.g. “at” instead of “in”) that are independent of the frequency of the preposition and the current context. Milton and Cheng (2010) describe a toolkit for helping Chinese learners of English become more independent writers. The toolkit gives the learners access to online resources including web searches, online concordance tools, and dictionaries. Users are provided with snapshots of the word or structure in context. In Milton (2006), 500 revisions to 323 journal entries were made using an earlier version of this tool. Around 70 of these revisions had misinterpreted the evidence presented or were careless mistakes; the remaining revisions resulted in more natural sounding sentences. for generating feedback and §5 presents"
W14-1810,N13-1055,1,0.848494,"le they were composing email messages. They reported that users were able to make effective use of the explicit feedback for that task. The tool had been offered as a web service but has since been discontinued. Our tool is similar to ESL Assistant in that both produce a list of possible corrections. The main difference between the tools is that ours automatically derives the ranked list of correction sugges3 Wikipedia Revisions Our goal is to build a tool that can provide explicit feedback about errors to writers. We take advantage of the recently released Wikipedia preposition error corpus (Cahill et al., 2013) and design our tool based on this large corpus containing sentences annotated for preposition errors and their corrections. The corpus was produced automatically by mining a total of 288 million revisions for 8.8 million articles present in a Wikipedia XML snapshot from 2011. The Wikipedia error corpus, as we refer to in the rest of the paper, contains 2 million sentences annotated with preposition errors and their respective corrections. There are two possible approaches to building an explicit feedback tool for preposition errors based on this corpus: 1. Classifier-based. We could train a c"
W14-1810,N10-1018,0,0.0709325,"Missing"
W14-1810,P11-1092,0,0.0400203,"Missing"
W14-1810,P12-2064,0,0.0264706,"Missing"
W14-1810,C08-1109,0,0.0855632,"Missing"
W14-1810,P10-2065,0,0.0504334,"Missing"
W14-1810,P98-2127,0,0.0102681,"he lower MRR values. In the field of information retrieval, a common practice is to expand the query with words similar to words in the query in order to increase the likelihood of finding documents relevant to the query (Sp¨arck-Jones and Tait, 1984). In this section, we examine whether we can use a similar technique to improve the coverage of the tool. We employ a simple query expansion technique for the cases where no results would otherwise be returned by the tool. For these cases, we first obtain a list of K words similar to the two words around the error from a distributional thesaurus (Lin, 1998), ranked by similarity. We then generate a list of additional queries by combining these two ranked lists of similar words. We then run each query in the list against the Wikipedia index until one of them yields results. Note that since we are using a word-based thesaurus, this expansion technique can only increase coverage when applied to the words1 condition, i.e., single word contexts. We investigate K = 1, 2, 5, or 10 expansions for each of the context words. Table 2 shows the a detailed breakdown of the distribution of the three classes and the MRR values with query expansion integrated i"
W14-1810,N03-1033,0,0.014126,"he erroneous preposition was changed to the correction in the Wikipedia revision index. In this example, the preposition of with the left context of <DT, NNS> and the right context of <DT, NN> was changed to the preposition in 242 times in the Wikipedia revisions. When the user clicks on a bar, the box on the top shows the sentence with the change and the gray box on the right shows 5 (randomly chosen) actual sentences from Wikipedia where the change represented by the bar was made. If parts-of-speech are chosen as context, the tool uses WebSockets to send the sentence to the Stanford Tagger (Toutanova et al., 2003) in the background and compute its part-of-speech tags before searching the index. viding evidence for each correction to the user. 2. Corpus-based. We could use the Wikipedia error corpus directly for feedback. Although this means that suggestions can only be generated for contexts occurring in the Wikipedia data, it also means that all suggestion would be grounded in actual revisions made by other humans on Wikipedia. We believe that anchoring suggestions to human-authored corrections affords greater utility to a language learner, in line with the current practice in lexicography that emphas"
W14-1810,P11-1019,0,0.0499453,"ion. • The words, bigrams, and trigrams before (and after) the preposition error (indexed separately). • The part-of-speech tags, tag bigrams, and tag trigrams before (and after) the error (indexed separately). 5 • The title and URL of the Wikipedia article in which the sentence occurred. Evaluation In order to determine how well the tool performs at suggesting corrections, we used sentences containing preposition errors from the CLC FCE dataset. The CLC FCE Dataset is a collection of 1,244 exam scripts written by learners of English as part of the Cambridge ESOL First Certificate in English (Yannakoudakis et al., 2011). Our evaluation set consists of 3,134 sentences, each containing a single preposition error. We evaluate the tool on two criteria: • The ID of the article revision containing the preposition error. • The ID of the article revision in which the correction was made. Once the index is constructed, eliciting explicit feedback is straightforward. The input to the system is a tokenized sentence with a marked up preposition error (e.g. from an automated preposition error detection system). For each input sentence, the Wikipedia index is then searched with the identified preposition error and the wor"
W14-1810,C98-2122,0,\N,Missing
W14-6106,P05-1022,0,0.170509,"Missing"
W14-6106,W07-1607,0,0.0535568,"Missing"
W14-6106,W11-2925,0,0.284118,"Missing"
W14-6106,N06-1020,0,0.0502823,"hat are difficult to interpret. These kinds of errors are known to cause difficulty for automated analyses (De Felice and Pulman, 2007; Lee and Knutsson, 2008). We explore a previously documented technique for adapting a state-of-the-art parser to be able to better parse learner text: domain adaptation using self-training. Self-training is a semi-supervised learning technique that relies on some labeled data to train an initial model, and then uses large amounts of unlabeled data to iteratively improve that model. Self-training was first successfully applied in the newspaper parsing domain by McClosky et al. (2006) who used the Penn Treebank WSJ as their labeled data and unlabeled data from the North American News Text corpus. Previous attempts (Charniak, 1997; Steedman et al., 2003) had not shown encouraging results, and McClosky et al. (2006) hypothesize that the gain they saw was due to the two-phase nature of the BLLIP parser used in their experiments. In a follow-up study (McClosky et al., 2008) they find that one major factor leading to successful self-training is when the process sees known words in new combinations. 2 Related Work Foster et al. (2011) compare edited newspaper text and unedited f"
W14-6106,C08-1071,0,0.0191689,"some labeled data to train an initial model, and then uses large amounts of unlabeled data to iteratively improve that model. Self-training was first successfully applied in the newspaper parsing domain by McClosky et al. (2006) who used the Penn Treebank WSJ as their labeled data and unlabeled data from the North American News Text corpus. Previous attempts (Charniak, 1997; Steedman et al., 2003) had not shown encouraging results, and McClosky et al. (2006) hypothesize that the gain they saw was due to the two-phase nature of the BLLIP parser used in their experiments. In a follow-up study (McClosky et al., 2008) they find that one major factor leading to successful self-training is when the process sees known words in new combinations. 2 Related Work Foster et al. (2011) compare edited newspaper text and unedited forum posts in a self-training parsing experiment, evaluating on a treebank of informal discussion forum entries about football. They find that both data sources perform about equally well on their small test set overall, but that the underlying grammars learned from the two sources were different. Ott and Ziai (2010) apply an out-of-the-box German dependency parser to learner text and analy"
W14-6106,P07-1078,0,0.0431852,"Missing"
W14-6106,E03-1008,0,0.0392158,"a previously documented technique for adapting a state-of-the-art parser to be able to better parse learner text: domain adaptation using self-training. Self-training is a semi-supervised learning technique that relies on some labeled data to train an initial model, and then uses large amounts of unlabeled data to iteratively improve that model. Self-training was first successfully applied in the newspaper parsing domain by McClosky et al. (2006) who used the Penn Treebank WSJ as their labeled data and unlabeled data from the North American News Text corpus. Previous attempts (Charniak, 1997; Steedman et al., 2003) had not shown encouraging results, and McClosky et al. (2006) hypothesize that the gain they saw was due to the two-phase nature of the BLLIP parser used in their experiments. In a follow-up study (McClosky et al., 2008) they find that one major factor leading to successful self-training is when the process sees known words in new combinations. 2 Related Work Foster et al. (2011) compare edited newspaper text and unedited forum posts in a self-training parsing experiment, evaluating on a treebank of informal discussion forum entries about football. They find that both data sources perform abo"
W15-0606,C12-1025,0,0.535925,"Missing"
W15-0606,C14-1185,0,0.431649,"Missing"
W15-0606,de-melo-2014-etymological,0,0.0331883,"Missing"
W15-0606,guthrie-etal-2006-closer,0,0.0332885,"L1 and word usage. Using the Etymological WordNet10 database (de Melo, 2014), we extracted two lists of English words with either Old English (508 words) or Latin origins (1,310 words). These words were used as unigram features to train two classifiers. The F1-scores for classification on T OEFL11 are shown in Figure 5. The Old English words, with their West Germanic roots, yield the best results for classifying German data. Conversely, the Latinate features achieve the 9 Hladka et al. (2013) and Henderson et al. (2013) previously used a skip-gram variant that did not include 0 skips as per (Guthrie et al., 2006) and did not improve accuracy. 10 http://www1.icsi.berkeley.edu/%7edemelo/etymwn/ ROOT → S^<ROOT&gt; S^<ROOT&gt; → NP^<S&gt; VP^<S&gt; . NP^<S&gt; → DT JJ JJ NN 0.4 0.35 0.3 0.25 Figure 7: Parent-annotated CFG rules from Fig. 4. 0.2 0.15 0.1 0.05 0 ARA CHI FRE GER HIN ITA Old English Word Features JPN KOR SPA TEL TUR Latin Word Features Figure 5: F1-scores for classifying L1 using English words with Old English or Latin origins. ROOT S JJ . VP NP DT JJ NN Production Rules Extracted from Tree: ROOT Ȳ S S Ȳ NP VP . NP Ȳ DT JJ JJ NN PP Ȳ IN NP VP Ȳ VBD PP NP Ȳ DT JJ NN . PP VBD The quick brown fox jumped NP IN"
W15-0606,W13-1713,0,0.0655331,"tes and word form similarities, but also semantics and meanings. We also examine the link between L1 and word usage. Using the Etymological WordNet10 database (de Melo, 2014), we extracted two lists of English words with either Old English (508 words) or Latin origins (1,310 words). These words were used as unigram features to train two classifiers. The F1-scores for classification on T OEFL11 are shown in Figure 5. The Old English words, with their West Germanic roots, yield the best results for classifying German data. Conversely, the Latinate features achieve the 9 Hladka et al. (2013) and Henderson et al. (2013) previously used a skip-gram variant that did not include 0 skips as per (Guthrie et al., 2006) and did not improve accuracy. 10 http://www1.icsi.berkeley.edu/%7edemelo/etymwn/ ROOT → S^<ROOT&gt; S^<ROOT&gt; → NP^<S&gt; VP^<S&gt; . NP^<S&gt; → DT JJ JJ NN 0.4 0.35 0.3 0.25 Figure 7: Parent-annotated CFG rules from Fig. 4. 0.2 0.15 0.1 0.05 0 ARA CHI FRE GER HIN ITA Old English Word Features JPN KOR SPA TEL TUR Latin Word Features Figure 5: F1-scores for classifying L1 using English words with Old English or Latin origins. ROOT S JJ . VP NP DT JJ NN Production Rules Extracted from Tree: ROOT Ȳ S S Ȳ NP VP . N"
W15-0606,W13-1730,0,0.123388,"Missing"
W15-0606,D14-1142,1,0.760542,"Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat character n-grams as lexical features in this work but restrict our investigation to 1–3-grams. Recent work has also shown improvements from longer sequences (Jarvis et al., 2013; Ionescu et al., 2014). 3 Obtained by training on the T OEFL11 train and development sets and evaluating on the test set. 4 Listed in alphabetical order. 49 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 49–55, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics Classification Accuracy By Feature fier outputs for each data point where the output values represent correct (1) or incorrect (0) predictions made by that learner. Each classifier Ci produces a result vector yi = [yi,1 , . . . , yi,N ] for a set of N documents where"
W15-0606,W13-1714,0,0.429447,"ong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat character n-grams as lexical features in this work but restrict our investigation to 1–3-grams. Recent work has also shown improvements from longer sequences (Jarvis et al., 2013; Ionescu et al., 2014). 3 Obtained by training on the T OEFL11 train and development sets and evaluating on the test set. 4 Listed in alphabetical order. 49 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 49–55, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics Classification Accuracy By Feature fier outputs for each data point where the output values represent correct (1) or incorrect (0) predictions made by that learner. Each classifier Ci produces a result vector yi = [yi,1 , . . . , yi,N ] for a s"
W15-0606,J98-4004,0,0.0134303,"Missing"
W15-0606,E14-4019,1,0.929774,"ich aims to infer the native language (L1) of an author based on texts written in a second language (L2). Machine Learning methods are usually used to identify language use patterns common to speakers of the same L1 (Tetreault et al., 2012). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. In this context, by identifying L1-specific language usage and error patterns, NLI can be used to better understand SLA and develop teaching methods, instructions and learner feedback that is tailored to their mother tongue (Malmasi and Dras, 2014b). Although researchers have employed tens of feature types, no effort has been made to measure the overlap of information they capture. Results from previous studies show that while some feature types yield similar accuracies independently, combining them can improve performance (Brooke and Hirst, Data and Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classi"
W15-0606,D14-1144,1,0.906635,"ich aims to infer the native language (L1) of an author based on texts written in a second language (L2). Machine Learning methods are usually used to identify language use patterns common to speakers of the same L1 (Tetreault et al., 2012). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. In this context, by identifying L1-specific language usage and error patterns, NLI can be used to better understand SLA and develop teaching methods, instructions and learner feedback that is tailored to their mother tongue (Malmasi and Dras, 2014b). Although researchers have employed tens of feature types, no effort has been made to measure the overlap of information they capture. Results from previous studies show that while some feature types yield similar accuracies independently, combining them can improve performance (Brooke and Hirst, Data and Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classi"
W15-0606,N15-1160,1,0.333488,"Missing"
W15-0606,W13-1716,1,0.897502,"tly, combining them can improve performance (Brooke and Hirst, Data and Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classification in our experiments. We experiment with a wide range of previously used syntactic and lexical features: Adaptor Grammars (AG) (Wong et al., 2012), character n-grams (Tsur and Rappoport, 2007),2 Function word unigrams and bigrams (Malmasi et al., 2013), Lemma and Word n-grams, CFG Production Rules (Wong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat character n-grams as lexical features in this work but restrict our investigation to 1–3-grams. Recent work ha"
W15-0606,W15-0620,1,0.830362,"Missing"
W15-0606,P14-2138,0,0.0110556,"ed with other languages (Malmasi and Dras, 2014a), this feature can be a good approximation of the dependencies feature for low-resourced languages without an accurate parser. However, results may vary by language and possibly genre (Liu, 2008). We also note that the skip-gram feature space grows prodigiously as k increases. Another related issue is whether sub-lexical character n-grams are independent of word features. Previously, Tsur and Rappoport (2007) hypothesized that these n-grams are discriminative due to writer choices “strongly influenced by the phonology of their native language”. Nicolai and Kondrak (2014) also investigate the source of L1 differences in the relative frequencies of character bigrams. They propose an algorithm to identify the most discriminative words and subsequently, the bigrams corresponding to these words. They found that removing a small set of highly discriminative words greatly degrades the accuracy of a bigrambased classifier. Based on this they conclude that bigrams capture differences in word usage and lexical transfer rather than L1 phonology. Evidence from our analysis also points to a similar pattern with the predictions of character bigrams and trigrams being stron"
W15-0606,P12-2038,0,0.0320334,"ar Support Vector Machine1 to perform multi-class classification in our experiments. We experiment with a wide range of previously used syntactic and lexical features: Adaptor Grammars (AG) (Wong et al., 2012), character n-grams (Tsur and Rappoport, 2007),2 Function word unigrams and bigrams (Malmasi et al., 2013), Lemma and Word n-grams, CFG Production Rules (Wong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat character n-grams as lexical features in this work but restrict our investigation to 1–3-grams. Recent work has also shown improvements from longer sequences (Jarvis et al., 2013; Ionescu et al., 2014). 3 Obtained by training on the T OEFL11 train and development sets and evaluating on the test set. 4 Listed in alphabetical order. 49 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Ap"
W15-0606,C12-1158,1,0.847225,"Service 660 Rosedale Rd Princeton, NJ 08541, USA acahill@ets.org 2 Introduction Researchers in Second Language Acquisition (SLA) investigate the multiplex of factors that influence our ability to acquire new languages and chief among these is the role of the learner’s mother tongue. This core factor has recently been studied in the task of Native Language Identification (NLI), which aims to infer the native language (L1) of an author based on texts written in a second language (L2). Machine Learning methods are usually used to identify language use patterns common to speakers of the same L1 (Tetreault et al., 2012). While NLI has applications in security, most research has a strong linguistic motivation relating to language teaching and learning. In this context, by identifying L1-specific language usage and error patterns, NLI can be used to better understand SLA and develop teaching methods, instructions and learner feedback that is tailored to their mother tongue (Malmasi and Dras, 2014b). Although researchers have employed tens of feature types, no effort has been made to measure the overlap of information they capture. Results from previous studies show that while some feature types yield similar a"
W15-0606,W13-1706,1,0.652589,"d error patterns, NLI can be used to better understand SLA and develop teaching methods, instructions and learner feedback that is tailored to their mother tongue (Malmasi and Dras, 2014b). Although researchers have employed tens of feature types, no effort has been made to measure the overlap of information they capture. Results from previous studies show that while some feature types yield similar accuracies independently, combining them can improve performance (Brooke and Hirst, Data and Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classification in our experiments. We experiment with a wide range of previously used syntactic and lexical features: Adaptor Grammars (AG) (Wong et al., 2012), character n-grams (Tsur and Rappoport, 2007),2 Function word unigrams and bigrams (Malmasi et al., 2013), Lemma and Word n-grams, CFG Production Rules (Wong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Depe"
W15-0606,W07-0602,0,0.417303,"hat while some feature types yield similar accuracies independently, combining them can improve performance (Brooke and Hirst, Data and Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classification in our experiments. We experiment with a wide range of previously used syntactic and lexical features: Adaptor Grammars (AG) (Wong et al., 2012), character n-grams (Tsur and Rappoport, 2007),2 Function word unigrams and bigrams (Malmasi et al., 2013), Lemma and Word n-grams, CFG Production Rules (Wong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat character n-grams as lexical features in this work"
W15-0606,D11-1148,0,0.441435,"nd Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classification in our experiments. We experiment with a wide range of previously used syntactic and lexical features: Adaptor Grammars (AG) (Wong et al., 2012), character n-grams (Tsur and Rappoport, 2007),2 Function word unigrams and bigrams (Malmasi et al., 2013), Lemma and Word n-grams, CFG Production Rules (Wong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat character n-grams as lexical features in this work but restrict our investigation to 1–3-grams. Recent work has also shown improvements from longer sequences (Jarvis et al., 2013"
W15-0606,D12-1064,0,0.159994,"e. Results from previous studies show that while some feature types yield similar accuracies independently, combining them can improve performance (Brooke and Hirst, Data and Methodology We use the T OEFL11 corpus (Blanchard et al., 2013) released with the 2013 NLI shared task (Tetreault et al., 2013). It includes 12,100 learner texts from 11 L1 groups, divided into train, dev. and test sets. We use a linear Support Vector Machine1 to perform multi-class classification in our experiments. We experiment with a wide range of previously used syntactic and lexical features: Adaptor Grammars (AG) (Wong et al., 2012), character n-grams (Tsur and Rappoport, 2007),2 Function word unigrams and bigrams (Malmasi et al., 2013), Lemma and Word n-grams, CFG Production Rules (Wong and Dras, 2011), Penn Treebank (PTB) part-of-speech n-grams, RASP part-ofspeech n-grams (Malmasi et al., 2013), Stanford Dependencies with POS transformations (Tetreault et al., 2012), and Tree Substitution Grammar (TSG) fragments (Swanson and Charniak, 2012). The individual feature accuracies3 are shown in Figure 1.4 1 We use LIBLINEAR. Additional preliminary experiments with alternative learners yielded similar results. 2 We treat char"
W15-0615,Q13-1032,0,0.124574,"d on the assumption that the classifier is best informed by responses that are as different as possible (i.e. in the words used). In the second approach, we simulate letting annotators score whole clusters with a label that is used for all instances in this cluster. The main advantage of this method is that it yields multiple training instances with just one decision from the annotator. At the same time, judging whole clusters – especially if they are large – is more difficult than judging a single response, so we need to take this into consideration when comparing the results. 2 Related Work Basu et al. (2013) describe a related study on Powergrading, an approach for computer-assisted scoring of short-answer questions. They carry out experiments using crowd-sourced responses to questions from the US citizenship test. The end goal of that 124 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 124–132, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics Figure 1: Comparison of the classical supervised approach with clustering approach where a subset of instances is selected for manual annotation. work is the clust"
W15-0615,P14-5011,1,0.871661,"Missing"
W15-0615,horbach-etal-2014-finding,0,0.375529,"ain combination of tokens appears in the document, but also makes sure they are in the same dependency relation. Table 2: List of features tem on a certain item led to decreased performance on the others. For our experiments consistency is more important than especially good baseline results, and so we choose to run the same system on all ten items rather than developing ten separate systems that require individual tuning. Impact of Training Data Size The main question that we are exploring in this paper is whether some answers are more valuable for training than others (Lewis and Gale, 1994; Horbach et al., 2014). By carefully selecting the training instances, we should be able to train a model with performance comparable to the full model that uses less training data and thus is cheaper to create. In order to assess the potential of this approach, it will be useful to compare against the upper and lower bound in performance. For this purpose, we need to find the best and worst subset of graded answers. As the number of possible combinations of k instances from n answers is much too high to search in its entirety, we test 1,000 random samples while making sure that all outcome classes are found in the"
W15-0619,N13-1055,1,0.848424,"on P1 may be incorrect. 3. Detailed Feedback 1. The incorrect preposition is highlighted and the feedback message is of the form: The highlighted preposition P1 may be incorrect; the preposition P2 may be more appropriate, where P2 is a human expert’s suggested correction for the error. 4. Detailed Feedback 2. The incorrect preposition is highlighted and the feedback message is of the form: The highlighted preposition P1 may be incorrect; the preposition P2 may be more appropriate, where P2 is the correction assigned the highest probability by an automated preposition error correction system (Cahill et al., 2013). 5. Detailed Feedback 3. The incorrect preposition is highlighted and the feedback message is of the form: The highlighted preposition P1 may be incorrect; the following is a list of prepositions that may be more appropriate, where the list contains the top 5 suggested corrections from the automated error correction system. For all three detailed feedback types, Turkers were told that the corrections were generated by an automated system. Table 1 shows the design of our experimental study wherein all recruited Turkers were divided into five mutually exclusive groups, each corresponding to one"
W15-0619,C10-2103,0,0.580405,"n the quite restricted question of the extent to which error correction influences writing accuracy for L2-English students. This study concluded that overt error correction actually has a small negative influence on learners’ abilities to write accurately. However, the meta-analysis was based on only six research studies, making it somewhat difficult to be confident about the generalizability of the findings.” Biber et al. (2011) also mention that “In actual practice, direct feedback is rarely used as a treatment in empirical research.” The work most directly relevant to our study is that of Nagata and Nakatani (2010), who attempt to measure actual impact of feedback on learning outcomes for English language learners whose native language is Japanese. At the beginning of the study, students wrote English essays on 10 different topics. Errors involving articles and noun number were then flagged either by a human or by two different automatic error detection systems: one with high precision and another with high recall. A control group received no error feedback. Learning was measured in terms of reduction of error rate for the noun phrases in the students’ essays. Results showed that learning was quite simi"
W15-0619,P11-1019,0,0.0336793,"completed Session 1, we were able to use our own qualifications and a range of qualification scores to assign Turkers to groups and control the order in which they completed the sessions. Although the Turkers were assigned randomly to groups, we manually ensured that the distributions of Session 1 scores were similar across groups. 165 amount increased by 50 cents for each new session, adding up to a total of $10 per Turker if they completed all five sessions. Table 2 shows the number of Turkers assigned to each group who participated in each of the five sessions. We used the CLC-FCE corpus (Yannakoudakis et al., 2011), which has been manually annotated for preposition errors by professional English language instructors. We randomly selected 90 sentences with preposition errors and 45 sentences without errors and manually reviewed them to ensure their suitability. Unsuitable sentences were replaced from the pool of automatically extracted sentences until we had a total of 135 suitable sentences. We annotated each sentence containing an error with a correct preposition. The 135 sentences were then randomly divided into 5 HITs (Human Intelligence Tasks, the basic unit of work on MTurk), one for each of the fi"
W15-1616,W14-6106,1,0.740825,"d. The second issue is that often, due to the errors, a traditional linguistic analysis of learner text is not possible or appropriate. One way of looking at the problem of training statistical NLP tools for learner texts is that learner text is of a different domain to the domain for which the NLP tools were designed. Many unsupervised approaches to domain adaptation have been proposed in the literature, which may be applicable in this scenario. Self-training (McClosky et al., 2006) is one very common and straightforward technique for improving NLP tool performance on text from a new domain. Cahill et al. (2014) showed that it was possible to improve the performance of a baseline constituency parser on learner text by applying selftraining. Another approach to adapting NLP tools to learner text is to train them directly on annotated data. The SALLE project (Syntactically Annotating Learner Language of English) at Indiana University is working towards developing a set of guidelines for annotating syntactic properties (in the form of dependencies) of texts written by learners of English (Ragheb and Dickinson, 2012; Ragheb and Dickinson, 2014). Their goal is to provide accurate syntactic dependency anal"
W15-1616,W09-2112,0,0.258986,"Missing"
W15-1616,P14-2029,1,0.721067,"University is working towards developing a set of guidelines for annotating syntactic properties (in the form of dependencies) of texts written by learners of English (Ragheb and Dickinson, 2012; Ragheb and Dickinson, 2014). Their goal is to provide accurate syntactic dependency analyses for learner text given the morphological realizations of tokens, and they do not attempt to connect directly to the intended meanings. They plan to release a manually annotated dataset, and are also planning to work on bootstrapping approaches to semi-automatically annotate data. 3 Parsing and Grammaticality Heilman et al. (2014) argue that grammaticality judgments for sentences should be made on an ordinal scale rather than the binary scale that is often used when talking about grammaticality. They propose a four-point scale where 1 is incomprehensible and 4 is native-sounding.1 Viewing grammaticality in this way, it is likely that the performance of a syntactic parser will be more or less impacted by the 1 Non-word spelling errors are ignored in that scheme. 145 severity of the grammatical error. In order to briefly test whether different error types impact syntactic parsing to different degrees, we carry out a prel"
W15-1616,N06-1020,0,0.0506759,"most always find some analysis. Depending on the kinds of grammatical errors in the learner text, this analysis can be seriously flawed. The second issue is that often, due to the errors, a traditional linguistic analysis of learner text is not possible or appropriate. One way of looking at the problem of training statistical NLP tools for learner texts is that learner text is of a different domain to the domain for which the NLP tools were designed. Many unsupervised approaches to domain adaptation have been proposed in the literature, which may be applicable in this scenario. Self-training (McClosky et al., 2006) is one very common and straightforward technique for improving NLP tool performance on text from a new domain. Cahill et al. (2014) showed that it was possible to improve the performance of a baseline constituency parser on learner text by applying selftraining. Another approach to adapting NLP tools to learner text is to train them directly on annotated data. The SALLE project (Syntactically Annotating Learner Language of English) at Indiana University is working towards developing a set of guidelines for annotating syntactic properties (in the form of dependencies) of texts written by learn"
W15-1616,C12-2094,0,0.0940272,"mon and straightforward technique for improving NLP tool performance on text from a new domain. Cahill et al. (2014) showed that it was possible to improve the performance of a baseline constituency parser on learner text by applying selftraining. Another approach to adapting NLP tools to learner text is to train them directly on annotated data. The SALLE project (Syntactically Annotating Learner Language of English) at Indiana University is working towards developing a set of guidelines for annotating syntactic properties (in the form of dependencies) of texts written by learners of English (Ragheb and Dickinson, 2012; Ragheb and Dickinson, 2014). Their goal is to provide accurate syntactic dependency analyses for learner text given the morphological realizations of tokens, and they do not attempt to connect directly to the intended meanings. They plan to release a manually annotated dataset, and are also planning to work on bootstrapping approaches to semi-automatically annotate data. 3 Parsing and Grammaticality Heilman et al. (2014) argue that grammaticality judgments for sentences should be made on an ordinal scale rather than the binary scale that is often used when talking about grammaticality. They"
W15-1616,W07-2460,0,0.377904,"Missing"
W15-1616,roark-etal-2006-sparseval,0,0.520903,"keting Precision Recall F-Score 90.23 89.82 90.03 89.73 89.24 89.48 89.52 89.39 89.45 82.10 79.94 81.01 75.49 74.65 75.07 71.63 71.41 71.52 73.68 68.49 70.99 trees. Unfortunately, SParseval does not take the alignment into account when computing dependency scores and so we are unable to report those scores for our experiments at this time.3 4 Table 1: The effect on parser performance on ungrammatical text as measured by labeled constituents. the original text as well as each modified version of section 23 with ZPar (Zhang and Clark, 2011). We evaluate the output of the parser using SParseval (Roark et al., 2006). This is necessary because the tokens in the gold standard are no longer necessarily in the parser output and standard evaluation software such as evalb cannot be applied. The labeled bracket constituency results are given in Table 1. The results show a large difference in parser performance across the 6 error types. Confusing singular and plural nouns, or confusing the form of the verb lead to only very minor changes in overall constituency structure compared to parsing the original text by Zpar. This is in some ways not that unexpected, since these kinds of errors (at least in the manner th"
W15-1616,J11-1005,0,\N,Missing
W16-0501,W13-1703,0,0.0958381,"analysis also depends on the quality of the NUCLE annotations. When correcting ungrammatical text, annotators are faced with the decisions of whether a text needs to be corrected and, if so, how to edit it. Previous work has found low interannotator agreement for the basic task of judging whether a sentence is grammatical (0.16 ≤ κ ≤ 0.40) (Rozovskaya and Roth, 2010). The NUCLE corpus is no different, with the three NUCLE annotators having moderate agreement on how to correct a span of text (κ = 0.48) and only fair agreement for identifying what span of text needs to be corrected (κ = 0.39) (Dahlmeier et al., 2013). Low inter-annotator agreement is not necessarily an indication of the quality of the annotations, since it could 4 Using the EnglishGrammaticalStructure class with the flags -nonCollapsed -keepPunct. 3 also be attributed to the diversity of appropriate corrections that have been made. We assume that the annotations are correct and complete, meaning that the spans and labels of annotations are correct and that all of the grammatical errors are annotated. We further assume that the annotations only fix grammatical errors, instead of providing a stylistic alternatives to grammatical text. 3 Met"
W16-0501,de-marneffe-etal-2014-universal,0,0.0410261,"Missing"
W16-0501,P08-2056,0,0.0742464,"Missing"
W16-0501,W11-2925,0,0.0638227,"Missing"
W16-0501,foster-2004-parsing,0,0.0548065,"ases likely reflect a structural change in the dependency graph of the ungrammatical sentence, which affects more relations than those containing the ungrammatical tokens. 6 Related work There is a modest body of work focused on improving parser performance of ungrammatical sentences. 7 Unlike our experiments, most previous work has used small (around 1,000 sentences) or artificially generated corpora of ungrammatical/grammatical sentence pairs. The most closely related works compared the structure of constituent parses of ungrammatical to corrected sentences: with naturally occurring errors, Foster (2004) and Kaljahi et al. (2015) and evaluate parses of ungrammatical text based on the constituent parse and Geertzen et al. (2013) evaluate performance over dependencies. Cahill (2015) examines the parser performance using artificially generated errors, and Foster (2007) analyzes the parses of both natural and artificial errors. In Wagner and Foster (2009), the authors compared the parse probabilities of naturally occurring and artificially generated ungrammatical sentences to the probabilities of the corrected sentences. They found that the natural ungrammatical sentences had a lower reduction in"
W16-0501,N10-1060,0,0.0256916,"l ungrammatical sentences had a lower reduction in parse probability than artificial sentences, suggesting that artificial errors are not interchangeable with spontaneous errors. This analysis suggests the importance of using naturally occurring errors, which is why we chose to generate sentences from the spontaneous NUCLE errors. Several studies have attempted to improve the accuracy of parsing ungrammatical text. Some approaches include self-training (Foster et al., 2011; Cahill et al., 2014), retraining (Foster et al., 2008), and transforming the input and training text to be more similar (Foster, 2010). Other work with ungrammatical learner text includes Caines and Buttery (2014), which identifies the need to improve parsing of spoken learner English, and Tetreault et al. (2010), which analyzes the accuracy of prepositional phrase attachment in the presence of preposition errors. 7 Conclusion and future work The performance of NLP tools over ungrammatical text is little understood. Given the expense of annotating a grammatical-error corpus, previous studies have used either small annotated corpora or generated artificial grammatical errors in clean text. This study represents the first larg"
W16-0501,D15-1157,0,0.0314948,"Missing"
W16-0501,P08-1021,0,0.0254338,"Missing"
W16-0501,W04-2407,0,0.0381016,"Missing"
W16-0501,W07-2460,0,0.0701427,"Missing"
W16-0501,roark-etal-2006-sparseval,0,0.0431963,"s the accuracy of the dependency triples from the candidate dependency graph with respect to those of the gold standard, where each triple represents one relation, consisting of the head, dependent, and type of relation. The LAS assumes that the surface forms of the sentences are identical but only the relations have changed. In this work, we require a method that accommodates unaligned tokens, which occur when an error involves deleting or inserting tokens and unequal surface forms (replacement errors). There are some metrics that compare the parses of unequal sentences, including SParseval (Roark et al., 2006) and TEDeval (Tsarfaty et al., 2011), however neither of these metrics operate over dependencies. We chose to evaluate dependencies because dependency-based evaluation has been shown to be more closely related to the linguistic intuition of good parses compared to two other tree-based evaluations (Rehbein and van Genabith, 2007). Since we cannot calculate the LAS over sentences of unequal lengths, we instead measure the F1 -score of the dependency relations. So that substitutions (such as morphological changes) are not severely penalized, we represent tokens with their index instead of the sur"
W16-0501,W10-1004,0,0.123171,"he Stanford Dependency Parser4 and the universal dependencies representation (De Marneffe et al., 2014). We make the over-confident assumption that the automatic analyses in our pipeline (tokenization, parsing, and error-type labeling) are all correct. Our analysis also depends on the quality of the NUCLE annotations. When correcting ungrammatical text, annotators are faced with the decisions of whether a text needs to be corrected and, if so, how to edit it. Previous work has found low interannotator agreement for the basic task of judging whether a sentence is grammatical (0.16 ≤ κ ≤ 0.40) (Rozovskaya and Roth, 2010). The NUCLE corpus is no different, with the three NUCLE annotators having moderate agreement on how to correct a span of text (κ = 0.48) and only fair agreement for identifying what span of text needs to be corrected (κ = 0.39) (Dahlmeier et al., 2013). Low inter-annotator agreement is not necessarily an indication of the quality of the annotations, since it could 4 Using the EnglishGrammaticalStructure class with the flags -nonCollapsed -keepPunct. 3 also be attributed to the diversity of appropriate corrections that have been made. We assume that the annotations are correct and complete, me"
W16-0501,P10-2065,0,0.0339927,"errors. This analysis suggests the importance of using naturally occurring errors, which is why we chose to generate sentences from the spontaneous NUCLE errors. Several studies have attempted to improve the accuracy of parsing ungrammatical text. Some approaches include self-training (Foster et al., 2011; Cahill et al., 2014), retraining (Foster et al., 2008), and transforming the input and training text to be more similar (Foster, 2010). Other work with ungrammatical learner text includes Caines and Buttery (2014), which identifies the need to improve parsing of spoken learner English, and Tetreault et al. (2010), which analyzes the accuracy of prepositional phrase attachment in the presence of preposition errors. 7 Conclusion and future work The performance of NLP tools over ungrammatical text is little understood. Given the expense of annotating a grammatical-error corpus, previous studies have used either small annotated corpora or generated artificial grammatical errors in clean text. This study represents the first large-scale analysis of the effect of grammatical errors on a NLP task. We have used a large, annotated corpus of grammatical errors to generate more than 44,000 sentences with up to f"
W16-0501,D11-1036,0,0.0309765,"triples from the candidate dependency graph with respect to those of the gold standard, where each triple represents one relation, consisting of the head, dependent, and type of relation. The LAS assumes that the surface forms of the sentences are identical but only the relations have changed. In this work, we require a method that accommodates unaligned tokens, which occur when an error involves deleting or inserting tokens and unequal surface forms (replacement errors). There are some metrics that compare the parses of unequal sentences, including SParseval (Roark et al., 2006) and TEDeval (Tsarfaty et al., 2011), however neither of these metrics operate over dependencies. We chose to evaluate dependencies because dependency-based evaluation has been shown to be more closely related to the linguistic intuition of good parses compared to two other tree-based evaluations (Rehbein and van Genabith, 2007). Since we cannot calculate the LAS over sentences of unequal lengths, we instead measure the F1 -score of the dependency relations. So that substitutions (such as morphological changes) are not severely penalized, we represent tokens with their index instead of the surface form. First, we align the token"
W16-0501,W09-3827,0,0.0140517,"ll (around 1,000 sentences) or artificially generated corpora of ungrammatical/grammatical sentence pairs. The most closely related works compared the structure of constituent parses of ungrammatical to corrected sentences: with naturally occurring errors, Foster (2004) and Kaljahi et al. (2015) and evaluate parses of ungrammatical text based on the constituent parse and Geertzen et al. (2013) evaluate performance over dependencies. Cahill (2015) examines the parser performance using artificially generated errors, and Foster (2007) analyzes the parses of both natural and artificial errors. In Wagner and Foster (2009), the authors compared the parse probabilities of naturally occurring and artificially generated ungrammatical sentences to the probabilities of the corrected sentences. They found that the natural ungrammatical sentences had a lower reduction in parse probability than artificial sentences, suggesting that artificial errors are not interchangeable with spontaneous errors. This analysis suggests the importance of using naturally occurring errors, which is why we chose to generate sentences from the spontaneous NUCLE errors. Several studies have attempted to improve the accuracy of parsing ungra"
W16-0501,P11-1019,0,0.0492129,"is is relies on the idiosyncrasies of this particular corpus, such as the typical sentence length and complexity. The essays were written by students at the National University of Singapore, who do not have a wide variety of native languages. The types and frequency of errors differ depending on the native language of the student (Rozovskaya and Roth, 2010), which may bias the analysis herein. The available corpora that contain a broader representation of native languages are much smaller than the NUCLE corpus: the Cambridge Learner Corpus–First Certificate in English has 420 thousand tokens (Yannakoudakis et al., 2011), and the corpus annotated by (Rozovskaya and Roth, 2010) contains only 63 thousand words. One limitation to our method for generating ungrammatical sentences is that relatively few sentences are the source of ungrammatical sentences with four errors. Even though we drew sentences from a large corpus, only 570 sentences had at least four errors (of the types we were considering), compared to 14,500 sentences with at least one error. Future work examining the effect of multiple errors 8 would need to consider a more diverse set of sentences with more instances of at least four errors, since the"
W16-0501,J11-1005,0,0.0284901,"tly n errors. with n = 1 to 4 errors, when there were at least n corrections to the original sentence. For example, a NUCLE sentence with 6 annotated corrections would yield the following number of ungrammatical  sentences: 6 sentences with one error, 62 = 15  sentences with two errors, 63 = 20 sentences with three errors, and so on. The number of original NUCLE sentences and generated sentences with each number of errors is shown in Table 1. We also generated a grammatical sentence with all of the corrections applied for comparison. We parsed each sentence with the ZPar constituent parser (Zhang and Clark, 2011) and generated dependency parses from the ZPar output using the Stanford Dependency Parser4 and the universal dependencies representation (De Marneffe et al., 2014). We make the over-confident assumption that the automatic analyses in our pipeline (tokenization, parsing, and error-type labeling) are all correct. Our analysis also depends on the quality of the NUCLE annotations. When correcting ungrammatical text, annotators are faced with the decisions of whether a text needs to be corrected and, if so, how to edit it. Previous work has found low interannotator agreement for the basic task of"
W16-0501,W14-6107,0,\N,Missing
W16-0501,W14-6106,1,\N,Missing
W16-0501,E14-3013,0,\N,Missing
W16-0501,W15-1616,1,\N,Missing
W16-0514,W11-1405,0,0.0308704,"features can be affected by a number of factors. First of all, many grammatical features rely on knowledge of sentence boundaries in order to parse the response into syntactic constituents. In written responses the sentence boundaries can be established 130 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 130–135, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics based on punctuation. In spoken responses, however, these have to be estimated using machine learning algorithms such as the ones described in Chen and Yoon (2011). Furthermore, sentence boundaries in speech are often ambiguous. These factors may lead to a decrease in feature performance. Second, in automated speech scoring the transcription of the spoken responses necessary to evaluate grammar and vocabulary is obtained using automated speech recognition (ASR) (Higgins et al., 2011). These systems may incorrectly recognize certain words introducing additional noise into the feature input and consequently lowering their performance. Finally, spoken and written discourse differ in what is considered appropriate in terms of language use (Chafe and Tannen,"
W16-0514,P11-1073,0,0.13936,"Missing"
W16-0514,W14-1802,0,0.179275,"Missing"
W16-0514,S13-2046,0,0.0763208,"prepositions and collocations (Futagi et al., 2008), and (f) sentence variety. To train a new scoring model, features are extracted from a training data set, and a linear model (roughly equivalent to non-negative least squares regression) is learned. 2.2 c-rater-ML (C) is an automated scoring engine originally designed to evaluate the content of a student response. It is typically applied to short responses ranging from a few words to a short paragraph. Therefore, in contrast to E and S many of the features used in the C engine are sparse lexicalized features similar to the ones described in Heilman and Madnani (2013). In addition to word and character n-gram features, the models also include syntactic dependency features. As a result of the large number of sparse features, the modeling technique for this kind of feature set needs to be different from a straightforward linear model. C employs a Support Vector Regressor with a radial basis function kernel. We use this alternative approach to scoring (with many sparse lexical features and a nonlinear learning function) to contrast with the typical scoring models used for evaluating speech or writing quality. 2.3 Automated scoring systems R 2.1 e-rater R e-ra"
W16-0514,N12-1011,0,0.37446,"Missing"
W16-0514,W12-2021,0,0.605655,"g automated speech recognition technology. The ASR engine incorporated into S was trained on over 800 hours of non-native speech from the same assessment used in this study with no speaker overlap. The ASR system uses a GMM-based crossword triphone acoustic model and a 4-gram language model with a vocabulary size of 65,000 words. The S model used in this study contained 18 features. Of these, 15 features covered various aspects of delivery such as fluency, pronunciation and rhythm. Three features measured language use. These feature were: (1) average log of the frequency of all content words (Yoon et al., 2012), (2) CVAbased comparison between the lexical content of each response and the reference corpus (based on Xie et al. (2012)) and (3) a CVA-based comparison computed based on part-of-speech tags (Bhat and Yoon, 2015). As in case of E, the final score is computed as a linear combination of these features. 3 3.1 Data and methodology Corpus of spoken responses The study is based on a corpus of 5,884 spoken responses to an English language proficiency test obtained from 996 speakers. The corpus contains up to six responses from each speaker. Each response was unscripted and around 1 minute long. Th"
W16-0515,N13-1055,1,0.856085,"lts with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach. 1 Introduction In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best. ∗ Michael Heilman is now a data scientist at Civis Analytics. Much of"
W16-0515,C12-1038,0,0.0157331,"e, we compute precision, recall, and F1 score for each dataset. Precision is the percentage of system corrections that are correct according to the gold standard, and recall is the percentage of the gold standard corrections that were correctly marked by the system. Our evaluation metric can be viewed as similar to a micro-averaged F1 score for a multi-class document classification task where documents are the original prepositions, classes are the possible corrections, and only documents for ungrammatical prepositions have class labels. Our F1 score is similar to the WAS evaluation scheme of Chodorow et al. (2012), except that we treat cases where the original preposition, system prediction, and gold standard all differ as false negatives. Chodorow et al. (2012) instead treat such cases as both false positives and false negatives, and as a result, the sum of true positives, false positives, true negatives, and false negatives does not equal the number of examples. 3 Methods This section describes our implementations of the classifier, language modeling, and system combina137 3.1 3.2 Classifier Language Model Our second system uses a language modeling approach. We use KenLM (Heafield, 2011) to estimate"
W16-0515,P11-1092,0,0.0184016,"result: a classifier approach yielded results with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach. 1 Introduction In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best. ∗ Michael Heilman is now a dat"
W16-0515,W11-2838,0,0.0292933,"Missing"
W16-0515,W12-2006,0,0.030744,"Missing"
W16-0515,1993.eamt-1.1,0,0.511941,"best F1 score and that it 139 Dataset FCE HOO System Classifier LM Heuristic Interpolation Ensemble Classifier LM Heuristic Interpolation Ensemble P 65.63 28.42 30.91 50.67 51.72 59.26 12.90 21.24 34.29 32.93 R 17.77 30.39 37.26 36.30 38.35 19.75 14.81 29.63 29.63 33.33 F1 27.97 29.37 33.79 42.30 44.04 29.63 13.79 24.74 31.79 33.13 Sig. ∗ ∗ ∗ ∗ ∗ ∗ Table 1: P, R and F1 scores for the FCE and HOO test sets. Bold indicates the best result for each metric. “∗” indicates that the F1 score for a system was significantly different (p < .05) from that of the ensemble system as per the BCa Bootstrap (Efron and Tibshirani, 1993) test with 10,000 replications. performs significantly better than all other approaches, including both the classifier and the language model. 3. For the HOO test set — which is quite different from the FCE data in both its genre (ACL papers) and the distribution of grammatical errors — the ensemble approach still attains the best performance and is significantly better than the language model and the heuristic system combination approach. Finally, we also compared the ensemble approach to a current state-of-the-art preposition error correction system. To do this, we evaluated on the CoNLL 201"
W16-0515,N10-1019,0,0.154274,"In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best. ∗ Michael Heilman is now a data scientist at Civis Analytics. Much of the previous work has used well-formed text when training contextual classifiers due to the lack of large error-annotated corpora. Han et al. (2010) conducted experiments with a relatively small error-annotated corpus and showed that it outperfor"
W16-0515,W11-2123,0,0.0204373,"me of Chodorow et al. (2012), except that we treat cases where the original preposition, system prediction, and gold standard all differ as false negatives. Chodorow et al. (2012) instead treat such cases as both false positives and false negatives, and as a result, the sum of true positives, false positives, true negatives, and false negatives does not equal the number of examples. 3 Methods This section describes our implementations of the classifier, language modeling, and system combina137 3.1 3.2 Classifier Language Model Our second system uses a language modeling approach. We use KenLM (Heafield, 2011) to estimate an unpruned model for n = 6 with modified KneserNey smoothing (Chen and Goodman, 1998) on the text of all articles contained in a snapshot of English Wikipedia from June 2012 (68,356,743 sentences). We use this n-gram language model to obtain scores (f (w,s,i)) gLM (w, s, i) = log10 pLM , where w is the |s|+1 preposition to be scored, s is the writer’s original sentence, i is the position of the original preposition in s, f is a function that returns a variant of s with the preposition at i replaced with w, and pLM returns the probability for a sentence. We divide the language mod"
W16-0515,N10-1018,0,0.0325042,", we report a more nuanced result: a classifier approach yielded results with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach. 1 Introduction In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best. ∗ Mich"
W16-0515,P11-1093,0,0.318922,"as studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best. ∗ Michael Heilman is now a data scientist at Civis Analytics. Much of the previous work has used well-formed text when training contextual classifiers due to the lack of large error-annotated corpora. Han et al. (2010) conducted experiments with a relatively small error-annotated corpus and showed that it outperformed a contextual classifier trained on well-edited text. More recently, Cahill et al. (2013) mined Wikipedia revisions to produce a large, publicly availabl"
W16-0515,P12-2064,0,0.0150656,"roach yielded results with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach. 1 Introduction In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear which approach is best. ∗ Michael Heilman is now a data scientist at Civ"
W16-0515,D14-1102,0,0.0183523,"th respect to the second question, Gamon (2010) previously reported that a combination of a contextual classifier trained on well-edited text and a language modeling approach outperformed each individual method. However, given that the performance of his classifier was lower than what has been reported on other datasets (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2011), we believe it is worth reinvestigating the merits of system combination but with publicly available data sets and with a classifier trained on error-annotated data instead of on well-edited text. This work differs from Susanto et al. (2014) in that we are interested in combining statistical models in order to more accurately correct individual preposition errors, while their work 136 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 136–141, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics combined — at the sentence level — the outputs of multiple systems designed to correct different types of grammatical errors. tion approaches to preposition error correction. 2 Our first system is a classifier trained on errorannotated data, following Ca"
W16-0515,C08-1109,0,0.253018,"h (2011) reported that classifiers outperformed a language modeling approach. Here, we report a more nuanced result: a classifier approach yielded results with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach. 1 Introduction In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeli"
W16-0515,P10-2065,0,0.0288913,"modeling approach. Here, we report a more nuanced result: a classifier approach yielded results with higher precision while a language modeling approach provided better recall. Most importantly, we found that a combined approach using a logistic regression ensemble outperformed both a classifier and a language modeling approach. 1 Introduction In this paper, we compare methods for correcting grammatical errors. Much of the previous work on grammatical error detection and correction has studied methods based on statistical classifiers (Tetreault and Chodorow, 2008; De Felice and Pulman, 2009; Tetreault et al., 2010; Rozovskaya and Roth, 2010; Dahlmeier and Ng, 2011; Seo et al., 2012; Cahill et al., 2013). In particular, Tetreault and Chodorow (2008) and Tetreault et al. (2010) found that classifiers based on a variety of contextual linguistic features performed well, especially in terms of precision (i.e., avoiding false positives). Gamon (2010), however, reported that a language modeling approach substantially outperformed a classifier using contextual features. Finally, Rozovskaya and Roth (2011) found that a classifier outperformed a language modeling approach on different data, making it unclear whi"
W16-0515,P11-1019,0,0.0850561,"Missing"
W16-0524,S13-2045,0,0.124319,"Missing"
W16-0524,P11-1076,0,0.152534,"17 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 217–222, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics Figure 1: A sample question from the music teaching proficiency test. Note that only a part of the entire music sample included with the question is shown here. ered similar in spirit to some of the previous work on short-answer scoring, where the focus is on scoring content-driven responses to math, biology, or computer science questions (Sukkarieh and Stoyanchev, 2009; Sukkarieh et al., 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 However, we claim that there is very little in that body of previous work that focuses on responses exhibiting all of the following characteristics: • The responses we examine are written by a population of adults that are generally proficient English writers. This is different from most previous work where the population is generally composed of middle- and high-school students with varying levels of English proficiency. • The average length of these responses is approximately 220 words which is m"
W16-0524,W15-0612,0,0.0952505,"tive Use of NLP for Building Educational Applications, pages 217–222, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics Figure 1: A sample question from the music teaching proficiency test. Note that only a part of the entire music sample included with the question is shown here. ered similar in spirit to some of the previous work on short-answer scoring, where the focus is on scoring content-driven responses to math, biology, or computer science questions (Sukkarieh and Stoyanchev, 2009; Sukkarieh et al., 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 However, we claim that there is very little in that body of previous work that focuses on responses exhibiting all of the following characteristics: • The responses we examine are written by a population of adults that are generally proficient English writers. This is different from most previous work where the population is generally composed of middle- and high-school students with varying levels of English proficiency. • The average length of these responses is approximately 220 words which is much longer than the responses considered in much of"
W16-0524,N15-1111,1,0.813781,"g Educational Applications, pages 217–222, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics Figure 1: A sample question from the music teaching proficiency test. Note that only a part of the entire music sample included with the question is shown here. ered similar in spirit to some of the previous work on short-answer scoring, where the focus is on scoring content-driven responses to math, biology, or computer science questions (Sukkarieh and Stoyanchev, 2009; Sukkarieh et al., 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 However, we claim that there is very little in that body of previous work that focuses on responses exhibiting all of the following characteristics: • The responses we examine are written by a population of adults that are generally proficient English writers. This is different from most previous work where the population is generally composed of middle- and high-school students with varying levels of English proficiency. • The average length of these responses is approximately 220 words which is much longer than the responses considered in much of the previous work (10-1"
W16-0524,W09-2509,0,0.0333287,"experts are chosen randomly from a pool of 9 experts. 217 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 217–222, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics Figure 1: A sample question from the music teaching proficiency test. Note that only a part of the entire music sample included with the question is shown here. ered similar in spirit to some of the previous work on short-answer scoring, where the focus is on scoring content-driven responses to math, biology, or computer science questions (Sukkarieh and Stoyanchev, 2009; Sukkarieh et al., 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 However, we claim that there is very little in that body of previous work that focuses on responses exhibiting all of the following characteristics: • The responses we examine are written by a population of adults that are generally proficient English writers. This is different from most previous work where the population is generally composed of middle- and high-school students with varying levels of English proficiency. • The average length of these re"
W16-0524,J11-1005,0,\N,Missing
W17-1605,W99-0411,1,0.465897,"the fairness of test scores as measured by the impact of construct-irrelevant factors. As Xi (2010) discusses in detail, unfair decisions based on scores assigned to test-takers from oft-disadvantaged 42 their own biases (Feldman et al., 2015) either due to an existing bias in the training data or due to a minority group being inadequately represented in the training data. Automated scoring is certainly not immune to such biases and, in fact, several studies have documented differing performance of automated scoring models for test-takers with different native languages or with disabilities (Burstein and Chodorow, 1999; Bridgeman et al., 2012; Wang and von Davier, 2014; Wang et al., 2016; An et al., 2016; Loukina and Buzick, In print). groups are likely to have profound consequences: they may be denied career opportunities and access to resources that they deserve. Therefore, it is important to ensure — among other things — that construct-irrelevant factors do not introduce systematic biases in test scores, irrespective of whether they are produced by human raters or by an automated scoring system. Over the last few years, there has been a significant amount of work done on ensuring fairness, accountability"
W17-1605,P98-1032,1,0.678341,"Missing"
W17-1605,W15-0610,1,0.829664,"en linear models with a small number of interpretable features because such models lend themselves more easily to a detailed fairness review and allow decisionmakers to understand how, and to what extent, different parts of the test-takers’ skill set are being covered by the features in the model (Loukina et al., 2015). For such linear models, RSMTool displays a detailed model description including the model fit (R2 ) computed on the training set as well as the contribution of each feature to the final score (via raw, standardized, and relative coefficients). At the same time, recent studies (Heilman and Madnani, 2015; Madnani et al., 2016) on scoring actual content rather than just language proficiency suggest that it is possible to achieve higher performance, as measured by agreement with human raters, by employing many low-level features and more sophisticated machine learning algorithms such as support vector machines or random forests. Generally, these models are built using sparse feature types such as word n-grams, often resulting in hundreds of thousands of predominantly binary features. Using models with such a large feature space means that it is no longer clear how to map the individual features"
W17-1605,D13-1180,0,0.0217575,"automated scoring model and a set of psychometric and statistical analyses aimed at detecting possible bias in 43 cus solely on the fairness-driven evaluation capabilities of the tool that are directly relevant to the issues we have discussed so far. Readers interested in other parts of the RSMToolare referred to the comprehensive documentation available at http://rsmtool.readthedocs.org. Before we describe the fairness analyses implemented in the tool, we want to acknowledge that there are many different ways in which researchers might approach building as well as evaluating scoring models (Chen and He, 2013; Shermis, 2014a). The list of learners and fairness analyses the tool provides is not, and cannot be, exhaustive. In fact, later in the paper, we discuss some analyses that could be implemented in future versions of the tool since one of the core characteristics of the tool is its flexible architecture. See §4.4 for more details. In the next section, we present in detail the analyses incorporated into RSMTool aimed at detecting the various sources of biases we introduced earlier. As it is easier to show the analyses in the context of an actual example, we use data from the Hewlett Foundation"
W17-1605,W15-0602,1,0.886546,"ws users to integrate various components of RSMTool into their own applications. 4.5 The automated scoring models used in systems such as e-rater for assessing writing proficiency in English (Attali and Burstein, 2006) or SpeechRater for spoken proficiency (Zechner et al., 2009) have traditionally been linear models with a small number of interpretable features because such models lend themselves more easily to a detailed fairness review and allow decisionmakers to understand how, and to what extent, different parts of the test-takers’ skill set are being covered by the features in the model (Loukina et al., 2015). For such linear models, RSMTool displays a detailed model description including the model fit (R2 ) computed on the training set as well as the contribution of each feature to the final score (via raw, standardized, and relative coefficients). At the same time, recent studies (Heilman and Madnani, 2015; Madnani et al., 2016) on scoring actual content rather than just language proficiency suggest that it is possible to achieve higher performance, as measured by agreement with human raters, by employing many low-level features and more sophisticated machine learning algorithms such as support"
W17-1605,C14-1090,1,0.844936,"view the features and ensure that their description and method of computation are in line with the definition of the specific set of skills that the given test purports to measure (Deane, 2013). However, features incorporated into a modern automated scoring system often rely on multiple underlying NLP components such as part-of-speech taggers and syntactic parsers as well as complex computational algorithms and, therefore, a qualitative review may not be sufficient. Furthermore, some aspects of spoken or written text can only be measured indirectly given the current state of NLP technologies (Somasundaran et al., 2014). RSMTool allows the user to explore the quantitative effect of two types of construct-irrelevant factors that may affect feature performance: categorical and continuous. 4.1.1 4.1.2 Continuous Factors This type of construct-irrelevant factors includes continuous covariates which despite being correlated with human scores are either not directly relevant to the construct measured by the test or, even if they are, should not be the primary contributor to the model’s predictions. Response length, as previously discussed, is an example of such covariates. Even though it provides an important indi"
W17-1605,W16-0524,1,0.808858,"ll number of interpretable features because such models lend themselves more easily to a detailed fairness review and allow decisionmakers to understand how, and to what extent, different parts of the test-takers’ skill set are being covered by the features in the model (Loukina et al., 2015). For such linear models, RSMTool displays a detailed model description including the model fit (R2 ) computed on the training set as well as the contribution of each feature to the final score (via raw, standardized, and relative coefficients). At the same time, recent studies (Heilman and Madnani, 2015; Madnani et al., 2016) on scoring actual content rather than just language proficiency suggest that it is possible to achieve higher performance, as measured by agreement with human raters, by employing many low-level features and more sophisticated machine learning algorithms such as support vector machines or random forests. Generally, these models are built using sparse feature types such as word n-grams, often resulting in hundreds of thousands of predominantly binary features. Using models with such a large feature space means that it is no longer clear how to map the individual features and their weights to v"
W17-1605,W15-0625,0,0.053079,"easurement guidelines currently implemented in RSMTool follow the psychometric framework suggested by Williamson et al. (2012). It was developed for the evaluation of e-rater, an automated system designed to score English writing proficiency (Attali and Burstein, 2006), but is generalizable to other applications of automated scoring. This framework was chosen because it offers a comprehensive set of criteria for both the accuracy as well as the fairness of the predicted scores. Note that not all of these recommendations are universally accepted by the automated scoring community. For example, Yannakoudakis and Cummins (2015) recently proposed a different set of metrics for evaluating the accuracy of automated scoring models. Furthermore, the machine learning community has recently developed various analyses aimed at detecting bias in algorithm performance that could be applied in the context of automated scoring. For example, in addition to reviewing individual features, one could also attempt to predict the subgroup membership from the features used to score the responses (Feldman et al., 2015). If this Figure 2: The performance of our scoring model (R2 ) for different subgroups of test-takers as defined by thei"
W17-1605,C98-1032,1,\N,Missing
W17-4609,N09-1050,0,0.435319,"ters, length bin 7 (blog2 151c) would be the binary feature that gets a value of 1 with the other two bins getting the value of 0. The second set of features (“quality”) captures how much the pronunciation of individual segments deviates from the pronunciation that would be expected from a proficient speaker. This includes the average confidence scores and acoustic model scores computed by the ASR system for the words in the 1-best ASR hypothesis. Since the ASR is trained on a wide range of proficiency levels, we also include features computed using the two-pass approach (Herron et al., 1999; Chen et al., 2009). In this approach, the acoustic model scores for words in the ASR hypothesis are recomputed using acoustic models trained on native We refer to these features as “text-driven” features in subsequent sections. 2 Speech-driven features See Table 3 in Burrows et al. (2015) for a detailed list. 70 speakers of English. The third set of features captures pausing patterns in the response such as mean duration of pauses, mean number of words between two pauses, and the ratio of pauses to speech. For all features in this group the pauses were determined based on silences in the ASR output. Only silenc"
W17-4609,N15-1111,1,0.937468,"ave some of the highest correlations with the overall human score. Text-driven features Scoring responses for writing quality requires measuring whether the student can organize and develop an argument and write fluently with no grammatical errors or misspellings. In contrast, scoring for content deals with responses to openended questions designed to test what the student knows, has learned, or can do in a specific subject area (such as Computer Science, Math, or Biology) (Sukkarieh and Stoyanchev, 2009; Sukkarieh, 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 In order to measure the content of the spoken responses in our data, we extract the following set of features from the 1-best ASR hypotheses for each response: Name speech rate quality pausing timing • lowercased word n-grams (n=1,2), including punctuation prosody • lowercased character n-grams (n=2,3,4,5) • syntactic dependency triples computed using the ZPar parser (Zhang and Clark, 2011) Description Speech rate Segmental quality Location and duration of pauses Patterns of durations of individual segments Time intervals between stressed syllables Nf eat 3 6 9 r .42 .41"
W17-4609,S13-2045,0,0.0218939,"nguage proficiency and the features in this group have some of the highest correlations with the overall human score. Text-driven features Scoring responses for writing quality requires measuring whether the student can organize and develop an argument and write fluently with no grammatical errors or misspellings. In contrast, scoring for content deals with responses to openended questions designed to test what the student knows, has learned, or can do in a specific subject area (such as Computer Science, Math, or Biology) (Sukkarieh and Stoyanchev, 2009; Sukkarieh, 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 In order to measure the content of the spoken responses in our data, we extract the following set of features from the 1-best ASR hypotheses for each response: Name speech rate quality pausing timing • lowercased word n-grams (n=1,2), including punctuation prosody • lowercased character n-grams (n=2,3,4,5) • syntactic dependency triples computed using the ZPar parser (Zhang and Clark, 2011) Description Speech rate Segmental quality Location and duration of pauses Patterns of durations of individual segments Time intervals"
W17-4609,W15-0605,0,0.0163845,"ent) & speech-driven features (for proficiency) will perform better than the individual text-only and speech-only models. Although it may seem obvious that, given the nature of the task, a model using both speechbased and content-based features should outperform models using only one of them, it may not turn out that way. Multiple studies that have developed new features measuring vocabulary, grammar or content for spoken responses have reported only limited improvements when these features were combined with features based on fluency and pronunciation (Bhat and Yoon, 2015; Yoon et al., 2012; Somasundaran et al., 2015). Crossley and McNamara (2013) used a large set of text-based measures including Coh-Metrix (Graesser et al., 2004) to obtain fairly accurate predictions of proficiency scores for spoken responses to general questions similar to the ones used in this study based on transcription only, without using any information based on acoustic analysis of speech. It is not possible to establish from published results how their system would compare to the one that also evaluates pronunciation and fluency. They did not compute any such features and their results based on text are not directly comparable to"
W17-4609,W09-2509,0,0.0139613,"has been consistently identified as one of the major covariates of language proficiency and the features in this group have some of the highest correlations with the overall human score. Text-driven features Scoring responses for writing quality requires measuring whether the student can organize and develop an argument and write fluently with no grammatical errors or misspellings. In contrast, scoring for content deals with responses to openended questions designed to test what the student knows, has learned, or can do in a specific subject area (such as Computer Science, Math, or Biology) (Sukkarieh and Stoyanchev, 2009; Sukkarieh, 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 In order to measure the content of the spoken responses in our data, we extract the following set of features from the 1-best ASR hypotheses for each response: Name speech rate quality pausing timing • lowercased word n-grams (n=1,2), including punctuation prosody • lowercased character n-grams (n=2,3,4,5) • syntactic dependency triples computed using the ZPar parser (Zhang and Clark, 2011) Description Speech rate Segmental quality Location and duration of paus"
W17-4609,W16-0514,1,0.524425,"xt-driven features was significantly better than both the text-only model as well as the speech-only model. The effect size of the improvement over the text-only model was small with the average R2 increasing only slightly from .335 to .352 for source-based questions and from .431 to .442 for general questions (p = 0.002). Model text + speech text-only speech-only Xie et al. Loukina & Cahill general source-based .60 .67 .59 .66 .58 .63 .40 .59 .64 (overall) Table 4: Average Pearson’s r achieved by the three of the models in this study and the best performing models reported in the literature; Loukina and Cahill (2016) combine language proficiency features from speech and text and do not report performance by question type; Xie et al. (2012) use content features based on cosine similarity but no other language proficiency features. If a paper reports results based on both ASR hypothesis and human transcription, we only use the results based on ASR hypothesis. 2. The performance of the text-only model was significantly better than the performance of each of the 5 models trained using only one group of speech-driven features (p &lt; 0.0001). 3. There was no significant difference between the performance of the t"
W17-4609,P11-1076,0,0.0132585,"ajor covariates of language proficiency and the features in this group have some of the highest correlations with the overall human score. Text-driven features Scoring responses for writing quality requires measuring whether the student can organize and develop an argument and write fluently with no grammatical errors or misspellings. In contrast, scoring for content deals with responses to openended questions designed to test what the student knows, has learned, or can do in a specific subject area (such as Computer Science, Math, or Biology) (Sukkarieh and Stoyanchev, 2009; Sukkarieh, 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 In order to measure the content of the spoken responses in our data, we extract the following set of features from the 1-best ASR hypotheses for each response: Name speech rate quality pausing timing • lowercased word n-grams (n=1,2), including punctuation prosody • lowercased character n-grams (n=2,3,4,5) • syntactic dependency triples computed using the ZPar parser (Zhang and Clark, 2011) Description Speech rate Segmental quality Location and duration of pauses Patterns of durations of individual"
W17-4609,N12-1011,0,0.0358578,"provement over the text-only model was small with the average R2 increasing only slightly from .335 to .352 for source-based questions and from .431 to .442 for general questions (p = 0.002). Model text + speech text-only speech-only Xie et al. Loukina & Cahill general source-based .60 .67 .59 .66 .58 .63 .40 .59 .64 (overall) Table 4: Average Pearson’s r achieved by the three of the models in this study and the best performing models reported in the literature; Loukina and Cahill (2016) combine language proficiency features from speech and text and do not report performance by question type; Xie et al. (2012) use content features based on cosine similarity but no other language proficiency features. If a paper reports results based on both ASR hypothesis and human transcription, we only use the results based on ASR hypothesis. 2. The performance of the text-only model was significantly better than the performance of each of the 5 models trained using only one group of speech-driven features (p &lt; 0.0001). 3. There was no significant difference between the performance of the text-only model and the 5 models combining the text-driven features with each of the individual speechdriven feature sets. 4.2"
W17-4609,W12-2021,0,0.219222,"features (for content) & speech-driven features (for proficiency) will perform better than the individual text-only and speech-only models. Although it may seem obvious that, given the nature of the task, a model using both speechbased and content-based features should outperform models using only one of them, it may not turn out that way. Multiple studies that have developed new features measuring vocabulary, grammar or content for spoken responses have reported only limited improvements when these features were combined with features based on fluency and pronunciation (Bhat and Yoon, 2015; Yoon et al., 2012; Somasundaran et al., 2015). Crossley and McNamara (2013) used a large set of text-based measures including Coh-Metrix (Graesser et al., 2004) to obtain fairly accurate predictions of proficiency scores for spoken responses to general questions similar to the ones used in this study based on transcription only, without using any information based on acoustic analysis of speech. It is not possible to establish from published results how their system would compare to the one that also evaluates pronunciation and fluency. They did not compute any such features and their results based on text are"
W17-4609,W15-0612,0,0.0223131,"he features in this group have some of the highest correlations with the overall human score. Text-driven features Scoring responses for writing quality requires measuring whether the student can organize and develop an argument and write fluently with no grammatical errors or misspellings. In contrast, scoring for content deals with responses to openended questions designed to test what the student knows, has learned, or can do in a specific subject area (such as Computer Science, Math, or Biology) (Sukkarieh and Stoyanchev, 2009; Sukkarieh, 2011; Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Sakaguchi et al., 2015; Zhu et al., 2016).2 In order to measure the content of the spoken responses in our data, we extract the following set of features from the 1-best ASR hypotheses for each response: Name speech rate quality pausing timing • lowercased word n-grams (n=1,2), including punctuation prosody • lowercased character n-grams (n=2,3,4,5) • syntactic dependency triples computed using the ZPar parser (Zhang and Clark, 2011) Description Speech rate Segmental quality Location and duration of pauses Patterns of durations of individual segments Time intervals between stressed syllables"
W17-4609,J11-1005,0,0.0609992,"Missing"
W17-5007,C14-1185,0,0.251184,"Missing"
W17-5007,W17-5023,0,0.0297696,"iple kernel learning. WLZ (Li and Zou, 2017) build an ensemble of single-feature SVMs fed into a multi-layer perceptron (MLP), which is a meta-classifier trained on the outputs of the base SVM classifiers. The single features are based on lexical and syntactic information and the best submission includes character, word, stem, and function word n-grams as well as syntactic dependencies. Essay-only Track The best essay-only submission for each team, along with rankings and other details, are listed in Table 4.1. Each team’s best system is briefly described below, ordered by rankings. Uvic-NLP (Chan et al., 2017) trained a single SVM model on word n-grams (1–3) and character n-grams (4-5). They also conducted several postevaluation experiments, improving their results to 0.8730 using an LDA meta-classifier trained on individual SVM classifiers. ItaliaNLP Lab (Cimino and Dell’Orletta, 2017) utilize a novel classifier stacking approach based on a sentence-level classifier whose predictions are used by a second document-level classifier. The sentence classifier is based on a Logistic Regression model trained on standard lexical, stylistic, and syntactic NLI features. The documentclassifier is an SVM, tra"
W17-5007,W17-5049,0,0.19025,"Missing"
W17-5007,W17-5041,0,0.0444426,"Missing"
W17-5007,W17-5024,0,0.0496381,"trained on word bigrams and character 7-grams. They tried a variety of n-gram combinations and found this to work best on the development data. A total of 19 teams participated in the task, 17 of which submitted system description papers. Participation across the three tracks varied, with 17 participants in the essay-only track, 9 in the speech-only track, and 10 in the fusion track. The results for each track are described in the following sections. For every track we briefly outline each team’s best system. Interested readers can refer to the team’s paper for more details. 4.1 UnibucKernel (Ionescu and Popescu, 2017) use different types of character-level string kernels which are combined with multiple kernel learning. WLZ (Li and Zou, 2017) build an ensemble of single-feature SVMs fed into a multi-layer perceptron (MLP), which is a meta-classifier trained on the outputs of the base SVM classifiers. The single features are based on lexical and syntactic information and the best submission includes character, word, stem, and function word n-grams as well as syntactic dependencies. Essay-only Track The best essay-only submission for each team, along with rankings and other details, are listed in Table 4.1."
W17-5007,W17-5025,0,0.0281986,"Missing"
W17-5007,D14-1142,1,0.743654,"Missing"
W17-5007,W17-5021,0,0.0649744,"Missing"
W17-5007,W13-1714,0,0.641856,"Missing"
W17-5007,P17-1134,1,0.867612,"NLI works by identifying language use patterns that are common to certain groups of speakers that share the same native language. This process is underpinned by the presupposition that an author’s linguistic background will dispose them towards particular language production patterns in their learned languages, as influenced by their mother tongue. Predicting the native language of a writer has applications in different fields. It can be used for authorship identification (Estival et al., 2007), forensic analysis (Gibbons, 2003), tracing linguistic influence in potentially multi-author texts (Malmasi et al., 2017), and naturally to support Second Language Acquisition research (Malmasi and Dras, 2014). It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and"
W17-5007,W15-0620,1,0.926713,"Missing"
W17-5007,W17-5048,0,0.0366318,"aseline: essay/transcript/i-vector 0.7901 0.7909 SVM trained on word unigrams (essay/transcript) + i-vectors Baseline: Essay + Transcript 0.7786 0.7791 Linear SVM trained on word unigrams (essays + transcripts) 4 ut.dsp 0.7748 0.7764 n-gram language models over chars/words (essay+transcript) 5 ltl 0.7346 0.7345 No paper submitted. 0.0910 0.0910 Randomly select an L1 Random Baseline Table 3: Official results in the fusion track. The official metric is the macro-averaged F1-score. Accuracy (Acc.) is also reported. Team rankings are determined by statistical significance testing (see §3.1). L2F (Kepler et al., 2017) designed a system that combined three types of text-based classifiers (an RNN with a bidirectional GRU layer, a Naive Bayes classifier with byte n-grams, and a Naive Bayes classifier with n-grams based on representations of the words using Byte Pair Encoding) with versions of the i-vector features that were postprocessed using centering and whitening in an attempt to reduce channel variability. These classifiers were combined together in a Neural Network fusion approach and the authors demonstrated that the i-vector features were the main driver of performance. ETRI-SLP (Oh et al., 2017) subm"
W17-5007,W16-4801,1,0.85993,"Missing"
W17-5007,W17-5043,0,0.116863,"Missing"
W17-5007,W17-5042,0,0.105,"labels. Their experiments indicate that inclusion of the sentence prediction features provides a small increase in performance. ETRI-SLP (Oh et al., 2017) designed a system that was based on word n-gram features (with n ranging from 1 to 3) and character n-gram features (with n ranging from 4 to 6). The normalized count vectors based on these features were used to extract LSA features, which were then reduced using LDA. The count and LSA-LDA features were used to train SVM and DNN classifiers whose outputs were subsequently combined via late fusion in a DNN-based ensemble classifier. CIC-FBK (Markov et al., 2017) build an SVM with multiple lexical and syntactic features. They introduce two new feature types – typed character n-grams and syntactic n-grams – and combine them with word, lemma, and POS n-grams, function words, and spelling error character n-grams. Features are weighted using log-entropy. CEMI (Ircing et al., 2017) use a Logistic Regression meta-classifier to achieve their best essay-only results. The meta-classifier is trained on the outputs of several base classifiers, which are trained on TF-IDF weighted word unigrams, word bigrams, character n-grams and POS n-grams. Groningen (Kulmizev"
W17-5007,W17-5044,0,0.0282337,"t data. A total of 19 teams participated in the task, 17 of which submitted system description papers. Participation across the three tracks varied, with 17 participants in the essay-only track, 9 in the speech-only track, and 10 in the fusion track. The results for each track are described in the following sections. For every track we briefly outline each team’s best system. Interested readers can refer to the team’s paper for more details. 4.1 UnibucKernel (Ionescu and Popescu, 2017) use different types of character-level string kernels which are combined with multiple kernel learning. WLZ (Li and Zou, 2017) build an ensemble of single-feature SVMs fed into a multi-layer perceptron (MLP), which is a meta-classifier trained on the outputs of the base SVM classifiers. The single features are based on lexical and syntactic information and the best submission includes character, word, stem, and function word n-grams as well as syntactic dependencies. Essay-only Track The best essay-only submission for each team, along with rankings and other details, are listed in Table 4.1. Each team’s best system is briefly described below, ordered by rankings. Uvic-NLP (Chan et al., 2017) trained a single SVM mode"
W17-5007,W17-5022,0,0.0430648,"). GadjahMada (Sari et al., 2017) apply a character embedding model with a feed-forward neural network classifier in the essay track. This is based on the relatively high performance of character ngrams in previous research. An embedding size of 25 was used with n-grams of length 2–5. NLI-ISU (Vajjala and Banerjee, 2017) explored the use of n-grams and embeddings in their submissions. Their best run was a Logistic Regression model trained on word 1-3 grams. They also report that spell checking features, as well as word and document embeddings did not work well on the development data. ut.dsp (Mohammadi et al., 2017) utilize n-gram language models over words and characters. For each L1, a language model over character 3- and 4-grams as well as word unigrams and bigrams is calculated and smoothing is applied. For each text in the test set, the probably of the whole text for all language models in each class is calculated and the class with the maximum probability is chosen as the predicted label. This approach does not involve any supervised learning. ¨ IUCL (Smiley and Kubler, 2017) investigated the use of phonetic features for the essay classification task based on the hypothesis that speakers from diffe"
W17-5007,W15-0606,1,0.926679,"vidence from various participants suggests that high-order character n-grams (as high as n = 10) are extremely useful for this task. This is likely because when extracted across word boundaries, these features capture not only sub-word (e.g. morphological) information, but also dependencies between words. However, it should also be noted that the top systems in all tracks made use of syntactic features which can give them a slight performance boost. This is not surprising as it has been shown that lexical and syntactic features each capture diverse types of information that are complementary (Malmasi and Cahill, 2015). Average performance is much higher than 2013. Although much of the training data remains the same, the submissions were much more competitive than the first NLI shared tasks. This is likely due to NLI being a much more established task, as well as the aforementioned prevalence of more sophisticated models such as metaclassifiers. A number of open questions remain. For example, it is not clear if any one approach is dominant across all tracks as most of the top-ranked teams in the essay track did not participate in the other tracks. It is hard to say how well their systems would have done in"
W17-5007,W17-5047,0,0.0363074,"ed on individual SVM classifiers. ItaliaNLP Lab (Cimino and Dell’Orletta, 2017) utilize a novel classifier stacking approach based on a sentence-level classifier whose predictions are used by a second document-level classifier. The sentence classifier is based on a Logistic Regression model trained on standard lexical, stylistic, and syntactic NLI features. The documentclassifier is an SVM, trained using the same features, as well as the sentence prediction labels. Their experiments indicate that inclusion of the sentence prediction features provides a small increase in performance. ETRI-SLP (Oh et al., 2017) designed a system that was based on word n-gram features (with n ranging from 1 to 3) and character n-gram features (with n ranging from 4 to 6). The normalized count vectors based on these features were used to extract LSA features, which were then reduced using LDA. The count and LSA-LDA features were used to train SVM and DNN classifiers whose outputs were subsequently combined via late fusion in a DNN-based ensemble classifier. CIC-FBK (Markov et al., 2017) build an SVM with multiple lexical and syntactic features. They introduce two new feature types – typed character n-grams and syntact"
W17-5007,D14-1144,1,0.925479,"Missing"
W17-5007,N01-1031,0,0.692118,"lly or via Automatic Speech Recognition) and audio features for dialect identification (Malmasi et al., 2016), a task that involves identifying specific dialects of pluricentric languages, such as Spanish or Arabic.1 The combination of transcripts and acoustic features has also provided good results for dialect identification (Zampieri et al., 2017b), demonstrating that it is possible to improve performance by combining this information. While there has been growing interest in using such features, the use of speech transcripts for NLI is not entirely new. In fact, the very first NLI study by Tomokiyo and Jones (2001) was based on applying a Naive Bayes classifier to transcriptions of speech from native and non-native speakers, albeit using limited data. However, this strand of NLI research has not received much attention, most likely due to the costly and laborious nature of collecting and transcribing non-native speech. Following this trend, the 2016 Computational Paralinguistics Challenge (Schuller et al., 2016) also included an NLI task based on the spoken response using the raw audio. The NLI Shared Task 2017 attempts to combine these approaches by including a written response (essay) and a spoken res"
W17-5007,W17-5028,0,0.0402701,"Missing"
W17-5007,W07-0602,0,0.514904,"author texts (Malmasi et al., 2017), and naturally to support Second Language Acquisition research (Malmasi and Dras, 2014). It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and Dras (2009), and Tetreault et al. (2012) set the stage for much of the recent research efforts. However, it was the 2013 Native Language Identification Shared Task (Tetreault et al., 2013) that led to an explosion of interest in this area by making public a large dataset developed specifically Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their language production in a learned language. It is typically framed as a classification task where the set of L1s is known a priori. Two previous shared tasks on"
W17-5007,W17-5026,0,0.110995,"N classifier 0.7104 0.7109 Linear SVM trained on word unigrams 0.0910 0.0910 Randomly select an L1 Table 1: Official results in the essay-only track. The official metric is the macro-averaged F1-score. Accuracy (Acc.) is also reported. Rankings are determined by statistical significance testing (see §3.1). GadjahMada (Sari et al., 2017) apply a character embedding model with a feed-forward neural network classifier in the essay track. This is based on the relatively high performance of character ngrams in previous research. An embedding size of 25 was used with n-grams of length 2–5. NLI-ISU (Vajjala and Banerjee, 2017) explored the use of n-grams and embeddings in their submissions. Their best run was a Logistic Regression model trained on word 1-3 grams. They also report that spell checking features, as well as word and document embeddings did not work well on the development data. ut.dsp (Mohammadi et al., 2017) utilize n-gram language models over words and characters. For each L1, a language model over character 3- and 4-grams as well as word unigrams and bigrams is calculated and smoothing is applied. For each text in the test set, the probably of the whole text for all language models in each class is"
W17-5007,P11-1093,0,0.0862778,"s, as influenced by their mother tongue. Predicting the native language of a writer has applications in different fields. It can be used for authorship identification (Estival et al., 2007), forensic analysis (Gibbons, 2003), tracing linguistic influence in potentially multi-author texts (Malmasi et al., 2017), and naturally to support Second Language Acquisition research (Malmasi and Dras, 2014). It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and Dras (2009), and Tetreault et al. (2012) set the stage for much of the recent research efforts. However, it was the 2013 Native Language Identification Shared Task (Tetreault et al., 2013) that led to an explosion of interest in this area by making public a large dataset developed specifically Native Language Iden"
W17-5007,U09-1008,0,0.367747,"l., 2017), and naturally to support Second Language Acquisition research (Malmasi and Dras, 2014). It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and Dras (2009), and Tetreault et al. (2012) set the stage for much of the recent research efforts. However, it was the 2013 Native Language Identification Shared Task (Tetreault et al., 2013) that led to an explosion of interest in this area by making public a large dataset developed specifically Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their language production in a learned language. It is typically framed as a classification task where the set of L1s is known a priori. Two previous shared tasks on NLI have been organiz"
W17-5007,W17-5027,0,0.03595,"els over characters (3-4) and words (1-2) Word Unigram Baseline Random Baseline 0.8318 0.8264 0.8264 0.8110 Ensemble of resnets, LSTM and document embeddings Logistic Regression model with word n-grams (1-3) Phonetic features combined in an SVM Char embeddings w/ a feed-forward NN classifier 0.7104 0.7109 Linear SVM trained on word unigrams 0.0910 0.0910 Randomly select an L1 Table 1: Official results in the essay-only track. The official metric is the macro-averaged F1-score. Accuracy (Acc.) is also reported. Rankings are determined by statistical significance testing (see §3.1). GadjahMada (Sari et al., 2017) apply a character embedding model with a feed-forward neural network classifier in the essay track. This is based on the relatively high performance of character ngrams in previous research. An embedding size of 25 was used with n-grams of length 2–5. NLI-ISU (Vajjala and Banerjee, 2017) explored the use of n-grams and embeddings in their submissions. Their best run was a Logistic Regression model trained on word 1-3 grams. They also report that spell checking features, as well as word and document embeddings did not work well on the development data. ut.dsp (Mohammadi et al., 2017) utilize n"
W17-5007,W17-5045,0,0.0520433,"nd the transcriptions, and a second SVM combining the unigrams from the essays and the transcriptions with the i-vectors. The test period for each track lasted 3 days, and teams could submit up to 12 systems per track. The essay-only and speech-only test phases ran concurrently. The IDs for the essay data and transcription data were generated by separate random processes for this test period. For the fusion test period, an updated package providing linked IDs between the essay and spoken transcription data was released. 2 3 http://kaldi-asr.org 65 For more details see §7.3 of Malmasi and Dras (2017) 4 Results tubasfs (Rama and C ¸ o¨ ltekin, 2017) used a single SVM classifier trained on word bigrams and character 7-grams. They tried a variety of n-gram combinations and found this to work best on the development data. A total of 19 teams participated in the task, 17 of which submitted system description papers. Participation across the three tracks varied, with 17 participants in the essay-only track, 9 in the speech-only track, and 10 in the fusion track. The results for each track are described in the following sections. For every track we briefly outline each team’s best system. Intere"
W17-5007,W17-1201,1,0.784529,"Missing"
W17-5007,W17-5046,0,0.0229943,"Missing"
W17-5007,W13-1706,1,0.61284,"cal error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and Dras (2009), and Tetreault et al. (2012) set the stage for much of the recent research efforts. However, it was the 2013 Native Language Identification Shared Task (Tetreault et al., 2013) that led to an explosion of interest in this area by making public a large dataset developed specifically Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their language production in a learned language. It is typically framed as a classification task where the set of L1s is known a priori. Two previous shared tasks on NLI have been organized where the aim was to identify the L1 of learners of English based on essays (2013) and spoken responses (2016) they provided during a standardized assessment of academic Engl"
W17-5007,C12-1158,1,0.924147,"Missing"
W17-5017,Q13-1032,0,0.169019,"stigate how several basic neural approaches similar to those used for automated essay scoring perform on short answer scoring. We show that neural architectures can outperform a strong nonneural baseline, but performance and optimal parameter settings vary across the more diverse types of prompts typical of short answer scoring. 1 1. Response length. Responses in SAS tasks are typically shorter. For example, while the ASAP-AES data contains essays that average between about 100 and 600 tokens (Shermis, 2014), short answer scoring datasets may have average answer lengths of just several words (Basu et al., 2013) to almost 60 words (Shermis, 2015). 2. Rubrics focus on content only in SAS vs. broader writing quality in AES. 3. Purpose and genre. AES tasks cover persuasive, narrative, and source-dependent reading comprehension and English Language Arts (ELA), while SAS tasks tend to be from science, math, and ELA reading comprehension. Introduction Deep neural network approaches have recently been successfully developed for several educational applications, including automated essay assessment. In several cases, neural network approaches exceeded the previous state of the art on essay scoring (Taghipour"
W17-5017,P14-5011,1,0.788102,".4) 522 140 449 (9.4) 540 (4.0) 48.4 3.9 9.8 12.5 0/1/2(/3) 0/1 2 or 5-way 2 or 5-way Table 1: Overview of the datasets used in this work. Since we train prompt-specific models for ASAPSAS and PG, we report the mean number of responses per set per prompt. For SRA, we train one model per label set across prompts and report the overall number of prompts per set as well as the mean number of responses per prompt per set (in parentheses). 3 Experiments 3.1 3.2 Method Baseline As a baseline system, we use a supervised learner based on a hand-crafted feature set. This baseline is based on DkPro TC (Daxenberger et al., 2014) and relies on support vector classification using Weka (Hall et al., 2009). We preprocess the data using the ClearNlp Segmenter 2 via DKPro Core (Eckart de Castilho and Gurevych, 2014). The features used in the baseline system comprise a commonly used and effective feature set for the SAS task. We use both binary word and character uni- to trigram occurrence features, using the top 10,000 most frequent ngrams in the training data, as well as answer length, measured by the number of tokens in a response. We carried out a series of experiments across datasets to discern the effect of specific p"
W17-5017,P16-1068,0,0.0802884,"ammar and spelling (Burstein et al., 2013). Short answer scoring, by contrast, typically focuses only on the accuracy Given these differences, the feature sets for AES and SAS systems are often different, with AES incorporating a larger set of features to capture writing quality (Shermis and Hamner, 2013). Nevertheless, deep learning approaches to AES have thus far demonstrated strong performance with minimal inputs consisting of unigrams and word embeddings. For example, Taghipour and Ng (2016) explore simple LSTM and CNN-based architectures with regression and evaluate on the ASAP-AES data. Alikaniotis et al. (2016) train score-specific word embeddings with several LSTM architectures. Dong and Zhang (2016) demonstrate that a hierarchical CNN architecture produces 159 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 159–168 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics strong results on the ASAP-AES data. Recently, Zhao et al. (2017) show state-of-the-art performance on the ASAP-AES dataset with a memory network architecture. In this work, we investigate whether deep neural network approaches with similarly mi"
W17-5017,D16-1115,0,0.0157986,"only on the accuracy Given these differences, the feature sets for AES and SAS systems are often different, with AES incorporating a larger set of features to capture writing quality (Shermis and Hamner, 2013). Nevertheless, deep learning approaches to AES have thus far demonstrated strong performance with minimal inputs consisting of unigrams and word embeddings. For example, Taghipour and Ng (2016) explore simple LSTM and CNN-based architectures with regression and evaluate on the ASAP-AES data. Alikaniotis et al. (2016) train score-specific word embeddings with several LSTM architectures. Dong and Zhang (2016) demonstrate that a hierarchical CNN architecture produces 159 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 159–168 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics strong results on the ASAP-AES data. Recently, Zhao et al. (2017) show state-of-the-art performance on the ASAP-AES dataset with a memory network architecture. In this work, we investigate whether deep neural network approaches with similarly minimal feature sets can produce good performance on the SAS task, including whether they can"
W17-5017,C16-1206,0,0.319922,"Missing"
W17-5017,N12-1021,0,0.0173533,"), so that there are no state-ofthe-art scoring results available for this dataset. For simplicity, we use the first out of three binary human-annotated correctness scores. 2.3 The three datasets we use cover different kinds of prompts and vary considerably in the length of the answers as well as their well-formedness. Table 1 shows basic statistics for each dataset. Figures 1, 2 and 3 show examples for each of the datasets. ASAP-SAS The Automated Student Assessment Prize Short Answer Scoring (ASAP-SAS) dataset1 contains 10 individual prompts, covering science, biology, 1 SRA The SRA dataset (Dzikovska et al., 2012) became widely known as the dataset used in SemEval2013 Shared Task 7 “The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge” (Dzikovska et al., 2013). It consists of two subsets: Beetle, with student responses from interacting with a tutorial dialogue system, and SciEntsBank (SEB) with science assessment questions. We use two label sets from the shared task: the 2-way labels classify responses as correct or incorrect, while the 5-way labels provide a more fine-grained classification of responses into the categories non domain, correct, partially correct incomple"
W17-5017,D14-1162,0,0.0971468,"lopment sets and evaluated on the test set. We report prompt-level results for this model in Section 3.6. For evaluation, we use quadratic weighted kappa (QWK) for the ASAP-SAS and Powergrading datasets. Because the class labels in the SRA dataset are unordered, we report the weighted F1 score, which was the preferred metric in the Semeval shared task (Dzikovska et al., 2016). 2 https://github.com/clir/clearnlp The networks are implemented in Keras and use the Theano backend. 3 162 ters for our experiments. For pretrained embeddings, in preliminary experiments the GloVe 100 dimension vectors (Pennington et al., 2014) performed slightly better than a selection of other offthe-shelf embeddings, and hence we use these for all conditions that involve pretrained embeddings. Embeddings for word tokens that are not found in the embeddings are randomly initialized from a uniform distribution. The convolutional layer uses a window length of 3 or 5 and 50 filters. We use a mean squared error loss for regression models and a cross-entropy loss for classification models. To train the network, we use RMSProp with ρ set to 0.9 and learning rate of 0.001. We clip the norm of the gradient to 10. The fully connected layer"
W17-5017,W15-0612,0,0.0298341,"character- and n-gram based system can learn somewhat more efficiently than the neural systems. • SRA: Because of the decreased performance of the combined best individual parameters on the development data, we use a 300dimensional unidirectional LSTM with attention mechanism. These models are “T&N tuned” in Table 4, which appear along with the non-neural baseline system. On ASAP-SAS, the “T&N tuned” parameter configuration outperformed the baseline system and the “T&N best” parameters. The tuned system does not reach the state-of-the-art Fisher-transformed mean score on the ASAPSAS dataset (Ramachandran et al., 2015)7 , which, like the winner of the ASAP-SAS competition (Tandalla, 2012), employed prompt-specific regular expressions. Other top performing systems used prompt-specific preprocessing and ensemble-based approaches over rich feature spaces (Higgins et al., 2014). On the SRA datasets, the “T&N tuned” model outperformed the baseline and the “T&N best” settings on average across prompts, by a larger margin than the other datasets. On the SRA data, as on the ASAP-SAS data, a gap remains between the tuned model’s performance and the state of the art. On SRA, this may be partly due to the use of “ques"
W17-5017,W14-5201,0,0.0715088,"Missing"
W17-5017,S13-2046,0,0.132328,"Tandalla, 2012), employed prompt-specific regular expressions. Other top performing systems used prompt-specific preprocessing and ensemble-based approaches over rich feature spaces (Higgins et al., 2014). On the SRA datasets, the “T&N tuned” model outperformed the baseline and the “T&N best” settings on average across prompts, by a larger margin than the other datasets. On the SRA data, as on the ASAP-SAS data, a gap remains between the tuned model’s performance and the state of the art. On SRA, this may be partly due to the use of “question indicator” features by the top performing systems (Heilman and Madnani, 2013; Ott et al., 2013). The performance improvement over the baseline system was larger on the development sets than on the test sets. Part of the reason for this is that the test set evaluation procedure likely did not choose the best-performing epoch for the neural models. 7 Ramachandran et al. (2015) state that their mean QWK is 0.0053 higher than the Tandalla result, so in Table 4 we report that score truncated to 3 decimal places rather than the rounded result reported in Ramachandran et al. (2015). 165 SRA Beetle 2-way 5-way SRA SEB 2-way 5-way Mean wF1 Mean wF1 Mean wF1 Mean wF1 Experiment"
W17-5017,D16-1193,0,0.229407,"l., 2013) to almost 60 words (Shermis, 2015). 2. Rubrics focus on content only in SAS vs. broader writing quality in AES. 3. Purpose and genre. AES tasks cover persuasive, narrative, and source-dependent reading comprehension and English Language Arts (ELA), while SAS tasks tend to be from science, math, and ELA reading comprehension. Introduction Deep neural network approaches have recently been successfully developed for several educational applications, including automated essay assessment. In several cases, neural network approaches exceeded the previous state of the art on essay scoring (Taghipour and Ng, 2016). The task of automated essay scoring (AES) is generally different from the task of automated short answer scoring (SAS). Essay scoring generally focuses on writing quality, a multidimensional construct that includes ideas and elaboration, organization, style, and writing conventions such as grammar and spelling (Burstein et al., 2013). Short answer scoring, by contrast, typically focuses only on the accuracy Given these differences, the feature sets for AES and SAS systems are often different, with AES incorporating a larger set of features to capture writing quality (Shermis and Hamner, 2013"
W17-5017,W16-0535,1,0.79015,"Missing"
W17-5017,S13-2045,0,\N,Missing
W17-5017,S13-2102,0,\N,Missing
W17-5052,Q13-1032,0,0.0143644,"and did not investigate any questions about specific modeling strategies such as the choice of learning algorithm or the impact of hyper-parameter tuning. be useful in guiding further development of supervised content scoring models. 2 Related Work Content scoring is sometimes also referred to in the literature as “short-answer” scoring. Although it is true that many content-based questions tend to be very specific and elicit responses that are relatively short, this is not always the case. Previously published studies have considered responses that span a range of lengths — from a few words (Basu et al., 2013) to a few dozen words (Madnani et al., 2013; Horbach et al., 2013) to a few hundred words (Madnani et al., 2016). Given that the primary facet of interest is the content of the response and not its length, we refer to the task as “content scoring” in the rest of the paper. Content scoring approaches fall into two general categories: (a) reference-based where responses are scored on the basis of their similarity to reference answers provided by the authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Su"
W17-5052,D13-1180,0,0.0163748,"on457 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 457–467 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics Our study is different from the work we have discussed so far in that its goal is not simply to obtain the best performance for a given question or a set of questions. Instead, we focus on metaanalyses of scoring performance as a function of modeling strategies and data set characteristics. Some previous studies have considered the choice of learner in automated scoring for writing quality. Chen and He (2013) compared support vector classification, regression, and ranking for automatically scoring writing quality using a single dataset. Chen et al. (2016) reported that using support vector regression with a radial kernel produced better performance than a simple linear model. In addition, several studies (Feng et al., 2003; Haberman and Sinharay, 2010; Santos et al., 2012) have consistently reported that use of probabilistic classifiers such as cumulative logistic regression might be more appropriate for the task of automated scoring than linear regression since such models incorporate the assumpt"
W17-5052,S13-2045,0,0.0134709,"he authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number of continuous-valued features, often with a single model trained for multiple questions. (b) response-based which use a large number of detailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a separate model for each question. The choice of whether the reference-based approach is better than the response-based approach depends on the open-ended nature of the question and whether there is a sufficient number of human-scored responses available. Sakaguchi et al. (2015) — who explored the combination of the two approaches — observed that if sufficient human-scored data is available, response-based approaches often work better than reference-based approaches. Since"
W17-5052,W15-1905,0,0.0142042,"Horbach et al., 2013) to a few hundred words (Madnani et al., 2016). Given that the primary facet of interest is the content of the response and not its length, we refer to the task as “content scoring” in the rest of the paper. Content scoring approaches fall into two general categories: (a) reference-based where responses are scored on the basis of their similarity to reference answers provided by the authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number of continuous-valued features, often with a single model trained for multiple questions. (b) response-based which use a large number of detailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a separate model for each question. The choice of whether the reference-ba"
W17-5052,W15-0610,1,0.919218,"ber of continuous-valued features. More generally in the machine learning literature, papers have analyzed and compared the performance of different learning algorithms on standard machine learning datasets from the UCI repository and/or synthetic datasets (Caruana and Niculescu-Mizil, 2006; Matykiewicz and Pestian, 2012; Doan and Kalita, 2015). These studies reported substantial variability in learner performance across problems which suggests that the learners that performed best for other applications may not necessarily do so for our task. The work that might be closest to ours is that of Heilman and Madnani (2015) who explored the impact of the amount of training data available on content scoring performance across a range of questions. However, they used a much smaller set of 44 questions and did not investigate any questions about specific modeling strategies such as the choice of learning algorithm or the impact of hyper-parameter tuning. be useful in guiding further development of supervised content scoring models. 2 Related Work Content scoring is sometimes also referred to in the literature as “short-answer” scoring. Although it is true that many content-based questions tend to be very specific a"
W17-5052,S13-1041,0,0.0176789,"trategies such as the choice of learning algorithm or the impact of hyper-parameter tuning. be useful in guiding further development of supervised content scoring models. 2 Related Work Content scoring is sometimes also referred to in the literature as “short-answer” scoring. Although it is true that many content-based questions tend to be very specific and elicit responses that are relatively short, this is not always the case. Previously published studies have considered responses that span a range of lengths — from a few words (Basu et al., 2013) to a few dozen words (Madnani et al., 2013; Horbach et al., 2013) to a few hundred words (Madnani et al., 2016). Given that the primary facet of interest is the content of the response and not its length, we refer to the task as “content scoring” in the rest of the paper. Content scoring approaches fall into two general categories: (a) reference-based where responses are scored on the basis of their similarity to reference answers provided by the authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015)"
W17-5052,W15-0612,0,0.275744,"on or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number of continuous-valued features, often with a single model trained for multiple questions. (b) response-based which use a large number of detailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a separate model for each question. The choice of whether the reference-based approach is better than the response-based approach depends on the open-ended nature of the question and whether there is a sufficient number of human-scored responses available. Sakaguchi et al. (2015) — who explored the combination of the two approaches — observed that if sufficient human-scored data is available, response-based approaches often work better than reference-based approaches. Since several of the questions in"
W17-5052,W13-1722,1,0.927832,"Missing"
W17-5052,N15-1111,1,0.947411,"tailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a separate model for each question. The choice of whether the reference-based approach is better than the response-based approach depends on the open-ended nature of the question and whether there is a sufficient number of human-scored responses available. Sakaguchi et al. (2015) — who explored the combination of the two approaches — observed that if sufficient human-scored data is available, response-based approaches often work better than reference-based approaches. Since several of the questions in our dataset are relatively open-ended and we have sufficient scored data available for all of them, we focus on the response-based approach in this paper. 2.1 Research Questions We aim to answer the following specific questions about supervised learning specifically in the context of automated scoring of content: 1. What type of learner has the best performance for respo"
W17-5052,W16-0524,1,0.927767,"ithm or the impact of hyper-parameter tuning. be useful in guiding further development of supervised content scoring models. 2 Related Work Content scoring is sometimes also referred to in the literature as “short-answer” scoring. Although it is true that many content-based questions tend to be very specific and elicit responses that are relatively short, this is not always the case. Previously published studies have considered responses that span a range of lengths — from a few words (Basu et al., 2013) to a few dozen words (Madnani et al., 2013; Horbach et al., 2013) to a few hundred words (Madnani et al., 2016). Given that the primary facet of interest is the content of the response and not its length, we refer to the task as “content scoring” in the rest of the paper. Content scoring approaches fall into two general categories: (a) reference-based where responses are scored on the basis of their similarity to reference answers provided by the authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number o"
W17-5052,W12-2424,0,0.0289494,"e consistently reported that use of probabilistic classifiers such as cumulative logistic regression might be more appropriate for the task of automated scoring than linear regression since such models incorporate the assumption that the score is categorical in nature. All of these studies used a small number of continuous-valued features. More generally in the machine learning literature, papers have analyzed and compared the performance of different learning algorithms on standard machine learning datasets from the UCI repository and/or synthetic datasets (Caruana and Niculescu-Mizil, 2006; Matykiewicz and Pestian, 2012; Doan and Kalita, 2015). These studies reported substantial variability in learner performance across problems which suggests that the learners that performed best for other applications may not necessarily do so for our task. The work that might be closest to ours is that of Heilman and Madnani (2015) who explored the impact of the amount of training data available on content scoring performance across a range of questions. However, they used a much smaller set of 44 questions and did not investigate any questions about specific modeling strategies such as the choice of learning algorithm or"
W17-5052,W11-2401,0,0.0204622,"ords (Basu et al., 2013) to a few dozen words (Madnani et al., 2013; Horbach et al., 2013) to a few hundred words (Madnani et al., 2016). Given that the primary facet of interest is the content of the response and not its length, we refer to the task as “content scoring” in the rest of the paper. Content scoring approaches fall into two general categories: (a) reference-based where responses are scored on the basis of their similarity to reference answers provided by the authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number of continuous-valued features, often with a single model trained for multiple questions. (b) response-based which use a large number of detailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a se"
W17-5052,W15-0625,0,0.122365,"eal values on a continuous scale. One advantage of the real-valued scores is that they allow for more fine-grained distinction than a small set of integers. In this paper, we predict real-valued scores on a continuous scale and evaluate the accuracy of the predicted scores by using mean squared error (MSE) as our default metric. Although some previous studies have used quadratically-weighted kappa (QWK) as another possible metric for evaluating content-scoring models, more recent work has shown that QWK may possess properties that render it less than suitable for automated scoring evaluation (Yannakoudakis and Cummins, 2015). 3.1 been shown to perform well with feature sets comparable to ours in previously published work — Mohler et al. (2011), Sakaguchi et al. (2015), and Zesch et al. (2015) all used support vector machines; Ramachandran et al. (2015) use a random forest regressor — or (b) are generally known to perform well with a large number of sparse features (Hastie et al., 2001; Fan et al., 2008; Chang and Lin, 2011). We use the scikit-learn (Pedregosa et al., 2011) implementations for all learners. All the implementations incorporate some means of reducing learner variance either by design — random forest"
W17-5052,P11-1076,0,0.350434,"answers provided by the authors of the question or selected from existing high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number of continuous-valued features, often with a single model trained for multiple questions. (b) response-based which use a large number of detailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a separate model for each question. The choice of whether the reference-based approach is better than the response-based approach depends on the open-ended nature of the question and whether there is a sufficient number of human-scored responses available. Sakaguchi et al. (2015) — who explored the combination of the two approaches — observed that if sufficient human-scored data is available, response-based approaches often work better than reference-"
W17-5052,W15-0615,1,0.936647,"g high-scoring responses (Alfonseca and P´erez, 2004; Nielsen et al., 2008; Meurers et al., 2011; Sukkarieh et al., 2011; Horbach et al., 2013; Pado and Kiefer, 2015). These studies generally use a small number of continuous-valued features, often with a single model trained for multiple questions. (b) response-based which use a large number of detailed features extracted from the student responses themselves (e.g., word ngrams, etc.) and human scores assigned to the responses to learn a supervised machine-learning model (Mohler et al., 2011; Dzikovska et al., 2013; Ramachandran et al., 2015; Zesch et al., 2015; Zhu et al., 2016). Response-based approaches generally require training a separate model for each question. The choice of whether the reference-based approach is better than the response-based approach depends on the open-ended nature of the question and whether there is a sufficient number of human-scored responses available. Sakaguchi et al. (2015) — who explored the combination of the two approaches — observed that if sufficient human-scored data is available, response-based approaches often work better than reference-based approaches. Since several of the questions in our dataset are rel"
W17-5052,J11-1005,0,0.0219845,"in responses to questions on a given subject. Madnani et al., 2016). We extract the following features for all of the responses in our corpus: scoring? Do non-linear learners offer a substantial advantage over linear models? Do margin-based methods such as support vector machines perform better than bagging ensembles like random forests? (a) character n-grams including whitespace and punctuation (n=2–5) 2. How do probabilistic classifiers perform compared to regressors when predicting realvalued scores? (b) word n-grams (n=1,2) (c) triples extracted over dependency parses obtained from ZPar (Zhang and Clark, 2011), and 3. Do hyper-parameters matter? Is it worth spending extra time and effort to tune the hyper-parameters of any given learner over simply using the default values provided by the implementation being used? 3 (d) length bins (specifically, whether the log of 1 plus the number of characters in the response, rounded down to the nearest integer, equals x, for all possible x from the training set). For example, consider a question for which the responses in the training data are between 50 and 200 characters long. For this question, we will have 3 length bins numbered from 5 (blog2 51c) to 7 (b"
Y04-1016,P04-1041,1,0.861636,"Missing"
Y04-1016,C02-1126,0,0.0611897,"Missing"
Y04-1016,P02-1043,0,0.030143,"robust, state-of-art resources. However, (with few exceptions) the grammars induced are mostly &quot;shallow&quot;, i.e. without the deep syntactic (dependency) or semantic information captured by deep, constraint-based grammar formalisms such as LFG or HPSG. A recent body of research had extended the basic paradigm of automatic PCFG acquisition from treebanks to the extraction of deep, wide-coverage, constraint-based grammars and lexical resources such as LFG (Cahill et al., 2002; Cahill et al., 2003; Cahill et al., 2004; O’Donovan et al., 2004), HPSG (Miyao et al., 2003; Miyao et al., 2004) and CCG (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). Cahill et al. have developed a methodology for the automatic f-structure annotation of treebanks from which LFG grammars and lexical resources are extracted. To date this research has been applied to the Penn-II treebank (Marcus et al., 1994) for English and the TIGER treebank (Brants et al., 2002) for German. In this paper, we report on an experiment to extend this research to a new language—Mandarin Chinese—via the Penn Chinese Treebank (CTB) (Xue et al., 2002). In Section 2 we first give a brief review of Lexical-Functional Grammar. Section 3 provides a short descripti"
Y04-1016,W03-2401,0,0.0943368,"Missing"
Y04-1016,H94-1020,0,0.543341,"ch had extended the basic paradigm of automatic PCFG acquisition from treebanks to the extraction of deep, wide-coverage, constraint-based grammars and lexical resources such as LFG (Cahill et al., 2002; Cahill et al., 2003; Cahill et al., 2004; O’Donovan et al., 2004), HPSG (Miyao et al., 2003; Miyao et al., 2004) and CCG (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). Cahill et al. have developed a methodology for the automatic f-structure annotation of treebanks from which LFG grammars and lexical resources are extracted. To date this research has been applied to the Penn-II treebank (Marcus et al., 1994) for English and the TIGER treebank (Brants et al., 2002) for German. In this paper, we report on an experiment to extend this research to a new language—Mandarin Chinese—via the Penn Chinese Treebank (CTB) (Xue et al., 2002). In Section 2 we first give a brief review of Lexical-Functional Grammar. Section 3 provides a short description of the CTB (Xue et al., 2002). We present an automatic f-structure annotation algorithm for the CTB. The algorithm generates proto-f-structures (Cahill et al., 2002). Proto-fstructures capture basic but possibly incomplete predicate-argument-adjunct structure a"
Y04-1016,P04-1047,1,0.631032,"Missing"
Y04-1016,P02-1035,0,0.530094,"In order to assess the quality of the extracted grammars we carried out three types of parsing experiments: • In experiment 1 we evaluate the CFG tree output of our parsers against the original trees for strings length &lt;= 40 in articles 301-325 CTB, reporting f-scores for labelled and unlabelled bracketings using evalb. • In experiment 2 we evaluate the f-structures generated by our grammars against the manually annotated 50 gold-standard f-structures for randomly selected trees from articles 301-325 using the triple-based dependency encoding and evaluation software from (Crouch et al., 2002; Riezler et al., 2002). • In experiment 3 we evaluate the f-structures generated by our grammars against the fstructures for the full 318 test strings as generated by the automatic f-structure annotation algorithm for the original trees in articles 301-325 CTB using the triple-based dependency encoding and evaluation software from (Crouch et al., 2002; Riezler et al., 2002). - 168 - PACLIC 18, December 8th-10th, 2004, Waseda University, Tokyo 7.2.1 Experiment 1 (Tree-Based Evaluation) Table 5 describes the results obtained in experiment 1. In this experiment we evaluate the parse output generated by our grammars ag"
Y04-1016,C02-1145,0,0.217827,"ill et al., 2004; O’Donovan et al., 2004), HPSG (Miyao et al., 2003; Miyao et al., 2004) and CCG (Hockenmaier and Steedman, 2002; Hockenmaier, 2003). Cahill et al. have developed a methodology for the automatic f-structure annotation of treebanks from which LFG grammars and lexical resources are extracted. To date this research has been applied to the Penn-II treebank (Marcus et al., 1994) for English and the TIGER treebank (Brants et al., 2002) for German. In this paper, we report on an experiment to extend this research to a new language—Mandarin Chinese—via the Penn Chinese Treebank (CTB) (Xue et al., 2002). In Section 2 we first give a brief review of Lexical-Functional Grammar. Section 3 provides a short description of the CTB (Xue et al., 2002). We present an automatic f-structure annotation algorithm for the CTB. The algorithm generates proto-f-structures (Cahill et al., 2002). Proto-fstructures capture basic but possibly incomplete predicate-argument-adjunct structure as they do not yet resolve long-distance dependencies. Section 4 outlines the architecture underlying the automatic fstructure annotation algorithm and how it was applied to the CTB. Section 5 provides an evaluation of the f-s"
Y04-1016,C04-1024,0,\N,Missing
Y04-1016,P03-1046,0,\N,Missing
Y04-1016,P03-1056,0,\N,Missing
