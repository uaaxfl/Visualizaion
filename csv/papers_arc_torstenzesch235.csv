2021.woah-1.22,{VL}-{BERT}+: Detecting Protected Groups in Hateful Multimodal Memes,2021,-1,-1,4,0.833333,93,piush aggarwal,Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021),0,"This paper describes our submission (winning solution for Task A) to the Shared Task on Hateful Meme Detection at WOAH 2021. We build our system on top of a state-of-the-art system for binary hateful meme classification that already uses image tags such as race, gender, and web entities. We add further metadata such as emotions and experiment with data augmentation techniques, as hateful instances are underrepresented in the data set."
2021.unimplicit-1.2,Implicit Phenomena in Short-answer Scoring Data,2021,-1,-1,3,0,656,marie bexte,Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language,0,"Short-answer scoring is the task of assessing the correctness of a short text given as response to a question that can come from a variety of educational scenarios. As only content, not form, is important, the exact wording including the explicitness of an answer should not matter. However, many state-of-the-art scoring models heavily rely on lexical information, be it word embeddings in a neural network or n-grams in an SVM. Thus, the exact wording of an answer might very well make a difference. We therefore quantify to what extent implicit language phenomena occur in short answer datasets and examine the influence they have on automatic scoring performance. We find that the level of implicitness depends on the individual question, and that some phenomena are very frequent. Resolving implicit wording to explicit formulations indeed tends to improve automatic scoring performance."
2021.teachingnlp-1.6,A Crash Course on Ethics for Natural Language Processing,2021,-1,-1,2,0,785,annemarie friedrich,Proceedings of the Fifth Workshop on Teaching NLP,0,"It is generally agreed upon in the natural language processing (NLP) community that ethics should be integrated into any curriculum. Being aware of and understanding the relevant core concepts is a prerequisite for following and participating in the discourse on ethical NLP. We here present ready-made teaching material in the form of slides and practical exercises on ethical issues in NLP, which is primarily intended to be integrated into introductory NLP or computational linguistics courses. By making this material freely available, we aim at lowering the threshold to adding ethics to the curriculum. We hope that increased awareness will enable students to identify potentially unethical behavior."
2021.konvens-1.18,Robustness of end-to-end Automatic Speech Recognition Models {--} A Case Study using Mozilla {D}eep{S}peech,2021,-1,-1,2,0,5571,aashish agarwal,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),0,None
2021.konvens-1.19,Effects of Layer Freezing on Transferring a Speech Recognition System to Under-resourced Languages,2021,-1,-1,2,0,5572,onno eberhard,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),0,None
2021.bea-1.19,{C}-Test Collector: A Proficiency Testing Application to Collect Training Data for {C}-Tests,2021,-1,-1,4,0,12245,christian haring,Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications,0,"We present the C-Test Collector, a web-based tool that allows language learners to test their proficiency level using c-tests. Our tool collects anonymized data on test performance, which allows teachers to gain insights into common error patterns. At the same time, it allows NLP researchers to collect training data for being able to generate c-test variants at the desired difficulty level."
2020.lrec-1.709,"Decomposing and Comparing Meaning Relations: Paraphrasing, Textual Entailment, Contradiction, and Specificity",2020,-1,-1,5,0.625,12841,venelin kovatchev,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we present a methodology for decomposing and comparing multiple meaning relations (paraphrasing, textual entailment, contradiction, and specificity). The methodology includes SHARel - a new typology that consists of 26 linguistic and 8 reason-based categories. We use the typology to annotate a corpus of 520 sentence pairs in English and we demonstrate that unlike previous typologies, SHARel can be applied to all relations of interest with a high inter-annotator agreement. We analyze and compare the frequency and distribution of the linguistic and reason-based phenomena involved in paraphrasing, textual entailment, contradiction, and specificity. This comparison allows for a much more in-depth analysis of the workings of the individual relations and the way they interact and compare with each other. We release all resources (typology, annotation guidelines, and annotated corpus) to the community."
2020.coling-main.76,Don{'}t take {``}nswvtnvakgxpm{''} for an answer {--}The surprising vulnerability of automatic content scoring systems to adversarial input,2020,-1,-1,5,1,21131,yuning ding,Proceedings of the 28th International Conference on Computational Linguistics,0,"Automatic content scoring systems are widely used on short answer tasks to save human effort. However, the use of these systems can invite cheating strategies, such as students writing irrelevant answers in the hopes of gaining at least partial credit. We generate adversarial answers for benchmark content scoring datasets based on different methods of increasing sophistication and show that even simple methods lead to a surprising decrease in content scoring performance. As an extreme example, up to 60{\%} of adversarial answers generated from random shuffling of words in real answers are accepted by a state-of-the-art scoring system. In addition to analyzing the vulnerabilities of content scoring systems, we examine countermeasures such as adversarial training and show that these measures improve system robustness against adversarial answers considerably but do not suffice to completely solve the problem."
2020.aacl-main.37,{C}hinese Content Scoring: Open-Access Datasets and Features on Different Segmentation Levels,2020,-1,-1,3,1,21131,yuning ding,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"In this paper, we analyse the challenges of Chinese content scoring in comparison to English. As a review of prior work for Chinese content scoring shows a lack of open-access data in the field, we present two short-answer data sets for Chinese. The Chinese Educational Short Answers data set (CESA) contains 1800 student answers for five science-related questions. As a second data set, we collected ASAP-ZH with 942 answers by re-using three existing prompts from the ASAP data set. We adapt a state-of-the-art content scoring system for Chinese and evaluate it in several settings on these data sets. Results show that features on lower segmentation levels such as character n-grams tend to have better performance than features on token level."
W19-4004,Annotating and analyzing the interactions between meaning relations,2019,0,0,3,1,95,darina gold,Proceedings of the 13th Linguistic Annotation Workshop,0,"Pairs of sentences, phrases, or other text pieces can hold semantic relations such as paraphrasing, textual entailment, contradiction, specificity, and semantic similarity. These relations are usually studied in isolation and no dataset exists where they can be compared empirically. Here we present a corpus annotated with these relations and the analysis of these results. The corpus contains 520 sentence pairs, annotated with these relations. We measure the annotation reliability of each individual relation and we examine their interactions and correlations. Among the unexpected results revealed by our analysis is that the traditionally considered direct relationship between paraphrasing and bi-directional entailment does not hold in our data."
S19-2078,ltl.uni-due at {S}em{E}val-2019 Task 5: Simple but Effective Lexico-Semantic Features for Detecting Hate Speech in {T}witter,2019,0,0,4,0,25028,huangpan zhang,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"In this paper, we present our contribution to SemEval 2019 Task 5 Multilingual Detection of Hate, specifically in the Subtask A (English and Spanish). We compare different configurations of shallow and deep learning approaches on the English data and use the system that performs best in both sub-tasks. The resulting SVM-based system with lexicosemantic features (n-grams and embeddings) is ranked 23rd out of 69 on the English data and beats the baseline system. On the Spanish data our system is ranked 25th out of 39."
S19-2121,{LTL}-{UDE} at {S}em{E}val-2019 Task 6: {BERT} and Two-Vote Classification for Categorizing Offensiveness,2019,0,3,4,0.833333,93,piush aggarwal,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"We present results for Subtask A and C of SemEval 2019 Shared Task 6. In Subtask A, we experiment with an embedding representation of postings and use BERT to categorize postings. Our best result reaches the 10th place (out of 103). In Subtask C, we applied a two-vote classification approach with minority fallback, which is placed on the 19th rank (out of 65)."
R19-1047,Divide and Extract {--} Disentangling Clause Splitting and Proposition Extraction,2019,0,0,2,1,95,darina gold,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Proposition extraction from sentences is an important task for information extraction systems Evaluation of such systems usually conflates two aspects: splitting complex sentences into clauses and the extraction of propositions. It is thus difficult to independently determine the quality of the proposition extraction step. We create a manually annotated proposition dataset from sentences taken from restaurant reviews that distinguishes between clauses that need to be split and those that do not. The resulting proposition evaluation dataset allows us to independently compare the performance of proposition extraction systems on simple and complex clauses. Although performance drastically drops on more complex sentences, we show that the same systems perform best on both simple and complex clauses. Furthermore, we show that specific kinds of subordinate clauses pose difficulties to most systems."
N19-1135,From legal to technical concept: Towards an automated classification of {G}erman political {T}witter postings as criminal offenses,2019,0,1,3,0,26131,frederike zufall,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Advances in the automated detection of offensive Internet postings make this mechanism very attractive to social media companies, who are increasingly under pressure to monitor and action activity on their sites. However, these advances also have important implications as a threat to the fundamental right of free expression. In this article, we analyze which Twitter posts could actually be deemed offenses under German criminal law. German law follows the deductive method of the Roman law tradition based on abstract rules as opposed to the inductive reasoning in Anglo-American common law systems. This allows us to show how legal conclusions can be reached and implemented without relying on existing court decisions. We present a data annotation schema, consisting of a series of binary decisions, for determining whether a specific post would constitute a criminal offense. This schema serves as a step towards an inexpensive creation of a sufficient amount of data for an automated classification. We find that the majority of posts deemed offensive actually do not constitute a criminal offense and still contribute to public discourse. Furthermore, laymen can provide sufficiently reliable data to an expert reference but are, for instance, more lenient in the interpretation of what constitutes a disparaging statement."
W18-7103,The Role of Diacritics in Increasing the Difficulty of {A}rabic Lexical Recognition Tests,2018,-1,-1,2,0,23514,osama hamed,Proceedings of the 7th workshop on {NLP} for Computer Assisted Language Learning,0,None
W18-0550,Cross-Lingual Content Scoring,2018,0,3,3,1,657,andrea horbach,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We investigate the feasibility of cross-lingual content scoring, a scenario where training and test data in an automatic scoring task are from two different languages. Cross-lingual scoring can contribute to educational equality by allowing answers in multiple languages. Training a model in one language and applying it to another language might also help to overcome data sparsity issues by re-using trained models from other languages. As there is no suitable dataset available for this new task, we create a comparable bi-lingual corpus by extending the English ASAP dataset with German answers. Our experiments with cross-lingual scoring based on machine-translating either training or test data show a considerable drop in scoring quality."
S18-2026,Agree or Disagree: Predicting Judgments on Nuanced Assertions,2018,0,0,2,1,25029,michael wojatzki,Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics,0,"Being able to predict whether people agree or disagree with an assertion (i.e. an explicit, self-contained statement) has several applications ranging from predicting how many people will like or dislike a social media post to classifying posts based on whether they are in accordance with a particular point of view. We formalize this as two NLP tasks: predicting judgments of (i) individuals and (ii) groups based on the text of the assertion and previous judgments. We evaluate a wide range of approaches on a crowdsourced data set containing over 100,000 judgments on over 2,000 assertions. We find that predicting individual judgments is a hard task with our best results only slightly exceeding a majority baseline, but that judgments of groups can be more reliably predicted using a Siamese neural network, which outperforms all other approaches by a wide margin."
L18-1224,Quantifying Qualitative Data for Understanding Controversial Issues,2018,0,0,3,1,25029,michael wojatzki,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Understanding public opinion on complex controversial issues such as xe2x80x98Legalization of Marijuanaxe2x80x99 and xe2x80x98Gun Rightsxe2x80x99 is of considerable importance for a number of objectives such as identifying the most divisive facets of the issue, developing a consensus, and making informed policy decisions. However, an individualxe2x80x99s position on a controversial issue is often not just a binary support-or-oppose stance on the issue, but rather a conglomerate of nuanced opinions and beliefs on various aspects of the issue. These opinions and beliefs are often expressed qualitatively in free text in issue-focused surveys or on social media. However, quantifying vast amounts of qualitative information remains a significant challenge. The goal of this work is to provide a new approach for quantifying qualitative data for the understanding of controversial issues. First, we show how we can engage people directly through crowdsourcing to create a comprehensive dataset of assertions (claims, opinions, arguments, etc.) relevant to an issue. Next, the assertions are judged for agreement and strength of support or opposition, again by crowdsourcing. The collected Dataset of Nuanced Assertions on Controversial Issues (NAoCI dataset) consists of over 2,000 assertions on sixteen different controversial issues. It has over 100,000 judgments of whether people agree or disagree with the assertions, and of about 70,000 judgments indicating how strongly people support or oppose the assertions. This dataset allows for several useful analyses that help summarize public opinion. Across the sixteen issues, we find that when people judge a large set of assertions they often do not disagree with the individual assertions that the opposite side makes, but that they differently judge the relative importance of these assertions. We show how assertions that cause dissent or consensus can be identified by ranking the whole set of assertions based on the collected judgments. We also show how free-text assertions in social media can be analyzed in conjunction with the crowdsourced information to quantify and summarize public opinion on controversial issues."
L18-1365,{ESCRITO} - An {NLP}-Enhanced Educational Scoring Toolkit,2018,0,0,1,1,96,torsten zesch,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1403,{D}eep{TC} {--} An Extension of {DKP}ro Text Classification for Fostering Reproducibility of Deep Learning Experiments,2018,0,1,2,1,25030,tobias horsmann,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-5908,The Influence of Spelling Errors on Content Scoring Performance,2017,0,0,3,1,657,andrea horbach,Proceedings of the 4th Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA} 2017),0,"Spelling errors occur frequently in educational settings, but their influence on automatic scoring is largely unknown. We therefore investigate the influence of spelling errors on content scoring performance using the example of the ASAP corpus. We conduct an annotation study on the nature of spelling errors in the ASAP dataset and utilize these finding in machine learning experiments that measure the influence of spelling errors on automatic content scoring. Our main finding is that scoring methods using both token and character n-gram features are robust against spelling errors up to the error frequency in ASAP."
W17-5017,Investigating neural architectures for short answer scoring,2017,19,11,4,0.576923,21132,brian riordan,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Neural approaches to automated essay scoring have recently shown state-of-the-art performance. The automated essay scoring task typically involves a broad notion of writing quality that encompasses content, grammar, organization, and conventions. This differs from the short answer content scoring task, which focuses on content accuracy. The inputs to neural essay scoring models {--} ngrams and embeddings {--} are arguably well-suited to evaluate content in short answer scoring tasks. We investigate how several basic neural approaches similar to those used for automated essay scoring perform on short answer scoring. We show that neural architectures can outperform a strong non-neural baseline, but performance and optimal parameter settings vary across the more diverse types of prompts typical of short answer scoring."
W17-5040,Fine-grained essay scoring of a complex writing task for native speakers,2017,0,0,4,1,657,andrea horbach,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Automatic essay scoring is nowadays successfully used even in high-stakes tests, but this is mainly limited to holistic scoring of learner essays. We present a new dataset of essays written by highly proficient German native speakers that is scored using a fine-grained rubric with the goal to provide detailed feedback. Our experiments with two state-of-the-art scoring systems (a neural and a SVM-based one) show a large drop in performance compared to existing datasets. This demonstrates the need for such datasets that allow to guide research on more elaborate essay scoring methods."
benikova-zesch-2017-different,"Same same, but different: Compositionality of paraphrase granularity levels",2017,0,0,2,0.740741,32428,darina benikova,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Paraphrases exist on different granularity levels, the most frequently used one being the sentential level. However, we argue that working on the sentential level is not optimal for both machines and humans, and that it would be easier and more efficient to work on sub-sentential levels. To prove this, we quantify and analyze the difference between paraphrases on both sentence and sub-sentence level in order to show the significance of the problem. First results on a preliminary dataset seem to confirm our hypotheses."
D17-1076,Do {LSTM}s really work so well for {P}o{S} tagging? {--} A replication study,2017,27,4,2,1,25030,tobias horsmann,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"A recent study by Plank et al. (2016) found that LSTM-based PoS taggers considerably improve over the current state-of-the-art when evaluated on the corpora of the Universal Dependencies project that use a coarse-grained tagset. We replicate this study using a fresh collection of 27 corpora of 21 languages that are annotated with fine-grained tagsets of varying size. Our replication confirms the result in general, and we additionally find that the advantage of LSTMs is even bigger for larger tagsets. However, we also find that for the very large tagsets of morphologically rich languages, hand-crafted morphological lexicons are still necessary to reach state-of-the-art performance."
W16-6507,Validating bundled gap filling {--} Empirical evidence for ambiguity reduction and language proficiency testing capabilities,2016,0,0,3,0,33349,niklas meyer,Proceedings of the joint workshop on {NLP} for Computer Assisted Language Learning and {NLP} for Language Acquisition,0,None
W16-6002,Bridging the gap between computable and expressive event representations in Social Media,2016,-1,-1,2,0.740741,32428,darina benikova,Proceedings of the Workshop on Uphill Battles in Language Processing: Scaling Early Achievements to Robust Methods,0,None
W16-2615,{LTL}-{UDE} @ {E}mpiri{ST} 2015: Tokenization and {P}o{S} Tagging of Social Media Text,2016,0,4,2,1,25030,tobias horsmann,Proceedings of the 10th Web as Corpus Workshop,0,None
W16-0508,Predicting the Spelling Difficulty of Words for Language Learners,2016,0,1,2,1,3245,lisa beinborn,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In many language learning scenarios, it is important to anticipate spelling errors. We model the spelling difficulty of words with new features that capture phonetic phenomena and are based on psycholinguistic findings. To train our model, we extract more than 140,000 spelling errors from three learner corpora covering English, German and Italian essays. The evaluation shows that our model can predict spelling difficulty with an accuracy of over 80% and yields a stable quality across corpora and languages. In addition, we provide a thorough error analysis that takes the native language of the learners into account and provides insights into cross-lingual transfer effects."
W16-0519,Bundled Gap Filling: A New Paradigm for Unambiguous Cloze Exercises,2016,15,1,3,1,25029,michael wojatzki,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,None
S16-1069,ltl.uni-due at {S}em{E}val-2016 Task 6: Stance Detection in Social Media Using Stacked Classifiers,2016,6,7,2,1,25029,michael wojatzki,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1675,{F}lex{T}ag: A Highly Flexible {P}o{S} Tagging Framework,2016,15,2,1,1,96,torsten zesch,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present FlexTag, a highly flexible PoS tagging framework. In contrast to monolithic implementations that can only be retrained but not adapted otherwise, FlexTag enables users to modify the feature space and the classification algorithm. Thus, FlexTag makes it easy to quickly develop custom-made taggers exactly fitting the research problem."
C16-1032,Assigning Fine-grained {P}o{S} Tags based on High-precision Coarse-grained Tagging,2016,19,1,2,1,25030,tobias horsmann,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We propose a new approach to PoS tagging where in a first step, we assign a coarse-grained tag corresponding to the main syntactic category. Based on this high-precision decision, in the second step we utilize specially trained fine-grained models with heavily reduced decision complexity. By analyzing the system under oracle conditions, we show that there is a quite large potential for significantly outperforming a competitive baseline. When we take error-propagation from the coarse-grained tagging into account, our approach is on par with the state of the art. Our approach also allows tailoring the tagger towards recognizing single word classes which are of interest e.g. for researchers searching for specific phenomena in large corpora. In a case study, we significantly outperform a standard model that also makes use of the same optimizations."
C16-1198,Predicting proficiency levels in learner writings by transferring a linguistic complexity model from expert-written coursebooks,2016,18,6,3,0,13178,ildiko pilan,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"The lack of a sufficient amount of data tailored for a task is a well-recognized problem for many statistical NLP methods. In this paper, we explore whether data sparsity can be successfully tackled when classifying language proficiency levels in the domain of learner-written output texts. We aim at overcoming data sparsity by incorporating knowledge in the trained model from another domain consisting of input texts written by teaching professionals for learners. We compare different domain adaptation techniques and find that a weighted combination of the two types of data performs best, which can even rival systems based on considerably larger amounts of in-domain data. Moreover, we show that normalizing errors in learners{'} texts can substantially improve classification when level-annotated in-domain data is not available."
W15-3603,Counting What Counts: Decompounding for Keyphrase Extraction,2015,23,1,3,1,36774,nicolai erbs,Proceedings of the {ACL} 2015 Workshop on Novel Computational Approaches to Keyphrase Extraction,0,"A core assumption of keyphrase extraction is that a concept is more important if it is mentioned more often in a document. Especially in languages like German that form large noun compounds, frequency counts might be misleading as concepts xe2x80x9chiddenxe2x80x9d in compounds are not counted. We hypothesize that using decompounding before counting term frequencies may lead to better keyphrase extraction. We identified two effects of decompounding: (i) enhanced frequency counts, and (ii) more keyphrase candidates. We created two German evaluation datasets to test our hypothesis and analyzed the effect of additional decompounding for keyphrase extraction."
W15-0601,Candidate evaluation strategies for improved difficulty prediction of language tests,2015,28,0,2,1,3245,lisa beinborn,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Language proficiency tests are a useful tool for evaluating learner progress, if the test difficulty fits the level of the learner. In this work, we describe a generalized framework for test difficulty prediction that is applicable to several languages and test types. In addition, we develop two ranking strategies for candidate evaluation inspired by automatic solving methods based on language model probability and semantic relatedness. These ranking strategies lead to significant improvements for the difficulty prediction of cloze tests."
W15-0615,Reducing Annotation Efforts in Supervised Short Answer Scoring,2015,9,8,1,1,96,torsten zesch,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Automated short answer scoring is increasingly used to give students timely feedback about their learning progress. Building scoring models comes with high costs, as stateof-the-art methods using supervised learning require large amounts of hand-annotated data. We analyze the potential of recently proposed methods for semi-supervised learning based on clustering. We find that all examined methods (centroids, all clusters, selected pure clusters) are mainly effective for very short answers and do not generalize well to severalsentence responses."
W15-0626,Task-Independent Features for Automated Essay Grading,2015,22,17,1,1,96,torsten zesch,Proceedings of the Tenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Automated scoring of student essays is increasingly used to reduce manual grading effort. State-of-the-art approaches use supervised machine learning which makes it complicated to transfer a system trained on one task to another. We investigate which currently used features are task-independent and evaluate their transferability on English and German datasets. We find that, by using our task-independent feature set, models transfer better between tasks. We also find that the transfer works even better between tasks of the same type."
W14-3503,Towards Automatic Scoring of Cloze Items by Selecting Low-Ambiguity Contexts,2014,22,2,2,1,25030,tobias horsmann,Proceedings of the third workshop on {NLP} for computer-assisted language learning,0,"In second language learning, cloze tests (also known as fill-in-the-blank tests) are frequently used for assessing the learning progress of students. While preparation effort for these tests is low, scoring needs to be done manually, as there usually is a huge number of correct solutions. In this paper, we examine whether the ambiguity of cloze items can be lowered to a point where automatic scoring becomes possible. We utilize the local context of a word to collect evidence of low-ambiguity. We do that by seeking for collocated word sequences, but also taking structural information on sentence level into account. We evaluate the effectiveness of our method in a user study on cloze items ranked by our method. For the top-ranked items (lowest ambiguity) the subjects provide the target word significantly more often than for the bottom-ranked items (59.9% vs. 36.5%). While this shows the potential of our method, we did not succeed in fully eliminating ambiguity. Thus, further research is necessary before fully automatic scoring becomes possible."
W14-1817,Automatic Generation of Challenging Distractors Using Context-Sensitive Inference Rules,2014,15,14,1,1,96,torsten zesch,Proceedings of the Ninth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Automatically generating challenging distractors for multiple-choice gap-fill items is still an unsolved problem. We propose to employ context-sensitive lexical inference rules in order to generate distractors that are semantically similar to the gap target word in some sense, but not in the particular sense induced by the gap-fill context. We hypothesize that such distractors should be particularly hard to distinguish from the correct answer. We focus on verbs as they are especially difficult to master for language learners and find that our approach is quite effective. In our test set of 20 items, our proposed method decreases the number of invalid distractors in 90% of the cases, and fully eliminates all of them in 65%. Further analysis on that dataset does not support our hypothesis regarding item difficulty as measured by average error rate of language learners. We conjecture that this may be due to limitations in our evaluation setting, which we plan to address in future work."
S14-1004,Sense and Similarity: A Study of Sense-level Similarity Measures,2014,20,2,3,1,36774,nicolai erbs,Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*{SEM} 2014),0,"In this paper, we investigate the difference between word and sense similarity measures and present means to convert a state-of-the-art word similarity measure into a sense similarity measure. In order to evaluate the new measure, we create a special sense similarity dataset and re-rate an existing word similarity dataset using two different sense inventories from WordNet and Wikipedia. We discover that word-level measures were not able to differentiate between different senses of one word, while sense-level measures actually increase correlation when shifting to sense similarities. Sense-level similarity measures improve when evaluated with a re-rated sense-aware gold standard, while correlation with word-level similarity measures decreases."
Q14-1040,Predicting the Difficulty of Language Proficiency Tests,2014,27,12,2,1,3245,lisa beinborn,Transactions of the Association for Computational Linguistics,0,"Language proficiency tests are used to evaluate and compare the progress of language learners. We present an approach for automatic difficulty prediction of C-tests that performs on par with human experts. On the basis of detailed analysis of newly collected data, we develop a model for C-test difficulty introducing four dimensions: solution difficulty, candidate ambiguity, inter-gap dependency, and paragraph difficulty. We show that cues from all four dimensions contribute to C-test difficulty."
P14-5006,{DKP}ro Keyphrases: Flexible and Reusable Keyphrase Extraction Experiments,2014,16,7,4,1,36774,nicolai erbs,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"DKPro Keyphrases is a keyphrase extraction framework based on UIMA. It offers a wide range of state-of-the-art keyphrase experiments approaches. At the same time, it is a workbench for developing new extraction approaches and evaluating their impact. DKPro Keyphrases is publicly available under an open-source license. 1"
P14-5011,{DKP}ro {TC}: A {J}ava-based Framework for Supervised Learning Experiments on Textual Data,2014,10,33,4,0,1556,johannes daxenberger,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We present DKPro TC, a framework for supervised learning experiments on textual data. The main goal of DKPro TC is to enable researchers to focus on the actual research task behind the learning problem and let the framework handle the rest. It enables rapid prototyping of experiments by relying on an easy-to-use workflow engine and standardized document preprocessing based on the Apache Unstructured Information Management Architecture (Ferrucci and Lally, 2004). It ships with standard feature extraction modules, while at the same time allowing the user to add customized extractors. The extensive reporting and logging facilities make DKPro TC experiments fully replicable."
S13-2007,{S}em{E}val-2013 Task 5: Evaluating Phrasal Semantics,2013,17,19,2,0,39671,ioannis korkontzelos,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"This paper describes the SemEval-2013 Task 5: xe2x80x9cEvaluating Phrasal Semanticsxe2x80x9d. Its first subtask is about computing the semantic similarity of words and compositional phrases of minimal length. The second one addresses deciding the compositionality of phrases in a given context. The paper discusses the importance and background of these subtasks and their structure. In succession, it introduces the systems that participated and discusses evaluation results."
S13-2048,{UKP}-{BIU}: Similarity and Entailment Metrics for Student Response Analysis,2013,17,11,2,0,3267,omer levy,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"Our system combines text similarity measures with a textual entailment system. In the main task, we focused on the influence of lexicalized versus unlexicalized features, and how they affect performance on unseen questions and domains. We also participated in the pilot partial entailment task, where our system significantly outperforms a strong baseline."
R13-1033,Hierarchy Identification for Automatically Generating Table-of-Contents,2013,21,4,3,1,36774,nicolai erbs,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"A table-of-contents (TOC) provides a quick reference to a documentxe2x80x99s content and structure. We present the first study on identifying the hierarchical structure for automatically generating a TOC using only textual features instead of structural hints e.g. from HTML-tags. We create two new datasets to evaluate our approaches for hierarchy identification. We find that our algorithm performs on a level that is sufficient for a fully automated system. For documents without given segment titles, we extend out work by auto matically generating segment titles.n We make the datasets and our experimental framework publicly available in order to foster future research in TOC generation."
P13-4007,{DKP}ro {WSD}: A Generalized {UIMA}-based Framework for Word Sense Disambiguation,2013,23,10,4,1,1740,tristan miller,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported."
P13-4021,{DKP}ro Similarity: An Open Source Framework for Text Similarity,2013,27,61,2,1,41371,daniel bar,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We present DKPro Similarity, an open source framework for text similarity. Ourn goal is to provide a comprehensive repository of text similarity measures whichn are implemented using standardized interfaces. DKPro Similarity comprises an wide variety of measures ranging from ones based on simple n-grams and common subsequences to high-dimensional vector comparisons and structural, stylistic, and phonetic measures. In order to promote the reproducibility of experimental results and to provide reliable, permanent experimental conditions for future studies, DKPro Similarity additionally comes with a set of full-featuredn experimental setups which can be run out-of-the-box and be used for futuren systems to built upon."
P13-2080,Recognizing Partial Textual Entailment,2013,12,17,2,0,3267,omer levy,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. It thus cannot capture the notion that the target fragment is xe2x80x9calmost entailedxe2x80x9d by the given text. The recently suggested idea of partial textual entailment may remedy this problem. We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to this setting. Indeed, our results show that these methods are useful for rec- ognizing partial entailment. We also provide a preliminary assessment of how partial entailment may be used for recognizing (complete) textual entailment."
I13-1112,Cognate Production using Character-based Machine Translation,2013,23,14,2,1,3245,lisa beinborn,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Cognates are words in different languages that are associated with each other by language learners. Thus, cognates are important indicators for the prediction of the perceived difficulty of a text. We introduce a method for automatic cognate production using character-based machine translation. We show that our approach is able to learn production patterns from noisy training data and that it works for a wide range of language pairs. It even works across different alphabets, e.g. we obtain good results on the tested language pairs English-Russian, English-Greek, and English-Farsi. Our method performs significantly better than similarity measures used in previous work on cognates."
W12-2036,{HOO} 2012 Shared Task: {UKP} Lab System Description,2012,16,2,1,1,96,torsten zesch,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"In this paper, we describe the UKP Lab system participating in the HOO 2012 Shared Task on preposition and determiner error correction. Our focus was to implement a highly flexible and modular system which can be easily augmented by other researchers. The system might be used to provide a level playground for subsequent shared tasks and enable further progress in this important research field on top of the state of the art identified by the shared task."
S12-1059,{UKP}: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures,2012,36,150,4,1,41371,daniel bar,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"We present the UKP system which performed best in the Semantic Textual Similarity (STS) task at SemEval-2012 in two out of three metrics. It uses a simple log-linear regression model, trained on the training data, to combine multiple text similarity measures of varying complexity. These range from simple character and word n-grams and common subsequences to complex features such as Explicit Semantic Analysis vector comparisons and aggregation of word similarity based on lexical-semantic resources. Further, we employ a lexical substitution system and statistical machine translation to add additional lexemes, which alleviates lexical gaps. Our final models, one per dataset, consist of a log-linear combination of about 20 features, out of the possible 300 features implemented."
E12-1054,Measuring Contextual Fitness Using Error Contexts Extracted from the {W}ikipedia Revision History,2012,31,16,1,1,96,torsten zesch,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We evaluate measures of contextual fitness on the task of detecting real-word spelling errors. For that purpose, we extract naturally occurring errors and their contexts from the Wikipedia revision history. We show that such natural errors are better suited for evaluation than the previously used artificially created errors. In particular, the precision of statistical methods has been largely over-estimated, while the precision of knowledge-based approaches has been under-estimated. Additionally, we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations. Finally, we show that statistical and knowledge-based methods can be combined for increased performance."
C12-1011,Text Reuse Detection using a Composition of Text Similarity Measures,2012,55,34,2,1,41371,daniel bar,Proceedings of {COLING} 2012,0,"Detecting text reuse is a fundamental requirement for a variety of tasks and applications, ranging from journalistic text reuse to plagiarism detection. Text reuse is traditionally detected by computing similarity between a source text and a possibly reused text. However, existing text similarity measures exhibit a major limitation: They compute similarity only on features which can be derived from the content of the given texts, thereby inherently implying that any other text characteristics are negligible. In this paper, we overcome this traditional limitation and compute similarity along three characteristic dimensions inherent to texts: content, structure, and style. We explore and discuss possible combinations of measures along these dimensions, and our results demonstrate that the composition consistently outperforms previous approaches on three standard evaluation datasets, and that text reuse detection greatly benefits from incorporating a diverse feature set that reflects a wide variety of text characteristics."
C12-1109,Using Distributional Similarity for Lexical Expansion in Knowledge-based Word Sense Disambiguation,2012,34,42,3,1,1740,tristan miller,Proceedings of {COLING} 2012,0,"We explore the contribution of distributional information for purely knowledge-based word sense disambiguation. Specifically, we use a distributional thesaurus, computed from a large parsed corpus, for lexical expansion of context and sense information. This bridges the lexical gap that is seen as the major obstacle for word overlapxe2x80x90based approaches. We apply this mechanism to two traditional knowledge-based methods and show that distributional information significantly improves disambiguation results across several data sets. This improvement exceeds the state of the art for disambiguation without sense frequency informationxe2x80x94a situation which is especially encountered with new domains or languages for which no sense-annotated corpus is available."
W11-2842,Helping Our Own 2011: {UKP} Lab System Description,2011,6,4,1,1,96,torsten zesch,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"This paper describes the UKP Lab system participating in the Helping Our Own Challenge 2011. We focus on the correction of real-word spelling errors (RWSEs) that are especially hard to detect. Our highly flexible system architecture is based on UIMA (Ferrucci and Lally, 2004) and integrates state-of-the-art approaches for detecting RWSEs."
R11-1071,A Reflective View on Text Similarity,2011,25,17,2,1,41371,daniel bar,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"While the concept ofsimilarityis well grounded in psychology,text similarityis less well-defined. Thus, we analyze text similarity with respect to its definition and the datasets used for evaluation. We formalize text similarity based on the geometric model ofconceptual spacesalong three dimensions inherent to texts:structure,style, andcontent. We empirically ground these dimensions in a set of annotation studies, and categorize applications according to these dimensions. Furthermore, we analyze the characteristics of the existing evaluation datasets, and use those datasets to assess the performance of common text similarity measures."
P11-4013,{W}ikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis,2011,15,5,3,1,41371,daniel bar,Proceedings of the {ACL}-{HLT} 2011 System Demonstrations,0,"We present Wikulu, a system focusing on supporting wiki users with their everyday tasks by means of an intelligent interface. Wikulu is implemented as an extensible architecture which transparently integrates natural language processing (NLP) techniques with wikis. It is designed to be deployed with any wiki platform, and the current prototype integrates a wide range of NLP algorithms such as keyphrase extraction, link discovery, text segmentation, summarization, or text similarity. Additionally, we show how Wikulu can be applied for visually analyzing the results of NLP algorithms, educational purposes, and enabling semantic wikis."
P11-4017,{W}ikipedia Revision Toolkit: Efficiently Accessing {W}ikipedia{'}s Edit History,2011,11,34,2,0.625,39042,oliver ferschke,Proceedings of the {ACL}-{HLT} 2011 System Demonstrations,0,"We present an open-source toolkit which allows (i) to reconstruct past states of Wikipedia, and (ii) to efficiently access the edit history of Wikipedia articles. Reconstructing past states of Wikipedia is a prerequisite for reproducing previous experimental work based on Wikipedia. Beyond that, the edit history of Wikipedia articles has been shown to be a valuable knowledge source for NLP, but access is severely impeded by the lack of efficient tools for managing the huge amount of provided data. By using a dedicated storage format, our toolkit massively decreases the data volume to less than 2% of the original size, and at the same time provides an easy-to-use interface to access the revision data. The language-independent design allows to process any language represented in Wikipedia. We expect this work to consolidate NLP research using Wikipedia in general, and to foster research making use of the knowledge encoded in Wikipedia's edit history."
zesch-gurevych-2010-better,The More the Better? Assessing the Influence of {W}ikipedia{'}s Growth on Semantic Relatedness Measures,2010,25,11,1,1,96,torsten zesch,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Wikipedia has been used as a knowledge source in many areas of natural language processing. As most studies only use a certain Wikipedia snapshot, the influence of Wikipedias massive growth on the results is largely unknown. For the first time, we perform an in-depth analysis of this influence using semantic relatedness as an example application that tests a wide range of Wikipedias properties. We find that the growth of Wikipedia has almost no effect on the correlation of semantic relatedness measures with human judgments, while the coverage steadily increases."
R09-1086,Approximate Matching for Evaluating Keyphrase Extraction,2009,22,43,1,1,96,torsten zesch,Proceedings of the International Conference {RANLP}-2009,0,"We propose a new evaluation strategy for keyphrase extraction based on approximate keyphrase matching. It corresponds well with human judgments and is better suited to assess the performance of keyphrase extraction approaches. Additionally, we propose a generalized framework for comprehensive analysis of keyphrase extraction that subsumes most existing approaches, which allows for fair testing conditions. For the first time, we compare the results of state-of-the-art unsupervised and supervised keyphrase extraction approaches on three evaluation datasets and show that the relative performance of the approaches heavily depends on the evaluation metric as well as on the properties of the evaluation dataset."
zesch-etal-2008-extracting,Extracting Lexical Semantic Knowledge from {W}ikipedia and {W}iktionary,2008,16,270,1,1,96,torsten zesch,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Recently, collaboratively constructed resources such as Wikipedia and Wiktionary have been discovered as valuable lexical semantic knowledge bases with a high potential in diverse Natural Language Processing (NLP) tasks. Collaborative knowledge bases however significantly differ from traditional linguistic knowledge bases in various respects, and this constitutes both an asset and an impediment for research in NLP. This paper addresses one such major impediment, namely the lack of suitable programmatic access mechanisms to the knowledge stored in these large semantic knowledge bases. We present two application programming interfaces for Wikipedia and Wiktionary which are especially designed for mining the rich lexical semantic information dispersed in the knowledge bases, and provide efficient and structured access to the available knowledge. As we believe them to be of general interest to the NLP community, we have made them freely available for research purposes."
W07-0201,Analysis of the {W}ikipedia Category Graph for {NLP} Applications,2007,27,112,1,1,96,torsten zesch,Proceedings of the Second Workshop on {T}ext{G}raphs: Graph-Based Algorithms for Natural Language Processing,0,"In this paper, we discuss two graphs in Wikipedia (i) the article graph, and (ii) the category graph. We perform a graphtheoretic analysis of the category graph, and show that it is a scale-free, small world graph like other well-known lexical semantic networks. We substantiate our findings by transferring semantic relatedness algorithms defined on WordNet to the Wikipedia category graph. To assess the usefulness of the category graph as an NLP resource, we analyze its coverage and the performance of the transferred semantic relatedness algorithms."
P07-1130,What to be? - Electronic Career Guidance Based on Semantic Relatedness,2007,23,44,3,0.133412,706,iryna gurevych,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We present a study aimed at investigating the use of semantic information in a novel NLP application, Electronic Career Guidance (ECG), in German. ECG is formulated as an information retrieval (IR) task, whereby textual descriptions of professions (documents) are ranked for their relevance to natural language descriptions of a personxe2x80x99s professional interests (the topic). We compare the performance of two semantic IR models: (IR-1) utilizing semantic relatedness (SR) measures based on either wordnet or Wikipedia and a set of heuristics, and (IR-2) measuring the similarity between the topic and documents based on Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). We evaluate the performance of SR measures intrinsically on the tasks of (T-1) computing SR, and (T-2) solving Readerxe2x80x99s Digest Word Power (RDWP) questions."
N07-2052,Comparing {W}ikipedia and {G}erman {W}ordnet by Evaluating Semantic Relatedness on Multiple Datasets,2007,9,40,1,1,96,torsten zesch,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"We evaluate semantic relatedness measures on different German datasets showing that their performance depends on: (i) the definition of relatedness that was underlying the construction of the evaluation dataset, and (ii) the knowledge source used for computing semantic relatedness. We analyze how the underlying knowledge source influences the performance of a measure. Finally, we investigate the combination of wordnets and Wikipedia to improve the performance of semantic relatedness measures."
D07-1060,Cross-Lingual Distributional Profiles of Concepts for Measuring Semantic Distance,2007,23,32,4,0,13005,saif mohammad,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We present the idea of estimating semantic distance in one, possibly resource-poor, language using a knowledge source in another, possibly resource-rich, language. We do so by creating cross-lingual distributional profiles of concepts, using a bilingual lexicon and a bootstrapping algorithm, but without the use of any sense-annotated data or word-aligned corpora. The cross-lingual measures of semantic distance are evaluated on two tasks: (1) estimating semantic distance between words and ranking the word pairs according to semantic distance, and (2) solving Readerxe2x80x99s Digest xe2x80x98Word Powerxe2x80x99 problems. In task (1), cross-lingual measures are superior to conventional monolingual measures based on a wordnet. In task (2), cross-lingual measures are able to solve more problems correctly, and despite scores being affected by many tied answers, their overall performance is again better than the best monolingual measures."
W06-1104,Automatically Creating Datasets for Measures of Semantic Relatedness,2006,17,60,1,1,96,torsten zesch,Proceedings of the Workshop on Linguistic Distances,0,"Semantic relatedness is a special form of linguistic distance between words. Evaluating semantic relatedness measures is usually performed by comparison with human judgments. Previous test datasets had been created analytically and were limited in size. We propose a corpus-based system for automatically creating test datasets. Experiments with human subjects show that the resulting datasets cover all degrees of relatedness. As a result of the corpus-based approach, test datasets cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts."
