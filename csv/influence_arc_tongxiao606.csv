2011.mtsummit-papers.13,2008.amta-papers.2,0,0.0702532,"ment contexts into current SMT systems for document-level translation. In particular, we focus on translation consistency which is one of the most important issues in document-level MT. We propose a 3-step approach to incorporating document contexts into a traditional SMT system, and demonstrate that our approach can effectively reduce the errors caused by inconsistent translation. More interestingly, it is observed that using document contexts is promising for BLEU improvement. 2 Related Work To date, only a few studies have improved MT systems with the use of document contexts. For example, Brown (2008) proposed a method to improve SMT and Example-Based Machine Translation (EBMT) systems using document-level similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent featu"
2011.mtsummit-papers.13,P05-1048,0,0.0299149,"slation (EBMT) systems using document-level similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent features to select appropriate target lexical items for SMT systems (Carpuat and Wu, 2005; Carpuat and Wu, 2007; Chan et al., 2007). However, these studies were all in the scenario of sentence-level MT. By contrast, we focus more on using document contexts to address the issue in document translation. Actually, the translation consistency issue has been discussed in some related tasks. For example, Wang et al. (2007)’s work showed that consistency information was very helpful in dealing with the out-ofvocabulary (OOV) problem for Chinese word segmentation. 3 Document-level Consistency Verification Given a source document Df, the task of document-level SMT is to find an optimal tar"
2011.mtsummit-papers.13,2007.tmi-papers.6,0,0.0313509,"using document-level similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent features to select appropriate target lexical items for SMT systems (Carpuat and Wu, 2005; Carpuat and Wu, 2007; Chan et al., 2007). However, these studies were all in the scenario of sentence-level MT. By contrast, we focus more on using document contexts to address the issue in document translation. Actually, the translation consistency issue has been discussed in some related tasks. For example, Wang et al. (2007)’s work showed that consistency information was very helpful in dealing with the out-ofvocabulary (OOV) problem for Chinese word segmentation. 3 Document-level Consistency Verification Given a source document Df, the task of document-level SMT is to find an optimal target document De* by: D"
2011.mtsummit-papers.13,P07-1005,0,0.0660032,"similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent features to select appropriate target lexical items for SMT systems (Carpuat and Wu, 2005; Carpuat and Wu, 2007; Chan et al., 2007). However, these studies were all in the scenario of sentence-level MT. By contrast, we focus more on using document contexts to address the issue in document translation. Actually, the translation consistency issue has been discussed in some related tasks. For example, Wang et al. (2007)’s work showed that consistency information was very helpful in dealing with the out-ofvocabulary (OOV) problem for Chinese word segmentation. 3 Document-level Consistency Verification Given a source document Df, the task of document-level SMT is to find an optimal target document De* by: De* arg max Pr( De |D"
2011.mtsummit-papers.13,P07-2045,0,0.00568597,"manually removed the candidate checkpoints where the consistency of translation is not strongly required. The system was evaluated by the number of errors at checkpoints. We also reported the BLEU(-SBP) (Chiang et al., 2008) score to show the impact of our approach on translation accuracy. 4 4.1 Experiments Baseline System Our experiments were conducted on ChineseEnglish translation based on the open-source phrase-based MT system NiuTrans 5 . The NiuTrans uses two reordering models, including a maximum entropy-based lexicalized reordering model (Xiong et al., 2006) and a MSD reordering model (Koehn et al., 2007). In addition, it adopts all standard features used in the state-of-the-art SMT system Moses (Koehn et al., 2007), such as bi-directional phrase translation probabilities and n-gram language model. The feature weights were optimized using MERT (Och, 2003). By default, the distortion limit was set to 8, k was set to 1 (Equations 3, 5-7), and D was set to 0 (Equation 5). 4.3 5 Results in Default Settings We first investigate the effectiveness of our methods on error reduction in the default settings. Table 1 compares various methods in terms of the number of errors at checkpoints, where Post and"
2011.mtsummit-papers.13,P03-1021,0,0.0566132,"h on translation accuracy. 4 4.1 Experiments Baseline System Our experiments were conducted on ChineseEnglish translation based on the open-source phrase-based MT system NiuTrans 5 . The NiuTrans uses two reordering models, including a maximum entropy-based lexicalized reordering model (Xiong et al., 2006) and a MSD reordering model (Koehn et al., 2007). In addition, it adopts all standard features used in the state-of-the-art SMT system Moses (Koehn et al., 2007), such as bi-directional phrase translation probabilities and n-gram language model. The feature weights were optimized using MERT (Och, 2003). By default, the distortion limit was set to 8, k was set to 1 (Equations 3, 5-7), and D was set to 0 (Equation 5). 4.3 5 Results in Default Settings We first investigate the effectiveness of our methods on error reduction in the default settings. Table 1 compares various methods in terms of the number of errors at checkpoints, where Post and Rede stand for the post-editing and the re-decoding methods used in final translation generation (Step 3) respectively, M1 and M2 stand for the two counting methods shown in Equations (6-7). We see that all our proposed methods are effective to reduce th"
2011.mtsummit-papers.13,P94-1019,0,0.0549308,"ave improved MT systems with the use of document contexts. For example, Brown (2008) proposed a method to improve SMT and Example-Based Machine Translation (EBMT) systems using document-level similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent features to select appropriate target lexical items for SMT systems (Carpuat and Wu, 2005; Carpuat and Wu, 2007; Chan et al., 2007). However, these studies were all in the scenario of sentence-level MT. By contrast, we focus more on using document contexts to address the issue in document translation. Actually, the translation consistency issue has been discussed in some related tasks. For example, Wang et al. (2007)’s work showed that consistency information was very helpful in dealing with the out-ofvocabulary (OOV) problem for Chine"
2011.mtsummit-papers.13,P06-1066,0,0.0486244,"ords7 as the candidate checkpoints; 2) and then manually removed the candidate checkpoints where the consistency of translation is not strongly required. The system was evaluated by the number of errors at checkpoints. We also reported the BLEU(-SBP) (Chiang et al., 2008) score to show the impact of our approach on translation accuracy. 4 4.1 Experiments Baseline System Our experiments were conducted on ChineseEnglish translation based on the open-source phrase-based MT system NiuTrans 5 . The NiuTrans uses two reordering models, including a maximum entropy-based lexicalized reordering model (Xiong et al., 2006) and a MSD reordering model (Koehn et al., 2007). In addition, it adopts all standard features used in the state-of-the-art SMT system Moses (Koehn et al., 2007), such as bi-directional phrase translation probabilities and n-gram language model. The feature weights were optimized using MERT (Och, 2003). By default, the distortion limit was set to 8, k was set to 1 (Equations 3, 5-7), and D was set to 0 (Equation 5). 4.3 5 Results in Default Settings We first investigate the effectiveness of our methods on error reduction in the default settings. Table 1 compares various methods in terms of the"
2020.acl-main.285,W17-5029,0,0.01302,"ught in the course videos. For each video, we extract 10 most representative course concepts from subtitles (Pan et al., 2017b). We also record the concept description from Wikidata and search top 10 related papers for each concept via AMiner3 (Tang et al., 2008) as external resource. Moreover, as many NLP types of research are interested in discovering semantic relationships among concepts, we further build a novel concept taxonomy with prerequisite chains as a concept graph (Gordon et al., 2016). Concept Taxonomy. A solid concept taxonomy is favorable for further research in course content (Gordon et al., 2017). However, existing taxonomies like ConceptNet (Liu and Singh, 2004) or Wiki Taxonomy (Ponzetto and Strube, 2007) cannot be directly applied to course concepts because course concepts are mostly academic terms and the non-academic categories greatly interfere with the quality of taxonomy. Thus, we select a crosslingual term taxonomy from CNCTST4 as a basis and lead manual annotation to build a serviceable course concept taxonomy for MOOCCube. Prerequisite Chain. Prerequisite relation is defined as: If concept A can help understanding concept B, then there is a prerequisite relation from A to B"
2020.acl-main.285,P16-1082,0,0.195076,"ikidata2 as an external resource. 1 2 2.3 Concept and Concept Graph Course concepts refer to the knowledge concepts taught in the course videos. For each video, we extract 10 most representative course concepts from subtitles (Pan et al., 2017b). We also record the concept description from Wikidata and search top 10 related papers for each concept via AMiner3 (Tang et al., 2008) as external resource. Moreover, as many NLP types of research are interested in discovering semantic relationships among concepts, we further build a novel concept taxonomy with prerequisite chains as a concept graph (Gordon et al., 2016). Concept Taxonomy. A solid concept taxonomy is favorable for further research in course content (Gordon et al., 2017). However, existing taxonomies like ConceptNet (Liu and Singh, 2004) or Wiki Taxonomy (Ponzetto and Strube, 2007) cannot be directly applied to course concepts because course concepts are mostly academic terms and the non-academic categories greatly interfere with the quality of taxonomy. Thus, we select a crosslingual term taxonomy from CNCTST4 as a basis and lead manual annotation to build a serviceable course concept taxonomy for MOOCCube. Prerequisite Chain. Prerequisite re"
2020.acl-main.285,D15-1193,0,0.0605022,"Missing"
2020.acl-main.285,P17-1133,1,0.842146,"p: //moocdata.cn/data/MOOCCube. 1 Introduction Massive open online courses (MOOCs) boom swiftly in recent years and have provided convenient education for over 100 million users worldwide (Shah, 2019). As a multi-media, large-scale online interactive system, MOOC is an excellent platform for advanced application research (Volery and Lord, 2000). Since MOOC is committed to helping students learn implicit knowledge concepts from diverse courses, many efforts from NLP and AI raise topics to build novel applications for assistance. From extracting course concepts and their prerequisite relations (Pan et al., 2017b; Roy et al., 2019; Li et al., 2019) to analyzing student behaviors (Zhang et al., 2019; Feng et al., 2019), MOOC-related topics, tasks, and methods snowball in recent years. Despite the plentiful research interests, the resource from real MOOCs is still impoverished. ∗ † Equal Contribution. Corresponding author. Most of the publicly available datasets are designed for a specific task or method, e.g., Zhang et al.(2019) build a MOOC enrollment dataset for course recommendation and (Yu et al., 2019) is only for course concept expansion, which merely contains a subset of MOOC elements. Conseque"
2020.acl-main.285,I17-1088,1,0.833296,"p: //moocdata.cn/data/MOOCCube. 1 Introduction Massive open online courses (MOOCs) boom swiftly in recent years and have provided convenient education for over 100 million users worldwide (Shah, 2019). As a multi-media, large-scale online interactive system, MOOC is an excellent platform for advanced application research (Volery and Lord, 2000). Since MOOC is committed to helping students learn implicit knowledge concepts from diverse courses, many efforts from NLP and AI raise topics to build novel applications for assistance. From extracting course concepts and their prerequisite relations (Pan et al., 2017b; Roy et al., 2019; Li et al., 2019) to analyzing student behaviors (Zhang et al., 2019; Feng et al., 2019), MOOC-related topics, tasks, and methods snowball in recent years. Despite the plentiful research interests, the resource from real MOOCs is still impoverished. ∗ † Equal Contribution. Corresponding author. Most of the publicly available datasets are designed for a specific task or method, e.g., Zhang et al.(2019) build a MOOC enrollment dataset for course recommendation and (Yu et al., 2019) is only for course concept expansion, which merely contains a subset of MOOC elements. Conseque"
2020.acl-main.285,P19-1421,1,0.843803,"applications for assistance. From extracting course concepts and their prerequisite relations (Pan et al., 2017b; Roy et al., 2019; Li et al., 2019) to analyzing student behaviors (Zhang et al., 2019; Feng et al., 2019), MOOC-related topics, tasks, and methods snowball in recent years. Despite the plentiful research interests, the resource from real MOOCs is still impoverished. ∗ † Equal Contribution. Corresponding author. Most of the publicly available datasets are designed for a specific task or method, e.g., Zhang et al.(2019) build a MOOC enrollment dataset for course recommendation and (Yu et al., 2019) is only for course concept expansion, which merely contains a subset of MOOC elements. Consequently, they are not feasible enough to support ideas that demand more types of information. Moreover, these datasets only contain a small size of specific entities or relation instances, e.g., prerequisite relation of TutorialBank (Fabbri et al., 2018) only has 794 cases, making it insufficient for advanced models (such as graph neural networks). Therefore, we present MOOCCube, a data repository that integrates courses, concepts, student behaviors, relationships, and external resources. Compared with"
2020.acl-main.322,N18-1118,0,0.194803,"NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT FrEn task by careful use of noise generation and dropout methods. 1 Introduction Sentence-level neural machine translation (NMT) systems ignore the discourse phenomena and encode the individual source sentences with no use of contexts. In recent years, the context-aware models which learn contextual information from surrounding sentences have shown promising results in generating consistent and coherent translations (Zhang et al., 2018; Voita et al., 2018; Kim et al., 2019; Voita et al., 2019; Bawden et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019). There are two common approaches to incorporating contexts into NMT: the simple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale docume"
2020.acl-main.322,P18-1163,0,0.0186764,"lts are reported on small training datasets. Here we examine the effects of the noise-based method on different sized datasets. We trained the Inside-Random model and the Gaussiannoise model on different datasets consisting of 500K to 5M sentence pairs. Seen from Figure 2, the baseline model achieves better translation performance when we increase the data size. More interestingly, it is observed that Inside-Random and Gaussian-noise perform slightly better than 3515 BLEU 24 Base Inside ments mainly come from the robust training instead of the leverage of contextual information. Additionally, Cheng et al. (2018) added the Gaussian noise to word embedding to simulate lexical-level perturbations for more robust training. Differently, we added the Gaussian noise to the encoder output which plays a similar role with context-encoder, which provides additional training signals. Gaussian 22 20 18 500k 1M 2M 5M Data Volume Figure 2: BLEU scores vs. different data volume on ZhEn sentence-level dataset. dropout = 0.1 and σ = 0.3. the baseline, and the gaps gradually decrease with the volume increasing. This is reasonable that models trained on large-scale data may suffer less from the overfitting problem. 5 Re"
2020.acl-main.322,D19-6503,0,0.513442,"Missing"
2020.acl-main.322,P18-1118,0,0.497129,"ll. Also, we establish a new state-of-the-art on IWSLT FrEn task by careful use of noise generation and dropout methods. 1 Introduction Sentence-level neural machine translation (NMT) systems ignore the discourse phenomena and encode the individual source sentences with no use of contexts. In recent years, the context-aware models which learn contextual information from surrounding sentences have shown promising results in generating consistent and coherent translations (Zhang et al., 2018; Voita et al., 2018; Kim et al., 2019; Voita et al., 2019; Bawden et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019). There are two common approaches to incorporating contexts into NMT: the simple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale document corpora are not easily available. Most context-"
2020.acl-main.322,N19-1313,0,0.476841,"new state-of-the-art on IWSLT FrEn task by careful use of noise generation and dropout methods. 1 Introduction Sentence-level neural machine translation (NMT) systems ignore the discourse phenomena and encode the individual source sentences with no use of contexts. In recent years, the context-aware models which learn contextual information from surrounding sentences have shown promising results in generating consistent and coherent translations (Zhang et al., 2018; Voita et al., 2018; Kim et al., 2019; Voita et al., 2019; Bawden et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019). There are two common approaches to incorporating contexts into NMT: the simple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale document corpora are not easily available. Most context-aware NMT systems are"
2020.acl-main.322,D18-1325,0,0.417812,"the training data is small. Also, we establish a new state-of-the-art on IWSLT FrEn task by careful use of noise generation and dropout methods. 1 Introduction Sentence-level neural machine translation (NMT) systems ignore the discourse phenomena and encode the individual source sentences with no use of contexts. In recent years, the context-aware models which learn contextual information from surrounding sentences have shown promising results in generating consistent and coherent translations (Zhang et al., 2018; Voita et al., 2018; Kim et al., 2019; Voita et al., 2019; Bawden et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019). There are two common approaches to incorporating contexts into NMT: the simple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale document corpora are not easily"
2020.acl-main.322,N19-4009,0,0.0376768,"larger EnglishRussian (En-Ru) dataset provided by Voita et al. (2018), consisting of 2M sentence pairs selected from publicly available OpenSubtitles2018 corpus. The data statistics of each language pair can be seen in Table 1. We chose the Transformer-base model as the sentence-level baseline. The context encoder also used the same setting as the sentence-level baseline. We used Adam (Kingma and Ba, 2014) for optimization, and trained the systems on a single TiTan V GPU4 . The learning rate strategy was the same as that used in Vaswani et al. (2017). Our implementation was based on Fairseq (Ott et al., 2019). More details can be found in our repository5 . 4 Results and Discussion To study whether the context-encoder network captures contextual information in training, we present three types of context as the input of the contextencoder: • Context: the previous sentence of the current sentence. • Random: a sentence consisting of words randomly sampled from the source vocabulary. • Fixed: a fixed sentence input for contextencoder. 4.1 Baseline Selection Weight sharing (Voita et al., 2018) and two-stage training (Zhang et al., 2018) strategies have been proven essential to build strong context-aware"
2020.acl-main.322,P16-1162,0,0.0932916,"y available datasets. For ChineseEnglish (Zh-En) and French-English (Fr-En), we used Ted talks from IWSLT15 and IWSLT16 (Cettolo et al., 2012) evaluation campaigns as the training data. We validated on dev2010, and tested on tst2010-2013 (Zh-En), tst2010 (Fr-En) respectively. For English-German (En-De), we evaluated on WMT18 task 1 . For more convincing results, we also randomly sampled 500k/1M/2M/5M sentence pairs from the Chinese-English corpus provided by WMT2 and test on newstest2017. We preprocessed the sentences with Moses tokenizer3 except Chinese sentences and used byte pair encoding (Sennrich et al., 2016) with 32K merged operations to 3513 1 We used the News-Commentary v14 as the train set http://www.statmt.org/wmt19/translation-task.html 3 http://www.statmt.org/moses 2 Lang. Zh-En Fr-En En-De En-Ru Train Valid Test doc. sent. doc. sent. doc. sent. 1708 209K 8 887 56 5473 1803 220K 8 887 11 1664 8462 329K 130 3004 122 2998 - 2M - 10k - 10k Table 1: Details of datasets on different language pairs. segment words into sub-word units. The Chinese sentences were word segmented by the tool provided within NiuTrans (Xiao et al., 2012). For Fr-En and Zh-En tasks, we lowercased all sentences to obtain"
2020.acl-main.322,P19-1021,0,0.0194415,"tention network to model the contextual information. Maruf and Haffari (2018) built a context-aware NMT system using a memory network, and Maruf et al. (2019) encoded the whole document with selective attention network. However, most of the work mentioned above utilized more complex modules to capture the contextual information, which can be approximately regarded as multi-encoder systems. For a fair evaluation of context-aware NMT methods, we argue that one should build a strong enough sentence-level baseline with carefully regularized methods, especially on small datasets (Kim et al., 2019; Sennrich and Zhang, 2019). Beyond this, Bawden et al. (2018) and Voita et al. (2019) acknowledged that BLEU score is insufficient to evaluate context-aware models, and they emphasized that multi-encoder architectures alone had a limited capacity to exploit discourse-level context. In this work, we take a further step to explore the main cause, showing that the context-encoder acts more like a noise generator, and the BLEU improve6 Conclusions We have shown that, in multi-encoder contextaware NMT, the BLEU improvement is not attributed to the leverage of contextual information. Even though we feed the incorrect context"
2020.acl-main.322,W17-4811,0,0.431167,"source sentences with no use of contexts. In recent years, the context-aware models which learn contextual information from surrounding sentences have shown promising results in generating consistent and coherent translations (Zhang et al., 2018; Voita et al., 2018; Kim et al., 2019; Voita et al., 2019; Bawden et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019). There are two common approaches to incorporating contexts into NMT: the simple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale document corpora are not easily available. Most context-aware NMT systems are evaluated on small datasets and significant BLEU improvements are reported (Wang et al., 2017; Zhang et al., 2018; Tu et al., 2018). In our experiments, we find that the improvement persists if we feed pseudo sentences into the cont"
2020.acl-main.322,Q18-1029,0,0.12793,"nd the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale document corpora are not easily available. Most context-aware NMT systems are evaluated on small datasets and significant BLEU improvements are reported (Wang et al., 2017; Zhang et al., 2018; Tu et al., 2018). In our experiments, we find that the improvement persists if we feed pseudo sentences into the context encoder, especially when we train the system on small-scale data. A natural question here is: How much does the improvement come from the leverage of contextual information in multi-encoder? In this work, we aim to investigate what kinds of information that the context-aware model captures. We re-implement several widely used context-aware architectures based on the multiencoder paradigm, and do an in-depth analysis to study whether the context encoder captures the contextual information. B"
2020.acl-main.322,P19-1116,0,0.218892,"Missing"
2020.acl-main.322,P18-1117,0,0.518113,"sy training plays an important role in multi-encoder-based NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT FrEn task by careful use of noise generation and dropout methods. 1 Introduction Sentence-level neural machine translation (NMT) systems ignore the discourse phenomena and encode the individual source sentences with no use of contexts. In recent years, the context-aware models which learn contextual information from surrounding sentences have shown promising results in generating consistent and coherent translations (Zhang et al., 2018; Voita et al., 2018; Kim et al., 2019; Voita et al., 2019; Bawden et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019). There are two common approaches to incorporating contexts into NMT: the simple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter"
2020.acl-main.322,D17-1301,0,0.0505238,"ple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale document corpora are not easily available. Most context-aware NMT systems are evaluated on small datasets and significant BLEU improvements are reported (Wang et al., 2017; Zhang et al., 2018; Tu et al., 2018). In our experiments, we find that the improvement persists if we feed pseudo sentences into the context encoder, especially when we train the system on small-scale data. A natural question here is: How much does the improvement come from the leverage of contextual information in multi-encoder? In this work, we aim to investigate what kinds of information that the context-aware model captures. We re-implement several widely used context-aware architectures based on the multiencoder paradigm, and do an in-depth analysis to study whether the context encoder"
2020.acl-main.322,P12-3004,1,0.784377,"kenizer3 except Chinese sentences and used byte pair encoding (Sennrich et al., 2016) with 32K merged operations to 3513 1 We used the News-Commentary v14 as the train set http://www.statmt.org/wmt19/translation-task.html 3 http://www.statmt.org/moses 2 Lang. Zh-En Fr-En En-De En-Ru Train Valid Test doc. sent. doc. sent. doc. sent. 1708 209K 8 887 56 5473 1803 220K 8 887 11 1664 8462 329K 130 3004 122 2998 - 2M - 10k - 10k Table 1: Details of datasets on different language pairs. segment words into sub-word units. The Chinese sentences were word segmented by the tool provided within NiuTrans (Xiao et al., 2012). For Fr-En and Zh-En tasks, we lowercased all sentences to obtain comparable results with previous work. We also conducted experiments on a larger EnglishRussian (En-Ru) dataset provided by Voita et al. (2018), consisting of 2M sentence pairs selected from publicly available OpenSubtitles2018 corpus. The data statistics of each language pair can be seen in Table 1. We chose the Transformer-base model as the sentence-level baseline. The context encoder also used the same setting as the sentence-level baseline. We used Adam (Kingma and Ba, 2014) for optimization, and trained the systems on a si"
2020.acl-main.322,D18-1049,0,0.278296,"Missing"
2020.acl-main.322,J96-1002,0,\N,Missing
2020.acl-main.322,2012.eamt-1.60,0,\N,Missing
2020.acl-main.592,N18-1127,0,0.0542982,"Missing"
2020.acl-main.592,P19-1285,0,0.0631671,"Missing"
2020.acl-main.592,N19-1078,0,0.0113589,"1 × 10−4 . A larger window size (= 70) for BPTT was applied for the WikiText103. All experiments were run on a single NVIDIA 1080Ti. After the search process, we trained the learned architectures on the same data. To make it comparable with previous work, we copied the setup in Merity et al. (2018b). For PTB, the size of hidden layers was set as 850 and the training epoch was 3,000. While for the WikiText-103, we enlarged the number of hidden units to 2,500 and trained the model for 30 epochs. Additionally, we transferred the learned architecture to NER and chunking tasks with the setting in Akbik et al. (2019). We only modified the batch size to 24 and hidden size to 512. 5.2 5.2.1 Results Language Modeling tasks Here we report the perplexity scores, number of parameters and search cost on the PTB and WikiText103 datasets (Table 2). First of all, the joint ESS method improves the performance on language modeling tasks significantly. Moreover, it does not introduce many parameters. Our ESS method achieves state-of-the-art result on the PTB task. It outperforms the manually designed MogrifierLSTM by 4.5 perplexity scores on the test set. On 6634 Perplexity Perplexity 63.5 NAS 700 joint intra 550 400"
2020.acl-main.592,D19-1539,0,0.04279,"Missing"
2020.acl-main.592,D19-1367,1,0.86799,"ecture on other tasks. Again, it shows promising improvements on both NER and chunking benchmarks, and yields new state-of-the-art results on NER tasks. This indicates a promising line of research on largescale pre-learned architectures. More interestingly, it is observed that the inter-cell NAS is helpful in modeling rare words. For example, it yields a bigger improvement on the rare entity recognition task (WNUT) than that on the standard NER task (CoNLL). Related work NAS is a promising method toward AutoML (Hutter et al., 2018), and has been recently applied to NLP tasks (So et al., 2019; Jiang et al., 2019; Li and Talwalkar, 2019). Several research teams have investigated search strategies for NAS. The very early approaches adopted evolutionary algorithms to model the problem (Angeline et al., 1994; Stanley and Miikkulainen, 2002), while Bayesian and reinforcement learning methods made big progresses in computer vision and NLP later (Bergstra et al., 2013; Baker et al., 2017; Zoph and Le, 2017). More recently, gradient-based methods were successfully applied to language modeling and image classification based on RNNs and CNNs (Liu et al., 2019a). In particular, differentiable architecture searc"
2020.acl-main.592,N16-1030,0,0.143047,"Missing"
2020.acl-main.592,P19-1233,0,0.232166,"n recently applied to NLP tasks (So et al., 2019; Jiang et al., 2019; Li and Talwalkar, 2019). Several research teams have investigated search strategies for NAS. The very early approaches adopted evolutionary algorithms to model the problem (Angeline et al., 1994; Stanley and Miikkulainen, 2002), while Bayesian and reinforcement learning methods made big progresses in computer vision and NLP later (Bergstra et al., 2013; Baker et al., 2017; Zoph and Le, 2017). More recently, gradient-based methods were successfully applied to language modeling and image classification based on RNNs and CNNs (Liu et al., 2019a). In particular, differentiable architecture search has been of great interest to the community because of its efficiency and compatibility to off-the-shelf tools of gradient-based optimization. Despite of great success, previous studies restricted themselves to a small search space of neural networks. For example, most NAS systems were designed to find an architecture of recurrent or convolutional cell, but the remaining parts of the network are handcrafted (Zhong et al., 2018; Brock et al., 2018; Elsken et al., 2019). For a larger search space, Zoph et al. (2018) optimized the normal cell"
2020.acl-main.592,N18-1202,0,0.0569163,"Missing"
2020.acl-main.592,N18-1117,0,0.0638926,"Missing"
2020.acl-main.592,P19-1176,1,0.891921,"Missing"
2020.acl-main.592,C18-1255,1,0.834066,"ock et al., 2018; Elsken et al., 2019). For a larger search space, Zoph et al. (2018) optimized the normal cell (i.e., the cell that preserves the dimensionality of the input) and reduction cell (i.e., the cell that reduces the spatial dimension) simultaneously and explored a larger region of the space than the singlecell search. But it is still rare to see studies on the issue of search space though it is an important factor to NAS. On the other hand, it has been proven that the additional connections between cells help in RNN or Transformer-based models (He et al., 2016; Huang et al., 2017; Wang et al., 2018, 2019). These results motivate us to take a step toward the automatic design of inter-cell connections and thus search in a larger space of neural architectures. 3 Inter-Cell and Intra-Cell NAS In this work we use RNNs for description. We choose RNNs because of their effectiveness at preserving past inputs for sequential data processing tasks. Note that although we will restrict ourselves to RNNs for our experiments, the method and discussion here can be applied to other types of models. 6630 3.1 Problem Statement F (α, β) For a sequence of input vectors {x1 , ..., xT }, an RNN makes a cell o"
2020.acl-main.592,D18-1279,0,0.0507525,"Missing"
2020.acl-main.592,P18-4013,0,0.0521793,"Missing"
2020.coling-main.352,P17-1176,0,0.0171192,"rce NMT Koehn and Knowles (2017) show that NMT systems result in worse translation performance in lowresource scenarios. Researchers have developed promising approaches to this problem which mainly focus on introducing external knowledge to improve low-resource NMT performance. Data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017) alleviates this problem by generating pseudo parallel data. A large amount of auxiliary parallel corpus from other related language pairs can be used to pre-train model parameters and transfer to target language pair (Zoph et al., 2016; Chen et al., 2017; Gu et al., 2018a; Gu et al., 2018b; Kocmi and Bojar, 2018). Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018; Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019; Yang et al., 2020; Zhu et al., 2020). However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data. Sennrich and Zhang (2019) demonstrate the competitive NMT model can be trained with the appropriate hyperp"
2020.coling-main.352,P16-1185,0,0.0451091,"urce machine translation benchmarks and different sized data of WMT’16 En-De. 1 Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009"
2020.coling-main.352,D19-5611,0,0.0191281,"ve low-resource NMT performance. Data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017) alleviates this problem by generating pseudo parallel data. A large amount of auxiliary parallel corpus from other related language pairs can be used to pre-train model parameters and transfer to target language pair (Zoph et al., 2016; Chen et al., 2017; Gu et al., 2018a; Gu et al., 2018b; Kocmi and Bojar, 2018). Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018; Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019; Yang et al., 2020; Zhu et al., 2020). However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data. Sennrich and Zhang (2019) demonstrate the competitive NMT model can be trained with the appropriate hyperparameters in low-resource scenarios without any external resources. This is consistent with our motivation. The difference lies in that they focus on the model settings, and we explore the training strategy which utilizes bilingual data effectively fo"
2020.coling-main.352,N19-1423,0,0.167108,"ted impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009; Kumar et al., 2010; Shrivastava et al., 2016). For example, Arpit et al. (2017) have pointed out that deep neural networks tend to prioritize learnin"
2020.coling-main.352,P17-2090,0,0.170636,"tion benchmarks and different sized data of WMT’16 En-De. 1 Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009; Kumar et al., 2010;"
2020.coling-main.352,N18-1032,0,0.0170843,"nowles (2017) show that NMT systems result in worse translation performance in lowresource scenarios. Researchers have developed promising approaches to this problem which mainly focus on introducing external knowledge to improve low-resource NMT performance. Data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017) alleviates this problem by generating pseudo parallel data. A large amount of auxiliary parallel corpus from other related language pairs can be used to pre-train model parameters and transfer to target language pair (Zoph et al., 2016; Chen et al., 2017; Gu et al., 2018a; Gu et al., 2018b; Kocmi and Bojar, 2018). Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018; Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019; Yang et al., 2020; Zhu et al., 2020). However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data. Sennrich and Zhang (2019) demonstrate the competitive NMT model can be trained with the appropriate hyperparameters in low-"
2020.coling-main.352,D18-1398,0,0.0159213,"nowles (2017) show that NMT systems result in worse translation performance in lowresource scenarios. Researchers have developed promising approaches to this problem which mainly focus on introducing external knowledge to improve low-resource NMT performance. Data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017) alleviates this problem by generating pseudo parallel data. A large amount of auxiliary parallel corpus from other related language pairs can be used to pre-train model parameters and transfer to target language pair (Zoph et al., 2016; Chen et al., 2017; Gu et al., 2018a; Gu et al., 2018b; Kocmi and Bojar, 2018). Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018; Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019; Yang et al., 2020; Zhu et al., 2020). However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data. Sennrich and Zhang (2019) demonstrate the competitive NMT model can be trained with the appropriate hyperparameters in low-"
2020.coling-main.352,kocmi-bojar-2017-curriculum,0,0.183166,", the neural network explores harder samples effectively utilizing the previous knowledge learned from easier samples. Weinshall et al. (2018) demonstrate curriculum learning speeds up the learning process, especially at the beginning of training. Curriculum learning has been applied to several tasks, including language modeling (Bengio et al., 2009), image classification (Weinshall et al., 2018), and human attribute analysis (Wang et al., 2019). Curriculum learning has recently shown to train large-scale translation tasks efficiently and effectively by controlling the way of feeding samples. Kocmi and Bojar (2017) construct mini-batch contains sentences similar in length and linguistic phenomena, then organize the order by increased complexity in one epoch. Zhang et al. (2018) group the training samples into shards based on model-based and linguistic difficulty criteria, then train with hand-crafted curriculum schedules. Platanios et al. (2019) propose competence-based curriculum learning that select training samples based on sample difficulty and model competence. Kumar et al. (2019) use reinforcement learning to learn the curriculum automatically. Liu et al. (2020) propose a norm-based curriculum lea"
2020.coling-main.352,W18-6325,0,0.0829692,"Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009; Kumar et al., 2010; Shrivastava et al., 2016). For example, Arpit et al. (2017) ha"
2020.coling-main.352,W17-3204,0,0.151456,"e to learn. We test our DCL method in a Transformerbased system. Experimental results show that DCL outperforms several strong baselines on three low-resource machine translation benchmarks and different sized data of WMT’16 En-De. 1 Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the"
2020.coling-main.352,P07-2045,0,0.0212126,"Missing"
2020.coling-main.352,N19-1208,0,0.037664,"shown to train large-scale translation tasks efficiently and effectively by controlling the way of feeding samples. Kocmi and Bojar (2017) construct mini-batch contains sentences similar in length and linguistic phenomena, then organize the order by increased complexity in one epoch. Zhang et al. (2018) group the training samples into shards based on model-based and linguistic difficulty criteria, then train with hand-crafted curriculum schedules. Platanios et al. (2019) propose competence-based curriculum learning that select training samples based on sample difficulty and model competence. Kumar et al. (2019) use reinforcement learning to learn the curriculum automatically. Liu et al. (2020) propose a norm-based curriculum learning method based on the norm of word embedding to improve the efficiency of training an NMT system. Zhou et al. (2020) propose uncertainty-aware cur3978 riculum learning. To the best of our knowledge, this is the first comprehensive discussion of curriculum learning in a low-resource setup. On the other hand, curriculum learning is similar to data selection and data sampling methods. More similar work is that Wang et al. (2018) propose a dynamic sampling method that calcula"
2020.coling-main.352,2020.acl-main.41,0,0.459194,"n be found in recent papers (Bengio et al., 2009; Kumar et al., 2010; Shrivastava et al., 2016). For example, Arpit et al. (2017) have pointed out that deep neural networks tend to prioritize learning “easy” samples first. This agrees with the idea of curriculum learning (Bengio et al., 2009) in that an easy-to-hard learning strategy can yield better convergence for training. In NMT, curriculum learning is not new. Several research groups have applied it to large-scale translation tasks although few of them discuss the issue in a low-resource setup (Zhang et al., 2018; Platanios et al., 2019; Liu et al., 2020). The first question here is how to define the “difficulty” of a training sample. Previous work resorts to functions that produce a difficulty score for each training sample. This score is then used to reorder samples before training. But the methods of this type enforce a static scoring strategy and somehow disagrees with the fact that the sample difficulty might be changing when the model is ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 3977 Proceedings of the 28th In"
2020.coling-main.352,W18-6301,0,0.0209758,"dle the entire training set Dtrain . We measure the sample difficulty and model competence before every phase, then the |Dtrain |∗ c(t) easiest training samples are selected to train the NMT model. Benefited from dynamic measurement, the newlyupdated model has enough competence to learn samples with the appropriate difficulties. 4.3 Batching Goyal et al. (2017) address the optimization difficulty when training a neural network with large batches and exhibit good generalization. Large batches have demonstrated better performance in high-resource NMT tasks due to the lower gradient noise scale (Ott et al., 2018; Popel and Bojar, 2018). However, this method degrades the performance in low-resource tasks due to poorer generalization (Keskar et al., 2017). The dominant NMT batches the samples with similar lengths to speed training up (Khomenko et al., 2017). To reduce gradient noise, we propose a batching method which batches the samples based on similar difficulty in our curriculum learning method. Samples with similar difficulty indicate their losses fall at a similar rate. It might have a stabilized gradient direction and leads to better performance. 4.4 Training Strategy Zhang et al. (2018) define"
2020.coling-main.352,N19-4009,0,0.0240534,"other language pairs, we apply the same tokenization using Moses scripts (Koehn et al., 2007). We learn Byte-Pair Encoding (Sennrich et al., 2016b) subword segmentation with 10,000 merge operations for IWSLT datasets and 32,000 merge operations for WMT dataset. Especially, we learn BPE with a shared vocabulary for WMT dataset. Dataset # Train # Dev # Test IWSLT’15 En-Th IWSLT’15 En-Zh IWSLT’17 En-Ja WMT’16 En-De 85019 213377 226834 50K/100K/300K/4.5M 4904 6360 6861 6003 756 1080 1452 2999 Table 1: Number of sentences in each dataset. 5.2 Model Settings In all experiments, we use the fairseq (Ott et al., 2019)3 implementation of the Transformer. Inspired by (Sennrich and Zhang, 2019), we select different hyperparameters for each dataset to build a strong baseline in low-resource NMT4 . The model hyperparameters are shown in Table 2. We use the Adam optimizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98, and  = 10−9 . We increase the learning rate from 10−7 to 0.0005 for IWSLT datasets and 0.0007 for WMT dataset with linear warmup over the first 4000 steps and decay the learning rate by inverse square root way. For all experiments, we use the dropout of 0.1 and label smoothing ls = 0.1 for regul"
2020.coling-main.352,P02-1040,0,0.10688,". Output: A NMT model with dynamic curriculum learning. 1: t = 0; Randomly initialize the model parameters θ 0 ; 2: while t &lt; T do 3: Evaluate the Dtrain and get loss l(s; θ t ) of every training sample s; 4: for all s in Dtrain do 5: Measure the difficulty of s by Equation 6; 6: end for 7: Sort Dtrain based on the difficulty of every training sample; 8: Evaluate the Ddev and get BLEUt ; 9: Estimate the model competence c(t) by Equation 8; 10: Train the NMT model with the |Dtrain |∗ c(t) easiest training samples; 11: t ← t + 1; 12: end while method, the sentence-level evaluation metric BLEU (Papineni et al., 2002) presents more superiorities (Shen et al., 2016). In this way, the model competence avoids the prior hypothesis of the training process and is related to the real performance on the unseen dataset. Specifically, we pre-train a vanilla NMT model and record the best BLEU value on the development set as curriculum length BLEUT . The model competence is estimated as: c(t) = min(1, BLEUt (1 − c0 ) + c0 ) BLEUT ∗ β (8) where BLEUt is the BLEU at phase t, β ∈ (0, 1] is a coefficient to control the curriculum speed. With a smaller β, the progress of curriculum learning is faster and the model can be t"
2020.coling-main.352,N18-1202,0,0.173326,"n (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009; Kumar et al., 2010; Shrivastava et al., 2016). For example, Arpit et al. (2017) have pointed out that deep neural networks tend"
2020.coling-main.352,N19-1119,0,0.284985,"l parameters and transfer to target language pair (Zoph et al., 2016; Chen et al., 2017; Gu et al., 2018a; Gu et al., 2018b; Kocmi and Bojar, 2018). Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018; Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019; Yang et al., 2020; Zhu et al., 2020). However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data. Sennrich and Zhang (2019) demonstrate the competitive NMT model can be trained with the appropriate hyperparameters in low-resource scenarios without any external resources. This is consistent with our motivation. The difference lies in that they focus on the model settings, and we explore the training strategy which utilizes bilingual data effectively for low-resource NMT. 2.2 Curriculum Learning Curriculum learning (Bengio et al., 2009) is motivated by the learning strategy of biological organisms which orders the training samples in an easy-to-hard manner (Elman, 1993). Benefited from organized training, the neural"
2020.coling-main.352,P19-1021,0,0.0793673,"d to pre-train model parameters and transfer to target language pair (Zoph et al., 2016; Chen et al., 2017; Gu et al., 2018a; Gu et al., 2018b; Kocmi and Bojar, 2018). Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018; Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019; Yang et al., 2020; Zhu et al., 2020). However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data. Sennrich and Zhang (2019) demonstrate the competitive NMT model can be trained with the appropriate hyperparameters in low-resource scenarios without any external resources. This is consistent with our motivation. The difference lies in that they focus on the model settings, and we explore the training strategy which utilizes bilingual data effectively for low-resource NMT. 2.2 Curriculum Learning Curriculum learning (Bengio et al., 2009) is motivated by the learning strategy of biological organisms which orders the training samples in an easy-to-hard manner (Elman, 1993). Benefited from organized training, the neural"
2020.coling-main.352,P16-1009,0,0.44275,"elines on three low-resource machine translation benchmarks and different sized data of WMT’16 En-De. 1 Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers"
2020.coling-main.352,P16-1162,0,0.837162,"elines on three low-resource machine translation benchmarks and different sized data of WMT’16 En-De. 1 Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers"
2020.coling-main.352,P16-1159,0,0.0193216,"ing. 1: t = 0; Randomly initialize the model parameters θ 0 ; 2: while t &lt; T do 3: Evaluate the Dtrain and get loss l(s; θ t ) of every training sample s; 4: for all s in Dtrain do 5: Measure the difficulty of s by Equation 6; 6: end for 7: Sort Dtrain based on the difficulty of every training sample; 8: Evaluate the Ddev and get BLEUt ; 9: Estimate the model competence c(t) by Equation 8; 10: Train the NMT model with the |Dtrain |∗ c(t) easiest training samples; 11: t ← t + 1; 12: end while method, the sentence-level evaluation metric BLEU (Papineni et al., 2002) presents more superiorities (Shen et al., 2016). In this way, the model competence avoids the prior hypothesis of the training process and is related to the real performance on the unseen dataset. Specifically, we pre-train a vanilla NMT model and record the best BLEU value on the development set as curriculum length BLEUT . The model competence is estimated as: c(t) = min(1, BLEUt (1 − c0 ) + c0 ) BLEUT ∗ β (8) where BLEUt is the BLEU at phase t, β ∈ (0, 1] is a coefficient to control the curriculum speed. With a smaller β, the progress of curriculum learning is faster and the model can be trained on the entire training set earlier. We su"
2020.coling-main.352,P18-2048,0,0.0699047,"ed on sample difficulty and model competence. Kumar et al. (2019) use reinforcement learning to learn the curriculum automatically. Liu et al. (2020) propose a norm-based curriculum learning method based on the norm of word embedding to improve the efficiency of training an NMT system. Zhou et al. (2020) propose uncertainty-aware cur3978 riculum learning. To the best of our knowledge, this is the first comprehensive discussion of curriculum learning in a low-resource setup. On the other hand, curriculum learning is similar to data selection and data sampling methods. More similar work is that Wang et al. (2018) propose a dynamic sampling method that calculates the decline of loss during training to improve the NMT training efficiency. They start training from the full training set and then gradually decrease. This is contrary to the idea of curriculum learning. 3 Problem Definition Let Dtrain be the training corpus and |Dtrain |be the corpus size. s = (x, y) is a training sample in Dtrain , where x is the source sentence and y is the target sentence. NMT systems learn a conditional probability P (y|x): P (y|x; θ) = |y| Y P (yi |x, y&lt;i ; θ) (1) i=1 where |y |is the length of y, θ is a set of model pa"
2020.coling-main.352,P12-3004,1,0.796735,"t2013 as the development set. We use tst2015 as the test set for Zh-En and En-Th, tst2017 for En-Ja. To simulate different amounts of training resources, we randomly subsample 50K/100K/300K sentence pairs from WMT’16 English-German dataset and denote them as 50K/100K/300K. Furthermore, we also verify the effect of our method in the high-resource scenarios with all WMT’16 English-German training set (4.5M). We concatenate newstest2012 and newstest2013 as the development set and newstest2016 as the test set. Data statistics are shown in Table 1. We tokenize the Chinese sentences using NiuTrans (Xiao et al., 2012) word segmentation tookit and Japanese sentences using MeCab2 . For other language pairs, we apply the same tokenization using Moses scripts (Koehn et al., 2007). We learn Byte-Pair Encoding (Sennrich et al., 2016b) subword segmentation with 10,000 merge operations for IWSLT datasets and 32,000 merge operations for WMT dataset. Especially, we learn BPE with a shared vocabulary for WMT dataset. Dataset # Train # Dev # Test IWSLT’15 En-Th IWSLT’15 En-Zh IWSLT’17 En-Ja WMT’16 En-De 85019 213377 226834 50K/100K/300K/4.5M 4904 6360 6861 6003 756 1080 1452 2999 Table 1: Number of sentences in each d"
2020.coling-main.352,2020.acl-main.620,0,0.108174,"order by increased complexity in one epoch. Zhang et al. (2018) group the training samples into shards based on model-based and linguistic difficulty criteria, then train with hand-crafted curriculum schedules. Platanios et al. (2019) propose competence-based curriculum learning that select training samples based on sample difficulty and model competence. Kumar et al. (2019) use reinforcement learning to learn the curriculum automatically. Liu et al. (2020) propose a norm-based curriculum learning method based on the norm of word embedding to improve the efficiency of training an NMT system. Zhou et al. (2020) propose uncertainty-aware cur3978 riculum learning. To the best of our knowledge, this is the first comprehensive discussion of curriculum learning in a low-resource setup. On the other hand, curriculum learning is similar to data selection and data sampling methods. More similar work is that Wang et al. (2018) propose a dynamic sampling method that calculates the decline of loss during training to improve the NMT training efficiency. They start training from the full training set and then gradually decrease. This is contrary to the idea of curriculum learning. 3 Problem Definition Let Dtrain"
2020.coling-main.352,D16-1163,0,0.0682361,"of WMT’16 En-De. 1 Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009; Kumar et al., 2010; Shrivastava et al., 2016). For example"
2020.coling-main.377,D18-1338,0,0.288652,"ment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the model’s flexibility, such as requiring the number of the encoder layers to be the same as the decoder layers (He et al., 2018). Instead, in this work, we propose layer-wise multi-view learning to address this problem from the perspective of model training, without changing the model structure. Our method’s highlight is that ∗ Work done during Ph.D. study at Northeastern University. Corresponding author. This work is licensed under a Creati"
2020.coling-main.377,P19-1425,0,0.0227968,"rm Transformer with a 12-layer encoder. The results were measured on the IWSLT’14 De→En validation set. i=0 indicates the embedding layer. to the primary view by feeding an auxiliary view. Figure 2 shows that the vector similarity between the i-th encoder layer and the topmost layer grows as the increase of i. Therefore, we can regard the middle layer’s auxiliary view as a noisy version of the primary view. Training with noises has been widely proven to effectively improve the model’s generalization ability, such as dropout (Srivastava et al., 2014), adversarial training (Miyato et al., 2017; Cheng et al., 2019) etc. We also experimentally confirm that our model is more robust than the single view model when injecting random noises into the encoding representation. Dark knowledge. Typically, the prediction target in Lnll is a one-hot distribution: Only the gold label is 1, while the others are 0. A better alternative is label smoothing (Szegedy et al., 2016), which reduces the probability of gold label by  and redistributes  to all non-gold labels on average. However, label smoothing ignores the relationship between non-gold labels. For example, if the current ground-truth is “improve”, then “promo"
2020.coling-main.377,D18-1217,0,0.12509,"e input data (Xu et al., 2013), where one of the keys is view construction. In our scenario, a view is the hidden representation of the input sentence (an array of hidden vectors for each token, e.g., H ∗ ). In this work, we further propose to take the off-the-shelf output of each encoder layer (i.e., H (l) 2 ) to construct the redundant views. In NLP, previous implementations of view construction generally require the model to recalculate on the reconstructed input, such as using different orders of n-grams in the bag-of-word model (Matsubara et al., 2005), randomly masking the input tokens (Clark et al., 2018). As opposed to them, our method is very cheap as the by-product of the standard layer-by-layer encoding process. According to the definition of a view, we can regard the vanilla Transformer as a single-view model since only the topmost encoder layer (also called primary view) is fed to the decoder. In contrast, MV-Transformer additionally contains an intermediate layer Ma (1 ≤ Ma < M ) as the auxiliary view 3 . The choice of Ma can be arbitrary, and we discuss its effect in § 4.2. Our goal is to learn a better single model with the help of the auxiliary view. Partially shared parameters. In t"
2020.coling-main.377,D18-1457,0,0.322233,"); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the model’s flexibility, such as requiring the number of the"
2020.coling-main.377,D16-1139,0,0.0284317,"l ensemble MV-Transformer can be thought of as consisting of two models: A large model as the primary view, and a small model (with shallower encoder) as the auxiliary view. Here we compare with the other three methods of integrating multiple models: • Oneway-KD. Similar to Eq. 7 but detach the teacher’s prediction, i.e., gradients of the teacher’s prediction is not tracked, posing a one-way transfer from primary view to auxiliary view. • Seq-KD. Train the large model first and then translate the original training set by beam search to construct the distilled training set for the small model (Kim and Rush, 2016). • Ensemble. Independently train the two models and combine their predictions at inference time, e.g., by algorithmic average. Experiments are done on IWSLT’14 De→En, where the small model has a 3-layer encoder. As shown in Table 2, we can see that: (1) Oneway-KD suffers from severe degradation than MV when detaching the primary view, which indicates that making mutual learning between the primary view and auxiliary view is critical; (2) Seq-KD is almost useless or even badly hurts the performance (vs. Baseline (3L)), which is against the previous belief that Seq-KD helps the small model a lo"
2020.coling-main.377,W04-3250,0,0.0198279,"le 1: BLEU scores on five translation tasks. For (deep) Transformer, Aux./Pri. denotes the independently trained model with Ma /M -layer encoder respectively. For (deep) MV-Transformer, Aux./Pri. denotes the used view at inference time. ∆ denotes the improved BLEU score over the Transformer baseline when using multi-view learning at the same encoder depth. † denotes our implementation. Boldface and ∗ represent local and global best results, respectively. All the MV-Transformer results are significantly better (p<0.01) than the Transformer counterparts, measured by paired bootstrap resampling (Koehn, 2004). uses sacrebleu 10 , all other datasets are evaluated by multi-bleu.perl. Only De→En is reported by case insensitive BLEU. 3.2 Main results In addition to Transformer, we also re-implemented three previously proposed models that incorporate multiple encoder layers: multi-layer representation fusion (MLRF)(Wang et al., 2018), hierarchical aggregation (HieraAgg) (Dou et al., 2018), and transparent attention (TA) (Bapna et al., 2018). Table 1 shows the results of the five translation tasks on PostNorm and PreNorm. First, our MV-Transformer outperforms all baselines across the board. Specifically"
2020.coling-main.377,N18-1202,0,0.027903,"der finds a multi-layer representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016; Vaswani et al., 2017). However, such overreliance on the topmost encoding layer is problematic in two aspects: (1) Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks (Wang et al., 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network ("
2020.coling-main.377,W18-5431,0,0.0247007,"er representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016; Vaswani et al., 2017). However, such overreliance on the topmost encoding layer is problematic in two aspects: (1) Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks (Wang et al., 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-li"
2020.coling-main.377,P19-1021,0,0.0615377,"Missing"
2020.coling-main.377,C18-1255,1,0.872017,"odel. 1 Introduction Neural Machine Translation (NMT) adopts the encoder-decoder paradigm to model the entire translation process (Bahdanau et al., 2015). Specifically, the encoder finds a multi-layer representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016; Vaswani et al., 2017). However, such overreliance on the topmost encoding layer is problematic in two aspects: (1) Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks (Wang et al., 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; D"
2020.coling-main.377,P19-1176,1,0.913927,"ake full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the model’s flexibility, such as requiring the number of the encoder layers to"
2020.coling-main.377,P19-1624,0,0.0591465,"ake full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the model’s flexibility, such as requiring the number of the encoder layers to"
2020.coling-main.526,D16-1250,0,0.053542,"Missing"
2020.coling-main.526,P17-1042,0,0.0446501,"dings. Early work (Mikolov et al., 2013) relies on a seed dictionary to learn the source-target word embedding mapping. Xing et al. (2015) enforce the word embeddings to be of unit length and the orthogonal constraint on the linear mapping. Faruqui and Dyer (2014) on the other hand use Canonical Correlation Analysis (CCA) to project both source and target embeddings to a common low-dimensional space. Artetxe et al. (2016) show that the above methods are variants of the same objective. Smith et al. (2017) further show that this objective is closely related to the orthogonal Procrustes problem. Artetxe et al. (2017) obtain competitive results using the self-learning with a seed dictionary of only 25 word pairs. Adversarial methods. Zhang et al. (2017a) attempt the unsupervised bilingual dictionary induction task using the adversarial network. They use a generator to transform the source word embeddings to the target word embeddings and a discriminator to classify whether the given embedding is sampled from the true target word embeddings or generated by the generator. The generator is trained to fool the discriminator and the discriminator is trained to identify the generated word embeddings. In the end,"
2020.coling-main.526,P18-1073,0,0.416762,"E is the raw embeddings, n is the initial dimension 2: D←∅ . Set the dictionary to empty 3: while n ≤ 300 do . 300 is the dimension of the raw embeddings ¯ with dimension min(n, 300) using PCA and dropmax 4: Reduce E to E 5: if D = ∅ then ¯ 6: Run the initialization and the self-learning on E 7: else ¯ with D as the initial dictionary 8: Run the self-learning on E 9: end if 10: Translate 4K most frequent words and store the results in D 11: n←n×2 12: end while 13: return WX and WY 14: end procedure 5993 System En-Es Es-En En-Fr Fr-En En-It It-En En-De De-En MUSE (Lample et al., 2018) VecMap (Artetxe et al., 2018) C-MUSE (Hartmann et al., 2019) POSTPROC (Vulic et al., 2020) 83.20 82.33 82.33 82.73 83.66 84.60 84.73 85.47 82.66 82.47 82.40 82.73 82.39 83.60 83.73 84.00 78.20 79.13 79.13 79.13 77.90 79.80 79.67 80.60 75.10 75.33 75.27 76.00 72.93 74.27 74.20 75.33 Proposed method (dim 50) Proposed method (dim 100) Proposed method (dim 200) Proposed method (dim 300) 40.33 63.47 80.33 82.40 37.40 61.80 80.27 84.60 53.53 72.73 80.40 82.60 48.07 71.13 79.67 83.67 48.13 68.40 76.47 78.93 45.00 66.73 76.67 79.67 31.60 63.67 71.27 75.33 30.33 61.27 71.20 74.33 Table 3: The accuracy of different UBDI systems on"
2020.coling-main.526,P19-1494,0,0.437928,"to project the word embeddings to a lower-dimensional space. Then they apply a variant of the Iterative Closest Point method to find the source and target word embeddings mapping. Zhou et al. (2019) use normalizing flows to match the distribution of source and target word embeddings. But they rely on a numeral seed dictionary and the additional word frequency information. More recently, Hartmann et al. (2019) find that more robust results can be obtained by using the adversarial method to produce the initial dictionary for the advanced self-learning (with the stochastic dictionary induction). Artetxe et al. (2019) first generate a pseudo parallel corpus by an unsupervised machine translation system. They then extract a bilingual dictionary from the word alignment learned on that corpus. This simple process shows much better results than previous methods. Vulic et al. (2020) introduce a simple post-processing step to improve UBDI performance on distant language pairs. 5998 8 Conclusion In this work, we pinpoint in which part the representative UBDI system, VecMap, fails on distant language pairs. We identify a gap between the initialization performance and the minimum initialization performance for the"
2020.coling-main.526,Q17-1010,0,0.0639702,"hod 37.33 35.27 48.87 33.08 47.6 55.53 21.60 13.64 - IDR - Dropmax 40.20 0 41.07 0 47.64 0 34.18 0 0.13 0.33 0.20 0.27 0.07 20.73 0 14.25 Table 5: Ablation study of the proposed method on distant language pairs. reproduce the C-MUSE and POSTPROC system using Python. All these systems are run with the default hyper-parameters settings. Our method is based on the open-sourced VecMap implementation. We evaluate the baseline and our method on 4 similar language pairs, En-{Es, Fr, It, De}, and 4 distant language pairs, En-{Zh, Ja, Vi, Th}. We use the pretrained 300-dimensional fastText embeddings (Bojanowski et al., 2017)3 . The evaluation dictionaries are from Lample et al. (2018). We trim all vocabularies to the 20K most frequent words for training. Specifically, VecMap retains the top-4K words for the initialization, while others use the whole vocabulary. All experiments are done on a single Nvidia GTX 1080Ti. We run each experiment 3 times but with different random seeds, then pick the one with the highest cosine similarity of induced nearest neighbors as the final result. This unsupervised model selection criterion has shown to correlate well with UBDI performance (Hartmann et al., 2019). 5.2 Results Tabl"
2020.coling-main.526,E14-1049,0,0.110886,"Missing"
2020.coling-main.526,P19-1070,0,0.114822,"Missing"
2020.coling-main.526,D18-1043,0,0.387086,"tween the k largest Pkgraphs as the sum 2 eigenvalues λ of L1 and L2 , 4 = i=1 (λ1i − λ2i ) . The higher 4 is, the less similar the graphs are. As shown in Figure 4, the eigenvector similarity drops significantly when the dimension is reduced. This implies that the underlying nearest neighbor graphs of two languages become similar in low-dimensional space. This helps the algorithm to succeed in low-dimensional space as the assumption it makes is held. This phenomenon might be the result that many language pairs share some principle axes of variation, especially the ones with high eigenvalues (Hoshen and Wolf, 2018). 6.3 Hubness Cross-lingual word embeddings are known to suffer from the hubness problem (Lample et al., 2018), where a few points (known as hubs) are the nearest neighbors of many other points in high-dimensional spaces. As suggested in Section 4, distant language pairs might suffer more from this problem and the dropmax trick helps to alleviate this problem. Thus we would like to know to what extent the dropmax trick helps in the hubness problem. Here we adopt the hubness metric proposed by Ormazabal et al. (2019) for evaluation. This metric measures the percentage of target words H that are"
2020.coling-main.526,D19-1328,0,0.436928,"Missing"
2020.coling-main.526,N19-1386,0,0.0551632,"t al., 2017b) minimizes Earth-Mover’s distance between the transformed source and target embeddings distribution. Lample et al. (2018) improve the results by treating the dictionary produced by the adversarial network as the seed dictionary of the self-learning. To mitigate the hubness problem (Radovanovic et al., 2010), they propose an effective nearest neighbors retrieval method CSLS for dictionary induction. Xu et al. (2018) minimize Sinkhorn distance instead and introduce the circle consistency such that a source word embedding can be translated back after translating it to a target word. Mohiuddin and Joty (2019) extract latent codes from word embeddings and align words according to their latent codes. Non-adversarial methods. There is another line of research that focuses on a non-adversarial approach. Artetxe et al. (2018) propose a heuristic to induce an initial dictionary by exploiting the structural similarity of embeddings. They also propose the stochastic dictionary induction method, which significantly improves the robustness as well as the performance of self-learning. Hoshen and Wolf (2018) assume that many language pairs share some principle axes of variation. Therefore they first use PCA t"
2020.coling-main.526,2020.acl-main.318,0,0.0343256,"Missing"
2020.coling-main.526,P18-1072,0,0.0848626,"Missing"
2020.coling-main.526,D19-1449,0,0.041175,"Missing"
2020.coling-main.526,2020.repl4nlp-1.7,0,0.0309337,"word embeddings. But they rely on a numeral seed dictionary and the additional word frequency information. More recently, Hartmann et al. (2019) find that more robust results can be obtained by using the adversarial method to produce the initial dictionary for the advanced self-learning (with the stochastic dictionary induction). Artetxe et al. (2019) first generate a pseudo parallel corpus by an unsupervised machine translation system. They then extract a bilingual dictionary from the word alignment learned on that corpus. This simple process shows much better results than previous methods. Vulic et al. (2020) introduce a simple post-processing step to improve UBDI performance on distant language pairs. 5998 8 Conclusion In this work, we pinpoint in which part the representative UBDI system, VecMap, fails on distant language pairs. We identify a gap between the initialization performance and the minimum initialization performance for the self-learning to succeed, which is responsible for its failure. We propose Iterative Dimension Reduction to bridge this gap. Our method obtains substantial gains in distant language pairs without scarifying the performance of similar language pairs. It has shown to"
2020.coling-main.526,N15-1104,0,0.0667128,"Missing"
2020.coling-main.526,D18-1268,0,0.134918,"ator and the discriminator is trained to identify the generated word embeddings. In the end, the generator will be used to induce the bilingual dictionary. Their following work (Zhang et al., 2017b) minimizes Earth-Mover’s distance between the transformed source and target embeddings distribution. Lample et al. (2018) improve the results by treating the dictionary produced by the adversarial network as the seed dictionary of the self-learning. To mitigate the hubness problem (Radovanovic et al., 2010), they propose an effective nearest neighbors retrieval method CSLS for dictionary induction. Xu et al. (2018) minimize Sinkhorn distance instead and introduce the circle consistency such that a source word embedding can be translated back after translating it to a target word. Mohiuddin and Joty (2019) extract latent codes from word embeddings and align words according to their latent codes. Non-adversarial methods. There is another line of research that focuses on a non-adversarial approach. Artetxe et al. (2018) propose a heuristic to induce an initial dictionary by exploiting the structural similarity of embeddings. They also propose the stochastic dictionary induction method, which significantly"
2020.coling-main.526,P17-1179,0,0.161033,"Missing"
2020.coling-main.526,D17-1207,0,0.0475176,"Missing"
2020.coling-main.526,N19-1161,0,0.645134,"Missing"
2020.emnlp-main.72,D18-1338,0,0.367244,"ork design: both the encoder and decoder learn representations of word sequences by a stack of layers (Vaswani et al., 2017; Wu et al., 2016; Gehring et al., 2017), building on an interesting line of work in improving such models. The simplest of these increases the model capacity by widening the network, whereas more recent work shows benefits from stacking more layers on the encoder side. For example, for the popular Transformer model (Vaswani et al., 2017), deep systems have shown ∗ Corresponding author. promising BLEU improvements by either easing the information flow through the network (Bapna et al., 2018) or constraining the gradient norm across layers (Zhang et al., 2019; Xu et al., 2020; Liu et al., 2020). An improved system can even learn a 35-layer encoder, which is 5× deeper than that of vanilla Transformer (Wang et al., 2019). Although these methods have enabled training deep neural MT (NMT) models, questions remain as to the nature of the problem. The main question here is: why and how deep networks help in NMT. Note that previous work evaluates these systems in a black-box manner (i.e., BLEU score). It is thus natural to study how much a deep NMT system is able to learn that is differe"
2020.emnlp-main.72,2020.emnlp-main.463,0,0.11086,"wani et al., 2017; Wu et al., 2016; Gehring et al., 2017), building on an interesting line of work in improving such models. The simplest of these increases the model capacity by widening the network, whereas more recent work shows benefits from stacking more layers on the encoder side. For example, for the popular Transformer model (Vaswani et al., 2017), deep systems have shown ∗ Corresponding author. promising BLEU improvements by either easing the information flow through the network (Bapna et al., 2018) or constraining the gradient norm across layers (Zhang et al., 2019; Xu et al., 2020; Liu et al., 2020). An improved system can even learn a 35-layer encoder, which is 5× deeper than that of vanilla Transformer (Wang et al., 2019). Although these methods have enabled training deep neural MT (NMT) models, questions remain as to the nature of the problem. The main question here is: why and how deep networks help in NMT. Note that previous work evaluates these systems in a black-box manner (i.e., BLEU score). It is thus natural to study how much a deep NMT system is able to learn that is different from the shallow counterpart. Beyond this, training an extremely deep model is expensive although a n"
2020.emnlp-main.72,N19-1423,0,0.0435555,"g et al., 2020). 7.2 Efficient Training Methods When the model goes deeper, a challenge is the long training time for model convergence and the huge GPU cost. To alleviate this issue, several attempts have been made. Chang et al. (2018) proposed a multi-level training method by interpolating a residual block right after each existing block to accelerate the training of ResNets in computer version. Similarly, Gong et al. (2019) adopted a progressive stacking strategy to transfer the knowledge from a shallow model to a deep model, thus successfully trained a large-scale pre-training model BERT (Devlin et al., 2019) at a faster rate with comparable performance on downstream tasks. Unlike previous work, we only copy parameters of the g top-most layers and employ sparse connections across each stacking block in our shallow to deep training method, which has not been discussed yet in learning deep MT models. 8 Conclusions We have investigated the behaviour of the welltrained deep Transformer models and found that stacking more layers could improve the representation ability of NMT systems. Higher layers share more global information over different positions and adjacent layers behave similarly. Also, we hav"
2020.emnlp-main.72,W18-6301,0,0.0183709,"both the WMT En-De and En-Fr tasks. For a stronger system, we employed relative position representation (RPR) to strengthen the position embedding model (Shaw et al., 2018). We only used the relative key in each layer. We batched sentence pairs by approximate length, and limited input/output tokens per batch to 4, 096/GPU. Following the method of (Wang et al., 3 BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a +version.1.2.12 4 https://github.com/tensorflow/ tensor2tensor 999 WMT En-De Systems WMT En-Fr Params Time Speedup BLEU Sacrebleu Params Vaswani et al. (2017) (Big) Shaw et al. (2018) (Big) Ott et al. (2018) (Big) Wu et al. (2019b) (Big) Wang et al. (2019) (Deep) Wei et al. (2020) (Deep) Wei et al. (2020) (Big+Deep) Base (Pre-Norm) Big (Pre-Norm) 213M N/A 210M N/A 210M N/A 270M N/A 137M N/A 272M N/A 512M N/A 63M 4.79 210M 36.05 Deep-24L S DT-24L Deep-RPR-24L S DT-RPR-24L Deep-48L S DT-48L Deep-RPR-48L S DT-RPR-48L Deep-24L (Big) S DT-24L (Big) Deep-RPR-24L (Big) S DT-RPR-24L (Big) 118M 118M 118M 118M 194M 194M 194M 194M 437M 437M 437M 437M 8.66 6.16 9.80 6.71 16.38 10.65 19.58 11.75 37.41 18.31 38.80 18.51 Time Speedup BLEU Sacrebleu N/A N/A N/A N/A N/A N/A N/A N/A N/A 28.40 29.20 29.30 29.92 29."
2020.emnlp-main.72,P16-1162,0,0.0678739,"ning steps. Here step num and warmup steps are the current training step number and the warmup-step number. dmodel is the size of the layer output. See Figure 5 for a comparison of different learning schemas. 5 Experiments We report the experimental results on two widely used benchmarks - WMT’16 English-German (EnDe) and WMT’14 English-French (En-Fr). Data For the En-De task, we used the same preprocessed data with (Vaswani et al., 2017; Ott et al., 2019; Wang et al., 2019), consisting of approximate 4.5M tokenized sentence pairs. All sentences were segmented into sequences of sub-word units (Sennrich et al., 2016) with 32K merge operations using a vocabulary shared by source and target sides. We selected newstest2012+newstest2013 as validation data and newstest2014 as test data. For the En-Fr task, we replicated the setup of Vaswani et al. (2017) with 36M training sentence pairs from WMT14. We validated the En-Fr system on the union set of newstest2012 and newstest2013, and tested it on newstest2014. We filtered out sentences of more than 200 words and generated a shared vocabulary with 40K merge operations on both source and target side. We re-merged sub-word units to form complete words in the final"
2020.emnlp-main.72,N18-2074,0,0.0610634,"timizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.997, and  = 10−8 . We adopted the same learning rate schedule as the latest implementation of Tensor2Tensor4 . For deep models, the learning rate (lr) first increased linearly for warmup = 8, 000 steps from 1e−7 to 2e−3 . After warmup, the learning rate decayed proportionally to the inverse square root of the current step. For our S DT method presented in Section 4, we set h = g = p = 6 on both the WMT En-De and En-Fr tasks. For a stronger system, we employed relative position representation (RPR) to strengthen the position embedding model (Shaw et al., 2018). We only used the relative key in each layer. We batched sentence pairs by approximate length, and limited input/output tokens per batch to 4, 096/GPU. Following the method of (Wang et al., 3 BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a +version.1.2.12 4 https://github.com/tensorflow/ tensor2tensor 999 WMT En-De Systems WMT En-Fr Params Time Speedup BLEU Sacrebleu Params Vaswani et al. (2017) (Big) Shaw et al. (2018) (Big) Ott et al. (2018) (Big) Wu et al. (2019b) (Big) Wang et al. (2019) (Deep) Wei et al. (2020) (Deep) Wei et al. (2020) (Big+Deep) Base (Pre-Norm) Big (Pre-Norm) 213M N/A 210M"
2020.emnlp-main.72,P19-1176,1,0.170458,"he simplest of these increases the model capacity by widening the network, whereas more recent work shows benefits from stacking more layers on the encoder side. For example, for the popular Transformer model (Vaswani et al., 2017), deep systems have shown ∗ Corresponding author. promising BLEU improvements by either easing the information flow through the network (Bapna et al., 2018) or constraining the gradient norm across layers (Zhang et al., 2019; Xu et al., 2020; Liu et al., 2020). An improved system can even learn a 35-layer encoder, which is 5× deeper than that of vanilla Transformer (Wang et al., 2019). Although these methods have enabled training deep neural MT (NMT) models, questions remain as to the nature of the problem. The main question here is: why and how deep networks help in NMT. Note that previous work evaluates these systems in a black-box manner (i.e., BLEU score). It is thus natural to study how much a deep NMT system is able to learn that is different from the shallow counterpart. Beyond this, training an extremely deep model is expensive although a narrow-anddeep network can speed up training (Wang et al., 2019). For example, it takes us 3× longer time to train the model whe"
2020.emnlp-main.72,D19-1083,0,0.0629027,"d sequences by a stack of layers (Vaswani et al., 2017; Wu et al., 2016; Gehring et al., 2017), building on an interesting line of work in improving such models. The simplest of these increases the model capacity by widening the network, whereas more recent work shows benefits from stacking more layers on the encoder side. For example, for the popular Transformer model (Vaswani et al., 2017), deep systems have shown ∗ Corresponding author. promising BLEU improvements by either easing the information flow through the network (Bapna et al., 2018) or constraining the gradient norm across layers (Zhang et al., 2019; Xu et al., 2020; Liu et al., 2020). An improved system can even learn a 35-layer encoder, which is 5× deeper than that of vanilla Transformer (Wang et al., 2019). Although these methods have enabled training deep neural MT (NMT) models, questions remain as to the nature of the problem. The main question here is: why and how deep networks help in NMT. Note that previous work evaluates these systems in a black-box manner (i.e., BLEU score). It is thus natural to study how much a deep NMT system is able to learn that is different from the shallow counterpart. Beyond this, training an extremely"
2020.emnlp-main.72,C18-1255,1,0.664526,"ral Language Processing, pages 995–1005, c November 16–20, 2020. 2020 Association for Computational Linguistics si+1 L LayerNorm SubLayer L LayerNorm SubLayer si si (a) Pre-norm (b) Post-norm where si and si+1 are the output of sub-layers i and i+1. See Figure 1 (a) for the architecture of a pre-norm sub-layer. Pre-norm residual network has been found to be more efficient for back-propagation over a large number of layers than the post-norm architecture (Wang et al., 2019; Li et al., 2019). si+1 • Dense Connections. Direct layer connections can make easy access to distant layers in the stack (Wang et al., 2018; Bapna et al., 2018). Let {y1 , ..., yN } be the output of the stacked layers. We define a network G(y1 , ..., yj−1 ) that reads all layer output vectors prior to layer j and generates a new vector. Then, G(y1 , ..., yj−1 ) is regarded as a part of the input of layer j. In this way, we create direct connections from layers {1, ..., j − 1} to layer j. For G(·), we choose a linear model as in (Wang et al., 2019). Figure 1: Pre-norm and Post-norm sub-layer architectures. of information through the deep network but does not require large memory footprint as in dense networks. We experiment with t"
2020.emnlp-main.72,2020.acl-main.40,0,0.230095,"ight prevent us from exploiting deeper models in large-scale systems. In this paper, we explore why deep architectures work to render learning NMT models more effectively. By investigating the change of the hidden states in different layers, we find that new representations are learned by continually stacking layers on top of the base model. More stacked layers lead to a stronger model of representing the sentence. This particularly makes sense in the deep NMT scenario because it has been proven that deep models can benefit from an enriched representation (Wang et al., 2019; Wu et al., 2019b; Wei et al., 2020). In addition, the finding here inspires us to develop a simple yet efficient method to train a deep NMT encoder: we train model parameters from shallow to deep, rather than training the entire model from scratch. To stabilize training, we design a sparse linear combination method of connecting lower-level layers to the top. It makes efficient pass 995 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 995–1005, c November 16–20, 2020. 2020 Association for Computational Linguistics si+1 L LayerNorm SubLayer L LayerNorm SubLayer si si (a) Pre-norm (b)"
2020.emnlp-main.72,P19-1558,0,0.335375,"48 layers. This might prevent us from exploiting deeper models in large-scale systems. In this paper, we explore why deep architectures work to render learning NMT models more effectively. By investigating the change of the hidden states in different layers, we find that new representations are learned by continually stacking layers on top of the base model. More stacked layers lead to a stronger model of representing the sentence. This particularly makes sense in the deep NMT scenario because it has been proven that deep models can benefit from an enriched representation (Wang et al., 2019; Wu et al., 2019b; Wei et al., 2020). In addition, the finding here inspires us to develop a simple yet efficient method to train a deep NMT encoder: we train model parameters from shallow to deep, rather than training the entire model from scratch. To stabilize training, we design a sparse linear combination method of connecting lower-level layers to the top. It makes efficient pass 995 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 995–1005, c November 16–20, 2020. 2020 Association for Computational Linguistics si+1 L LayerNorm SubLayer L LayerNorm SubLayer si"
2020.emnlp-main.72,2020.acl-main.38,0,0.0602053,"Missing"
2020.findings-emnlp.385,P84-1044,0,0.330199,"Missing"
2020.findings-emnlp.385,D16-1139,0,0.172076,"tions, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method——LayerDrop. 1 Introduction As neural machine translation models become heavier and heavier (Vaswani et al., 2017), we have to resort to model compress techniques (e.g., knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016)) to deploy smaller models in devices with limited resources, such as mobile phones. However, a practical challenge is that the hardware conditions of different devices vary greatly. To ensure the same calculation latency, customizing distinct model sizes (e.g., depth, width) for different devices is necessary, which leads to huge model training and maintenance costs (Yu et al., 2019). For example, we need to distill the pretrained large model into N individual small models. The situation becomes worse for the industry when considering more translation directions and more frequent model iterat"
2020.findings-emnlp.385,N18-1202,0,0.0161069,"sub-network is unique and deterministic in our method, resulting in consistent sub-network used between training and inference. 4308 where t(i) is the number ofP tasks in which the i-th d ˆ φ(D) layer participates and t¯ = d∈D . The second is average layer distance (ALD), which requires the distance between adjacent layers in the subnetwork SN(d) = {La1 , La2 , . . . , Lad } should be large. For example, for a 6-layer network, if we want to build a 2-layer sub-network, it is unreasonable to select {L1 , L2 } directly because the features extracted by adjacent layers are semantically similar (Peters et al., 2018; Raganato and Tiedemann, 2018). Therefore, we use the average distance between layers in all sub-networks as the metric: P P |ai+1 − ai | Algorithm 1: Training Flexible Depth Model by Multi-Task Learning. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 pre-train MM −N on training data D; generate distillation data D0 by MM −N ; M0M −N ← MM −N ; for t in 1, 2, . . . , T do B ← sample batch from D0 ; gradient G ← 0; ˆ ) ⊗ φ(N ˆ ) do for (mi , ni ) in φ(M SNe , SNd ← F(mi , M ), F(ni , N ); Feed B into network (SNe , SNd ); Collect gradient g by Back-Propa.; G ← G + g; end Optimize M0M −N with gradient G; e"
2020.findings-emnlp.385,W18-5431,0,0.0232718,"e and deterministic in our method, resulting in consistent sub-network used between training and inference. 4308 where t(i) is the number ofP tasks in which the i-th d ˆ φ(D) layer participates and t¯ = d∈D . The second is average layer distance (ALD), which requires the distance between adjacent layers in the subnetwork SN(d) = {La1 , La2 , . . . , Lad } should be large. For example, for a 6-layer network, if we want to build a 2-layer sub-network, it is unreasonable to select {L1 , L2 } directly because the features extracted by adjacent layers are semantically similar (Peters et al., 2018; Raganato and Tiedemann, 2018). Therefore, we use the average distance between layers in all sub-networks as the metric: P P |ai+1 − ai | Algorithm 1: Training Flexible Depth Model by Multi-Task Learning. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 pre-train MM −N on training data D; generate distillation data D0 by MM −N ; M0M −N ← MM −N ; for t in 1, 2, . . . , T do B ← sample batch from D0 ; gradient G ← 0; ˆ ) ⊗ φ(N ˆ ) do for (mi , ni ) in φ(M SNe , SNd ← F(mi , M ), F(ni , N ); Feed B into network (SNe , SNd ); Collect gradient g by Back-Propa.; G ← G + g; end Optimize M0M −N with gradient G; end Return M0M −N ALD = where Z"
2020.findings-emnlp.385,P19-1176,1,0.908774,"e 1, we first demonstrate that when there is a large gap between the predefined layer dropout during training and the actual pruning ratio during inference, LayerDrop’s performance is poor. To solve this problem, we propose to use multitask learning to train a flexible depth model by treating each supported depth configuration as a task. We reduce the supported depth space for the aggressive model compression rate and propose an effective deterministic sub-network assignment method to eliminate the mismatch between training and inference in LayerDrop. Experimental results on deep Transformer (Wang et al., 2019) show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and LayerDrop. 2 2.1 Flexible depth model and LayerDrop Flexible depth model We first give the definition of flexible depth model (FDM): given a neural machine translation model MM −N whose encoder depth is M and decoder depth is N , in addition to (M,N), if MM −N can also simultaneously decode with different depth configurations (mi , ni )ki=1 where mi ≤ M and ni ≤ N and obtain the comparable performance with independently trained model Mmi −ni , we refer to MM −N"
2020.ngt-1.24,D16-1139,0,0.150864,"an opensource tensor toolkit written in C++ and CUDA 1 https://github.com/NiuTrans/NiuTensor based on dynamic computational graphs. NiuTensor is developed for facilitating NLP research and industrial deployment. The system is lightweight, high-quality, production-ready, and incorporated with the latest research ideas. We investigated with a different number of encoder/decoder layers to make trade-offs between translation performance and speed. We first trained several strong teacher models and then compressed teachers to compact student models via knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016). We find that using a deep encoder (up to 35 layers) and a shallow decoder (1 layer) gives reasonable improvements in speed while maintaining high translation quality. We also optimized the Transformer model decoding in engineering, such as caching the decoder’s attention results and using low precision data type. We present teacher models and training details in Section 2, then in Section 3 we describe how to obtain lightweight student models for efficient decoding. Optimizations for the decoding across different devices are discussed in Section 4. We show the details of our submissions and"
2020.ngt-1.24,W17-3207,0,0.044102,"Missing"
2020.ngt-1.24,P07-2045,0,0.0211435,"0 , . . . , yl ) G (y0 , . . . , yl ) = l X (l+1) Wk LN (yk ) (1) (2) k=0 where yl is the output of the lt h layer and W is the weights of different layers. We employed the dynamic linear combination of layers Transformer architecture incorporated with relative position representations as our teacher network, call it Transformer-DLCL-RPR. 2.2 Training Details We followed the constrained condition of the WMT 2019 English-German news translation task and used the same data filtering method as (Li et al., 2019). We also normalized punctuation and tokenized all sentences with the Moses tokenizer (Koehn et al., 2007). The training set contains about 10M sentences pairs after processed. In our systems, the data was tokenized, and jointly byte pair encoded (Sennrich et al., 2016) with 32K merge operations using a shared vocabulary. After decoding, we removed the BPE separators and de-tokenize all tokens. We trained four teacher models using newstest2018 as the development set with fairseq (Ott et al., 2019). Table 1 shows the results of all teacher models and their ensemble, where we report SacreBLEU (Post, 2018) and the model size. The difference between teachers is the number of encoder layers and whether"
2020.ngt-1.24,N19-4009,0,0.0194714,"n of the WMT 2019 English-German news translation task and used the same data filtering method as (Li et al., 2019). We also normalized punctuation and tokenized all sentences with the Moses tokenizer (Koehn et al., 2007). The training set contains about 10M sentences pairs after processed. In our systems, the data was tokenized, and jointly byte pair encoded (Sennrich et al., 2016) with 32K merge operations using a shared vocabulary. After decoding, we removed the BPE separators and de-tokenize all tokens. We trained four teacher models using newstest2018 as the development set with fairseq (Ott et al., 2019). Table 1 shows the results of all teacher models and their ensemble, where we report SacreBLEU (Post, 2018) and the model size. The difference between teachers is the number of encoder layers and whether they contain a dynamic linear combination of layers. All teachers have 6 decoder layers, 512 hidden dimensions, and 8 attention heads. We shared the source-side and target-side embeddings with the decoder output weights. The maximum relative length was 8, and the maximum position for both source and target was 1024. We used the Adam optimizer (Kingma Model Transformer-35-6 Transformer-35-6+DL"
2020.ngt-1.24,W18-6319,0,0.0115812,"). We also normalized punctuation and tokenized all sentences with the Moses tokenizer (Koehn et al., 2007). The training set contains about 10M sentences pairs after processed. In our systems, the data was tokenized, and jointly byte pair encoded (Sennrich et al., 2016) with 32K merge operations using a shared vocabulary. After decoding, we removed the BPE separators and de-tokenize all tokens. We trained four teacher models using newstest2018 as the development set with fairseq (Ott et al., 2019). Table 1 shows the results of all teacher models and their ensemble, where we report SacreBLEU (Post, 2018) and the model size. The difference between teachers is the number of encoder layers and whether they contain a dynamic linear combination of layers. All teachers have 6 decoder layers, 512 hidden dimensions, and 8 attention heads. We shared the source-side and target-side embeddings with the decoder output weights. The maximum relative length was 8, and the maximum position for both source and target was 1024. We used the Adam optimizer (Kingma Model Transformer-35-6 Transformer-35-6+DLCL Transformer-40-6 Transformer-40-6+DLCL Ensemble Param. 152M 152M 168M 168M 640M BLEU 43.3 43.7 44.5 43.9"
2020.ngt-1.24,W18-2715,0,0.0430122,"Missing"
2020.ngt-1.24,P16-1162,0,0.0585894,"mployed the dynamic linear combination of layers Transformer architecture incorporated with relative position representations as our teacher network, call it Transformer-DLCL-RPR. 2.2 Training Details We followed the constrained condition of the WMT 2019 English-German news translation task and used the same data filtering method as (Li et al., 2019). We also normalized punctuation and tokenized all sentences with the Moses tokenizer (Koehn et al., 2007). The training set contains about 10M sentences pairs after processed. In our systems, the data was tokenized, and jointly byte pair encoded (Sennrich et al., 2016) with 32K merge operations using a shared vocabulary. After decoding, we removed the BPE separators and de-tokenize all tokens. We trained four teacher models using newstest2018 as the development set with fairseq (Ott et al., 2019). Table 1 shows the results of all teacher models and their ensemble, where we report SacreBLEU (Post, 2018) and the model size. The difference between teachers is the number of encoder layers and whether they contain a dynamic linear combination of layers. All teachers have 6 decoder layers, 512 hidden dimensions, and 8 attention heads. We shared the source-side an"
2020.ngt-1.24,N18-2074,0,0.168728,"it for NLP tasks. We explored the combination of deep encoder and shallow decoder in Transformer models via model compression and knowledge distillation. The neural machine translation decoding also benefits from FP16 inference, attention caching, dynamic batching, and batch pruning. Our systems achieve promising results in both translation quality and efficiency, e.g., our fastest system can translate more than 40,000 tokens per second with an RTX 2080 Ti while maintaining 42.9 BLEU on newstest2018. 1 Introduction In recent years, the Transformer model and its variants (Vaswani et al., 2017; Shaw et al., 2018; So et al., 2019; Wu et al., 2019; Wang et al., 2019) have established state-of-the-art results on machine translation (MT) tasks. However, achieving high performance requires an enormous amount of computations (Strubell et al., 2019), limiting the deployment of these models on devices with constrained hardware resources. The efficiency task aims at developing MT systems to achieve not only translation accuracy but also memory efficiency or translation speed across different devices. This competition constraints systems to translate 1 million English sentences within 2 hours. Our goal is to i"
2020.ngt-1.24,P17-2091,0,0.0278461,"Missing"
2020.ngt-1.24,P19-1355,0,0.0295517,"ttention caching, dynamic batching, and batch pruning. Our systems achieve promising results in both translation quality and efficiency, e.g., our fastest system can translate more than 40,000 tokens per second with an RTX 2080 Ti while maintaining 42.9 BLEU on newstest2018. 1 Introduction In recent years, the Transformer model and its variants (Vaswani et al., 2017; Shaw et al., 2018; So et al., 2019; Wu et al., 2019; Wang et al., 2019) have established state-of-the-art results on machine translation (MT) tasks. However, achieving high performance requires an enormous amount of computations (Strubell et al., 2019), limiting the deployment of these models on devices with constrained hardware resources. The efficiency task aims at developing MT systems to achieve not only translation accuracy but also memory efficiency or translation speed across different devices. This competition constraints systems to translate 1 million English sentences within 2 hours. Our goal is to improve the quality of translations while maintaining enough speed. We participated in both CPUs and GPUs tracks in the shared task. Our system was built with NiuTensor, an opensource tensor toolkit written in C++ and CUDA 1 https://git"
2020.ngt-1.24,P19-1176,1,0.920074,"encoder and shallow decoder in Transformer models via model compression and knowledge distillation. The neural machine translation decoding also benefits from FP16 inference, attention caching, dynamic batching, and batch pruning. Our systems achieve promising results in both translation quality and efficiency, e.g., our fastest system can translate more than 40,000 tokens per second with an RTX 2080 Ti while maintaining 42.9 BLEU on newstest2018. 1 Introduction In recent years, the Transformer model and its variants (Vaswani et al., 2017; Shaw et al., 2018; So et al., 2019; Wu et al., 2019; Wang et al., 2019) have established state-of-the-art results on machine translation (MT) tasks. However, achieving high performance requires an enormous amount of computations (Strubell et al., 2019), limiting the deployment of these models on devices with constrained hardware resources. The efficiency task aims at developing MT systems to achieve not only translation accuracy but also memory efficiency or translation speed across different devices. This competition constraints systems to translate 1 million English sentences within 2 hours. Our goal is to improve the quality of translations while maintaining e"
2020.ngt-1.24,C18-1255,1,0.752758,"ang et al., 2019; Li et al., 2020) focus on designing new attention mechanisms and Transformer architectures. Shaw et al. (2018) extended the self-attention to consider the relative position representations or distances between words. Wu et al. (2019) replaced the self-attention components with lightweight and dynamic convolutions. Deep Transformer mod204 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 204–210 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d els also attracted a lot of attention. Wang et al. (2018) proposed a multi-layer representation fusion approach to learn a better representation from the stack. Wang et al. (2019) analyzed the high risk of gradient vanishing or exploring in the standard Transformer, which place the layer normalization (Ba et al., 2016) after the attention and feed-forward components. They showed that a deep Transformer model can surpass the big one by proper use of layer normalization and dynamic combinations of different layers. In their method, the input of layer l + 1 is defined by: xl+1 = G (y0 , . . . , yl ) G (y0 , . . . , yl ) = l X (l+1) Wk LN (yk ) (1) (2)"
2020.ngt-1.24,D19-1083,0,0.0348527,"Missing"
2020.wmt-1.117,C04-1046,0,0.164638,"models and multilingual pretraining methods significantly improve translation quality estimation performance. Our system achieved remarkable results in multiple level tasks, e.g., our submissions obtained the best results on all tracks in the sentence-level Direct Assessment task1 . 1 • We propose a simple strategy to convert document-level tasks into word- and sentencelevel tasks. • We explore effective ensemble methods for both word- and sentence-level predictions. Introduction Quality estimation (QE) evaluates the quality of machine translation output without human reference translations (Blatz et al., 2004). It has a wide range of applications in post-editing and quality control for machine translation. We participated in all tasks and language pairs at the WMT 2020 QE shared task2 , including sentence-level Direct Assessment tasks, word and sentence-level post-editing effort tasks, and document-level QE tasks. We investigated transfer learning and ensemble methods using recently proposed multilingual pre-trained models (Devlin et al., 2019; Conneau et al., 2020) as well as deep transformer models (Wang et al., 2019a). Our main contributions are as follows: • We apply multi-phase pretraining (Gu"
2020.wmt-1.117,2020.acl-main.747,0,0.128071,"Missing"
2020.wmt-1.117,N19-1423,0,0.176242,"nd sentence-level predictions. Introduction Quality estimation (QE) evaluates the quality of machine translation output without human reference translations (Blatz et al., 2004). It has a wide range of applications in post-editing and quality control for machine translation. We participated in all tasks and language pairs at the WMT 2020 QE shared task2 , including sentence-level Direct Assessment tasks, word and sentence-level post-editing effort tasks, and document-level QE tasks. We investigated transfer learning and ensemble methods using recently proposed multilingual pre-trained models (Devlin et al., 2019; Conneau et al., 2020) as well as deep transformer models (Wang et al., 2019a). Our main contributions are as follows: • We apply multi-phase pretraining (Gururangan et al., 2020) methods under both high- and low-resource settings to QE tasks. 1 Our number of submissions exceeded the daily or total limit. 2 http://www.statmt.org/wmt20/ quality-estimation-task.html Results on different level tasks show that our methods are very competitive. Our submissions achieved the best Pearson correlation on all language pairs of the sentence-level Direct Assessment task and the best results on English-Ch"
2020.wmt-1.117,2020.acl-main.740,0,0.0493901,"Missing"
2020.wmt-1.117,W19-5406,0,0.0713145,"Missing"
2020.wmt-1.117,P19-3020,0,0.0497136,"Missing"
2020.wmt-1.117,W17-4763,0,0.0651883,"ps://en.wikipedia.org/wiki/ Matthews_correlation_coefficient 1019 3.1 4 Datasets and Resources The labeled data consists of 7K sentences for training and 1K sentences for development for each language pair. We used the additional parallel data provided by the organizers to train predictors, containing about 20M En-Zh sentence pairs and 23M En-De sentence pairs after pre-processing with the NiuTrans SMT toolkit (Xiao et al., 2012). Pretrained language models include mBERT and XLM-R, were also used for Task 2. 3.2 Predictor-Estimator Models The predictor-estimator architecture and its variants (Kim et al., 2017; Kepler et al., 2019b) had established state-of-the-art on WMT QE tasks. The system consists of a word prediction module (predictor) trained from additional large-scale parallel corpora and a quality estimation module (estimator) trained from quality-annotated data. For the sentence-level tasks and target-side wordlevel tasks, we employed the official bi-RNN predictor-estimator trained with OpenKiwi (Kepler et al., 2019b) as the baseline. Similar to Wang et al. (2019b), we used NMT models trained with back-translation as predictors. The original predictor and estimator use RNNs to encode the"
2020.wmt-1.117,P16-1162,0,0.0348623,"tasks, we employed the official bi-RNN predictor-estimator trained with OpenKiwi (Kepler et al., 2019b) as the baseline. Similar to Wang et al. (2019b), we used NMT models trained with back-translation as predictors. The original predictor and estimator use RNNs to encode the source and predict tags or scores. We also implemented two transformer-based predictors which replace the RNN with transformer (Vaswani et al., 2017) or deep transformer architectures (Wang et al., 2019a; Li et al., 2019). We compared different tokenizing strategies such as word segmentation and byte pair encoding (BPE) (Sennrich et al., 2016) for all language pairs. 3.3 Multi-task learning The word- and sentence-level tasks are highly related to their annotations are commonly based on the HTER measure. We used a linear summation of sentence-level and target word-level objective losses as follows: L = Lmt.word + Lmt.gap + LHT ER (4) where the components denote the loss of targetword, target-gap, and predictions for HTER score. We also trained models using source sentence and origin/post-edited MT output to predict the source-side word level tags: LSRC = Lsrc−mt + Lsrc−pe (5) Document-Level QE Task This task aims to predict document"
2020.wmt-1.117,2020.wmt-1.79,0,0.0617894,"Missing"
2020.wmt-1.117,2020.acl-main.558,0,0.597273,"Missing"
2020.wmt-1.117,2016.amta-researchers.2,0,0.256191,"Missing"
2020.wmt-1.117,P19-1176,1,0.909104,"he quality of machine translation output without human reference translations (Blatz et al., 2004). It has a wide range of applications in post-editing and quality control for machine translation. We participated in all tasks and language pairs at the WMT 2020 QE shared task2 , including sentence-level Direct Assessment tasks, word and sentence-level post-editing effort tasks, and document-level QE tasks. We investigated transfer learning and ensemble methods using recently proposed multilingual pre-trained models (Devlin et al., 2019; Conneau et al., 2020) as well as deep transformer models (Wang et al., 2019a). Our main contributions are as follows: • We apply multi-phase pretraining (Gururangan et al., 2020) methods under both high- and low-resource settings to QE tasks. 1 Our number of submissions exceeded the daily or total limit. 2 http://www.statmt.org/wmt20/ quality-estimation-task.html Results on different level tasks show that our methods are very competitive. Our submissions achieved the best Pearson correlation on all language pairs of the sentence-level Direct Assessment task and the best results on English-Chinese post-editing effort tasks. We present methods for the sentence-level Di"
2020.wmt-1.117,P12-3004,1,0.778359,"earson’s correlation for the HTER prediction. There are two language pairs in both the word- and sentencelevel tasks, including English-German (En-De) and English-Chinese (En-Zh). 5 https://en.wikipedia.org/wiki/ Matthews_correlation_coefficient 1019 3.1 4 Datasets and Resources The labeled data consists of 7K sentences for training and 1K sentences for development for each language pair. We used the additional parallel data provided by the organizers to train predictors, containing about 20M En-Zh sentence pairs and 23M En-De sentence pairs after pre-processing with the NiuTrans SMT toolkit (Xiao et al., 2012). Pretrained language models include mBERT and XLM-R, were also used for Task 2. 3.2 Predictor-Estimator Models The predictor-estimator architecture and its variants (Kim et al., 2017; Kepler et al., 2019b) had established state-of-the-art on WMT QE tasks. The system consists of a word prediction module (predictor) trained from additional large-scale parallel corpora and a quality estimation module (estimator) trained from quality-annotated data. For the sentence-level tasks and target-side wordlevel tasks, we employed the official bi-RNN predictor-estimator trained with OpenKiwi (Kepler et al"
2020.wmt-1.117,W19-5410,0,0.0493715,"Missing"
2020.wmt-1.117,W19-5407,0,0.0375846,"Missing"
2020.wmt-1.37,W19-5304,0,0.0164178,"ments on the structures of the model in Table 1. And we kept six decoder layers unchanged because it only could gain a few improvements tough many model parameters increased. Deep Network: This model structure simply changes encoder layers, hidden size and other hyper-parameters based on vanilla Transformer. DLCL Network: For a deeper network, we employed DLCL (Wang et al., 2019) to get more diverse models. Filter size: This hyper-parameter represents the dimension size of feed-forward network (FFN) and simply increasing this could bring some improvements (Wang et al., 2018; Sun et al., 2019; Bawden et al., 2019). Notably, when using the deep Transformer architecture, the training time and model parameters will increase sharply with the augment of the FFN size. egy is a very effective approach to improve performance when the test set only composed of manual translations and we mainly reused (Li et al., 2019) iterative KD strategy to implement selflearning. Specifically, we designed a new iterative fine-tuning process which consists of three steps: 1)using ensemble models to decode valid and test source side sentences then fine-tune models with those pseudo data, 2) fine-tune with the valid set by a sm"
2020.wmt-1.37,P07-2045,0,0.0128684,"and wider Transformer (Vaswani et al., 2017) architectures to get reliable baselines, nucleus sampling (Holtzman et al., 2020) in backtranslation to generate more suitable pseudo bilingual sentences, more effectively fine-tuning strategy to adapt domain. Particularly in the lowresources tasks, {TA,IU}→EN, we built multilingual neural machine translation by using some similar language to get better performance and further 2 System Overview 2.1 Data Preprocessing and Filtering For EN→ZH and JA↔EN tasks, we first normalized the punctuation in Chinese and Japanese monolingual data by using Moses (Koehn et al., 2007) normalize-punctuation.perl script. English and Inuktitut sentences were segmented by Moses, while Chinese, Japanese and Tamil used NiuTrans (Xiao et al., 2012), MeCab1 and IndicNLP2 separately for word segmentation. After converting numbers and punctuation into English pattern, and then we normalized English words in Japanese sentences to Japanese by using Sudachi (Takaoka et al., 2018). As previous work (Wang et al., 2018) indicated that it’s important to clean data strictly, so this year 1 2 https://github.com/taku910/mecab https://github.com/anoopkunchukuttan/indic nlp library 338 Proceedi"
2020.wmt-1.37,W19-5325,1,0.881035,"Missing"
2020.wmt-1.37,2020.emnlp-main.72,1,0.848762,"Missing"
2020.wmt-1.37,W19-5333,0,0.0709345,"tle translation quality. RPR and relative length: The relative position representation (RPR) (Shaw et al., 2018) improves self-attention by adding relative position information. The relative length which we set 8 is the key parameter of this method. For choosing models to ensemble, we utilized the ensemble search method which used a script to traverse all possible combinations then recorded the best one. For JA↔EN, we chose 6 of 10 while other tasks were 4 of 10. 2.5 Iterative KD and Fine-tuning Sun et al. (2019) showed the self-learning strat340 2.6 Reranking For JA↔EN tasks, we followed the Ng et al. (2019), using a neural language model, and a reverse translation model. Different from the last year, we used several length penalties to generate more candidates. 2.7 Post Editing For tasks to the English side, we only confirmed the numbers whether to generate correctly by designing a rule-based script which generated two lists for source and target sentences separately. For EN→ZH, the strategy was the same as the last year Li et al. (2019) and particularly dealt with the name’s translation by using rules to delete the English name copy in Chinese sentences. For EN→JA task, we transferred English p"
2020.wmt-1.37,N19-4009,0,0.0212851,"Japanese pattern. ... Model n Model 2 Model 1 Start End Ensemble decode Pseudo data using {valid, test} Models Models (Original) (Original) Valid data Models Models (Domain (Domain tune) tune) Pseudo data using test Models Models (Dev (Dev tune) tune) Models Models (Test (Test self-learing) self-learing) Figure 1: Iterative fine-tuning process 3 3.1 Experiment Experiment Settings For all tasks, we implemented the TransformerBase as our baseline and all of our architectures were pre-normalize Wang et al. (2019) for stable training except Transformer-Big. We implemented models based on Fairseq (Ott et al., 2019) and trained on eight 2080Ti GPUs. We used Adam optimizer (Kingma and Ba, 2014) during training, β1 = 0.9, β2 = 0.997 for pre-normalize architectures and training batch was 2048 token while we accumulated gradient 4 times for achieving bigger batch size. We shuffled the training data before generate training batch and the training batch each epoch, so we didn’t consider the document information. The max learning rate and warmup-steps we set were 0.002 and 8000 separately for deep models but 0.0016 and 16000 for deep and wide models. During training, we used fp16 to accelerate training with few"
2020.wmt-1.37,P16-1162,0,0.0526379,"ion Task Corpus, TED Talks total six parallel data corpus about 14.35 million and News crawl, News Commentary, Common Crawl , TED Talks 4 Japanese monolingual data corpus about 1.7 billion. After the data filter, 12 million parallel data was left and 11 million selected by the neural language model was used as training data. Cleaning several billion low-quality monolingual data will cost too much time, so here we shuffled all the data then split it into dozens of parts, one of which was 20 million. Finally we used total eight of them, each piece was carefully cleaned. Before we also used BPE (Sennrich et al., 2016) models with 32,000 merge operations for both sides to reduce UNK size in vocabulary. We implemented back-translation two times, the first was beam search while the second was Nucleus Sampling to generate translations. Each time we selected 12 million mono data sampled from all the remaining data. Tough the second time didn’t increase significantly compared with the first time, the performance was further improved with the increase of the model parameters. Considering the training time, we finally chose 35 million training data on both sides. Notably, as the official stated that the test targe"
2020.wmt-1.37,N18-2074,0,0.0243475,"lid set by a small training batch and learning rate, 3) selflearning with in-domain data which chose by only test source side. Repeat these steps two or three times according to the increase of the valid score in the third step. Figure 1 shows these steps. Notably, for being consistent with the composition of the test set, we picked out the data that the source side is real while the target side is manual from the previous valid set. In this way, we found that iterative fine-tuning can promote news title translation quality. RPR and relative length: The relative position representation (RPR) (Shaw et al., 2018) improves self-attention by adding relative position information. The relative length which we set 8 is the key parameter of this method. For choosing models to ensemble, we utilized the ensemble search method which used a script to traverse all possible combinations then recorded the best one. For JA↔EN, we chose 6 of 10 while other tasks were 4 of 10. 2.5 Iterative KD and Fine-tuning Sun et al. (2019) showed the self-learning strat340 2.6 Reranking For JA↔EN tasks, we followed the Ng et al. (2019), using a neural language model, and a reverse translation model. Different from the last year,"
2020.wmt-1.37,W19-5341,0,0.0935248,"carried out experiments on the structures of the model in Table 1. And we kept six decoder layers unchanged because it only could gain a few improvements tough many model parameters increased. Deep Network: This model structure simply changes encoder layers, hidden size and other hyper-parameters based on vanilla Transformer. DLCL Network: For a deeper network, we employed DLCL (Wang et al., 2019) to get more diverse models. Filter size: This hyper-parameter represents the dimension size of feed-forward network (FFN) and simply increasing this could bring some improvements (Wang et al., 2018; Sun et al., 2019; Bawden et al., 2019). Notably, when using the deep Transformer architecture, the training time and model parameters will increase sharply with the augment of the FFN size. egy is a very effective approach to improve performance when the test set only composed of manual translations and we mainly reused (Li et al., 2019) iterative KD strategy to implement selflearning. Specifically, we designed a new iterative fine-tuning process which consists of three steps: 1)using ensemble models to decode valid and test source side sentences then fine-tune models with those pseudo data, 2) fine-tune with"
2020.wmt-1.37,D19-1167,0,0.0285461,"d gained 0.18 increase, because different models were too similar after fine-tuning. And we fixed the punctuation and the score improved 0.52 BLEU. During the post process, we fixed the number and punctuation translation. 3.5 Valid 12.8 14.2 19.2 20.9 22.8 23.4 23.6 23.8 TA→EN Results The Ta→EN task is similar to IU→EN but more complicated, because more data corpus and language can be used to build the multilingual system. Specifically, we total used {Hindi (HI), Kannada (KN), Malayalam (ML), Punjabi (PA), Telugu (TE), Urdu (UR)}→EN total six other languages, 17 million sentences according to Kudugunta et al. (2019)’s work showed similar languages with TA. From Table 5, it can be seen that using similar languages to build a multilingual system can indeed improve the performance. Also, using iterative back-translation is still an effective way but couldn’t add too much pseudo language data because this will make the real target language data account for the whole data was too small, which leaded to performance damage. During the backtranslation process, due to too many languages in one model, we followed Johnson et al. (2017)’s approach to build a reverse model to ensure translation quality. For the model"
2020.wmt-1.37,L18-1355,0,0.0265925,"o get better performance and further 2 System Overview 2.1 Data Preprocessing and Filtering For EN→ZH and JA↔EN tasks, we first normalized the punctuation in Chinese and Japanese monolingual data by using Moses (Koehn et al., 2007) normalize-punctuation.perl script. English and Inuktitut sentences were segmented by Moses, while Chinese, Japanese and Tamil used NiuTrans (Xiao et al., 2012), MeCab1 and IndicNLP2 separately for word segmentation. After converting numbers and punctuation into English pattern, and then we normalized English words in Japanese sentences to Japanese by using Sudachi (Takaoka et al., 2018). As previous work (Wang et al., 2018) indicated that it’s important to clean data strictly, so this year 1 2 https://github.com/taku910/mecab https://github.com/anoopkunchukuttan/indic nlp library 338 Proceedings of the 5th Conference on Machine Translation (WMT), pages 338–345 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics we used a stricter data filter scheme than Li et al. (2019) and the rules were following: • Filter sentences length ratio lower than 0.4 or upper than 3 and punctuation ratio more than 0.3. • Remove sentences that have the long word which co"
2020.wmt-1.37,W18-6430,1,0.892322,"he performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut→English and Tamil→English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance. 1 Introduction This paper describes the NiuTrans submissions to the WMT20 news tasks, including English→Chinese (EN→ZH), Tamil→English (TA→EN), Inuktitut→English (IU→EN) and Japanese↔English (JA↔EN) five directions and all of our systems were built with constrained data sets. Some useful methods in the WMT18 (Wang et al., 2018) and WMT19 (Li et al., 2019) submissions are also reused this time, such as model ensemble, knowledge distillation (KD) et al., and we explore some novel approaches this year. For this participation, we experimented with some deeper and wider Transformer (Vaswani et al., 2017) architectures to get reliable baselines, nucleus sampling (Holtzman et al., 2020) in backtranslation to generate more suitable pseudo bilingual sentences, more effectively fine-tuning strategy to adapt domain. Particularly in the lowresources tasks, {TA,IU}→EN, we built multilingual neural machine translation by using so"
2020.wmt-1.37,P19-1176,1,0.912468,"r learning, we utilized only one model which all the language shared the same parameters including word embeddings and vocab. Bilingual data were reused to fine-tune the model for adapting parameters to the target language after model convergence. 339 Model Tag Base Big Deep25 Deep25-filter Deep30-RPR DLCL35-RPR DLCL40-RPR Deep15-filter-768-RPR Depth 6 6 25 25 30 35 40 15 Hidden Size 512 1024 512 512 512 512 512 768 Filter Size 2048 4096 2048 4096 2048 2048 2048 4096 RPR Attention 7 7 7 7 3 3 3 3 Table 1: Transformer Architectures. 2.4 Model Architectures and Ensemble Inspired by deep network Wang et al. (2019), we tried to use simple deep, or deep and wide network architectures based on the Transformer to explore the relationship of performance and model parameters. We mainly carried out experiments on the structures of the model in Table 1. And we kept six decoder layers unchanged because it only could gain a few improvements tough many model parameters increased. Deep Network: This model structure simply changes encoder layers, hidden size and other hyper-parameters based on vanilla Transformer. DLCL Network: For a deeper network, we employed DLCL (Wang et al., 2019) to get more diverse models. F"
2020.wmt-1.37,P12-3004,1,0.800483,"e suitable pseudo bilingual sentences, more effectively fine-tuning strategy to adapt domain. Particularly in the lowresources tasks, {TA,IU}→EN, we built multilingual neural machine translation by using some similar language to get better performance and further 2 System Overview 2.1 Data Preprocessing and Filtering For EN→ZH and JA↔EN tasks, we first normalized the punctuation in Chinese and Japanese monolingual data by using Moses (Koehn et al., 2007) normalize-punctuation.perl script. English and Inuktitut sentences were segmented by Moses, while Chinese, Japanese and Tamil used NiuTrans (Xiao et al., 2012), MeCab1 and IndicNLP2 separately for word segmentation. After converting numbers and punctuation into English pattern, and then we normalized English words in Japanese sentences to Japanese by using Sudachi (Takaoka et al., 2018). As previous work (Wang et al., 2018) indicated that it’s important to clean data strictly, so this year 1 2 https://github.com/taku910/mecab https://github.com/anoopkunchukuttan/indic nlp library 338 Proceedings of the 5th Conference on Machine Translation (WMT), pages 338–345 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics we used a s"
2021.acl-long.162,R19-1050,0,0.0227496,"89.11 sent./s 289.80 sent./s 88.42 sent./s 225.46 sent./s 214.06 sent./s 247.90 sent./s 194.23 sent./s 199.74 sent./s 199.29 sent./s 158.29 sent./s 321.79 sent./s 412.91 sent./s 406.68 sent./s 281.97 sent./s 306.91 sent./s 309.11 sent./s Speedup 1.00× 2.34× 2.51× 2.60× 2.03× 2.09× 2.09× 1.00× 2.55× 2.42× 2.80× 2.20× 2.26× 2.25× 1.00× 2.03× 2.61× 2.57× 1.78× 1.94× 1.95× Table 1: Results of Transformer-base on different tasks (sent./s: translated sentences per second). 4.2 Model Setup Our baseline system is based on the open-source implementation of the Transformer model presented in Ott et al. (2019)’s work. For all machine translation tasks, we experiment with the Transformer-base (base) setting. We additionally run the Transformer-big (big) (Vaswani et al., 2017) and Transformer-deep (deep) (Wang et al., 2019; Zhang et al., 2020) settings on the large En-De dataset. All systems consist of a 6-layer encoder and a 6-layer decoder, except that the Transformerdeep encoder has 48 layers (depth) (Li et al., 2020). The embedding size (width) is set to 512 for Transformer-base/deep and 1,024 for Transformerbig. The FFN hidden size equals to 4× embedding size in all settings. We stop training un"
2021.acl-long.162,P19-1356,0,0.0175529,"ight matrices by the weight class they belong to, i.e., different weight classes clusters all their instantiations to form their own groups. In the previous example, the W1 weight class will form a group [T1 , T2 , · · · , TLt ], where each Ti is the W1 weight instance in the i-th FFN and Lt is the number of layers in the teacher network. These weight matrices are then used to generate the W1 weight instances in the student network. The parameter generator further divides each group into smaller subsets with weight matrices from adjacent layers, because the adjacent layers function similarly (Jawahar et al., 2019) and so as their weights. This way additionally makes the later transformation more light-weighted. Namely, given a group of Lt weight matrices, the parameter generator splits it into Ls subsets, where Ls is the number of layers in the student network. For example, the i-th subset of the group of  W1 weight class in the previous example will be T(i−1)∗Lt /Ls +1 , T(i−1)∗Lt /Ls +2 , · · · , Ti∗Lt /Ls . This subset is used to generate the weight matrix Si , which corresponds to W1 weight instance in the i-th FFN of the student network. 3.1.2 Weight Transformation Given a subset of teacher weig"
2021.acl-long.162,P07-1034,0,0.104151,"ew source of knowledge and a new way to leverage this knowledge. It transfers the knowledge in parameters of the teacher network to the student network via a parameter generator. Therefore, it is orthogonal to other knowledge distillation variants. 6.2 Transfer Learning Transfer learning aims at transferring knowledge from a source domain to a target domain. Based on what knowledge is transferred to the model in the target domain, transfer learning methods can be classified into three categories (Pan and Yang, 2010): instance-based methods reuse certain parts of the data in the source domain (Jiang and Zhai, 2007; Dai et al., 2007); feature-based methods use the representation from the model learned in the source domain as the input (Peters et al., 2018; Gao et al., 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019). Perhaps the most related work is Platanios et al. (2018)’s work. Their method falls into the parameter-based category. They use a universal parameter generator to share the knowledge among translation tasks. This parameter generator pro2083 duces a translation model"
2021.acl-long.162,N18-1202,0,0.0431075,"network via a parameter generator. Therefore, it is orthogonal to other knowledge distillation variants. 6.2 Transfer Learning Transfer learning aims at transferring knowledge from a source domain to a target domain. Based on what knowledge is transferred to the model in the target domain, transfer learning methods can be classified into three categories (Pan and Yang, 2010): instance-based methods reuse certain parts of the data in the source domain (Jiang and Zhai, 2007; Dai et al., 2007); feature-based methods use the representation from the model learned in the source domain as the input (Peters et al., 2018; Gao et al., 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019). Perhaps the most related work is Platanios et al. (2018)’s work. Their method falls into the parameter-based category. They use a universal parameter generator to share the knowledge among translation tasks. This parameter generator pro2083 duces a translation model from a given languagespecific embedding. Though we similarly employ the idea of a parameter generator, our weight distillation aims at transfe"
2021.acl-long.162,D18-1039,0,0.0266079,"transferred to the model in the target domain, transfer learning methods can be classified into three categories (Pan and Yang, 2010): instance-based methods reuse certain parts of the data in the source domain (Jiang and Zhai, 2007; Dai et al., 2007); feature-based methods use the representation from the model learned in the source domain as the input (Peters et al., 2018; Gao et al., 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019). Perhaps the most related work is Platanios et al. (2018)’s work. Their method falls into the parameter-based category. They use a universal parameter generator to share the knowledge among translation tasks. This parameter generator pro2083 duces a translation model from a given languagespecific embedding. Though we similarly employ the idea of a parameter generator, our weight distillation aims at transferring knowledge from one model to another rather than from one translation task to another. Therefore our parameter generator takes a model instead of a language-specific embedding as its input and is only used once. 7 Conclusion In this work, we"
2021.acl-long.162,2020.acl-main.686,0,0.0350292,"0.37 +0.76 +0.80 Valid 50.83 51.87 51.62 51.34 51.22 51.26 51.28 ∆BLEU 0.00 +1.04 +0.79 +0.51 +0.39 +0.43 +0.45 W D 1 2 3 4 5 6 256 512 BLEUKD/WD Params BLEUKD/WD Params 38.46/40.34 30M 43.51/45.39 65M 45.33/47.21 32M 50.02/50.45 72M 47.30/49.09 34M 51.18/51.99 80M 47.90/50.08 36M 51.05/52.05 87M 48.87/50.70 38M 52.15/52.00 94M 49.78/50.73 40M 52.40/53.09 102M Table 4: Ablation study of using different weight matrices solely. Table 5: Compression study with various depth (D) and width (W) of both the encoder and decoder. widths, we slice the teacher weight matrices to fit the student network (Wang et al., 2020). Table 3 shows that initializing the student networks with the teacher parameters improves KD, supporting our claim that knowledge in parameters is complementary to KD but missed. We also see that WD outperforms this simple initialization, which implies that using all teacher parameters helps to obtain a better student. the warmup steps than the learning rate. This is because more warmup steps will run the network with a high learning rate in a longer period. A high learning rate has been proven to be harmful as shown in the middle part of Fig. 3. 5.2 Sensitivity Analysis The left part of Fig"
2021.acl-long.162,P19-1176,1,0.909905,"./s 306.91 sent./s 309.11 sent./s Speedup 1.00× 2.34× 2.51× 2.60× 2.03× 2.09× 2.09× 1.00× 2.55× 2.42× 2.80× 2.20× 2.26× 2.25× 1.00× 2.03× 2.61× 2.57× 1.78× 1.94× 1.95× Table 1: Results of Transformer-base on different tasks (sent./s: translated sentences per second). 4.2 Model Setup Our baseline system is based on the open-source implementation of the Transformer model presented in Ott et al. (2019)’s work. For all machine translation tasks, we experiment with the Transformer-base (base) setting. We additionally run the Transformer-big (big) (Vaswani et al., 2017) and Transformer-deep (deep) (Wang et al., 2019; Zhang et al., 2020) settings on the large En-De dataset. All systems consist of a 6-layer encoder and a 6-layer decoder, except that the Transformerdeep encoder has 48 layers (depth) (Li et al., 2020). The embedding size (width) is set to 512 for Transformer-base/deep and 1,024 for Transformerbig. The FFN hidden size equals to 4× embedding size in all settings. We stop training until the model stops improving on the validation set. All experiments are done on 8 NVIDIA TITIAN V GPUs with mixed-precision training (Micikevicius et al., 2018). At test time, the model is decoded with a beam of wi"
2021.acl-long.162,P12-3004,1,0.76102,"ovided within NIST12 OpenMT1 . We choose the evaluation data of mt06 as the validation set, and mt08 as the test set. For the En-De task, we use the WMT14 EnglishGerman dataset (4.5M pairs). We share the source and target vocabularies. We choose newstest-2013 as the validation set and newstest-2014 as the test set. For all datasets, we tokenize every sentence using the script in the Moses toolkit and segment every word into subword units using Byte-Pair Encoding (Sennrich et al., 2016). The number of the BPE merge operations is set to 32K. We remove sentences with more than 250 subword units (Xiao et al., 2012). In addition, we evaluate the results using multi-bleu.perl. 1 LDC2000T46, LDC2000T47, LDC2000T50, LDC2003E14, LDC2005T10, LDC2002E18, LDC2007T09, LDC2004T08 2079 WMT16 En-Ro NIST12 Zh-En WMT14 En-De System Teacher T INY + KD + WD S MALL + KD + WD Teacher T INY + KD + WD S MALL + KD + WD Teacher T INY + KD + WD S MALL + KD + WD Depth 6 1 1 1 2 2 2 6 1 1 1 2 2 2 6 1 1 1 2 2 2 Width 512 256 256 256 512 512 512 512 256 256 256 512 512 512 512 256 256 256 512 512 512 Test 31.64 29.65 30.03 30.89 31.22 30.97 31.65 45.14 41.90 42.78 44.60 44.30 44.89 46.20 27.47 24.62 26.51 27.12 26.68 27.47 28.18"
2021.acl-long.204,2020.emnlp-main.644,0,0.062272,"9; Dong et al., 2021). Generally, MTL requires a careful design of the loss functions and more complicated architectures. In a similar way, more recent work pre-trains different components of the ST system, and consolidates them into one. For example, one can initialize the encoder with an ASR model, and initialize the decoder with the target-language side of an MT model (Berard et al., 2018; Bansal et al., 2019; Stoian et al., 2020). More sophisticated methods include better training and fine-tuning (Wang et al., 2020a,b), the shrink mechanism (Liu et al., 2020), the adversarial regularizer (Alinejad and Sarkar, 2020), and etc. Although pre-trained models have quickly become dominant in many NLP tasks, they are still found to underperform the cascaded model in ST. This motivates us to explore the reasons why this happens and methods to solve the problems accordingly. 3 Why is ST Encoding Difficult? Following previous work in end-to-end models (Berard et al., 2016; Weiss et al., 2017), we envision an encoding-decoding process in which an input sequence is encoded into a representation vector, and the vector is then decoded into an output sequence. 2620 ASR MT Below CTC 0.60 0.40 0.20 BLEU(ST) 0.60 Localness"
2021.acl-long.204,N18-1008,0,0.0443448,"Missing"
2021.acl-long.204,N19-1006,0,0.077468,"additional training data is allowed for ASR and MT. Introduction Corresponding author The source code is available at https://github.com/xuchen neu/SATE 1 Model the pipeline of translation (Duong et al., 2016; Berard et al., 2016; Weiss et al., 2017). Promising results on small-scale tasks are generally favorable. However, speech-to-translation paired data is scarce. Researchers typically use pre-trained Automatic Speech Recognition (ASR) and Machine Translation (MT) models to boost ST systems (Berard et al., 2018). For example, one can initialize the ST encoder using a large-scale ASR model (Bansal et al., 2019). But we note that, despite significant development effort, our end-to-end ST system with pre-trained models was not able to outperform the cascaded ST counterpart when the ASR and MT data size was orders of magnitude larger than that of ST (see Table 1). In this paper, we explore reasons why pretraining has been challenging in ST, and how pretrained ASR and MT models might be used together to improve ST. We find that the ST encoder plays both roles of acoustic encoding and textual encoding. This makes it problematic to view an ST encoder as either an individual ASR encoder or an individual MT"
2021.acl-long.204,N16-1109,0,0.272408,"reated ST as a pipeline of running an ASR system and an MT system sequentially (Ney, 1999; Mathias and Byrne, 2006; Schultz et al., 2004). This allows the use of off-the-shelf models, and was (and is) popular in practical ST systems. However, these systems were sensitive to the errors introduced by different component systems and the high latency of the long pipeline. As another stream in the ST area, end-to-end methods have been promising recently (Berard et al., 2016; Weiss et al., 2017; Berard et al., 2018). The rise of end-to-end ST can be traced back to the success of deep neural models (Duong et al., 2016). But, unlike other well-defined tasks in deep learning, annotated speech-to-translation data is scarce, which prevents well-trained ST models. A simple solution to this issue is data augmentation (Pino et al., 2019, 2020). This method is model-free but generating large-scale synthetic data is time consuming. As an alternative, researchers used multi-task learning (MTL) to robustly train the ST model so that it could benefit from additional guide signals (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021). Generally, MTL requires"
2021.acl-long.204,N19-1202,0,0.186725,"Missing"
2021.acl-long.204,2020.acl-demos.34,0,0.0147882,"the TED talks. We run the experiments on the English-German speech translation dataset of 400 hours speech with 230K utterances. We select the model on the dev set (1,408 utterances) and report results on the tstCOMMON set (2,641 utterances). Unrestricted Setting We use the additional ASR and MT data for pre-training. The 960 hours LibriSpeech ASR corpus is used for the English ASR model. We extract 10M sentences pairs from the WMT14 English-French and 18M sentence pairs from the Opensubtitle20183 English-German translation datasets. Preprocessing Followed the preprocessing recipes of ESPnet (Inaguma et al., 2020), we remove the utterances of more than 3,000 frames and augment speech data by speed perturbation with factors of 0.9, 1.0, and 1.1. The 80-channel log-mel filterbank coefficients with 3-dimensional pitch features are extracted for speech data. We use the lower-cased transcriptions without punctuations. The text is tokenized using the scripts of Moses (Koehn et al., 2007). We learn Byte-Pair Encoding (Sennrich et al., 2016) subword segmentation with 10,000 merge operations based on a shared source and target vocabulary for all datasets. All experiments are implemented based on the ESPnet tool"
2021.acl-long.204,L18-1001,0,0.0282569,", and MT models. Q(·|·) is the teacher distribution and P(·|·) is the student distribution. θASR , θCTC , θMT and θST are the model parameters. We can rewrite Eq. (8) to obtain a new loss: L = α · β · LCTC + (1 − β) · LKD CTC  +(1 − α) ·  γ · LTrans + (1 − γ) · LKD Trans (14) where both β and γ are the hyper-parameters that balance the preference between the teacher distribution and the ground truth. 2623 5 Experiments 5.1 5.2 Datasets and Preprocessing We consider restricted and unrestricted settings on speech translation tasks. We run experiments on the LibriSpeech English-French (En-Fr) (Kocabiyikoglu et al., 2018) and MuST-C EnglishGerman (En-De) (Gangi et al., 2019) corpora, which correspond to the low-resource and highresource datasets respectively. Available ASR and MT data is only from the ST data under the restricted setting. For comparison in practical scenarios, the unrestricted setting allows the additional data for ASR and MT models. LibriSpeech En-Fr Followed previous work, we use the clean speech translation training set of 100 hours, including 45K utterances and doubled translations of Google Translate. We select the model on the dev set (1,071 utterances) and report results on the test set"
2021.acl-long.204,P07-2045,0,0.0130899,"Missing"
2021.acl-long.204,N18-1202,0,0.014006,"ply use an ASR encoder as the acoustic encoder, and use an MT encoder as the textual encoder. Note that SATE is in general a cascaded model, in response to the pioneering work in ST (Ney, 1999). It can be seen as cascading the ASR and MT systems in an end-to-end fashion. 4.2 The Adaptor Now we turn to the design of the adaptor. Note that the pre-trained MT encoder assumes that the input is a word embedding sequence. Simply stacking the MT encoder and the ASR encoder obviously 2622 hssoft = P(π|hs ) · W e (9) Also, an informative representation should contain information in the original input (Peters et al., 2018). The output acoustic representation of the ASR encoder generally involves paralinguistic information, such as emotion, accent, and emphasis. They are not expressed in the form of text explicitly but might be helpful for translation. For example, the generation of the declarative or exclamatory sentences depends on the emotions of the speakers. We introduce a single-layer neural network to learn to map the acoustic representation to the latent space of the textual encoder, which preserves the acoustic information: A(hs , P (π|hs )) = λ · hsmap + (1 − λ) · hssoft (11) where λ is the weight of h"
2021.acl-long.204,W18-6319,0,0.0196814,"048 feed-forward size. For the unrestricted setting, we use the superior architecture Conformer (Gulati et al., 2020) on the ASR and ST tasks and widen the model by increasing the hidden size to 512 and attention heads to 8. The ASR5 and MT models pre-train with the additional data and fine-tune the model parameters with the task-specific data. During inference, we average the model parameters on the best 5 checkpoints based on the performance of the development set. We use beam search with a beam size of 4 for all models. Different from previous work, we report the case-sensitive SacreBLEU6 (Post, 2018) for future standardization comparison across papers. 5.3 http://opus.nlpl.eu/OpenSubtitles-v2018.php Results Results on MuST-C En-De Table 2 summaries the experimental results on the MuST-C En-De task. Under the restricted setting, the cascaded ST model translates the output of the ASR model, which degrades the performance compared with the MT model that translates from the reference transcription. The performance of the E2E ST baseline with pre-training is only slightly lower than the cascaded counterpart. SATE outperforms the baseline 4 https://github.com/espnet/espnet We use the pre-traine"
2021.acl-long.204,P16-1162,0,0.0364284,"the WMT14 English-French and 18M sentence pairs from the Opensubtitle20183 English-German translation datasets. Preprocessing Followed the preprocessing recipes of ESPnet (Inaguma et al., 2020), we remove the utterances of more than 3,000 frames and augment speech data by speed perturbation with factors of 0.9, 1.0, and 1.1. The 80-channel log-mel filterbank coefficients with 3-dimensional pitch features are extracted for speech data. We use the lower-cased transcriptions without punctuations. The text is tokenized using the scripts of Moses (Koehn et al., 2007). We learn Byte-Pair Encoding (Sennrich et al., 2016) subword segmentation with 10,000 merge operations based on a shared source and target vocabulary for all datasets. All experiments are implemented based on the ESPnet toolkit4 . We use the Adam optimizer with β1 = 0.9, β2 = 0.997 and adopt the default learning schedule in ESPnet. We apply dropout with a rate of 0.1 and label smoothing ls = 0.1 for regularization. For reducing the computational cost, the input speech features are processed by two convolutional layers, which have a stride of 2 × 2 and downsample the sequence by a factor of 4 (Weiss et al., 2017). The encoder consists of 12 lay"
2021.acl-long.204,Q19-1020,0,0.0177703,"ced back to the success of deep neural models (Duong et al., 2016). But, unlike other well-defined tasks in deep learning, annotated speech-to-translation data is scarce, which prevents well-trained ST models. A simple solution to this issue is data augmentation (Pino et al., 2019, 2020). This method is model-free but generating large-scale synthetic data is time consuming. As an alternative, researchers used multi-task learning (MTL) to robustly train the ST model so that it could benefit from additional guide signals (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021). Generally, MTL requires a careful design of the loss functions and more complicated architectures. In a similar way, more recent work pre-trains different components of the ST system, and consolidates them into one. For example, one can initialize the encoder with an ASR model, and initialize the decoder with the target-language side of an MT model (Berard et al., 2018; Bansal et al., 2019; Stoian et al., 2020). More sophisticated methods include better training and fine-tuning (Wang et al., 2020a,b), the shrink mechanism (Liu et al., 2020), the adversarial regularizer (A"
2021.acl-long.204,2020.acl-main.344,0,0.0182873,"Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021). Generally, MTL requires a careful design of the loss functions and more complicated architectures. In a similar way, more recent work pre-trains different components of the ST system, and consolidates them into one. For example, one can initialize the encoder with an ASR model, and initialize the decoder with the target-language side of an MT model (Berard et al., 2018; Bansal et al., 2019; Stoian et al., 2020). More sophisticated methods include better training and fine-tuning (Wang et al., 2020a,b), the shrink mechanism (Liu et al., 2020), the adversarial regularizer (Alinejad and Sarkar, 2020), and etc. Although pre-trained models have quickly become dominant in many NLP tasks, they are still found to underperform the cascaded model in ST. This motivates us to explore the reasons why this happens and methods to solve the problems accordingly. 3 Why is ST Encoding Difficult? Following previous work in end-to-end models (Berard et al., 2016; Weiss et al., 2017), we envision an encoding-decoding process in which an input sequence is encoded into a representation vector, and the vector"
2021.acl-long.204,D18-1475,0,0.118472,"e Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2619–2630 August 1–6, 2021. ©2021 Association for Computational Linguistics • Modeling deficiency: the MT encoder tries to capture long-distance dependency structures of language, but the ASR encoder focuses more on local dependencies in the input sequence. Since the ST encoder is initialized by the pre-trained ASR encoder (Berard et al., 2018), it fails to model large contexts in the utterance. But a large scope of representation learning is necessary for translation (Yang et al., 2018). • Representation inconsistency: on the decoder side of ST, the MT decoder is in general used to initialize the model. The assumption here is that the upstream component is an MT-like encoder, whereas the ST encoder actually behaves more like an ASR encoder. We address these problems by marrying the world of ASR encoding with the world of MT encoding. We propose a Stacked Acoustic-andTextual Encoding (SATE) method to cascade the ASR encoder and the MT encoder. It first reads and processes the sequence of acoustic features as a usual ASR encoder. Then an adaptor module passes the acoustic enco"
2021.findings-emnlp.357,2020.emnlp-main.211,0,0.0322675,"d Tu, 2020). In this paper we show that Transformer can be optimized for efficiency by a bag of techniques. These techniques are easy to implement and some of them have been tested in related studies. Here we focus on using them in combination for Transformer speedup which has not been well investigated. In particular, our work is based on the following facts: ∗ † Authors contributed equally. Corresponding author. Attention FFN Attention FFN Output Time (s) Baseline MDN 5.81 16.93 5.53 18.79 223.55 8.74 38.26 0.00 48.51 8.61 • The attention model does not need to be multiheaded in some cases (Behnke and Heafield, 2020). • The feedforward network sub-layer is removable (Hsu et al., 2020). • Knowledge Distillation (Hinton et al., 2015) is crucial to squeeze out the last potential. Removing some regularization measures like label smoothing (Szegedy et al., 2016) also helps when training such models. All these methods are compatible with popular Transformer codebases. In this work, we implement them on the decoder side because it occupies the inference time in many sequence generation tasks (Hsu et al., 2020; Kim et al., 2019). The end result is a simplified and fast Transformer decoder (see Table 1) - Mini-Dec"
2021.findings-emnlp.357,2020.findings-emnlp.433,0,0.0815094,"Missing"
2021.findings-emnlp.357,W17-4123,0,0.0274478,"r model on WMT14 En-De (FFN: the feedforward network, Output: the output projection). • The default Byte-Pair Encoding (BPE) setting (Sennrich et al., 2016) has a great impact on efficiency but is generally not optimal. • A shallow decoder (with a deeper encoder) is preferred for a fast system (Kasai et al., 2020). Introduction Standard implementation of Transformer (Vaswani et al., 2017) is not efficient for inference. Researchers have explored more efficient architectures (Zhang et al., 2018; Xiao et al., 2019; Li et al., 2021) or break the auto-regressive constraint in sequence generation (Gu et al., 2017). But most of these require significant updates of the model or hardware-dependent designs. It is still natural to ask whether the Transformer system can be optimized in a simple way (Hsu et al., 2020; Kasai et al., 2020; Kim et al., 2019; Wang and Tu, 2020). In this paper we show that Transformer can be optimized for efficiency by a bag of techniques. These techniques are easy to implement and some of them have been tested in related studies. Here we focus on using them in combination for Transformer speedup which has not been well investigated. In particular, our work is based on the followi"
2021.findings-emnlp.357,2020.sustainlp-1.7,0,0.133224,"erally not optimal. • A shallow decoder (with a deeper encoder) is preferred for a fast system (Kasai et al., 2020). Introduction Standard implementation of Transformer (Vaswani et al., 2017) is not efficient for inference. Researchers have explored more efficient architectures (Zhang et al., 2018; Xiao et al., 2019; Li et al., 2021) or break the auto-regressive constraint in sequence generation (Gu et al., 2017). But most of these require significant updates of the model or hardware-dependent designs. It is still natural to ask whether the Transformer system can be optimized in a simple way (Hsu et al., 2020; Kasai et al., 2020; Kim et al., 2019; Wang and Tu, 2020). In this paper we show that Transformer can be optimized for efficiency by a bag of techniques. These techniques are easy to implement and some of them have been tested in related studies. Here we focus on using them in combination for Transformer speedup which has not been well investigated. In particular, our work is based on the following facts: ∗ † Authors contributed equally. Corresponding author. Attention FFN Attention FFN Output Time (s) Baseline MDN 5.81 16.93 5.53 18.79 223.55 8.74 38.26 0.00 48.51 8.61 • The attention model"
2021.findings-emnlp.357,D19-5632,0,0.0197573,"ns in the inference speed are expected. 3 3.1 Experiments Setup We evaluate our methods on the WMT14 En-De, WMT14 En-Fr and NIST12 Zh-En machine translation tasks. We tokenize every sentence using a script from Moses and segment every word into subword units using BPE (Sennrich et al., 2016). The number of the BPE merge operations is set to 32K in the baseline and 10K for the target language Deep Configuration. Because our model is deep, in our model. In addition, we remove sentences we follow the deep model training setup provided with more than 250 subword units (Xiao et al., in Wang et al. (2019). 2012). We choose Transformer-base (Vaswani et al., Weight Distillation. We also adopt a simplified version of weight distillation (WD) for training 2017) as our baseline. The hyper-parameters of (Lin et al., 2020). This method initializes the stu- the Mini-Decoder Network (MDN) are the same as dent model with the corresponding weights from the baseline except for those mentioned in Section the teacher model, e.g., the first layer in the teacher 2.3. To produce consistent results for distillation, encoder is reused in the first layer in the student we choose the baseline with 10K BPE merges a"
2021.findings-emnlp.357,P16-1162,0,0.677523,"ransformer can be improved by combining some simple and hardware-agnostic methods, including tuning hyper-parameters, better design choices and training strategies. On the WMT news translation tasks, we improve the inference efficiency of a strong Transformer system by 3.80× on CPU and 2.52× on GPU. The code is publicly available at https://github.com/Lollipop321/minidecoder-network. 1 Encoder Decoder Table 1: Profiling results of the Transformer baseline and our model on WMT14 En-De (FFN: the feedforward network, Output: the output projection). • The default Byte-Pair Encoding (BPE) setting (Sennrich et al., 2016) has a great impact on efficiency but is generally not optimal. • A shallow decoder (with a deeper encoder) is preferred for a fast system (Kasai et al., 2020). Introduction Standard implementation of Transformer (Vaswani et al., 2017) is not efficient for inference. Researchers have explored more efficient architectures (Zhang et al., 2018; Xiao et al., 2019; Li et al., 2021) or break the auto-regressive constraint in sequence generation (Gu et al., 2017). But most of these require significant updates of the model or hardware-dependent designs. It is still natural to ask whether the Transform"
2021.findings-emnlp.357,P19-1580,0,0.0170754,"ach method in “Model Structure Updates” influences the model performance and inference speed after applying techniques from “Byte-Pair Encoding”. Shallow Decoder. Recent work has shown that the deep encoder and shallow decoder architecture is promising in system speedup (Kasai et al., 2020; Li et al., 2021). In this work we follow the same idea by restricting the decoder to a 1-layer network and stacking more encoder layers until the total number of parameters matches the baseline. Pruning Heads. Researchers have found that most heads could be safely pruned and leaving the performance intact (Voita et al., 2019; Michel et al., 2019). So we retain only one head in decoder attentions. Dropping FFN. Hsu et al. (2020) suggests that FFN is the least important component in the decoder. So we drop all FFNs in the decoder. After dropping FFN, there are only attentions and no other non-linearity except layer normalization in the model. Inspired by the observations in Table 1, the Transformer decoder can be improved for each of its components. In this section, we describe how to Factorizing Output. 4228 The weight matrix W System BLEU Baseline1 + Shallow Decoder + Pruning Heads + Dropping FFN + Factorizing Ou"
2021.findings-emnlp.357,P19-1176,1,0.796681,"ome deviations in the inference speed are expected. 3 3.1 Experiments Setup We evaluate our methods on the WMT14 En-De, WMT14 En-Fr and NIST12 Zh-En machine translation tasks. We tokenize every sentence using a script from Moses and segment every word into subword units using BPE (Sennrich et al., 2016). The number of the BPE merge operations is set to 32K in the baseline and 10K for the target language Deep Configuration. Because our model is deep, in our model. In addition, we remove sentences we follow the deep model training setup provided with more than 250 subword units (Xiao et al., in Wang et al. (2019). 2012). We choose Transformer-base (Vaswani et al., Weight Distillation. We also adopt a simplified version of weight distillation (WD) for training 2017) as our baseline. The hyper-parameters of (Lin et al., 2020). This method initializes the stu- the Mini-Decoder Network (MDN) are the same as dent model with the corresponding weights from the baseline except for those mentioned in Section the teacher model, e.g., the first layer in the teacher 2.3. To produce consistent results for distillation, encoder is reused in the first layer in the student we choose the baseline with 10K BPE merges a"
2021.findings-emnlp.357,2020.coling-main.529,0,0.0329191,"ncoder) is preferred for a fast system (Kasai et al., 2020). Introduction Standard implementation of Transformer (Vaswani et al., 2017) is not efficient for inference. Researchers have explored more efficient architectures (Zhang et al., 2018; Xiao et al., 2019; Li et al., 2021) or break the auto-regressive constraint in sequence generation (Gu et al., 2017). But most of these require significant updates of the model or hardware-dependent designs. It is still natural to ask whether the Transformer system can be optimized in a simple way (Hsu et al., 2020; Kasai et al., 2020; Kim et al., 2019; Wang and Tu, 2020). In this paper we show that Transformer can be optimized for efficiency by a bag of techniques. These techniques are easy to implement and some of them have been tested in related studies. Here we focus on using them in combination for Transformer speedup which has not been well investigated. In particular, our work is based on the following facts: ∗ † Authors contributed equally. Corresponding author. Attention FFN Attention FFN Output Time (s) Baseline MDN 5.81 16.93 5.53 18.79 223.55 8.74 38.26 0.00 48.51 8.61 • The attention model does not need to be multiheaded in some cases (Behnke and"
2021.findings-emnlp.357,P12-3004,1,0.665214,"Missing"
2021.findings-emnlp.357,P18-1166,0,0.104635,"hub.com/Lollipop321/minidecoder-network. 1 Encoder Decoder Table 1: Profiling results of the Transformer baseline and our model on WMT14 En-De (FFN: the feedforward network, Output: the output projection). • The default Byte-Pair Encoding (BPE) setting (Sennrich et al., 2016) has a great impact on efficiency but is generally not optimal. • A shallow decoder (with a deeper encoder) is preferred for a fast system (Kasai et al., 2020). Introduction Standard implementation of Transformer (Vaswani et al., 2017) is not efficient for inference. Researchers have explored more efficient architectures (Zhang et al., 2018; Xiao et al., 2019; Li et al., 2021) or break the auto-regressive constraint in sequence generation (Gu et al., 2017). But most of these require significant updates of the model or hardware-dependent designs. It is still natural to ask whether the Transformer system can be optimized in a simple way (Hsu et al., 2020; Kasai et al., 2020; Kim et al., 2019; Wang and Tu, 2020). In this paper we show that Transformer can be optimized for efficiency by a bag of techniques. These techniques are easy to implement and some of them have been tested in related studies. Here we focus on using them in com"
2021.iwslt-1.9,N19-1202,0,0.0653287,"Missing"
2021.iwslt-1.9,P19-1285,0,0.0140972,"f CTC objective α is set to 0.3 for all ASR and ST models. The model architecture is showed in Figure 14 . 3.2 Conformer Conformer (Gulati et al., 2020) models both local and global dependencies by combining the Convolutional Neural Network and Transformers. It has shown superiority and achieved promising results in ASR tasks. We replace the Transformer blocks in the encoder by the conformer blocks, which include two macaron-like feed-forward networks, multihead self attention modules, and convolution modules. Note that we use the RPE proposed in Shaw et al. (2018) rather than Transformer-XL (Dai et al., 2019). Model Architecture In this section, we describe the baseline model and the architecture improvements. Then, the experimental results are shown to demonstrate the effectiveness. 3.1 Position Embedding Acoustic Feature Table 1: Data statistics of the ASR, MT, and ST corpora. 3 Masked Multi-Head Attention Baseline Model Our system is based on deep Transformer (Vaswani et al., 2017) implemented on the fairseq toolkit (Ott et al., 2019). Furthermore, dynamic linear combination of layers (DLCL) (Wang et al., 2019) method is employed to train the deep model effectively (Li et al., 2020a,b). To redu"
2021.iwslt-1.9,N16-1109,0,0.018984,"English transcription. We use the allowed English-German translation data from WMT 2020 (Barrault et al., 2020) and OpenSubtitles2018 (Lison and Tiedemann, 2016). We filter the training bilingual data followed Li et al. (2019), which includes length ratio, language detection, and so on. Speech translation (ST) aims to learn models that can predict, given some speech in the source language, the translation into the target language. Endto-end (E2E) approaches have become popular recently for its ability to free designers from cascading different systems and shorten the pipeline of translation (Duong et al., 2016; Berard et al., 2016; Weiss et al., 2017). This paper describes the submission of the NiuTrans E2E ST system for the IWSLT 2021 (Anastasopoulos et al., 2021) offline task, which translates from the English audio to the German text translation directly without intermediate transcription. Our baseline model is based on the DLCL Transformer (Vaswani et al., 2017; Wang et al., 2019) model with Connectionist Temporal Classification (CTC) (Graves et al., 2006) loss on the encoders (Bahar et al., 2019). We enhance it with the superior model architecture Conformer (Gulati et al., 1 We only described"
2021.iwslt-1.9,D16-1139,0,0.0604143,"Missing"
2021.iwslt-1.9,2020.emnlp-main.72,1,0.776088,"former-XL (Dai et al., 2019). Model Architecture In this section, we describe the baseline model and the architecture improvements. Then, the experimental results are shown to demonstrate the effectiveness. 3.1 Position Embedding Acoustic Feature Table 1: Data statistics of the ASR, MT, and ST corpora. 3 Masked Multi-Head Attention Baseline Model Our system is based on deep Transformer (Vaswani et al., 2017) implemented on the fairseq toolkit (Ott et al., 2019). Furthermore, dynamic linear combination of layers (DLCL) (Wang et al., 2019) method is employed to train the deep model effectively (Li et al., 2020a,b). To reduce the computational cost, the input speech features are processed by two convolutional layers, which have a stride of 2. This downsamples 3.3 Relative Position Encoding Due to the non-sequential modeling of the original self attention modules, the vanilla Transformer employs the position embedding by a deterministic sinusoidal function to indicate the absolute position of each input element (Vaswani et al., 2017). However, this scheme is far from ideal for acoustic modeling (Pham et al., 2020). 2 We use the latest MusST-C v2 dataset released by IWSLT 2021. 3 http://i13pc106.ira.u"
2021.iwslt-1.9,W18-6319,0,0.0138889,"yers. The decoder consists of 6 Transformer layers. Each layer comprises 512 hidden units, 8 attention heads, and 2048 feed-forward size. Pre-norm is applied for training a deep model. The weight of CTC objective α for multitask learning is set to 0.3 for all models. All the models are trained for 50 epochs on one machine with 8 NVIDIA 2080Ti GPUs. During inference, we average the model parameters on the final 10 checkpoints. We use beam search with a beam size of 5 for all models. The coefficient of length normalization is tuned on the development set. We report the case-sensitive SacreBLEU (Post, 2018) on the MuST-C tst-COMMON set, IWSLT tst2019 and tst2020 test set. The organizers provide the segmentation of the test sets and allow the participants to use the own segmentation. We simply use the segmentation provided by the WerRTCVAD8 toolkit. Ensemble Decoding Ensemble decoding is an effective method to improve performance by integrating the predictions from multiple models. It has been proved in the WMT competitions (Wang et al., 2018; Li et al., 2019). In our systems, we train multiple ST models with different training data for diverse ensemble decoding. The models are chosen based on th"
2021.iwslt-1.9,N18-2074,0,0.128356,"et al., 2019; Bahar et al., 2019). The weight of CTC objective α is set to 0.3 for all ASR and ST models. The model architecture is showed in Figure 14 . 3.2 Conformer Conformer (Gulati et al., 2020) models both local and global dependencies by combining the Convolutional Neural Network and Transformers. It has shown superiority and achieved promising results in ASR tasks. We replace the Transformer blocks in the encoder by the conformer blocks, which include two macaron-like feed-forward networks, multihead self attention modules, and convolution modules. Note that we use the RPE proposed in Shaw et al. (2018) rather than Transformer-XL (Dai et al., 2019). Model Architecture In this section, we describe the baseline model and the architecture improvements. Then, the experimental results are shown to demonstrate the effectiveness. 3.1 Position Embedding Acoustic Feature Table 1: Data statistics of the ASR, MT, and ST corpora. 3 Masked Multi-Head Attention Baseline Model Our system is based on deep Transformer (Vaswani et al., 2017) implemented on the fairseq toolkit (Ott et al., 2019). Furthermore, dynamic linear combination of layers (DLCL) (Wang et al., 2019) method is employed to train the deep m"
2021.iwslt-1.9,L16-1147,0,0.033184,"tion. The training data can be divided into three categories: ASR, MT, and ST corpora1 . ASR corpora. ASR corpora are used to generate synthetic speech translation data. We only use the Common Voice (Ardila et al., 2020) and LibriSpeech (Panayotov et al., 2015) corpora. Furthermore, we filter the noisy training data in the Common Voice corpus by force decoding and keep 1 million utterances. MT corpora. Machine translation (MT) corpora are used to translate the English transcription. We use the allowed English-German translation data from WMT 2020 (Barrault et al., 2020) and OpenSubtitles2018 (Lison and Tiedemann, 2016). We filter the training bilingual data followed Li et al. (2019), which includes length ratio, language detection, and so on. Speech translation (ST) aims to learn models that can predict, given some speech in the source language, the translation into the target language. Endto-end (E2E) approaches have become popular recently for its ability to free designers from cascading different systems and shorten the pipeline of translation (Duong et al., 2016; Berard et al., 2016; Weiss et al., 2017). This paper describes the submission of the NiuTrans E2E ST system for the IWSLT 2021 (Anastasopoulos"
2021.iwslt-1.9,N19-4009,0,0.020884,"d-forward networks, multihead self attention modules, and convolution modules. Note that we use the RPE proposed in Shaw et al. (2018) rather than Transformer-XL (Dai et al., 2019). Model Architecture In this section, we describe the baseline model and the architecture improvements. Then, the experimental results are shown to demonstrate the effectiveness. 3.1 Position Embedding Acoustic Feature Table 1: Data statistics of the ASR, MT, and ST corpora. 3 Masked Multi-Head Attention Baseline Model Our system is based on deep Transformer (Vaswani et al., 2017) implemented on the fairseq toolkit (Ott et al., 2019). Furthermore, dynamic linear combination of layers (DLCL) (Wang et al., 2019) method is employed to train the deep model effectively (Li et al., 2020a,b). To reduce the computational cost, the input speech features are processed by two convolutional layers, which have a stride of 2. This downsamples 3.3 Relative Position Encoding Due to the non-sequential modeling of the original self attention modules, the vanilla Transformer employs the position embedding by a deterministic sinusoidal function to indicate the absolute position of each input element (Vaswani et al., 2017). However, this sche"
2021.iwslt-1.9,1983.tc-1.13,0,0.333474,"Missing"
2021.iwslt-1.9,W18-6430,1,0.910281,"versity, Shenyang, China 2 NiuTrans Research, Shenyang, China {xuchenneu,liuxiaoqianneu,liuxiaowenneu}@outlook.com, {tigerneu,huangcananneu}@outlook.com, {xiaotong,zhujingbo}@mail.neu.edu.cn Abstract 2020), relative position encoding (RPE) (Shaw et al., 2018), and stacked acoustic and textual encoding (SATE) (Xu et al., 2021). To augment the training data, the English transcriptions of the automatic speech recognition (ASR) and speech translation corpora are translated to the German translation. Finally, we employ the ensemble decoding method to integrate the predictions from multiple models (Wang et al., 2018) trained with the different datasets. This paper is structured as follows. The training data is summarized in Section 2, then we describe the model architecture in Section 3 and data augmentation in Section 4. We present the ensemble decoding method in Section 5. The experimental settings and final results are shown in Section 6. This paper describes the submission of the NiuTrans end-to-end speech translation system for the IWSLT 2021 offline task, which translates from the English audio to German text directly without intermediate transcription. We use the Transformer-based model architectur"
2021.iwslt-1.9,P19-1176,1,0.898759,"Missing"
2021.iwslt-1.9,2021.acl-long.204,1,0.760178,"strengthen the encoding and achieve an improvement of 0.45 and 0.26 BLEU points. SATE achieves a remarkable improvement by encoding the acoustic representation and textual representation respectively. We will explore better architecture designs in the future. Stacked Acoustic and Textual Encoding The previous work (Bahar et al., 2019) employs the CTC loss on the top layer of the encoder, which forces the encoders to learn soft alignments between speech and transcription. However, the CTC loss demonstrates strong preference for locally attentive models, which is inconsistent with the ST model (Xu et al., 2021). In our systems, we use the stacked acoustic-andtextual encoding (SATE) (Xu et al., 2021) method to encode the speech features. It calculates the CTC loss based on the hidden states of the intermediate layer rather than the top layer. The layers below CTC also extract the acoustic representation like an ASR encoder, while the upper layers with no CTC encode the global representation for translation. An adaptor layer is introduced to bridge the acoustic and textual encoding. 3.5 Size Total The latest work (Pham et al., 2020; Gulati et al., 2020) points out that the relative position encoding e"
2021.naacl-main.458,D19-1633,0,0.553828,"y. Experiment results show Roy et al., 2018) of latent codes for discrete latent that our model achieves comparable or better performance in machine translation tasks than spaces, which may hurt the translation efficiency— several strong baselines. the essential goal of non-autoregressive decoding. Akoury et al. (2019) introduce syntactic labels 1 Introduction as a proxy to the learned discrete latent space and Non-autoregressive Transformer (NAT, Gu et al., improve the NATs’ performance. The syntactic 2018; Wang et al., 2019; Lee et al., 2018; label greatly reduces the search space of latent Ghazvininejad et al., 2019) is a promising text gen- codes, leading to a better performance in both quality and speed. However, it needs an external syneration model for machine translation. It introduces tactic parser to produce the reference syntactic tree, the conditional independent assumption among the target language outputs and simultaneously gener- which may only be effective in limited scenarios. Thus, it is still challenging to model the dependency ates the whole sentence, bringing in a remarkable efficiency improvement (more than 10× speed-up) between latent variables for non-autoregressive decoding efficient"
2021.naacl-main.458,P07-2045,0,0.0151311,"Missing"
2021.naacl-main.458,D18-1149,0,0.555263,"arge number (more than 215 , Kaiser et al., 2018; the model capacity. Experiment results show Roy et al., 2018) of latent codes for discrete latent that our model achieves comparable or better performance in machine translation tasks than spaces, which may hurt the translation efficiency— several strong baselines. the essential goal of non-autoregressive decoding. Akoury et al. (2019) introduce syntactic labels 1 Introduction as a proxy to the learned discrete latent space and Non-autoregressive Transformer (NAT, Gu et al., improve the NATs’ performance. The syntactic 2018; Wang et al., 2019; Lee et al., 2018; label greatly reduces the search space of latent Ghazvininejad et al., 2019) is a promising text gen- codes, leading to a better performance in both quality and speed. However, it needs an external syneration model for machine translation. It introduces tactic parser to produce the reference syntactic tree, the conditional independent assumption among the target language outputs and simultaneously gener- which may only be effective in limited scenarios. Thus, it is still challenging to model the dependency ates the whole sentence, bringing in a remarkable efficiency improvement (more than 10"
2021.naacl-main.458,D19-1573,0,0.253454,"Missing"
2021.naacl-main.458,D19-1437,0,0.449018,"to the lack of dependencies modeling for To learn these codes in an unsupervised way, we the target outputs, making it harder to model the use each latent code to represent a fuzzy target generation of the target side translation. category instead of a chunk as the previous reA promising way is to model the dependencies search (Akoury et al., 2019). More specifically, of the target language by the latent variables. A line we first employ vector quantization (Roy et al., of research works (Kaiser et al., 2018; Roy et al., 2018) to discretize the target language to the la2018; Shu et al., 2019; Ma et al., 2019) introduce tent space with a smaller number (less than 128) latent variable modeling to the non-autoregressive of latent variables, which can serve as the fuzzy Transformer and improves translation quality. The word-class information each target language word. latent variables could be regarded as the spring- We then model the latent variables with conditional board to bridge the modeling gap, introducing random fields (CRF, Lafferty et al., 2001; Sun et al., more informative decoder inputs than the previ- 2019). To avoid the mismatch of the training and 5749 Proceedings of the 2021 Conference"
2021.naacl-main.458,N19-4009,0,0.0191697,"parated subword embeddings for the IWSLT14 dataset. Model Setting. In the case of IWSLT14 task, we use a small setting (dmodel = 256, dhidden = 512, pdropout = 0.1, nlayer = 5 and nhead = 4) for Transformer and NAT models. For the WMT tasks, we use the Transformer-base setting (dmodel = 512, dhidden = 512, pdropout = 0.3, nhead = 8 and nlayer = 6) of the Vaswani et al. (2017). We set the hyperparameter α used in Eq. 15 and λ in Eq. 7-8 to 1.0 and 0.999, respectively. The categorical number K is set to 64 in our experiments. We implement our model based on the open-source framework of fairseq (Ott et al., 2019). Optimization. We optimize the parameter with the Adam (Kingma and Ba, 2015) with β = (0.9, 0.98). We use inverse square root learning rate scheduling (Vaswani et al., 2017) for the WMT tasks and linear annealing schedule (Lee et al., 2018) from 3 × 10−4 to 1 × 10−5 for the IWSLT14 task. Each mini-batch consists of 2048 tokens for IWSLT14 and 32K tokens for WMT tasks. WMT14 EN-DE DE-EN IWSLT14 DE-EN LV-NAR AXE CMLM SynST Flowseq 11.80 20.40 20.74 20.85 / 24.90 25.50 25.40 / / 23.82 24.75 NAT (ours) CNAT (ours) 9.80 21.30 11.02 25.73 17.77 29.81 Table 1: Results of the NAT models with argmax d"
2021.naacl-main.458,P02-1040,0,0.111587,"uces tactic parser to produce the reference syntactic tree, the conditional independent assumption among the target language outputs and simultaneously gener- which may only be effective in limited scenarios. Thus, it is still challenging to model the dependency ates the whole sentence, bringing in a remarkable efficiency improvement (more than 10× speed-up) between latent variables for non-autoregressive decoding efficiently. versus the autoregressive model. However, the NAT models still lay behind the autoregressive models in In this paper, we propose to learn a set of latent terms of BLEU (Papineni et al., 2002) for machine codes that can act like the syntactic label, which is translation. We attribute the low-quality of NAT learned without using the explicit syntactic trees. models to the lack of dependencies modeling for To learn these codes in an unsupervised way, we the target outputs, making it harder to model the use each latent code to represent a fuzzy target generation of the target side translation. category instead of a chunk as the previous reA promising way is to model the dependencies search (Akoury et al., 2019). More specifically, of the target language by the latent variables. A line"
2021.naacl-main.458,D07-1043,0,0.0071131,"e a sharp distribution for each latent variable, showing that our learned fuzzy classes are meaningful. 5 Related Work Non-autoregressive Machine Translation. Gu et al. (2018) first develop a non-autoregressive Transformer (NAT) for machine translation, which produces the outputs in parallel, and the inference speed is thus significantly boosted. Due to the missing of dependencies among the target outputs, the translation quality is largely sacrificed. A line of work proposes to mitigate such perQuantitative Results. We first compute the Vformance degradation by enhancing the decoder Measure (Rosenberg and Hirschberg, 2007) score between the latent categories to POS tags and sub- inputs. Lee et al. (2018) propose a method of itwords frequencies. The results are listed in Table 7. erative refinement based on the previous outputs. Overall, the “w/ POS tags” achieves a higher V- Guo et al. (2019) enhance decoder input by introducing the phrase table in statistical machine transMeasure score, indicating that the latent codes are more related to the POS tags than sub-words fre- lation and embedding transformation. There are quencies. The homogeneity score (H-score) evalu- also some work focuses on improving the decod"
2021.naacl-main.458,P16-1162,0,0.0910943,"arg max p(y|z ∗ , x; θ), y y∗ where identifying only requires independently One potential issue is that the mismatch of the maximizing the local probability for each output training and inference stage for the used categorical position. 5752 4 Experiments Model Datasets. We conduct the experiments on the most widely used machine translation benchmarks: WMT14 English-German (WMT14 EN-DE, 4.5M pairs)1 and IWSLT14 German-English (IWSLT14, 160K pairs)2 . The datasets are processed with the Moses script (Koehn et al., 2007), and the words are segmented into subword units using byte-pair encoding (Sennrich et al., 2016, BPE). We use the shared subword embeddings between the source language and target language for the WMT datasets and the separated subword embeddings for the IWSLT14 dataset. Model Setting. In the case of IWSLT14 task, we use a small setting (dmodel = 256, dhidden = 512, pdropout = 0.1, nlayer = 5 and nhead = 4) for Transformer and NAT models. For the WMT tasks, we use the Transformer-base setting (dmodel = 512, dhidden = 512, pdropout = 0.3, nhead = 8 and nlayer = 6) of the Vaswani et al. (2017). We set the hyperparameter α used in Eq. 15 and λ in Eq. 7-8 to 1.0 and 0.999, respectively. The"
2021.naacl-main.458,P19-1125,0,0.679393,"le (a) AT (b)Non-Autoregressive NATDecoding Decoding (c) LT Experiment results on WMT14 and IWSLT14 show that CNAT achieves the new state-of-the- Figure 1: Different inference process of different Transart performance without knowledge distillation. former models. With the sequence-level knowledge distillation and y x reranking techniques, the CNAT is comparable to • Inputs Initialization: With the target sethe current state-of-the-art iterative-based model quence length m, we can compute the dewhile keeping a competitive decoding speedup. coder inputs h = h1:m with Softcopy (Li et al., 2019; Wei et al., 2019) as: 2 Background hj = Neural machine translation (NMT) is formulated as a conditional probability model p(y|x), which models a sentence y = {y1 , y2 , · · · , ym } in the target language given the input x = {x1 , x2 , · · · , xn } from the source language. 2.1 Non-Autoregressive Neural Machine Translation Gu et al. (2018) proposes Non-Autoregressive Transformer (NAT) for machine translation, breaking the dependency among target tokens, thus achieving simultaneous decoding for all tokens. For a source sentence, a non-autoregressive decoder factorizes the probability of its target sentence as:"
2021.naacl-main.458,D19-1072,0,0.0211239,": Approach In this section, we present our proposed CNAT, an extension to the Transformer incorporated with non-autoregressive decoding for target tokens and autoregressive decoding for latent sequences. In brief, CNAT follows the architecture of Latent Transformer (Kaiser et al., 2018), except for the latent variable modeling (in § 3.1 and § 3.2) and inputs initialization (in § 3.3). 3.1 Modeling Target Categorical Information by Vector Quantization Categorical information has achieved great success in neural machine translation, such as partof-speech (POS) tag in autoregressive translation (Yang et al., 2019) and syntactic label in nonautoregressive translation (Akoury et al., 2019). Inspired by the broad application of categorical information, we propose to model the implicit categorical information of target words in a nonautoregressive Transformer. Each target sequence y = y1:m will be assigned to a discrete latent variable sequence z = z1:m . We assume that each zi will capture the fuzzy category of its token yi . Then, the conditional probability p(y|x) is factorized with respect to the categorical latent variable: p(y|x) = X zi = k, qi = Qk , and k = arg min ||repr(yi ) − Qj ||2 , j∈[K] repr"
C10-1151,P01-1005,0,0.014086,"-decoding) approach to improve parsing accuracy by leveraging bracket structure consensus between multiple parsing decoders trained on individual treebanks. Experimental results show the effectiveness of the proposed approach, which outperforms stateof-the-art baselines, especially on long sentences. 1 Introduction Recent years have seen extensive applications of machine learning methods to natural language processing problems. Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). However, annotating data by human is expensive in time and labor. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks. For example, for constituent syntactic parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) in Chinese, in addition to the most popular treebank Chinese Treebank (CTB) (Xue et al., 2002), there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (Zhou, 1996). For the purpose of full use of readily available human annotations for the same"
C10-1151,D09-1161,0,0.0635626,"tures in the parse trees are different. Specifically put, although the internal structures of the parse trees are different, both CTB and TCT agree to take “中国 传统 文化” as a noun phrase. Motivated by this observation, we would like to guide parsers that are trained on CTB and TCT respectively to verify their output interactively by using consensus information implicitly contained in these treebanks. Better performance is expected when such information is considered. A feasible framework to make use of consensus information is n-best combination (Henderson and Brill, 1999; Sagae and Lavie, 2006; Zhang et al., 2009; Fossum and Knight, 2009). In contrast 1345 to previous work on n-best combination where multiple parsers, say, Collins parser (Collins, 1999) and Berkeley parser (Petrov et al., 2006) are trained on the same training data, n-best combination for heterogeneous parsing is instead allowed to use either a single parser or multiple parsers which are trained on heterogeneous treebanks. Consensus information can be incorporated during the combination of the output (n-best list of full parse trees following distinct annotation standards) of individual parsers. However, despite the success of n-best"
C10-1151,A00-2018,0,0.0940505,"years have seen extensive applications of machine learning methods to natural language processing problems. Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). However, annotating data by human is expensive in time and labor. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks. For example, for constituent syntactic parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) in Chinese, in addition to the most popular treebank Chinese Treebank (CTB) (Xue et al., 2002), there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (Zhou, 1996). For the purpose of full use of readily available human annotations for the same tasks, it is significant if such corpora can be used jointly. At first sight, a direct combination of multiple corpora is a way to this end. However, corpora created for the same NLP tasks are generally built by different organizations. Thus such corpora often follow different annotation standards and/or even diffe"
C10-1151,N09-2064,0,0.0808228,"rees are different. Specifically put, although the internal structures of the parse trees are different, both CTB and TCT agree to take “中国 传统 文化” as a noun phrase. Motivated by this observation, we would like to guide parsers that are trained on CTB and TCT respectively to verify their output interactively by using consensus information implicitly contained in these treebanks. Better performance is expected when such information is considered. A feasible framework to make use of consensus information is n-best combination (Henderson and Brill, 1999; Sagae and Lavie, 2006; Zhang et al., 2009; Fossum and Knight, 2009). In contrast 1345 to previous work on n-best combination where multiple parsers, say, Collins parser (Collins, 1999) and Berkeley parser (Petrov et al., 2006) are trained on the same training data, n-best combination for heterogeneous parsing is instead allowed to use either a single parser or multiple parsers which are trained on heterogeneous treebanks. Consensus information can be incorporated during the combination of the output (n-best list of full parse trees following distinct annotation standards) of individual parsers. However, despite the success of n-best combination methods, they"
C10-1151,W99-0623,0,0.0413618,"bracket structures. That is, not all bracket structures in the parse trees are different. Specifically put, although the internal structures of the parse trees are different, both CTB and TCT agree to take “中国 传统 文化” as a noun phrase. Motivated by this observation, we would like to guide parsers that are trained on CTB and TCT respectively to verify their output interactively by using consensus information implicitly contained in these treebanks. Better performance is expected when such information is considered. A feasible framework to make use of consensus information is n-best combination (Henderson and Brill, 1999; Sagae and Lavie, 2006; Zhang et al., 2009; Fossum and Knight, 2009). In contrast 1345 to previous work on n-best combination where multiple parsers, say, Collins parser (Collins, 1999) and Berkeley parser (Petrov et al., 2006) are trained on the same training data, n-best combination for heterogeneous parsing is instead allowed to use either a single parser or multiple parsers which are trained on heterogeneous treebanks. Consensus information can be incorporated during the combination of the output (n-best list of full parse trees following distinct annotation standards) of individual parse"
C10-1151,D07-1117,0,0.0143621,"S))) is an indicator function where I(c, CS(H which returns one if c ∈ CS(T ) is compatible ˆ k (S)), zero othwith all the elements in CS(H erwise. Two spans, [a, b] and [i, j] are said to be compatible if they satisfy one of the following conditions: 1) i &gt; b; 2) a &gt; j; 3) a ≤ i ≤ b and j ≤ b; 4) i ≤ a ≤ j and b ≤ j. Fig 4 uses two example to illustrate the concept of compatibility. 3 Experiments 3.1 Data and Performance Metric The most recent version of the CTB corpus, CTB 6.0 and the CIPS ParsEval data are used as heterogeneous treebanks in the experiments. Following the split utilized in (Huang et al., 2007), we divided the dataset into blocks of 10 files. For each block, the first file was added to the CTB development data, the second file was added to the CTB testing data, and the remaining 8 files were added to the CTB training data. For the sake of parsing efficiency, we randomly sampled 1,000 sentences of no more than 40 words from the CTB test set. CTB-Partitions #Sentences #Words Ave-Length TCT-Partitions #Sentences #Words Ave-Length Train 22,724 627,833 30.1 Train 32,771 354,767 10.6 Dev 2,855 78,653 30.0 Dev N/A N/A N/A Test 1,000 25,100 20.3 Test 1,000 10,400 10.4 Table 1: Basic statist"
C10-1151,P09-1066,0,0.394278,"rained on heterogeneous treebanks. Consensus information can be incorporated during the combination of the output (n-best list of full parse trees following distinct annotation standards) of individual parsers. However, despite the success of n-best combination methods, they suffer from the limited scope of n-best list. Taking this into account, we prefer to apply the co-decoding approach such that consensus information is expected to affect the entire procedure of searching hypothesis space. 2.2 System Overview The idea of co-decoding is recently extensively studied in the literature of SMT (Li et al., 2009; Liu et al., 2009). As the name shows, co-decoding requires multiple decoders be combined and proceed collaboratively. As with n-best combination, there are at least two ways to build multiple decoders: we can either use multiple parsers trained on the same training data (use of diversity of models), or use a single parser on different training data (use of diversity of datasets) 1 . Both ways can build multiple decoders which are to be integrated into co-decoding. For the latter case, one method to get diverse training data is to use different portions of the same training set. In this study"
C10-1151,P09-1065,0,0.141873,"eneous treebanks. Consensus information can be incorporated during the combination of the output (n-best list of full parse trees following distinct annotation standards) of individual parsers. However, despite the success of n-best combination methods, they suffer from the limited scope of n-best list. Taking this into account, we prefer to apply the co-decoding approach such that consensus information is expected to affect the entire procedure of searching hypothesis space. 2.2 System Overview The idea of co-decoding is recently extensively studied in the literature of SMT (Li et al., 2009; Liu et al., 2009). As the name shows, co-decoding requires multiple decoders be combined and proceed collaboratively. As with n-best combination, there are at least two ways to build multiple decoders: we can either use multiple parsers trained on the same training data (use of diversity of models), or use a single parser on different training data (use of diversity of datasets) 1 . Both ways can build multiple decoders which are to be integrated into co-decoding. For the latter case, one method to get diverse training data is to use different portions of the same training set. In this study we extend the case"
C10-1151,P09-1006,0,0.40052,"his paper is dedicated to solving the problem of how to use jointly multiple disparate treebanks for constituent syntactic parsing. Hereafter, treebanks of different annotations are 1344 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1344–1352, Beijing, August 2010 called heterogeneous treebanks, and correspondingly, the problem of syntactic parsing with heterogeneous treebanks is referred to as heterogeneous parsing. Previous work on heterogeneous parsing is often based on treebank transformation (or treebank conversion) (Wang et al., 1994; Niu et al., 2009). The basic idea is to transform annotations of one treebank (source treebank) to fit the standard of another treebank (target treebank). Due to divergences of treebank annotations, such transformation is generally achieved in an indirect way by selecting transformation results from the output of a parser trained on the target treebank. A common property of all the work mentioned above is that transformation accuracy is heavily dependent on the performance of parsers trained on the target treebank. Sometimes transformation accuracy is not so satisfactory that techniques like instance pruning a"
C10-1151,P06-1055,0,0.317518,"extensive applications of machine learning methods to natural language processing problems. Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). However, annotating data by human is expensive in time and labor. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks. For example, for constituent syntactic parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) in Chinese, in addition to the most popular treebank Chinese Treebank (CTB) (Xue et al., 2002), there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (Zhou, 1996). For the purpose of full use of readily available human annotations for the same tasks, it is significant if such corpora can be used jointly. At first sight, a direct combination of multiple corpora is a way to this end. However, corpora created for the same NLP tasks are generally built by different organizations. Thus such corpora often follow different annotation standards and/or even different linguistic theori"
C10-1151,N06-2033,0,0.0629553,"Missing"
C10-1151,C02-1145,0,0.0329048,"lly, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). However, annotating data by human is expensive in time and labor. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks. For example, for constituent syntactic parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) in Chinese, in addition to the most popular treebank Chinese Treebank (CTB) (Xue et al., 2002), there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (Zhou, 1996). For the purpose of full use of readily available human annotations for the same tasks, it is significant if such corpora can be used jointly. At first sight, a direct combination of multiple corpora is a way to this end. However, corpora created for the same NLP tasks are generally built by different organizations. Thus such corpora often follow different annotation standards and/or even different linguistic theories. We take CTB and TCT as a case study. Although both CTB and TCT are Chomskian-style treebank"
C10-1151,P94-1034,0,0.886825,"ed independently. This paper is dedicated to solving the problem of how to use jointly multiple disparate treebanks for constituent syntactic parsing. Hereafter, treebanks of different annotations are 1344 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1344–1352, Beijing, August 2010 called heterogeneous treebanks, and correspondingly, the problem of syntactic parsing with heterogeneous treebanks is referred to as heterogeneous parsing. Previous work on heterogeneous parsing is often based on treebank transformation (or treebank conversion) (Wang et al., 1994; Niu et al., 2009). The basic idea is to transform annotations of one treebank (source treebank) to fit the standard of another treebank (target treebank). Due to divergences of treebank annotations, such transformation is generally achieved in an indirect way by selecting transformation results from the output of a parser trained on the target treebank. A common property of all the work mentioned above is that transformation accuracy is heavily dependent on the performance of parsers trained on the target treebank. Sometimes transformation accuracy is not so satisfactory that techniques like"
C10-1151,J03-4003,0,\N,Missing
C10-2154,D07-1079,0,0.0844003,"extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been proved to be an important one that affects the translation accuracy of syntax-based systems (DeNeefe et al., 2007; Shen et al., 2008). However, these systems suffer from a problem that translation rules are extracted using only 1-best parse tree generated by a single parser, which generally results in relatively low rule coverage due to the limited scope in rule extraction (Mi and Huang, 2008). To alleviate this problem, a straightforward solution is to enlarge the scope of rule extraction, and obtain translation rules by using a group of diversified parse trees instead of a single parse tree. For example, Mi and Huang (2008) used k-best parses and forest to extract translation rules for improving the ru"
C10-2154,P05-1067,0,0.0120742,"nglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been prove"
C10-2154,P03-2041,0,0.0924117,"od on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule"
C10-2154,N04-1035,0,0.20786,"terminals) and variables (non-terminals) at leaves. Figure 1 shows the translation rules extracted from a word-aligned sentence pair with a targetside parse tree. Figure 1: Translation rules extracted from a string-tree pair. 1346 Figure 2: Rule extraction using two different parsers (Berkeley Parser and Collins Parser). The shaded rectangles denote the translation rules that can be extracted from the parse tree generated by one parser but cannot be extracted from the parse tree generated by the other parser. To obtain basic translation rules, the (minimal) GHKM extraction method proposed in (Galley et al, 2004) is utilized. The basic idea of GHKM extraction is to compute the set of the minimally-sized translation rules that can explain the mappings between source-language string and target-language tree while respecting the alignment and reordering between the two languages. For example, from the string-tree pair shown at the top of Figure 1, we extract the minimal GHKM translation rules r1-6. In addition to GHKM extraction, the SPMT models (Marcu et al., 2006) are employed to obtain phrasal rules that are not covered by GHKM extraction. For example, rule r8 in Figure 1 is a SPMT rule that is not ob"
C10-2154,P07-1019,0,0.0312328,"5.3 Parser Indicator Features For each rule, we define N indicator features (i.e. τ (r , i ) ) to indicate a rule is extracted by using which parsers, and add them into the translation model. By training the feature weights with Minimum Error Rate Training (MERT), the system can learn preferences for different parsers automatically. 6 Experiments The experiments are conducted on ChineseEnglish translation in a state-of-the-art string-totree SMT system. 6.1 and the composed rules are generated by composing two or three minimal GHKM and SPMT rules3. We use a CKY-style decoder with cube pruning (Huang and Chiang, 2007) and beam search to decode new Chinese sentences. By default, the beam size is set to 30. For integrating n-gram language model into decoding efficiently, rules containing more than two variables or source word sequences are binarized using the synchronous binarization method (Zhang et al., 2006; Xiao et al., 2009). The system is evaluated in terms of the caseinsensitive NIST version BLEU (using the shortest reference length), and statistical significant test is conducted using the re-sampling method proposed by Koehn (2004). 6.2 Four syntactic parsers are chosen for the experiments. They are"
C10-2154,2006.amta-papers.8,0,0.642775,"ts show that our method improves the baseline system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora, even achieves a +1 BLEU improvement when working with the kbest extraction method. More interestingly, we observe that the MT performance is not very sensitive to the parsing performance of the parsers used in rule extraction. Actually, the MT system does not show different preferences for different parsers. 2 Related Work In machine translation, some efforts have been made to improve rule coverage and advance the performance of syntax-based systems. For example, Galley et al. (2006) proposed the idea of rule composing which composes two or more rules with shared states to form a larger, composed rule. Their experimental results showed that the rule composing method could significantly improve the translation accuracy of their syntax-based system. Following Galley et al. (2006)’s work, Marcu et al. (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. Wang et al. (2007) described a binarization method that binarized parse trees to improve the rule coverag"
C10-2154,W04-3250,0,0.0602451,"SPMT rules3. We use a CKY-style decoder with cube pruning (Huang and Chiang, 2007) and beam search to decode new Chinese sentences. By default, the beam size is set to 30. For integrating n-gram language model into decoding efficiently, rules containing more than two variables or source word sequences are binarized using the synchronous binarization method (Zhang et al., 2006; Xiao et al., 2009). The system is evaluated in terms of the caseinsensitive NIST version BLEU (using the shortest reference length), and statistical significant test is conducted using the re-sampling method proposed by Koehn (2004). 6.2 Four syntactic parsers are chosen for the experiments. They are Stanford Parser4, Berkeley Parser 5 , Collins Parser (Dan Bikel’s reimplementation of Collins Model 2) 6 and Charniak Parser7. The former two are state-of-the-art nonlexicalized parsers, while the latter two are stateof-the-art lexicalized parsers. All the parsers are trained on sections 02-21 of the Wall Street Journal (WSJ) Treebank, and tuned on section 22. Table 2 summarizes the performance of the parsers. Experimental Setup Our bilingual data consists of 370K sentence pairs (9M Chinese words + 10M English words) which h"
C10-2154,P06-1077,0,0.134429,"imple and effective method to improve rule coverage by using multiple parsers in translation rule extraction, and then empirically investigate the effectiveness of our method on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performa"
C10-2154,D08-1022,0,0.719676,"shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been proved to be an important one that affects the translation accuracy of syntax-based systems (DeNeefe et al., 2007; Shen et al., 2008). However, these systems suffer from a problem that translation rules are extracted using only 1-best parse tree generated by a single parser, which generally results in relatively low rule coverage due to the limited scope in rule extraction (Mi and Huang, 2008). To alleviate this problem, a straightforward solution is to enlarge the scope of rule extraction, and obtain translation rules by using a group of diversified parse trees instead of a single parse tree. For example, Mi and Huang (2008) used k-best parses and forest to extract translation rules for improving the rule coverage in their forest-based SMT system, and achieved promising results. However, most previous work used the parse trees generated by only one parser, which still suffered somewhat from the relatively low diversity in the outputs of a single parser. Addressing this issue, we i"
C10-2154,P05-1034,0,0.0951562,"Missing"
C10-2154,P08-1066,0,0.0390355,"empirically investigate the effectiveness of our method on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributi"
C10-2154,2008.amta-papers.18,0,0.0357989,"Missing"
C10-2154,D07-1078,0,0.0361915,"Missing"
C10-2154,D09-1038,1,0.814184,"utomatically. 6 Experiments The experiments are conducted on ChineseEnglish translation in a state-of-the-art string-totree SMT system. 6.1 and the composed rules are generated by composing two or three minimal GHKM and SPMT rules3. We use a CKY-style decoder with cube pruning (Huang and Chiang, 2007) and beam search to decode new Chinese sentences. By default, the beam size is set to 30. For integrating n-gram language model into decoding efficiently, rules containing more than two variables or source word sequences are binarized using the synchronous binarization method (Zhang et al., 2006; Xiao et al., 2009). The system is evaluated in terms of the caseinsensitive NIST version BLEU (using the shortest reference length), and statistical significant test is conducted using the re-sampling method proposed by Koehn (2004). 6.2 Four syntactic parsers are chosen for the experiments. They are Stanford Parser4, Berkeley Parser 5 , Collins Parser (Dan Bikel’s reimplementation of Collins Model 2) 6 and Charniak Parser7. The former two are state-of-the-art nonlexicalized parsers, while the latter two are stateof-the-art lexicalized parsers. All the parsers are trained on sections 02-21 of the Wall Street Jo"
C10-2154,P01-1067,0,0.0737178,"rsers in translation rule extraction, and then empirically investigate the effectiveness of our method on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al."
C10-2154,N06-1033,1,0.816388,"different parsers automatically. 6 Experiments The experiments are conducted on ChineseEnglish translation in a state-of-the-art string-totree SMT system. 6.1 and the composed rules are generated by composing two or three minimal GHKM and SPMT rules3. We use a CKY-style decoder with cube pruning (Huang and Chiang, 2007) and beam search to decode new Chinese sentences. By default, the beam size is set to 30. For integrating n-gram language model into decoding efficiently, rules containing more than two variables or source word sequences are binarized using the synchronous binarization method (Zhang et al., 2006; Xiao et al., 2009). The system is evaluated in terms of the caseinsensitive NIST version BLEU (using the shortest reference length), and statistical significant test is conducted using the re-sampling method proposed by Koehn (2004). 6.2 Four syntactic parsers are chosen for the experiments. They are Stanford Parser4, Berkeley Parser 5 , Collins Parser (Dan Bikel’s reimplementation of Collins Model 2) 6 and Charniak Parser7. The former two are state-of-the-art nonlexicalized parsers, while the latter two are stateof-the-art lexicalized parsers. All the parsers are trained on sections 02-21 o"
C10-2154,P08-1064,0,0.0131556,"lts show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been proved to be an important one that affects th"
C10-2154,C10-1151,1,0.824466,"k-best parses to improve multi-parser based rule extraction in practice. z The MT performance is not influenced by the parsing performance of the parsers used in rule extraction very much. Actually, the MT system does not show different preferences for different parsers. 1-best 8.5 Figure 4: Multi-parser based rule extraction & rule extraction with k-best parses (MT05). 7 8 In this paper, we present a simple and effective method to improve rule coverage by using multiple parsers in translation rule extraction. Experimental results show that 38.4 38.2 geneous decoding (or parsing) techniques (Zhu et al., 2010) to make use of heterogeneous grammars in the stage of decoding. Both topics are very interesting and worth studying in our future work. Besides k-best extraction, our method can also be applied to other rule extraction schemes, such as forest-based rule extraction. As (Mi and Huang, 2008) has shown that forest-based extraction is more effective than k-best extraction in improving translation accuracy, it is expected to achieve further improvements by using multiparser based rule extraction and forest-based rule extraction together. Discussion and Future Work In this work, all the parsers are"
C10-2154,W06-1606,0,0.380496,"Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been proved to be an important one that affects the translation accuracy of syntax-based systems (DeNeefe et al., 2007; Shen et al., 2008). However, these systems suffer from a problem that translation rules are extracted using only 1-best parse tree generated by a single parser, which generally results in relatively low rule coverage due to the limited scope in rule extraction (Mi and Huang, 2008). To alleviate this problem, a straightforward solution is to enlarge the scope of rule extraction, and"
C10-2154,P09-1063,0,\N,Missing
C10-2154,W06-1628,0,\N,Missing
C10-2154,P06-1121,0,\N,Missing
C12-1106,D12-1133,0,0.381527,"tructural features on both sides of the attachment point are accessible so as to make more informed decision. In this work, we further generalize the algorithm of (Goldberg and Elhadad, 2010) to a general sequential labelling framework. We apply this framework to Chinese POS tagging and dependency parsing which has never been studied under the easy-first framework before. One characteristic of Chinese dependency parsing is that parsing performance can be dramatically affected by the quality of POS tags of the input sentence (Li et al., 2010). Recent work (Li et al., 2010; Hatori et al., 2011; Bohnet and Nivre, 2012) empirically verified that solving POS tagging and dependency parsing jointly can boost the performance of both the two tasks. To further improve tagging and parsing accuracy, we also solve the two tasks jointly. While previous joint methods are either graph-based or transition-based algorithm, in this work we propose the first joint tagging and dependency parsing algorithm under the easy-first framework. In addition, we also adopt a different training strategy to learn the model parameters. Previous approaches all train their joint model with the objective of minimizing the loss between the r"
C12-1106,W02-1001,0,0.139634,"uracy and runs fast. And our joint model achieves tagging accuracy of 94.27 which is the best result reported so far. KEYWORDS: dependency parsing, POS tagging, perceptron, easy-first Proceedings of COLING 2012: Technical Papers, pages 1731–1746, COLING 2012, Mumbai, December 2012. 1731 1 Introduction To sequential labelling problems, such as POS tagging or incremental parsing, traditional approaches 1) decompose the input sequence into several individual items each of which corresponds to a token of the input sequence; 2) predict these items in a fixed left-to-right (or right-to-left) order (Collins 2002; Ratnaparkhi 1996). The drawback of such fixed order approach is that when predicting one item, only the labels on the left side can be used while the labels on the right side is still unavailable. (Goldberg and Elhadad, 2010) proposed the easy-first dependency parsing algorithm to relax the fixed left-to-right order and to incorporate more structural features from both sides of the attachment point. Comparing with a deterministic transition based parser which parses the sentence left-to-right, their deterministic easy-first parser achieves significant better accuracy. The key idea behind the"
C12-1106,D07-1098,0,0.0695046,"is selected, then both tagging and parsing error cause parameter update; if is selected, then only tagging errors can cause parameter update. This can be done by adding all valid attach actions to the compatible set regardless whether those actions are indeed compatible with the gold reference (line 6 to line 7); For , only parsing errors cause parameter update which can be achieved similar to the case of . 4 Experiments To make comparison with previous works, we use Penn Chinese Treebank 5.1 (CTB5) (Xue et al., 2005) to evaluate our method. We use the standard split of CTB5 as described in (Duan et al., 2007): section 001-815 and 1001-1136 are used as training set, section 886-931 and 1148-1151 are used as development set, section 816-885 and 1137-1147 are used as test set. Head finding rules of (Zhang and Clark 2008b) are used to convert the constituent trees into dependency trees. An Intel Core i7 870 2.93 GHz machine is used for evaluation. For POS tagging and dependency parsing, the number of training iterations are selected according to the model’s performance on the development set. The model which achieves the highest score on the development set is selected to run on the test set. 4.1 POS"
C12-1106,D09-1127,0,0.0172753,"Word – – – – 79.03 79.29 78.43 78.73 78.87 Root – – – – 74.70 74.65 67.14 68.29 68.50 Compl – – – – 27.19 27.24 28.98 29.34 29.29 – – 32.7 9 J-N Speed 93.82 93.51 5.8 93.84 2 391 385 355 TABLE 4 – Parsing performance. H&S-10 and Z&N-11 denote parsers in Huang and Sagae (2010) and Zhang and Nirve (2011), respectively. H&S-H and Z&N-H denote Hatori et al., (2011)’s re-implementation of H&S-10 and Z&N-11, respectively. Li-10-O2/O3 denotes the 2rd/3rd graph based model of Li et al., (2010) either or or is a preposition. For Chinese, PP attachment ambiguity is not as prevalent as that of English (Huang et al., 2009) and we found that use these features without any limitation yields better results. VTT includes valence features, tri-gram features and third order features which were proved useful for transition based parsers (Zhang and Nivre, 2011). For OP , some additional order preference feature templates are added. Parsing results are shown in table 4. “GoldPOS” denotes the input with gold standard POS tag. “AutoPOS” denotes that the training set are assigned with gold standard POS tag while the test set are tagged by our easy-first tagger. “J-N” denotes that we use 10-fold Jack-Nifing to train the mod"
C12-1106,P10-1110,0,0.0608227,"0 GoldPOS Word 85.20 86.00 Root 78.32 – Compl 33.72 36.90 VTT OP 85.12 85.96 86.18 86.00 84.62 85.18 85.22 78.30 80.87 78.58 77.59 74.70 75.38 75.48 32.77 35.03 34.07 34.02 36.12 36.27 36.80 Tag Accuracy – – AutoPOS Word – – 77.13 78.04 – – 77.45 77.64 77.66 Root – – 72.49 75.55 – – 68.50 68.92 68.35 Compl – – 25.13 26.07 – – 28.89 28.19 28.45 Word – – – – 79.03 79.29 78.43 78.73 78.87 Root – – – – 74.70 74.65 67.14 68.29 68.50 Compl – – – – 27.19 27.24 28.98 29.34 29.29 – – 32.7 9 J-N Speed 93.82 93.51 5.8 93.84 2 391 385 355 TABLE 4 – Parsing performance. H&S-10 and Z&N-11 denote parsers in Huang and Sagae (2010) and Zhang and Nirve (2011), respectively. H&S-H and Z&N-H denote Hatori et al., (2011)’s re-implementation of H&S-10 and Z&N-11, respectively. Li-10-O2/O3 denotes the 2rd/3rd graph based model of Li et al., (2010) either or or is a preposition. For Chinese, PP attachment ambiguity is not as prevalent as that of English (Huang et al., 2009) and we found that use these features without any limitation yields better results. VTT includes valence features, tri-gram features and third order features which were proved useful for transition based parsers (Zhang and Nivre, 2011). For OP , some additio"
C12-1106,P11-1089,0,0.0401458,"hm. By incorporating additional loss during training, our method achieves the best tagging accuracy reported so far. Shen et al. (2007) proposed a bi-directional POS tagging algorithm which achieves state-of-the-art accuracy on English POS tagging. Comparing to their method, our tagging algorithm in this paper is much simpler and we are the first to use order preference features in POS tagging. Also this is the first work that applies easy-first tagging on Chinese. For joint POS tagging and (unlabelled, projective) dependency parsing, Li et al. (2010) proposed the first graph based algorithm. Lee et al. (2011) proposed a graphical model to solve the joint problem. Hatori et al. (2011) proposed the first transition based algorithm. Bohnet and Nivre (2012) extended Hatori et al. (2011) to labelled non-projective dependency parsing. Different from the works talked above, our method is based on the easy-first framework. In addition, all previous joint methods optimize a single loss in the training phase while we are the first to train the joint model with additional loss. Hall et al. (2011) proposed the augmented-loss training for dependency parser that aims at adapting the parser to other domains or t"
C12-1106,H05-1059,0,0.0869697,"Missing"
C12-1106,P08-1101,0,0.0685946,"le set regardless whether those actions are indeed compatible with the gold reference (line 6 to line 7); For , only parsing errors cause parameter update which can be achieved similar to the case of . 4 Experiments To make comparison with previous works, we use Penn Chinese Treebank 5.1 (CTB5) (Xue et al., 2005) to evaluate our method. We use the standard split of CTB5 as described in (Duan et al., 2007): section 001-815 and 1001-1136 are used as training set, section 886-931 and 1148-1151 are used as development set, section 816-885 and 1137-1147 are used as test set. Head finding rules of (Zhang and Clark 2008b) are used to convert the constituent trees into dependency trees. An Intel Core i7 870 2.93 GHz machine is used for evaluation. For POS tagging and dependency parsing, the number of training iterations are selected according to the model’s performance on the development set. The model which achieves the highest score on the development set is selected to run on the test set. 4.1 POS tagging Following Zhang and Clark (2008a), in the experiments, we also use a dictionary to limit the number of candidate tags for each word. That is, for a word which occurs more than M times in the training data"
C12-1106,D08-1059,0,0.0610951,"le set regardless whether those actions are indeed compatible with the gold reference (line 6 to line 7); For , only parsing errors cause parameter update which can be achieved similar to the case of . 4 Experiments To make comparison with previous works, we use Penn Chinese Treebank 5.1 (CTB5) (Xue et al., 2005) to evaluate our method. We use the standard split of CTB5 as described in (Duan et al., 2007): section 001-815 and 1001-1136 are used as training set, section 886-931 and 1148-1151 are used as development set, section 816-885 and 1137-1147 are used as test set. Head finding rules of (Zhang and Clark 2008b) are used to convert the constituent trees into dependency trees. An Intel Core i7 870 2.93 GHz machine is used for evaluation. For POS tagging and dependency parsing, the number of training iterations are selected according to the model’s performance on the development set. The model which achieves the highest score on the development set is selected to run on the test set. 4.1 POS tagging Following Zhang and Clark (2008a), in the experiments, we also use a dictionary to limit the number of candidate tags for each word. That is, for a word which occurs more than M times in the training data"
C12-1106,P11-2033,0,0.0311594,"N-11 denote parsers in Huang and Sagae (2010) and Zhang and Nirve (2011), respectively. H&S-H and Z&N-H denote Hatori et al., (2011)’s re-implementation of H&S-10 and Z&N-11, respectively. Li-10-O2/O3 denotes the 2rd/3rd graph based model of Li et al., (2010) either or or is a preposition. For Chinese, PP attachment ambiguity is not as prevalent as that of English (Huang et al., 2009) and we found that use these features without any limitation yields better results. VTT includes valence features, tri-gram features and third order features which were proved useful for transition based parsers (Zhang and Nivre, 2011). For OP , some additional order preference feature templates are added. Parsing results are shown in table 4. “GoldPOS” denotes the input with gold standard POS tag. “AutoPOS” denotes that the training set are assigned with gold standard POS tag while the test set are tagged by our easy-first tagger. “J-N” denotes that we use 10-fold Jack-Nifing to train the model. “Tag Accuracy” denotes the test set tagging accuracy. From table 4, we can see that valence and tri-gram features are also effective for easy-first parser. For GoldPOS, word accuracy boosted from 84.62 to 85.18. For AutoPOS and J-N"
C12-1106,D11-1138,0,\N,Missing
C12-1106,D11-1109,0,\N,Missing
C12-1106,I11-1136,0,\N,Missing
C12-1106,N10-1115,0,\N,Missing
C12-1106,P07-1096,0,\N,Missing
C14-1195,D07-1090,0,0.00939615,"The test sets (newswire: 1,779 sentences, web: 1768 sentences) contain all newswire and web evaluation data of MT08 (mt08), MT12 (mt12), and MT08 progress test (mt08.p). All Chinese sentences in the training, development and test sets were parsed using the Berkeley parser (Petrov and Klein, 2007). A Kneser-Ney 4-gram language model was trained on the AFP and Xinhua portions of the English Gigaword in addition to the English side of the parallel corpus. A stronger 5-gram language model was trained on all English data of NIST MT12 and the Google counts corpus using the ”stupid” backoff method (Brants et al., 2007). For decoding we use HiFST, which is implemented with weighted finite state transducers (de Gispert et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009). The span limit was set to 10 in extracting basic phrases and decoding. All"
C14-1195,P05-1033,0,0.528215,"traction and decoding for hierarchical phrase-based translation. We obtain tree-to-string rules by the GHKM method and use them to complement Hiero-style rules. All these rules are then employed to decode new sentences with source language parse trees. We experiment with our approach in a state-of-the-art Chinese-English system and demonstrate +1.2 and +0.8 BLEU improvements on the NIST newswire and web evaluation data of MT08 and MT12. 1 Introduction Synchronous context free grammars (SCFGs) are widely used in statistical machine translation (SMT), with hierarchical phrase-based translation (Chiang, 2005) as the dominant approach. Hiero grammars are easily extracted from word-aligned parallel corpora and can capture complex nested translation relationships. Hiero grammars are formally syntactic, but rules are not constrained by source or target language syntax. This lack of constraint can lead to intractable decoding and bad performance due to the over-generation of derivations in translation. To avoid these problems, the extraction and application of SCFG rules is typically constrained by a source language span limit; (non-glue) rules are lexicalised; and rules are limited to two non-terminal"
C14-1195,P10-1146,0,0.0917775,"obustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM formalism and find, for example, that our approach works well with binarized trees. This work is licenced under a Creative Commons Attribution 4"
C14-1195,J10-3008,1,0.897734,"Missing"
C14-1195,P03-2041,0,0.455159,"Missing"
C14-1195,D12-1109,0,0.0143501,"ed in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingual sentences with both word alignment and source (or targe"
C14-1195,P06-1121,0,0.157895,"to intractable decoding and bad performance due to the over-generation of derivations in translation. To avoid these problems, the extraction and application of SCFG rules is typically constrained by a source language span limit; (non-glue) rules are lexicalised; and rules are limited to two non-terminals which are not allowed to be adjacent in the source language. These constraints can yield good performing translation systems, although at a sacrifice in the ability to model long-distance movement and complex reordering of multiple constituents. By contrast, the GHKM approach to translation (Galley et al., 2006) relies on a syntactic parse on either the source or target language side to guide SCFG extraction and translation. The parse tree provides linguistically-motivated constraints both in grammar extraction and in translation. This allows for looser span constraints; rules need not be lexicalised; and rules can have more than two non-terminals to model complex reordering multiple constituents. There are also modelling benefits as more meaningful features can be used to encourage derivations with ”well-formed” syntactic tree structures. However, GHKM can have robustness problems in that translatio"
C14-1195,W10-1761,0,0.0406427,"Missing"
C14-1195,P07-1019,0,0.0225341,"type 1 rules directly. For tree-to-string rules associated with type 2, we attempt to match rules to the source syntactic tree. If a match is found: the source span of the matching tree fragment is noted and the CYK cell for that span is selected; the tree-to-string rule is converted to a Hiero-style rule; and that rule is added to the list of rules in the selected CYK cell. Once this process is finished, RTN construction, expansion, and language model composition proceeds as usual. Similar modifications could be made to incorporate these rules into cube pruning (Chiang, 2007), cube growing (Huang and Chiang, 2007), and PDT intersection and expansion (Iglesias et al., 2011). We now elaborate on the rule matching strategy. Type 1 Rules The source sentence is parsed as is usual in Hiero-style translation, with the exception that we impose no span limit on rule applications for source spans corresponding to constituents in the Chinese syntactic tree. Rule matching, the procedure that determines if a rule applies to a source span, is based on string matching (see Figure 4(a)). For example, the type 1 rule h9 in Figure 4(c) can be applied to spans (1,13) and (2,13) since both of them agree with tree constitu"
C14-1195,D10-1027,0,0.0168031,"an limit to be applied in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingual sentences with both word alignment"
C14-1195,2006.amta-papers.8,0,0.366008,"Missing"
C14-1195,E09-1044,1,0.894124,"Missing"
C14-1195,D11-1127,1,0.87318,"Missing"
C14-1195,N13-1060,0,0.0232239,"Missing"
C14-1195,P06-1077,0,0.614988,"Typically these are: (a) a rule span limit to be applied in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingua"
C14-1195,P09-1063,0,0.365919,"Missing"
C14-1195,P09-1065,0,0.0321399,"Missing"
C14-1195,D08-1076,0,0.0276395,"sducers (de Gispert et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009). The span limit was set to 10 in extracting basic phrases and decoding. All features weights were optimized using lattice-based minimum error rate training (Macherey et al., 2008). For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules with a Chinese-to-English translation probability of < 0.10. 4.2 Results We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments a"
C14-1195,W06-1606,0,0.0321305,"ifference lies in that the tree-to-string rule indicator feature does not distinguish between different syntactic labels, whereas soft syntactic features do. • Features in syntactic MT. In general tree-to-string rules have their own features which are different from those used in Hiero-style systems. For example, the features in syntactic MT systems can be defined as the generation probabilities conditioned on the root symbol of the tree-fragment. Here we choose five popular features used in syntactic MT systems, including the bi-directional phrase-based conditional translation probabilities (Marcu et al., 2006) and three syntax-based conditional probabilities (Mi and Huang, 2008). All these probabilities can be computed by relative-frequency estimates. For example, the phrase-based features are the probabilities of translating between the frontier nodes of sr and tr . The syntax-based features are the probabilities of generating r conditioned on its root, 2 We experimented with soft syntactic features (Marton and Resnik, 2008) but found no improvement over our baseline system. 2068 source and target language sides, respectively. More formally, we use the following estimates for these probabilities:"
C14-1195,P08-1114,0,0.143232,"multiple constituents. There are also modelling benefits as more meaningful features can be used to encourage derivations with ”well-formed” syntactic tree structures. However, GHKM can have robustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We al"
C14-1195,D08-1022,0,0.0227804,"not distinguish between different syntactic labels, whereas soft syntactic features do. • Features in syntactic MT. In general tree-to-string rules have their own features which are different from those used in Hiero-style systems. For example, the features in syntactic MT systems can be defined as the generation probabilities conditioned on the root symbol of the tree-fragment. Here we choose five popular features used in syntactic MT systems, including the bi-directional phrase-based conditional translation probabilities (Marcu et al., 2006) and three syntax-based conditional probabilities (Mi and Huang, 2008). All these probabilities can be computed by relative-frequency estimates. For example, the phrase-based features are the probabilities of translating between the frontier nodes of sr and tr . The syntax-based features are the probabilities of generating r conditioned on its root, 2 We experimented with soft syntactic features (Marton and Resnik, 2008) but found no improvement over our baseline system. 2068 source and target language sides, respectively. More formally, we use the following estimates for these probabilities: P Pphr (tr |sr ) = r00 :ϕ(sr00 )=ϕ(sr )∧tr00 =tr P P Pphr (sr |tr ) ="
C14-1195,P08-1023,0,0.02228,"re: (a) a rule span limit to be applied in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingual sentences with"
C14-1195,P02-1040,0,0.0950826,"ror rate training (Macherey et al., 2008). For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules with a Chinese-to-English translation probability of < 0.10. 4.2 Results We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments are organized as follows: • Baseline and Span Limits (exp01 and exp02) First we study the effect of removing the span limit for tree constituents, that is, SCFG rules can be 2069 Entry System exp01 exp02 exp03 exp04 exp05 exp06 exp07 exp08 exp09 exp10 baseline += no span limit += t-to-s rules += t-to-s features t-to-s baseline exp04 on spans &gt; 10 exp04 with null trans. exp04 + left binariz. exp04 + right binariz. exp04 + forest binariz. tune (1755) 35.84 36.05 36.63 36.82 34.63 36.17 36.10 37.11 36.58 37.03 mt08 (691) 35.85 36.08 36.51 36.49 34.44 36.11 36.03 37.46 36.56 37.2"
C14-1195,N07-1051,0,0.0249299,"track. Word alignments are obtained using MTTK (Deng and Byrne, 2008) in both Chinese-to-English and English-to-Chinese directions, and then unioning the links. The data from newswire and web genres was used for tuning and test. The development sets contain 1,755 sentences and 2160 sentences for the two genres respectively. The test sets (newswire: 1,779 sentences, web: 1768 sentences) contain all newswire and web evaluation data of MT08 (mt08), MT12 (mt12), and MT08 progress test (mt08.p). All Chinese sentences in the training, development and test sets were parsed using the Berkeley parser (Petrov and Klein, 2007). A Kneser-Ney 4-gram language model was trained on the AFP and Xinhua portions of the English Gigaword in addition to the English side of the parallel corpus. A stronger 5-gram language model was trained on all English data of NIST MT12 and the Google counts corpus using the ”stupid” backoff method (Brants et al., 2007). For decoding we use HiFST, which is implemented with weighted finite state transducers (de Gispert et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the translation model are activated; and then, the 5-gram language model is app"
C14-1195,W13-2225,1,0.875894,"Missing"
C14-1195,2010.iwslt-papers.18,0,0.0179447,"inese syntactic structure indicating the reordered translations of NP and VP. However, such a rule would not normally be included in a Hiero grammar, as it would require consecutive source language non-terminals (see Figure 3). 3 The Proposed Approach Both the tree-to-string model and the hierarchical phrase-based model have their own strengths and weaknesses. For example, tree-to-string systems are good at modelling long distance reordering, while hierarchical phrase-based systems are relatively more powerful in handling ill-formed sentences1 and free translations (Zhao and Al-Onaizan, 2008; Vilar et al., 2010). Here we present a method to enhance hierarchical phrase-based systems with tree-to-string rules and benefit from both models. The idea is simple: we obtain both the tree-to-string grammar and the Hiero-style SCFG from the training data, and then use tree-to-string rules as additional rules in decoding with the SCFG. Figure 2 shows an overview of our approach and the usual hierarchical phrase-based approach. Our approach requires source language parse trees to be input in both rule extraction and decoding. In rule extraction, we acquire tree-to-string rules using the GHKM method and Hiero-sty"
C14-1195,J10-2004,0,0.0237385,"es the baseline result using the Hiero model (i.e., type 1 rules only). To investigate the effect of failed parse trees on system performance, we also report the BLEU score including null translations for which the parser fails. As shown in exp07, there are significantly lower BLEU scores when null translations are included. It indicates that our approach is more robust than standard treeto-string systems which would generate an empty translation if the source language parser fails. • Results on Binarization (exp08-10) Tree binarization is a widely used method to improve syntactic MT systems (Wang et al., 2010). exp08-10 show the results of our improved system with left-heavy, right-heavy and forest-based bina2070 Reference: After North Korea demanded concessions from U.S. again before the start of a new round of six-nation talks , ... Baseline: In the new round of six-nation talks on North Korea again demanded that U.S. in the former promise concessions , ... GHKM+Hiero: After North Korea again demanded that U.S. promised concessions before the new round of six-nation talks , ... a Hiero rule X → h 在 X1 后, after X1 i is applied on span (1,15) Input: 北韩2 再度3 要求4 美国5 于6 新7 回合8 六9 国10 会谈11 前12 承诺13 让步"
C14-1195,P12-3004,1,0.88289,"guage model and the translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009). The span limit was set to 10 in extracting basic phrases and decoding. All features weights were optimized using lattice-based minimum error rate training (Macherey et al., 2008). For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules with a Chinese-to-English translation probability of < 0.10. 4.2 Results We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments are organized as follows: • Baseline and Span Limits (exp01 and exp02) First we study the effect of"
C14-1195,D11-1020,0,0.361476,"Missing"
C14-1195,P08-1064,0,0.375097,"Missing"
C14-1195,D08-1060,0,0.302527,"s. However, GHKM can have robustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM formalism and find, for example, that our approach works well with binarized trees. This work is licenced under a Creative Common"
C14-1195,W06-3119,0,0.264258,"rmed” syntactic tree structures. However, GHKM can have robustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM formalism and find, for example, that our approach works well with binarized trees. This work is licen"
C14-1195,2010.iwslt-keynotes.2,0,\N,Missing
C14-1195,J07-2003,0,\N,Missing
C18-1255,2014.iwslt-evaluation.1,0,0.0832556,"Missing"
C18-1255,W17-3203,0,0.0145395,"ut size of the 0. All other parameters are initialized from U(− √f an in in parameter matrix. We use negative Maximum Likelihood Estimation (MLE) as loss function, and train all the models using Adam (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98, and ϵ = 10−9 . We run training for 40 epochs with a mini-batch of 80. The learning rate is scheduled as described in (Vaswani et al., 2017): lr = d−0.5 · min(t−0.5 , t · 16k−1.5 ), where t is the step number. After that, we restart Adam and continue the training for additional 20 epochs with a fixed learning rate 5e−5 and a smaller mini-batch of 32 (Denkowski and Neubig, 2017). At test time, translations are generated by beam search with length normalization. By tuning on the validation set, we use a beam of width 8 and a length normalization weight of 1.6. For Chinese-English systems, we use a 6-layer encoder and a 6-layer decoder, with d = 512 and 2048 hidden units in the FNN sub-layer. We restart Adam after 10 epochs and train the model for 5 additional epochs. A beam of width 12 and a length normalization weight of 1.3 are employed. Note that the network equipped with our fusion layer increases a fraction of the computation cost than original one. E.g., in Chin"
C18-1255,P17-1138,0,0.0356522,"test. For Chinese-English translation, we use parts of the bitext provided within NIST12 OpenMT5 . We choose NIST 2006 (MT06) as the validation set, and 2004 (MT04), 2005 (MT05), 2008 (MT08) as the 4 5 W1 can be unique for each layer. We discuss this issue in Section 4.4. LDC2000T46, LDC2000T47, LDC2000T50, LDC2003E14, LDC2005T10, LDC2002E18, LDC2007T09, LDC2004T08 3019 System Existing Systems Baselines MLRF Systems RNN-MIXER (Ranzato et al., 2015) RNN-BSO (Wiseman and Rush, 2016) RNN-AC(Bahdanau et al., 2016) RNN-NPMT (Huang et al., 2017b) RNN-NPMT + LM (Huang et al., 2017b) ConvSeq2Seq-MLE (Edunov et al., 2017) ConvSeq2Seq-Risk (Edunov et al., 2017) Transformer-MLE +RestartAdam +RestartAdam-4Layers +RestartAdam-6Layers Enc-AVG Enc-FNN Enc-SA (nhop =4) Enc-SA (nhop =6) Dec-AVG Dec-FNN Dec-SA (nhop =4) Dec-SA (nhop =6) Both-FNN Both-SA (nhop = 4) Both-FNN-SA (nhop =4) #Param. 10.97M 10.97M 12.82M 16.50M 10.97M 11.63M 11.90M 12.16M 10.97M 11.63M 11.90M 13.47M 12.29M 12.82M 12.55M Valid. 32.96 33.91 33.58 34.14 34.20 34.35 34.34 34.55 34.48 34.61 34.02 34.38 34.83 34.73 34.30 34.59 34.55 Test 20.73 26.36 28.53 28.96 29.16 31.74 32.85 31.75 32.67 32.57 32.97 32.71 33.31 33.06 33.40 32.80 32.93 33.29 33.5"
C18-1255,P17-1012,0,0.245091,"rong Transformer baseline on IWSLT German-English and NIST Chinese-English MT tasks respectively. The result is new state-ofthe-art in German-English translation. 1 Introduction Neural models that use the encoder-decoder architecture to capture the translation equivalence relation between languages have been widely adopted over the last few years. The simplest of these relies on one recurrent neural network layer on both the encoder and decoder sides (Bahdanau et al., 2015), whereas others have successfully explored the high-level representation of language via deeper models (Wu et al., 2016; Gehring et al., 2017b; Vaswani et al., 2017). It has been noted that increasing the network depth is one of the factors contributing to the success of neural machine translation (NMT). To this end, one can stack a number of layers for an enriched sentence representation. E.g., popular NMT systems require 4 stacked layers or more for state-of-the-art results on large-scale translation tasks (Wu et al., 2016). Unfortunately, a straightforward implementation of deep neural networks has been found to underperform shallow models due to the poor convergence. One solution to this problem is to add identity mapping (or s"
C18-1255,P15-1002,0,0.0958461,"Missing"
C18-1255,P16-1162,0,0.10073,"method. Therefore, we choose the version described in Eqs. (6-7) for the empirical study. Table 1 shows a comparison of the methods used in this work. For good convergence, we use layer normalization after the fusion layer in this work. 4 Experiments We evaluate our proposed approach on German-English and Chinese-English translation tasks. 4.1 Setup For German-English translation, we use the data of the IWSLT 2014 German-English track (Cettolo et al., 2014). We follow Ranzato et al. (2015)’s work for preprocessing. We use a joint source and target byte-pair encoding with 10k merge operations (Sennrich et al., 2016). The source and target vocabulary sizes are 8,389 and 6,428 respectively. We remove the sentences with more than 175 words or 100 subword units. This results in 160K sentence pairs for training. We randomly sample 7K sentences from the training data for held-out validation, and concatenate dev2010, dev2012, tst2010, tst2011, and tst2012 for test. For Chinese-English translation, we use parts of the bitext provided within NIST12 OpenMT5 . We choose NIST 2006 (MT06) as the validation set, and 2004 (MT04), 2005 (MT05), 2008 (MT08) as the 4 5 W1 can be unique for each layer. We discuss this issue"
C18-1255,E17-3017,0,0.0607178,"Missing"
C18-1255,P17-1013,0,0.0272153,"with multiple stacked layers is challenging. It has been observed that introducing direct connections between layers can drastically improve the performance of deep neural models. Methods include highway networks (Srivastava et al., 2015), residual connections (He et al., 2016a), dense connections (Huang et al., 2017a), and fast-forward connections (Zhou et al., 2016). E.g., in machine translation, residual networks have been a popular way to address the issue due to its simplicity (Wu et al., 2016; Gehring et al., 2017a; Gehring et al., 2017b; Vaswani et al., 2017). Another related study is Wang et al. (2017). They introduce linear associative units to reduce the length of gradient propagation in recurrent neural networks (RNNs), and demonstrate promising improvements on their RNN-based NMT systems. But previous studies all focus on using the top-level sentence representation for prediction, and ignore the access to the representations encoded in lower-level layers. The next obvious step is toward models that make full use of all stacked layers for prediction (call it representation fusion). Some research groups have been aware of this and explored solutions. E.g., Gehring et al. (2017b) find that"
C18-1255,D16-1137,0,0.0537151,"Missing"
C18-1255,P12-3004,1,0.885328,"Missing"
C18-1255,Q16-1027,0,0.0219878,"e word embedding layer has a larger weight. This is reasonable because most of these words have specific meanings and do not need high-level representations for modeling large context in disambiguation. 3023 5 Related Work Training neural networks with multiple stacked layers is challenging. It has been observed that introducing direct connections between layers can drastically improve the performance of deep neural models. Methods include highway networks (Srivastava et al., 2015), residual connections (He et al., 2016a), dense connections (Huang et al., 2017a), and fast-forward connections (Zhou et al., 2016). E.g., in machine translation, residual networks have been a popular way to address the issue due to its simplicity (Wu et al., 2016; Gehring et al., 2017a; Gehring et al., 2017b; Vaswani et al., 2017). Another related study is Wang et al. (2017). They introduce linear associative units to reduce the length of gradient propagation in recurrent neural networks (RNNs), and demonstrate promising improvements on their RNN-based NMT systems. But previous studies all focus on using the top-level sentence representation for prediction, and ignore the access to the representations encoded in lower-le"
D09-1038,W98-1115,0,0.0439035,"or the SCFG learnt automatically from the training corpus. It can work with an efficient CKY-style binarizer to search for the lowest-cost binarization. We apply our method into a state-of-the-art string-to-tree SMT system. The experimental results show that our method outperforms the synchronous binarization method (Zhang et al., 2006) with over 0.8 BLEU scores on both NIST 2005 and NIST 2008 Chinese-to-English evaluation data sets. 2 Related Work The problem of binarization originates from the parsing problem in which several binarization methods are studied such as left/right binarization (Charniak et al., 1998; Tsuruoka and Tsujii, 2004) and head binarization (Charniak et al., 2006). Generally, the pruning issue in SMT decoding is unnecessary for the parsing problem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tr"
D09-1038,P05-1033,0,0.0773653,"owever, as shown by Chiang (2007), SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtai"
D09-1038,J07-2003,0,0.862926,"method in Zhang et al. (2006) on the NIST machine translation evaluation tasks. 1 Microsoft Research Asia Sigma Center Beijing, China, 100080 muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com binarize an SCFG in such a way that both the source-side and target-side virtual non-terminals have contiguous spans. This property of synchronous binarization guarantees the polynomial time complexity of SCFG parsers even when an n-gram language model is integrated, which has been proved to be one of the keys to the success of a string-to-tree syntax-based SMT system. However, as shown by Chiang (2007), SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004"
D09-1038,P06-1121,0,0.465965,"SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtain a set of equivalent binary rules using th"
D09-1038,W07-0405,0,0.0851011,"oblem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2006) for syntaxbased SMT systems. In addition, it cannot be easily integrated into the decoding of some syntax-based models (Galley et al., 2004; Marcu et al., 2006), because it does not guarantee contiguous spans on the source language side. the ? ?ℎ binary rule in ℬ ?? . Figure 3 illustrates the meanings of these notations with a sample grammar. G R1 : VP → VB NP 会 JJR , VB NP"
D09-1038,W04-3250,0,0.186026,"Missing"
D09-1038,W06-1606,0,0.331083,"2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2006) for syntaxbased SMT systems. In addition, it cannot be easily integrated into the decoding of some syntax-based models (Galley et al., 2004; Marcu et al., 2006), because it does not guarantee contiguous spans on the source language side. the ? ?ℎ binary rule in ℬ ?? . Figure 3 illustrates the meanings of these notations with a sample grammar. G R1 : VP → VB NP 会 JJR , VB NP will be JJR R2 : S → NP 会 VP , NP will VP binarization G’ (R1) v11 : VP → V12 JJR , V12 JJR v12 : V12 → VB V13 , VB V13 v13 : V13 → NP 会 , NP will be (R2) v21 : S → V22 VP , v22 : V22 → NP 会 , rule bucket v11 3 Synchronous Binarization Optimization by Cost Reduction As discussed in Section 1, binarizing an SCFG in a fixed (left-heavy) way (Zhang et al., 2006) may lead to a large n"
D09-1038,H05-1101,0,0.0456339,"h it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtain a set of equivalent binary rules using the synchronous binarization method (Zhang et al., 2006) as follows: VP → V1 JJR , V1 → VB V2 , V2 → NP 会 , V1 JJR VB V2 NP will be This binarization is shown with the solid lines as binarization (a) in Figure 1."
D09-1038,D08-1018,0,0.0770688,"NIST 2008 Chinese-to-English evaluation data sets. 2 Related Work The problem of binarization originates from the parsing problem in which several binarization methods are studied such as left/right binarization (Charniak et al., 1998; Tsuruoka and Tsujii, 2004) and head binarization (Charniak et al., 2006). Generally, the pruning issue in SMT decoding is unnecessary for the parsing problem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2"
D09-1038,D07-1078,0,0.038172,"r development data set comes from NIST2003 evaluation data in which the sentences of more than 20 words are excluded to speed up the Minimum Error Rate Training (MERT). The test data sets are the NIST evaluation sets of 2005 and 2008. Our string-to-tree SMT system is built based on the work of (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the training corpus, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Before the rule extraction, we also binarize the parse trees on the English side using Wang et al. (2007) „s method to increase the coverage of GHKM and SPMT rules. There are totally 4.26M rules after the low frequency rules are filtered out. The pruning strategy is similar to the cube pruning described in (Chiang, 2007). To achieve acceptable translation speed, the beam size is set to 50 by default. The baseline system is based on the synchronous binarization (Zhang et al., 2006). 4.2 Binarization Schemes Besides the baseline (Zhang et al., 2006) and iterative cost reduction binarization methods, we also perform right-heavy and random synchronous binarizations for comparison. In this paper, the"
D09-1038,N06-1033,0,0.250451,"ing polynomial time complexity of decoding for SCFG parsing based machine translation systems. In this paper, we first investigate the excess edge competition issue caused by a leftheavy binary SCFG derived with the method of Zhang et al. (2006). Then we propose a new binarization method to mitigate the problem by exploring other alternative equivalent binary SCFGs. We present an algorithm that iteratively improves the resulting binary SCFG, and empirically show that our method can improve a string-to-tree statistical machine translations system based on the synchronous binarization method in Zhang et al. (2006) on the NIST machine translation evaluation tasks. 1 Microsoft Research Asia Sigma Center Beijing, China, 100080 muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com binarize an SCFG in such a way that both the source-side and target-side virtual non-terminals have contiguous spans. This property of synchronous binarization guarantees the polynomial time complexity of SCFG parsers even when an n-gram language model is integrated, which has been proved to be one of the keys to the success of a string-to-tree syntax-based SMT system. However, as shown by Chiang (2007), SCFGbased decod"
D09-1038,N04-1035,0,\N,Missing
D09-1038,N06-1022,0,\N,Missing
D09-1114,D08-1011,0,0.0122079,"mbination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translat"
D09-1114,2008.amta-srw.3,0,0.173771,". As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment betw"
D09-1114,P05-1033,0,0.863722,"e principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level co"
D09-1114,C08-1005,0,0.0133609,"el or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diver"
D09-1114,N07-2015,0,0.183044,"Missing"
D09-1114,P05-3026,0,0.0288038,"pts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models a"
D09-1114,W04-3250,0,0.482397,"typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, di"
D09-1114,P09-1066,1,0.820368,"re to be matched in computing ?(?, ℋ(?)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement features as Li et al. (2009) did in their co-decoding method. 4 ?∈ℋ ? ?+ (?, ℋ ? ) = tions, we also introduce a set of n-gram disagreement features in the combination model: 4.1 Experiments Data Experiments were conducted on the NIST evaluation sets of 2004 (MT04) and 2005 (MT05) for Chinese-to-English translation tasks. Both corpora provide 4 reference translations per source sentence. Parameters were tuned with MERT algorithm (Och, 2003) on the NIST evaluation set of 2003 (MT03) for both the baseline systems and the system combination model. Translation performance was measured in terms of caseinsensitive NIST version"
D09-1114,2006.amta-papers.11,0,0.0160705,"ization project settles in Zhoushan China 's largest desalination project in Zhoushan China 's largest sea water desalination project in Zhoushan Chinese 海水 淡化 海水 淡化 English desalination sea water desalination ? ?? 0.4000 0.1748 0.0923 Table 2: Parameters of related phrases for examples in Table 1. The second aspect motivating our work comes from the subspace learning method in machine learning literature (Ho, 1998), in which an ensemble of classifiers are trained on subspaces of the full feature space, and final classification results are based on the vote of all classifiers in the ensemble. Lopez and Resnik (2006) also showed that feature engineering could be used to overcome deficiencies of poor alignment. To illustrate the usefulness of feature subspace in the SMT task, we start with the example shown in Table 1. In the example, the Chinese source sentence is translated with two settings of a hierarchical phrase-based system (Chiang, 2005). In the default setting all the features are used as usual in the decoder, and we find that the translation of the Chinese word 海水 (sea water) is missing in the output. This can be explained with the data shown in Table 2. Because of noises and word alignment error"
D09-1114,P06-1077,0,0.0298112,"e sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypot"
D09-1114,W06-1606,0,0.0444251,"systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original o"
D09-1114,D07-1105,0,0.0609944,"multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translation outputs. In this paper, we address the problem of building an ensemble of diversified machine translation systems from a single translation engine for system combination. In particular, we propose a novel Feature Subspace method for the ensemble construction based on any baseline SMT model which can be formulated as a standard l"
D09-1114,E06-1005,0,0.0510437,"multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combination strategy, different methods can be used. Sentence-level combination methods directly select hypotheses from original outputs of single SMT systems (Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand a"
D09-1114,P02-1038,0,0.183819,", for a specific sentence some individual systems could generate better translations. It is expected that employing an ensemble of subspace-based systems and making use of consensus between them will outperform the baseline system. 3 Feature Subspace Method for SMT System Ensemble Construction In this section, we will present in detail the method for systematically deriving SMT systems from a standard linear SMT model based on feature subspaces for system combination. 3.1 SMT System Ensemble Generation Nowadays most of the state-of-the-art SMT systems are based on linear models as proposed in Och and Ney (2002). Let ? (?, ?) be a feature function, and ?? be its weight, an SMT model ? can be formally written as: ? ∗ = argmax ? ?? ? (?, ?) (1) ? Noticing that Equation (1) is a general formulation independent of any specific features, technically for any subset of features used in ?, a new SMT system can be constructed based on it, which we call a sub-system. Next we will use Ω to denote the full feature space defined by the entire set of features used in ?, and ? ⊆ Ω is a feature subset that belongs to ?(Ω), the power set of Ω. The derived subsystem based on subset ? ⊆ Ω is denoted by ?? . Although"
D09-1114,P03-1021,0,0.0402856,"occurrences of n-grams of ? in ? ′ : ?? ?, ? ′ = ? −?+1 ?=1 ?(???+?−1 , ? ′ ) ?− (?, ℋ ? ) = ( ? − ? + 1 − ?? (?, ? ′ )) ? ′ ∈ℋ ? ,? ′ ≠? (6) Because each order of n-gram match introduces two features, the total number of features in the combination model will be 2? + 2 if ? orders of n-gram are to be matched in computing ?(?, ℋ(?)). Since we also adopt a linear scoring function in Equation (3), the feature weights of our combination model can also be tuned on a development data set to optimize the specified evaluation metrics using the standard Minimum Error Rate Training (MERT) algorithm (Och 2003). Our method is similar to the work proposed by Hildebrand and Vogel (2008). However, except the language model and translation length, we only use intra-hypothesis n-gram agreement features as Hildebrand and Vogel did and use additional intra-hypothesis n-gram disagreement features as Li et al. (2009) did in their co-decoding method. 4 ?∈ℋ ? ?+ (?, ℋ ? ) = tions, we also introduce a set of n-gram disagreement features in the combination model: 4.1 Experiments Data Experiments were conducted on the NIST evaluation sets of 2004 (MT04) and 2005 (MT05) for Chinese-to-English translation tasks. B"
D09-1114,J03-1002,0,0.00495516,"∙,∙) is the indicator function ? ???+?−1 , ? ′ is 1 when the n-gram ???+?−1 appears in ? ′ , otherwise it is 0. In order to give the combination model an opportunity to penalize long but inaccurate transla1099 Data set #Sentences MT03 (dev) 919 MT04 (test) 1,788 MT05 (test) 1,082 #Words 23,782 47,762 29,258 Table 3: Data set statistics. We use the parallel data available for the NIST 2008 constrained track of Chinese-toEnglish machine translation task as bilingual training data, which contains 5.1M sentence pairs, 128M Chinese words and 147M English words after pre-processing. GIZA++ toolkit (Och and Ney, 2003) is used to perform word alignment in both directions with default settings, and the intersect-diag-grow method is used to generate symmetric word alignment refinement. The language model used for all systems is a 5-gram model trained with the English part of bilingual data and Xinhua portion of LDC English Gigaword corpus version 3. In experiments, multiple language model features with the order ranging from 2 to 5 can be easily obtained from the 5gram one without retraining. 4.2 System Description Theoretically our method is applicable to all linear model –based SMT systems. In our experimen"
D09-1114,J04-4002,0,0.0458893,"he baseline model (typically a subset of it). We will discuss the principles to determine the feature sets for derived systems, and present in detail the system combination model used in our work. Evaluation is performed on the data sets for NIST 2004 and NIST 2005 Chinese-to-English machine translation tasks. Experimental results show that our method can bring significant improvements to baseline systems with state-of-the-art performance. 1 Introduction Research on Statistical Machine Translation (SMT) has shown substantial progress in recent years. Since the success of phrase-based methods (Och and Ney, 2004; Koehn, 2004), models based on formal syntax (Chiang, 2005) or linguistic syntax (Liu et al., 2006; Marcu et al., 2006) have also achieved state-of-the-art performance. As a result of the increasing numbers of available machine translation systems, studies on system combination have been drawing more and more attention in SMT research. There have been many successful attempts to combine outputs from multiple machine translation systems to further improve translation quality. A system combination model usually takes nbest translations of single systems as input, and depending on the combinatio"
D09-1114,N07-1029,0,0.0865499,"(Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed t"
D09-1114,P07-1040,0,0.17902,"(Sim et al., 2007; Hildebrand and Vogel, 2008), while phrase-level or word–level combination methods are more complicated and could produce new translations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed t"
D09-1114,W08-0329,0,0.0169325,"ranslations different from any translations in the input (Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006; Sim et al., 2007). Among all the factors contributing to the success of system combination, there is no doubt that the availability of multiple machine translation systems is an indispensable premise. Although various approaches to SMT system combination have been explored, including enhanced combination model structure (Rosti et al., 2007), better word alignment between translations (Ayan et al., 2008; He et al., 2008) and improved confusion network construction (Rosti et al., 2008), most previous work simply used the ensemble of SMT systems based on different models and paradigms at hand and did not tackle the issue of how to obtain the ensemble in a principled way. To our knowledge the only work discussed this problem is Macherey and Och (2007), in which they experimented with building different SMT systems by varying one or more sub-models (i.e. translation model or distortion model) of an existing SMT system, and observed that changes in early-stage model training introduced most diversities in translation outputs. In this paper, we address the problem of building an"
D09-1114,P06-1066,0,0.0451678,"veloped systems are used to validate our method. The first one (SYS1) is a system based on the hierarchical phrase-based model as proposed in (Chiang, 2005). Phrasal rules are extracted from all bilingual sentence pairs, while hierarchical rules with variables are extracted from selected data sets including LDC2003E14, LDC2003E07, LDC2005T06 and LDC2005T10, which contain around 350,000 sentence pairs, 8.8M Chinese words and 10.3M English words. The second one (SYS2) is a reimplementation of a phrase-based decoder with lexicalized reordering model based on maximum entropy principle proposed by Xiong et al. (2006). All bilingual data are used to extract phrases up to length 3 on the source side. In following experiments, we only consider removing common features shared by both baseline systems for feature subspace generation. Rule penalty feature and lexicalized reordering feature, which are particular to SYS1 and SYS2, are not used. We list the features in consideration as follows:  PEF and PFE: phrase translation probabilities ? ? ? and ? ? ?  PEFLEX and PFELEX: lexical weights ???? ? ? and ???? ? ?  PP: phrase penalty  WP: word penalty  BLP: bi-lexicon pair counting how many entries of a conven"
D09-1114,2005.eamt-1.20,0,\N,Missing
D17-1150,P17-1175,0,0.0355135,"Missing"
D17-1150,W14-4012,0,0.241662,"Missing"
D17-1150,D14-1179,0,0.0632807,"Missing"
D17-1150,P16-1078,0,0.067992,"ence of tokens. The most fundamental approaches transform the source sentence sequentially into a fixed-length context vector, and the annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b). Although Bahdanau et al. (2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly, these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the translation performance. The tree-to-sequence model encodes a source sentence according to a given syntactic tree 1432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics over the sentence. The existing tree-based encoders (Tai et al., 2015; Erig"
D17-1150,D13-1176,0,0.458623,"lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks. 1 PRP1 VP3 VBP5 I Neural machine translation (NMT) automatically learns the abstract features of and semantic relationship between the source and target sentences, and has recently given state-of-the-art results for various translation tasks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). The most widely used model is the encoder-decoder framework (Sutskever et al., 2014), in which the source sentence is encoded into a dense representation, followed by a decoding process which generates the target translation. By exploiting the attention mechanism (Bahdanau et al., 2015), the generation of target words is conditional on the source hidden states, rather than on the context vector alone. From a model architecture perspective, prior studies of the attentive Corresponding author take NP4 PRT6 up NP7 PP8 DT9 NN10 IN11 a position in N"
D17-1150,E17-2093,0,0.113445,"Missing"
D17-1150,D15-1166,0,0.384069,"GRU. The i-th leaf node vector is calculated as: l hli = fGRU (xi , hli−1 ), (1) where xi is the i-th source word embedding and hli−1 denotes the previous hidden state. The parent hidden state h↑i,j summarizes its left child h↑i,k and a non-linear function fsof tmax : p(yj |y1 , ..., yj−1 , x, tr; θ) = fsof tmax (cj ), where cj is the composite hidden state, which consists of a target hidden state sj and a context vector dj : cj = ftanh ([sj , dj ]). Given the previous target word yj−1 , the concatenation of the previous hidden state sj−1 and the previous context vector cj−1 (input-feeding) (Luong et al., 2015), sj , is calculated using a standard sequential GRU network: dec sj = fgru (yj−1 , [sj−1 , cj−1 ]). The context vector dj is computed using an attention model which is used to softly summarize the attended part of the source-side representations. Eriguchi et al. (2016) adopted a tree-based attention mechanism to consider both the word and phrase vectors: right child h↑k+1,j (i &lt; k &lt; j) by applying the tree-GRU (Zhou et al., 2016) as follows: ↑ zi,j = ↑ = ri,k ↑ = rk+1,j e h↑i,j = h↑i,j = R ↑ U(z) hk+1,j i=1 αj (t) = et = + N −1 X k=1 αj (k)hpk , (2) PN exp(et ) PN −1 l i=1 exp(ei ) + (Va )T t"
D17-1150,P02-1040,0,0.0997821,"ectively set as 620 and 1,000. Due to the concatenation in the bidirectional leaf-node encoding, the dimensions of the forward and backward vectors, which are half of those of the other hidden states, are set to 500. In order to prevent over-fitting, the training data is shuffled following each epoch. Moreover, the model parameters are optimized using AdaDelta (Zeiler, 2012), due to its capability for dynamically adapting the learning rate. We set the mini-batch size to 16 and the beam search size to 5. The accuracy of the translation relative to a reference is assessed using the BLEU metric (Papineni et al., 2002). In order to give an equitable comparison, all the NMT models used for comparison are implemented or re-implemented using GRU in our code, based on dl4mt4 . 4.3 Enhanced Hierarchical Representations Firstly, the effectiveness of the enhanced hierarchical representations is evaluated through a set of experiments, the results of which are summarized in Table 3. Compared with the original tree-based encoder (Eriguchi et al., 2016), the model with bidirectional leaf-node encoding (described in Section 3.1) shows better performance. This also reveals that the future context at leaf level can contr"
D17-1150,P16-2049,0,0.0377221,"annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b). Although Bahdanau et al. (2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly, these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the translation performance. The tree-to-sequence model encodes a source sentence according to a given syntactic tree 1432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics over the sentence. The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotati"
D17-1150,P15-1150,0,0.247232,"Missing"
D17-1150,P12-3004,1,0.83203,"ed 1.4M sentence pairs, in which the maximum length of the sentence was 40, from the LDC parallel corpus3 as our training data. The models were developed using NIST mt08 data and were examined using NIST mt04, mt05, and mt06 data. The number of sentences in each dataset is shown in Table 1. On the English side, we used the constituent parser (Zeng et al., 2014, 2015) to produce a binary syntactic tree for each sentence, in constrast to the use of the HPSG parser by Eriguchi et al. (2016). On the Chinese side, the sentences are segmented using the Chinese word segmentation toolkit of NiuTrans (Xiao et al., 2012). To avoid data sparsity, words referring to time, date and number, which are low in frequency, are generalized as ‘$time’, ‘$date’ and ‘$number’. In addition, as described in Section 3.3, the vocabularies are further compressed by segmenting the rare words into sub-word units using BPE. 4.2 Experimental Settings As shown in Table 2, which gives the statistics of the token types, we limit the source and target vo3 Our training data was selected from LDC2000T46, LDC2000T50, LDC2003E14, LDC2004T08, LDC2004T08 and LDC2005T10. Training Set |V |in En |V |in Zh Original 159k 198k Generalization 120k"
D17-1150,C16-1274,0,0.0951232,". By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the translation performance. The tree-to-sequence model encodes a source sentence according to a given syntactic tree 1432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics over the sentence. The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases. As a result, the learned representations are limited to local information, while failing to capture the global meaning of a sentence. As illustrated in Figure 1, the phrases “take up”1 and “a position”2 have different meanings in different contexts. However, in composing the representations hVP3 and hNP7 for phrases VP3 and NP7 , the current approaches do not account for the differences in meaning which arise as a result of ig"
D17-1150,W16-2209,0,0.0354317,"length context vector, and the annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b). Although Bahdanau et al. (2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly, these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the translation performance. The tree-to-sequence model encodes a source sentence according to a given syntactic tree 1432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics over the sentence. The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up f"
D17-1150,P16-1162,0,0.147293,"formation between the word and phrase vectors. To alleviate the out-ofvocabulary (OOV) problem, we further extend the proposed tree-based model to the sub-word level 1 Take up has the meanings of start doing something new, use space/time, accept an offer, etc. 2 Position has the meanings of location, job offer, rank/status, etc. hp1,8 hp2,8 hp2,3 hp4,8 hp6,8 hp4,5 hp7,8 hl1 hl2 hl3 hl4 hl5 I e tak up a i pos hl6 n tio in hl7 hl8 hl9 m i the roo heos Figure 2: The tree-based model of Eriguchi et al. (2016) comprising a structured and sequential encoder. by integrating byte-pair encoding (BPE) (Sennrich et al., 2016) into the tree-based model (as described in Section 3.3). Experimental results for the NIST English-to-Chinese translation task reveal that the proposed model significantly outperforms the vanilla tree-based (Eriguchi et al., 2016) and sequential NMT models (Bahdanau et al., 2015) (Section 4.1). 2 Tree-Based Neural Machine Translation A neural machine translation system (NMT) aims to use a single neural network to build a translation model, which is trained to maximize the conditional distribution of sentence pairs using a parallel training corpus (Kalchbrenner and Blunsom, 2013; Sutskever et"
D19-1367,N19-1078,0,0.39835,"dation perplexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF 3587 Valid Perplexity model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016). We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout to the input and output of the RNN layer. We train the network using SGD with a learning rate of 0.1 and a gradient clipping threshold of 5.0. We reduce the learning rate by a factor of 0.25 if the test error does not decrease for 2 epochs. Table 2 shows a comparison of different methods. Our baseline uses RNN cells generated from random initialized whose F1-score varies greatly and is lower than that of"
D19-1367,C18-1139,0,0.163871,"tion perplexities over 4 different runs at different search epochs. We see that I-DARTS is easier to converge than DARTS (4 hours). It is 1.4X faster than that of DARTS. Another interesting finding is that I-DARTS achieves a lower validation perplexity than DARTS during architecture search. This may indicate better architectures found by I-DARTS because the search model is optimized with respect to validation perplexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF 3587 Valid Perplexity model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016). We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout"
D19-1367,N16-1030,0,0.483792,"TS during architecture search. This may indicate better architectures found by I-DARTS because the search model is optimized with respect to validation perplexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF 3587 Valid Perplexity model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016). We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout to the input and output of the RNN layer. We train the network using SGD with a learning rate of 0.1 and a gradient clipping threshold of 5.0. We reduce the learning rate by a factor of 0.25 if the test error does not decrease for 2 epochs. Table 2 shows a compa"
D19-1367,P16-1101,0,0.0541327,"e search. This may indicate better architectures found by I-DARTS because the search model is optimized with respect to validation perplexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF 3587 Valid Perplexity model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016). We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout to the input and output of the RNN layer. We train the network using SGD with a learning rate of 0.1 and a gradient clipping threshold of 5.0. We reduce the learning rate by a factor of 0.25 if the test error does not decrease for 2 epochs. Table 2 shows a comparison of different m"
D19-1367,D14-1162,0,0.0787964,"because the search model is optimized with respect to validation perplexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF 3587 Valid Perplexity model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016). We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout to the input and output of the RNN layer. We train the network using SGD with a learning rate of 0.1 and a gradient clipping threshold of 5.0. We reduce the learning rate by a factor of 0.25 if the test error does not decrease for 2 epochs. Table 2 shows a comparison of different methods. Our baseline uses RNN cells generated from random initialized w"
D19-1367,P17-1161,0,0.0154193,"er 4 different runs at different search epochs. We see that I-DARTS is easier to converge than DARTS (4 hours). It is 1.4X faster than that of DARTS. Another interesting finding is that I-DARTS achieves a lower validation perplexity than DARTS during architecture search. This may indicate better architectures found by I-DARTS because the search model is optimized with respect to validation perplexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF 3587 Valid Perplexity model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016). We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout to the input and outpu"
D19-1367,N18-1202,0,0.0612954,"Missing"
D19-1367,W03-0419,0,0.113383,"Missing"
I17-1052,N16-1174,0,0.0201665,"el and train the top sentiment analysis model. All models are trained by minimizing the sum of cross-entropy loss and a L2 regularization loss of all trainable weights ∆W . Target-dependent Sentiment Model We use the attention-based model of Liu and Zhang (2017) as our top level model. The overall structure is shown in Figure 5. Given a sentence, it first uses several BiLSTM layers to learn its syntactic features, and then an attention layer is used to select the relative wegihts of the words according to the target entity over the untargeted words in the whole sentence (Bahdanau et al. 2014; Yang et al. 2016). In particular, for a target word, it applies the target word hidden vector to find a weight forevery word (except the target words) in the sentence (see Figure 5). The model also uses a BiLSTM to represent the feature layer from bottom syntactic model fb (b) and the word embedding wet (w) of a word sequence w1 , w2 , ..., wn as the hidden vector of each word. loss = 1 n n P i σ(yi , yi0 ) + λ2 ||∆W ||2 , (11) The model feature inputs (word embeddings, POS-tag embeddings) are the sum of a trainable embedding and a pre-trained (or learned) embedding. All the weight matrix will be initialized w"
I17-1052,P16-1147,0,0.0678698,"Missing"
I17-1052,D16-1070,1,0.814639,"should be useful for sentiment analysis given a target, since target-related semantic information such as predicate-argument structure information is contained in syntactic structures. The main issue of Dong et al. (2014)’s method is that explicit syntactic structures are inaccurate and noisy. We try to avoid this issue by using implicit syntactic information, by integrating the hidden feature layers of a state-of-the-art neural dependency parsing as features to the state-of-the-art targeted sentiment classification models of Liu and Zhang (2017), using neural stacking (Zhang and Weiss 2016; Chen et al. 2016). The main structure of our model is shown in Figure 2. We choose the parser of Dozat and Manning 2 Model As shown in Figure 2, our neural stacking model consists of two brief components: a bottom level syntactic model for obtaining the syntactic information and a top level sentiment model for targetdependent sentiment classification. 2.1 Input representation Given an input sentence, we first obtain its word representations. In particular, we train two separate word embedding sets for the bottom level syntactic model and top level sentiment model, respectively, denoted as web and wet , respect"
I17-1052,J11-1005,1,0.792449,"in Table 2. A best model on the devset is saved for the neural stacking bottom syntax model. Once obtaining the pre-trained bottom syntax model, we build the top sentiment model based on intermediate output syntax model features fb and top word embedding wet . Data We conduct experiments on two datasets, one being the training/dev/test dataset of Zhang et al. (2016) (Z-Set), which consists of the MPQA corpus1 and Mitchell et al. (2013)’s corpus2 , the other being the dataset of the benchmark training/test dataset of ? (T-Set), we label these datasets’ POStags with the open parser tools ZPar (Zhang and Clark, 2011). Two sets of word embedding are used in this experiment: The GloVe3 (Pennington et al., 2014) twitter embedding (100 dimensions) for the bottom model, and the GloVe twit3.3 Hyper-parameters Embedding Size: Our embedding is a superposition of a trainable and a pre-trained word embedding. We fixed the word embedding dimension of web and wet to 100 and 200, respectively to match two pre-trained GloVe word embeddings set from Pennington et al. (2014). 1 http://mpqa.cs.pitt.edu/corpora/mpqa corpus/ http://www.m-mitchell.com/code/index.html 3 https://nlp.stanford.edu/projects/glove/ 2 520 Models PO"
I17-1052,P14-2009,0,0.155984,"vements on various benchmark datasets. We obtain the best accuracies on two different test sets for targeted sentiment. 1 Table 1: Target-dependent sentiment analysis - + 0 L Left Context ⊕ Target R Right Context w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13 w14 Figure 1: Sentence level context the target Twitter and a negative (−) sentiment label on the target Facebook. The task has been addressed using neural network models, which learn target-specific representations of the input sentence. These representations are then used for predicting target-dependent sentiment polarities. In particular, Dong et al. (2014) derive the syntactic structure of input sentence using a dependency grammar, before transforming the tree structure to a target-centered form. A recursive neural network is used to transform the dependency syntax of a sentence into a target-specific vector for sentiment classification. More recently Vo and Zhang (2015) split the input sentence into three segments, with the target entity mention being in the center, and its left and right contexts surrounding it, as shown in Figure 1. Introduction Target-dependent sentiment analysis investigates the problem of assigning sentiment polarity labe"
I17-1052,P81-1022,0,0.740008,"Missing"
I17-1052,P11-1016,0,0.109046,"Missing"
I17-1052,D15-1025,0,0.0214339,"Missing"
I17-1052,E17-2091,1,0.902556,"owed Vo and Zhang (2015), avoiding the use of syntactic information explicitly. Zhang et al. (2016) applied a bidirectional Gated RNN to learn a dense representation of the input sentence, and then use a threeway gated network structure to integrate target entity mention and its left and right contexts. The final resulting representation is used for softmax classification. Tang et al. (2015) also use a RNN (LSTM) to represent the input sentence, yet directly integrating the target embedding to each hidden state for deriving a target-specific vector, which is used for sentiment classification. Liu and Zhang (2017) extended both Zhang et al. (2016) and Tang et al. (2015) by introducing the attention mechanism, obtaining the best accuracies on both datasets so far. Intuitively, syntactic information should be useful for sentiment analysis given a target, since target-related semantic information such as predicate-argument structure information is contained in syntactic structures. The main issue of Dong et al. (2014)’s method is that explicit syntactic structures are inaccurate and noisy. We try to avoid this issue by using implicit syntactic information, by integrating the hidden feature layers of a sta"
I17-1052,D13-1171,0,0.0674043,"Missing"
I17-1052,D14-1162,0,0.0789272,"Missing"
P10-1076,P05-1033,0,0.691849,"is work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combin"
P10-1076,J07-2003,0,0.484468,"Parser is used to generate the English parse trees for the rule extraction of the syntax-based system. The data set used for weight training in boostingbased system combination comes from NIST MT03 evaluation set. To speed up MERT, all the sentences with more than 20 Chinese words are removed. The test sets are the NIST evaluation sets of MT04, MT05 and MT06. The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric. Statistical significant test is conducted using the bootstrap resampling method proposed by Koehn (2004). Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems. By default, both of the beam size and the size of n-best list are set to 20. In the settings of boosting-based system combination, the maximum number of iterations is set to 30, and k (in Equation 7) is set to 5. The ngram consensuses-based features (in Equation 9) used in system combination ranges from unigram to 4-gram. 5.3 Evaluation of Translations First we investigate the effectiveness of the boosting-based system combination on the three systems. Figures 2-5 show the BLEU curves on the development and test sets, where"
P10-1076,D08-1024,0,0.0598251,"Missing"
P10-1076,W99-0613,0,0.098175,".47 41.51* Hiero 46.51 47.44* 44.52 45.47* 42.47 43.44* 39.39 40.10* Syntaxbased 46.92 48.70* 46.88 49.40* 45.21 47.02* 40.52 41.88* Table 2: Oracle performance of various systems. * = significantly better than baseline (p < 0.05). 6 Related Work Boosting is a machine learning (ML) method that has been well studied in the ML community (Freund, 1995; Freund and Schapire, 1997; Collins et al., 2002; Rudin et al., 2007), and has been successfully adopted in natural language processing (NLP) applications, such as document classification (Schapire and Singer, 2000) and named entity classification (Collins and Singer, 1999). However, most of the previous work did not study the issue of how to improve a single SMT engine using boosting algorithms. To our knowledge, the only work addressing this issue is (Lagarda and Casacuberta, 2008) in which the boosting algorithm was adopted in phrase-based SMT. However, Lagarda and Casacuberta (2008)’s method calculated errors over the phrases that were chosen by phrase-based systems, and could not be applied to many other SMT systems, such as hierarchical phrase-based systems and syntax-based systems. Differing from Lagarda and Casacuberta’s work, we are concerned more with"
P10-1076,P05-1067,0,0.0542414,"nglish Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and"
P10-1076,D09-1114,1,0.814642,"iffering from Lagarda and Casacuberta’s work, we are concerned more with proposing a general framework which can work with most of the current SMT models and empirically demonstrating its effectiveness on various SMT systems. There are also some other studies on building diverse translation systems from a single translation engine for system combination. The first attempt is (Macherey and Och, 2007). They empirically showed that diverse translation systems could be generated by changing parameters at early-stages of the training procedure. Following Macherey and Och (2007)’s work, Duan et al. (2009) proposed a feature subspace method to build a group of translation systems from various different sub-models of an existing SMT system. However, Duan et al. (2009)’s method relied on the heuristics used in feature sub-space selection. For example, they used the remove-one-feature strategy and varied the order of n-gram language model to obtain a satisfactory group of diverse systems. Compared to Duan et al. (2009)’s method, a main advantage of our method is that it can be applied to most of the SMT systems without designing any heuristics to adapt it to the specified systems. 7 Discussion and"
P10-1076,P03-2041,0,0.0433043,"n Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combi"
P10-1076,D08-1089,0,0.0533933,"Missing"
P10-1076,2008.amta-srw.3,0,0.132024,"nd Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al., 2006; Rosti et al., 2007), can generate new translations differing from any of the original translations. One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. However, this requirement cannot be met in many cases, sin"
P10-1076,P07-1019,0,0.0341874,"Berkeley Parser is used to generate the English parse trees for the rule extraction of the syntax-based system. The data set used for weight training in boostingbased system combination comes from NIST MT03 evaluation set. To speed up MERT, all the sentences with more than 20 Chinese words are removed. The test sets are the NIST evaluation sets of MT04, MT05 and MT06. The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric. Statistical significant test is conducted using the bootstrap resampling method proposed by Koehn (2004). Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems. By default, both of the beam size and the size of n-best list are set to 20. In the settings of boosting-based system combination, the maximum number of iterations is set to 30, and k (in Equation 7) is set to 5. The ngram consensuses-based features (in Equation 9) used in system combination ranges from unigram to 4-gram. 5.3 Evaluation of Translations First we investigate the effectiveness of the boosting-based system combination on the three systems. Figures 2-5 show the BLEU curves on the development and test sets, where"
P10-1076,N03-1017,0,0.0224707,"Missing"
P10-1076,W04-3250,0,0.091731,"e Xinhua portion of English Gigaword corpus. Berkeley Parser is used to generate the English parse trees for the rule extraction of the syntax-based system. The data set used for weight training in boostingbased system combination comes from NIST MT03 evaluation set. To speed up MERT, all the sentences with more than 20 Chinese words are removed. The test sets are the NIST evaluation sets of MT04, MT05 and MT06. The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric. Statistical significant test is conducted using the bootstrap resampling method proposed by Koehn (2004). Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems. By default, both of the beam size and the size of n-best list are set to 20. In the settings of boosting-based system combination, the maximum number of iterations is set to 30, and k (in Equation 7) is set to 5. The ngram consensuses-based features (in Equation 9) used in system combination ranges from unigram to 4-gram. 5.3 Evaluation of Translations First we investigate the effectiveness of the boosting-based system combination on the three systems. Figures 2-5 show"
P10-1076,2006.amta-papers.25,0,0.0162744,"ore of the group of baseline candidates that are generated in advance (Section 5.1). We see that the diversities of all the systems increase during iterations in most cases, though a few drops occur at a few points. It indicates that our method is very effective to generate diversified member systems. In addition, the diversities of baseline systems (iteration 1) are much lower 5.4 Diversity among Member Systems We also study the change of diversity among the outputs of member systems during iterations. The diversity is measured in terms of the Translation Error Rate (TER) metric proposed in (Snover et al., 2006). A higher TER score means that more edit operations are performed if we transform one translation output into another 745 than those of the systems generated by boosting (iterations 2-30). Together with the results shown in Figures 2-5, it confirms our motivation that the diversified translation outputs can lead to performance improvements over the baseline systems. Also as shown in Figures 6-9, the diversity of the Hiero system is much lower than that of the phrase-based and syntax-based systems at each individual setting of iteration number. This interesting finding supports the observation"
P10-1076,2008.eamt-1.14,0,0.432255,"n baseline (p < 0.05). 6 Related Work Boosting is a machine learning (ML) method that has been well studied in the ML community (Freund, 1995; Freund and Schapire, 1997; Collins et al., 2002; Rudin et al., 2007), and has been successfully adopted in natural language processing (NLP) applications, such as document classification (Schapire and Singer, 2000) and named entity classification (Collins and Singer, 1999). However, most of the previous work did not study the issue of how to improve a single SMT engine using boosting algorithms. To our knowledge, the only work addressing this issue is (Lagarda and Casacuberta, 2008) in which the boosting algorithm was adopted in phrase-based SMT. However, Lagarda and Casacuberta (2008)’s method calculated errors over the phrases that were chosen by phrase-based systems, and could not be applied to many other SMT systems, such as hierarchical phrase-based systems and syntax-based systems. Differing from Lagarda and Casacuberta’s work, we are concerned more with proposing a general framework which can work with most of the current SMT models and empirically demonstrating its effectiveness on various SMT systems. There are also some other studies on building diverse transla"
P10-1076,D09-1038,1,0.848696,"irs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Synchronous binarization (Zhang et al., 2006; Xiao et al., 2009) is performed on each translation rule for the CKYstyle decoding. In this work, baseline system refers to the system produced by the boosting-based system combination when the number of iterations (i.e. T ) is set to 1. To obtain satisfactory baseline performance, we train each SMT system for 5 times using MERT with different initial values of feature weights to generate a group of baseline candidates, and then select the best-performing one from this group as the final baseline system (i.e. the starting point in the boosting process) for the following experiments. 5.2 Experimental Setup Our b"
P10-1076,P09-1066,0,0.0115942,"ng function: 4 e* = arg max ∑ t =1 β t ⋅ φt (e) + ψ (e, H (v)) (8) T e∈H ( v ) where φt (e) is the log-scaled model score of e in the t-th member system, and β t is the corresponding feature weight. It should be noted that e ∈ Hi may not exist in any Hi &apos; ≠ i . In this case, we can still calculate the model score of e in any other member systems, since all the member systems are based on the same model and share the same feature space. ψ (e, H (v)) is a consensusbased scoring function which has been successfully adopted in SMT system combination (Duan et al., 2009; Hildebrand and Vogel, 2008; Li et al., 2009). The computation of ψ (e, H (v)) is based on a linear combination of a set of n-gram consensuses-based features. ψ (e, H (v)) = ∑θ n+ ⋅ hn+ (e, H (v)) + n ∑θ − n ⋅ hn− (e, H (v)) (9) n For each order of n-gram, hn+ (e, H (v)) and hn− (e, H (v)) are defined to measure the n-gram agreement and disagreement between e and other translation candidates in H(v), respectively. θ n+ and θ n− are the feature weights corresponding to hn+ (e, H (v)) and hn− (e, H (v)) . As hn+ (e, H (v)) and hn− (e, H (v)) used in our work are exactly the same as the features used in (Duan et al., 2009) and similar to th"
P10-1076,P06-1066,0,0.0539712,"Missing"
P10-1076,P06-1096,0,0.025681,"in Equation 4 with two parameters α t and li in it. α t can be regarded as a measure of the importance that the t-th weak system gains in boosting. The definition of α t guarantees that α t always has a positive value3. A main effect of α t is to scale the weight updating (e.g. a larger α t means a greater update). li is the loss on the i-th sample. For each i, let {ei1, ..., ein} be the n-best translation candidates produced by the system. The loss function is defined to be: li = BLEU(ei * , ri ) − 1 k ∑ j =1 BLEU(eij, ri) (7) k where BLEU(eij, ri) is the smoothed sentence-level BLEU score (Liang et al., 2006) of the translation e with respect to the reference translations ri, and ei* is the oracle translation which is selected from {ei1, ..., ein} in terms of BLEU(eij, ri). li can be viewed as a measure of the average cost that we guess the top-k translation candidates instead of the oracle translation. The value of li counts for the magnitude of weight update, that is, a larger li means a larger weight update on Dt(i). The definition of the loss function here is similar to the one used in (Chiang et al., 2008) where only the top-1 translation candidate (i.e. k = 1) is taken into account. 3.3 Syst"
P10-1076,N06-1033,0,0.0353874,"system all phrase pairs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Synchronous binarization (Zhang et al., 2006; Xiao et al., 2009) is performed on each translation rule for the CKYstyle decoding. In this work, baseline system refers to the system produced by the boosting-based system combination when the number of iterations (i.e. T ) is set to 1. To obtain satisfactory baseline performance, we train each SMT system for 5 times using MERT with different initial values of feature weights to generate a group of baseline candidates, and then select the best-performing one from this group as the final baseline system (i.e. the starting point in the boosting process) for the following experiments. 5.2 Expe"
P10-1076,P06-1077,0,0.0363268,"ion (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simpl"
P10-1076,D07-1105,0,0.0909649,"g from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al., 2006; Rosti et al., 2007), can generate new translations differing from any of the original translations. One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. However, this requirement cannot be met in many cases, since we do not always have the access to multiple SMT engines due to the high cost of developing and tuning SMT systems. To reduce the burden of system development, it might be a nice way to combine a set of translation systems built from a single translation engine. A key issue here is how to generate an ensemble of diversified translation systems from a single translation engine in a"
P10-1076,W06-1606,0,0.0329812,"stem with two reordering models including the maximum entropy-based lexicalized reordering model proposed by Xiong et al. (2006) and the hierarchical phrase reordering model proposed by Galley and Manning (2008). In this system all phrase pairs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Synchronous binarization (Zhang et al., 2006; Xiao et al., 2009) is performed on each translation rule for the CKYstyle decoding. In this work, baseline system refers to the system produced by the boosting-based system combination when the number of iterations (i.e. T ) is set to 1. To obtain satisfactory baseline performance, we train each SMT system for 5 times using MERT with different initial values of feature weight"
P10-1076,E06-1005,0,0.0445527,"studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al., 2006; Rosti et al., 2007), can generate new translations differing from any of the original translations. One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. However, this requirement cannot be met in many cases, since we do not always have the access to multiple SMT engines due to the high cost of developing and tuning SMT systems. To reduce the burden of system devel"
P10-1076,P02-1038,0,0.15398,"a single engine u(λ*) by adjusting the weight vector λ* in a principled way. In this work, we assume that u1 = u2 =...= uT = u. Our goal is to find a series of λ*i and build a combined system from {u(λ*i)}. To achieve this goal, we propose a Background Given a source string f, the goal of SMT is to find a target string e* by the following equation. e* = arg max(Pr(e |f )) (1) e where Pr(e |f ) is the probability that e is the translation of the given source string f. To model the posterior probability Pr(e |f ) , most of the state-of-the-art SMT systems utilize the loglinear model proposed by Och and Ney (2002), as follows, exp(∑ m =1 λ m ⋅ hm ( f , e)) M Pr(e |f ) = ∑ e &apos; exp(∑ m=1 λ m ⋅ hm( f , e &apos;)) M (2) where {hm( f, e ) |m = 1, ..., M} is a set of features, and λm is the feature weight corresponding to the m-th feature. hm( f, e ) can be regarded as a function that maps every pair of source string f and target string e into a non-negative value, and λm can be viewed as the contribution of hm( f, e ) to the overall score Pr(e |f ) . In this paper, u denotes a log-linear model that has M fixed features {h1( f ,e ), ..., hM( f ,e )}, λ = {λ1, ..., λM} denotes the M parameters of u, and u(λ) denot"
P10-1076,P03-1021,0,0.0703408,"(Freund and Schapire, 1997; Schapire, 2001), the basic idea of this method is to use weak systems (member systems) to form a strong system (combined system) by repeatedly calling weak system trainer on different distributions over the training samples. However, since most of the boosting algorithms are designed for the classification problem that is very different from the translation problem in natural language processing, several key components have to be redesigned when boosting is adapted to SMT system combination. 3.1 Training In this work, Minimum Error Rate Training (MERT) proposed by Och (2003) is used to estimate feature weights λ over a series of training samples. As in other state-of-the-art SMT systems, BLEU is selected as the accuracy measure to define the error function used in MERT. Since the weights of training samples are not taken into account in BLEU2, we modify the original definition of BLEU to make it sensitive to the distribution Dt(i) over the training samples. The modified version of BLEU is called weighted BLEU (WBLEU) in this paper. Let E = e1 ... em be the translations produced by the system, R = r1 ... rm be the reference translations where ri = {ri1, ..., riN},"
P10-1076,P07-1040,0,0.0300808,"combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al., 2006; Rosti et al., 2007), can generate new translations differing from any of the original translations. One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. However, this requirement cannot be met in many cases, since we do not always have the access to multiple SMT engines due to the high cost of developing and tuning SMT systems. To reduce the burden of system development, it might be a"
P10-1076,W06-1628,0,\N,Missing
P10-1076,P06-1121,0,\N,Missing
P11-2073,P05-1033,0,0.180947,"Missing"
P11-2073,P10-1146,0,0.0250602,"Missing"
P11-2073,P03-2041,0,0.0681744,"Missing"
P11-2073,D08-1022,0,0.0304453,"Missing"
P11-2073,N04-1035,0,0.110983,"Missing"
P11-2073,W07-0405,0,0.0361647,"Missing"
P11-2073,P07-1019,0,0.0571337,"Missing"
P11-2073,2006.amta-papers.8,0,0.0606234,"Missing"
P11-2073,D10-1014,0,0.0221351,"Missing"
P11-2073,C10-1080,0,0.0231974,"Missing"
P11-2073,P06-1055,0,0.0390111,"Missing"
P11-2073,C10-2154,1,0.850668,"Missing"
P11-2073,P09-1020,0,\N,Missing
P11-2073,P08-1023,0,\N,Missing
P11-2073,P06-1077,0,\N,Missing
P11-2073,N09-1027,0,\N,Missing
P11-2073,N09-1025,0,\N,Missing
P11-2073,W06-1606,0,\N,Missing
P11-2073,P03-1021,0,\N,Missing
P11-2073,P08-1114,0,\N,Missing
P11-2073,W04-3250,0,\N,Missing
P12-2055,P06-2014,0,0.0795763,"es on the source/target side to learn syntactic transformation rules from parallel data. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spurious links and adding new valuable links based on bilingual translation span correspondences. The proposed approach has two promising properties. Some blocked Tree-to-string Rules: r1: AS(了) → have r2: NN(进口) → the imports r3: S (NN:x1 VP:x2)"
P12-2055,P07-1003,0,0.0953072,"et side to learn syntactic transformation rules from parallel data. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spurious links and adding new valuable links based on bilingual translation span correspondences. The proposed approach has two promising properties. Some blocked Tree-to-string Rules: r1: AS(了) → have r2: NN(进口) → the imports r3: S (NN:x1 VP:x2) → x1 x2 Some blocked Tr"
P12-2055,W08-0306,0,0.081699,"AD VP NN S Figure 1. A real example of Chinese-English sentence pair with word alignment and both-side parse trees. Introduction Most syntax-based statistical machine translation (SMT) systems typically utilize word alignments and parse trees on the source/target side to learn syntactic transformation rules from parallel data. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spuriou"
P12-2055,N04-1035,0,0.148628,"Missing"
P12-2055,C04-1154,0,0.0867544,"Missing"
P12-2055,D09-1024,0,0.082941,"ta. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spurious links and adding new valuable links based on bilingual translation span correspondences. The proposed approach has two promising properties. Some blocked Tree-to-string Rules: r1: AS(了) → have r2: NN(进口) → the imports r3: S (NN:x1 VP:x2) → x1 x2 Some blocked Tree-to-tree Rules: r4: AS(了) → VBZ(have) r5: NN(进口) → NP(D"
P12-2055,H05-1012,0,0.0275651,"of our TSA method on translation quality in tree-to-string and tree-totree translation tasks. Table 5 shows that our TSA method can improve both syntax-based translation systems. As mentioned before, the resulting TSAs are essentially optimized by the translation model. Based on such TSAs, experiments show that spurious link deletion and new valuable link insertion can improve translation quality for tree-to-string and tree-to-tree systems. 5 Related Work Previous studies have made great efforts to incorporate statistics and linguistic heuristics or syntactic information into word alignments (Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore et al. 2006; Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some prev"
P12-2055,J10-3002,0,0.064516,"suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spurious links and adding new valuable links based on bilingual translation span correspondences. The proposed approach has two promising properties. Some blocked Tree-to-string Rules: r1: AS(了) → have r2: NN(进口) → the imports r3: S (NN:x1 VP:x2) → x1 x2 Some blocked Tree-to-tree Rules: r4: AS(了) → VBZ(have) r5: NN(进口) → NP(DT(the) NNS(imports"
P12-2055,D07-1038,0,0.261463,"anshao 了 le VV AS VP AD VP NN S Figure 1. A real example of Chinese-English sentence pair with word alignment and both-side parse trees. Introduction Most syntax-based statistical machine translation (SMT) systems typically utilize word alignments and parse trees on the source/target side to learn syntactic transformation rules from parallel data. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extractio"
P12-2055,P03-1021,0,0.00511311,"erated by composing two or three minimal rules. A 5-gram language model was trained on the Xinhua portion of English Gigaword corpus. Beam search and cube pruning techniques (Huang and Chiang 2007) were used to prune the search space for all the systems. The base feature set used for all systems is similar to that used in (Marcu et al. 2006), including 14 base features in total such as 5gram language model, bidirectional lexical and phrase-based translation probabilities. All features were log-linearly combined and their weights were optimized by performing minimum error rate training (MERT) (Och 2003). The development data set used for weight training comes from NIST MT03 evaluation set, consisting of 326 sentence pairs of less than 20 words in each Chinese sentence. Two test sets are NIST MT04 (1788 sentence pairs) and MT05 (1082 sentence pairs) evaluation sets. The translation quality is evaluated in terms of the caseinsensitive IBM-BLEU4 metric. 4.2 Effect on Word Alignment To investigate the effect of the TSA method on word alignment, we designed an experiment to evaluate alignment quality against gold standard annotations. There are 200 random chosen and manually aligned Chinese-Engli"
P12-2055,N10-1014,0,0.0172675,"(2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls et al. 2010). Some previous methods infer syntactic correspondences between the source and the 283 target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence between languages can prevent accurate syntactic alignments between the source and tar"
P12-2055,P10-1032,0,0.0197267,"r example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls et al. 2010). Some previous methods infer syntactic correspondences between the source and the 283 target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence between languages can prevent accurate syntactic alignment"
P12-2055,C10-1118,0,0.01806,"r example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls et al. 2010). Some previous methods infer syntactic correspondences between the source and the 283 target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence between languages can prevent accurate syntactic alignment"
P12-2055,P12-3004,1,0.889912,"Missing"
P12-2055,P06-1065,0,\N,Missing
P12-2055,P06-1055,0,\N,Missing
P12-2055,W06-1606,0,\N,Missing
P12-2055,P07-1019,0,\N,Missing
P12-2055,2007.mtsummit-papers.62,0,\N,Missing
P12-2055,H05-1010,0,\N,Missing
P12-3004,J07-2003,0,0.61242,"it, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1 Introduction We present NiuTrans, a new open source machine translation toolkit, which was developed for constructing high quality machine translation systems. The NiuTrans toolkit supports most statistical machine translation (SMT) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (Koehn et al., 2003), the hierarchical phrase-based model (Chiang, 2007), and various syntax-based models (Galley et al., 2004; Liu et al., 2006). In particular, a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms. Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system. In addition, the toolkit provides easy-to-use APIs for the development of new features. The toolkit has been used to build transla"
P12-3004,P03-2041,0,0.010524,"o-tree rules on all pairs of source and target tree-fragments. 21 Decoding the hierarchical phrase-based and syntaxbased models. For efficient integration of ngram language model into decoding, rules containing more than two variables are binarized into binary rules. In addition to the rules learned from bilingual data, glue rules are employed to glue the translations of a sequence of chunks. z z 4 Decoding as tree-parsing (or tree-based decoding). If the parse tree of source sentence is provided, decoding (for tree-tostring and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). In tree-parsing, translation rules are first mapped onto the nodes of input parse tree. This results in a translation tree/forest (or a hypergraph) where each edge represents a rule application. Then decoding can proceed on the hypergraph as usual. That is, we visit in bottom-up order each node in the parse tree, and calculate the model score for each edge rooting at the node. The final output is the 1-best/k-best translations maintained by the root node of the parse tree. Since tree-parsing restricts its search space to the derivations that exactly match with the input parse tree, it in gen"
P12-3004,N04-1035,0,0.0610923,"a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1 Introduction We present NiuTrans, a new open source machine translation toolkit, which was developed for constructing high quality machine translation systems. The NiuTrans toolkit supports most statistical machine translation (SMT) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (Koehn et al., 2003), the hierarchical phrase-based model (Chiang, 2007), and various syntax-based models (Galley et al., 2004; Liu et al., 2006). In particular, a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms. Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system. In addition, the toolkit provides easy-to-use APIs for the development of new features. The toolkit has been used to build translation systems that have placed well at recent MT evalua"
P12-3004,D08-1089,0,0.0327335,"ls. To build new translation systems, all you need is a collection of wordaligned sentences 3 , and a set of additional sentences with one or more reference translations for weight tuning and test. Once the data is prepared, the MT system can be created using a 2 http://www.nlp.org.cn/project/project.php?proj_id=14 To obtain word-to-word alignments, several easy-to-use toolkits are available, such as GIZA++ and Berkeley Aligner. 3 20 4 Term MSD refers to the three orientations (reordering types), including Monotone (M), Swap (S), and Discontinuous (D). probabilities of the three orientations (Galley and Manning, 2008). 3.2 Translation Rule Extraction For the hierarchical phrase-based model, we follow the general framework of SCFG where a grammar rule has three parts – a source-side, a target-side and alignments between source and target nonterminals. To learn SCFG rules from word-aligned sentences, we choose the algorithm proposed in (Chiang, 2007) and estimate the associated feature values as in the phrase-based system. For the syntax-based models, all non-terminals in translation rules are annotated with syntactic labels. We use the GHKM algorithm to extract (minimal) translation rules from bilingual sen"
P12-3004,N03-1017,0,0.0449568,"Missing"
P12-3004,P07-2045,0,0.0149649,"current approaches to statistical machine translation, NiuTrans is based on a log-linear 1 http://www.gnu.org/licenses/gpl-2.0.html 19 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrasebased models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z 3 It fully supports most state-of-the-art SMT models. Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) s"
P12-3004,W09-0424,0,0.0618017,"tical machine translation, NiuTrans is based on a log-linear 1 http://www.gnu.org/licenses/gpl-2.0.html 19 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrasebased models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z 3 It fully supports most state-of-the-art SMT models. Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) source and (or) target lang"
P12-3004,P06-1077,0,0.106485,"nguage model, and an implementation of minimum error rate training for weight tuning. 1 Introduction We present NiuTrans, a new open source machine translation toolkit, which was developed for constructing high quality machine translation systems. The NiuTrans toolkit supports most statistical machine translation (SMT) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (Koehn et al., 2003), the hierarchical phrase-based model (Chiang, 2007), and various syntax-based models (Galley et al., 2004; Liu et al., 2006). In particular, a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms. Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system. In addition, the toolkit provides easy-to-use APIs for the development of new features. The toolkit has been used to build translation systems that have placed well at recent MT evaluations, such as the"
P12-3004,P03-1021,0,0.231643,"threading techniques to speed-up the system. sequence of commands. Given a number of sentence-pairs and the word alignments between them, the toolkit first extracts a phrase table and two reordering models for the phrase-based system, or a Synchronous Context-free/Tree-substitution Grammar (SCFG/STSG) for the hierarchical phrase-based and syntax-based systems. Then, an n-gram language model is built on the targetlanguage corpus. Finally, the resulting models are incorporated into the decoder which can automatically tune feature weights on the development set using minimum error rate training (Och, 2003) and translate new sentences with the optimized weights. In the following, we will give a brief review of the above components and the main features provided by the toolkit. 3.1 Phrase Extraction and Reordering Model We use a standard way to implement the phrase extraction module for the phrase-based model. That is, we extract all phrase-pairs that are consistent with word alignments. Five features are associated with each phrase-pair. They are two phrase translation probabilities, two lexical weights, and a feature of phrase penalty. We follow the method proposed in (Koehn et al., 2003) to es"
P12-3004,P11-1027,0,0.0234971,"otated with syntactic labels. We use the GHKM algorithm to extract (minimal) translation rules from bilingual sentences with parse trees on source-language side and/or target-language side 5 . Also, two or more minimal rules can be composed together to obtain larger rules and involve more contextual information. For unaligned words, we attach them to all nearby rules, instead of using the most likely attachment as in (Galley et al., 2006). 3.3 N-gram Language Modeling The toolkit includes a simple but effective n-gram language model (LM). The LM builder is basically a “sorted” trie structure (Pauls and Klein, 2011), where a map is developed to implement an array of key/value pairs, guaranteeing that the keys can be accessed in sorted order. To reduce the size of resulting language model, low-frequency n-grams are filtered out by some thresholds. Moreover, an n-gram cache is implemented to speed up n-gram probability requests for decoding. 3.4 is repeated for several times until no better weights (i.e., weights with a higher BLEU score) are found. In this way, our program can introduce some randomness into weight training. Hence users do not need to repeat MERT for obtaining stable and optimized weights"
P12-3004,W10-1738,0,0.0161795,"ngs of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrasebased models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z 3 It fully supports most state-of-the-art SMT models. Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) source and (or) target language side(s). z It offers a wide choice of decoding algorithms. For example, the toolkit has several useful decoding o"
P12-3004,P96-1021,0,0.0239638,"s to obtain new items. Once a new item is created, the associated scores are computed (with an integrated n-gram language model). Then, the item is added into the list of the corresponding cell. This procedure stops when we reach the final state (i.e., the cell associates with the entire source span). The decoder can work with all (hierarchical) phrase-based and syntax-based models. In particular, our toolkit provides the following decoding modes. z Phrase-based decoding. To fit the phrasebased model into the CKY paring framework, we restrict the phrase-based decoding with the ITG constraint (Wu, 1996). In this way, each pair of items in adjunct cells can be composed in either monotone order or inverted order. Hence the decoding can be trivially implemented by a three-loop structure as in standard CKY parsing. This algorithm is actually the same as that used in parsing with bracketing transduction grammars. z Decoding as parsing (or string-based decoding). This mode is designed for decoding with SCFGs/STSGs which are used in the hierarchical phrase-based and syntax-based systems. In the general framework of synchronous grammars and tree transducers, decoding can be regarded as a parsing pro"
P12-3004,P06-1066,0,0.0492136,"Missing"
P12-3004,W06-3119,0,0.0205075,"n, NiuTrans is based on a log-linear 1 http://www.gnu.org/licenses/gpl-2.0.html 19 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrasebased models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z 3 It fully supports most state-of-the-art SMT models. Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) source and (or) target language side(s). z It offers a wide choi"
P12-3004,P08-1023,0,\N,Missing
P12-3004,P10-4002,0,\N,Missing
P12-3004,P06-1121,0,\N,Missing
P13-2020,P07-1096,0,0.0845905,"htly improved. This may because that the accuracy of the greedy baseline tagger is already very high and it is hard to get further improvement. Table 2 and table 3 also show that the speed of both tagging and dependency parsing drops linearly with the growth of beam width. 5.2 s 1 3 5 PTB uas compl 91.77 45.29 92.29 46.28 92.50 46.82 92.74 48.12 CTB speed uas compl 84.54 33.75 221 85.11 34.62 124 85.62 37.11 71 86.00 35.87 39 Table 3: Parsing accuracy vs beam width. ‘uas’ and ‘compl’ denote unlabeled score and complete match rate respectively (all excluding punctuations). PTB (Collins, 2002) (Shen et al., 2007) (Huang et al., 2012) this work 1 this work CTB 97.11 (Hatori et al., 2012) 97.33 (Li et al., 2012) 97.35 (Ma et al., 2012) 97.22 this work 1 97.28 this work 93.82 93.88 93.84 93.87 94.01† Table 4: Tagging results on the test set. ‘†’ denotes statistically significant over the greedy baseline by McNemar’s test ( ) Systems (Huang and Sagae, 2010) (Zhang and Nivre, 2011) (Li et al., 2012) this work this work s 8 64 － 1 8 uas compl 85.20 33.72 86.00 36.90 86.55 － 84.79 32.98 86.33† 36.13 Table 5: Parsing results on CTB test set. Systems (Huang and Sagae, 2010) (Zhang and Nivre, 2011) (Koo and Col"
P13-2020,W02-1001,0,0.934328,"2012; Søggard and Wulff, 2012). By processing the input tokens in an easyto-hard order, the algorithm could make use of structured information on both sides of the hard token thus making more indicative predictions. However, rich structured information also causes exhaustive inference intractable. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). To enlarge the search space, a natural extension to greedy search is beam search. Recent work also shows that beam search together with perceptron-based global learning (Collins, 2002) enable the use of non-local features that are helpful to improve parsing performance without overfitting (Zhang and Nivre, 2012). Due to these advantages, beam search and global learning has been applied to many NLP tasks (Collins and Roark 2004; Zhang and Clark, 2007). However, to the best of our knowledge, no work in the literature has ever applied the two techniques to easy-first dependency parsing. While applying beam-search is relatively straightforward, the main difficulty comes from combining easy-first dependency parsing with perceptron-based global learning. In particular, one needs"
P13-2020,D08-1052,0,0.0537551,"Missing"
P13-2020,C12-2105,0,0.0224662,"Missing"
P13-2020,P08-1101,0,0.238503,"uld fail to ensure validity of update (see the example in figure 1). For validity of update, we propose a simple solution which is based on “early update” and which can accommodate spurious ambiguity. The basic idea is to use the correct action sequence which was 3 Following (Zhang and Nivre, 2012), we say the training algorithm is global if it optimizes the score of an entire action sequence. A local learner trains a classifier which distinguishes between single actions. 4 As shown in (Goldberg and Nivre 2012), most transitionbased dependency parsers (Nivre et al., 2003; Huang and Sagae 2010;Zhang and Clark 2008) ignores spurious ambiguity by using a static oracle which maps a dependency tree to a single action sequence. pruned right at the step when all correct sequence falls off the beam (as C1 in figure 1). Algorithm 2 shows the pseudo-code of the training procedure over one training sample ( ), a sentence-tree pair. Here we assume to be the set of all correct action sequences/subsequences. At step k, the algorithm constructs a correct action sequence  of length k by extending those in (line 3). It also checks whether no longer contains any correct sequence. If so,  together with are used for par"
P13-2020,P11-2033,0,0.43518,"use CTB 5.1 and the split suggested by (Duan et al., 2007) for both tagging and dependency parsing. We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. For dependency parsing, we assume gold segmentation and POS tags for the input. 5 112 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html Features used in English dependency parsing are listed in table 1. Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency parsing (Zhang and Nivre, 2011). For English POS tagging, we use the same features as in (Shen et al., 2007). For Chinese POS tagging and dependency parsing, we use the same features as (Ma et al., 2012). All of our experiments are conducted on a Core i7 (2.93GHz) machine, both the tagger and parser are implemented using C++. 5.1 Final results Tagging results on the test set together with some previous results are listed in table 4. Dependency parsing results on CTB and PTB are listed in table 5 and table 6, respectively. On CTB, tagging accuracy of our greedy baseline is already comparable to the state-of-the-art. As the b"
P13-2020,C12-2136,0,0.156696,"structured information on both sides of the hard token thus making more indicative predictions. However, rich structured information also causes exhaustive inference intractable. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). To enlarge the search space, a natural extension to greedy search is beam search. Recent work also shows that beam search together with perceptron-based global learning (Collins, 2002) enable the use of non-local features that are helpful to improve parsing performance without overfitting (Zhang and Nivre, 2012). Due to these advantages, beam search and global learning has been applied to many NLP tasks (Collins and Roark 2004; Zhang and Clark, 2007). However, to the best of our knowledge, no work in the literature has ever applied the two techniques to easy-first dependency parsing. While applying beam-search is relatively straightforward, the main difficulty comes from combining easy-first dependency parsing with perceptron-based global learning. In particular, one needs to guarantee that each parameter update is valid, i.e., the correct action sequence has lower model score than the predicted one1"
P13-2020,N10-1115,0,0.631914,"tension of is: ( ) ( ) Here, ‘ ’ means insert to the end of . Following (Huang et al., 2012), in order to formalize beam search, we also use the ( ) operation which returns the top s action sequences in according to ( ). Here, denotes a set of action sequences, ( ) denotes the sum of feature vectors of each action in Pseudo-code of easy-first with beam search is shown in algorithm 1. Beam search grows s (beam width) action sequences in parallel using a 111 Algorithm 2: Perceptron-based training over one training sample ( ) Input: ( ), s, parameter Output: new parameter ( ) ( ( )) Features of (Goldberg and Elhadad, 2010) for p in pi-1, pi, pi+1 wp-vlp, wp-vrp, tp-vlp, tp-vrp, tlcp, trcp, wlcp, wlcp for p in pi-2, pi-1, pi, pi+1, pi+2 tp-tlcp, tp-trcp, tp-tlcp-trcp for p, q, r in (pi-2, pi-1, pi), (pitp-tq-tr, tp-tq-wr ( ) 1, pi+1, pi), (pi+1, pi+2 ,pi) for p, q in (pi-1, pi) tp-tlcp-tq, tp-trcp-tq, ,tp-tlcp-wq,, // top correct extension from the beam tp-trcp-wq, tp-wq-tlcq, tp-wq-trcq 1 2 for 1 1 do ( ) 3  ( ) 4 5 if // all correct seq. falls off the beam ( 6 ) ( ) 7 break 8 if ( ) // full update ( ) 9 ( ) 10 return Table 1: Feature templates for English dependency parsing. wp denotes the head word of p, t"
P13-2020,N12-1015,0,0.372593,"ing with perceptron-based global learning. In particular, one needs to guarantee that each parameter update is valid, i.e., the correct action sequence has lower model score than the predicted one1. The difficulty in ensuring validity of parameter update for the easy-first algorithm is caused by its spurious ambiguity, i.e., the same result might be derived by more than one action sequences. For algorithms which do not exhibit spurious ambiguity, “early update” (Collins and Roark 2004) is always valid: at the k-th step when the single correct action sequence falls off the beam, 1 As shown by (Huang et al., 2012), only valid update guarantees the convergence of any perceptron-based training. Invalid update may lead to bad learning or even make the learning not converge at all. 110 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 110–114, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Algorithm 1: Easy-first with beam search Input: sentence of n words, beam width s Output: one best dependency tree ( ) ( ) ( ) // top s extensions from the beam // initially, empty beam Figure 2: An example of parsing “I am valid”. Spurious"
P13-2020,P10-1001,0,0.0840505,"Missing"
P13-2020,C12-1103,0,0.101632,"Missing"
P13-2020,P10-1110,0,\N,Missing
P13-2020,P07-1106,0,\N,Missing
P13-2020,C12-2115,0,\N,Missing
P13-2020,C12-1106,1,\N,Missing
P14-2092,P03-1021,0,0.0193089,"Missing"
P14-2092,P10-1146,0,0.0475086,"Missing"
P14-2092,2006.amta-papers.17,0,0.0641054,"Missing"
P14-2092,P06-1048,0,0.0113997,"standard phrase-based model for gf ull (d) in which not all segments of the sentence need to respect syntactic constraints. Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 (2) 2.3 Model Score Computation In this work both the skeleton translation model gskel (d) and full translation model gf ull (d) resemble the usual forms used in phrase-based MT, i.e., the model score is computed by a linear combination of a g"
P14-2092,P07-1040,0,0.081951,"Missing"
P14-2092,C08-1018,0,0.0204848,"h not all segments of the sentence need to respect syntactic constraints. Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 (2) 2.3 Model Score Computation In this work both the skeleton translation model gskel (d) and full translation model gf ull (d) resemble the usual forms used in phrase-based MT, i.e., the model score is computed by a linear combination of a group of phrase-based features and language models"
P14-2092,W08-0329,0,0.0606487,"Missing"
P14-2092,P03-2041,0,0.0613666,"Missing"
P14-2092,N07-1023,0,0.0336756,"del for gf ull (d) in which not all segments of the sentence need to respect syntactic constraints. Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 (2) 2.3 Model Score Computation In this work both the skeleton translation model gskel (d) and full translation model gf ull (d) resemble the usual forms used in phrase-based MT, i.e., the model score is computed by a linear combination of a group of phrase-based featu"
P14-2092,J82-2005,0,0.777665,"Missing"
P14-2092,2006.amta-papers.8,0,0.0791381,"Missing"
P14-2092,P10-1096,0,0.0144214,"he sentence need to respect syntactic constraints. Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 (2) 2.3 Model Score Computation In this work both the skeleton translation model gskel (d) and full translation model gf ull (d) resemble the usual forms used in phrase-based MT, i.e., the model score is computed by a linear combination of a group of phrase-based features and language models. In phrase-based MT, the tr"
P14-2092,P12-2068,0,0.0175741,"syntactic constraints. Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 (2) 2.3 Model Score Computation In this work both the skeleton translation model gskel (d) and full translation model gf ull (d) resemble the usual forms used in phrase-based MT, i.e., the model score is computed by a linear combination of a group of phrase-based features and language models. In phrase-based MT, the translation problem is mode"
P14-2092,N03-1017,0,0.0309021,"Missing"
P14-2092,P06-1077,0,0.106715,"Missing"
P14-2092,2011.mtsummit-papers.6,0,0.0466045,"Missing"
P14-2092,P08-1114,0,0.0575302,"Missing"
P14-2092,2006.amta-papers.13,0,0.0778089,"Missing"
P14-2092,2006.eamt-1.24,0,0.25591,"Missing"
P14-2092,W99-0604,0,0.190929,"tions on the NIST Chinese-English MT evaluation data. 1 • We develop a skeleton-based model which divides translation into two sub-models: a skeleton translation model (i.e., translating the key elements) and a full translation model (i.e., translating the remaining source words and generating the complete translation). Introduction Current Statistical Machine Translation (SMT) approaches model the translation problem as a process of generating a derivation of atomic translation units, assuming that every unit is drawn out of the same model. The simplest of these is the phrase-based approach (Och et al., 1999; Koehn et al., 2003) which employs a global model to process any sub-strings of the input sentence. In this way, all we need is to increasingly translate a sequence of source words each time until the entire sentence is covered. Despite good results in many tasks, such a method ignores the roles of each source word and is somewhat different from the way used by translators. For example, an important-first strategy is generally adopted in human translation - we translate the key elements/structures (or skeleton) of the sentence first, and then translate the remaining parts. This especially mak"
P14-2092,P12-3004,1,\N,Missing
P14-2092,P08-1064,0,\N,Missing
P15-4025,J08-2004,0,0.0346422,"is extremely heavy. We will optimize the implementation of dependency parsing in our future work. Experiments We ran our system on several benchmarks. Specifically, we trained and tested word segmentation, POS tagging, chunking, and constituent parsing on CTB5.1: articles 001-270 and 440-1151 were used for training and articles 271-300 were used for testing. The performance of named entity recognition was reported on OntoNotes, where 49,011 sentences were used for training and 1,340 sentences were used for testing. For semantic role labeling, we adopted the same data set and splitting as in (Xue, 2008). Finally, the data set and splitting in (Zhang and Clark, 2011) were used to evaluate the performance of dependency parsing. All results were reported on a machine with a 4 Conclusions and Future Work We have presented the NiuParser Chinese syntactic and semantic analysis toolkit. It can handle several parsing tasks for Chinese, including word segmentation, part-of-speech tagging, named entity recognition, chunking, constituent parsing, dependency parsing, and constituent parser-based semantic role labeling. The NiuParser system is fast and shows state-of-the-art performance on several benchm"
P15-4025,J11-1005,0,0.0323647,"tion of dependency parsing in our future work. Experiments We ran our system on several benchmarks. Specifically, we trained and tested word segmentation, POS tagging, chunking, and constituent parsing on CTB5.1: articles 001-270 and 440-1151 were used for training and articles 271-300 were used for testing. The performance of named entity recognition was reported on OntoNotes, where 49,011 sentences were used for training and 1,340 sentences were used for testing. For semantic role labeling, we adopted the same data set and splitting as in (Xue, 2008). Finally, the data set and splitting in (Zhang and Clark, 2011) were used to evaluate the performance of dependency parsing. All results were reported on a machine with a 4 Conclusions and Future Work We have presented the NiuParser Chinese syntactic and semantic analysis toolkit. It can handle several parsing tasks for Chinese, including word segmentation, part-of-speech tagging, named entity recognition, chunking, constituent parsing, dependency parsing, and constituent parser-based semantic role labeling. The NiuParser system is fast and shows state-of-the-art performance on several benchmarks. Moreover, it supports several advanced features, such as t"
P15-4025,P11-2033,0,0.0370241,"ilities, the identification stage applies the algorithm of enforcing non-overlapping arguments (Jiang and Ng, 2006) to maximize the logprobability of the entire labeled parse tree. In the classification stage, the classifier assigns labels to arguments independently. Transition-based Parsing Syntactic parsers can be grouped into two categories according to decoding algorithms: dynamic programming-based and transition-based. For the purpose of efficiency, we implement the constituent and two versions of dependency parsers in the NiuParser system with transition-based methods (Zhu et al., 2013; Zhang and Nivre, 2011; Chen and Manning, 2014). Specifically, parsers are variants of shift-reduce parsers, which start from an initial state and reach a final state by performing an action in each stage transition. Figure 2 and Figure 3 present an example parse of the two parsers, respectively. One version of the dependency parsers follows the work in (Chen and Manning, 2014), regarding the state transition process as a sequence of classification decisions. In each transition, a best action is chosen by a Neural Network classifier. The 2.3 Improvements and Advanced Features 2.3.1 Word Segmentation In Chinese sent"
P15-4025,P13-1043,1,0.842721,". With such probabilities, the identification stage applies the algorithm of enforcing non-overlapping arguments (Jiang and Ng, 2006) to maximize the logprobability of the entire labeled parse tree. In the classification stage, the classifier assigns labels to arguments independently. Transition-based Parsing Syntactic parsers can be grouped into two categories according to decoding algorithms: dynamic programming-based and transition-based. For the purpose of efficiency, we implement the constituent and two versions of dependency parsers in the NiuParser system with transition-based methods (Zhu et al., 2013; Zhang and Nivre, 2011; Chen and Manning, 2014). Specifically, parsers are variants of shift-reduce parsers, which start from an initial state and reach a final state by performing an action in each stage transition. Figure 2 and Figure 3 present an example parse of the two parsers, respectively. One version of the dependency parsers follows the work in (Chen and Manning, 2014), regarding the state transition process as a sequence of classification decisions. In each transition, a best action is chosen by a Neural Network classifier. The 2.3 Improvements and Advanced Features 2.3.1 Word Segme"
P15-4025,J96-1002,0,0.110432,"ents in a parse tree as arguments with respect to a specified predicate (See Figure 4). Here, semantic role labeling is formalized as a two-stage classification problem. The first stage (called identification) conducts a binary classification to decide whether a constituent in a parse tree is an argument. After the first stage, a set of constituents is fed to the second stage (called classification) classifier which is a multi-class classifier, used for assigning each argument an appropriate semantic label. The statistical model used in the semantic role labeling subsystem is Maximum Entropy (Berger et al., 1996), which provides classification decisions with corresponding probabilities. With such probabilities, the identification stage applies the algorithm of enforcing non-overlapping arguments (Jiang and Ng, 2006) to maximize the logprobability of the entire labeled parse tree. In the classification stage, the classifier assigns labels to arguments independently. Transition-based Parsing Syntactic parsers can be grouped into two categories according to decoding algorithms: dynamic programming-based and transition-based. For the purpose of efficiency, we implement the constituent and two versions of"
P15-4025,D14-1082,0,0.0489682,"tion stage applies the algorithm of enforcing non-overlapping arguments (Jiang and Ng, 2006) to maximize the logprobability of the entire labeled parse tree. In the classification stage, the classifier assigns labels to arguments independently. Transition-based Parsing Syntactic parsers can be grouped into two categories according to decoding algorithms: dynamic programming-based and transition-based. For the purpose of efficiency, we implement the constituent and two versions of dependency parsers in the NiuParser system with transition-based methods (Zhu et al., 2013; Zhang and Nivre, 2011; Chen and Manning, 2014). Specifically, parsers are variants of shift-reduce parsers, which start from an initial state and reach a final state by performing an action in each stage transition. Figure 2 and Figure 3 present an example parse of the two parsers, respectively. One version of the dependency parsers follows the work in (Chen and Manning, 2014), regarding the state transition process as a sequence of classification decisions. In each transition, a best action is chosen by a Neural Network classifier. The 2.3 Improvements and Advanced Features 2.3.1 Word Segmentation In Chinese sentences, words like dates,"
P15-4025,W06-1617,0,0.0276101,") conducts a binary classification to decide whether a constituent in a parse tree is an argument. After the first stage, a set of constituents is fed to the second stage (called classification) classifier which is a multi-class classifier, used for assigning each argument an appropriate semantic label. The statistical model used in the semantic role labeling subsystem is Maximum Entropy (Berger et al., 1996), which provides classification decisions with corresponding probabilities. With such probabilities, the identification stage applies the algorithm of enforcing non-overlapping arguments (Jiang and Ng, 2006) to maximize the logprobability of the entire labeled parse tree. In the classification stage, the classifier assigns labels to arguments independently. Transition-based Parsing Syntactic parsers can be grouped into two categories according to decoding algorithms: dynamic programming-based and transition-based. For the purpose of efficiency, we implement the constituent and two versions of dependency parsers in the NiuParser system with transition-based methods (Zhu et al., 2013; Zhang and Nivre, 2011; Chen and Manning, 2014). Specifically, parsers are variants of shift-reduce parsers, which s"
P15-4025,W06-0127,0,\N,Missing
P18-2047,D17-1151,0,0.0294931,"Missing"
P18-2047,D14-1179,0,0.0120422,"Missing"
P18-2047,D13-1176,0,0.0550282,"imited n-best outputs. Moreover, it is robust to large beam sizes, which is not well studied in previous work. On the Chinese-English and English-German translation tasks, our approach yields +0.4 ∼ 1.5 BLEU improvements over the state-of-the-art baselines. 1 Introduction In the past few years, Neural Machine Translation (NMT) has achieved state-of-the-art performance in many translation tasks. It models the translation problem using neural networks with no assumption of the hidden structures between two languages, and learns the model parameters from bilingual texts in an end-to-end fashion (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). In such systems, target words are generated over a sequence of time steps. The model score is simply defined as the sum of the log-scale word probabilities: |y | X log P(y|x) = log P(yj |y&lt;j , x) (1) j=1 where x and y are the source and target sentences, and P(yj |y&lt;j , x) is the probability of generating the j-th word yj given the previously-generated words y&lt;j and the source sentence x. However, the straightforward implementation of this model suffers from many problems, the most obvious one being the bias that the system tends to choose shorter t"
P18-2047,P09-5002,0,0.0505731,"2 aij , β) = log 0.8 + log 1.2 ... = 1.5 Figure 1: The coverage score for a running example (Chinese pinyin-English and β = 0.8). We test our approach on the NIST ChineseEnglish and WMT English-German translation tasks, and it outperforms several state-of-the-art baselines by 0.4∼1.5 BLEU points. illustration): 2 where β is a parameter that can be tuned on a development set. This model has two properties: c(x, y) = |x | X i The Coverage Score Given a word sequence, a coverage vector indicates whether the word of each position is translated. This is trivial for statistical machine translation (Koehn, 2009) because there is no overlap between the translation units of a hypothesis, i.e., we have a 0-1 coverage vector. However, it is not the case for NMT where the coverage is modeled in a soft way. In NMT, no explicit translation units or rules are used. The attention mechanism is used instead to model the correspondence between a source position and a target position (Bahdanau et al., 2015). For a given target position j, the attention-based NMT computes attention score aij for each source position i. aij can be regarded as the measure of the correspondent strength between i and j, and is normali"
P18-2047,W17-3204,0,0.0209735,"a coverage-based feature into NMT. Unlike previous studies, we do not resort to developing extra models nor reranking the limited n-best translations. Instead, we develop a coverage score and apply it to each decoding step. Our approach has several benefits, • Our approach does not require to train a huge neural network and is easy to implement. • Our approach works on beam search for each target position and thus can access more translation hypotheses. • Our approach works consistently well under different sized beam search and sentence lengths contrary to what is observed in other systems (Koehn and Knowles, 2017). We offer a simple and effective method to seek a better balance between model confidence and length preference for Neural Machine Translation (NMT). Unlike the popular length normalization and coverage models, our model does not require training nor reranking the limited n-best outputs. Moreover, it is robust to large beam sizes, which is not well studied in previous work. On the Chinese-English and English-German translation tasks, our approach yields +0.4 ∼ 1.5 BLEU improvements over the state-of-the-art baselines. 1 Introduction In the past few years, Neural Machine Translation (NMT) has"
P18-2047,D15-1166,0,0.0766835,"re β is similar to model warm-up, which makes the model easy to run in the first few decoding steps. Note that our way of truncation is different from Wu et al. (2016)’s, where they clip the coverage into [0, 1] and ignore the fact that a source word may be translated into multiple target words and its coverage should be of a value larger than 1. For decoding, we incorporate the coverage score into beam search via linear combination with the NMT model score as below, 1 As the discussion of the attention mechanism is out of the scope of this work, we refer the reader to Bahdanau et al. (2015); Luong et al. (2015) for more details. 293 Entry b=10 s(x, y) = (1 − α) · log P(y|x) + α · c(x, y) (3) 3 b=500 b=100 where y is a partial translation generated during decoding, log P(y|x) is the model score, and α is the coefficient for linear interpolation. In standard implementation of NMT systems, once a hypothesis is finished, it is removed from the beam and the beam shrinks accordingly. Here we choose a different decoding strategy. We keep the finished hypotheses in the beam until the decoding completes, which means that we compare the finished hypotheses with partial translations at each step. This method h"
P18-2047,P16-1162,0,0.0732288,"baseline systems were based on the opensource implementation of the NMT model presented in Luong et al. (2017). The model was consisted of a 4-layer bi-directional LSTM encoder and a 4-layer LSTM decoder. The size of the embedding and hidden layers was set to 1024. We applied the additive attention model on top of the multi-layer LSTMs (Bahdanau et al., 2015). For training, we used the Adam optimizer (Kingma and Ba, 2015) where the learning rate and batch size were set to 0.001 and 128. We selected the top 30k entries for both source and target vocabularies. For the English-German task, BPE (Sennrich et al., 2016) was used for better performance. For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods (Wu et al., 2016). We used grid search to tune all hyperparameters on the development set as Wu et al. (2016). Specifically, weights for both CP and our CS are evaluated in interval [0, 1] with step 0.1, while the weight for LN is in interval [0.5, 1.5]. We found that the settings determined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10. For Chinese-English translation,"
P18-2047,P16-1008,0,0.222956,"s. It is in general to normalize the model score by translation length (say length normalization) to eliminate this system bias (Wu et al., 2016). Though widely used, length normalization is not a perfect solution. NMT systems still have under-translation and over-translation problem even with a normalized model. It is due to the lack of the coverage model that indicates the degree a source word is translated. As an extreme case, a source word might be translated for several times, which results in many duplicated target words. Several research groups have proposed solutions to this bad case (Tu et al., 2016; Mi et al., 2016). E.g., Tu et al. (2016) developed a coveragebased model to measure the fractional count that a source word is translated during decoding. It can be jointly learned with the NMT model. Alternatively, one can rerank the n-best outputs by coverage-sensitive models, but this method just affects the final output list which has a very limited scope (Wu et al., 2016). In this paper we present a simple and effective approach by introducing a coverage-based feature into NMT. Unlike previous studies, we do not resort to developing extra models nor reranking the limited n-best translat"
P18-2047,P12-3004,1,0.800817,"70 24.17 23.57 24.17 23.69 Table 1: BLEU results of NMT systems. base = base system, LN = length normalization, CP = coverage penalty, and CS = our coverage score. Setup We evaluated our approach on Chinese-English and German-English translation tasks. We used 1.8M sentence Chinese-English bitext provided within NIST12 OpenMT2 and 4.5M sentence German-English bitext provided within WMT16. For Chinese-English translation, we chose the evaluation data of NIST MT06 as the development set, and MT08 as the test set. All Chinese sentences were word segmented using the tool provided within NiuTrans (Xiao et al., 2012). For German-English translation, we chose newstest2013 as the development set and newstest2014 as the test set. Our baseline systems were based on the opensource implementation of the NMT model presented in Luong et al. (2017). The model was consisted of a 4-layer bi-directional LSTM encoder and a 4-layer LSTM decoder. The size of the embedding and hidden layers was set to 1024. We applied the additive attention model on top of the multi-layer LSTMs (Bahdanau et al., 2015). For training, we used the Adam optimizer (Kingma and Ba, 2015) where the learning rate and batch size were set to 0.001"
P19-1176,P17-4012,0,0.233608,"ntion model. In this work, we continue the line of research and go towards a much deeper encoder for Transformer. We choose encoders to study because they have a greater impact on performance than decoders and require less computational cost (Domhan, 2018). Our contributions are threefold: • We show that the proper use of layer normalization is the key to learning deep encoders. The deep network of the encoder can be optimized smoothly by relocating the layer normalization unit. While the location of layer normalization has been discussed in recent systems (Vaswani et al., 2018; Domhan, 2018; Klein et al., 2017), as far as we know, its impact has not been studied in deep Trans2 For example, a standard Transformer encoder has 6 layers. Each of them consists of two sub-layers. More sub-layers are involved on the decoder side. 1810 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810–1822 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics xl F L yl LN xl+1 (a) post-norm residual unit xl LN F yl L xl+1 (b) pre-norm residual unit Figure 1: Examples of pre-norm residual unit and postnorm residual unit. F = sub-layer, an"
P19-1176,D15-1166,0,0.0342435,"Transformer-Big/Base baseline (6-layer encoder) by 0.4∼2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big1 . 1 Introduction Neural machine translation (NMT) models have advanced the previous state-of-the-art by learning mappings between sequences via neural networks and attention mechanisms (Sutskever et al., 2014; Bahdanau et al., 2015). The earliest of these read and generate word sequences using a series of recurrent neural network (RNN) units, and the improvement continues when 4-8 layers are stacked for a deeper model (Luong et al., 2015; Wu et al., 2016). More recently, the system based on multi-layer self-attention (call it Transformer) has shown strong results on several large∗ Corresponding author. The source code is available at https://github. com/wangqiangneu/dlcl 1 scale tasks (Vaswani et al., 2017). In particular, approaches of this kind benefit greatly from a wide network with more hidden states (a.k.a. Transformer-Big), whereas simply deepening the network has not been found to outperform the “shallow” counterpart (Bapna et al., 2018). Do deep models help Transformer? It is still an open question for the discipline"
P19-1176,W18-6301,0,0.170192,"Big is updated for 100k/300k steps on the En-De task as Vaswani et al. (2017), 50k/100k steps on the Zh-En-Small task, and 200k/500k steps on the Zh-En-Large task. In our model, we use the dynamic linear combination of layers for both encoder and decoder. For efficient computation, we only combine the output of a complete layer rather than a sub-layer. It should be noted that for deep models (e.g. L ≥ 20), it is hard to handle a full batch in a single GPU due to memory size limitation. We solve this issue by accumulating gradients from two small batches (e.g. batch = 2048) before each update (Ott et al., 2018). In our primitive experiments, we observed that training with larger batches and learning rates worked well for deep models. Therefore all the results of deep models are reported with batch = 8192, lr = 2×10−3 and warmup = 16,000 unless otherwise stated. For fairness, we only use half of the updates of baseline (e.g. update = 50k) to ensure the same amount of data that we actually 12 https://github.com/tensorflow/ tensor2tensor Results Results on the En-De Task In Table 1, we first report results on WMT En-De where we compare to the existing systems based on self-attention. Obviously, while a"
P19-1176,P18-1167,0,0.157394,"canu et al., 2013; Bapna et al., 2018). We note that, despite the significant development effort, simply stacking more layers cannot benefit the system and leads to a disaster of training in some of our experiments. A promising attempt to address this issue is Bapna et al. (2018)’s work. They trained a 16layer Transformer encoder by using an enhanced attention model. In this work, we continue the line of research and go towards a much deeper encoder for Transformer. We choose encoders to study because they have a greater impact on performance than decoders and require less computational cost (Domhan, 2018). Our contributions are threefold: • We show that the proper use of layer normalization is the key to learning deep encoders. The deep network of the encoder can be optimized smoothly by relocating the layer normalization unit. While the location of layer normalization has been discussed in recent systems (Vaswani et al., 2018; Domhan, 2018; Klein et al., 2017), as far as we know, its impact has not been studied in deep Trans2 For example, a standard Transformer encoder has 6 layers. Each of them consists of two sub-layers. More sub-layers are involved on the decoder side. 1810 Proceedings of"
P19-1176,D18-1457,0,0.24854,"value. For pre-norm Transformer, we define G(·) 5 Some of the other single-step methods, e.g. the RungeKutta method, can obtain a higher order by taking several intermediate steps (Butcher, 2003). Higher order generally means more accurate. 1812 y0 y1 y2 y3 x1 x2 x3 x4 y0 y1 y2 y3 x1 x2 x3 x4 1 0 1 0 0 1 0 0 0 1 y0 y1 y2 y3 x1 x2 x3 x4 1 (a) 1 1 1 1 1 1 1 1 1 (b) 1 0 1 0 0 1 .1 .3 .2 .4 y0 y1 y2 y3 x1 x2 x3 x4 1.8 (c) .4 1.2 .3 .2 .8 .1 .3 .5 .7 (d) Figure 2: Connection weights for 3-layer encoder: (a) residual connection (He et al., 2016a), (b) dense residual connection (Britz et al., 2017; Dou et al., 2018), (c) multi-layer representation fusion (Wang et al., 2018b)/transparent attention (Bapna et al., 2018) and (d) our approach. y0 denotes the input embedding. Red denotes the weights are learned by model. et al., 2017; Dou et al., 2018). Multi-layer representation fusion (Wang et al., 2018b) and transparent attention (call it TA) (Bapna et al., 2018) methods can learn a weighted model to fuse layers but they are applied to the topmost layer only. The DLCL model can cover all these methods. It provides ways of weighting and connecting layers in the entire stack. We emphasize that although the id"
P19-1176,N18-1202,0,0.0478456,"encoder by resorting to auxiliary losses in intermediate layers. This method is orthogonal to our DLCL method, though it is used for language modeling, which is not a very heavy task. Densely Residual Connections. Densely residual connections are not new in NMT. They have been studied for different architectures, e.g., RNN (Britz et al., 2017) and Transformer (Dou et al., 2018). Some of the previous studies fix the weight of each layer to a constant, while others learn a weight distribution by using either the self-attention model (Wang et al., 2018b) or a softmax-normalized learnable vector (Peters et al., 2018). They focus more on learning connections from lower-level layers to the topmost layer. Instead, we introduce additional connectivity into the network and learn more densely connections for each layer in an end-to-end fashion. 8 Conclusion We have studied deep encoders in Transformer. We have shown that the deep Transformer models can be easily optimized by proper use of layer normalization, and have explained the reason behind it. Moreover, we proposed an approach based on a dynamic linear combination of layers and successfully trained a 30-layer Transformer system. It is the deepest encoder"
P19-1176,P16-1009,0,0.0361058,"example, the standard residual network is a special case of DLCL, where Wll+1 = 1, and Wkl+1 = 0 for k < l. Figure (2) compares different methods of connecting a 3-layer network. We see that the densely residual network is a fully-connected network with a uniform weighting schema (Britz 4 Experimental Setup We first evaluated our approach on WMT’16 English-German (En-De) and NIST’12 ChineseEnglish (Zh-En-Small) benchmarks respectively. To make the results more convincing, we also experimented on a larger WMT’18 Chinese-English dataset (Zh-En-Large) with data augmentation by back-translation (Sennrich et al., 2016a). 4.1 Datasets and Evaluation For the En-De task, to compare with Vaswani et al. (2017)’s work, we use the same 4.5M preprocessed data 7 , which has been tokenized and 6 Let the encoder depth be M and the decoder depth be N (M &gt; N for a deep encoder model). Then TA newly adds O(M × N ) connections, which are fewer than ours of O(M 2 ) 7 https://drive.google.com/uc?export= download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8 1813 Model Param. Vaswani et al. (2017) (Base) 65M 137M 213M 379M † 210M † 210M 356M 210M 62M 211M 106M 62M 121M 62M 211M 106M 62M 137M Bapna et al. (2018)-deep (Base, 16L) Vaswani e"
P19-1176,P16-1162,0,0.138954,"example, the standard residual network is a special case of DLCL, where Wll+1 = 1, and Wkl+1 = 0 for k < l. Figure (2) compares different methods of connecting a 3-layer network. We see that the densely residual network is a fully-connected network with a uniform weighting schema (Britz 4 Experimental Setup We first evaluated our approach on WMT’16 English-German (En-De) and NIST’12 ChineseEnglish (Zh-En-Small) benchmarks respectively. To make the results more convincing, we also experimented on a larger WMT’18 Chinese-English dataset (Zh-En-Large) with data augmentation by back-translation (Sennrich et al., 2016a). 4.1 Datasets and Evaluation For the En-De task, to compare with Vaswani et al. (2017)’s work, we use the same 4.5M preprocessed data 7 , which has been tokenized and 6 Let the encoder depth be M and the decoder depth be N (M &gt; N for a deep encoder model). Then TA newly adds O(M × N ) connections, which are fewer than ours of O(M 2 ) 7 https://drive.google.com/uc?export= download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8 1813 Model Param. Vaswani et al. (2017) (Base) 65M 137M 213M 379M † 210M † 210M 356M 210M 62M 211M 106M 62M 121M 62M 211M 106M 62M 137M Bapna et al. (2018)-deep (Base, 16L) Vaswani e"
P19-1176,N18-2074,0,0.109651,"Missing"
P19-1176,1983.tc-1.13,0,0.401031,"Missing"
P19-1176,P12-3004,1,0.877973,"Updates, which can be used to approximately measure the required training time. † denotes an estimate value. Note that “-deep” represents the best-achieved result as depth changes. jointly byte pair encoded (BPE) (Sennrich et al., 2016b) with 32k merge operations using a shared vocabulary 8 . We use newstest2013 for validation and newstest2014 for test. For the Zh-En-Small task, we use parts of the bitext provided within NIST’12 OpenMT9 . We choose NIST MT06 as the validation set, and MT04, MT05, MT08 as the test sets. All the sentences are word segmented by the tool provided within NiuTrans (Xiao et al., 2012). We remove the sentences longer than 100 and end up with about 1.9M sentence pairs. Then BPE with 32k operations is used for both sides independently, resulting in a 44k Chinese vocabulary and a 33k English vocabulary respectively. For the Zh-En-Large task, we use exactly the same 16.5M dataset as Wang et al. (2018a), composing of 7.2M-sentence CWMT corpus, 4.2M-sentence UN and News-Commentary combined corpus, and back-translation of 5M-sentence monolingual data from NewsCraw2017. We refer the reader to Wang et al. (2018a) for the details. 8 The tokens with frequencies less than 5 are filtere"
P19-1176,Q16-1027,0,0.0280018,"Missing"
P19-1176,W18-1819,0,0.243782,"er encoder by using an enhanced attention model. In this work, we continue the line of research and go towards a much deeper encoder for Transformer. We choose encoders to study because they have a greater impact on performance than decoders and require less computational cost (Domhan, 2018). Our contributions are threefold: • We show that the proper use of layer normalization is the key to learning deep encoders. The deep network of the encoder can be optimized smoothly by relocating the layer normalization unit. While the location of layer normalization has been discussed in recent systems (Vaswani et al., 2018; Domhan, 2018; Klein et al., 2017), as far as we know, its impact has not been studied in deep Trans2 For example, a standard Transformer encoder has 6 layers. Each of them consists of two sub-layers. More sub-layers are involved on the decoder side. 1810 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810–1822 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics xl F L yl LN xl+1 (a) post-norm residual unit xl LN F yl L xl+1 (b) pre-norm residual unit Figure 1: Examples of pre-norm residual unit and postno"
P19-1176,P17-1013,0,0.0334172,"ntext of neural machine translation since the emergence of RNN-based models. To ease optimization, researchers tried to reduce the number of non-linear transitions (Zhou et al., x1 4 8 x1 x6 6 x2 x3 4 x4 2 x11 2 x5 x6 0 x7 y0 x16 0 y1 y2 4.1 3.3 3.2 1.7 2.3 0.2 0.5 y5 y10 y15 y20 y25 y6 −2 1.1 0.0 0.0 0.1 0.8 0.2 0.0 0.5 x22 ∼ x31 −4 y0 y5 x11 ∼ x21 −2 x31 y4 (b) 6-layer decoder of DLCL x21 x26 y3 0.0 0.5 0.2 0.0 0.0 0.1 y30 (a) 30-layer encoder of DLCL (c) Weight distribution of y10 in the encoder Figure 5: A visualization example of learned weights in our 30-layer pre-norm DLCL model. 2016; Wang et al., 2017). But these attempts are limited to the RNN architecture and may not be straightforwardly applicable to the current Transformer model. Perhaps, the most relevant work to what is doing here is Bapna et al. (2018)’s work. They pointed out that vanilla Transformer was hard to train if the depth of the encoder was beyond 12. They successfully trained a 16-layer Transformer encoder by attending the combination of all encoder layers to the decoder. In their approach, the encoder layers are combined just after the encoding is completed, but not during the encoding process. In contrast, our approach a"
P19-1176,W18-6430,1,0.928098,"proach to learning deep networks, and plays an important role in Transformer. In principle, residual networks can be seen as instances of the ordinary differential equation (ODE), behaving like the forward Euler discretization with an initial value (Chang et al., 2018; Chen et al., 2018b). Euler’s method is probably the most popular firstorder solution to ODE. But it is not yet accurate enough. A possible reason is that only one previous step is used to predict the current value 5 (Butcher, 2003). In MT, the single-step property of the residual network makes the model “forget” distant layers (Wang et al., 2018b). As a result, there is no easy access to features extracted from lower-level layers if the model is very deep. Here, we describe a model which makes direct links with all previous layers and offers efficient access to lower-level representations in a deep stack. We call it dynamic linear combination of layers (DLCL). The design is inspired by the linear multi-step method (LMM) in numerical ODE (Ascher and Petzold, 1998). Unlike Euler’s method, LMM can effectively reuse the information in the previous steps by linear combination to achieve a higher order. Let {y0 , ..., yl } be the output of"
P19-1176,C18-1255,1,0.936275,"proach to learning deep networks, and plays an important role in Transformer. In principle, residual networks can be seen as instances of the ordinary differential equation (ODE), behaving like the forward Euler discretization with an initial value (Chang et al., 2018; Chen et al., 2018b). Euler’s method is probably the most popular firstorder solution to ODE. But it is not yet accurate enough. A possible reason is that only one previous step is used to predict the current value 5 (Butcher, 2003). In MT, the single-step property of the residual network makes the model “forget” distant layers (Wang et al., 2018b). As a result, there is no easy access to features extracted from lower-level layers if the model is very deep. Here, we describe a model which makes direct links with all previous layers and offers efficient access to lower-level representations in a deep stack. We call it dynamic linear combination of layers (DLCL). The design is inspired by the linear multi-step method (LMM) in numerical ODE (Ascher and Petzold, 1998). Unlike Euler’s method, LMM can effectively reuse the information in the previous steps by linear combination to achieve a higher order. Let {y0 , ..., yl } be the output of"
P19-1176,D18-1338,0,\N,Missing
P19-1352,P18-2049,0,0.0200019,"This is similar to the Chinese word “sangsheng” (paired with “killed”) and the English words “died” and “killed”. Figure 6(c) shows that the representations of the Chinese and English words which relate to “president” are very close. 4 Related Work Many previous works focus on improving the word representations of NMT by capturing the fine-grained (character) or coarse-grained (sub-word) monolingual characteristics, such as character-based NMT (Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Cho et al., 2014; Chen et al., 2016), sub-word NMT (Sennrich et al., 2016b; Johnson et al., 2017; Ataman and Federico, 2018), and hybrid NMT (Luong and Manning, 2016). They effectively consider and utilize the morphological information to enhance the word representations. Our work aims to enhance word representations through the bilingual features that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the sou"
P19-1352,P18-1008,0,0.0298591,"n vectors. While in (b), the two word embeddings are made up of two parts, indicating the shared (lined nodes) and the private (unlined nodes) features. This enables the two words to make use of common representation units, leading to a closer relationship between them. With the introduction of ever more powerful architectures, neural machine translation (NMT) has become the most promising machine translation method (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). For word representation, different architectures— including, but not limited to, recurrence-based (Chen et al., 2018), convolution-based (Gehring et al., 2017) and transformation-based (Vaswani et al., 2017) NMT models—have been taking advantage of the distributed word embeddings to capture the syntactic and semantic properties of words (Turian et al., 2010). Corresponding author Rd (a) Standard Introduction ∗ Long NMT usually utilizes three matrices to represent source embeddings, target input embeddings, and target output embeddings (also known as pre-softmax weight), respectively. These embeddings occupy most of the model parameters, which constrains the improvements of NMT because the recent methods beco"
P19-1352,P16-1186,0,0.0318903,"ion. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Slm ∈ R2×3 Long (Lange) Ju@@( (Ju@@) Sur ∈ R2×1 Sundial (Fiehlt) Exwf ∈ R2×5 ⊕ → ˜ ⊕ De@@( (De@@) Laden (Bericht) Long Pxwf ∈ R2×3 Swf ∈ R2×2 Ex ∈ R6×5 → ˜ ⊕ Italy (Italien) Exlm ∈ R2×5 Italy → Ju@@ De@@ Pxur ∈ R2×4 ˜ ⊕ Exur ∈ R2×5 ⊕ Laden Sundial → Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embed"
P19-1352,P16-2058,0,0.0190662,"ion. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Slm ∈ R2×3 Long (Lange) Ju@@( (Ju@@) Sur ∈ R2×1 Sundial (Fiehlt) Exwf ∈ R2×5 ⊕ → ˜ ⊕ De@@( (De@@) Laden (Bericht) Long Pxwf ∈ R2×3 Swf ∈ R2×2 Ex ∈ R6×5 → ˜ ⊕ Italy (Italien) Exlm ∈ R2×5 Italy → Ju@@ De@@ Pxur ∈ R2×4 ˜ ⊕ Exur ∈ R2×5 ⊕ Laden Sundial → Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embed"
P19-1352,P17-1106,1,0.801916,"ical meaning. Based on these observations, we find that the alignment quality is not a key factor affecting the model performance. In contrast, pairing as many as similar words possible helps the model to better learn the bilingual vector space, which improves the translation performance. The following qualitative analyses support these observations either. 3.5 Analysis of the Translation Results Table 6 shows two translation examples of the NIST Chinese-English translation task. To better understand the translations produced by these two models, we use layer-wise relevance propagation (LRP) (Ding et al., 2017) to produce the attention maps of the selected translations, as shown in Figure 4 and 5. In the first example, the Chinese word “sangsheng” is a low-frequency word and its ground truth is “killed”. It is observed the inadequate representation of “sangsheng” leads to a decline in the translation quality of the vanilla, direct bridging, and decoder WT methods. In our proposed 3619 president chief 0 zongtong also zhuxi sangsheng killed −0.1 died zhuxi president zongtong chief also 0.3 yebing 0.35 juzhang president chairman zongtong weiyuanzhang chief premier 0.3 0.2 −0.2 bing −0.3 ye −0.4 −0.2 0"
P19-1352,N13-1073,0,0.201607,"as parallel words that are the translation of each other. According to the word frequency, each source word x is paired with a target aligned word yˆ that has the highest alignment probability among the candidates, and is computed as follows: yˆ = arg max logA(y|x) (1) y∈a(x) where a(·) denotes the set of aligned candidates. It is worth noting the target words that have been paired with the source words cannot be used as candidates. A(·|·) denotes the alignment probability. These can be obtained by either the intrinsic attention mechanism (Bahdanau et al., 2015) or unsupervised word aligner (Dyer et al., 2013). 2.1.2 Words with Same Word Form As shown in Figure 2(b), the sub-word “Ju@@” simultaneously exists in English and German sentences. This kind of word tends to share a medium number of features of the word embeddings. Most of the time, the source and target words with the same word form also share similar lexical meaning. This category of words generally includes Arabic numbers, punctuations, named entities, cognates and loanwords. However, there are some bilingual homographs where the words in the source and target languages look the same but have completely different meanings. For example,"
P19-1352,D17-1146,0,0.228186,"Missing"
P19-1352,N18-1032,0,0.0214543,"pturing the fine-grained (character) or coarse-grained (sub-word) monolingual characteristics, such as character-based NMT (Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Cho et al., 2014; Chen et al., 2016), sub-word NMT (Sennrich et al., 2016b; Johnson et al., 2017; Ataman and Federico, 2018), and hybrid NMT (Luong and Manning, 2016). They effectively consider and utilize the morphological information to enhance the word representations. Our work aims to enhance word representations through the bilingual features that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation. The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017), leading to more accurate and adequate"
P19-1352,P02-1040,0,0.104132,"Missing"
P19-1352,D13-1176,0,0.102581,"and (b) shared-private word embeddings. In (a), the English word “Long” and the German word “Lange”, which have similar lexical meanings, are represented by two private d-dimension vectors. While in (b), the two word embeddings are made up of two parts, indicating the shared (lined nodes) and the private (unlined nodes) features. This enables the two words to make use of common representation units, leading to a closer relationship between them. With the introduction of ever more powerful architectures, neural machine translation (NMT) has become the most promising machine translation method (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). For word representation, different architectures— including, but not limited to, recurrence-based (Chen et al., 2018), convolution-based (Gehring et al., 2017) and transformation-based (Vaswani et al., 2017) NMT models—have been taking advantage of the distributed word embeddings to capture the syntactic and semantic properties of words (Turian et al., 2010). Corresponding author Rd (a) Standard Introduction ∗ Long NMT usually utilizes three matrices to represent source embeddings, target input embeddings, and target output embeddings (also kno"
P19-1352,W04-3250,0,0.118362,"Missing"
P19-1352,P18-1164,0,0.205747,"is method can also be adapted to sub-word NMT with a shared source-target sub-word vocabulary and it performs well in language pairs with many of the same characters, such as English-German and English-French (Vaswani et al., 2017). Unfortunately, this method is not applicable to languages that are written in different alphabets, such as Chinese-English (Hassan et al., 2018). Another challenge facing the source and target word embeddings of NMT is the lack of interactions. This degrades the attention performance, leading to some unaligned translations that hurt the translation quality. Hence, Kuang et al. (2018) propose to bridge the source and target embeddings, which brings better attention to the related source and target words. Their method is applicable to any language pairs, providing a tight interaction between the source and target word pairs. However, their method requires additional components and model parameters. In this work, we aim to enhance the word representations and the interactions between the source and target words, while using even fewer parameters. To this end, we present a languageindependent method, which is called sharedprivate bilingual word embeddings, to share a part of"
P19-1352,E17-2025,0,0.382093,"ce and target words with the same word form also share similar lexical meaning. This category of words generally includes Arabic numbers, punctuations, named entities, cognates and loanwords. However, there are some bilingual homographs where the words in the source and target languages look the same but have completely different meanings. For example, the German word “Gift” means “Poison” in English. That is the reason we propose to first pair the words with similar lexical meaning instead of those words with same word forms. This might be the potential limitation of the three-way WT method (Press and Wolf, 2017), where words with the same word form indiscriminately share the same word embedding. 2.1.3 Unrelated Words We regard source and target words that cannot be paired with each other as unrelated words. Figure 2(c) shows an example of a pair of unrelated words. This category is mainly composed of lowfrequency words, such as misspelled words, special characters, and foreign words. In standard NMT, the embeddings of low-frequency words are usually inadequately trained, resulting in a poor word representation. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Sl"
P19-1352,W17-4739,0,0.0393269,"Missing"
P19-1352,P16-1162,0,0.847068,"nt the word embeddings Experiments We carry out our experiments on the small-scale IWSLT’17 {Arabic (Ar), Japanese (Ja), Korean (Ko), Chinese (Zh)}-to-English (En) translation tasks, medium-scale NIST Chinese-English (ZhEn) translation task, and large-scale WMT’14 English-German (En-De) translation task. For the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks, there are respectively 236K, 234K, 227K, and 235K sentence pairs in each training set.4 The validation set is IWSLT17.TED.tst2014 and the test set is IWSLT17.TED.tst2015. For each language, we learn a BPE model with 16K merge operations (Sennrich et al., 2016b). For the NIST Zh-En translation task, the training corpus consists of 1.25M sentence pairs with 27.9M Chinese words and 34.5M English words. We use the NIST MT06 dataset as the validation set and the test sets are the NIST MT02, MT03, MT04, MT05, MT08 datasets. To compare with the recent works, the vocabulary size is limited to 4 https://wit3.fbk.eu/mt.php?release= 2017-01-trnted 3616 Architecture SMT* RNNsearch* Transformer Zh⇒En Vanilla Source bridging Target bridging Direct bridging Vanilla Direct bridging Decoder WT Shared-private Params 74.8M 78.5M 76.6M 78.9M 90.2M 90.5M 74.9M 62.8M E"
P19-1352,P10-1040,0,0.341652,"Missing"
P19-1352,D18-1100,0,0.0187321,"the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embedding matrices. As shown in Figure 3, the source embedding Ex ∈ R|V |×d is computed as follows:: Ex = Exlm ⊕ Exwf ⊕ Exur (2) where ⊕ is the row concatenation operator. Ex(·) ∈ R|V(·) |×d represents the word embeddings of the source words belong to different categories, e.g. lm represents the words with similar lexical meaning. |V(·) |denotes the vocabulary size of the corresponding category. The process of feature sharing is also implemented by matrix co"
P19-1352,D17-1154,0,0.0174627,"w that our model with fewer parameters yields consistent improvements over the strong Transformer baselines. 2 Approach In monolingual vector space, similar words tend to have commonalities in the same dimensions of their word vectors (Mikolov et al., 2013). These commonalities include: (1) a similar degree (value) of the same dimension and (2) a similar positive or negative correlation of the same dimension. Many previous works have noticed this phenomenon and have proposed to use shared vectors to represent similar words in monolingual vector space toward model compression (Li et al., 2016; Zhang et al., 2017b; Li et al., 2018). Motivated by these works, in NMT, we assume that the source and target words that have similar characteristics should also have similar vectors. Hence, we propose to perform this sharing technique in bilingual vector space. More precisely, we share the features (dimensions) between the paired source and target embeddings (vectors). However, in contrast to the previous studies, we also model the private features of the word embedding to preserve the private characteristics of words for source and target languages. The private 3614 features allow the words to better learn th"
P19-1352,C16-1291,0,0.0216196,"tures that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation. The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017), leading to more accurate and adequate translation results with the assistance of prior guidance. We provide an alternative that integrates the prior alignments through the sharing of features, which can also leads to a reduction of model parameters. Kuang et al. (2018) propose to shorten the path length between the related source and target embeddings to enhance the embedding layer. We believe that the shared features can be seem as the zero distance between the paired word embeddings. Our proposed method also uses several ideas from the three-way WT m"
P19-1352,P16-1100,0,0.041235,"ion. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Slm ∈ R2×3 Long (Lange) Ju@@( (Ju@@) Sur ∈ R2×1 Sundial (Fiehlt) Exwf ∈ R2×5 ⊕ → ˜ ⊕ De@@( (De@@) Laden (Bericht) Long Pxwf ∈ R2×3 Swf ∈ R2×2 Ex ∈ R6×5 → ˜ ⊕ Italy (Italien) Exlm ∈ R2×5 Italy → Ju@@ De@@ Pxur ∈ R2×4 ˜ ⊕ Exur ∈ R2×5 ⊕ Laden Sundial → Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embed"
P19-1352,D16-1249,0,0.0233985,"the bilingual features that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation. The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017), leading to more accurate and adequate translation results with the assistance of prior guidance. We provide an alternative that integrates the prior alignments through the sharing of features, which can also leads to a reduction of model parameters. Kuang et al. (2018) propose to shorten the path length between the related source and target embeddings to enhance the embedding layer. We believe that the shared features can be seem as the zero distance between the paired word embeddings. Our proposed method also uses several ideas from"
P19-1352,D14-1179,0,\N,Missing
P19-1352,Q17-1024,0,\N,Missing
W10-4168,H05-1097,0,0.0601431,"Missing"
W10-4168,P95-1026,0,0.398386,"Missing"
W10-4168,P96-1006,0,0.281134,"Missing"
W10-4168,P04-3026,0,0.0410563,"Missing"
W10-4168,J98-1004,0,\N,Missing
W13-2225,J93-2003,0,0.0654043,"the model parameters are stored on disk in the HFile format (Pino et al., 2012) for fast querying. Rule extraction and feature computation takes about 2h30. The HFile format requires data to be stored in a key-value structure. For the key, we use shared source side of many rules. The value is a list of tuples containing the possible targets for the source key and the associated parameters of the full rule. The query set of keys for the test set is all possible source phrases (including nonterminals) found in the test set. During HFile querying we add other features. These include IBM Model 1 (Brown et al., 1993) lexical probabilities. Loading these models in memory doesn’t fit well with the MapReduce model so lexical features are computed for each # Tokens 652.5M 654.1M 1594.3M 874.1M 1429.3M 66.4M 326.5M 1744.3M 425.3M 7766.9M Table 2: Statistics for English monolingual corpora. 2.5 Decoding For translation, we use the HiFST decoder (Iglesias et al., 2009). HiFST is a hierarchical decoder that builds target word lattices guided by a probabilistic synchronous context-free grammar. Assuming N to be the set of non-terminals and T the set of terminals or words, then we can define the grammar as a set R"
W13-2225,P11-2080,0,0.0174695,"al grammars (de Gispert et al., 2010) in our experiments. This model is constrained enough that the decoder can build exact search spaces, i.e. there is no pruning in search that may lead to spurious undergeneration errors. 2.6 R X E Y 1 pM 1 (ene |rur ) (E + 1)R r=1 e=0 (1) where ru are the terminals in the Russian side of a rule, en are the terminals in the English side of a rule, including the null word, R is the number of Russian terminals, E is the number of English terminals and pM 1 is the IBM Model 1 probability. In addition to these standard features, we also use provenance features (Chiang et al., 2011). The parallel data is divided into four subcorpora: the Common Crawl (CC) corpus, the News Commentary (NC) corpus, the Yandex (Yx) corpus and the Wiki Headlines (Wiki) corpus. For each of these subcorpora, source-to-target and target-to-source translation and lexical scores are computed. This requires computing IBM Model 1 for each subcorpus. In total, there are 28 features, 12 standard features and 16 provenance features. When retrieving relevant rules for a particular test set, various thresholds are applied, such as number of targets per source or translation probability cutoffs. Threshold"
W13-2225,J07-2003,0,0.681038,"ides of the parallel corpus are then lowercased, so mixed case is restored in post-processing. Corpus statistics after filtering and for various segmentations are summarised in Table 1. Introduction This paper describes the University of Cambridge system submission to the ACL 2013 Eighth Workshop on Statistical Machine Translation (WMT13). Our translation system is HiFST (Iglesias et al., 2009), a hierarchical phrase-based decoder that generates translation lattices directly. Decoding is guided by a CYK parser based on a synchronous context-free grammar induced from automatic word alignments (Chiang, 2007). The decoder is implemented with Weighted Finite State Transducers (WFSTs) using standard operations available in the OpenFst libraries (Allauzen et al., 2007). The use of WFSTs allows fast and efficient exploration of a vast translation search space, avoiding search errors in decoding. It also allows better integration with other steps in our translation pipeline such as 5-gram language model (LM) rescoring and lattice minimum Bayes-risk (LMBR) decoding (Blackwood, 2010). We participate in the Russian-English translation shared task in the Russian-English direction. This is the first time we"
W13-2225,N09-2019,1,0.905812,"Missing"
W13-2225,J10-3008,1,0.911969,"Missing"
W13-2225,D07-1090,0,0.0739642,"to-source lexical scores • target word count • rule count • glue rule count 2.7 • deletion rule count (each source unigram, except for OOVs, is allowed to be deleted) Lattice Rescoring The HiFST decoder is set to directly generate large translation lattices encoding many alternative translation hypotheses. These first-pass lattices are rescored with second-pass higher-order LMs prior to LMBR. • binary feature indicating whether a rule is extracted once, twice or more than twice (Bender et al., 2007) 202 2.7.1 5-gram LM Lattice Rescoring We build a sentence-specific, zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram LMs estimated over the data described in section 2.4. Lattices obtained by first-pass decoding are rescored with this 5-gram LM (Blackwood, 2010). We assume for human judgment purposes that it is better to have a non English word in Latin alphabet than in Cyrillic (e.g. uprazdnyayushchie); sometimes, transliteration can also give a correct output (e.g. Movember), especially in the case of proper nouns. 2.7.2 LMBR Decoding Minimum Bayes-risk decoding (Kumar and Byrne, 2004) over the full evidence space of the 5gram rescored lattices is applied to select the translation hypothesis that m"
W13-2225,W08-0333,0,0.0160843,"ction and Retrieval A synchronous context-free grammar (Chiang, 2007) is extracted from the alignments. The constraints are set as in the original publication with the following exceptions: • phrase-based rule maximum number of source words: 9 Corpus EU + NC + UN + CzEng + Yx Giga + CC + Wiki News Crawl afp apw cna + wpb ltw nyt xin Total • maximum number of source element (terminal or nonterminal): 5 • maximum span for nonterminals: 10 Maximum likelihood estimates for the translation probabilities are computed using MapReduce. We use a custom Hadoop-based toolkit which implements method 3 of Dyer et al. (2008). Once computed, the model parameters are stored on disk in the HFile format (Pino et al., 2012) for fast querying. Rule extraction and feature computation takes about 2h30. The HFile format requires data to be stored in a key-value structure. For the key, we use shared source side of many rules. The value is a list of tuples containing the possible targets for the source key and the associated parameters of the full rule. The query set of keys for the test set is all possible source phrases (including nonterminals) found in the test set. During HFile querying we add other features. These incl"
W13-2225,P13-2121,0,0.0522527,"Missing"
W13-2225,N09-1049,1,0.900817,"Missing"
W13-2225,2005.iwslt-1.8,0,0.0447139,"9.3M 66.4M 326.5M 1744.3M 425.3M 7766.9M Table 2: Statistics for English monolingual corpora. 2.5 Decoding For translation, we use the HiFST decoder (Iglesias et al., 2009). HiFST is a hierarchical decoder that builds target word lattices guided by a probabilistic synchronous context-free grammar. Assuming N to be the set of non-terminals and T the set of terminals or words, then we can define the grammar as a set R = {R} of rules R : N → hγ,αi / p, where N ∈ N, γ, α ∈ {N ∪ T}+ and p the rule score. 201 No alignment information is used when computing lexical scores as done in Equation (4) in (Koehn et al., 2005). Instead, the source-to-target lexical score is computed in Equation 1: HiFST translates in three steps. The first step is a variant of the CYK algorithm (Chappelier and Rajman, 1998), in which we apply hypothesis recombination without pruning. Only the source language sentence is parsed using the corresponding source-side context-free grammar with rules N → γ. Each cell in the CYK grid is specified by a non-terminal symbol and position: (N, x, y), spanning sxx+y−1 on the source sentence s1 ...sJ . For the second step, we use a recursive algorithm to construct word lattices with all possible"
W13-2225,N04-1022,1,0.716856,"(Bender et al., 2007) 202 2.7.1 5-gram LM Lattice Rescoring We build a sentence-specific, zero-cutoff stupidbackoff (Brants et al., 2007) 5-gram LMs estimated over the data described in section 2.4. Lattices obtained by first-pass decoding are rescored with this 5-gram LM (Blackwood, 2010). We assume for human judgment purposes that it is better to have a non English word in Latin alphabet than in Cyrillic (e.g. uprazdnyayushchie); sometimes, transliteration can also give a correct output (e.g. Movember), especially in the case of proper nouns. 2.7.2 LMBR Decoding Minimum Bayes-risk decoding (Kumar and Byrne, 2004) over the full evidence space of the 5gram rescored lattices is applied to select the translation hypothesis that maximises the conditional expected gain under the linearised sentence-level BLEU score (Tromble et al., 2008; Blackwood, 2010). The unigram precision p and average recall ratio r are set as described in Tromble et al. (2008) using the newstest2012.tune development set. 3 2.8 Results are reported in Table 3. We use the internationalisation switch for the NIST BLEU scoring script in order to properly lowercase the hypothesis and the reference. This introduces a slight discrepancy wit"
W13-2225,D08-1076,0,0.0967682,"M Model 1 for each subcorpus. In total, there are 28 features, 12 standard features and 16 provenance features. When retrieving relevant rules for a particular test set, various thresholds are applied, such as number of targets per source or translation probability cutoffs. Thresholds involving source-totarget translation scores are applied separately for each provenance and the union of all surviving rules for each provenance is kept. This strategy gives slight gains over using thresholds only for the general translation table. We use an implementation of lattice minimum error rate training (Macherey et al., 2008) to optimise under the BLEU score (Papineni et al., 2001) the feature weights with respect to the odd sentences of the newstest2012 development set (newstest2012.tune). The weights obtained match our expectation, for example, the source-to-target translation feature weight is higher for the NC corpus than for other corpora since we are translating news. s(ru, en) = Features and Parameter Optimisation We use the following standard features: • language model • source-to-target and target-to-source translation scores • source-to-target and target-to-source lexical scores • target word count • rul"
W13-2225,2001.mtsummit-papers.68,0,0.0503054,"ures, 12 standard features and 16 provenance features. When retrieving relevant rules for a particular test set, various thresholds are applied, such as number of targets per source or translation probability cutoffs. Thresholds involving source-totarget translation scores are applied separately for each provenance and the union of all surviving rules for each provenance is kept. This strategy gives slight gains over using thresholds only for the general translation table. We use an implementation of lattice minimum error rate training (Macherey et al., 2008) to optimise under the BLEU score (Papineni et al., 2001) the feature weights with respect to the odd sentences of the newstest2012 development set (newstest2012.tune). The weights obtained match our expectation, for example, the source-to-target translation feature weight is higher for the NC corpus than for other corpora since we are translating news. s(ru, en) = Features and Parameter Optimisation We use the following standard features: • language model • source-to-target and target-to-source translation scores • source-to-target and target-to-source lexical scores • target word count • rule count • glue rule count 2.7 • deletion rule count (each"
W13-2225,D08-1065,0,0.12026,"t-pass decoding are rescored with this 5-gram LM (Blackwood, 2010). We assume for human judgment purposes that it is better to have a non English word in Latin alphabet than in Cyrillic (e.g. uprazdnyayushchie); sometimes, transliteration can also give a correct output (e.g. Movember), especially in the case of proper nouns. 2.7.2 LMBR Decoding Minimum Bayes-risk decoding (Kumar and Byrne, 2004) over the full evidence space of the 5gram rescored lattices is applied to select the translation hypothesis that maximises the conditional expected gain under the linearised sentence-level BLEU score (Tromble et al., 2008; Blackwood, 2010). The unigram precision p and average recall ratio r are set as described in Tromble et al. (2008) using the newstest2012.tune development set. 3 2.8 Results are reported in Table 3. We use the internationalisation switch for the NIST BLEU scoring script in order to properly lowercase the hypothesis and the reference. This introduces a slight discrepancy with official results going into the English language. The newstest2012.test development set consists of even sentences from newstest2012. We observe that the CoreNLP system (A) outperforms the other two systems. The CoreNLP+"
W13-2225,P02-1040,0,\N,Missing
W13-2225,2010.iwslt-keynotes.2,1,\N,Missing
W18-6430,W17-4739,0,0.0438288,"there is the Pas@@ port , which was released last September . so there is the Passport , which was released last September . Furious residents have savaged Sol@@ i@@ hull Council saying it was “ useless at dealing with the problem ”. 愤怒的居民猛烈抨击了 S@@ ol@@ i@@ h@@ ou@@ s@@ 委员会, 称它 “ 在处理这个问题上是无用的” 。 愤怒的居民猛烈抨击了 Solihull 委员会, 称它 “ 在处理这个问题上是无用的” 。 Table 1: Samples of the inconsistent translation of the constant literal between source and target sentence. The subword is split by “@@”. The two samples are picked up from newstest2018. proved effective in the WMT competitions (Sennrich and Haddow, 2016; Sennrich et al., 2017; Wang et al., 2017). Existing experimental results about ensemble decoding mainly concentrate upon a small number of models (e.g. 4 models (Wang et al., 2017; Sennrich et al., 2016c, 2017)). Besides, the ensembled models generally lack of sufficient diversity, for example, Sennrich et al. (2016c) use the last N checkpoints of a single training run, while Wang et al. (2017) use the same network architecture with different random initializations. In this paper, we study the effects of more diverse ensemble decoding from two perspectives: the number of models and the diversity of integrated mode"
W18-6430,P16-1009,0,0.589679,"nsformer architecture. We further improve the translation performance 2.4-2.6 BLEU points from four aspects, including architectural improvements, diverse ensemble decoding, reranking, and post-processing. Among constrained submissions, we rank 2nd out of 16 submitted systems on Chinese → English task and 3rd out of 16 on English → Chinese task, respectively. 1 Introduction Neural machine translation (NMT) exploits an encoder-decoder framework to model the whole translation process in an end-to-end fashion, and has achieved state-of-the-art performance in many language pairs (Wu et al., 2016; Sennrich et al., 2016c). This paper describes the submission of the NiuTrans neural machine translation system for the WMT 2018 Chinese ↔ English news translation tasks. Our baseline systems are based on the Transformer model due to the excellent translation performance and fast training thanks to the self-attention mechanism. Then we enhance it with checkpoint ensemble (Sennrich et al., 2016c) that averages the last N checkpoints of a single training run. To enable openvocabulary translation, all the words are segmented via byte pair encoding (BPE) (Sennrich et al., 2016b) for both Chinese and English. Also, we u"
W18-6430,D08-1024,0,0.0548014,"reranking and post-processing. For architectural improvements, we add relu dropout and attention dropout to improve the generalization ability and increase the inner dimension of feed-forward neural network to enlarge the model capacity (Hassan et al., 2018). We also use the novel Swish activation function (Ramachandran et al., 2018) and self-attention with relative positional representations (Shaw et al., 2018). Next, we explore more diverse ensemble decoding via increasing the number of models and using the models generated by different ways. Furthermore, at most 17 features tuned by MIRA (Chiang et al., 2008) are used to rerank the N-best hypotheses. At last, a post-processing algorithmic is proposed to correct the inconsistent English literals between the source and target sentence. Through these techniques, we can achieve 2.4-2.6 BLEU points improvement over the baselines. As a result, our systems rank the second out of 16 submitted systems on Chinese → English task and the third out of 16 on English → Chinese task among constrained submissions, respectively. This paper describes the submission of the NiuTrans neural machine translation system for the WMT 2018 Chinese ↔ English news translation"
W18-6430,P16-1162,0,0.758646,"nsformer architecture. We further improve the translation performance 2.4-2.6 BLEU points from four aspects, including architectural improvements, diverse ensemble decoding, reranking, and post-processing. Among constrained submissions, we rank 2nd out of 16 submitted systems on Chinese → English task and 3rd out of 16 on English → Chinese task, respectively. 1 Introduction Neural machine translation (NMT) exploits an encoder-decoder framework to model the whole translation process in an end-to-end fashion, and has achieved state-of-the-art performance in many language pairs (Wu et al., 2016; Sennrich et al., 2016c). This paper describes the submission of the NiuTrans neural machine translation system for the WMT 2018 Chinese ↔ English news translation tasks. Our baseline systems are based on the Transformer model due to the excellent translation performance and fast training thanks to the self-attention mechanism. Then we enhance it with checkpoint ensemble (Sennrich et al., 2016c) that averages the last N checkpoints of a single training run. To enable openvocabulary translation, all the words are segmented via byte pair encoding (BPE) (Sennrich et al., 2016b) for both Chinese and English. Also, we u"
W18-6430,N18-2074,0,0.117801,"Missing"
W18-6430,W17-4742,0,0.0210284,", which was released last September . so there is the Passport , which was released last September . Furious residents have savaged Sol@@ i@@ hull Council saying it was “ useless at dealing with the problem ”. 愤怒的居民猛烈抨击了 S@@ ol@@ i@@ h@@ ou@@ s@@ 委员会, 称它 “ 在处理这个问题上是无用的” 。 愤怒的居民猛烈抨击了 Solihull 委员会, 称它 “ 在处理这个问题上是无用的” 。 Table 1: Samples of the inconsistent translation of the constant literal between source and target sentence. The subword is split by “@@”. The two samples are picked up from newstest2018. proved effective in the WMT competitions (Sennrich and Haddow, 2016; Sennrich et al., 2017; Wang et al., 2017). Existing experimental results about ensemble decoding mainly concentrate upon a small number of models (e.g. 4 models (Wang et al., 2017; Sennrich et al., 2016c, 2017)). Besides, the ensembled models generally lack of sufficient diversity, for example, Sennrich et al. (2016c) use the last N checkpoints of a single training run, while Wang et al. (2017) use the same network architecture with different random initializations. In this paper, we study the effects of more diverse ensemble decoding from two perspectives: the number of models and the diversity of integrated models. We explore at mo"
W19-5325,D18-1045,0,0.0432723,"ta by mixing the pseudo corpus with the parallel part, in that the target side lexicon coverage is insufficient, such as EN ↔ {KK, GU} only consist of 0.11M and 0.5M bilingual data, respectively. How to select the appropriate sentences from the abundant monolingual data is a crucial issue due to the limitation of equipment and huge overhead time. We trained a 5-gram language model based on the mixture of development set and bilingual-target side data to score the monolingual sentences. In addition, considering the impact of sequence length, we set a threshold range from 10 to 50. Recent work (Edunov et al., 2018) has shown that different methods of generating pseudo corpus made discrepant influence on translation performance. Edunov et al. (2018) indicated that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. This year we attempted several data augmentation methods as follows: Greedy Based Ensemble Ensemble decoding is an effective system combination method to boost machine translation quality via integrating the predictions of several single models at each decode step. It has been proved effective in the past few years’ WMT tasks (Wa"
W19-5325,D16-1139,0,0.0400724,"knowledge distillation method to enhance the single model performance. Experiment results showed that the accuracy of the reverse model was extremely necessary, or you may even get worse results. Student 1 Iteration 2 Distillation Ensemble Figure 2: A simple example of Iterative Knowledge Distillation with 5 students, 2 teachers and 2 iterations 2015; Freitag et al., 2017) and ensemble iteratively. The naive approach started with a list of single model candidates as the students and the best 4 models combination retrieved from Algorithm 1 as the teacher. Sequence-level knowledge distillation (Kim and Rush, 2016) was then applied to finetune each student model with additional source data. With these enhanced student models, a stronger 4 models combination can be produced through Algorithm 1. We iterated this process until less than 0.1 BLEU improvement on the validation set. However, in the preliminary experiments we found that such iteration didn’t yield good results as we expected. We attributed this phenomenon to the deficiency of model diversity, due to the fact that all students were collapsed to a similar optimum induced by the same teacher they learnt from, which limited the potential gain from"
W19-5325,P19-1019,0,0.0718891,"ngual data. We also applied iterative knowledge distillation (Freitag et al., 2017) to leverage the source-side monolingual data. Our system also employed the conventional combination methods including ensemble and feature-based re-ranking to further improve the translation quality. We proposed a simple greedy search algorithm to find the best ensemble combination effectively and efficiently. Hypothesis combination (Hassan et al., 2018) was also adopted to generate more diverse hypotheses for better reranking. For unsupervised tasks, we mainly investigated the methodology of unsupervised SMT (Artetxe et al., 2019) and NMT (Lample and Conneau, 2019) to build our baselines, then presented a joint training strategy on top of these baselines to boost their performances. This paper was structured as follows: we described the details of our novel Deep-Transformer in Section 2, then in Section 3 we presented an overview of our universal training flow for all supervised language pairs and the unsupervised methods. The experiment settings and main results were shown in Section 4. This paper described NiuTrans neural machine translation systems for the WMT 2019 news translation tasks. We participated in 13 trans"
W19-5325,D18-1338,0,0.0219504,"dard Transformer-Big significantly in terms of both translation quality and convergence speed. As for the data augmentation aspect, we experimented several back-translation methods (Sennrich et al., 2016b), including beam search, unrestricted sampling and sampling-topK proposed 2 Deep Transformer Neural machine translation models based on multi-layer self-attention (Vaswani et al., 2017) has shown strong results on several large-scale tasks. Enlarging the model capacity is an effective way to obtain stronger networks, including widening the hidden representation or deepening the model layers. Bapna et al. (2018) has shown that learning deeper networks is not easy for vanilla Transformer due to the gradient vanishing/exploding problem. 257 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 257–266 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics xl F L yl LN unique edge representations per layer and head. Pre-Norm Transformer-DLCL: The Transformer-DLCL employed direct links with all the previous layers and offered efficient access to lower-level representations in a deep stack. An additional weight matrix Wl+"
W19-5325,J82-2005,0,0.707119,"Missing"
W19-5325,N12-1047,0,0.0587276,"he n-best hypothesis and the source sentence-level vectors (Hassan et al., 2018) . Sentence-Align Score: We used fast-align tool to evaluate the alignment probability between the source and the target. Translation Coverage: A SMT phrase-table to obtain the top-50 translation for each source-totarget word pair. In this way, the translation coverage score can be easily gained with respect to the dual direction hits in the dictionary with length normalization. We rescored 96-best outputs generated by several ensemble systems using a rescoring model consisting of features above by K-batched MIRA (Cherry and Foster, 2012) algorithm which is widely used in Moses4 . Feature Reranking This year we adopted an hypothesis combination strategy to pick up a potentially better translation from the N-best consisting of several different ensemble outputs. For example we generated 96 hypothesises by 8 different ensemble systems, and set the beam size as 12 during the decoding procedure instead of obtaining all 96 outputs from a sin4 https://github.com/moses-smt/ mosesdecoder 260 3.6 Unsupervised NMT our deep models can even outperform the standard Transformer-Big by 0.7-1.3 BLEU scores on different language pairs. All of"
W19-5325,W16-2323,0,0.221721,"e distillation and ensemble+reranking were also employed to obtain stronger models. Our unsupervised submissions were based on NMT enhanced by SMT. As a result, we achieved the highest BLEU scores in {KK↔EN, GU→EN} directions, ranking 2nd in {RU→EN, DE↔CS} and 3rd in {ZH→EN, LT→EN, EN→RU, EN↔DE} among all constrained submissions. 1 Introduction Our NiuTrans team participated in 13 WMT19 shared news translation tasks, including 11 supervised and 2 unsupervised sub-tracks. We reused some effective approaches of our WMT18 submissions (Wang et al., 2018), including backtranslation by beam search (Sennrich et al., 2016b), BPE (Sennrich et al., 2016c) and further strengthened our systems by exploiting some new techniques this year. For our supervised task submissions, all the language pairs shared similar model architectures and training flow. We proposed four novel Deep-Transformer architectures based on (Wang et al., 2019) as our baseline, which outperformed the standard Transformer-Big significantly in terms of both translation quality and convergence speed. As for the data augmentation aspect, we experimented several back-translation methods (Sennrich et al., 2016b), including beam search, unrestricted s"
W19-5325,W18-6408,0,0.0279134,"ifferent methods of generating pseudo corpus made discrepant influence on translation performance. Edunov et al. (2018) indicated that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. This year we attempted several data augmentation methods as follows: Greedy Based Ensemble Ensemble decoding is an effective system combination method to boost machine translation quality via integrating the predictions of several single models at each decode step. It has been proved effective in the past few years’ WMT tasks (Wang et al., 2018; Deng et al., 2018; Junczys-Dowmunt, 2018; Sennrich et al., 2016a). We enhanced the single model by employing deep self-attentional models. Note that the improvement is poor if the single models performed strong enough and no significant benefits from increasing the participant quantity. So it’s necessary to utilize the models sufficiently to search for a better combination on the development set. We adopted an easily operable greedy-base strategy as the following: Algorithm 1 An Simple ensemble algorithm based on greedy search Input: a model list Ωcand sorted by the development scores. Output: a final model li"
W19-5325,P16-1009,0,0.552243,"e distillation and ensemble+reranking were also employed to obtain stronger models. Our unsupervised submissions were based on NMT enhanced by SMT. As a result, we achieved the highest BLEU scores in {KK↔EN, GU→EN} directions, ranking 2nd in {RU→EN, DE↔CS} and 3rd in {ZH→EN, LT→EN, EN→RU, EN↔DE} among all constrained submissions. 1 Introduction Our NiuTrans team participated in 13 WMT19 shared news translation tasks, including 11 supervised and 2 unsupervised sub-tracks. We reused some effective approaches of our WMT18 submissions (Wang et al., 2018), including backtranslation by beam search (Sennrich et al., 2016b), BPE (Sennrich et al., 2016c) and further strengthened our systems by exploiting some new techniques this year. For our supervised task submissions, all the language pairs shared similar model architectures and training flow. We proposed four novel Deep-Transformer architectures based on (Wang et al., 2019) as our baseline, which outperformed the standard Transformer-Big significantly in terms of both translation quality and convergence speed. As for the data augmentation aspect, we experimented several back-translation methods (Sennrich et al., 2016b), including beam search, unrestricted s"
W19-5325,P16-1162,0,0.845681,"e distillation and ensemble+reranking were also employed to obtain stronger models. Our unsupervised submissions were based on NMT enhanced by SMT. As a result, we achieved the highest BLEU scores in {KK↔EN, GU→EN} directions, ranking 2nd in {RU→EN, DE↔CS} and 3rd in {ZH→EN, LT→EN, EN→RU, EN↔DE} among all constrained submissions. 1 Introduction Our NiuTrans team participated in 13 WMT19 shared news translation tasks, including 11 supervised and 2 unsupervised sub-tracks. We reused some effective approaches of our WMT18 submissions (Wang et al., 2018), including backtranslation by beam search (Sennrich et al., 2016b), BPE (Sennrich et al., 2016c) and further strengthened our systems by exploiting some new techniques this year. For our supervised task submissions, all the language pairs shared similar model architectures and training flow. We proposed four novel Deep-Transformer architectures based on (Wang et al., 2019) as our baseline, which outperformed the standard Transformer-Big significantly in terms of both translation quality and convergence speed. As for the data augmentation aspect, we experimented several back-translation methods (Sennrich et al., 2016b), including beam search, unrestricted s"
W19-5325,N18-2074,0,0.0318833,"attentional counterparts in pre-norm way as default. In this section we described the details about our deep architectures as below: Pre-Norm Transformer: In recent Tensor2Tensor implementations2 , layer normalization (Lei Ba et al., 2016) was applied to the input of every sub-layer which the computation sequence could be expressed as: normalize→Transform→dropout→residual-add. In this way we could successfully train a deeper pre-norm Transformer within comparable performance with Transformer-Big or even better, only one fourth training cost. Pre-Norm Transformer-RPR: We found Transformer-RPR (Shaw et al., 2018) which simultaneously incorporating relative position information with sinusoidal position encodings for sequences in pre-norm style could outperform the pre-norm Transformer with the same encoder depth. We used clipping distance k = 20 with the 3 System Overview 3.1 Data Filter Previous work (Junczys-Dowmunt, 2018; Wang et al., 2018; Stahlberg et al., 2018) indicated that rigorous data filtering scheme is crucial, or it will lead to catastrophic loss in quality, especially in EN↔DE and EN↔RU. For most language pairs, we filter the training bilingual corpus with the following rules: • Normaliz"
W19-5325,W18-6427,0,0.0367684,"Missing"
W19-5325,W18-6430,1,0.915803,"ormer and several back-translation methods. Iterative knowledge distillation and ensemble+reranking were also employed to obtain stronger models. Our unsupervised submissions were based on NMT enhanced by SMT. As a result, we achieved the highest BLEU scores in {KK↔EN, GU→EN} directions, ranking 2nd in {RU→EN, DE↔CS} and 3rd in {ZH→EN, LT→EN, EN→RU, EN↔DE} among all constrained submissions. 1 Introduction Our NiuTrans team participated in 13 WMT19 shared news translation tasks, including 11 supervised and 2 unsupervised sub-tracks. We reused some effective approaches of our WMT18 submissions (Wang et al., 2018), including backtranslation by beam search (Sennrich et al., 2016b), BPE (Sennrich et al., 2016c) and further strengthened our systems by exploiting some new techniques this year. For our supervised task submissions, all the language pairs shared similar model architectures and training flow. We proposed four novel Deep-Transformer architectures based on (Wang et al., 2019) as our baseline, which outperformed the standard Transformer-Big significantly in terms of both translation quality and convergence speed. As for the data augmentation aspect, we experimented several back-translation method"
W19-5325,P19-1176,1,0.921404,"ined submissions. 1 Introduction Our NiuTrans team participated in 13 WMT19 shared news translation tasks, including 11 supervised and 2 unsupervised sub-tracks. We reused some effective approaches of our WMT18 submissions (Wang et al., 2018), including backtranslation by beam search (Sennrich et al., 2016b), BPE (Sennrich et al., 2016c) and further strengthened our systems by exploiting some new techniques this year. For our supervised task submissions, all the language pairs shared similar model architectures and training flow. We proposed four novel Deep-Transformer architectures based on (Wang et al., 2019) as our baseline, which outperformed the standard Transformer-Big significantly in terms of both translation quality and convergence speed. As for the data augmentation aspect, we experimented several back-translation methods (Sennrich et al., 2016b), including beam search, unrestricted sampling and sampling-topK proposed 2 Deep Transformer Neural machine translation models based on multi-layer self-attention (Vaswani et al., 2017) has shown strong results on several large-scale tasks. Enlarging the model capacity is an effective way to obtain stronger networks, including widening the hidden r"
W19-5325,P12-3004,1,0.913581,"Missing"
