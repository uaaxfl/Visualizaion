2003.mtsummit-papers.53,W03-1502,1,0.772759,"re NEs have been manually or automatically annotated. Starting from a bilingual corpus where NEs are automatically tagged for each language, NE pairs are aligned in order to minimize a multi-feature alignment cost including the transliteration cost, the NE tagging cost, and word-based translation cost. These features are designed to capture the semantic or phonetic similarities between NE pairs as well as NE tagging confidence, and are derived from several information sources using unsupervised and partly supervised methods. A greedy search algorithm is applied to minimize the alignment cost (Huang et al., 2003). Online NE translation is specially designed for translating NEs which appear in the given test document, but are not covered by the Offline translation. The missing source NEs and target NE translations are “retrieved” cross-lingually from topic-relevant documents (w.r.t. the test document). Relevant documents are retrieved from a monolingual corpus using a 1st-pass translation of the test document as the query. NEs in the retrieved documents are extracted and aligned with source NEs according to their transliteration cost. The NE pairs with minimum transliteration cost are considered as tra"
2003.mtsummit-papers.53,J93-2003,0,0.119946,"ical machine translation system. This system combines phrase-tophrase translations extracted from a bilingual corpus using different alignment approaches. Special methods to extract and align named entities are used. We show how a manual lexicon can be incorporated into the statistical system in an optimized way. Experiments on Chinese-toEnglish and Arabic-to-English translation tasks are presented. 1 Introduction Statistical machine translation is currently the most promising approach to large vocabulary text translation. In the spirit of the Candide system developed in the early 90s at IBM (Brown et al., 1993), a number of statistical machine translation systems have been presented in the last few years (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamada and Knight, 2000). These systems share the basic underlying principles of applying a translation model to capture the lexical and word reordering relationships between two languages, complemented by a target language model to drive the search process through translation model hypotheses. Their primary differences lie in the structure and source of their translation models. Whereas the original IBM system was based on purely word-based translation"
2003.mtsummit-papers.53,W02-1018,0,0.110892,"Missing"
2003.mtsummit-papers.53,P03-1041,1,0.837652,"Missing"
2003.mtsummit-papers.53,C96-2141,1,0.3498,"four different approaches to phrase pair extraction, each of which will be described below. We also describe our technique for adding generalization power by allowing for overlapping phrases. 2.1 From Viterbi Path of HMM Word Alignment A simple approach to extract phrase translations from a bilingual corpus is to harvest the Viterbi path generated by a word alignment model. A number of probabilistic word alignment models have been proposed (Brown et al., 1993) (Och and Ney, 2000) and shown to be effective for statistical machine translation. We use the HMM-based alignment model introduced in (Vogel et al., 1996) which estimates position alignment probabilities in addition to lexical probabilities. The HMM-based alignment model is based on relative positions: it addresses the likelihood that the word at source position j +1 is aligned to target position i0 when source position j is aligned to target position i. The Viterbi path can be used not only to map source words to target words, i.e. building a statistical lexicon, but also to map source phrases to target phrases. For each source phrase ranging from positions j1 to j2 the corresponding target phrase is given by imin = minj {i = a(j)} and imax ="
2003.mtsummit-papers.53,P00-1004,1,0.851861,"Missing"
2003.mtsummit-papers.53,J97-3002,0,0.054098,"ng from positions j1 to j2 the corresponding target phrase is given by imin = minj {i = a(j)} and imax = maxj {i = a(j)}, where j = j1 ...j2 . This is a very simple criterion which does not test if the source phrase actually aligns to two or more noncontiguous sequences of words in the target sentence. Due to the potential for alignment errors, such a test would be unreliable. However, by preventing the length of the aligned target phrase from exceeding the length of the source phrase by a given factor, the problem of non-contiguous alignments can be reduced. 2.2 From Bilingual Bracketing In (Wu, 1997) a word alignment model was proposed which adds additional alignment restrictions over the IBM-style alignment models. The bilingual bracketing builds an hierarchical alignment, which can be viewed as a simple top-down bilingual parse: split source and target segment into two halves f˜l , f˜r and e˜l , e˜r . Then either align f˜l to e˜l and f˜r to e˜r , which is called a straight alignment, or align f˜l to e˜r and f˜r to e˜l , called a reversed alignment. Repeat this for each aligned segment pair down to the word level. At each level the optimization is over the split points and the direction,"
2003.mtsummit-papers.53,W03-0303,1,0.85137,"Missing"
2003.mtsummit-papers.53,P02-1040,0,\N,Missing
2003.mtsummit-papers.53,P01-1067,0,\N,Missing
2003.mtsummit-papers.53,P00-1056,0,\N,Missing
2004.eamt-1.14,J90-2002,0,0.23859,"ransfer rules, which encode how syntactic constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpu"
2004.eamt-1.14,J93-2003,0,0.0115785,"encode how syntactic constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods requi"
2004.eamt-1.14,1997.tmi-1.13,0,0.0783073,"stem to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods require very large volumes of sentence-aligned parallel text for the two languages – on the order of magnitude of a million words or more. Such resources are curre"
2004.eamt-1.14,P02-1040,0,0.0790722,"Missing"
2004.eamt-1.14,2001.mtsummit-road.7,1,0.788397,"al of learning compositional syntactic transfer rules. For example, simple noun phrases are elicited before prepositional phrases and simple sentences, so that during rule learning, the system can detect cases where transfer rules for NPs can serve as components within higher-level transfer rules for PPs and sentence structures. The current controlled elicitation corpus contains about 2000 phrases and sentences. It is by design very limited in vocabulary. A more detailed description of the elicitation corpus, the elicitation process and the interface tool used for elicitation can be found in (Probst et al, 2001), (Probst and Levin, 2002). 4. Automatic Transfer Rule Learning The rule learning system takes the elicited, wordaligned data as input. Based on this information, it then infers syntactic transfer rules. The learning system also learns the composition of transfer rules. In the compositionality learning stage, the learning system identifies cases where transfer rules for “lower-level” constituents (such as NPs) can serve as components within “higher-level” transfer rules (such as PPs and sentence structures). This process generalizes the applicability of the learned transfer rules and captures"
2004.eamt-1.14,2002.tmi-papers.17,1,0.666028,"tional syntactic transfer rules. For example, simple noun phrases are elicited before prepositional phrases and simple sentences, so that during rule learning, the system can detect cases where transfer rules for NPs can serve as components within higher-level transfer rules for PPs and sentence structures. The current controlled elicitation corpus contains about 2000 phrases and sentences. It is by design very limited in vocabulary. A more detailed description of the elicitation corpus, the elicitation process and the interface tool used for elicitation can be found in (Probst et al, 2001), (Probst and Levin, 2002). 4. Automatic Transfer Rule Learning The rule learning system takes the elicited, wordaligned data as input. Based on this information, it then infers syntactic transfer rules. The learning system also learns the composition of transfer rules. In the compositionality learning stage, the learning system identifies cases where transfer rules for “lower-level” constituents (such as NPs) can serve as components within “higher-level” transfer rules (such as PPs and sentence structures). This process generalizes the applicability of the learned transfer rules and captures the compositional makeup o"
2004.eamt-1.14,2003.mtsummit-papers.53,1,0.816612,"ich four English reference translations are available. The following systems were evaluated in the experiment: 1. Three versions of the Hindi-to-English XFER system: 1a. XFER with No Grammar: the XFER system with no syntactic transfer rules (i.e. only lexical phrase-to-phrase matches and word-toword lexical transfer rules, with and without morphology). 1b. XFER with Learned Grammar: The XFER system with automatically learned syntactic transfer rules. 1c. XFER with Manual Grammar: The XFER system with the manually developed syntactic transfer rules. 2. SMT: The CMU Statistical MT (SMT) system (Vogel et al, 2003), trained on the limited-data parallel text resources. 3. EBMT: The CMU Example-based MT (EBMT) system (Brown, 1997), trained on the limiteddata parallel text resources. 4. MEMT: A “multi-engine” version that combines the lattices produced by the SMT system, and the XFER system with manual grammar. The decoder then selects an output from the joint lattice. Performance of the systems was measured using the NIST scoring metric (Doddington, 2002), as well as the BLEU score (Papineni et al, 2002). In order to validate the statistical significance of the differences in NIST and BLEU scores, we appl"
2004.eamt-1.14,P01-1067,0,0.0309867,"he source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods require very large volumes of sentence-aligned parallel tex"
2004.eamt-1.14,C90-3044,0,\N,Missing
2004.eamt-1.14,P02-1038,0,\N,Missing
2004.iwslt-evaluation.11,C96-2141,1,0.873342,"Missing"
2004.iwslt-evaluation.11,2003.mtsummit-papers.53,1,0.846307,"Missing"
2004.iwslt-evaluation.11,J93-2003,0,0.0333438,"Missing"
2004.iwslt-evaluation.11,1999.mtsummit-1.31,0,0.0819927,"Missing"
2004.iwslt-evaluation.11,P00-1056,0,0.138706,"Missing"
2004.iwslt-evaluation.11,P02-1040,0,0.0695583,"Missing"
2004.iwslt-evaluation.11,P01-1067,0,\N,Missing
2004.iwslt-evaluation.11,takezawa-etal-2002-toward,0,\N,Missing
2004.tmi-1.9,2003.mtsummit-papers.10,0,0.0455842,"Missing"
2004.tmi-1.9,1993.eamt-1.1,0,0.0164711,"Missing"
2004.tmi-1.9,2003.mtsummit-papers.32,0,0.0313837,"ne whether two MT systems are significantly different from each other. We study the effect of test set size and number of reference translations on the confidence intervals for these MT evaluation metrics. 1. Introduction Automatic evaluation for Machine Translation (MT) systems has become prominent with the development of data driven MT. The essential idea comes from the highly successful word error rate metric used by the speech recognition community. For MT evaluation this has been extended to multiple reference translations (Nießen et al. 2000), and allowing for differences in word order (Leusch et al. 2003). In (Papineni et al, 2002) the BLEU metric was proposed, which averages the precision for unigram, bigram and up to 4-grams and applies a length penalty for translations too short. A variant of BLEU has been developed by NIST, using the information gain of the ngrams. Additional modifications to BLEU-type metrics have been proposed to improve the correlation with human evaluation scores (Melamed 2003, Pepescu-Belis 2003). Both BLEU/NIST metrics require a test suite to evaluate the MT systems. A test suite consists of two parts: testing sentences in the source language and multiple human refer"
2004.tmi-1.9,J82-2005,0,0.0606381,"Missing"
2004.tmi-1.9,2003.mtsummit-papers.30,0,0.0808556,"Missing"
2004.tmi-1.9,niessen-etal-2000-evaluation,0,0.0644291,"NIST scores using bootstrapping. With this method, we can determine whether two MT systems are significantly different from each other. We study the effect of test set size and number of reference translations on the confidence intervals for these MT evaluation metrics. 1. Introduction Automatic evaluation for Machine Translation (MT) systems has become prominent with the development of data driven MT. The essential idea comes from the highly successful word error rate metric used by the speech recognition community. For MT evaluation this has been extended to multiple reference translations (Nießen et al. 2000), and allowing for differences in word order (Leusch et al. 2003). In (Papineni et al, 2002) the BLEU metric was proposed, which averages the precision for unigram, bigram and up to 4-grams and applies a length penalty for translations too short. A variant of BLEU has been developed by NIST, using the information gain of the ngrams. Additional modifications to BLEU-type metrics have been proposed to improve the correlation with human evaluation scores (Melamed 2003, Pepescu-Belis 2003). Both BLEU/NIST metrics require a test suite to evaluate the MT systems. A test suite consists of two parts:"
2004.tmi-1.9,2003.mtsummit-papers.41,0,0.068372,"Missing"
2004.tmi-1.9,zhang-etal-2004-interpreting,1,0.841855,"Missing"
2005.eamt-1.18,J93-2003,0,0.00549346,"translations produced by the TM system were compared with the translations produced by the statistical decoder. In our current experiments TM system did not show an improvement in terms of automatic evaluation metrics. However, a subjective human evaluation found that, in several instances, the TM system produced better translations than the statistical decoder. In the following section we explain the TM system in detail. We also describe the phrase extraction method we used to identify alignments between source words and target words, which is a modified version of the IBM1 alignment model (Brown et al. 1993). In Section 3, we present the experimental setting and the results of the evaluation. It is followed by a discussion in section 4, and conclusions in section 5. We have identified a number of improvements to the current system, some of which are already in progress. 2. Translation Memory System 2.1. Extracting Similar Sentences For each new test sentence F, we find a set of similar source sentences {F1, F2, …} from the training corpus. The similarity is measured in terms of the standard edit distance criterion with equal penalties for insertion, deletion and substitution operations. The corre"
2005.eamt-1.18,langlais-simard-2002-merging,0,0.0180005,"ne translation memory with other machine translation approaches. In (Marcu, 2001) an automatically derived TM is used along with a statistical model to obtain translations of higher probability than those found using only a statistical model. Sumita (2001) describes an example-based technique which extracts similar translations and modifies them using a bilingual dictionary. Watanabe and Sumita (2003) proposed an example-based decoder that start with close matching example translations, and then modify them using a greedy search algorithm. Instead of extracting complete sentences from the TM, Langlais and Simard, (2002) work on sub sentential leEAMT 2005 Conference Proceedings Augmenting a statistical translation system with a translation memory vel. Translations for word sequences are extracted from a TM and then fed into a statistical engine to generate the desired translation. In this paper, we present an experiment where we attempted to augment a statistical translation system with a translation memory. For a sentence which has a close match in the training corpus, the idea is to start with the available translation and apply specific modifications to produce the desired translation. By a close match, we"
2005.eamt-1.18,P01-1050,0,0.0198843,"words; sometimes even an exact matching sentence. Translation memory (TM) systems typically work well in these situations. In its pure form, a TM system is simply a database of past translations, stored as sentence pairs in source and target languages. Whenever an exact match is found for a new sentence to be translated, the desired translation is extracted from the translation memory. TM systems have been successfully used in Computer Aided Translations (CAT) as a tool for human translators. There have been attempts to combine translation memory with other machine translation approaches. In (Marcu, 2001) an automatically derived TM is used along with a statistical model to obtain translations of higher probability than those found using only a statistical model. Sumita (2001) describes an example-based technique which extracts similar translations and modifies them using a bilingual dictionary. Watanabe and Sumita (2003) proposed an example-based decoder that start with close matching example translations, and then modify them using a greedy search algorithm. Instead of extracting complete sentences from the TM, Langlais and Simard, (2002) work on sub sentential leEAMT 2005 Conference Proceed"
2005.eamt-1.18,lavie-etal-2002-nespole,0,0.0280647,"Missing"
2005.eamt-1.18,1999.mtsummit-1.31,0,0.0492566,"Missing"
2005.eamt-1.18,2001.mtsummit-papers.68,0,0.0451136,"Missing"
2005.eamt-1.18,takezawa-etal-2002-toward,0,0.0143,"with other machine translation approaches. In (Marcu, 2001) an automatically derived TM is used along with a statistical model to obtain translations of higher probability than those found using only a statistical model. Sumita (2001) describes an example-based technique which extracts similar translations and modifies them using a bilingual dictionary. Watanabe and Sumita (2003) proposed an example-based decoder that start with close matching example translations, and then modify them using a greedy search algorithm. Instead of extracting complete sentences from the TM, Langlais and Simard, (2002) work on sub sentential leEAMT 2005 Conference Proceedings Augmenting a statistical translation system with a translation memory vel. Translations for word sequences are extracted from a TM and then fed into a statistical engine to generate the desired translation. In this paper, we present an experiment where we attempted to augment a statistical translation system with a translation memory. For a sentence which has a close match in the training corpus, the idea is to start with the available translation and apply specific modifications to produce the desired translation. By a close match, we"
2005.eamt-1.18,2004.iwslt-evaluation.11,1,\N,Missing
2005.eamt-1.18,W01-1401,0,\N,Missing
2005.eamt-1.18,P02-1040,0,\N,Missing
2005.eamt-1.19,J93-2003,0,0.00707137,"Missing"
2005.eamt-1.19,eck-etal-2004-language,1,0.491287,"Missing"
2005.eamt-1.19,P02-1040,0,0.0949776,"Missing"
2005.eamt-1.19,takezawa-etal-2002-toward,0,0.0178463,"Missing"
2005.eamt-1.19,2003.mtsummit-papers.53,1,0.418648,"Missing"
2005.eamt-1.19,P04-3002,0,0.0319308,"Missing"
2005.eamt-1.19,C04-1059,1,0.531263,"Missing"
2005.eamt-1.19,C96-2141,1,\N,Missing
2005.eamt-1.36,2001.mtsummit-papers.68,0,0.0243081,"Missing"
2005.eamt-1.36,P03-1021,0,0.105237,"Missing"
2005.eamt-1.36,N04-1023,0,0.0346647,"Missing"
2005.eamt-1.39,J90-2002,0,0.15842,"Missing"
2005.eamt-1.39,N03-1017,0,0.114144,"Missing"
2005.eamt-1.39,W02-1018,0,0.0398997,"Missing"
2005.eamt-1.39,W99-0604,0,0.0696638,"Missing"
2005.eamt-1.39,2001.mtsummit-papers.68,0,0.0566669,"Missing"
2005.eamt-1.39,C96-2141,1,0.334597,"Missing"
2005.eamt-1.39,2003.mtsummit-papers.53,1,0.745977,"Missing"
2005.eamt-1.39,J01-1001,0,0.204291,"Missing"
2005.eamt-1.39,zhang-etal-2004-interpreting,1,0.160165,"Missing"
2005.eamt-1.39,P02-1040,0,\N,Missing
2005.iwslt-1.6,J93-2003,0,0.00594062,"For Chinese-English direction we also worked on ASR output of the supplied data, and with additional data in unrestricted and C-STAR tracks. 1. Introduction Large vocabulary text translation has been the primary focus in machine translation research during the past. Much improvements have been achieved with projects such as TIDES, which focused on large vocabulary text translation. With the availability of reliable speech recognition systems and spoken language corpora, now the focus is shifting towards speech translation; and further towards speech-to-speech translation. With the IBM system [1] in early 90’s, statistical machine translation (SMT) has been the most promising approach for machine translation. Many approaches for SMT have been proposed since then [2], [3], [4]. Whereas the original IBM system was based on purely word translation models, current SMT systems incorporate more sophisticated models. The CMU statistical machine translation system uses phrase-to-phrase translations as the primary building blocks to capture local context information, leading to better lexical choice and more reliable local reordering. In section 2, we describe the phrase alignment approaches u"
2005.iwslt-1.6,P03-1021,0,0.00823941,"ity is restricted. However, the reliability of one model might be higher than the reliability of another model. So, we should put more weight on this model in the overall decision. This can be done by doing a log-linear combination of the models. In other words, each model score is weighted and we have to find an optimal set of these weights or scaling factors. When dealing with two or three models, grid search is still feasible. When adding more and more features (models) this no longer is the case and automatic optimization needs to be done. We use the Minimum Error Training as described in [11], which uses rescoring of the n-best list to find the scaling factors with maximize BLEU or NIST score. Starting with some reasonably chosen model weights a first decoding for some development test set is done. An n-best list is generated, typically a 1000-best list. Then a multi-linear search is performed, for each model weight in turn. The weight, for which the change gives the best improvement in the MT evaluation metric, is then fixed to the new value, and the search repeated, till no further improvement is possible. The optimization is therefore based on an n-best list, which resulted fro"
2005.iwslt-1.6,P00-1056,0,0.0770785,"t translation has been the primary focus in machine translation research during the past. Much improvements have been achieved with projects such as TIDES, which focused on large vocabulary text translation. With the availability of reliable speech recognition systems and spoken language corpora, now the focus is shifting towards speech translation; and further towards speech-to-speech translation. With the IBM system [1] in early 90’s, statistical machine translation (SMT) has been the most promising approach for machine translation. Many approaches for SMT have been proposed since then [2], [3], [4]. Whereas the original IBM system was based on purely word translation models, current SMT systems incorporate more sophisticated models. The CMU statistical machine translation system uses phrase-to-phrase translations as the primary building blocks to capture local context information, leading to better lexical choice and more reliable local reordering. In section 2, we describe the phrase alignment approaches used by our system. The main obstacle in using additional data for a translation task is that the new data may belong to a different domain. We explored methods of adapting both t"
2005.iwslt-1.6,P01-1067,0,0.0435513,"nslation has been the primary focus in machine translation research during the past. Much improvements have been achieved with projects such as TIDES, which focused on large vocabulary text translation. With the availability of reliable speech recognition systems and spoken language corpora, now the focus is shifting towards speech translation; and further towards speech-to-speech translation. With the IBM system [1] in early 90’s, statistical machine translation (SMT) has been the most promising approach for machine translation. Many approaches for SMT have been proposed since then [2], [3], [4]. Whereas the original IBM system was based on purely word translation models, current SMT systems incorporate more sophisticated models. The CMU statistical machine translation system uses phrase-to-phrase translations as the primary building blocks to capture local context information, leading to better lexical choice and more reliable local reordering. In section 2, we describe the phrase alignment approaches used by our system. The main obstacle in using additional data for a translation task is that the new data may belong to a different domain. We explored methods of adapting both the tr"
2005.iwslt-1.6,takezawa-etal-2002-toward,0,0.0745974,"ip} where sp is a skip penalty (sp &lt; 0); d(vm−1 , vm ) is the number of skipped words between vm−1 and vm ; ip is an insertion penalty [14]. The skip penalty is incorporated to avoid high compression of the original sentence because high compression of a sentence often alters the meaning of the sentence. The insertion penalty is used to control the overall compression ratio. 6. Evaluation The evaluations were primarily based on the Basic Travel Expression Corpus (BTEC) which contains conversations in tourism-related activities. The corpus was originally created in Japanese and English by ATR [15] and was later extended to other languages. We participated in the supplied data track for the translation directions Arabic-English, Chinese-English, Japanese-English and Korean-English. For ChineseEnglish direction we also worked on ASR output. In both unrestricted and C-STAR tracks, we participated for Chinese-English direction. For each translation direction, except KoreanEnglish, two development sets (C-STAR’03 and Supplied Data Track Chinese Japanese Korean Manual ASR 20,000 176,199 198,453 208,763 8,687 9,277 9,132 506 3,511 2,835 4,130 4,084 913 1,024 920 976 117 245 70 95 500 3,590 2,"
2005.iwslt-1.6,2005.mtsummit-papers.33,1,0.835754,"ecoder that combines the translation model, language model, and other models to generate the complete translation. When translating speech recognition output, we integrate multiple translation hypotheses into a single structure and then derive the best hypothesis. This approach is described in section 5. Finally, in section 6 we give an overview of the data and tasks and present the results of the experiments we carried out for different data conditions. 2. Phrase Alignment In this evaluation, we applied a variation of the alignment-free approach, which is an extension to the previous work in [5] and [6] to extract bilingual phrase pairs for the supplied data tracks. In this extension, we used eleven feature functions including phrase level fertilities and phrase level IBM Model-1 probabilities aiming to locate the phrase pairs from the parallel sentences. The feature functions are then combined in a log-linear model as follows: PM exp( m=1 λm φm (X, e, f )) P (X|e, f )= P PM 0 {X 0 } exp( m=1 λm φm (X ,e,f )) where X→(fjj+l , ei+k ) corresponds to a phrase-pair cani didate extracted from a given sentence-pair (e, f ); φm is a feature function designed to be informative for phrase ext"
2005.iwslt-1.6,W05-0825,1,0.830118,"hat combines the translation model, language model, and other models to generate the complete translation. When translating speech recognition output, we integrate multiple translation hypotheses into a single structure and then derive the best hypothesis. This approach is described in section 5. Finally, in section 6 we give an overview of the data and tasks and present the results of the experiments we carried out for different data conditions. 2. Phrase Alignment In this evaluation, we applied a variation of the alignment-free approach, which is an extension to the previous work in [5] and [6] to extract bilingual phrase pairs for the supplied data tracks. In this extension, we used eleven feature functions including phrase level fertilities and phrase level IBM Model-1 probabilities aiming to locate the phrase pairs from the parallel sentences. The feature functions are then combined in a log-linear model as follows: PM exp( m=1 λm φm (X, e, f )) P (X|e, f )= P PM 0 {X 0 } exp( m=1 λm φm (X ,e,f )) where X→(fjj+l , ei+k ) corresponds to a phrase-pair cani didate extracted from a given sentence-pair (e, f ); φm is a feature function designed to be informative for phrase extraction."
2005.iwslt-1.6,I05-3011,1,0.902,"nsion, we used eleven feature functions including phrase level fertilities and phrase level IBM Model-1 probabilities aiming to locate the phrase pairs from the parallel sentences. The feature functions are then combined in a log-linear model as follows: PM exp( m=1 λm φm (X, e, f )) P (X|e, f )= P PM 0 {X 0 } exp( m=1 λm φm (X ,e,f )) where X→(fjj+l , ei+k ) corresponds to a phrase-pair cani didate extracted from a given sentence-pair (e, f ); φm is a feature function designed to be informative for phrase extraction. Feature function weights {λm }, are the same as in our previous experiments [7]. This log-linear model serves as a performance measure function in a local search. The search starts from fetching a test-set specific source phrase (e.g. Chinese ngram); it localizes the candidate ngram’s center in the English sentence; and then around the projected center, it finds out all the candidate phrase pairs ranked with the log-linear model scores. In the local search, down-hill moves are allowed so that functional words can be attached to the left or right boundaries of the candidate phrase-pairs. The eleven (M =11) feature functions that compute different aspects of phrase pair (f"
2005.iwslt-1.6,J03-1002,0,0.00619115,"1 scores for the phrase-pairs P (fjj+l |eii+k ) and P (ei+k |fjj+l ); i the remaining parts of (e, f ) excluding the phrasepair is modeled by P (fj 0 ∈[j,j+l] |ei0 ∈[i,i+k] ) and / / 0 P (ei0 ∈[i,i+k] |f ) using the translation lex/ j ∈[j,j+l] / icons of P (f |e) and P (e|f ). • Another two of the scores aim to bracket the sentence pair with the phrase-pair as detailed in [7]. • The last function computes the average word alignment links per source word in the candidate phrasepair. We assume each phrase-pair should contain at least one word alignment link. We train the IBM Model-4 with GIZA++ [8] in both directions and grow the intersection with word pairs in the union to collect the word alignment. Because of the last feature-function, our approach is no longer truly “alignment-free”. More details of the log-linear model and experimental analysis of the feature-functions are given in [7]. To use the extracted phrase-pairs in the decoder, a set of eight scores for each phrase-pair are computed: relative frequency of both directions, phraselevel fertility scores for both directions computed via dynamic programming, the standard IBM Model-1 scores for P both directions (i.e. P (fjj+l |e"
2005.iwslt-1.6,W05-0829,1,0.826281,"ecoder a scaling factor can be used to modify the contribution of this model to the overall score. Varying this scaling factors can change the performance of the system considerable. Minimum error training is used to find a good set of scaling factors. In the following sub-sections, these different steps will be described in some more detail. 4.1. Building Translation Lattice The CMU SMT decoder can use phrase tables, generated at training time, but can also do just-in-time phrase alignment. This means that the entire bilingual corpus is loaded and the source side indexed using a suffix array [10]. For all ngrams in the test sentence, occurrences in the corpus are located using the suffix array. For a number of occurrences, where the number can be given as a parameter to the decoder, phrase alignment as described in section 2 is performed and the found target phrase added to the translation lattice. If phrase translations have already been collected during training time, then this phrase table is loaded into the decoder and a prefix tree constructed over the source phrases. This allows for an efficient search to find all source phrases in the phrase table which match a sequence of word"
2005.iwslt-1.6,P02-1040,0,0.0779885,"1 2,835 4,130 4,084 913 1,024 920 976 117 245 70 95 500 3,590 2,896 4,131 975 1,068 945 116 223 61 506 3,743 3,003 4,226 4,563 963 1,091 975 969 155 249 169 84 English 183,452 6,956 - IWSLT’04) were made available. For Korean-English only C-STAR’03 test set was available. Table 1 shows corpus statistics for the training and test sets. As a preprocessing step, we separated punctuations from words in the English (target) side and converted the text into lowercase. No preprocessing was done on any of the source side data. We report translation results using the well known evaluation metrics BLEU [16] and NIST [17]. For our primary system and the best system, we report results also in WER, PER, METEOR [18] and GTM [19]. 6.1. Supplied Data Track During the evaluation our primary focus was on the Chinese-English direction. We applied both PESA and Alignment-Free phrase extraction methods to the supplied data track. In building phrase tables using the Alignment Free method, we extracted phrase-pairs with source side up to 8-gram in length. PESA online phrase extraction method can extract phrases up to full length of the sentence. Table 2 summarizes the official translation results for our pri"
2005.iwslt-1.6,W05-0909,0,0.0223458,"3 3,003 4,226 4,563 963 1,091 975 969 155 249 169 84 English 183,452 6,956 - IWSLT’04) were made available. For Korean-English only C-STAR’03 test set was available. Table 1 shows corpus statistics for the training and test sets. As a preprocessing step, we separated punctuations from words in the English (target) side and converted the text into lowercase. No preprocessing was done on any of the source side data. We report translation results using the well known evaluation metrics BLEU [16] and NIST [17]. For our primary system and the best system, we report results also in WER, PER, METEOR [18] and GTM [19]. 6.1. Supplied Data Track During the evaluation our primary focus was on the Chinese-English direction. We applied both PESA and Alignment-Free phrase extraction methods to the supplied data track. In building phrase tables using the Alignment Free method, we extracted phrase-pairs with source side up to 8-gram in length. PESA online phrase extraction method can extract phrases up to full length of the sentence. Table 2 summarizes the official translation results for our primary submissions. We also give contrastive results for the Arabic-English and Chinese-English directions in"
2005.iwslt-1.6,2003.mtsummit-papers.51,0,0.0310678,"4,563 963 1,091 975 969 155 249 169 84 English 183,452 6,956 - IWSLT’04) were made available. For Korean-English only C-STAR’03 test set was available. Table 1 shows corpus statistics for the training and test sets. As a preprocessing step, we separated punctuations from words in the English (target) side and converted the text into lowercase. No preprocessing was done on any of the source side data. We report translation results using the well known evaluation metrics BLEU [16] and NIST [17]. For our primary system and the best system, we report results also in WER, PER, METEOR [18] and GTM [19]. 6.1. Supplied Data Track During the evaluation our primary focus was on the Chinese-English direction. We applied both PESA and Alignment-Free phrase extraction methods to the supplied data track. In building phrase tables using the Alignment Free method, we extracted phrase-pairs with source side up to 8-gram in length. PESA online phrase extraction method can extract phrases up to full length of the sentence. Table 2 summarizes the official translation results for our primary submissions. We also give contrastive results for the Arabic-English and Chinese-English directions in Table 3. The"
2005.iwslt-1.6,2005.mtsummit-papers.30,1,\N,Missing
2005.iwslt-1.6,W06-1626,1,\N,Missing
2005.iwslt-1.6,2005.iwslt-1.16,0,\N,Missing
2005.iwslt-1.6,2005.eamt-1.19,1,\N,Missing
2005.iwslt-1.7,J93-2003,0,0.00423534,"t the time we select the actual training data. Statistical machine translation can be described in a formal way as follows: t * = arg max P (t |s) = arg max P( s |t ) ⋅ P (t ) t t Here t is the target sentence, and s is the source sentence. P(t) is the target language model and P(s|t) is the translation model used in the decoder. Statistical machine translation searches for the best target sentence from the space defined by the target language model and the translation model. Statistical translation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM ([1], [2], [3]). Some recent developments focused on online phrase extraction ([4], [5]). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation (SMT) is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with fewer resources ([6], [7]). To improve the data situation for these languages it is necessary to hire human translators at enormo"
2005.iwslt-1.7,C96-2141,1,0.561503,"time we select the actual training data. Statistical machine translation can be described in a formal way as follows: t * = arg max P (t |s) = arg max P( s |t ) ⋅ P (t ) t t Here t is the target sentence, and s is the source sentence. P(t) is the target language model and P(s|t) is the translation model used in the decoder. Statistical machine translation searches for the best target sentence from the space defined by the target language model and the translation model. Statistical translation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM ([1], [2], [3]). Some recent developments focused on online phrase extraction ([4], [5]). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation (SMT) is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with fewer resources ([6], [7]). To improve the data situation for these languages it is necessary to hire human translators at enormous co"
2005.iwslt-1.7,P05-1032,0,0.147595,"can be described in a formal way as follows: t * = arg max P (t |s) = arg max P( s |t ) ⋅ P (t ) t t Here t is the target sentence, and s is the source sentence. P(t) is the target language model and P(s|t) is the translation model used in the decoder. Statistical machine translation searches for the best target sentence from the space defined by the target language model and the translation model. Statistical translation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM ([1], [2], [3]). Some recent developments focused on online phrase extraction ([4], [5]). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation (SMT) is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with fewer resources ([6], [7]). To improve the data situation for these languages it is necessary to hire human translators at enormous costs who translate corpora that can later be used to train SMT systems. Ou"
2005.iwslt-1.7,2005.eamt-1.39,1,0.90465,"be described in a formal way as follows: t * = arg max P (t |s) = arg max P( s |t ) ⋅ P (t ) t t Here t is the target sentence, and s is the source sentence. P(t) is the target language model and P(s|t) is the translation model used in the decoder. Statistical machine translation searches for the best target sentence from the space defined by the target language model and the translation model. Statistical translation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM ([1], [2], [3]). Some recent developments focused on online phrase extraction ([4], [5]). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation (SMT) is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with fewer resources ([6], [7]). To improve the data situation for these languages it is necessary to hire human translators at enormous costs who translate corpora that can later be used to train SMT systems. Our ide"
2005.iwslt-1.7,mcenery-etal-2000-corpus,0,0.160881,"l translation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM ([1], [2], [3]). Some recent developments focused on online phrase extraction ([4], [5]). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation (SMT) is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with fewer resources ([6], [7]). To improve the data situation for these languages it is necessary to hire human translators at enormous costs who translate corpora that can later be used to train SMT systems. Our idea focuses on sorting the available source sentences that should be translated by a human translator according to their approximate importance. The importance is estimated using a frequency based and an information retrieval approach. 2. Motivation There are three inherently different motivations for the goal of limiting the amount of necessary training data for a competitive translation system. We describ"
2005.iwslt-1.7,2004.eamt-1.14,1,0.922835,"nslation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM ([1], [2], [3]). Some recent developments focused on online phrase extraction ([4], [5]). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation (SMT) is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with fewer resources ([6], [7]). To improve the data situation for these languages it is necessary to hire human translators at enormous costs who translate corpora that can later be used to train SMT systems. Our idea focuses on sorting the available source sentences that should be translated by a human translator according to their approximate importance. The importance is estimated using a frequency based and an information retrieval approach. 2. Motivation There are three inherently different motivations for the goal of limiting the amount of necessary training data for a competitive translation system. We described th"
2005.iwslt-1.7,2005.mtsummit-papers.30,1,0.0889638,"ecessary to hire human translators at enormous costs who translate corpora that can later be used to train SMT systems. Our idea focuses on sorting the available source sentences that should be translated by a human translator according to their approximate importance. The importance is estimated using a frequency based and an information retrieval approach. 2. Motivation There are three inherently different motivations for the goal of limiting the amount of necessary training data for a competitive translation system. We described those motivations and their applications already in the paper [8]. Application 1: Reducing Human Translation Cost The main problem of portability of SMT systems to new languages is the involved cost to generate parallel bilingual training data as it is necessary to have sentences translated by human translators. An assumption could be that a 1 million word corpus needs to be translated to a new language in order to build a decent SMT system. A human translator could charge in the range of approximately 0.10-0.25 USD per word depending on the involved languages and the difficulty of the text. The translation of a 1 million word corpus would then cost between"
2005.iwslt-1.7,J04-3001,0,0.0607757,"able to hold them in memory. (The last issue will certainly be resolved by the widespread introduction of 64 bit machines which can theoretically address 17 million terabytes of memory.) 3. Previous Work This research can generally be regarded as an example of active learning. This means the machine learning algorithm does not just passively train on the available training data but plays an active role in selecting the best training data. Active learning, as a standard method in machine learning, has been applied to a variety of problems in natural language processing, for example to parsing ([9]) and to automatic speech recognition ([10]). It is important to note the difference between this approach and approaches to Translation Model Adaptation ([11]) or simple subsampling techniques that are based on the actual test data. Here we assume that the test data is not known at selection time so the intention is to get the best possible translation system for every possible test data. Our previous work in this area focused on improving the ngram (type-) coverage by selecting the sentences based on the number of previously unseen n-grams they contain [8]. Section 4.2 will give a short over"
2005.iwslt-1.7,2005.eamt-1.19,1,0.842734,"illion terabytes of memory.) 3. Previous Work This research can generally be regarded as an example of active learning. This means the machine learning algorithm does not just passively train on the available training data but plays an active role in selecting the best training data. Active learning, as a standard method in machine learning, has been applied to a variety of problems in natural language processing, for example to parsing ([9]) and to automatic speech recognition ([10]). It is important to note the difference between this approach and approaches to Translation Model Adaptation ([11]) or simple subsampling techniques that are based on the actual test data. Here we assume that the test data is not known at selection time so the intention is to get the best possible translation system for every possible test data. Our previous work in this area focused on improving the ngram (type-) coverage by selecting the sentences based on the number of previously unseen n-grams they contain [8]. Section 4.2 will give a short overview over our previous best method. 4. Description of sentence sorting 4.1. Algorithm The sentences are sorted according to the following very simple algorithm"
2005.iwslt-1.7,takezawa-etal-2002-toward,0,0.0243937,"easily be generalized to n-grams by using every ngram as an entry in the document vectors (instead of only using words). We tried this for n-grams up to bigrams and plan on doing experiments with higher n-grams. The following section 5 will give an overview over the experiments that were done using the three presented approaches to sort sentences according to their estimated importance. 5. Experiments English-Spanish Baseline The full training data for the translation experiments consisted of 123,416 English sentences with 903,525 English words (tokens). This data is part of the BTEC corpus ([12]) with relatively simple sentences from the travel domain. The whole training data was also available in Spanish (852,362 words). The testing data which was used to measure the machine translation performance consisted of 500 lines of data from the medical domain. All translations in this task were done translating English to Spanish. 5.2. Machine Translation System The applied statistical machine translation system uses an online phrase extraction algorithm based on IBM1 lexicon probabilities ([3], [13]). The language model is a trigram language model with Kneser-Ney-discounting built with th"
2005.iwslt-1.7,2004.iwslt-evaluation.11,1,0.807644,"glish sentences with 903,525 English words (tokens). This data is part of the BTEC corpus ([12]) with relatively simple sentences from the travel domain. The whole training data was also available in Spanish (852,362 words). The testing data which was used to measure the machine translation performance consisted of 500 lines of data from the medical domain. All translations in this task were done translating English to Spanish. 5.2. Machine Translation System The applied statistical machine translation system uses an online phrase extraction algorithm based on IBM1 lexicon probabilities ([3], [13]). The language model is a trigram language model with Kneser-Ney-discounting built with the SRI-Toolkit ([14]) using only the Spanish part of the training data. We applied the standard metrics introduced for machine translation, NIST ([15]) and BLEU ([16]). NIST score 5.1. Test and Training Data Previous best 4.4 4.2 4.0 3.8 3.6 3.4 3.2 3.0 2.8 2.6 2.4 2.2 2.0 1.8 1.6 0 200000 400000 600000 800000 translated words Diagram 1: NIST scores for Baseline and Previous best The picture is similar for the BLEU scores. The previous best selection reached a BLEU score of 0.13 at 400,000 translated word"
2005.iwslt-1.7,P02-1040,0,0.0815562,"to measure the machine translation performance consisted of 500 lines of data from the medical domain. All translations in this task were done translating English to Spanish. 5.2. Machine Translation System The applied statistical machine translation system uses an online phrase extraction algorithm based on IBM1 lexicon probabilities ([3], [13]). The language model is a trigram language model with Kneser-Ney-discounting built with the SRI-Toolkit ([14]) using only the Spanish part of the training data. We applied the standard metrics introduced for machine translation, NIST ([15]) and BLEU ([16]). NIST score 5.1. Test and Training Data Previous best 4.4 4.2 4.0 3.8 3.6 3.4 3.2 3.0 2.8 2.6 2.4 2.2 2.0 1.8 1.6 0 200000 400000 600000 800000 translated words Diagram 1: NIST scores for Baseline and Previous best The picture is similar for the BLEU scores. The previous best selection reached a BLEU score of 0.13 at 400,000 translated words. The reason for the necessity to translate more words to reach a BLEU score in the confidence interval of the final system could be that the BLEU score puts higher importance on fluency. Larger systems might benefit from more robust estimations of the la"
2005.iwslt-1.7,W01-1409,0,0.0433966,".02 2.63 2.57 2.40 3.05 3.00 3.02 3.00 2.05 3.25 3.29 2.98 3.30 3.27 2.90 2.82 2.58 3.56 3.31 3.49 3.50 3.40 3.63 3.63 3.36 3.65 3.62 3.23 3.19 3.34 3.81 3.42 3.72 3.71 3.55 3.86 3.85 3.57 3.80 3.77 3.53 3.50 650k 170k 380k 230k 240k 410k 140k 190k 400k 180k 220k 360k 370k 850k 220k 760k 300k 320k 450k 300k 280k 450k 220k 270k 390k 430k Table 1: Performance Overview One might argue that improvements at very small data sizes are not relevant, as the translations will still be very deficient. This might be the case, but there are applications where even a low-quality translation can be helpful ([17]). And as we showed in [8] - some translations are surprisingly good, even for very small amounts of training data. 6. Future Work The presented weighting schemes could certainly incorporate other features of the original training data. The pure frequency based approach “tries” to cover every n gram once and then does not consider it anymore. It might be helpful to have a goal of covering every n-gram a number of times to get better estimates of translation probabilities. The TF-IDF based sorting did not yet show improvements over the earlier approaches. We hope that it will be beneficial to f"
2005.mtsummit-papers.30,takezawa-etal-2002-toward,0,0.0872405,"nces are more difficult for the training of statistical translation models. (When training the translation model IBM1, for example, every possible word alignment between sentences is considered.) 5 5.1 To fix these shortcomings, we changed the weighting terms to incorporate the actual length of a sentence by dividing the number of unseen n-grams by the length of the sentence (in words): Experiments English-Spanish Test and Training Data The training data for the first translations consisted of 123,416 English sentences with 903,525 English words (tokens). This data is part of the BTEC corpus (Takezawa et al., 2002) with relatively simple sentences from the travel domain. The whole training data was also available in Spanish (852,362 words). The testing data which was used to measure the machine translation performance consisted of 500 lines of data from the medical domain. All translations in this task were done translating English to Spanish. j weight j ( sentence) = ∑ # (unseen n − grams) ∑ # (unseen n − grams) n =1 sentence This changes the weight to – informally speaking – “new n-grams per word to translate."" As noted earlier, the algorithms for training translation models in statistical machine tra"
2005.mtsummit-papers.30,2004.iwslt-evaluation.11,1,0.910197,"d on N-gram Coverage Matthias Eck, Stephan Vogel and Alex Waibel Interactive Systems Laboratories Carnegie Mellon University Pittsburgh, PA, 15213, USA matteck@cs.cmu.edu, vogel+@cs.cmu.edu, waibel@cs.cmu.edu Statistical machine translation searches for the best target sentence from the space defined by the target language model and the translation model. Statistical translation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM (Brown et al., 1993; Vogel et al., 1996; Vogel et al., 2003). Some recent developments focused on online phrase extraction (Vogel et al., 2004). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with less resources. To improve the data situation for these languages, it is necessary to hire human translators at enormous costs who translate corpora that can later be used to train statistical machine translation system"
2005.mtsummit-papers.30,C96-2141,1,0.68059,"Missing"
2005.mtsummit-papers.30,J93-2003,0,0.0094299,"Missing"
2005.mtsummit-papers.30,P05-1032,0,0.0461345,"Missing"
2005.mtsummit-papers.30,W01-1409,0,0.043825,"o term weight0,j. We notice the same lower scores after about translating 200,000 words and a score recovery towards the end. The optimizations based on uni- and bigrams and uni-, bi- and trigrams are clearly improved compared to weight0,j. We also do not see any significant differences between the optimization based on uni- plus bigrams and the optimization incorporating These problems are clearly fixed by incorporating the bi- and trigrams into the optimization process. The scores no longer fall 231 be the case, but there are applications where even a low quality translation can be helpful (Germann, 2001). Besides that, some translations are surprisingly good and show considerable improvements over the baseline system (Table 1). The first sentence especially demonstrates the improved coverage; here for the words “heart”, “beating” and “normally”. The word “beating” is unfortunately not correctly translated but the final result is still much better than the translation of the baseline system. trigrams, too. The performance is very similar with slight advantages for the optimization based on uni- and bigrams only. In this case a NIST score of 4.0 was already reached at 170,000 translated words w"
2005.mtsummit-papers.30,2005.eamt-1.19,1,0.848924,"y et al., 2000). 2.2 3 In general this research can be regarded as an example of active learning. This means the machine learning algorithm does not just passively train on the available training data but plays an active role in selecting the best training data. Active learning, as a standard method in machine learning, has been applied to a variety of problems in natural language processing, for example to parsing (Hwa, 2004) and to automatic speech recognition (Kamm and Meyer, 2002). It is important to note the difference between this approach and approaches to Translation Model Adaptation (Hildebrand et al., 2005) or simple subsampling techniques that are based on the actual test data. Here, we assume that the test data is not known at selection time, so the intention is to get the best possible translation system for every possible test data. Application 2: Translation on Small Devices Another possible application is the usage of statistical machine translation on portable small devices like PDAs or cell phones. Those devices tend to have a limited amount of memory available which limits the size of the models the device can actually hold and a larger training corpus will usually result in a larger mo"
2005.mtsummit-papers.30,J04-3001,0,0.0636434,"nslation effort, a considerable amount of money could be saved. This could especially be applied to low density languages with limited resources (compare Lavie et al., 2004; McEnery et al., 2000). 2.2 3 In general this research can be regarded as an example of active learning. This means the machine learning algorithm does not just passively train on the available training data but plays an active role in selecting the best training data. Active learning, as a standard method in machine learning, has been applied to a variety of problems in natural language processing, for example to parsing (Hwa, 2004) and to automatic speech recognition (Kamm and Meyer, 2002). It is important to note the difference between this approach and approaches to Translation Model Adaptation (Hildebrand et al., 2005) or simple subsampling techniques that are based on the actual test data. Here, we assume that the test data is not known at selection time, so the intention is to get the best possible translation system for every possible test data. Application 2: Translation on Small Devices Another possible application is the usage of statistical machine translation on portable small devices like PDAs or cell phones"
2005.mtsummit-papers.30,2004.eamt-1.14,1,0.843763,"ately 0.10-0.25 USD per word depending on the involved languages and the difficulty of the text. The translation of a 1 million word corpus would then cost between 100,000 and 250,000 USD. The concept here is to select the most important sentences from the original 1 million word corpus and have only those translated by the human translators. If it would still be possible to get a similar translation performance with a significantly lower translation effort, a considerable amount of money could be saved. This could especially be applied to low density languages with limited resources (compare Lavie et al., 2004; McEnery et al., 2000). 2.2 3 In general this research can be regarded as an example of active learning. This means the machine learning algorithm does not just passively train on the available training data but plays an active role in selecting the best training data. Active learning, as a standard method in machine learning, has been applied to a variety of problems in natural language processing, for example to parsing (Hwa, 2004) and to automatic speech recognition (Kamm and Meyer, 2002). It is important to note the difference between this approach and approaches to Translation Model Adap"
2005.mtsummit-papers.30,mcenery-etal-2000-corpus,0,0.233194,"Missing"
2005.mtsummit-papers.30,P02-1040,0,0.071487,"Missing"
2005.mtsummit-papers.30,2005.eamt-1.39,1,\N,Missing
2005.mtsummit-papers.33,J93-2003,0,0.0558831,"alignment models. Phrase alignment is viewed as a sentence splitting task. For a given spitting of the source sentence (source phrase, left segment, right segment) find a splitting for the target sentence, which optimizes the overall sentence alignment probability. Experiments on different translation tasks show that this phrase alignment method leads to highly competitive translation results. 1 Introduction Statistical machine translation (SMT) is currently the most promising approach to large vocabulary text translation. In the spirit of the Candide system developed in the early 90s at IBM (Brown et al., 1993), a number of statistical machine translation systems have been presented in the last few years (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamada and Knight, 2000). These systems share the basic underlying principles of applying a translation model to capture the lexical and word reordering relationships between two languages, complemented by a target language model to drive the search process through translation model hypotheses. The primary differences among systems lie in the structure of their translation models. Whereas the original IBM system was based on purely word-based translatio"
2005.mtsummit-papers.33,W02-1018,0,0.0698847,"Missing"
2005.mtsummit-papers.33,P00-1056,0,0.53712,"egment, right segment) find a splitting for the target sentence, which optimizes the overall sentence alignment probability. Experiments on different translation tasks show that this phrase alignment method leads to highly competitive translation results. 1 Introduction Statistical machine translation (SMT) is currently the most promising approach to large vocabulary text translation. In the spirit of the Candide system developed in the early 90s at IBM (Brown et al., 1993), a number of statistical machine translation systems have been presented in the last few years (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamada and Knight, 2000). These systems share the basic underlying principles of applying a translation model to capture the lexical and word reordering relationships between two languages, complemented by a target language model to drive the search process through translation model hypotheses. The primary differences among systems lie in the structure of their translation models. Whereas the original IBM system was based on purely word-based translation models, modern systems try to incorporate more complex structure. Most state of the art data-driven translation systems use phrase transla"
2005.mtsummit-papers.33,2004.iwslt-evaluation.11,1,0.842704,"as based on purely word-based translation models, modern systems try to incorporate more complex structure. Most state of the art data-driven translation systems use phrase translations as the primary 251 building blocks to capture local context information, leading to better lexical choice and more reliable local reordering. The quality of the translations is largely dependent on the quality of phrase pairs extracted from bilingual corpora. Different phrase alignment methods have been developed. Most of them rely on word-to-word alignment. A short introduction will be given in Section 2. In (Vogel 2004) a new phrase alignment algorithm was introduced, which is not based on calculating a Viterbi alignment. In this paper a detailed study of this approach, including several extensions, will be presented. 2 Extracting Phrase Translations from Bilingual Corpora A simple approach to extract phrase translations from a bilingual corpus is to harvest the Viterbi path generated by a word alignment model. A number of probabilistic word alignment models have been proposed (Brown et al., 1993) (Vogel et al., 1996) (Och and Ney, 2000) and shown to be effective for statistical machine translation. Phrase a"
2006.eamt-1.12,W05-0909,0,0.182464,"Missing"
2006.eamt-1.12,W05-0900,0,0.214133,"Missing"
2006.eamt-1.12,niessen-etal-2000-evaluation,0,0.194064,"Missing"
2006.eamt-1.12,P02-1040,0,0.0945405,"Missing"
2006.eamt-1.12,2004.tmi-1.9,1,0.83745,"Missing"
2006.eamt-1.12,2005.iwslt-1.1,1,\N,Missing
2006.eamt-1.12,2004.iwslt-evaluation.1,0,\N,Missing
2006.iwslt-evaluation.19,P02-1040,0,0.0877239,"irections, phrase-level fertility scores in both directions (via DP as in section 3.1.3), The IBM-1 Viterbi scores in both directions, un-normalized scores in both directions Q IBM-1 lexicon Q P (fjj+l |eii+k ) = j ′ ∈[j,j+l] i′ ∈[i,i+k] P (fj ′ |ei′ ) (favoring long phrase-pairs, see [2]), the phrase-level normalized frequency, and the normalized number of alignment links within the phrase-pair (as in section 3.1.3). 4. Translation Results This section gives an overview of all official and contrastive results achieved by the UKA/CMU translation systems. All results will only be given in BLEU([9]) and NIST([10]) scores according to the official evaluation specifications (mixed case with punctuation marks). For other scores for the submitted systems please refer to the official scoring publication of IWSLT 2006. Submissions were done for all language pairs in the Open and C-STAR data conditions. We always translated ASR output (as 1-best) and as a comparison the correct recognition results (CRR). The development set numbers always refer to the development set that was also provided for the evaluation campaign for IWSLT 2006. 4.1. Data Conditions For each language pair 20,000 or 40,000"
2006.iwslt-evaluation.19,2005.mtsummit-papers.33,1,0.840167,"lation system that was used is basically the same in both places. It has to be seen as a combined effort of a close-knit research group [1],[2]. 3. Translation System 3.1. Phrase Alignment 3.1.1. Overview We used two phrase alignment methods for IWSLT 2006, namely PESA and LogLin. The LogLin phrase extraction method is computationally intensive so it was not used for all submitted systems. More extensive experiments with contrastive systems showed that LogLin generally improves the results compared to the PESA phrase extraction method. 3.1.2. PESA: Phrase pair extraction as sentence splitting [3] The PESA phrase extraction method is based on the well known IBM-1 word alignment model [4]. The IBM-1 model assigns a probability to all possible word alignments of respective sentences in the training data. Assuming a sentence in the bilingual corpus contains a phrase from a source sentence eii21 = ei1 ...ei2 we are inter130 ested in the sequence of words fjj12 = fj1 ...fj2 from the respective target sentence that is the optimal translation for this source phrase. We can now estimate the quality of a translation candidate by using the IBM-1 word alignment probabilities between the source an"
2006.iwslt-evaluation.19,J93-2003,0,0.0164133,"bined effort of a close-knit research group [1],[2]. 3. Translation System 3.1. Phrase Alignment 3.1.1. Overview We used two phrase alignment methods for IWSLT 2006, namely PESA and LogLin. The LogLin phrase extraction method is computationally intensive so it was not used for all submitted systems. More extensive experiments with contrastive systems showed that LogLin generally improves the results compared to the PESA phrase extraction method. 3.1.2. PESA: Phrase pair extraction as sentence splitting [3] The PESA phrase extraction method is based on the well known IBM-1 word alignment model [4]. The IBM-1 model assigns a probability to all possible word alignments of respective sentences in the training data. Assuming a sentence in the bilingual corpus contains a phrase from a source sentence eii21 = ei1 ...ei2 we are inter130 ested in the sequence of words fjj12 = fj1 ...fj2 from the respective target sentence that is the optimal translation for this source phrase. We can now estimate the quality of a translation candidate by using the IBM-1 word alignment probabilities between the source and target phrases. If the candidate is actually a good translation of the source phrase we ex"
2006.iwslt-evaluation.19,W05-0825,1,0.839004,"j1 and j2 we can determine the optimal sentence splitting and the best translation candidate. The same ideas can be applied if we use the IBM-1 probabilities for the reverse direction thus calculating pj1 ,j2 (f |e) and we interpolate the two phrase alignment probabilities to get the optimal translation candidate. In the actual system we not only use the top translation candidate but all candidates to a certain threshold. This covers translation alternatives and leaves the final decision to other models, mainly the language model. 3.1.3. LogLin: Phrase pair extraction with Log-Linear Features [5],[6] Another phrase extraction method applied was LogLin. LogLin formulates the phrase-extraction problem as a local search, guided by a simple heuristic function. Given the source n-gram ei+k (span from position i to i position i+k, with a length of k+1), the local search starts by first localizing the projected center of the target phrase, and then it searches the best scored width, corresponding to the left- and right- boundaries (j, j+l) of the target phrase: fjj+l . Previously the heuristic function included phrase-level fertility score, a simple IBM-1 lexicon score, and a phraselevel pos"
2006.iwslt-evaluation.19,2005.eamt-1.19,1,0.837963,"Full BTEC + Travel Books + 4xSupplied Data Scores (BLEU & NIST) ASR spont. 0.1622 5.1865 ASR read 0.1645 5.2372 CRR 0.2057 6.0548 8.2301 6.6064 7.6730 8.5923 7.0812 8.1408 Table 7: Contrastive Results for Italian → English 4.5. Chinese → English 4.5.1. Training Corpora For Chinese → English 40,000 lines of data were supplied. The Full BTEC data for this language pair is complete at about 160,000 sentences. In addition we added 106,826 lines of data that was gathered from freely available bilingual newswire data using the test set of IWSLT 2005 as a query. We applied the technique described in [12]. Also the monolingual travel books were added to the language model for the C-STAR data track. Data Condition Supplied Full BTEC IR data Travel books #lines 39,953 163,326 106,826 31,388 #wordsChinese/English 351,060/306,149 1,008,568/954,591 1,838,597/1,871,748 English: 255,534 Table 9: Official submissions Chinese → English 4.5.4. Contrastive Results For the contrastive results we first investigated the impact of the LogLin phrase alignment model. The results in table 10 clearly show that for the Supplied Data and BTEC data the LogLin model shows consistent improvements, especially regardin"
2006.iwslt-evaluation.19,2005.mtsummit-papers.30,1,0.831989,"131 0.2434 0.1830 0.2009 5.7787 6.4009 5.5749 5.6201 Table 15: Contrastive Results for Japanese → English Full BTEC data PESA Dev Set (ASR) 0.1938 5.5076 Dev Set (CRR) 0.2222 6.2152 Test Set (ASR) 0.1850 5.4349 Test Set (CRR) 0.2045 5.8719 Full BTEC + any available data (C-STAR track) PESA Dev Set (ASR) 0.2001 5.5914 Dev Set (CRR) 0.2289 5.3980 Test Set (ASR) 0.1841 5.3980 Test Set (CRR) 0.2007 5.8584 Table 16: Contrastive Results for Japanese → English “informative” and not to contain too much repetition given the already available translations. This was done using the technique presented in [14]. The 55,000 lines of Italian → English data were produced using a similar method. This means that the bare number of lines underestimates the impact of these additional translations in comparison to the impact the Full BTEC corpus of 160,000 lines would have. This means that the relative increase in training data (especially useful training data) is probably larger for the Italian → English and Arabic → English systems than for the Japanese → English and Chinese → English systems. Concerning the two investigated phrase extraction methods PESA and LogLin we notice the general trend that LogLin"
2006.iwslt-evaluation.19,I05-3011,1,0.762827,"nd j2 we can determine the optimal sentence splitting and the best translation candidate. The same ideas can be applied if we use the IBM-1 probabilities for the reverse direction thus calculating pj1 ,j2 (f |e) and we interpolate the two phrase alignment probabilities to get the optimal translation candidate. In the actual system we not only use the top translation candidate but all candidates to a certain threshold. This covers translation alternatives and leaves the final decision to other models, mainly the language model. 3.1.3. LogLin: Phrase pair extraction with Log-Linear Features [5],[6] Another phrase extraction method applied was LogLin. LogLin formulates the phrase-extraction problem as a local search, guided by a simple heuristic function. Given the source n-gram ei+k (span from position i to i position i+k, with a length of k+1), the local search starts by first localizing the projected center of the target phrase, and then it searches the best scored width, corresponding to the left- and right- boundaries (j, j+l) of the target phrase: fjj+l . Previously the heuristic function included phrase-level fertility score, a simple IBM-1 lexicon score, and a phraselevel positio"
2006.iwslt-evaluation.20,P05-1033,0,0.0890792,"x Augmented Machine Translation System that was used in the IWSLT-06 evaluation campaign. We participated in the C-Star data track using only the Full BTEC corpus, for Chinese-English translation, focusing on transcript translation. We applied techniques that produce true-cased, punctuated translations from non-punctuated Chinese transcripts, generating translations which score higher against the Official metric than against the lower-cased, punctuation removed metric. Our results demonstrate the impact of syntax and hierarchy based models for speech transcript translation. 1. Introduction As [1] and [2] note, phrase-based models suffer from sparse data effects when required to translate conceptual elements that span or skip across several words, and distortion based re-ordering techniques tend to limit their range of operation for reasons of efficiency and model strength [3]. Syntax driven [4], [5] and hierarchical translation models [1] model structured re-ordering constraints and extend the domain of locality in the decoding process. Systems such as [6] and [7] demonstrate effective decoding using these models. For the IWSLT-06 evaluation, we applied syntax augmented translation as"
2006.iwslt-evaluation.20,P04-1083,0,0.0414364,"non-punctuated Chinese transcripts, generating translations which score higher against the Official metric than against the lower-cased, punctuation removed metric. Our results demonstrate the impact of syntax and hierarchy based models for speech transcript translation. 1. Introduction As [1] and [2] note, phrase-based models suffer from sparse data effects when required to translate conceptual elements that span or skip across several words, and distortion based re-ordering techniques tend to limit their range of operation for reasons of efficiency and model strength [3]. Syntax driven [4], [5] and hierarchical translation models [1] model structured re-ordering constraints and extend the domain of locality in the decoding process. Systems such as [6] and [7] demonstrate effective decoding using these models. For the IWSLT-06 evaluation, we applied syntax augmented translation as per [7] to the speech translation task, using the only Full BTEC corpus to model translational equivalence. We begin by extracting lexical phrases as per [2] as the basis for our syntax augmented translation rules. We annotate and generalize these phrases by parsing the target side of the training data with"
2006.iwslt-evaluation.20,W06-1606,0,0.0327353,"ric. Our results demonstrate the impact of syntax and hierarchy based models for speech transcript translation. 1. Introduction As [1] and [2] note, phrase-based models suffer from sparse data effects when required to translate conceptual elements that span or skip across several words, and distortion based re-ordering techniques tend to limit their range of operation for reasons of efficiency and model strength [3]. Syntax driven [4], [5] and hierarchical translation models [1] model structured re-ordering constraints and extend the domain of locality in the decoding process. Systems such as [6] and [7] demonstrate effective decoding using these models. For the IWSLT-06 evaluation, we applied syntax augmented translation as per [7] to the speech translation task, using the only Full BTEC corpus to model translational equivalence. We begin by extracting lexical phrases as per [2] as the basis for our syntax augmented translation rules. We annotate and generalize these phrases by parsing the target side of the training data with the Stanford Parser [8] (trained on the Penn Treebank). Our results indicate steady improvements as we introduce hierarchy and syntax into the translation proc"
2006.iwslt-evaluation.20,W06-3119,1,0.82497,"results demonstrate the impact of syntax and hierarchy based models for speech transcript translation. 1. Introduction As [1] and [2] note, phrase-based models suffer from sparse data effects when required to translate conceptual elements that span or skip across several words, and distortion based re-ordering techniques tend to limit their range of operation for reasons of efficiency and model strength [3]. Syntax driven [4], [5] and hierarchical translation models [1] model structured re-ordering constraints and extend the domain of locality in the decoding process. Systems such as [6] and [7] demonstrate effective decoding using these models. For the IWSLT-06 evaluation, we applied syntax augmented translation as per [7] to the speech translation task, using the only Full BTEC corpus to model translational equivalence. We begin by extracting lexical phrases as per [2] as the basis for our syntax augmented translation rules. We annotate and generalize these phrases by parsing the target side of the training data with the Stanford Parser [8] (trained on the Penn Treebank). Our results indicate steady improvements as we introduce hierarchy and syntax into the translation process desp"
2006.iwslt-evaluation.20,P03-1054,0,0.00574076,"nslation models [1] model structured re-ordering constraints and extend the domain of locality in the decoding process. Systems such as [6] and [7] demonstrate effective decoding using these models. For the IWSLT-06 evaluation, we applied syntax augmented translation as per [7] to the speech translation task, using the only Full BTEC corpus to model translational equivalence. We begin by extracting lexical phrases as per [2] as the basis for our syntax augmented translation rules. We annotate and generalize these phrases by parsing the target side of the training data with the Stanford Parser [8] (trained on the Penn Treebank). Our results indicate steady improvements as we introduce hierarchy and syntax into the translation process despite the domain mismatch with the English parser. As defined in the evaluation’s Official Specifications, translations are evaluated considering case and punctuation markers, artifacts not typically present in Chinese ASR transcripts. We incorporate the generation of punctuation directly into the translation process, learning translation rules that represent case and punctuation decisions. Our Official evaluation results demonstrate the effectiveness of"
2006.iwslt-evaluation.20,P03-1021,0,0.0307265,"ctuation decisions. Our Official evaluation results demonstrate the effectiveness of these approaches; our submission achieved higher performance when the system output was evaluated with punctuation and case than in the lower-cased, punctuation-free evaluation. This result is especially relevant considering the presence of multiple sentences (which should be punctuation-separated in English) within each speech transcript utterance. We begin by summarizing the syntax augmented rule extraction process from [7] and the decoding settings used to train the model parameters to maximize performance [9] on the BLEU metric [10]. We provide a detailed description of the data-processing used to generate our evaluation submission, and demonstrate the impact of syntax for the IWSLT-06 speech translation task. 2. Syntax Augmented Translation Traditional phrase-based translations serves as the lexical foundation for the syntactic synchronous grammar (SynCFG) presented in [7]. Syntactic, since its nonterminals are syntactic categories derived from parsing the target (English) side of the parallel training corpus, and synchronous because they define operations to derive the source and target language"
2006.iwslt-evaluation.20,P02-1040,0,0.0723637,"Official evaluation results demonstrate the effectiveness of these approaches; our submission achieved higher performance when the system output was evaluated with punctuation and case than in the lower-cased, punctuation-free evaluation. This result is especially relevant considering the presence of multiple sentences (which should be punctuation-separated in English) within each speech transcript utterance. We begin by summarizing the syntax augmented rule extraction process from [7] and the decoding settings used to train the model parameters to maximize performance [9] on the BLEU metric [10]. We provide a detailed description of the data-processing used to generate our evaluation submission, and demonstrate the impact of syntax for the IWSLT-06 speech translation task. 2. Syntax Augmented Translation Traditional phrase-based translations serves as the lexical foundation for the syntactic synchronous grammar (SynCFG) presented in [7]. Syntactic, since its nonterminals are syntactic categories derived from parsing the target (English) side of the parallel training corpus, and synchronous because they define operations to derive the source and target language simultaneously. We trai"
2006.iwslt-evaluation.20,P99-1039,0,0.0211737,"lignment file. Calling phrase-extract (a binary provided in the workshop tools) with these marked files produces a phrase table with separators between each set of phrases extracted from a sentence. For the set of phrases extracted for each sentence, we consider the corresponding parse tree for the whole target sentence. For each source phrase-pair f1 . . . fm /e1 . . . en , we annotate this phrase-pair with the constituent that spans e1 . . . en in the target side parse tree. As an extension to the nonterminal set provided by the Penn Treebank, we consider the CCG nonterminal set proposed by [11]. Under this approach, rules can be assigned partially formed categories, like DT N P , indicating a constituent that forms a noun-phrase, but is missing its determiner at the left. [12] demonstrates the importance of considering phrases not corresponding to pure syntactic constituents in translation, and in [13], we demonstrate the value of using extended categories in our translation system. If neither a Penn Treebank or CCG constituent can be found for e1 . . . en , we associate a generic nonterminal symbol _X with this rule, allowing it to still take part in hierarchically motivated synch"
2006.iwslt-evaluation.20,koen-2004-pharaoh,0,0.137793,"a sentence. For the set of phrases extracted for each sentence, we consider the corresponding parse tree for the whole target sentence. For each source phrase-pair f1 . . . fm /e1 . . . en , we annotate this phrase-pair with the constituent that spans e1 . . . en in the target side parse tree. As an extension to the nonterminal set provided by the Penn Treebank, we consider the CCG nonterminal set proposed by [11]. Under this approach, rules can be assigned partially formed categories, like DT N P , indicating a constituent that forms a noun-phrase, but is missing its determiner at the left. [12] demonstrates the importance of considering phrases not corresponding to pure syntactic constituents in translation, and in [13], we demonstrate the value of using extended categories in our translation system. If neither a Penn Treebank or CCG constituent can be found for e1 . . . en , we associate a generic nonterminal symbol _X with this rule, allowing it to still take part in hierarchically motivated synchronous derivations. The set of rules resulting from the process above is more than 10 times the size of the initial lexical rules / phrase pairs used to create them. As an initial pruning"
2006.iwslt-evaluation.20,N06-1001,0,0.0236721,"umeric or ordinal terms in the development data, which typically lead to unreliable language model estimates (due to the sparsity in encountered numeric forms). We handle source and target number forms as described below. 4.1. Case Information Word alignment and subsequent phrase and rule extraction rely on well estimated lexical probabilities; a requirement that is often strained in limited data scenarios with true-cased data. Common solutions include training translation and language models on lower-cased text, generating system output in lower-case and then applying a true-case system like [16], or training all models in true-case. We take a intermediate approach, based on the premise that the real source of sparsity in the direct true-case approach comes from the first word of the sentence being upper-cased due to its position. Upper-case words used within the sentence (rather than in the first position) tend to be consistently cased across the corpus (a notable exception being “May” vs “may”). For each first word of the target side of the corpus we estimate its most common case in the corpus (ignoring first word occurrences). If the most common case is lower, we lower case the wor"
2006.iwslt-evaluation.20,J04-4002,0,\N,Missing
2006.iwslt-evaluation.20,N03-1017,0,\N,Missing
2007.iwslt-1.4,W06-3711,0,0.0292572,"able speech translation system which allows an English speaker to converse with a target language speaker. Our systems have been evaluated on a regular basis as part of the DARPA TransTac program. These evaluations are run by NIST, and involve military users and target language users who have never used our system before. The evaluations consist of communicating through the translation device for a number of pre-designed scenarios (which were previously unknown to us). The tests take place both indoors and outdoors. Other systems in the TransTac program include those developed by BBN [5], IBM [6], SRI, Sehda/Fluential, and USC [7] [8]. 2. Challenges The two target languages in the TransTac program are Iraqi Arabic and Farsi. Iraqi Arabic is defined as the spoken form of Arabic used by the people of Iraq in everyday conversations. It is distinct from the formal Modern Standard Arabic (MSA) used in written communication. As Iraqi Arabic is normally not written, even with transcription conventions there is greater variability in the spelling conventions than in a standard written language. Farsi (Persian), mainly spoken in Iran and areas of Afghanistan, also uses the Arabic script, thoug"
2007.iwslt-1.4,2005.mtsummit-papers.33,1,0.758733,"ple raftin vs. raftid ""you went"". The word forms (inside of the word) may be modified to represent their colloquial pronunciation for instance khune vs. khAne 'house', midam vs. midaham 'i give'). 5.4. Language models The language model is a standard 6-gram language model with Good-Turing smoothing implemented as a suffix array (SA LM) [10]. Another option of language model is the 4-gram modified Kneser-Ney smoothing trained using the SRI language modeling toolkit (SRI LM) [11]. 5.5. Translation models 5.5.1. PESA phrase extraction In Iraqi-English we applied the PESA phrase extraction method [9]. For a given source phrase PESA tries to find the optimal sentence splits of the training sentences containing this source phrase based on inner and outer IBM1 word alignment probabilities. We applied PESA as an online phrase extraction which means that phrase pairs are dynamically extracted from the training data as needed during the translation of the test set. We compared the performance here with a standard Pharaoh phrase table but we saw considerable improvements using the PESA approach. For Iraqi-English a considerable amount of training data is available and parts of the test dialogs a"
2007.iwslt-1.4,2005.eamt-1.39,1,0.834249,"lization steps need to be agreed upon. However, it is not easy to reach a consensus since Iraqi Arabic lacks a standard writing system. Furthermore, there are issues with speaking style. Words can be used with their formal or informal/colloquial endings for example raftin vs. raftid ""you went"". The word forms (inside of the word) may be modified to represent their colloquial pronunciation for instance khune vs. khAne 'house', midam vs. midaham 'i give'). 5.4. Language models The language model is a standard 6-gram language model with Good-Turing smoothing implemented as a suffix array (SA LM) [10]. Another option of language model is the 4-gram modified Kneser-Ney smoothing trained using the SRI language modeling toolkit (SRI LM) [11]. 5.5. Translation models 5.5.1. PESA phrase extraction In Iraqi-English we applied the PESA phrase extraction method [9]. For a given source phrase PESA tries to find the optimal sentence splits of the training sentences containing this source phrase based on inner and outer IBM1 word alignment probabilities. We applied PESA as an online phrase extraction which means that phrase pairs are dynamically extracted from the training data as needed during the t"
2007.iwslt-1.4,2005.eamt-1.36,1,0.72462,"ar in the training corpus, because they occur in the phrase table only embedded in longer phrases. This leads to an unnecessary high number of untranslated words. On the other side, the PESA phrase alignment will generate translations for all n-grams including all individual words, which can be found in the training corpus. To guarantee that the phrase table can cover all source vocabulary and to leverage the PESA’s strength in arbitrary long matching, we trained two phrase tables and interpolated them. The interpolation parameters are optimized through a minimum-error-rate training framework [12]. 5.5.4. Speed constraint To limit delays, the translation has to be performed during the replay of the ASR output. This has to be the case for even very long sentences. For all practical considerations we assume to have about 200 ms on average to do the translation. Some of speeding strategies we applied is phrase table pruning and restrict the search space during the decoding process. Those techniques help to decrease the system running time significantly. 5.5.5. Decoder For this evaluation the system is running on a standard laptop with 2 GB of memory so we could use our regular decoder [2]"
2007.iwslt-1.4,2007.mtsummit-papers.72,1,0.769394,"ery long sentences. For all practical considerations we assume to have about 200 ms on average to do the translation. Some of speeding strategies we applied is phrase table pruning and restrict the search space during the decoding process. Those techniques help to decrease the system running time significantly. 5.5.5. Decoder For this evaluation the system is running on a standard laptop with 2 GB of memory so we could use our regular decoder [2]. The previous system described in [1] was running on a PDA. Due to lack of memory and computing power an earlier version of the decoder described in [18] had to be used that did not support word reordering and required heavily pruned models. 5.5.6. Translation results We report the performance of translation component in terms of BLEU score [20]. On the test sets the system achieved a score of 42.12 for English to Iraqi and 63.49 for Iraqi to English. The Farsi systems use similar technologies as the Iraqi systems. Table 8 shows the translation performance of the provided training data on various setups. Table 8: Farsi translation performance (in BLEU) Farsi→English Dev. Pharaoh + 4-gram SRI LM 24.64 PESA + 6-gram SA LM 23.06 English→Farsi Pha"
2007.iwslt-1.4,koen-2004-pharaoh,0,0.0298013,"online phrase extraction does not have to extract the phrases pairs dynamically. Instead, the online phrase extraction is only used for long or rarely seen phrases. This did not give any significant change in performance but resulted in a considerable speedup. The system uses the same corpora to extract online PESA phrases for both translation directions so we combined the Iraqi-English and English-Iraqi corpora for this. However, the pre-extracted phrases were extracted separately for each direction from the respective corpus. 5.5.3. Interpolate Pharaoh and PESA We observed that the Pharaoh [19] phrase table does not contain entries for all words in the source vocabulary. This comes from the heuristics applied to avoid unlikely translations. Therefore, some words will not be translated, even though they appear in the training corpus, because they occur in the phrase table only embedded in longer phrases. This leads to an unnecessary high number of untranslated words. On the other side, the PESA phrase alignment will generate translations for all n-grams including all individual words, which can be found in the training corpus. To guarantee that the phrase table can cover all source v"
2007.iwslt-1.4,P02-1040,0,0.073748,"the search space during the decoding process. Those techniques help to decrease the system running time significantly. 5.5.5. Decoder For this evaluation the system is running on a standard laptop with 2 GB of memory so we could use our regular decoder [2]. The previous system described in [1] was running on a PDA. Due to lack of memory and computing power an earlier version of the decoder described in [18] had to be used that did not support word reordering and required heavily pruned models. 5.5.6. Translation results We report the performance of translation component in terms of BLEU score [20]. On the test sets the system achieved a score of 42.12 for English to Iraqi and 63.49 for Iraqi to English. The Farsi systems use similar technologies as the Iraqi systems. Table 8 shows the translation performance of the provided training data on various setups. Table 8: Farsi translation performance (in BLEU) Farsi→English Dev. Pharaoh + 4-gram SRI LM 24.64 PESA + 6-gram SA LM 23.06 English→Farsi Pharaoh + SRI LM 10.07 PESA + SA LM 9.45 Pharaoh + SA LM 10.41 Pharaoh + PESA + SA LM 10.23 Unseen 23.3 19.9 14.87 14.67 15.42 16.44 6. Text-to-Speech Text-to-speech was provided by Cepstral, LLC's"
2007.iwslt-1.4,2005.iwslt-1.16,0,\N,Missing
2007.iwslt-1.4,2005.iwslt-1.6,1,\N,Missing
2007.iwslt-1.9,J93-2003,0,0.0119348,"Missing"
2007.iwslt-1.9,P03-1021,0,0.0132789,"hood, given: eˆI1 = arg max P (eI1 |f1J ) eI1 exp = arg max P eI1 e′ I1 ′ P exp M m=1 P λm hm eI1 , f1J M m=1  λm hm e′ I1 , f1J ′  In this framework, the posterior probability P (eI1 |f1J ) is directly maximized using  a log-linear combination of feature functions hm eI1 , f1J (during decoding the denominator is dropped since it depends only on f1J ). Feature functions applied during translation include: language models, translation models, and sentence length models. The scaling factors (λ1 , λ2 , . . . , λM ) applied during search are optimized via MERT (minimum error rate training) [3] for a specific translation metric such as BLEU [4]. Search is performed using our STTK beam-search-decoder [5] which allows restricted word re-ordering during translation. 3. Topic-Aware Japanese-to-English SLT For the Japanese-to-English submission we focused on two research areas. First, we compared various methods to recover intra-utterance sentence boundaries and secondary punctuation (commas); and second, we investigated approaches to incorporate topic-knowledge into the SMT framework. These works are described in Sections 3.3 and 3.4, respectively. By incorporating publicly available co"
2007.iwslt-1.9,P02-1040,0,0.0730825,"arg max P eI1 e′ I1 ′ P exp M m=1 P λm hm eI1 , f1J M m=1  λm hm e′ I1 , f1J ′  In this framework, the posterior probability P (eI1 |f1J ) is directly maximized using  a log-linear combination of feature functions hm eI1 , f1J (during decoding the denominator is dropped since it depends only on f1J ). Feature functions applied during translation include: language models, translation models, and sentence length models. The scaling factors (λ1 , λ2 , . . . , λM ) applied during search are optimized via MERT (minimum error rate training) [3] for a specific translation metric such as BLEU [4]. Search is performed using our STTK beam-search-decoder [5] which allows restricted word re-ordering during translation. 3. Topic-Aware Japanese-to-English SLT For the Japanese-to-English submission we focused on two research areas. First, we compared various methods to recover intra-utterance sentence boundaries and secondary punctuation (commas); and second, we investigated approaches to incorporate topic-knowledge into the SMT framework. These works are described in Sections 3.3 and 3.4, respectively. By incorporating publicly available corpora from related domains and applying the propose"
2007.iwslt-1.9,2003.mtsummit-papers.53,1,0.904427,"λm hm e′ I1 , f1J ′  In this framework, the posterior probability P (eI1 |f1J ) is directly maximized using  a log-linear combination of feature functions hm eI1 , f1J (during decoding the denominator is dropped since it depends only on f1J ). Feature functions applied during translation include: language models, translation models, and sentence length models. The scaling factors (λ1 , λ2 , . . . , λM ) applied during search are optimized via MERT (minimum error rate training) [3] for a specific translation metric such as BLEU [4]. Search is performed using our STTK beam-search-decoder [5] which allows restricted word re-ordering during translation. 3. Topic-Aware Japanese-to-English SLT For the Japanese-to-English submission we focused on two research areas. First, we compared various methods to recover intra-utterance sentence boundaries and secondary punctuation (commas); and second, we investigated approaches to incorporate topic-knowledge into the SMT framework. These works are described in Sections 3.3 and 3.4, respectively. By incorporating publicly available corpora from related domains and applying the proposed techniques the translation accuracy of our system improved"
2007.iwslt-1.9,2005.mtsummit-papers.33,1,0.822923,"Missing"
2007.iwslt-1.9,P03-1010,0,0.0472683,"Missing"
2007.iwslt-1.9,2006.iwslt-evaluation.12,0,0.057142,"Missing"
2007.iwslt-1.9,W06-3119,1,0.869494,"well structured target language output under the premise that human language is essentially hierarchical in its generation. Hierarchical approaches gain their representational power by allowing transformation rules to condition on larger fragments of target language tree structure. The application of hierarchically structured models to statistical machine translation requires the development of techniques to induce and estimate transformation rules from parallel data (grammar induction), and efficient algorithms to apply these rules to translate source language text (decoding). In recent work [11], we presented the first results that leverage target language syntactic structure to achieve higher performance than comparable phrase based translation. [12] presents results that show the impact of hierarchical structure alone, and [13] achieves significant improvements using treeto-string transformations. For the Chinese-to-English task, we used the latest version of the Syntax-Augmented Machine Translation (SAMT) system first described in [11]. The system is available opensource under the GNU General Public License at: www.cs.cmu.edu/˜zollmann/samt 4.1. Synchronous Grammars for SMT Probab"
2007.iwslt-1.9,P05-1033,0,0.0721247,"representational power by allowing transformation rules to condition on larger fragments of target language tree structure. The application of hierarchically structured models to statistical machine translation requires the development of techniques to induce and estimate transformation rules from parallel data (grammar induction), and efficient algorithms to apply these rules to translate source language text (decoding). In recent work [11], we presented the first results that leverage target language syntactic structure to achieve higher performance than comparable phrase based translation. [12] presents results that show the impact of hierarchical structure alone, and [13] achieves significant improvements using treeto-string transformations. For the Chinese-to-English task, we used the latest version of the Syntax-Augmented Machine Translation (SAMT) system first described in [11]. The system is available opensource under the GNU General Public License at: www.cs.cmu.edu/˜zollmann/samt 4.1. Synchronous Grammars for SMT Probabilistic synchronous context-free grammars (PSCFGs) are defined by a source terminal set (source vocabulary) TS , a target terminal set (target vocabulary) TT ,"
2007.iwslt-1.9,W06-1606,0,0.0261321,"ragments of target language tree structure. The application of hierarchically structured models to statistical machine translation requires the development of techniques to induce and estimate transformation rules from parallel data (grammar induction), and efficient algorithms to apply these rules to translate source language text (decoding). In recent work [11], we presented the first results that leverage target language syntactic structure to achieve higher performance than comparable phrase based translation. [12] presents results that show the impact of hierarchical structure alone, and [13] achieves significant improvements using treeto-string transformations. For the Chinese-to-English task, we used the latest version of the Syntax-Augmented Machine Translation (SAMT) system first described in [11]. The system is available opensource under the GNU General Public License at: www.cs.cmu.edu/˜zollmann/samt 4.1. Synchronous Grammars for SMT Probabilistic synchronous context-free grammars (PSCFGs) are defined by a source terminal set (source vocabulary) TS , a target terminal set (target vocabulary) TT , a shared nonterminal set N and induce rules of the form X → hγ, α, ∼, wi 3.5. T"
2007.iwslt-1.9,N04-1035,0,0.024464,"om nonterminal tokens in γ to nonterminal tokens in α, and S qqMMMMM q q MM qqq VP qMMMM q q MMM q q qq VB PRN AUX RB NP • w ∈ [0, ∞) is a nonnegative real-valued weight assigned to the rule. In our notation, we will assume ∼ to be implicitly defined by indexing the NT occurrences in γ from left to right starting with 1, and by indexing the NT occurrences in α by the indices of their corresponding counterparts in γ. Syntaxoriented PSCFG approaches often ignore source structure, instead focusing on generating syntactically well-formed target derivations. [12] use a single nonterminal category, [14] use syntactic constituents for the PSCFG nonterminal set, and [11] take advantage of CCG [15] inspired “slash” and “plus” categories. he il go not qMMMMqqq q q M q q qqq MM qqq ne va pas does Figure 3: Alignment graph (word alignment and target parse tree) for a French-English sentence pair. 4.2. Grammar Induction The SAMT model generates a PSCFG given parallel sentence pairs hf, ei, a parse tree π for each e, the maximum a posteriori word alignment a over hf, ei, and a set of phrase pairs Phrases(a) identified by any alignment-driven phrase induction technique such as e.g. [16]. Each phrase"
2007.iwslt-1.9,P99-1039,0,0.0467032,"MM q q qq VB PRN AUX RB NP • w ∈ [0, ∞) is a nonnegative real-valued weight assigned to the rule. In our notation, we will assume ∼ to be implicitly defined by indexing the NT occurrences in γ from left to right starting with 1, and by indexing the NT occurrences in α by the indices of their corresponding counterparts in γ. Syntaxoriented PSCFG approaches often ignore source structure, instead focusing on generating syntactically well-formed target derivations. [12] use a single nonterminal category, [14] use syntactic constituents for the PSCFG nonterminal set, and [11] take advantage of CCG [15] inspired “slash” and “plus” categories. he il go not qMMMMqqq q q M q q qqq MM qqq ne va pas does Figure 3: Alignment graph (word alignment and target parse tree) for a French-English sentence pair. 4.2. Grammar Induction The SAMT model generates a PSCFG given parallel sentence pairs hf, ei, a parse tree π for each e, the maximum a posteriori word alignment a over hf, ei, and a set of phrase pairs Phrases(a) identified by any alignment-driven phrase induction technique such as e.g. [16]. Each phrase in Phrases(a) is first annotated with a syntactic category to produce initial rules, where γ i"
2007.iwslt-1.9,J04-4002,0,0.0986589,"nal category, [14] use syntactic constituents for the PSCFG nonterminal set, and [11] take advantage of CCG [15] inspired “slash” and “plus” categories. he il go not qMMMMqqq q q M q q qqq MM qqq ne va pas does Figure 3: Alignment graph (word alignment and target parse tree) for a French-English sentence pair. 4.2. Grammar Induction The SAMT model generates a PSCFG given parallel sentence pairs hf, ei, a parse tree π for each e, the maximum a posteriori word alignment a over hf, ei, and a set of phrase pairs Phrases(a) identified by any alignment-driven phrase induction technique such as e.g. [16]. Each phrase in Phrases(a) is first annotated with a syntactic category to produce initial rules, where γ is set to the source side of the phrase, α is set to the target side of the phrase, and X is assigned based on the corresponding target side span in π. If the target span of the phrase does not match a constituent in π, heuristics are used to assign categories that correspond to partial rewriting of the tree. These heuristics first consider concatenation operations, forming categories like “NP+VP”, and then resort to CCG style “slash” categories like “NP/NN.” Preference for the concatenat"
2007.iwslt-1.9,N07-1063,1,0.825446,"l symbols spanned by D. Our distribution p over derivations is defined by a loglinear model. The probability of a derivation D is defined in terms of the rules r that are used in D: Q Q pLM (tgt(D))λLM × r∈D i φi (r)λi p(D) = (2) Z(λ) where φi refers to features defined on each rule, pLM is a n-gram LM probability applied to the target terminal symbols generated by the derivation D, and Z(λ) is a normalization constant chosen such that the probabilities sum up to one. The computational challenges of this search task (compounded by the integration of the language model) are addressed elsewhere [18, 19]. All feature weights λi are trained in concert with the language model weight via minimumerror training [3]. Here, we focus on the estimation of the feature values φ during the grammar induction process. The feature values are statistics estimated from rule counts. where lhs returns the left-hand-side of a rule, src returns the source side γ, and tgt returns the target side α of a rule r. The function ul removes all syntactic labels from its arguments, but retains ordering notation. For example, ul(NP+AUX1 does not go) = 1 does not go. The last two features are extensions to the feature set"
2007.iwslt-1.9,N03-1017,0,0.0142849,"]. They represent the same kind of relative frequency estimates commonly used in phrase based systems. The ul function allows us to calculate these estimates for rules with nonterminals as well. To estimate these probabilistic features, we use maximum likelihood estimates based on counts of the rules extracted from the training data. For example, pˆ(r|lhs(r)) is estimated by computing #(r)/#(lhs(r)), aggregating counts from all extracted rules. As in phrase-based translation model estimation, φ also contains two lexical weights pˆw (lex(src(r)) |lex(tgt(r))) and pˆw (lex(tgt(r)) |lex(src(r))) [20] that are based on the lexical symbols of γ, α. These weights are estimated based on an pair of statistical lexicons that represent pˆ(s|t), pˆ(t|s), where s and t are single words in the source and target vocabulary. These word-level translation models are typically estimated by maximum likelihood, considering the word-toword links from “single-best” alignments as evidence. We also store several boolean and count features in φ: the rule is purely lexical in α and γ; the rule is purely non-lexical in α and γ; the number of target words in the rule. 4.5. Training Corpora We used the provided 40"
2007.iwslt-1.9,H05-1060,0,0.0547858,"age is critical. However, for very diverse language pairs, i.e. translating between a rich morphology language such as Arabic and a poor morphology language such as English, a significant mismatch is present. For this language-pair, prefixes and suffixes of an Arabic word often correspond to separate English words. When translating from Arabic to English, a preprocessing step on Arabic is necessary to maintain consistency between two languages. In our A→E submission system we applied full morphological decomposition to the training corpora using a stateof-the-art Arabic morphological analyzer [22]. Morphological decomposition replaces each word in the training corpora with a sequence of its component morphemes prefix-, stem, -suffix. This approach also improves the coverage of the system, enabling it to translate words that do not occur in the training data, by performing translation at the sub-word, morpheme level. The prefix of an Arabic word can be a combination of conjunction (wa - and), article (Al - the), and preposition (li - to/for). Its’ suffix can be a pronoun (hm - their/them), case marker (u, i, a) gender (f - female singular), number, or voice, etc. Even though many morphe"
2007.iwslt-1.9,2006.iwslt-evaluation.19,1,0.861871,"ssion system and compare various approaches to recover punctuation (Section 3.3). In Section 3.4 we investigate methods to incorporate topic-knowledge into the translation framework via N -best list re-scoring. Our syntax-augmented SMT framework is detailed in Section 4. Sections 4.5 and 4.6 describe its application to the C→E task. Our A→E translation system which incorporates morphological decomposition is introduced in Section 5. 2. The CMU-UKA Phrase-based SMT System Our J→E and A→E systems built upon the STTK (Statistical Translation Toolkit) framework used for our IWSLT 2006 submissions [1]. STTK implements phrase-based statistical machine translation using a log-linear model [2] in which a foreign language sentence f1J = f1 , f2 , . . . , fJ is translated into another language eI1 = e1 , e2 , . . . , eI by searching for the hypothesis eˆI1 with maximum likelihood, given: eˆI1 = arg max P (eI1 |f1J ) eI1 exp = arg max P eI1 e′ I1 ′ P exp M m=1 P λm hm eI1 , f1J M m=1  λm hm e′ I1 , f1J ′  In this framework, the posterior probability P (eI1 |f1J ) is directly maximized using  a log-linear combination of feature functions hm eI1 , f1J (during decoding the denominator is dro"
2007.iwslt-1.9,C96-2141,1,0.692703,"|tgt(r)) : Probability of a rule given its target side pˆ(ul(src(r)), ul(tgt(r)) |ul(src(r)) : Probability of the unlabeled source and target side of the rule given its unlabeled source side. 5. Morphological-Decomposition for Robust Arabic-to-English SMT Statistical machine translation relies on a word alignment model, between source and target language, to extract and score phrase translations. In current word alignment methods Development Data IWSLT 04 500 sent. IWSLT 05 506 sent. 157,795 Arabic words 189,861 English words 0.7 16 references 16 references 0.5 Table 5: Arabic - English Data [2, 21] the one-to-one mapping between tokens in the source and target language is critical. However, for very diverse language pairs, i.e. translating between a rich morphology language such as Arabic and a poor morphology language such as English, a significant mismatch is present. For this language-pair, prefixes and suffixes of an Arabic word often correspond to separate English words. When translating from Arabic to English, a preprocessing step on Arabic is necessary to maintain consistency between two languages. In our A→E submission system we applied full morphological decomposition to the tr"
2007.iwslt-1.9,J03-1002,0,0.00627488,", case marker (u, i, a) gender (f - female singular), number, or voice, etc. Even though many morphemes have an equivalent English translation, some specific morphemes like gender, number, and case markers are redundant and can be discarded when translating to English. Previous work [23], used knowledge of the Arabic language to explicitly remove inflectional features from Arabic text before translation. In this work, we attempt to discard non-informative morphemes using a data driven approach. First, Arabic full morphology analysis and English text are passed to Giza++ word alignment toolkit [24]. Figure 4 shows the fertility distributions of Arabic morphemes from this alignment for the training corpora defined below. Morphemes that are not aligned to any English word are indicated by high zerofertility probability. These morphemes must be discarded to reduce NULL alignments and balance sentence length between the Arabic and English data. Morphemes for which the zero-fertility are greater than a threshold θth are discarded from the Arabic text. The threshold θth is selected to maximize translation quality (BLEU) on a development set. In the experimental evaluation, the “IWSLT A→E prov"
2007.iwslt-1.9,2005.iwslt-1.6,1,\N,Missing
2007.iwslt-1.9,J07-2003,0,\N,Missing
2007.mtsummit-papers.12,P02-1040,0,0.0909687,"Missing"
2007.mtsummit-papers.12,2003.mtsummit-papers.53,1,0.93227,"Missing"
2007.mtsummit-papers.12,2007.mtsummit-papers.72,1,0.609053,"earch, we are developing an image-based system that translates Arabic document images into English. The system works as follows. After an image is captured from a digital camera, the system preprocesses the image to account for fonts, skew, rotation, illumination, shadows, glare, reflection, and other sources of variability. Subsequently, it automatically detects text regions in the image, performs recognition using off-the-shelf OCR (Optical Character Recognition) software on the text regions, and then translates the text strings into English using our state-of-the-art statistical MT system (Zhang and Vogel, 2007). An image-based MT system consists of many components. Its performance relies on not only the machine translation (text-to-text) technology, but also other component technologies. A good image-based document translation system requires robust technologies for text detection, OCR, and language translation. Traditional pipeline approaches yield error propagation, and errors in any component of an image based document translation system can affect the end-to-end performance of the entire system. The errors can be propagated through the system and even amplified during the propagation process, e."
2007.mtsummit-papers.22,P05-1032,0,0.0129915,"the decoding process and form a translation lattice (or word graph, Ueffing et al., 2002). Each possible path through this translation lattice is evaluated according to a number of models and the best path is chosen as the final translation (model-best path). The target sides of the phrase pairs in this path are combined to form the final translation. Various algorithms for phrase pair extraction have been proposed (e.g. Koehn et al. 2003; Vogel, 2005; Zhao and Waibel, 2005). Recent developments extract the phrase pairs from the bilingual data as needed depending on the actual test sentence (Callison-Burch et al., 2005; Zhang and Vogel, 2005). These techniques usually improve the performance, but need more computing power and memory compared to pre-extracting the phrase pairs so they will most likely not be used for small devices. For this reason we concentrated on translation models consisting of pre-extracted phrase pairs. 1.2 Phrase Pair Pruning To limit the memory requirements of a translation system we now try to eliminate some of these phrase pairs (pruning of the translation model). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, wh"
2007.mtsummit-papers.22,2006.iwslt-evaluation.19,1,0.863264,"Missing"
2007.mtsummit-papers.22,N07-2006,1,0.773168,"Missing"
2007.mtsummit-papers.22,koen-2004-pharaoh,0,0.0534213,"ecially ones that do not have a clear translation in the target language sometimes have a high number of possible translation candidates. The final translation path will have to choose one out of those. It seems natural to restrict the translation variety, especially if memory space is limited. The translation variety threshold imposes this limit. The pruning is accomplished by sorting the phrase pairs for each source phrase according to their probability and eliminating low probability ones until the threshold is reached. Both threshold pruning strategies are well known. The Pharaoh decoder (Koehn, 2004) for example has an option to directly apply them to a phrase table. 2.2 Pruning via usage statistics In Eck et al. (2007) we introduced a pruning strategy that utilizes usage statistics to eliminate phrase pairs. This pruning strategy was inspired by the Optimal Brain Damage algorithm (Le Cun et al., 1990) and collected statistics for phrase pairs by translating the whole training corpus with the originally extracted phrase pairs. For each phrase pair two statistics were collected during this translation: • c(phrase pair) = Count how often a phrase pair was considered during decoding (i.e. wa"
2007.mtsummit-papers.22,N03-1017,0,0.0249865,"anslation model assigns translation probabilities to phrase pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied during the decoding process and form a translation lattice (or word graph, Ueffing et al., 2002). Each possible path through this translation lattice is evaluated according to a number of models and the best path is chosen as the final translation (model-best path). The target sides of the phrase pairs in this path are combined to form the final translation. Various algorithms for phrase pair extraction have been proposed (e.g. Koehn et al. 2003; Vogel, 2005; Zhao and Waibel, 2005). Recent developments extract the phrase pairs from the bilingual data as needed depending on the actual test sentence (Callison-Burch et al., 2005; Zhang and Vogel, 2005). These techniques usually improve the performance, but need more computing power and memory compared to pre-extracting the phrase pairs so they will most likely not be used for small devices. For this reason we concentrated on translation models consisting of pre-extracted phrase pairs. 1.2 Phrase Pair Pruning To limit the memory requirements of a translation system we now try to eliminat"
2007.mtsummit-papers.22,P02-1040,0,0.0718021,"Missing"
2007.mtsummit-papers.22,2005.mtsummit-papers.33,1,0.852423,"igns translation probabilities to phrase pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied during the decoding process and form a translation lattice (or word graph, Ueffing et al., 2002). Each possible path through this translation lattice is evaluated according to a number of models and the best path is chosen as the final translation (model-best path). The target sides of the phrase pairs in this path are combined to form the final translation. Various algorithms for phrase pair extraction have been proposed (e.g. Koehn et al. 2003; Vogel, 2005; Zhao and Waibel, 2005). Recent developments extract the phrase pairs from the bilingual data as needed depending on the actual test sentence (Callison-Burch et al., 2005; Zhang and Vogel, 2005). These techniques usually improve the performance, but need more computing power and memory compared to pre-extracting the phrase pairs so they will most likely not be used for small devices. For this reason we concentrated on translation models consisting of pre-extracted phrase pairs. 1.2 Phrase Pair Pruning To limit the memory requirements of a translation system we now try to eliminate some of the"
2007.mtsummit-papers.22,2005.eamt-1.39,1,0.84793,"m a translation lattice (or word graph, Ueffing et al., 2002). Each possible path through this translation lattice is evaluated according to a number of models and the best path is chosen as the final translation (model-best path). The target sides of the phrase pairs in this path are combined to form the final translation. Various algorithms for phrase pair extraction have been proposed (e.g. Koehn et al. 2003; Vogel, 2005; Zhao and Waibel, 2005). Recent developments extract the phrase pairs from the bilingual data as needed depending on the actual test sentence (Callison-Burch et al., 2005; Zhang and Vogel, 2005). These techniques usually improve the performance, but need more computing power and memory compared to pre-extracting the phrase pairs so they will most likely not be used for small devices. For this reason we concentrated on translation models consisting of pre-extracted phrase pairs. 1.2 Phrase Pair Pruning To limit the memory requirements of a translation system we now try to eliminate some of these phrase pairs (pruning of the translation model). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the tr"
2007.mtsummit-papers.22,I05-3011,1,0.640476,"ion probabilities to phrase pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied during the decoding process and form a translation lattice (or word graph, Ueffing et al., 2002). Each possible path through this translation lattice is evaluated according to a number of models and the best path is chosen as the final translation (model-best path). The target sides of the phrase pairs in this path are combined to form the final translation. Various algorithms for phrase pair extraction have been proposed (e.g. Koehn et al. 2003; Vogel, 2005; Zhao and Waibel, 2005). Recent developments extract the phrase pairs from the bilingual data as needed depending on the actual test sentence (Callison-Burch et al., 2005; Zhang and Vogel, 2005). These techniques usually improve the performance, but need more computing power and memory compared to pre-extracting the phrase pairs so they will most likely not be used for small devices. For this reason we concentrated on translation models consisting of pre-extracted phrase pairs. 1.2 Phrase Pair Pruning To limit the memory requirements of a translation system we now try to eliminate some of these phrase pairs (pruning"
2007.mtsummit-papers.33,J93-2003,0,0.0148959,"c; parsing Arabic side and extracting corresponding English NP translations. However, the Arabic parsers available did not produce desired accuracy. Therefore we use Charniak’s parser (Charniak, 2000) to parse English side of the training data. From the resulting parse trees we extract base NPs; i.e. NPs that do not contain other NPs embedded in them. As mentioned in the previous section these NPs are fairly short and are good candidates for a hierarchical system. Arabic 135K 3.5M 145K Sentences Tokens Vocabulary English 135K 4.3M 63K Table 1: Training data statistics We generate IBM model 3 (Brown et al., 1993) alignments by running GIZA++ (Och and Ney, 2003) with the parallel text. GIZA++ training is done for both directions and the word alignments are generated by the intersection of the two. For each English NP, we search the aligned corpus for sentences that contain the NP and read off the alignment as its translation. To compensate for alignment errors we also include partial alignments as follows: We find maximum (max) and minimum (min) Arabic word indices that are aligned to the words in the English NP. All the Arabic words between min and max are considered to be the translation of the Engli"
2007.mtsummit-papers.33,A00-2018,0,0.144624,"وﺣﺪة وﻃﻨﻴﺔ ﺑﻌﺜﺔ ﺟﺪﻳﺪة ﻟﻼﻣﻢ اﻟﻤﺘﺤﺪة اﻓﺎد ﻣﺮاﺳﻞ وآﺎﻟﺔ ﻓﺮاﻧﺲ ﺑﺮس اﻟﻤﻨﺘﺠﺎت اﻟﺰراﻋﻴﺔ واﻟﻐﺬاﺋﻴﺔ # # # # # # a military campaign a military cooperation protocol a national unity government a new united nations mission agence france presse correspondent agricultural and food products Figure 1: Sample of NP translation table As our system translates Arabic text into English, it would be logical to start with Arabic; parsing Arabic side and extracting corresponding English NP translations. However, the Arabic parsers available did not produce desired accuracy. Therefore we use Charniak’s parser (Charniak, 2000) to parse English side of the training data. From the resulting parse trees we extract base NPs; i.e. NPs that do not contain other NPs embedded in them. As mentioned in the previous section these NPs are fairly short and are good candidates for a hierarchical system. Arabic 135K 3.5M 145K Sentences Tokens Vocabulary English 135K 4.3M 63K Table 1: Training data statistics We generate IBM model 3 (Brown et al., 1993) alignments by running GIZA++ (Och and Ney, 2003) with the parallel text. GIZA++ training is done for both directions and the word alignments are generated by the intersection of th"
2007.mtsummit-papers.33,P05-1033,0,0.0437426,"ss; long exact matching phrases are relatively rare in the training data. In the decoder, these phrases have to compete with abundant shorter phrases. Due to this reason, Koehn et al. (2003) find that phrases longer than three words give little performance improvement. However, with limited reordering strategies used in most of the statistical machines translation systems, a combination of small short phrases does not always generate the desired translation. Zhang (2005) shows improved translation performance by using phrases of arbitrary length. Hierarchical models, such as the Hiero system (Chiang, 2005), that uses phrases with words as well as subphrases have shown better performance than standard phrase based systems. In this paper, we investigate a simplified two-level machine translations system that uses a linguistically motivated phrase decomposition. We think noun phrases (NPs) are good candidates for a hierarchical system. Semantically noun phrases describe objects and concepts using one or more nouns and adjectives. The vast majority of words in a language are nouns and hence NPs appear frequently in sentences. Noun phrases can often be translated independently into other languages i"
2007.mtsummit-papers.33,N03-1017,0,0.37612,"ization introduced by tagging NPs. 1. Introduction When using statistical machine translation (SMT) systems, we often notice that the phrases used to construct the translations are rather short. On average these phrases are less than two words long. This is in spite of that fact that some phrase extraction methods allow the extraction of arbitrarily long phrases. The main reason for this behavior is data sparseness; long exact matching phrases are relatively rare in the training data. In the decoder, these phrases have to compete with abundant shorter phrases. Due to this reason, Koehn et al. (2003) find that phrases longer than three words give little performance improvement. However, with limited reordering strategies used in most of the statistical machines translation systems, a combination of small short phrases does not always generate the desired translation. Zhang (2005) shows improved translation performance by using phrases of arbitrary length. Hierarchical models, such as the Hiero system (Chiang, 2005), that uses phrases with words as well as subphrases have shown better performance than standard phrase based systems. In this paper, we investigate a simplified two-level machi"
2007.mtsummit-papers.33,E03-1035,0,0.0121748,"nigram bigram trigrams 4-grams Original 630K 841K 2293K 3242K NP-tagged 628K 770K 2015K 2894K 1.3 words. For English sentences, the drop is about 2 words. Corpus Original NP-tagged We also compared the average length of the corpus before and after NP-tagging. These numbers are given in Table 6. Avg. length of an Arabic sentence has dropped by about NP-tagged 35.89 33.40 Table 6: Avg. length of training corpus before and after NP-tagging 2.3 Extract Phrases from NP-tagged Corpus We use the NP-tagged parallel corpus to extract phrase translation pairs. Our phrase extraction method is similar to Moore (2003) which is a variation of the IBM-1 word alignment model (Brown et al., 1993). Assuming a source sentence s1 = s1 K s I in the i bilingual corpus contains a phrase s i12 = s i K si2 we are j interested in the sequence of words t j12 =1 t j1 ...t j2 from J the respective target sentence t1 = t1 ...t J that is the optimal translation for this source phrase. We can estimate the quality of a translation candidate by using the IBM-1 word alignment probabilities between the source and target phrases. If the candidate is actually a good translation of the source phrase we expect higher IBM-1 probabili"
2007.mtsummit-papers.33,J03-1002,0,0.00323774,"g English NP translations. However, the Arabic parsers available did not produce desired accuracy. Therefore we use Charniak’s parser (Charniak, 2000) to parse English side of the training data. From the resulting parse trees we extract base NPs; i.e. NPs that do not contain other NPs embedded in them. As mentioned in the previous section these NPs are fairly short and are good candidates for a hierarchical system. Arabic 135K 3.5M 145K Sentences Tokens Vocabulary English 135K 4.3M 63K Table 1: Training data statistics We generate IBM model 3 (Brown et al., 1993) alignments by running GIZA++ (Och and Ney, 2003) with the parallel text. GIZA++ training is done for both directions and the word alignments are generated by the intersection of the two. For each English NP, we search the aligned corpus for sentences that contain the NP and read off the alignment as its translation. To compensate for alignment errors we also include partial alignments as follows: We find maximum (max) and minimum (min) Arabic word indices that are aligned to the words in the English NP. All the Arabic words between min and max are considered to be the translation of the English NP. We filter out unbalance NP translation pai"
2007.mtsummit-papers.33,P03-1021,0,0.0302532,"er (Vogel et al., 2003). For our experiments, the decoder uses two translation resources in two levels to generate a hierarchy of phrases. NP translation table is used in the first level to identify possible NPs in the test sentence. NP-tagged phrase table is then used in the next level to build a translation lattice. The decoding process is organized into two steps: 1. 2. Build a translation lattice using all available word/phrase translation resources Find the best combination of partial translations by searching through the lattice In addition, it also performs minimum error-rate training (Och, 2003) to find the best scaling factors for each model used in the decoder. 2.4.1 Building the Translation Lattice The first step in decoding is building the translation lattice. We illustrate this process by using the following Arabic sentence. Note that the Arabic sentence is written from right-to-left. Arabic sentence: اﺑﺮاهﻴﻢ ﻳﺴﺘﻘﺒﻞ ﺿﺎﺑﻂ ﻓﻲ ﺑﻐﺪاد Reference translation: Ibrahim receives Baghdad officer in First the decoder converts the Arabic sentence into a lattice structure where words are attached to the edges (see figure 4a). Next, for each word sequence starting from the left-most node, it"
2007.mtsummit-papers.33,P02-1040,0,0.0861526,"Missing"
2007.mtsummit-papers.33,2005.eamt-1.39,1,0.884469,"Missing"
2007.mtsummit-papers.38,J93-2003,0,0.0334045,"ntained in these resources can also used backwards to help build or improve the lexicon. The system we propose here alternates lexicon building and phrasal alignment. Evaluation on Arabic to English translation showed a statistically significant 1.5 BLEU point improvement. 1 Introduction In data-driven machine translation paradigms such as Statistical Machine Translation (SMT) and Example-Based Machine Translation (EBMT), the lexicon is an essential component since the systems look up translation candidates from the lexicon either as the primary or as the secondary resource. In word-based SMT(Brown et al., 1993b), when an input sentence is given, the system looks up,in the lexicon, candidate translations for each token in the input sentence and then uses fertility and distortion information to to determine the number of translations and their proper placement in a hypothesis sentence. And even in an advanced system such as a phrase-based SMT (Koehn et al., 2003; Vogel et al., 2003) using a phrase table, the lexicon is still a core component which is looked up together with the phrase table. Stephan Vogel Language Technologies Institute 5000 Forbes av. Pittsburgh, PA 15213 stephan.vogel@cs.cmu.edu In"
2007.mtsummit-papers.38,C96-1030,0,0.0582651,"tem looks up,in the lexicon, candidate translations for each token in the input sentence and then uses fertility and distortion information to to determine the number of translations and their proper placement in a hypothesis sentence. And even in an advanced system such as a phrase-based SMT (Koehn et al., 2003; Vogel et al., 2003) using a phrase table, the lexicon is still a core component which is looked up together with the phrase table. Stephan Vogel Language Technologies Institute 5000 Forbes av. Pittsburgh, PA 15213 stephan.vogel@cs.cmu.edu In string-based EBMT (Nirenburg et al., 1994; Brown, 1996), when an input sentence is given, the system first retrieves the longest matches from the stored examples and then, in the lexicon, looks up for the words which don’t have matches. In other EMBT systems (Sumita and Iida, 1991; Veale and Way, 1997), after the closest examples are found, the lexicon is used to find translations for the parts that differ between the retrieved source example and the input sentence. In addition to its use in data-driven methods, a lexicon can also be used in different ways in other machine translation systems. For example, the Context-Based Machine Translation sys"
2007.mtsummit-papers.38,2006.amta-papers.3,0,0.0174521,"n an input sentence is given, the system first retrieves the longest matches from the stored examples and then, in the lexicon, looks up for the words which don’t have matches. In other EMBT systems (Sumita and Iida, 1991; Veale and Way, 1997), after the closest examples are found, the lexicon is used to find translations for the parts that differ between the retrieved source example and the input sentence. In addition to its use in data-driven methods, a lexicon can also be used in different ways in other machine translation systems. For example, the Context-Based Machine Translation system (Carbonell et al., 2006) uses a hand-made lexicon to produce a lattice given an input sentence. Later it uses a large monolingual corpus in the target language to select and place translation tokens properly. Because of the great cost of a hand-built lexicon, it can also be replaced by a statistically generated one. The prevalence of the lexicon in the various machine translation systems above indicates that any improvement in lexicon quality has the potential to make a significant contribution in the field. 1.1 Motivation Word alignment has been a core part in lexicon building while phrasal alignment has been used i"
2007.mtsummit-papers.38,H05-1098,0,0.0143955,"al., 1993a) and HMM Model (Vogel et al., 1996) based on the word-to-word translation assumption. On the other hand, SMT researchers noticed the limitation of the word-to-word assumption and developed phrasal alignment methods. Since word-toword translation cannot convey local reordering and context, they tried to extract phrase pairs based on lexical scores using heuristics. (Och and Ney, 2004) suggested an alignment template method that finds alignment templates by replacing words with their word classes. The word class information was automatically generated by a word clustering algorithm. (Chiang et al., 2005) extracted hierarchical structural alignment information from the word alignments and built grammar-like rules which are used in decoding in his HIERO system. (Koehn, 2004) extracted a phrase table from word alignment and used it in his phrasal decoder directly. While the above systems extract phrase pairs from word alignment information directly, PESA (Vogel, 2005) and SPA (Kim et al., 2005) extract target phrases given any n-gram source phrase on the fly. Both systems as the best target phrase which has the highest bi-directional translation score. 2 System Design Our system was designed as"
2007.mtsummit-papers.38,H91-1026,0,0.194468,"oost both algorithms by using alignment output from the other iteratively. In other words, we feed a word aligner a phrase table built by a phrasal aligner and this word aligner updates the lexicon which will be then fed to the phrase aligner to generate a better phrase table. We repeat these two steps until we don’t observe any more benefit. 1.2 Previous Work There have been many studies on lexicon building. Some researchers have studied non-probabilistic methods which use similarity functions between a source word and a target word and then use a threshold to filter out less reliable pairs (Gale and Church, 1991; Wu and Xia, 1994). Others have studied probabilistic methods such as IBM Models (Brown et al., 1993a) and HMM Model (Vogel et al., 1996) based on the word-to-word translation assumption. On the other hand, SMT researchers noticed the limitation of the word-to-word assumption and developed phrasal alignment methods. Since word-toword translation cannot convey local reordering and context, they tried to extract phrase pairs based on lexical scores using heuristics. (Och and Ney, 2004) suggested an alignment template method that finds alignment templates by replacing words with their word class"
2007.mtsummit-papers.38,2005.eamt-1.21,1,0.834335,"suggested an alignment template method that finds alignment templates by replacing words with their word classes. The word class information was automatically generated by a word clustering algorithm. (Chiang et al., 2005) extracted hierarchical structural alignment information from the word alignments and built grammar-like rules which are used in decoding in his HIERO system. (Koehn, 2004) extracted a phrase table from word alignment and used it in his phrasal decoder directly. While the above systems extract phrase pairs from word alignment information directly, PESA (Vogel, 2005) and SPA (Kim et al., 2005) extract target phrases given any n-gram source phrase on the fly. Both systems as the best target phrase which has the highest bi-directional translation score. 2 System Design Our system was designed as illustrated in Figure 1. The system consists of a lexicon refining system and an evaluation system. The lexicon refining system consists of a Lexicon Builder and a Phrasal Aligner and the evaluation system consists of a Decoder. - Lexicon Builder: This component first finds word-to-word alignments in both directions using IBM Model 1. It then combines them using a union operation at the sente"
2007.mtsummit-papers.38,N03-1017,0,0.0212556,"Missing"
2007.mtsummit-papers.38,koen-2004-pharaoh,0,0.035389,"ption and developed phrasal alignment methods. Since word-toword translation cannot convey local reordering and context, they tried to extract phrase pairs based on lexical scores using heuristics. (Och and Ney, 2004) suggested an alignment template method that finds alignment templates by replacing words with their word classes. The word class information was automatically generated by a word clustering algorithm. (Chiang et al., 2005) extracted hierarchical structural alignment information from the word alignments and built grammar-like rules which are used in decoding in his HIERO system. (Koehn, 2004) extracted a phrase table from word alignment and used it in his phrasal decoder directly. While the above systems extract phrase pairs from word alignment information directly, PESA (Vogel, 2005) and SPA (Kim et al., 2005) extract target phrases given any n-gram source phrase on the fly. Both systems as the best target phrase which has the highest bi-directional translation score. 2 System Design Our system was designed as illustrated in Figure 1. The system consists of a lexicon refining system and an evaluation system. The lexicon refining system consists of a Lexicon Builder and a Phrasal"
2007.mtsummit-papers.38,J04-4002,0,0.043939,"nctions between a source word and a target word and then use a threshold to filter out less reliable pairs (Gale and Church, 1991; Wu and Xia, 1994). Others have studied probabilistic methods such as IBM Models (Brown et al., 1993a) and HMM Model (Vogel et al., 1996) based on the word-to-word translation assumption. On the other hand, SMT researchers noticed the limitation of the word-to-word assumption and developed phrasal alignment methods. Since word-toword translation cannot convey local reordering and context, they tried to extract phrase pairs based on lexical scores using heuristics. (Och and Ney, 2004) suggested an alignment template method that finds alignment templates by replacing words with their word classes. The word class information was automatically generated by a word clustering algorithm. (Chiang et al., 2005) extracted hierarchical structural alignment information from the word alignments and built grammar-like rules which are used in decoding in his HIERO system. (Koehn, 2004) extracted a phrase table from word alignment and used it in his phrasal decoder directly. While the above systems extract phrase pairs from word alignment information directly, PESA (Vogel, 2005) and SPA"
2007.mtsummit-papers.38,2001.mtsummit-papers.68,0,0.0254744,"1 through 7 and Set Training - Source side Training - Target side Development Test # Sentences 19847 19847 500 506 # Tokens 137948 170014 2159 2060 Table 1: Data sets used Figure 2: Log perplexity on the training set 10. The minimum phrase length was always set to 1 in all the cases. For a development set, we used devset2 IWSLT04 of 500 source sentences with 16 references. This set was used in parameter optimization in Minimum Error Rate(MER) training. For an unseen test set, we used devset3 IWSLT05 of 506 source sentences with 16 references. 3.2 Evaluation Metric For evaluation, we used BLEU(Papineni et al., 2001) which is widely used in machine translation evaluation. 4 Results 4.1 Convergence In figure 2, the training set log perplexity converges fast and there is no significant change after the third iteration. Because of this, we limited the number of iterations to three for all the following experiments. Iteration 1 Best-DEV 0.4637 TEST 0.4324 Table 2: Baseline 4.2 The effects of different maximum phrase lengths In table 3, Phrase Table Only and Phrase Table + Original Corpus show the system performance with different maximum phrase lengths when we use only the phrase table and a combination of th"
2007.mtsummit-papers.38,P91-1024,0,0.067611,"pothesis sentence. And even in an advanced system such as a phrase-based SMT (Koehn et al., 2003; Vogel et al., 2003) using a phrase table, the lexicon is still a core component which is looked up together with the phrase table. Stephan Vogel Language Technologies Institute 5000 Forbes av. Pittsburgh, PA 15213 stephan.vogel@cs.cmu.edu In string-based EBMT (Nirenburg et al., 1994; Brown, 1996), when an input sentence is given, the system first retrieves the longest matches from the stored examples and then, in the lexicon, looks up for the words which don’t have matches. In other EMBT systems (Sumita and Iida, 1991; Veale and Way, 1997), after the closest examples are found, the lexicon is used to find translations for the parts that differ between the retrieved source example and the input sentence. In addition to its use in data-driven methods, a lexicon can also be used in different ways in other machine translation systems. For example, the Context-Based Machine Translation system (Carbonell et al., 2006) uses a hand-made lexicon to produce a lattice given an input sentence. Later it uses a large monolingual corpus in the target language to select and place translation tokens properly. Because of th"
2007.mtsummit-papers.38,C96-2141,1,0.64208,"phrasal aligner and this word aligner updates the lexicon which will be then fed to the phrase aligner to generate a better phrase table. We repeat these two steps until we don’t observe any more benefit. 1.2 Previous Work There have been many studies on lexicon building. Some researchers have studied non-probabilistic methods which use similarity functions between a source word and a target word and then use a threshold to filter out less reliable pairs (Gale and Church, 1991; Wu and Xia, 1994). Others have studied probabilistic methods such as IBM Models (Brown et al., 1993a) and HMM Model (Vogel et al., 1996) based on the word-to-word translation assumption. On the other hand, SMT researchers noticed the limitation of the word-to-word assumption and developed phrasal alignment methods. Since word-toword translation cannot convey local reordering and context, they tried to extract phrase pairs based on lexical scores using heuristics. (Och and Ney, 2004) suggested an alignment template method that finds alignment templates by replacing words with their word classes. The word class information was automatically generated by a word clustering algorithm. (Chiang et al., 2005) extracted hierarchical st"
2007.mtsummit-papers.38,2003.mtsummit-papers.53,1,0.885508,"T) and Example-Based Machine Translation (EBMT), the lexicon is an essential component since the systems look up translation candidates from the lexicon either as the primary or as the secondary resource. In word-based SMT(Brown et al., 1993b), when an input sentence is given, the system looks up,in the lexicon, candidate translations for each token in the input sentence and then uses fertility and distortion information to to determine the number of translations and their proper placement in a hypothesis sentence. And even in an advanced system such as a phrase-based SMT (Koehn et al., 2003; Vogel et al., 2003) using a phrase table, the lexicon is still a core component which is looked up together with the phrase table. Stephan Vogel Language Technologies Institute 5000 Forbes av. Pittsburgh, PA 15213 stephan.vogel@cs.cmu.edu In string-based EBMT (Nirenburg et al., 1994; Brown, 1996), when an input sentence is given, the system first retrieves the longest matches from the stored examples and then, in the lexicon, looks up for the words which don’t have matches. In other EMBT systems (Sumita and Iida, 1991; Veale and Way, 1997), after the closest examples are found, the lexicon is used to find transl"
2007.mtsummit-papers.38,2005.mtsummit-papers.33,1,0.754294,". (Och and Ney, 2004) suggested an alignment template method that finds alignment templates by replacing words with their word classes. The word class information was automatically generated by a word clustering algorithm. (Chiang et al., 2005) extracted hierarchical structural alignment information from the word alignments and built grammar-like rules which are used in decoding in his HIERO system. (Koehn, 2004) extracted a phrase table from word alignment and used it in his phrasal decoder directly. While the above systems extract phrase pairs from word alignment information directly, PESA (Vogel, 2005) and SPA (Kim et al., 2005) extract target phrases given any n-gram source phrase on the fly. Both systems as the best target phrase which has the highest bi-directional translation score. 2 System Design Our system was designed as illustrated in Figure 1. The system consists of a lexicon refining system and an evaluation system. The lexicon refining system consists of a Lexicon Builder and a Phrasal Aligner and the evaluation system consists of a Decoder. - Lexicon Builder: This component first finds word-to-word alignments in both directions using IBM Model 1. It then combines them using a u"
2007.mtsummit-papers.38,1994.amta-1.26,0,0.0703492,"using alignment output from the other iteratively. In other words, we feed a word aligner a phrase table built by a phrasal aligner and this word aligner updates the lexicon which will be then fed to the phrase aligner to generate a better phrase table. We repeat these two steps until we don’t observe any more benefit. 1.2 Previous Work There have been many studies on lexicon building. Some researchers have studied non-probabilistic methods which use similarity functions between a source word and a target word and then use a threshold to filter out less reliable pairs (Gale and Church, 1991; Wu and Xia, 1994). Others have studied probabilistic methods such as IBM Models (Brown et al., 1993a) and HMM Model (Vogel et al., 1996) based on the word-to-word translation assumption. On the other hand, SMT researchers noticed the limitation of the word-to-word assumption and developed phrasal alignment methods. Since word-toword translation cannot convey local reordering and context, they tried to extract phrase pairs based on lexical scores using heuristics. (Och and Ney, 2004) suggested an alignment template method that finds alignment templates by replacing words with their word classes. The word class"
2007.mtsummit-papers.38,2004.tmi-1.9,1,0.405139,"ginal Corpus TEST 0.4178 0.4286 0.4313 0.4224 0.4318 0.4381 0.4428 0.4446 0.4441 0.4491 0.4456 0.4388 0.4462 0.4451 0.4428 0.4431 Best-DEV 0.4552 0.4584 0.4650 0.4596 0.4673 0.4678 0.4744 0.4691 0.4697 0.4691 0.4671 0.4716 0.4732 0.4721 0.4758 0.4768 TEST 0.4431 0.4385 0.4305 0.4301 0.4287 0.4369 0.4438 0.4467 0.4405 0.4484 0.4395 0.4395 0.4445 0.4430 0.4379 0.4443 Table 3: Comparison of two different inputs for lexicon builder ues (4 or more). This improvement, on both BestDEV and TEST, is significant, as attested by significance testing using bootstrapping for NIST/BLEU confidence intervals(Zhang and Vogel, 2004). The reason why we had score drops at maximum phrase length 1 and 2 is discussed in section 5. Overall, we see score improvement on both Best-DEV and TEST and the test set improvement is more than 1.5 BLEU points. In the case of Phrase Table + Original Corpus, we see improvement when the maximum phrase length is 1. This time, we use the original corpus together with the phrase table and this mitigates the effect of errors in the phrase table. But we also see performance degradation when the maximum phrase length is 2 and this is also discussed in section 5. We have a slightly better score tha"
2007.mtsummit-papers.72,J90-2002,0,0.649389,"Missing"
2007.mtsummit-papers.72,P05-1032,0,0.0228048,"Missing"
2007.mtsummit-papers.72,2007.mtsummit-papers.12,1,0.563462,"Missing"
2007.mtsummit-papers.72,P05-1033,0,0.0196931,"nerated in a context-free manner. At each step, a non-terminal X can generate its span in two ways: either straight: X → < f1 f 2 , e1e2 > , or inverted: X → < f1 f 2 , e2e1 > , where e1 is the translation for f1 and e2 for f2. Even though it is quite simple and straightforward, ITG has been shown to have high expressiveness. In other words, most of the reordering patterns in natural language translation can be expressed by ITG. The ITG-style decoding in the PanDoRA system is a CKY parser with beam search. The idea of translating by parsing is similar to the approach used in the Hiero system (Chiang, 2005). Given a source sentence f, the decoder finds the best derivation that generates <f, e> for some e. Unlike the monotone decoder which works on the source sentence from left to right, the CKY parser works bottomup starting with spans of length 1. While moving up the parsing chart, the decoder adds new partial hypotheses to cell [j1, j2] in the chart table if: 1. there is an entry in the translation table where the j source phrase is f j , then add the corresponding translation as a partial hypothesis; or, 2. there exist a partial hypothesis h1 covering the subspans (j1, k) and h2 covering (k+1"
2007.mtsummit-papers.72,W02-1001,0,0.00669522,"e model plays an important role in deciding the correct reordering pattern during decoding, we need to explicitly model the “negative examples” to prevent those ungrammatical n-grams from being generated. In addition to the standard n-gram language model, we use a discriminative language model to alleviate the limitations of the generative LM. Discriminative training has been shown to improve the translation quality (Liang et al., 2006). The idea of using an “anti-language model” has also been tried in speech recognition (Stolcke et al., 2000). We use the perceptron algorithm as described in (Collins, 2002) to train a discriminative language model. Given the current translation model and generative language model, we translate the source side of the bilingual training corpus f into e’. Unlike Example-based Machine Translation (EBMT) systems, SMT systems usually can not reproduce the same translation as used in the training data, thus the target side of the training corpus e is usually different from e’. We enumerate all the n-grams from the union of e and e’. For each n-gram, we increase the n-gram’s weight if its frequency in e’ is less than its frequency in e and decrease its weight if it has"
2007.mtsummit-papers.72,2005.iwslt-1.1,0,0.0206795,"two shorter ones, the language model probability of the new hypothesis can be estimated from the LM probabilities of the shorter ones with some adjustment based on the words across the boundaries. This makes the language model probability estimation efficient compared to the naive way of calculating the LM probability for all the words in the hypothesis when a new hypothesis is created. With ITG-style reordering decoding, the qualities are significantly improved for Japanese↔English translation as shown in the next section. Experiments We evaluate the performance of PanDoRA on the IWSLT 2005 (Eck and Hori, 2005) Arabic→English and the Japanese↔English test sets. Both BLEU (Papineni, 2001) and NIST (NIST, 2003) metrics are used to evaluate the translation quality. PanDoRA runs on a HP iPAQ hx2700 series Pocket PC. hx2700 models are powered by the Intel PXA270 processor with a frequency at 624 MHz. The system has 256 MB total memory (192 MB ROM and 64 MB SDRAM) that includes up to 144 MB user available persistent storage memory. We used one 1GB SD card to store the TM/LM models. Arabic-English Experiments For Arabic (A) → English (E) system, the training data is from the Basic Travel Expression Corpus"
2007.mtsummit-papers.72,W06-3113,0,0.0279403,"Missing"
2007.mtsummit-papers.72,N03-1017,0,0.00996293,"Missing"
2007.mtsummit-papers.72,koen-2004-pharaoh,0,0.0303593,"card to store the TM/LM models. Arabic-English Experiments For Arabic (A) → English (E) system, the training data is from the Basic Travel Expression Corpus (BTEC), which contains 20,000 Arabic/English sentence pairs for the travel domain (Table 3.). The development data (500 Arabic sentences) and testing data (506 sentences) are drawn from the same domain, each with 16 reference translations. Word Tokens Word Types Sentences Avg. Sent. Len. Arabic 130K 18K 20K 6.5 words English 154K 6.9K 20K 7.7 words 2 1 Table 3. Statistics of the BTEC Ar./En training data We used tools provided by Pharaoh (Koehn, 2004) to extract the phrase translation pairs from the corpus. The Arabic to English phrase table has about 155K translation pairs (Table 4). Ar/En Pairs Uniq. Arabic Phrases Uniq. English Phrases 155,825 137,836 122,460 Table 4. Arabic to English phrase translation model The English language model is a 3-gram LM trained from the English side of the bilingual corpus using the SRI-LM toolkit (Stolcke, 2002). All the models are converted into compact data structure as described in the previous section. The complete model is of 6.2MB when stored on disk. Table 5. shows the translation results and spee"
2007.mtsummit-papers.72,P06-1096,0,0.0121864,"will assign low probabilities to them. But assigning a low probability to “the </s>” or “<s> ga” can not prevent them from being generated by the SMT decoder. In PanDoRA, language model plays an important role in deciding the correct reordering pattern during decoding, we need to explicitly model the “negative examples” to prevent those ungrammatical n-grams from being generated. In addition to the standard n-gram language model, we use a discriminative language model to alleviate the limitations of the generative LM. Discriminative training has been shown to improve the translation quality (Liang et al., 2006). The idea of using an “anti-language model” has also been tried in speech recognition (Stolcke et al., 2000). We use the perceptron algorithm as described in (Collins, 2002) to train a discriminative language model. Given the current translation model and generative language model, we translate the source side of the bilingual training corpus f into e’. Unlike Example-based Machine Translation (EBMT) systems, SMT systems usually can not reproduce the same translation as used in the training data, thus the target side of the training corpus e is usually different from e’. We enumerate all the"
2007.mtsummit-papers.72,P02-1038,0,0.0497497,"l is subject to the storage capacity limitation of the device running the system. The remainder of this paper is organized as follows: we first describe the general concepts of phrase-based statistical machine translation systems and then we introduce the PanDoRA system and its major components. We show the performance of PanDoRA system running on a PDA with standard training/testing data sets and discuss the results in the experiments section. Another alternative to the classical source-channel approach is the direct modeling of the posterior probability P (e |f ) using the log-linear model (Och and Ney, 2002): Phrase-based Statistical Machine Translation Each φ m is a feature function that estimates some feature values from (e, f). The two knowledge sources used in the classical source-channel approaches can be converted into two feature functions such that: In statistical machine translation (SMT), we are given a J source language sentence f1 = f1 ... f j ... f J , which is to be translated into a target language sentence e1I = e1...ei ...eI . Among all possible target language sentences, the decoder will choose the one with the highest probability such that the output translation: e* = arg max P"
2007.mtsummit-papers.72,W99-0604,0,0.0398153,"the source-channel approach which allows us to make use of two types of knowledge sources: translation model (TM) P ( f |e) and language model (LM) P(e) . TM models how likely a source sentence is the translation of the target sentence and LM describes the well-formedness of the generated translation. The original SMT work described in Brown et al. (1990) models the translation process as a word-to-word mapping. In recent years, various approaches have been developed to use phrase-to-phrase translation models to encapsulate more local context inside the phrases during the translation process (Och et al. 1999; Zhang et al. 2003; Koehn et al. 2003; Vogel 2005). The so-called “phrases” are not linguistically motivated and they could be n-grams running across linguistic constituent boundaries such as phrase “the spokesman said today at.” Phrase-based SMT systems outperform word-based systems and -- despite their lack of linguistic grounding -- have become one of the dominant approaches in machine translation research. Figure 2 shows some examples of Arabic→English phrase translation pairs extracted automatically from the bilingual training data using the PESA method (Vogel 2005).  احتفال المدرسة# s"
2007.mtsummit-papers.72,P03-1021,0,0.0153876,"ly needs to calculate the numerator part in Eq. 2 to search for the optimal translation e* for f. Under the log-linear model, we could convert the translation model, language model, distortion model, sentence length model and other models into feature functions. This allows us to incorporate more knowledge sources than is the case in the classical source-channel approach. The weights for each feature function are trained using the Minimum Error (MER) optimization on the development set. MER optimizes the feature weights to minimize the errors, or equivalently, maximizing the BLEU/NIST scores (Och, 2003). PanDoRA System Based on the general concepts of phrase-based SMT, PanDoRA is engineered from scratch to cope with the limitations on hand-held devices. The code base for PanDoRA is completely different from our phrase-based SMT system for PC platforms. Compact Data Structure When running SMT systems on PCs, we usually load all models into memory. The size of phrase-based SMT models can become very large when the training data size increases, or when we consider longer phrases in the translation model. Callison-Burch(2005) estimates that if we consider phrases up to 10 words long, storing all"
2007.mtsummit-papers.72,2001.mtsummit-papers.68,0,0.0934542,"Missing"
2007.mtsummit-papers.72,2003.mtsummit-papers.53,1,0.910826,"e generated translation results towards the reference translation. Decoding Given a testing sentence, PanDoRA applies the translation model on the sentence and builds a translation lattice. The decoder then searches in this lattice for the optimal path as the output translation for the input sentence. PanDoRA implements two types of search method in its decoder: a left-to-right monotone decoding and a bottomup CKY-parsing using the Inverted Transduction Grammar (ITG, Wu 1997). Monotone Decoding The monotone decoding in the PanDoRA system is a beam search decoder based on the idea described in Vogel et al. (2003). Once the complete translation lattice has been built, a best-first search through this lattice is performed. In addition to the translation costs, the language model costs are added and the path which minimizes the combined cost is returned. Starting with a special begin-of-sentence hypothesis attached to the first node in the translation lattice, hypotheses are expanded over all outgoing edges from the current node. The decoder allows for recombination of hypotheses in a flexible way. It is important to keep hypotheses apart if the partial translations end in different words, as this will r"
2007.mtsummit-papers.72,2005.mtsummit-papers.33,1,0.85311,"of two types of knowledge sources: translation model (TM) P ( f |e) and language model (LM) P(e) . TM models how likely a source sentence is the translation of the target sentence and LM describes the well-formedness of the generated translation. The original SMT work described in Brown et al. (1990) models the translation process as a word-to-word mapping. In recent years, various approaches have been developed to use phrase-to-phrase translation models to encapsulate more local context inside the phrases during the translation process (Och et al. 1999; Zhang et al. 2003; Koehn et al. 2003; Vogel 2005). The so-called “phrases” are not linguistically motivated and they could be n-grams running across linguistic constituent boundaries such as phrase “the spokesman said today at.” Phrase-based SMT systems outperform word-based systems and -- despite their lack of linguistic grounding -- have become one of the dominant approaches in machine translation research. Figure 2 shows some examples of Arabic→English phrase translation pairs extracted automatically from the bilingual training data using the PESA method (Vogel 2005).  احتفال المدرسة# school festival is # 0.0034  احتفال المدرسة# schoo"
2007.mtsummit-papers.72,P03-2023,0,0.0201358,"s a hierarchical tree structure to the reorganized sentence as predicted by the statistical model. Next, high level semantic translation is performed by the NLU module. The system uses a bilingual dictionary for the domain. To decrease the memory requirement, the size of the dictionary is cut to 9K entries from English to Chinese and 15K entries from Chinese to English. Zhou et al. (2006) introduces FOLSOM system: a phrasebased statistical machine translation system using weighted finite-state transducers (WFST). FOLSOM is applied in real-time speech translation on scalable computing devices. Yamabana et al. (2003) used a client-server approach for a mobile speech to speech translation system. The handheld device is treated as a client, and it sends the compressed speech via Wi-Fi (IEEE 802.11b) to the translation server. The entire speech-to-speech translation process is conducted on the server side, and the translated speech in the target language is later sent back to the client. Waibel et al. (2003) developed an interlingua-based two way translation system on a consumer PDA that translates between English and Egyptian Arabic. The developed prototype is limited. It was aimed at medical interviews, an"
2007.tmi-papers.21,P06-1067,0,0.032559,"striction to word reordering was introduced in (Wu, 95). The ITG (inverse transduction grammar) constraint allows only reorderings, which can be generated by swapping subtrees in a binary branching tree. Still, for longer sentences the number of possible reorderings is too large to be enumerated; severe pruning is necessary. To make the distortion models more informative the aligned positions can be conditioned on the length of the sentences, on the words (lexicalized distortion models), or on word classes (parts-of-speech) or automatically generated word classes, using clustering techniques (Al-Onaizan and Papineno, 2006). State-of-the-art SMT systems use phrases. One advantage is that phrases can capture some of the local reordering patterns. However, this is rather limited as the average length of matching phrases is typically less then two words. To capture longer ranging word reorderings these phrases need to be reordered, which brings us back to the central questions: • How to model word reordering? • How to estimate the parameters of the model? • How to apply the model at translation (decoding) time? These questions will –at least to some extent– be dealt with in subsequent sections. 2.1 Related Work Dif"
2007.tmi-papers.21,2005.iwslt-1.8,0,0.613577,"found in (Zens and Ney, 2003). They have in common that they do not use any syntactic or lexical information, therefore they rely on a strong language model or on long phrases to get the right word order. Other approaches were introduced that use more linguistic knowledge, for example the use of bitext grammars that allow parsing the source and target language (Wu, 1997). In (Shen et al., 2004) and (Och et al., 2004) syntactic information was used to rerank the output of a translation system with the idea of accounting for different reordering at this stage. In (Tillmann and Zhang, 2005) and (Koehn et al., 2005) a lexicalised block-oriented reordering model is proposed that decides for a given 172 phrase whether the next phrase should be oriented to its left or right. The most recent and very promising approaches that have been demonstrated, reorder the source sentences based on rules learned from an aligned training corpus with a POS-tagged source side (Chen et al., 2006), (Popovic and Ney, 2006) and (Crego and Marino, 2006). These rules are then used to reorder the word sequence in the most likely way. 3 Syntactic Reordering Rules In our approach we follow the idea proposed in (Crego and Marino, 20"
2007.tmi-papers.21,P00-1056,0,0.160842,"Missing"
2007.tmi-papers.21,N04-1021,0,0.0628454,"Missing"
2007.tmi-papers.21,P03-1021,0,0.0636616,"text are favored. The monotone path however, gets the minimum of all scores computed for the monotone path over the different context rules. 4 Experiments To study the effect of the POS-based distortion model we did a number of experiments on German-to-English, English-to-German, and English-to-Spanish translation tasks. We used the European Parliament Speeches Corpus as used in the TC-Star1 project and the SMT-Workshop evaluations. Some details of the corpus are given in Table 2. Here train-xx is the complete training corpus, dev-xx denotes the development test set used for the MER-training (Och, 2003), and eval-xx is the unseen test set used for evaluation. In the case of 1 http://www.tc-star.org 174 Slovak 0.9 by 1 0 Mr 1 1 Wiersma 1 2 3 on 1 4 the 1 5 6 Republic 1 Slovak 1 Republic 0.1 8 ;1 9 7 Figure 1: Example for a very small reordering lattice carefully 0.1 23 to 1.0 24 this 1.0 25 proposal 1.0 26 listened 1.0 proposal 1.0 has 0.2 0 Madam 1.0 1 President 1.0 2 , 1.0 3 7 my 1.0 8 group 1.0 group 1.0 my 0.8 4 has 0.3 group 0.7 6 9 carefully 0.1 19 carefully 0.1 16 to 1.0 listened 0.1 carefully 0.1 10 13 17 has 1.0 listened 1.0 this 1.0 listened 1.0 21 listened 1.0 this 1.0 27 . 1.0 28"
2007.tmi-papers.21,popovic-ney-2006-pos,0,0.24131,", 2004) and (Och et al., 2004) syntactic information was used to rerank the output of a translation system with the idea of accounting for different reordering at this stage. In (Tillmann and Zhang, 2005) and (Koehn et al., 2005) a lexicalised block-oriented reordering model is proposed that decides for a given 172 phrase whether the next phrase should be oriented to its left or right. The most recent and very promising approaches that have been demonstrated, reorder the source sentences based on rules learned from an aligned training corpus with a POS-tagged source side (Chen et al., 2006), (Popovic and Ney, 2006) and (Crego and Marino, 2006). These rules are then used to reorder the word sequence in the most likely way. 3 Syntactic Reordering Rules In our approach we follow the idea proposed in (Crego and Marino, 2006) of using a parallel training corpus with a tagged source side to extract rules which allow a reordering before the translation task. By doing it this way we are able to keep the translation process in the decoder monotone and make it significantly faster compared to allowing reorderings in the decoder. To avoid making any hard decisions in reordering the source side we use a lattice str"
2007.tmi-papers.21,N04-1023,0,0.0224885,"ings at decoding time (Berger et al., 1996). In (Wu, 1996) the alignment model already introduces restrictions in word order, which leads also to restrictions at decoding time. A comparison of these two approaches can be found in (Zens and Ney, 2003). They have in common that they do not use any syntactic or lexical information, therefore they rely on a strong language model or on long phrases to get the right word order. Other approaches were introduced that use more linguistic knowledge, for example the use of bitext grammars that allow parsing the source and target language (Wu, 1997). In (Shen et al., 2004) and (Och et al., 2004) syntactic information was used to rerank the output of a translation system with the idea of accounting for different reordering at this stage. In (Tillmann and Zhang, 2005) and (Koehn et al., 2005) a lexicalised block-oriented reordering model is proposed that decides for a given 172 phrase whether the next phrase should be oriented to its left or right. The most recent and very promising approaches that have been demonstrated, reorder the source sentences based on rules learned from an aligned training corpus with a POS-tagged source side (Chen et al., 2006), (Popovic"
2007.tmi-papers.21,J96-1002,0,0.0461665,"her limited as the average length of matching phrases is typically less then two words. To capture longer ranging word reorderings these phrases need to be reordered, which brings us back to the central questions: • How to model word reordering? • How to estimate the parameters of the model? • How to apply the model at translation (decoding) time? These questions will –at least to some extent– be dealt with in subsequent sections. 2.1 Related Work Different approaches have been developed to deal with the word order problem. First approaches worked by constraining reorderings at decoding time (Berger et al., 1996). In (Wu, 1996) the alignment model already introduces restrictions in word order, which leads also to restrictions at decoding time. A comparison of these two approaches can be found in (Zens and Ney, 2003). They have in common that they do not use any syntactic or lexical information, therefore they rely on a strong language model or on long phrases to get the right word order. Other approaches were introduced that use more linguistic knowledge, for example the use of bitext grammars that allow parsing the source and target language (Wu, 1997). In (Shen et al., 2004) and (Och et al., 2004) s"
2007.tmi-papers.21,P05-1069,0,0.0158424,"of these two approaches can be found in (Zens and Ney, 2003). They have in common that they do not use any syntactic or lexical information, therefore they rely on a strong language model or on long phrases to get the right word order. Other approaches were introduced that use more linguistic knowledge, for example the use of bitext grammars that allow parsing the source and target language (Wu, 1997). In (Shen et al., 2004) and (Och et al., 2004) syntactic information was used to rerank the output of a translation system with the idea of accounting for different reordering at this stage. In (Tillmann and Zhang, 2005) and (Koehn et al., 2005) a lexicalised block-oriented reordering model is proposed that decides for a given 172 phrase whether the next phrase should be oriented to its left or right. The most recent and very promising approaches that have been demonstrated, reorder the source sentences based on rules learned from an aligned training corpus with a POS-tagged source side (Chen et al., 2006), (Popovic and Ney, 2006) and (Crego and Marino, 2006). These rules are then used to reorder the word sequence in the most likely way. 3 Syntactic Reordering Rules In our approach we follow the idea proposed"
2007.tmi-papers.21,J95-4004,0,0.0630389,"ev-en dev-de eval-en eval-de train-en train-es dev-en eval-en Sentences 1.2M 1.2M 2K 2K 2K 2K 1.2M 1.2M 1.2K 1.1K Words 35M 33M 58K 54K 58K 55K 33M 34M 30K 30K Voc/OOV 97K 298K 6103 / 62 8762 / 306 6246 / 250 9008 / 551 94K 135K 4084 / 79 4100 / 105 Table 2: Corpus statistics EPPS training and test corpora. German ↔ English translation the evaluation is based on 1 reference, for English → Spanish on 2 references. For the alignment and the phrase extraction we used the Pharaoh training package (Koehn et al., 2005). To tag the corpora we used the following taggers: for English the Brill tagger (Brill, 1995) with a tag set size of 36 and for German the Stuttgart tree-tagger with a tag set size of 57 tags (Schmid, 1994). From the training corpora and the POS tagged source side we extracted the reordering rules according to the method described in Section 3.1. For the experiments reported in this paper we only learned rules up to a length of 15, since longer rules do not occur often enough in the training corpus. Table 3 displays the counts of rules that consist only of the tag sequence and those that use additional context with the tag to the left and the tag to the right learned from the training"
2007.tmi-papers.21,C96-2141,1,0.690868,"nslation from English into German, where arrive needs to generate both ’arrive’ and ’an’ at different positions in the target sentence. To generate the correct word sequence the translation system needs to have strong, restricting evidence of how to rearrange the words, this is the approach taken in grammar-based systems, or it has to have weak evidence in the form of probabilities, and then test all (or at least a large number) of reorderings, as is the strategy in typical phrase-based statistical translation systems. The well-known IBM and HMM word alignment models (Brown et al., 1993) and (Vogel et al., 1996) contain as one component a so-called distortion model to capture the different word orders in different languages. These distortion models can be formulated in terms of absolute positions, as in the IBM2 model, or in terms of relative positions, as in the HMM and IBM4 alignment models. These distortion models are rather weak. They essentially boil down to saying that long distance reorderings are less likely then short distance reorderings. It is important to notice that these distortion models do not pose any restrictions as to which reorderings are possible. At decoding time all permutation"
2007.tmi-papers.21,J93-2003,0,0.0261397,"more difficult is the translation from English into German, where arrive needs to generate both ’arrive’ and ’an’ at different positions in the target sentence. To generate the correct word sequence the translation system needs to have strong, restricting evidence of how to rearrange the words, this is the approach taken in grammar-based systems, or it has to have weak evidence in the form of probabilities, and then test all (or at least a large number) of reorderings, as is the strategy in typical phrase-based statistical translation systems. The well-known IBM and HMM word alignment models (Brown et al., 1993) and (Vogel et al., 1996) contain as one component a so-called distortion model to capture the different word orders in different languages. These distortion models can be formulated in terms of absolute positions, as in the IBM2 model, or in terms of relative positions, as in the HMM and IBM4 alignment models. These distortion models are rather weak. They essentially boil down to saying that long distance reorderings are less likely then short distance reorderings. It is important to notice that these distortion models do not pose any restrictions as to which reorderings are possible. At deco"
2007.tmi-papers.21,2003.mtsummit-papers.53,1,0.833189,"sh translations, using the European Parliament Plenary Sessions corpus. 1 Introduction Statistical machine translation (SMT) is currently the most promising approach to large vocabulary text translation. In the spirit of the Candide system developed in the early 90s at IBM (Brown et Stephan Vogel InterACT Language Technologies Institute Carnegie Mellon University 5000 Forbes Av. Pittsburgh, PA 15213 vogel+@cs.cmu.edu al., 1993), a number of statistical machine translation systems have been presented in the last few years (Wang and Waibel, 98), (Och and Ney., 2000), (Yamada and Knight, 2000), (Vogel et al., 2003). These systems share the basic underlying principles of applying a translation model to capture the lexical and word reordering relationships between two languages, complemented by a target language model to drive the search process through translation model hypotheses. The reordering of words in machine translation still remains one of the hardest problems. Here we will describe our approach using syntax-based reordering rules to create a lattice structure for test sentences that encodes all word reorderings consistent with the reordering rules learned from a word aligned training corpus. 2"
2007.tmi-papers.21,2006.iwslt-papers.4,0,0.0294697,"997). In (Shen et al., 2004) and (Och et al., 2004) syntactic information was used to rerank the output of a translation system with the idea of accounting for different reordering at this stage. In (Tillmann and Zhang, 2005) and (Koehn et al., 2005) a lexicalised block-oriented reordering model is proposed that decides for a given 172 phrase whether the next phrase should be oriented to its left or right. The most recent and very promising approaches that have been demonstrated, reorder the source sentences based on rules learned from an aligned training corpus with a POS-tagged source side (Chen et al., 2006), (Popovic and Ney, 2006) and (Crego and Marino, 2006). These rules are then used to reorder the word sequence in the most likely way. 3 Syntactic Reordering Rules In our approach we follow the idea proposed in (Crego and Marino, 2006) of using a parallel training corpus with a tagged source side to extract rules which allow a reordering before the translation task. By doing it this way we are able to keep the translation process in the decoder monotone and make it significantly faster compared to allowing reorderings in the decoder. To avoid making any hard decisions in reordering the source"
2007.tmi-papers.21,J82-2005,0,0.306737,"Missing"
2007.tmi-papers.21,P96-1021,0,0.0584317,"length of matching phrases is typically less then two words. To capture longer ranging word reorderings these phrases need to be reordered, which brings us back to the central questions: • How to model word reordering? • How to estimate the parameters of the model? • How to apply the model at translation (decoding) time? These questions will –at least to some extent– be dealt with in subsequent sections. 2.1 Related Work Different approaches have been developed to deal with the word order problem. First approaches worked by constraining reorderings at decoding time (Berger et al., 1996). In (Wu, 1996) the alignment model already introduces restrictions in word order, which leads also to restrictions at decoding time. A comparison of these two approaches can be found in (Zens and Ney, 2003). They have in common that they do not use any syntactic or lexical information, therefore they rely on a strong language model or on long phrases to get the right word order. Other approaches were introduced that use more linguistic knowledge, for example the use of bitext grammars that allow parsing the source and target language (Wu, 1997). In (Shen et al., 2004) and (Och et al., 2004) syntactic inform"
2007.tmi-papers.21,J97-3002,0,0.086974,"raining reorderings at decoding time (Berger et al., 1996). In (Wu, 1996) the alignment model already introduces restrictions in word order, which leads also to restrictions at decoding time. A comparison of these two approaches can be found in (Zens and Ney, 2003). They have in common that they do not use any syntactic or lexical information, therefore they rely on a strong language model or on long phrases to get the right word order. Other approaches were introduced that use more linguistic knowledge, for example the use of bitext grammars that allow parsing the source and target language (Wu, 1997). In (Shen et al., 2004) and (Och et al., 2004) syntactic information was used to rerank the output of a translation system with the idea of accounting for different reordering at this stage. In (Tillmann and Zhang, 2005) and (Koehn et al., 2005) a lexicalised block-oriented reordering model is proposed that decides for a given 172 phrase whether the next phrase should be oriented to its left or right. The most recent and very promising approaches that have been demonstrated, reorder the source sentences based on rules learned from an aligned training corpus with a POS-tagged source side (Chen"
2007.tmi-papers.21,P03-1019,0,0.0199675,"ions: • How to model word reordering? • How to estimate the parameters of the model? • How to apply the model at translation (decoding) time? These questions will –at least to some extent– be dealt with in subsequent sections. 2.1 Related Work Different approaches have been developed to deal with the word order problem. First approaches worked by constraining reorderings at decoding time (Berger et al., 1996). In (Wu, 1996) the alignment model already introduces restrictions in word order, which leads also to restrictions at decoding time. A comparison of these two approaches can be found in (Zens and Ney, 2003). They have in common that they do not use any syntactic or lexical information, therefore they rely on a strong language model or on long phrases to get the right word order. Other approaches were introduced that use more linguistic knowledge, for example the use of bitext grammars that allow parsing the source and target language (Wu, 1997). In (Shen et al., 2004) and (Och et al., 2004) syntactic information was used to rerank the output of a translation system with the idea of accounting for different reordering at this stage. In (Tillmann and Zhang, 2005) and (Koehn et al., 2005) a lexical"
2007.tmi-papers.21,W07-0401,0,0.182812,"are then used to reorder the word sequence in the most likely way. 3 Syntactic Reordering Rules In our approach we follow the idea proposed in (Crego and Marino, 2006) of using a parallel training corpus with a tagged source side to extract rules which allow a reordering before the translation task. By doing it this way we are able to keep the translation process in the decoder monotone and make it significantly faster compared to allowing reorderings in the decoder. To avoid making any hard decisions in reordering the source side we use a lattice structure as input (Crego and Marino, 2006), (Zhang et al., 2007) for our decoder. Lattices are created for the source sentences and contain all the possible reorderings and of course also the original word sequence. As a new feature we use the context in which a reordering pattern is seen in the training data. Context refers to the words or tags to the left or to the right of the sequence for which a reordering has been observed. By doing this we hope to differentiate between reorderings that are dependent on their context. 3.1 Learning Reordering Rules The rules that are later applied to the source sentences are learned via an aligned corpus for which the"
2008.amta-papers.18,2008.amta-papers.18,1,0.106103,"Missing"
2008.amta-papers.18,W06-3105,0,0.0471548,"Missing"
2008.amta-papers.18,P08-1115,0,0.150282,"Missing"
2008.amta-papers.18,P08-1112,0,0.107632,"Missing"
2008.amta-papers.18,N03-1017,0,0.0707978,"Missing"
2008.amta-papers.18,W02-1018,0,0.0820035,"Missing"
2008.amta-papers.18,W06-1606,0,0.0455186,"Missing"
2008.amta-papers.18,P08-1023,0,0.111838,"Missing"
2008.amta-papers.18,J93-2003,0,0.0142579,"Missing"
2008.amta-papers.18,A00-2018,0,0.0833625,"Missing"
2008.amta-papers.18,P05-1033,0,0.0186015,"Missing"
2008.amta-papers.18,P03-1021,0,0.063759,"Missing"
2008.amta-papers.18,P02-1040,0,0.103367,"Missing"
2008.amta-papers.18,W06-1608,0,0.0844503,"Missing"
2008.amta-papers.18,N07-1063,1,0.905406,"Missing"
2008.amta-papers.18,J97-3002,0,0.0508368,"Missing"
2008.amta-papers.18,W06-3119,1,0.0891131,"Missing"
2008.amta-papers.18,C08-1144,1,0.680453,"Missing"
2008.amta-papers.18,J04-4002,0,\N,Missing
2008.amta-papers.18,J03-1002,0,\N,Missing
2008.amta-papers.18,2006.iwslt-evaluation.1,0,\N,Missing
2008.amta-papers.18,J07-2003,0,\N,Missing
2008.amta-srw.3,2008.amta-srw.3,1,0.106149,"Missing"
2008.amta-srw.3,J90-2002,0,0.815965,"Missing"
2008.amta-srw.3,N07-2015,0,0.172956,"Missing"
2008.amta-srw.3,D07-1029,0,0.0608163,"Missing"
2008.amta-srw.3,P05-3026,0,0.199469,"Missing"
2008.amta-srw.3,J03-1002,0,0.00361753,"Missing"
2008.amta-srw.3,P02-1040,0,0.107864,"Missing"
2008.amta-srw.3,N07-1029,0,0.360449,"Missing"
2008.amta-srw.3,J07-1003,0,0.0983666,"Missing"
2008.amta-srw.3,W06-3110,0,0.0455138,"Missing"
2008.amta-srw.3,2005.iwslt-1.20,0,0.0529642,"Missing"
2008.amta-srw.3,N04-1021,0,\N,Missing
2008.amta-srw.3,2004.iwslt-evaluation.13,0,\N,Missing
2008.amta-srw.5,2007.mtsummit-papers.20,0,0.0161906,"tion and ignoring vowel endings. The empirical experiments showed that the machine translation approaches perform better than the sequence labeling approaches concerning the error rates. 1 Introduction Modern Arabic texts are normally composed of scripts without diacritic marks. The problem is that many words have different meanings depending on their diacritization. This leads to ambiguity when processing data for text-to-speech and speechto-text applications. A reduction of this ambiguity with the help of diacritization in a text document may benefit other language processing tasks as well. Diab et al. (2007) report an improvement from 0.4389 to 0.4416 BLEU scores (Papineni et al., 2002) in Arabic-English SMT after inserting the 2 Arabic Language and Diacritics The Arabic script is written, read and encoded from right to left. Many Arabic letters change their ap270 [8th AMTA conference, Hawaii, 21-25 October 2008] pearance depending on their position in a word. The Arabic alphabet consists of 28 consonant letters. Arabic diacritics are located below and above each character within a word. They are vowelization marks and usually absent. “shadda” is the only diacritic which appears in several modern"
2008.amta-srw.5,W02-0504,0,0.14399,"Systems Lab Universit¨at Karlsruhe (TH) Karlsruhe, Germany schlippe@ira.uka.de ThuyLinh Nguyen Stephan Vogel Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {thuylinh,vogel}@cs.cmu.edu Abstract passivization diacritic “damma”. Since diacritics at the word endings mark the cases in Arabic, a resulting diacritized translation in Arabic language is easier to understand for native speakers even if the word order is wrong. The techniques used for Arabic diacritization are applicable to other languages such as Romanian, French (Tufis and Chitu, 1999) and Hebrew (Gal, 2002). We study two solutions for the diacritization problem. First, we regard the diacritization problem as a phrase-based translation task. In this case a SMT system is used as a tool for our experiments. We also combine a rule-based approach with our SMT methods by post-editing the output of a rulebased diacritizer. Then we solve the problem as a sequence labeling problem with the help of conditional random fields (CRFs). That means we integrate global features to determine our diacritized output sequence and create dependencies between a non-diacritized input sequence, global features such as p"
2008.amta-srw.5,N07-2014,0,0.0892932,"Missing"
2008.amta-srw.5,2006.bcs-1.4,0,0.839836,"and the correct meaning of a word without diacritic marks in an automatic way by considering the context and the position of the word in a sentence. Their instinctive knowledge of Arabic grammar and vocabulary enable them to correctly vocalize words in written texts based on the context. For example, the bare form “Elm” may have different meanings (Figure 1) depending on the diacritization: “Eilm” is translated as “science” or “learning”, while “Ealam” means “flag”. Ambiguity may also occur on the grammatical level as diacritics at word endings are correlated with case and verbal information (Maamouri et al., 2006). Figure 2: Arabic Diacritics. cher, 2005), morphological and contextual-based (Vergyri and Kirchhoff, 2004) as well as methods with Hidden Markov Models (Mustafa Elshafei and Alghamdi, 2006) and weighted finite state machines (Nelken and Shieber, 2005) have been applied for the diacritization of Arabic text. Some authors treated diacritization as a machine translation problem. El-Sadany and Hashish (1989) and El-Imam (2004) proposed rule-based methods for the translation from non-diacritized text to diacritized text. One drawback of these systems is the difficulty to keep the rules consistent"
2008.amta-srw.5,W05-0711,0,0.296578,"in written texts based on the context. For example, the bare form “Elm” may have different meanings (Figure 1) depending on the diacritization: “Eilm” is translated as “science” or “learning”, while “Ealam” means “flag”. Ambiguity may also occur on the grammatical level as diacritics at word endings are correlated with case and verbal information (Maamouri et al., 2006). Figure 2: Arabic Diacritics. cher, 2005), morphological and contextual-based (Vergyri and Kirchhoff, 2004) as well as methods with Hidden Markov Models (Mustafa Elshafei and Alghamdi, 2006) and weighted finite state machines (Nelken and Shieber, 2005) have been applied for the diacritization of Arabic text. Some authors treated diacritization as a machine translation problem. El-Sadany and Hashish (1989) and El-Imam (2004) proposed rule-based methods for the translation from non-diacritized text to diacritized text. One drawback of these systems is the difficulty to keep the rules consistent, up-to-date and extend them to other Arabic dialects. Emam and Fischer (2005) suggested an example-based hierarchical top-down approach, similar to examplebased translation approaches. In order to translate a sentence in the test set the system tries t"
2008.amta-srw.5,J03-1002,0,0.00492344,"Missing"
2008.amta-srw.5,P02-1040,0,0.0755796,"achine translation approaches perform better than the sequence labeling approaches concerning the error rates. 1 Introduction Modern Arabic texts are normally composed of scripts without diacritic marks. The problem is that many words have different meanings depending on their diacritization. This leads to ambiguity when processing data for text-to-speech and speechto-text applications. A reduction of this ambiguity with the help of diacritization in a text document may benefit other language processing tasks as well. Diab et al. (2007) report an improvement from 0.4389 to 0.4416 BLEU scores (Papineni et al., 2002) in Arabic-English SMT after inserting the 2 Arabic Language and Diacritics The Arabic script is written, read and encoded from right to left. Many Arabic letters change their ap270 [8th AMTA conference, Hawaii, 21-25 October 2008] pearance depending on their position in a word. The Arabic alphabet consists of 28 consonant letters. Arabic diacritics are located below and above each character within a word. They are vowelization marks and usually absent. “shadda” is the only diacritic which appears in several modern Arabic scripts. Native speakers distinguish the right pronunciation and the cor"
2008.amta-srw.5,W07-0728,0,0.0173107,"ility of the sequence of tags given the sequence of the consonants in the training data Figure 3: Lattice with Edges from Character to Character and from Word to Word. 4.1.4 Diacritization as Sequence Labeling Problem Diacritization: Rule-based Translation with Statistical Phrase-based Post-Editing The approaches described so far do not use linguistic rules for the restoration of diacritics. Recently, there have been a number of studies showing that a statistical machine translation system can successfully be used to post-edit and thereby improve the output of a rule-based translation system (Simard et al., 2007). If appropriate training material is provided, it is possible to train a SMT system to automatically correct systematic errors made by rulebased systems. A similar approach can be used in our case: given the output of a rule-based diacritizer, we can use the statistical approach to perform a postediting step. 1 273 Application Technology Inc., US company [8th AMTA conference, Hawaii, 21-25 October 2008] T as given by the following equation: X  ∗ θ = argmax log p Y |X, θ θ from this data source. A development set to tune the parameters of the systems and a test set, each consisting of 1,190 s"
2008.amta-srw.5,W00-1308,0,0.0903408,"Missing"
2008.amta-srw.5,W04-1612,0,0.131532,"xt and the position of the word in a sentence. Their instinctive knowledge of Arabic grammar and vocabulary enable them to correctly vocalize words in written texts based on the context. For example, the bare form “Elm” may have different meanings (Figure 1) depending on the diacritization: “Eilm” is translated as “science” or “learning”, while “Ealam” means “flag”. Ambiguity may also occur on the grammatical level as diacritics at word endings are correlated with case and verbal information (Maamouri et al., 2006). Figure 2: Arabic Diacritics. cher, 2005), morphological and contextual-based (Vergyri and Kirchhoff, 2004) as well as methods with Hidden Markov Models (Mustafa Elshafei and Alghamdi, 2006) and weighted finite state machines (Nelken and Shieber, 2005) have been applied for the diacritization of Arabic text. Some authors treated diacritization as a machine translation problem. El-Sadany and Hashish (1989) and El-Imam (2004) proposed rule-based methods for the translation from non-diacritized text to diacritized text. One drawback of these systems is the difficulty to keep the rules consistent, up-to-date and extend them to other Arabic dialects. Emam and Fischer (2005) suggested an example-based hi"
2008.amta-srw.5,2003.mtsummit-papers.53,1,0.83339,"ouni et al., 2006). Our sentences are in Buckwalter Transliteration, as pictured in Section 5.3, and do not include any punctuation marks. Since the corpus contains complete vowelization including case endings, the diacritics a, u, i, F, N, K, B and o had to be deleted in order to create the non-vowelized part of the parallel corpus. All systems except the post-editing system used a training set of 23 k sentences with 613 k words 6 6.1 Experiments and Results The Translation Systems For the translation on word and character level we work with a phrase-based translation system as described in (Vogel et al., 2003). As starting points both a word-based and a character-based baseline SMT system were established and evaluated. Both systems contain 10-gram Suffix Array Language Models (Zhang, 2006). The phrase tables contain up to 5-gram entries and appropriate phrase translation 274 [8th AMTA conference, Hawaii, 21-25 October 2008] final vow no final vow WER DER WER DER word-based 22.8 7.4 9.9 4.3 char-based 21.8 4.8 7.4 1.8 Table 1: Results of Word-based and Character-based Baseline Systems. final vow no final vow WER DER WER DER baseline system 21.8 4.8 7.4 1.8 max. phrase length 7 21.6 4.8 7.5 1.9 lexi"
2008.amta-srw.5,P06-1073,0,0.047701,"ansliteration and back it is a one-to-one mapping without gain or loss of ambiguity. Figure 4 describes the function of each diacritic, their pronunciation as well as their corresponding character in Buckwalter Transliteration. (We use ”B” to represent the ”shadda” for processing reasons.) LDC’s Arabic Treebank The data to train, tune and test the translation- and conditional random fields-based diacritizers are extracted from the LDC’s Arabic Treebank of diacritized An Nahar News stories. This data set has been used by several researchers (Maamouri et al., 2006), (Nelken and Shieber, 2005), (Zitouni et al., 2006). Our sentences are in Buckwalter Transliteration, as pictured in Section 5.3, and do not include any punctuation marks. Since the corpus contains complete vowelization including case endings, the diacritics a, u, i, F, N, K, B and o had to be deleted in order to create the non-vowelized part of the parallel corpus. All systems except the post-editing system used a training set of 23 k sentences with 613 k words 6 6.1 Experiments and Results The Translation Systems For the translation on word and character level we work with a phrase-based translation system as described in (Vogel et al., 2003"
2008.amta-srw.5,P07-2045,0,\N,Missing
2008.iwslt-evaluation.2,W06-3119,1,0.870292,".A. The CMU Syntax-Augmented Machine Translation System: SAMT on Hadoop with N -best alignments Andreas Zollmann, Ashish Venugopal, Stephan Vogel interACT, Language Technology Institute School of Computer Science Carnegie Mellon University, Pittsburgh, USA {zollmann,ashishv,vogel+}@cs.cmu.edu Abstract We present the CMU Syntax Augmented Machine Translation System that was used in the IWSLT-08 evaluation campaign. We participated in the Full-BTEC data track for Chinese-English translation, focusing on transcript translation. For this year’s evaluation, we ported the Syntax Augmented MT toolkit [1] to the Hadoop MapReduce [2] parallel processing architecture, allowing us to efficiently run experiments evaluating a novel “wider pipelines” approach to integrate evidence from N -best alignments into our translation models. We describe each step of the MapReduce pipeline as it is implemented in the open-source SAMT toolkit, and show improvements in translation quality by using N -best alignments in both hierarchical and syntax augmented translation systems. 1. Introduction While the IWSLT evaluation represents a limited domain, scarce resource condition machine translation task, the choice"
2008.iwslt-evaluation.2,J93-2003,0,0.0190723,"eline that carries data from the initial parallel corpora, through the identification and estimation of probabilistic synchronous context-free grammar (PSCFG) rules, and finally to the decoder, where this data is used with an n-gram language model to generate translations. Current phrase-based and hierarchically structured systems rely on the output of a sequential “pipeline” of maximum a posteriori inference steps to identify hidden translation structure and estimate the parameters of their transla- 18 - tion models. The first step in this pipeline typically involves learning word-alignments [3] over parallel sentence-aligned training data. The outputs of this step are the word alignment model’s most probable word-to-word correspondences within each parallel sentence pair. These alignments are used as the input to a phrase extraction step, where multi-word phrase pairs are identified and scored (with multiple features) based on statistics computed across the training data. The most successful methods extract phrases that adhere to heuristic constraints [4, 5]. Thus, errors made within the single-best alignment are propagated (1) to the identification of phrases, since errors in the a"
2008.iwslt-evaluation.2,N03-1017,0,0.396725,"te the parameters of their transla- 18 - tion models. The first step in this pipeline typically involves learning word-alignments [3] over parallel sentence-aligned training data. The outputs of this step are the word alignment model’s most probable word-to-word correspondences within each parallel sentence pair. These alignments are used as the input to a phrase extraction step, where multi-word phrase pairs are identified and scored (with multiple features) based on statistics computed across the training data. The most successful methods extract phrases that adhere to heuristic constraints [4, 5]. Thus, errors made within the single-best alignment are propagated (1) to the identification of phrases, since errors in the alignment affect which phrases are extracted, and (2) to the estimation of phrase weights, since each extracted phrase is counted as evidence for relative frequency estimates. Methods like those described in [6] and [7, 8] address this problem by jointly modeling alignment and phrase identification, yet have not achieved the same empirical results as surface heuristic based methods, or require substantially more computational effort to train. For this evaluation we expe"
2008.iwslt-evaluation.2,J04-4002,0,0.192284,"te the parameters of their transla- 18 - tion models. The first step in this pipeline typically involves learning word-alignments [3] over parallel sentence-aligned training data. The outputs of this step are the word alignment model’s most probable word-to-word correspondences within each parallel sentence pair. These alignments are used as the input to a phrase extraction step, where multi-word phrase pairs are identified and scored (with multiple features) based on statistics computed across the training data. The most successful methods extract phrases that adhere to heuristic constraints [4, 5]. Thus, errors made within the single-best alignment are propagated (1) to the identification of phrases, since errors in the alignment affect which phrases are extracted, and (2) to the estimation of phrase weights, since each extracted phrase is counted as evidence for relative frequency estimates. Methods like those described in [6] and [7, 8] address this problem by jointly modeling alignment and phrase identification, yet have not achieved the same empirical results as surface heuristic based methods, or require substantially more computational effort to train. For this evaluation we expe"
2008.iwslt-evaluation.2,J97-3002,0,0.048594,"as the input to a phrase extraction step, where multi-word phrase pairs are identified and scored (with multiple features) based on statistics computed across the training data. The most successful methods extract phrases that adhere to heuristic constraints [4, 5]. Thus, errors made within the single-best alignment are propagated (1) to the identification of phrases, since errors in the alignment affect which phrases are extracted, and (2) to the estimation of phrase weights, since each extracted phrase is counted as evidence for relative frequency estimates. Methods like those described in [6] and [7, 8] address this problem by jointly modeling alignment and phrase identification, yet have not achieved the same empirical results as surface heuristic based methods, or require substantially more computational effort to train. For this evaluation we experimented with an approach that “widens” the pipeline, rather than performing two steps jointly. We present N -best alignments to the downstream phrase extraction algorithm and define a probability distribution over these alternatives to generate expected, possibly fractional counts for the extracted translation rules, under that distri"
2008.iwslt-evaluation.2,W02-1018,0,0.0249834,"input to a phrase extraction step, where multi-word phrase pairs are identified and scored (with multiple features) based on statistics computed across the training data. The most successful methods extract phrases that adhere to heuristic constraints [4, 5]. Thus, errors made within the single-best alignment are propagated (1) to the identification of phrases, since errors in the alignment affect which phrases are extracted, and (2) to the estimation of phrase weights, since each extracted phrase is counted as evidence for relative frequency estimates. Methods like those described in [6] and [7, 8] address this problem by jointly modeling alignment and phrase identification, yet have not achieved the same empirical results as surface heuristic based methods, or require substantially more computational effort to train. For this evaluation we experimented with an approach that “widens” the pipeline, rather than performing two steps jointly. We present N -best alignments to the downstream phrase extraction algorithm and define a probability distribution over these alternatives to generate expected, possibly fractional counts for the extracted translation rules, under that distribution. The"
2008.iwslt-evaluation.2,W06-3105,0,0.0163331,"input to a phrase extraction step, where multi-word phrase pairs are identified and scored (with multiple features) based on statistics computed across the training data. The most successful methods extract phrases that adhere to heuristic constraints [4, 5]. Thus, errors made within the single-best alignment are propagated (1) to the identification of phrases, since errors in the alignment affect which phrases are extracted, and (2) to the estimation of phrase weights, since each extracted phrase is counted as evidence for relative frequency estimates. Methods like those described in [6] and [7, 8] address this problem by jointly modeling alignment and phrase identification, yet have not achieved the same empirical results as surface heuristic based methods, or require substantially more computational effort to train. For this evaluation we experimented with an approach that “widens” the pipeline, rather than performing two steps jointly. We present N -best alignments to the downstream phrase extraction algorithm and define a probability distribution over these alternatives to generate expected, possibly fractional counts for the extracted translation rules, under that distribution. The"
2008.iwslt-evaluation.2,P03-1021,0,0.128394,"is opportunity to port the Syntax Augmented Machine Translation (SAMT) toolkit to the Hadoop MapReduce [2] parallel processing architecture. Extending the SAMT open-source toolkit has allowed us to experiment with wider pipelines for the IWSLT 2008 evaluation task as well as apply these techniques to large scale tasks such as those in the NIST MT evaluation. In Section 2 we formally define PSCFGs and the features on each rule that are estimated from rule occurrence data. In Section 3, we detail how the SAMT pipeline, from extraction of rules through to decoding and Minimum Error Rate training [9] are ported to the Hadoop MapReduce architecture. We present runtimes for end-to-end systems built on small, medium and large resource conditions to show the effectiveness of our implementation. We then show how we used SAMT on Hadoop, extending [1] to handle N -best alignments to deliver significant improvements in translation quality for the IWSLT evaluation. 2. Synchronous Grammars for SMT Probabilistic synchronous context-free grammars (PSCFGs) are defined by a source terminal set (source vocabulary) TS , a target terminal set (target vocabulary) TT , and a shared nonterminal set N , and i"
2008.iwslt-evaluation.2,P05-1033,0,0.653446,") of nonterminal tokens in α, • ∼: {1, . . . , #NT(γ)} → {1, . . . , #NT(α)} is a one-toone mapping from nonterminal tokens in γ to nonterminal tokens in α, and • w ∈ [0, ∞) is a nonnegative real-valued weight assigned to the rule. In our notation, we will assume ∼ to be implicitly defined by indexing the NT occurrences in γ from left to right starting with 1, and by indexing the NT occurrences in α by the indices of their corresponding counterparts in γ. Syntaxoriented PSCFG approaches often ignore source structure, focusing instead on generating syntactically well-formed target derivations. [10] uses a single nonterminal category, [11] use syntactic constituents for the PSCFG nonterminal set, and [1] take advantage of CCG-inspired “slash” categories [12] and concatenated “plus” categories. We now briefly describe the identification and estimation of PSCFG rules from parallel sentence aligned corpora under the framework proposed by [1]. Our contribution of integrating evidence from N -best alignments can be applied to - 19 - any of the other PSCFG approaches mentioned above in a straight-forward manner. 2.1. Grammar Construction [1] describe a process to generate a PSCFG given paralle"
2008.iwslt-evaluation.2,N07-1063,1,0.805731,"ation to its target yield and src(·) maps a derivation to its source yield. Our distribution p over derivations is defined by a loglinear model. The probability of a derivation D is defined in terms of the rules r that are used in D: Q Q pLM (tgt(D))λLM × r∈D i φi (r)λi p(D) = (2) Z(λ) where φi is a feature function on rules, pLM is an n-gram probability of the target yield tgt(D), and Z(λ) is a normalization constant chosen such that the probabilities sum up to one.1 The computational challenges of this search task (compounded by the integration of the language model) are addressed elsewhere [14, 15]. All feature weights λi are trained in concert with the language model weight λLM via minimum-error training (MER) [9]. Now, we focus on the estimation of the feature values φ during the grammar construction process. The feature values are statistics estimated from rule counts. 2.3. Feature Value Statistics The features φ represent multiple criteria by which the decoding process can judge the quality of each rule and, by extension, each derivation. We include both real-valued and boolean-valued features for each rule. The following probabilistic quantities are estimated and used as feature va"
2008.iwslt-evaluation.2,W08-0333,0,0.0175518,"s. For each MapReduce phase of the pipeline, we specify the MapInput (data received by the Map task), MapOptions (parameters to the Map task), MapOutput (key-value pairs output by the Map task), ReduceInput (input guaranteed to be contiguous to the Reduce task), ReduceOptions (parameters to the Reduce task), and ReduceOutput (unstructured output format from the Reduce task). The SAMT pipeline assumes input of the format e, f, a(e, f ), π(e), where e is a target language sentence from the training data, f is a source language sentence from the training data, a(e, f ) is a wordto-word alignment [18, 3] on e, f and π(e) is phrase structure parse tree on e. The SAMT pipeline can be split into the following phases: Phrase Extraction, Rule Extraction, Rule Filtering, LM filtering (optional), Decoding, N-Best Merge and MER Training. In each phase we try to limit the number of key-value pairs to reduce I/O overhead, outputting multiple values that share the same key from the same Map task on a single line. The Rule Filtering and LM Filtering phases build sentence specific models for each sentence in the development and test corpus allowing the Decoding phrase to load these models directly into me"
2008.iwslt-evaluation.2,P08-1115,0,0.0220817,"alignment evidence. 4. N -best Evidence The PSCFG rule extraction procedure described above relies on high quality word alignments and parses. The quality of the alignments affects the set of phrases that can be identified by the heuristics in [4]. Improving or diversifying the set of initial phrases also affects the rules with nonterminals that are identified via the procedure described above. Since PSCFG systems rely on rules with nonterminal symbols to represent reordering operations, the set of these initial phrases can have a profound impact on translation quality. Several recent studies [19, 20, 21, 22], explore the relationship between the quality of the initial decisions in the “pipeline” and final translation quality. Here we experiment with weighted N -best alignments to build PSCFGs and estimate their features. Our approach toward the integration of N -best evidence into the grammar construction process allows us to take advantage of the diversity found in the N best alternatives, while reducing the negative impact of errors made in these alternatives. 4.1. Counting from N -Best Lists For this evaluation we experimented with the extraction of PSCFG rules from N -best alignments making u"
2008.iwslt-evaluation.2,D08-1022,0,0.0139539,"alignment evidence. 4. N -best Evidence The PSCFG rule extraction procedure described above relies on high quality word alignments and parses. The quality of the alignments affects the set of phrases that can be identified by the heuristics in [4]. Improving or diversifying the set of initial phrases also affects the rules with nonterminals that are identified via the procedure described above. Since PSCFG systems rely on rules with nonterminal symbols to represent reordering operations, the set of these initial phrases can have a profound impact on translation quality. Several recent studies [19, 20, 21, 22], explore the relationship between the quality of the initial decisions in the “pipeline” and final translation quality. Here we experiment with weighted N -best alignments to build PSCFGs and estimate their features. Our approach toward the integration of N -best evidence into the grammar construction process allows us to take advantage of the diversity found in the N best alternatives, while reducing the negative impact of errors made in these alternatives. 4.1. Counting from N -Best Lists For this evaluation we experimented with the extraction of PSCFG rules from N -best alignments making u"
2008.iwslt-evaluation.2,P08-1112,0,0.0222965,"alignment evidence. 4. N -best Evidence The PSCFG rule extraction procedure described above relies on high quality word alignments and parses. The quality of the alignments affects the set of phrases that can be identified by the heuristics in [4]. Improving or diversifying the set of initial phrases also affects the rules with nonterminals that are identified via the procedure described above. Since PSCFG systems rely on rules with nonterminal symbols to represent reordering operations, the set of these initial phrases can have a profound impact on translation quality. Several recent studies [19, 20, 21, 22], explore the relationship between the quality of the initial decisions in the “pipeline” and final translation quality. Here we experiment with weighted N -best alignments to build PSCFGs and estimate their features. Our approach toward the integration of N -best evidence into the grammar construction process allows us to take advantage of the diversity found in the N best alternatives, while reducing the negative impact of errors made in these alternatives. 4.1. Counting from N -Best Lists For this evaluation we experimented with the extraction of PSCFG rules from N -best alignments making u"
2008.iwslt-evaluation.2,2008.amta-papers.18,1,0.439795,"0.440 0.470 0.467 0.478 0.460 0.472 0.477 0.463 2007 Time (s) 8108 8024 15376 19298 29500 895 906 944 979 2008 Time (s) 8367 8250 15577 19469 30894 1451 1476 1516 1596 Table 3: Grammar statistics and translation quality (IBM-BLEU) on development (IWSLT Devset4) and test sets (IWSLT 2007, 2008) when integrating N -best alignments. # Rules reflect rules that are applicable to the first sentence in IWSLT 2007. Decoding times in seconds are cumulative over all sentences in respective test set. and additional details regarding selecting N -best alignments and even N 0 -best parses can be found in [23]. We use α = 1 in the experiments presented here. 5. Translation Results 5.1. Experimental Setup We present results on the IWSLT 2007 and 2008 Chineseto-English translation task, based on the full BTEC corpus of travel expressions with 120K parallel sentences (906K source words and 1.2M target words) as well as the evaluation corpora from the evaluation years preceding 2007. The development data consists of 489 sentences (average length of 10.6 words) from the 2006 evaluation, the 2007 test set contains 489 sentence (average length of 6.47 words) sentences and the 2008 test set contains 507 se"
2008.iwslt-evaluation.2,P02-1040,0,0.0796515,"vidence can be discarded in this way. As an alternative, we use IBM Model 4 translation weights to estimate lexical weights. Using these IBM Model 4 weights allows a larger number of rules to be added to the grammar since more rules have non-zero lexical weights. The n-gram language model is trained on the target side of the parallel training corpus and translation experiments use the decoder and MER trainer available in the same toolkit. We use the cube-pruning decoder option [14] in these experiments. - 24 - 5.2. Empirical Results We measure translation quality using the mixed-cased IBMBLEU [24] metric as we vary N . Each value of N implies that the first N alignment alternatives have been considered when building the grammar. For each grammar we also track the number of rules relevant for the first sentence in the IWSLT 2007 test set (grammars are subsampled on a per-sentence basis to keep memory requirements low during decoding). We also note the number of seconds required to translate each test set. N -best alignments (Syntax augmented grammar. Table 3 shows translation results on the IWSLT translation task for the development (IWSLT 2006) and two test corpora (IWSLT 2007 and 2008"
2008.iwslt-evaluation.2,2004.tmi-1.9,1,0.720406,"corpus than the test set(s). (Judging from the sentence length distribution alone, this seems indeed to be the case.) 6. Conclusion For the 2008 IWSLT evaluation we evaluated the use of N -best alignments to take more advantage of the available data in this scarce resource scenario task. While we did see substantial improvements in translation quality using N -best alignments, we note that the variance in BLEU score results on the IWSLT evaluation sets is still very high. While none of the result presented here can be considered “statistically significant” according to the testing criteria in [25], we are able to see trends across multiple evaluation corpora that we can use to evaluate the potential of a technique. In order to efficiently run these experiments, which modify the very beginning of the long MT pipeline, we ported the SAMT toolkit to the Hadoop parallel processing architecture, allowing us to quickly run experiments for this evaluation as well as for medium and large data scenarios. This Hadoop port is integrated into the publicly available open-source SAMT toolkit. 7. Acknowledgments This work would not have been possible without access to the M45 cluster, which was gener"
2008.iwslt-evaluation.2,D07-1090,0,\N,Missing
2008.iwslt-evaluation.2,P08-1058,0,\N,Missing
2008.iwslt-evaluation.2,J07-2003,0,\N,Missing
2009.mtsummit-papers.1,P06-1067,0,0.0161793,"Knight, 1999). Despite the importance of word movement, the popular phrase-based translation paradigm (Koehn et al., 2003) devotes surprisingly little modeling capacity to the issue. A very simple reordering model is to base the cost for word movement only on the distance in the source sentence between the previous and the current word or phrase during the translation process. Later on, lexicalized reordering models, which condition the probability of phrase-to-phrase transitions on the words involved, have been proposed to address the word reordering issue (Tillman, 2004; Koehn et al., 2005; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). Alternatively, one can employ syntax in the modeling of movement. By viewing sentence in terms of its hierarchical structure, one can more easily expose regularities in the sorts of movement that occur during translation. A number of syntactic methods are driven by formal syntax alone (Wu, 1997; Chiang, 2005; Shen et al., 2008), while others employ linguistic syntax derived from a parse tree (Galley et al., 2004; Quirk et al., 2005; Liu et al., 2006). Each of these approaches requires a parser-like decoder, and represents a departure from phrase-based decoding. Galley and"
2009.mtsummit-papers.1,N09-2001,1,0.591169,"O 0.119 0.719 0.146 SO 0.009 0.053 0.750 DO 0.038 0.228 0.210 Table 3: inside and outside probabilities for phrase “ask you”- “pedirle” according to three parameter estimation methods 2.3 Decoding The beam search strategy is unchanged from the phrasebased system. Our proposed source-tree reordering models concern monolingualy and syntactically movements in the source sentence. However, computing source-tree reordering model scores can be done in two scenarios 1) not using and 2) using cohesive constraints. Cohesive constraints can be enforced by the interruption check algorithm (Cherry, 2008; Bach et al., 2009). One can consider the first scenario as the decoder does not have any information about the source dependency tree during decoding time, therefore, we allow the decoder to consider both events inside and outside. The decision of selecting a preferable feature is made by the tuning procedure. On the other hand, when the source dependency tree is available, subtree movements are informed to the decoder via cohesive constraints, as a result, we are able to allow the decoder to make a harder choice to consider either inside or outside. More specifically, if the decoder chooses to decode without c"
2009.mtsummit-papers.1,P08-1009,0,0.420379,"movement. Search space constraints limit the phrasal decoder’s translation search using syntactic intuitions. Zens et al.(2004) demonstrated how to incorporate formally syntactic binary-bracketing constraints into phrase-based decoding. Recently, it has been shown that syntactic cohesion, the notion that syntactic phrases in the source sentence tend to remain contiguous in the target (Fox, 2002), can be incorporated into phrasal decoding as well, by following the simple intuition that any source subtree that has begun translation, must be completed before translating another part of the tree (Cherry, 2008; Yamamoto et al., 2008). In this paper, we introduce a novel reordering model for phrase-based systems which exploits dependency subtree movements and constraints. In order to do, we must first consider several questions. Should subtree movements be conditioned on source dependency structures? How can we estimate reliable probability distributions from training data? How do we incorporate the reordering model with dependency structures and cohesive constraints into a phrase-based decoder? We investigate these questions by presenting the model, training and decoding procedure in Section 2. Fur"
2009.mtsummit-papers.1,P05-1033,0,0.091645,"during the translation process. Later on, lexicalized reordering models, which condition the probability of phrase-to-phrase transitions on the words involved, have been proposed to address the word reordering issue (Tillman, 2004; Koehn et al., 2005; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). Alternatively, one can employ syntax in the modeling of movement. By viewing sentence in terms of its hierarchical structure, one can more easily expose regularities in the sorts of movement that occur during translation. A number of syntactic methods are driven by formal syntax alone (Wu, 1997; Chiang, 2005; Shen et al., 2008), while others employ linguistic syntax derived from a parse tree (Galley et al., 2004; Quirk et al., 2005; Liu et al., 2006). Each of these approaches requires a parser-like decoder, and represents a departure from phrase-based decoding. Galley and Manning (2008) demonstrated how to integrate hierarchical phrase structures to lexicalized reordering models. The well-studied phrase-based architecture can also benefit from syntactic intuitions. Phrasal decoding can be augmented easily, either by syntactic pre-processing or through search-space constraints. Pre-processing appr"
2009.mtsummit-papers.1,P05-1066,0,0.0533476,"oding. Galley and Manning (2008) demonstrated how to integrate hierarchical phrase structures to lexicalized reordering models. The well-studied phrase-based architecture can also benefit from syntactic intuitions. Phrasal decoding can be augmented easily, either by syntactic pre-processing or through search-space constraints. Pre-processing approaches parse the source sentence and use the tree to apply rules which reorder the source into a more target-like structure before the translation begins. These rules can be learned (Xia and McCord, 2004; Rottmann and Vogel, 2007) or designed by hand (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009). The pre-processing approach benefits from its simplicity and modularity, but it suffers from limitation of providing at most a first-best guess at syntactic movement. Search space constraints limit the phrasal decoder’s translation search using syntactic intuitions. Zens et al.(2004) demonstrated how to incorporate formally syntactic binary-bracketing constraints into phrase-based decoding. Recently, it has been shown that syntactic cohesion, the notion that syntactic phrases in the source sentence tend to remain contiguous in the target (Fox, 2002), can"
2009.mtsummit-papers.1,W02-1039,0,0.0880813,"ins et al., 2005; Wang et al., 2007; Xu et al., 2009). The pre-processing approach benefits from its simplicity and modularity, but it suffers from limitation of providing at most a first-best guess at syntactic movement. Search space constraints limit the phrasal decoder’s translation search using syntactic intuitions. Zens et al.(2004) demonstrated how to incorporate formally syntactic binary-bracketing constraints into phrase-based decoding. Recently, it has been shown that syntactic cohesion, the notion that syntactic phrases in the source sentence tend to remain contiguous in the target (Fox, 2002), can be incorporated into phrasal decoding as well, by following the simple intuition that any source subtree that has begun translation, must be completed before translating another part of the tree (Cherry, 2008; Yamamoto et al., 2008). In this paper, we introduce a novel reordering model for phrase-based systems which exploits dependency subtree movements and constraints. In order to do, we must first consider several questions. Should subtree movements be conditioned on source dependency structures? How can we estimate reliable probability distributions from training data? How do we incor"
2009.mtsummit-papers.1,D08-1089,0,0.013748,"neni, 2006; Kuhn et al., 2006). Alternatively, one can employ syntax in the modeling of movement. By viewing sentence in terms of its hierarchical structure, one can more easily expose regularities in the sorts of movement that occur during translation. A number of syntactic methods are driven by formal syntax alone (Wu, 1997; Chiang, 2005; Shen et al., 2008), while others employ linguistic syntax derived from a parse tree (Galley et al., 2004; Quirk et al., 2005; Liu et al., 2006). Each of these approaches requires a parser-like decoder, and represents a departure from phrase-based decoding. Galley and Manning (2008) demonstrated how to integrate hierarchical phrase structures to lexicalized reordering models. The well-studied phrase-based architecture can also benefit from syntactic intuitions. Phrasal decoding can be augmented easily, either by syntactic pre-processing or through search-space constraints. Pre-processing approaches parse the source sentence and use the tree to apply rules which reorder the source into a more target-like structure before the translation begins. These rules can be learned (Xia and McCord, 2004; Rottmann and Vogel, 2007) or designed by hand (Collins et al., 2005; Wang et al"
2009.mtsummit-papers.1,N04-1035,0,0.0451016,"ility of phrase-to-phrase transitions on the words involved, have been proposed to address the word reordering issue (Tillman, 2004; Koehn et al., 2005; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). Alternatively, one can employ syntax in the modeling of movement. By viewing sentence in terms of its hierarchical structure, one can more easily expose regularities in the sorts of movement that occur during translation. A number of syntactic methods are driven by formal syntax alone (Wu, 1997; Chiang, 2005; Shen et al., 2008), while others employ linguistic syntax derived from a parse tree (Galley et al., 2004; Quirk et al., 2005; Liu et al., 2006). Each of these approaches requires a parser-like decoder, and represents a departure from phrase-based decoding. Galley and Manning (2008) demonstrated how to integrate hierarchical phrase structures to lexicalized reordering models. The well-studied phrase-based architecture can also benefit from syntactic intuitions. Phrasal decoding can be augmented easily, either by syntactic pre-processing or through search-space constraints. Pre-processing approaches parse the source sentence and use the tree to apply rules which reorder the source into a more targ"
2009.mtsummit-papers.1,W08-0509,1,0.81976,"2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a lexicalized reordering model with a reordering window of 3, and the maximum number of target phrases restricted to 5. Results are reported using lowercase BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). All model weights were trained on development sets via minimum-error rate training (MERT) (Venugopal and Vogel, 2005) with an unique 200-best list and optimizing toward BLEU. To shorten the training time, a multi-threaded GIZA++ version was used to utilize multi-processor servers (Gao and Vogel, 2008). We used the MALT parser (Nivre et al., 2006) to get English dependency trees. We perform experiments on English→Spanish and English→Iraqi tasks. Detailed corpus statistics are shown in Table 4. sent. pairs uniq. pairs avg. sent. length # words vocabulary English→Spanish English Spanish 1,310,127 1,287,016 27.4 28.6 35.8 M 37.4 M 117 K 173 K English→Iraqi English Iraqi 654,556 510,314 8.4 5.9 5.5 M 3.8 M 34 K 109 K Table 4: Corpus statistics of English→Spanish and English→Iraqi systems We experiment systems in different configurations of the source-tree reordering model such as DO, DOD and DO"
2009.mtsummit-papers.1,J99-4005,0,0.0543722,"movements to be learned alongside other reordering features. We show improvements in translation quality in English→Spanish and English→Iraqi translation tasks. 1 Introduction Word movement is a defining characteristic of the machine translation problem. The fact that word order can change during translation makes the problem fundamentally different from related tasks such as tagging and automatic speech recognition. In fact, if one allows unrestricted changes in word order during translation, that alone is sufficient to show it to be NP complete, by analogy to the Traveling Salesman Problem (Knight, 1999). Despite the importance of word movement, the popular phrase-based translation paradigm (Koehn et al., 2003) devotes surprisingly little modeling capacity to the issue. A very simple reordering model is to base the cost for word movement only on the distance in the source sentence between the previous and the current word or phrase during the translation process. Later on, lexicalized reordering models, which condition the probability of phrase-to-phrase transitions on the words involved, have been proposed to address the word reordering issue (Tillman, 2004; Koehn et al., 2005; Al-Onaizan an"
2009.mtsummit-papers.1,N03-1017,0,0.0186467,"y in English→Spanish and English→Iraqi translation tasks. 1 Introduction Word movement is a defining characteristic of the machine translation problem. The fact that word order can change during translation makes the problem fundamentally different from related tasks such as tagging and automatic speech recognition. In fact, if one allows unrestricted changes in word order during translation, that alone is sufficient to show it to be NP complete, by analogy to the Traveling Salesman Problem (Knight, 1999). Despite the importance of word movement, the popular phrase-based translation paradigm (Koehn et al., 2003) devotes surprisingly little modeling capacity to the issue. A very simple reordering model is to base the cost for word movement only on the distance in the source sentence between the previous and the current word or phrase during the translation process. Later on, lexicalized reordering models, which condition the probability of phrase-to-phrase transitions on the words involved, have been proposed to address the word reordering issue (Tillman, 2004; Koehn et al., 2005; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). Alternatively, one can employ syntax in the modeling of movement. By vi"
2009.mtsummit-papers.1,2005.iwslt-1.8,0,0.275442,"g Salesman Problem (Knight, 1999). Despite the importance of word movement, the popular phrase-based translation paradigm (Koehn et al., 2003) devotes surprisingly little modeling capacity to the issue. A very simple reordering model is to base the cost for word movement only on the distance in the source sentence between the previous and the current word or phrase during the translation process. Later on, lexicalized reordering models, which condition the probability of phrase-to-phrase transitions on the words involved, have been proposed to address the word reordering issue (Tillman, 2004; Koehn et al., 2005; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). Alternatively, one can employ syntax in the modeling of movement. By viewing sentence in terms of its hierarchical structure, one can more easily expose regularities in the sorts of movement that occur during translation. A number of syntactic methods are driven by formal syntax alone (Wu, 1997; Chiang, 2005; Shen et al., 2008), while others employ linguistic syntax derived from a parse tree (Galley et al., 2004; Quirk et al., 2005; Liu et al., 2006). Each of these approaches requires a parser-like decoder, and represents a departure from ph"
2009.mtsummit-papers.1,P07-2045,0,0.00723181,"g inside and outside a subtree T (n) given it is swap orientation on flat word structures. In the second scenario, the decoder uses cohesive constraints after detecting the orientation of the current phrase, for example swap. The decoder only considers one source-tree reordering feature. The choice of feature depends on the output of the interruption check algorithm on the current phrase. If the return is inside then S I will be used otherwise S O. 3 Experimental Results We built baseline systems using GIZA++ (Och and Ney, 2003), Moses’ phrase extraction with the growdiag-final-and heuristic (Koehn et al., 2007), a standard phrase-based decoder (Vogel, 2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a lexicalized reordering model with a reordering window of 3, and the maximum number of target phrases restricted to 5. Results are reported using lowercase BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). All model weights were trained on development sets via minimum-error rate training (MERT) (Venugopal and Vogel, 2005) with an unique 200-best list and optimizing toward BLEU. To shorten the training time, a multi-threaded GIZA++ version was"
2009.mtsummit-papers.1,N06-1004,0,0.0146954,"rtance of word movement, the popular phrase-based translation paradigm (Koehn et al., 2003) devotes surprisingly little modeling capacity to the issue. A very simple reordering model is to base the cost for word movement only on the distance in the source sentence between the previous and the current word or phrase during the translation process. Later on, lexicalized reordering models, which condition the probability of phrase-to-phrase transitions on the words involved, have been proposed to address the word reordering issue (Tillman, 2004; Koehn et al., 2005; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). Alternatively, one can employ syntax in the modeling of movement. By viewing sentence in terms of its hierarchical structure, one can more easily expose regularities in the sorts of movement that occur during translation. A number of syntactic methods are driven by formal syntax alone (Wu, 1997; Chiang, 2005; Shen et al., 2008), while others employ linguistic syntax derived from a parse tree (Galley et al., 2004; Quirk et al., 2005; Liu et al., 2006). Each of these approaches requires a parser-like decoder, and represents a departure from phrase-based decoding. Galley and Manning (2008) demo"
2009.mtsummit-papers.1,P06-1077,0,0.0170765,"the words involved, have been proposed to address the word reordering issue (Tillman, 2004; Koehn et al., 2005; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). Alternatively, one can employ syntax in the modeling of movement. By viewing sentence in terms of its hierarchical structure, one can more easily expose regularities in the sorts of movement that occur during translation. A number of syntactic methods are driven by formal syntax alone (Wu, 1997; Chiang, 2005; Shen et al., 2008), while others employ linguistic syntax derived from a parse tree (Galley et al., 2004; Quirk et al., 2005; Liu et al., 2006). Each of these approaches requires a parser-like decoder, and represents a departure from phrase-based decoding. Galley and Manning (2008) demonstrated how to integrate hierarchical phrase structures to lexicalized reordering models. The well-studied phrase-based architecture can also benefit from syntactic intuitions. Phrasal decoding can be augmented easily, either by syntactic pre-processing or through search-space constraints. Pre-processing approaches parse the source sentence and use the tree to apply rules which reorder the source into a more target-like structure before the translatio"
2009.mtsummit-papers.1,nivre-etal-2006-maltparser,0,0.0360778,"suffix-array language model (Zhang and Vogel, 2005), a lexicalized reordering model with a reordering window of 3, and the maximum number of target phrases restricted to 5. Results are reported using lowercase BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). All model weights were trained on development sets via minimum-error rate training (MERT) (Venugopal and Vogel, 2005) with an unique 200-best list and optimizing toward BLEU. To shorten the training time, a multi-threaded GIZA++ version was used to utilize multi-processor servers (Gao and Vogel, 2008). We used the MALT parser (Nivre et al., 2006) to get English dependency trees. We perform experiments on English→Spanish and English→Iraqi tasks. Detailed corpus statistics are shown in Table 4. sent. pairs uniq. pairs avg. sent. length # words vocabulary English→Spanish English Spanish 1,310,127 1,287,016 27.4 28.6 35.8 M 37.4 M 117 K 173 K English→Iraqi English Iraqi 654,556 510,314 8.4 5.9 5.5 M 3.8 M 34 K 109 K Table 4: Corpus statistics of English→Spanish and English→Iraqi systems We experiment systems in different configurations of the source-tree reordering model such as DO, DOD and DOO means parameters estimation using Equation 7"
2009.mtsummit-papers.1,J03-1002,0,0.00633233,". In other words, the decoder considers both events that the current phrase is moving inside and outside a subtree T (n) given it is swap orientation on flat word structures. In the second scenario, the decoder uses cohesive constraints after detecting the orientation of the current phrase, for example swap. The decoder only considers one source-tree reordering feature. The choice of feature depends on the output of the interruption check algorithm on the current phrase. If the return is inside then S I will be used otherwise S O. 3 Experimental Results We built baseline systems using GIZA++ (Och and Ney, 2003), Moses’ phrase extraction with the growdiag-final-and heuristic (Koehn et al., 2007), a standard phrase-based decoder (Vogel, 2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a lexicalized reordering model with a reordering window of 3, and the maximum number of target phrases restricted to 5. Results are reported using lowercase BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). All model weights were trained on development sets via minimum-error rate training (MERT) (Venugopal and Vogel, 2005) with an unique 200-best list and optim"
2009.mtsummit-papers.1,J04-4002,0,0.148149,"Missing"
2009.mtsummit-papers.1,P02-1040,0,0.0786927,"e output of the interruption check algorithm on the current phrase. If the return is inside then S I will be used otherwise S O. 3 Experimental Results We built baseline systems using GIZA++ (Och and Ney, 2003), Moses’ phrase extraction with the growdiag-final-and heuristic (Koehn et al., 2007), a standard phrase-based decoder (Vogel, 2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a lexicalized reordering model with a reordering window of 3, and the maximum number of target phrases restricted to 5. Results are reported using lowercase BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). All model weights were trained on development sets via minimum-error rate training (MERT) (Venugopal and Vogel, 2005) with an unique 200-best list and optimizing toward BLEU. To shorten the training time, a multi-threaded GIZA++ version was used to utilize multi-processor servers (Gao and Vogel, 2008). We used the MALT parser (Nivre et al., 2006) to get English dependency trees. We perform experiments on English→Spanish and English→Iraqi tasks. Detailed corpus statistics are shown in Table 4. sent. pairs uniq. pairs avg. sent. length # words vocabulary English→S"
2009.mtsummit-papers.1,P05-1034,0,0.0526796,"rase transitions on the words involved, have been proposed to address the word reordering issue (Tillman, 2004; Koehn et al., 2005; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). Alternatively, one can employ syntax in the modeling of movement. By viewing sentence in terms of its hierarchical structure, one can more easily expose regularities in the sorts of movement that occur during translation. A number of syntactic methods are driven by formal syntax alone (Wu, 1997; Chiang, 2005; Shen et al., 2008), while others employ linguistic syntax derived from a parse tree (Galley et al., 2004; Quirk et al., 2005; Liu et al., 2006). Each of these approaches requires a parser-like decoder, and represents a departure from phrase-based decoding. Galley and Manning (2008) demonstrated how to integrate hierarchical phrase structures to lexicalized reordering models. The well-studied phrase-based architecture can also benefit from syntactic intuitions. Phrasal decoding can be augmented easily, either by syntactic pre-processing or through search-space constraints. Pre-processing approaches parse the source sentence and use the tree to apply rules which reorder the source into a more target-like structure be"
2009.mtsummit-papers.1,2007.tmi-papers.21,1,0.773976,"nd represents a departure from phrase-based decoding. Galley and Manning (2008) demonstrated how to integrate hierarchical phrase structures to lexicalized reordering models. The well-studied phrase-based architecture can also benefit from syntactic intuitions. Phrasal decoding can be augmented easily, either by syntactic pre-processing or through search-space constraints. Pre-processing approaches parse the source sentence and use the tree to apply rules which reorder the source into a more target-like structure before the translation begins. These rules can be learned (Xia and McCord, 2004; Rottmann and Vogel, 2007) or designed by hand (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009). The pre-processing approach benefits from its simplicity and modularity, but it suffers from limitation of providing at most a first-best guess at syntactic movement. Search space constraints limit the phrasal decoder’s translation search using syntactic intuitions. Zens et al.(2004) demonstrated how to incorporate formally syntactic binary-bracketing constraints into phrase-based decoding. Recently, it has been shown that syntactic cohesion, the notion that syntactic phrases in the source sentence tend to remain"
2009.mtsummit-papers.1,P08-1066,0,0.117381,"nslation process. Later on, lexicalized reordering models, which condition the probability of phrase-to-phrase transitions on the words involved, have been proposed to address the word reordering issue (Tillman, 2004; Koehn et al., 2005; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). Alternatively, one can employ syntax in the modeling of movement. By viewing sentence in terms of its hierarchical structure, one can more easily expose regularities in the sorts of movement that occur during translation. A number of syntactic methods are driven by formal syntax alone (Wu, 1997; Chiang, 2005; Shen et al., 2008), while others employ linguistic syntax derived from a parse tree (Galley et al., 2004; Quirk et al., 2005; Liu et al., 2006). Each of these approaches requires a parser-like decoder, and represents a departure from phrase-based decoding. Galley and Manning (2008) demonstrated how to integrate hierarchical phrase structures to lexicalized reordering models. The well-studied phrase-based architecture can also benefit from syntactic intuitions. Phrasal decoding can be augmented easily, either by syntactic pre-processing or through search-space constraints. Pre-processing approaches parse the sou"
2009.mtsummit-papers.1,2006.amta-papers.25,0,0.041987,"ck algorithm on the current phrase. If the return is inside then S I will be used otherwise S O. 3 Experimental Results We built baseline systems using GIZA++ (Och and Ney, 2003), Moses’ phrase extraction with the growdiag-final-and heuristic (Koehn et al., 2007), a standard phrase-based decoder (Vogel, 2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a lexicalized reordering model with a reordering window of 3, and the maximum number of target phrases restricted to 5. Results are reported using lowercase BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). All model weights were trained on development sets via minimum-error rate training (MERT) (Venugopal and Vogel, 2005) with an unique 200-best list and optimizing toward BLEU. To shorten the training time, a multi-threaded GIZA++ version was used to utilize multi-processor servers (Gao and Vogel, 2008). We used the MALT parser (Nivre et al., 2006) to get English dependency trees. We perform experiments on English→Spanish and English→Iraqi tasks. Detailed corpus statistics are shown in Table 4. sent. pairs uniq. pairs avg. sent. length # words vocabulary English→Spanish English Spanish 1,310,1"
2009.mtsummit-papers.1,N04-4026,0,0.0632242,"to the Traveling Salesman Problem (Knight, 1999). Despite the importance of word movement, the popular phrase-based translation paradigm (Koehn et al., 2003) devotes surprisingly little modeling capacity to the issue. A very simple reordering model is to base the cost for word movement only on the distance in the source sentence between the previous and the current word or phrase during the translation process. Later on, lexicalized reordering models, which condition the probability of phrase-to-phrase transitions on the words involved, have been proposed to address the word reordering issue (Tillman, 2004; Koehn et al., 2005; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). Alternatively, one can employ syntax in the modeling of movement. By viewing sentence in terms of its hierarchical structure, one can more easily expose regularities in the sorts of movement that occur during translation. A number of syntactic methods are driven by formal syntax alone (Wu, 1997; Chiang, 2005; Shen et al., 2008), while others employ linguistic syntax derived from a parse tree (Galley et al., 2004; Quirk et al., 2005; Liu et al., 2006). Each of these approaches requires a parser-like decoder, and represents"
2009.mtsummit-papers.1,2005.eamt-1.36,1,0.721487,"esults We built baseline systems using GIZA++ (Och and Ney, 2003), Moses’ phrase extraction with the growdiag-final-and heuristic (Koehn et al., 2007), a standard phrase-based decoder (Vogel, 2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a lexicalized reordering model with a reordering window of 3, and the maximum number of target phrases restricted to 5. Results are reported using lowercase BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). All model weights were trained on development sets via minimum-error rate training (MERT) (Venugopal and Vogel, 2005) with an unique 200-best list and optimizing toward BLEU. To shorten the training time, a multi-threaded GIZA++ version was used to utilize multi-processor servers (Gao and Vogel, 2008). We used the MALT parser (Nivre et al., 2006) to get English dependency trees. We perform experiments on English→Spanish and English→Iraqi tasks. Detailed corpus statistics are shown in Table 4. sent. pairs uniq. pairs avg. sent. length # words vocabulary English→Spanish English Spanish 1,310,127 1,287,016 27.4 28.6 35.8 M 37.4 M 117 K 173 K English→Iraqi English Iraqi 654,556 510,314 8.4 5.9 5.5 M 3.8 M 34 K 1"
2009.mtsummit-papers.1,D07-1077,0,0.036312,"ing (2008) demonstrated how to integrate hierarchical phrase structures to lexicalized reordering models. The well-studied phrase-based architecture can also benefit from syntactic intuitions. Phrasal decoding can be augmented easily, either by syntactic pre-processing or through search-space constraints. Pre-processing approaches parse the source sentence and use the tree to apply rules which reorder the source into a more target-like structure before the translation begins. These rules can be learned (Xia and McCord, 2004; Rottmann and Vogel, 2007) or designed by hand (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009). The pre-processing approach benefits from its simplicity and modularity, but it suffers from limitation of providing at most a first-best guess at syntactic movement. Search space constraints limit the phrasal decoder’s translation search using syntactic intuitions. Zens et al.(2004) demonstrated how to incorporate formally syntactic binary-bracketing constraints into phrase-based decoding. Recently, it has been shown that syntactic cohesion, the notion that syntactic phrases in the source sentence tend to remain contiguous in the target (Fox, 2002), can be incorporated int"
2009.mtsummit-papers.1,J97-3002,0,0.117563,"or phrase during the translation process. Later on, lexicalized reordering models, which condition the probability of phrase-to-phrase transitions on the words involved, have been proposed to address the word reordering issue (Tillman, 2004; Koehn et al., 2005; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). Alternatively, one can employ syntax in the modeling of movement. By viewing sentence in terms of its hierarchical structure, one can more easily expose regularities in the sorts of movement that occur during translation. A number of syntactic methods are driven by formal syntax alone (Wu, 1997; Chiang, 2005; Shen et al., 2008), while others employ linguistic syntax derived from a parse tree (Galley et al., 2004; Quirk et al., 2005; Liu et al., 2006). Each of these approaches requires a parser-like decoder, and represents a departure from phrase-based decoding. Galley and Manning (2008) demonstrated how to integrate hierarchical phrase structures to lexicalized reordering models. The well-studied phrase-based architecture can also benefit from syntactic intuitions. Phrasal decoding can be augmented easily, either by syntactic pre-processing or through search-space constraints. Pre-p"
2009.mtsummit-papers.1,C04-1073,0,0.031985,"parser-like decoder, and represents a departure from phrase-based decoding. Galley and Manning (2008) demonstrated how to integrate hierarchical phrase structures to lexicalized reordering models. The well-studied phrase-based architecture can also benefit from syntactic intuitions. Phrasal decoding can be augmented easily, either by syntactic pre-processing or through search-space constraints. Pre-processing approaches parse the source sentence and use the tree to apply rules which reorder the source into a more target-like structure before the translation begins. These rules can be learned (Xia and McCord, 2004; Rottmann and Vogel, 2007) or designed by hand (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009). The pre-processing approach benefits from its simplicity and modularity, but it suffers from limitation of providing at most a first-best guess at syntactic movement. Search space constraints limit the phrasal decoder’s translation search using syntactic intuitions. Zens et al.(2004) demonstrated how to incorporate formally syntactic binary-bracketing constraints into phrase-based decoding. Recently, it has been shown that syntactic cohesion, the notion that syntactic phrases in the sour"
2009.mtsummit-papers.1,N09-1028,0,0.0414708,"ated how to integrate hierarchical phrase structures to lexicalized reordering models. The well-studied phrase-based architecture can also benefit from syntactic intuitions. Phrasal decoding can be augmented easily, either by syntactic pre-processing or through search-space constraints. Pre-processing approaches parse the source sentence and use the tree to apply rules which reorder the source into a more target-like structure before the translation begins. These rules can be learned (Xia and McCord, 2004; Rottmann and Vogel, 2007) or designed by hand (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009). The pre-processing approach benefits from its simplicity and modularity, but it suffers from limitation of providing at most a first-best guess at syntactic movement. Search space constraints limit the phrasal decoder’s translation search using syntactic intuitions. Zens et al.(2004) demonstrated how to incorporate formally syntactic binary-bracketing constraints into phrase-based decoding. Recently, it has been shown that syntactic cohesion, the notion that syntactic phrases in the source sentence tend to remain contiguous in the target (Fox, 2002), can be incorporated into phrasal decoding"
2009.mtsummit-papers.1,W08-0401,0,0.0111863,"ch space constraints limit the phrasal decoder’s translation search using syntactic intuitions. Zens et al.(2004) demonstrated how to incorporate formally syntactic binary-bracketing constraints into phrase-based decoding. Recently, it has been shown that syntactic cohesion, the notion that syntactic phrases in the source sentence tend to remain contiguous in the target (Fox, 2002), can be incorporated into phrasal decoding as well, by following the simple intuition that any source subtree that has begun translation, must be completed before translating another part of the tree (Cherry, 2008; Yamamoto et al., 2008). In this paper, we introduce a novel reordering model for phrase-based systems which exploits dependency subtree movements and constraints. In order to do, we must first consider several questions. Should subtree movements be conditioned on source dependency structures? How can we estimate reliable probability distributions from training data? How do we incorporate the reordering model with dependency structures and cohesive constraints into a phrase-based decoder? We investigate these questions by presenting the model, training and decoding procedure in Section 2. Furthermore, we present exp"
2009.mtsummit-papers.1,C04-1030,0,0.0404651,"Missing"
2009.mtsummit-papers.1,2005.eamt-1.39,1,0.844347,"ive constraints after detecting the orientation of the current phrase, for example swap. The decoder only considers one source-tree reordering feature. The choice of feature depends on the output of the interruption check algorithm on the current phrase. If the return is inside then S I will be used otherwise S O. 3 Experimental Results We built baseline systems using GIZA++ (Och and Ney, 2003), Moses’ phrase extraction with the growdiag-final-and heuristic (Koehn et al., 2007), a standard phrase-based decoder (Vogel, 2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a lexicalized reordering model with a reordering window of 3, and the maximum number of target phrases restricted to 5. Results are reported using lowercase BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). All model weights were trained on development sets via minimum-error rate training (MERT) (Venugopal and Vogel, 2005) with an unique 200-best list and optimizing toward BLEU. To shorten the training time, a multi-threaded GIZA++ version was used to utilize multi-processor servers (Gao and Vogel, 2008). We used the MALT parser (Nivre et al., 2006) to get English dependency trees."
2009.mtsummit-papers.5,W08-0509,1,0.870933,"Missing"
2009.mtsummit-papers.5,N03-1017,0,0.00797689,"verall, we do observe the tendency that less number of unaligned words in the word alignment leads to better quality of the extracted phrase pairs. In other words low precision/ high recall alignment results in fewer but higher quality phrase pairs. To balance the trade-off between higher quality phrases and coverage, we conducted a series of translation experiments were the number of unaligned words was taken as a feature. In next section, we describe them thoroughly. 6 From Phrases to Translations After the phrases are extracted, they are scored according to the MLE estimation described in (Koehn et al., 2003). Also, the reordering models and the lexical weighting are estimated. Then, these models (along with the language model) are used during decoding. For this study, we wanted to analyze the Setup For this experiment, we used a training data set consisting of the GALE P3 Data3 . The data was filtered to have maximum sentence length 30. The final training set contains one million sentences. The different systems that were used, were built upon the alignments from the DWA with p = {0.1..0.0}, and the symmetrized alignment (grow-diag-final). The DWA Tuning remained the same as for Sec 3. We use the"
2009.mtsummit-papers.5,P07-2045,0,0.0177331,"o detangle the intricate relationships between the word alignment and the phrase extraction. In the following section we analyze different characteristics of the alignment that have an impact on the phrase-table generation. 3 performing training through the standard sequence of word alignment models IBM1, HMM, IBM3 and finally IBM4, in both directions, i.e. source to target (S2T) and target to source (T2S). We used the modified GIZA toolkit (Gao and Vogel, 2008). In addition, we generated the symmetrized alignment, using the grow-diag-final heuristic implemented and used in the MOSES package (Koehn et al., 2007). For the discriminative alignments, we used the approach described in (Niehues and Vogel, 2008), because the output alignment matrix generated by such a system is composed of continuous values representing the alignment strength between source and target word. Therefore it allows to easily control the density of the alignment matrix, by using different intensity thresholds, without having to recalculate the alignment. The different thresholds used throughout this paper are p = {0.1, 0.2, ..., 0.9}. In the following experiments, the discriminative word aligner (DWA) uses the models from the GI"
2009.mtsummit-papers.5,W08-0303,1,0.837733,"d alignments in machine translation remains rather unclear. Furthermore, there are several processing steps that follow the word alignment which rarely are taken into account. Most notably the algorithm used to extract phrase pairs consistent with the word alignment (Och and Ney, 2004). The goal of better understanding the relationship between the alignment metrics (AER, precision, recall) and translation quality, is to make improvements in word alignment carry over to improvements in the end-to-end system performance. This is especially important in the case of discriminative word alignment (Niehues and Vogel, 2008), where optimization towards a given manual alignment is used. In this paper we study in more detail the dependencies between the word alignment and the phrase extraction, as an effort to better understand the role of word alignments in phrase extraction. We explore characteristics of the alignment such as link density and number of unaligned words, and their implications on the phrase-pairs extracted from them. We also make a first attempt to include the findings of our analysis as new features of the translation model. The remainder of this paper is organized as follows: in Section 2 we give"
2009.mtsummit-papers.5,J04-4002,0,0.11637,"re being used in translation (e.g. phrase extraction), the quality of word alignments is not necessarily related to the quality of translation. This poses the question of whether alignment quality metrics (such as AER) might not be as good predicting translation quality as other metrics. As a result, the role of the quality of word alignments in machine translation remains rather unclear. Furthermore, there are several processing steps that follow the word alignment which rarely are taken into account. Most notably the algorithm used to extract phrase pairs consistent with the word alignment (Och and Ney, 2004). The goal of better understanding the relationship between the alignment metrics (AER, precision, recall) and translation quality, is to make improvements in word alignment carry over to improvements in the end-to-end system performance. This is especially important in the case of discriminative word alignment (Niehues and Vogel, 2008), where optimization towards a given manual alignment is used. In this paper we study in more detail the dependencies between the word alignment and the phrase extraction, as an effort to better understand the role of word alignments in phrase extraction. We exp"
2009.mtsummit-papers.5,2006.iwslt-papers.7,0,0.293207,"Missing"
2009.mtsummit-papers.5,J07-3002,0,\N,Missing
2009.mtsummit-papers.5,P06-1002,0,\N,Missing
2011.iwslt-evaluation.23,2005.mtsummit-papers.33,1,0.410244,"Sanjika Hewavitharana and Stephan Vogel Language Technologies Institute, Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 {mridulg, sanjika, vogel+}@cs.cmu.edu Abstract Phrase alignment is a crucial step in phrase-based statistical machine translation. We explore a way of improving phrase alignment by adding syntactic information in the form of chunks as soft constraints guided by an in-depth and detailed analysis on a hand-aligned data set. We extend a probabilistic phrase alignment model that extracts phrase pairs by optimizing phrase pair boundaries over the sentence pair [1]. The boundaries of the target phrase are chosen such that the overall sentence alignment probability is optimal. Viterbi alignment information is also added in the extended model with a view of improving phrase alignment. We extract phrase pairs using a relatively larger number of features which are discriminatively trained using a large-margin online learning algorithm, i.e., Margin Infused Relaxed Algorithm (MIRA) and integrate it in our approach. Initial experiments show improvements in both phrase alignment and translation quality for Arabic-English on a moderate-size translation task. 1."
2011.iwslt-evaluation.23,N03-1017,0,0.0201838,"s which are discriminatively trained using a large-margin online learning algorithm, i.e., Margin Infused Relaxed Algorithm (MIRA) and integrate it in our approach. Initial experiments show improvements in both phrase alignment and translation quality for Arabic-English on a moderate-size translation task. 1. Introduction Phrase-based statistical machine translation has been around for several years It has been well decribed and discussed in [2] and [3]. Most of these phrase-based approaches rely on robust word alignment strategies for phrase pair extraction like the IBM word alignment models [4]. The now standard approach proposed by [2] relied on heuristics to extract phrase pairs by reading off the Viterbi path generated from word alignment models [5] and using maximum likelihood estimates (MLE) for phrase scoring. [1] proposed a novel probabilistic phrase extraction algorithm which viewed phrase alignment as a sentence splitting problem (PESA). Given a source phrase, the algorithm finds boundaries of the target phrase by optimizing overall sentence alignment probability. This method does not rely on the traditional Viterbi alignment approach as cited above. Phrase pairs are extrac"
2011.iwslt-evaluation.23,D07-1080,0,0.0205948,"ranslation model by adding a set of features based on syntax and alignment. It is then important to combine features in such a way that the phrase extraction step is optimized over aligning the entire sentence pair. Hence, we use an online large-margin training algorithm, i.e., Margin Infused Relaxed Algorithm, (MIRA) developed by [9] to optimize weights over an extended set of features optimized towards an oracle selection. Online discriminative learning algorithms have been popular in the SMT domain, and researchers have used the MIRA algorithm to train MT systems in the past. For instance, [10], [11] used MIRA algorithm to train MT systems over a large number of features during decoding time. The PESA approach described in [1] only used a manually derived set of weights for optimization, since it relied only on lexical information features obtained from word alignment models. The main contributions of this work are: 1. Extending a phrase alignment approach by adding syntax and alignment information. 2. Incorporating an online large-margin training method to optimize weights during phrase extraction. In Section 2 we give a brief overview of related work. 175 Section 3 we describe our"
2011.iwslt-evaluation.23,D08-1024,0,0.115306,"tion model by adding a set of features based on syntax and alignment. It is then important to combine features in such a way that the phrase extraction step is optimized over aligning the entire sentence pair. Hence, we use an online large-margin training algorithm, i.e., Margin Infused Relaxed Algorithm, (MIRA) developed by [9] to optimize weights over an extended set of features optimized towards an oracle selection. Online discriminative learning algorithms have been popular in the SMT domain, and researchers have used the MIRA algorithm to train MT systems in the past. For instance, [10], [11] used MIRA algorithm to train MT systems over a large number of features during decoding time. The PESA approach described in [1] only used a manually derived set of weights for optimization, since it relied only on lexical information features obtained from word alignment models. The main contributions of this work are: 1. Extending a phrase alignment approach by adding syntax and alignment information. 2. Incorporating an online large-margin training method to optimize weights during phrase extraction. In Section 2 we give a brief overview of related work. 175 Section 3 we describe our analy"
2011.iwslt-evaluation.23,2010.eamt-1.27,0,0.0718072,"ased analysis 2. Related work There has been a strong line of research focused on incorporating syntax in SMT systems, chunk information being one approach. Our work is also based on adding chunk-based information as syntactic features to an existing phrase alignment method. Hence, we focus our attention on incorporating chunk information for SMT. A chunk has been well defined by [12]. Combining locally grouped words (a constituent) as one translation unit has been shown helpful in improving performance of various machine translation systems. One of the recent work in this line of research is [13]. They built a chunk-based examplebased machine translation (EBMT) system in which each chunk is treated as a translation unit. It combined a typical EBMT system and a chunk-based system to produce target translations in the form of linguistically motivated chunks. It backed off to the standard EBMT approach, for translating target fragments that were not chunks. Following the chunk-based paradigm, they adapted standard word alignment models to align chunks by treating each chunk as an individual word. This was done to account for sparseness in terms of statistical evidence for words locally g"
2011.iwslt-evaluation.23,P03-1039,0,0.0235137,"the form of linguistically motivated chunks. It backed off to the standard EBMT approach, for translating target fragments that were not chunks. Following the chunk-based paradigm, they adapted standard word alignment models to align chunks by treating each chunk as an individual word. This was done to account for sparseness in terms of statistical evidence for words locally grouped as chunks. They achieved improved performance over baseline systems for Korean-English and Chinese-English translation tasks using this approach. Some of the previous related work on chunk MT also include that of [14] and [15]. [15] had proposed an SMT approach based on combining chunking knowledge. They decomposed the translation model into three levels: sentence level reordering, chunk mapping and translation of words within a chunk pair. [14] treated each translation unit as a chunk by breaking down the translation model into chunk alignment, and word alignment within chunks for translation. They subsequently performed chunk reordering in a sentence pair chunked on both sides. Each of the above cited work treats chunks as translation units in the translation model. They either completely backoff to a ph"
2011.iwslt-evaluation.23,P08-1114,0,0.271189,"formation in phrasebased SMT systems proves to be a tradeoff between unlinguistically motivated phrase pair extraction from parallel text versus incorporating benefits of linguistic analyses derived before training time. Hard linguistic constraints improve quality somewhat but lose out on coverage. It is thus important to use this linguistic a priori knowledge as a ‘soft constraint’ rather than forcing the MT system to completely ignore unlinguistic but strong mappings in the parallel corpus which could result in deterioration of performance. This fact has been emphasized in previous works of [6], [7] and [8]. As stated above, we extend the translation model by adding a set of features based on syntax and alignment. It is then important to combine features in such a way that the phrase extraction step is optimized over aligning the entire sentence pair. Hence, we use an online large-margin training algorithm, i.e., Margin Infused Relaxed Algorithm, (MIRA) developed by [9] to optimize weights over an extended set of features optimized towards an oracle selection. Online discriminative learning algorithms have been popular in the SMT domain, and researchers have used the MIRA algorithm"
2011.iwslt-evaluation.23,W08-0509,1,0.809186,"er similar to that of NIST BLEU evaluation script3 . The smoothed version  of BLEU is computed by adding a partial count of 1/2k , for each precision score whose matching n-gram count is zero, where k=1 for the first ‘n’ value for which the n-gram match count is zero. The two different loss functions are given by: L(yt , y 0 ) = W ER(yt , y 0 ) 3 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl 7.1. Data and experimental setup Our data consisted of 641,414 Arabic-English parallel sentences (18.5+19.1 million words) from the news domain obtained from LDC. We trained IBM4 model with MGIZA [19] on the dataset in both directions. Alignment points were refined by using the grow-diag-final heuristic as proposed by [3]. We also ran AMIRA toolkit [17] with ATB segmentation on the Arabic side of the parallel corpus and TreeTagger on English side of the parallel corpus for POS-tagging and chunking. MGIZA was again used to train IBM4 alignment model for the chunked corpus consisting only of chunk labels on both sides. We trained a 5-gram language model on the English Gigaword corpus. We used the Moses toolkit 4 for SMT for decoding. We also experimented with a 21107-sentence parallel hand-a"
2011.iwslt-evaluation.23,P06-1121,0,0.0467558,"tion in phrasebased SMT systems proves to be a tradeoff between unlinguistically motivated phrase pair extraction from parallel text versus incorporating benefits of linguistic analyses derived before training time. Hard linguistic constraints improve quality somewhat but lose out on coverage. It is thus important to use this linguistic a priori knowledge as a ‘soft constraint’ rather than forcing the MT system to completely ignore unlinguistic but strong mappings in the parallel corpus which could result in deterioration of performance. This fact has been emphasized in previous works of [6], [7] and [8]. As stated above, we extend the translation model by adding a set of features based on syntax and alignment. It is then important to combine features in such a way that the phrase extraction step is optimized over aligning the entire sentence pair. Hence, we use an online large-margin training algorithm, i.e., Margin Infused Relaxed Algorithm, (MIRA) developed by [9] to optimize weights over an extended set of features optimized towards an oracle selection. Online discriminative learning algorithms have been popular in the SMT domain, and researchers have used the MIRA algorithm to tr"
2011.iwslt-evaluation.23,P03-1021,0,0.00573253,"5.32 33.65 TERp Scores MT04 MT05 66.69 61.01 64.45 59.70 64.72 58.77 MT06 23.01 24.92 24.49 MT08 22.60 23.96 24.29 Avg na +1.93 +1.63 MT06 69.32 69.43 69.51 MT08 71.73 70.60 71.71 Avg na -1.14 -1.01 The results reported in this paper are based on one particular set of features in addition to the baseline per system. Hence, we intend to incorporate all sets of features into a single translation model and optimize system performance. This approach of integration of all sources of information in the model requires online discriminative training methods since minimum error rate training algorithm [20] has been shown to be unreliable for a larger number of features. Hence, we would like to integrate the MIRA algorithm into a decoder [21] with which results were first reported for the baseline PESA method in [1]. We also intend to integrate the extended model with an online phrase alignment step ([1]) which eliminates the need for generating large phrase tables offline. 181 8. Conclusion and future work We presented an approach that can be incorporated successfully into a probabilistic phrase alignment and scoring system. This system treats phrase pair extraction as a sentence splitting prob"
2011.iwslt-evaluation.23,J93-2003,0,\N,Missing
2011.iwslt-evaluation.23,J03-1002,0,\N,Missing
2011.iwslt-evaluation.23,J07-2003,0,\N,Missing
2011.mtsummit-papers.12,ambati-etal-2010-active,1,0.722407,"re-training and re-tuning an SMT system after translating every single sentence is computationally inefﬁcient and may not have a signiﬁcant effect on the underlying models. We, therefore continue to select a batch of N sentences before retraining the system on newly created labeled set Lk=1 . Our framework for active learning in SMT is discussed in Algorithm 1. 3.2 P hrases(s) d(s) = u(s) = 124 P (x|U ) ∗ e−λcount(x|L) |P hrases(S)| P hrases(s) Sentence Selection Our sentence selection strategy to be independent of the underlying SMT system or the models and has been shown to perform well (Ambati et al., 2010). For the sake of comprehensiveness we discuss the approach here as well. We use only monolingual data U and bilingual corpus L to select sentences. This makes our approach applicable to any corpusbased MT paradigm and system, even though we test with SMT. The basic units of an SMT system are x α |P hrases(s)| x α = Score(s) = 4 1 x∈ / P hrases(L) 0 (1 + β 2 )d(s) ∗ u(s) β 2 d(s) + u(s) DUAL Strategy Let us consider the DWDS approach in more detail. It has two components for scoring a sentence Figure 1: Density vs Diversity performance curves S, a density component d(s) and a diversity compone"
2011.mtsummit-papers.12,2005.mtsummit-papers.30,1,0.845432,"requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. (Gangadharaiah et al., 2009) use a pool-based strategy that maximizes a measure of expected future improvement, to sample instances from a large parallel corpus. Their goal is to select the most informative sentence pairs to build an MT system, and hence they assume the existence of target-side translations along with the source-side sentences. We however are interested in selecting most informative sentences to reduce the effort and cost involved in translation. (Eck et al., 2005) use a weighting scheme to select more informative sentences, wherein the importance is estimated using unseen n-grams in previously selected sentences. Although our selection strategy has a density based motivation similar to theirs, we augment this by adding a diminishing effect to discourage the domination of density and favor unseen n-grams. Our approach, therefore, naturally works well in pool-based active learning strategy when compared to (Eck et al., 2005). In case of instancebased active learning, both approaches work comparably, with our approach working slightly better. Ensemble app"
2011.mtsummit-papers.12,W09-4633,1,0.857303,"004; Steedman et al., 2003; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For SMT, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. (Gangadharaiah et al., 2009) use a pool-based strategy that maximizes a measure of expected future improvement, to sample instances from a large parallel corpus. Their goal is to select the most informative sentence pairs to build an MT system, and hence they assume the existence of target-side translations along with the source-side sentences. We however are interested in selecting most informative sentences to reduce the effort and cost involved in translation. (Eck et al., 2005) use a weighting scheme to select more informative sentences, wherein the importance is estimated using unseen n-grams in previously selected"
2011.mtsummit-papers.12,N09-1047,0,0.23323,"on 2. 123 2 Related Work Active learning has been applied to various ﬁelds of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Steedman et al., 2003; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For SMT, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. (Gangadharaiah et al., 2009) use a pool-based strategy that maximizes a measure of expected future improvement, to sample instances from a large parallel corpus. Their goal is to select the most informative sentence pairs to build an MT system, and hence they assume the existence of target-side translations along with the source-side sentences. We however are interested in selecting most informative sentences to reduce the effort and cost involved"
2011.mtsummit-papers.12,J04-3001,0,0.0205627,"ction 3 we present our framework for active learning in SMT and discuss our sentence selection algorithm. Section 4 describes DUAL, a multi-strategy approach that focuses on switching between two strategies. Section 5 discusses GraDUAL, another hybrid approach that addresses some of the issues with DUAL. Section 6 presents experiments and results on Spanish-English language pair. We conclude with discussion of related work in Section 2. 123 2 Related Work Active learning has been applied to various ﬁelds of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Steedman et al., 2003; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For SMT, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. (Gan"
2011.mtsummit-papers.12,P07-2045,0,0.00472538,"ting with other functions for f (β). β = Abs(Δ(DW DS) − Δ(DIV )) α = ⎧ ⎪ ⎨ 1 β>δ 0 β<δ ⎪ ⎩ f (β) Score(s) = αDW DS(s) + (1 − α)DIV (s) Figure 3: Spanish-English results:Single-Strategy 6 6.1 Experiments Setup We perform our experiments on the Spanish-English language pair in order to simulate a resource-poor language pair. We have parallel corpora and evaluation data sets for the Spanish-English language pair allowing us to run multiple experiments efﬁciently. We use BTEC parallel corpus (Takezawa et al., 2002) from the IWSLT tasks with 127K sentence pairs. We use the standard Moses pipeline (Koehn et al., 2007) for extraction, training and tuning our system. We built an SRILM language model using English data consisting of 1.6M words. While experimenting with data sets of varying size, we do not vary the language model. The weights of the different translation features are tuned using standard MERT (Och, 2003). Our development set consists of 506 sentences and test set consists of 343 sentences. We report results on the test set. 6.2 Results: AL for MT We ﬁrst test the performance of our active learning sentence selection strategy. We start with an initial system trained on 1000 sentence pairs. We t"
2011.mtsummit-papers.12,P03-1021,0,0.0278712,"uage pair. We have parallel corpora and evaluation data sets for the Spanish-English language pair allowing us to run multiple experiments efﬁciently. We use BTEC parallel corpus (Takezawa et al., 2002) from the IWSLT tasks with 127K sentence pairs. We use the standard Moses pipeline (Koehn et al., 2007) for extraction, training and tuning our system. We built an SRILM language model using English data consisting of 1.6M words. While experimenting with data sets of varying size, we do not vary the language model. The weights of the different translation features are tuned using standard MERT (Och, 2003). Our development set consists of 506 sentences and test set consists of 343 sentences. We report results on the test set. 6.2 Results: AL for MT We ﬁrst test the performance of our active learning sentence selection strategy. We start with an initial system trained on 1000 sentence pairs. We then train the system iteratively on datasets of increasing size. In each iteration, we ﬁrst selectively sample 1000 Spanish sentences from source side of the entire corpus. We simulate human translation in our experiment, as we already have access to the translations from the BTEC corpus. We then retrain"
2011.mtsummit-papers.12,P04-1075,0,0.0204503,"or active learning in SMT and discuss our sentence selection algorithm. Section 4 describes DUAL, a multi-strategy approach that focuses on switching between two strategies. Section 5 discusses GraDUAL, another hybrid approach that addresses some of the issues with DUAL. Section 6 presents experiments and results on Spanish-English language pair. We conclude with discussion of related work in Section 2. 123 2 Related Work Active learning has been applied to various ﬁelds of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Steedman et al., 2003; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For SMT, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. (Gangadharaiah et al., 2009) use a pool-based s"
2011.mtsummit-papers.12,N03-1031,0,0.0377548,"present our framework for active learning in SMT and discuss our sentence selection algorithm. Section 4 describes DUAL, a multi-strategy approach that focuses on switching between two strategies. Section 5 discusses GraDUAL, another hybrid approach that addresses some of the issues with DUAL. Section 6 presents experiments and results on Spanish-English language pair. We conclude with discussion of related work in Section 2. 123 2 Related Work Active learning has been applied to various ﬁelds of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Steedman et al., 2003; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For SMT, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. (Gangadharaiah et al., 2009"
2011.mtsummit-papers.12,takezawa-etal-2002-toward,0,0.0636683,"Missing"
2013.iwslt-evaluation.8,P07-2045,0,0.0254044,"ibe a specialized normalization scheme for evaluating Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by"
2013.iwslt-evaluation.8,W10-1738,0,0.130794,"r evaluating Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, a"
2013.iwslt-evaluation.8,P10-4002,0,0.0806549,"Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc."
2013.iwslt-evaluation.8,N06-2013,0,0.10965,"coders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc. We experimented with standard segmentation schemes such as D0, D1, D2, D3, S2 and ATB, as defined in MADA [4, 5]. See Section 5 for details. • Domain adaptation: We experimented with three domain adaptation methods to make better use of the huge UN data, which is out-of-domain: (i) Modified Moore-Lewis filtering, (ii) phrase table merging, and (iii) phrase table backoff. See Section 7 for details. For our final submission, we synthesized a translation by combining the output of our best individual system with the output of other systems that are both relatively strong and can contribute to having more diversity, e.g., using a different decoder or a different segmentation scheme. We achieved the most not"
2013.iwslt-evaluation.8,P08-2039,0,0.073174,"coders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc. We experimented with standard segmentation schemes such as D0, D1, D2, D3, S2 and ATB, as defined in MADA [4, 5]. See Section 5 for details. • Domain adaptation: We experimented with three domain adaptation methods to make better use of the huge UN data, which is out-of-domain: (i) Modified Moore-Lewis filtering, (ii) phrase table merging, and (iii) phrase table backoff. See Section 7 for details. For our final submission, we synthesized a translation by combining the output of our best individual system with the output of other systems that are both relatively strong and can contribute to having more diversity, e.g., using a different decoder or a different segmentation scheme. We achieved the most not"
2013.iwslt-evaluation.8,P12-1016,0,0.268259,"lish BLEU 1-TER System English IWSLT mono 109 English-French SETimes UN (Es-En + En-Fr) UN (Ar-En) News Crawl 2007-2009 News Crawl 2009-2012 Common Crawl Wiki Headlines Europarl v.7 News Commentary v.8 Gigaword v.5 2.7M 575M 4.2M 597M 115M 643M 745M 185M 1.1M 54M 5.3M 4,032M Arabic IWSLT mono UN News Commentary Arabic v.8 Gigaword Arabic v.5 2.7M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We"
2013.iwslt-evaluation.8,J93-2003,0,0.0246271,"abic IWSLT mono UN News Commentary Arabic v.8 Gigaword Arabic v.5 2.7M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard"
2013.iwslt-evaluation.8,N03-1017,0,0.0173015,"M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, th"
2013.iwslt-evaluation.8,2005.iwslt-1.8,0,0.0225691,"rd tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. De"
2013.iwslt-evaluation.8,W11-2123,0,0.408919,"sing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On"
2013.iwslt-evaluation.8,P02-1040,0,0.0879883,", thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On testing, we further used cube pruning. Table 2 shows the results3 for the baseline English-toArabic and Arabic-to-English SMT systems, compared to the baseline results reported on the WIT3 webpage. 3 For tst2010, we report MultEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers"
2013.iwslt-evaluation.8,D11-1125,0,0.032033,"each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On testing, we further used cube pruning. Table 2 shows the results3 for the baseline English-toArabic and Arabic-to-English SMT systems, compared to the baseline results reported on the WIT3 webpage. 3 For tst2010, we report MultEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers used slightly different scorers. IWSL"
2013.iwslt-evaluation.8,C12-1121,1,0.915839,"ltEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers used slightly different scorers. IWSLT baseline Our baseline 23.6 24.7 English-Arabic BLEU 1-TER 43.0 45.6 11.9 12.6 28.6 29.1 Table 2: Our vs. IWSLT baseline results for English-toArabic and Arabic-to-English SMT, evaluated on tst2010. 4. System Settings Below we discuss the decoder settings and extensions we experimented with, focusing on Arabic-to-English. Table 3 shows the impact of each feature when added to the baseline. Tuning. [13] have shown that PRO tends to generate too short translations.4 They have suggested that the root of the problem was that PRO optimizes sentence-level BLEU+1, which smooths the precision component of BLEU, but leaves the brevity penalty intact, which destroys the balance between them. They have proposed a number of fixes, the simplest and most efficient among them being to smooth the brevity penalty as well.5 In our experiments, this yielded +0.2 BLEU for Arabic-to-English on tst2010. Operation sequence model. The operation sequence model (OSM) is an n-gram-based model, which represents the al"
2013.iwslt-evaluation.8,P13-2003,1,0.839319,"Missing"
2013.iwslt-evaluation.8,P13-2071,0,0.0200742,"of operations, e.g., generate a sequence of source and target words or perform reordering. The model memorizes Markov chains over such sequences, thus fusing lexical generation and reordering into a single generative model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0"
2013.iwslt-evaluation.8,W13-2212,0,0.0142268,"urce and target words or perform reordering. The model memorizes Markov chains over such sequences, thus fusing lexical generation and reordering into a single generative model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Trans"
2013.iwslt-evaluation.8,N04-1022,0,0.108744,"e model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1"
2013.iwslt-evaluation.8,P11-1044,1,0.871958,"o best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1 word alignments from a subset of the UN bitext, and we used them to train a character-level transliteration system [18, 19] using Moses. As Table 3 shows this did not help, probably due to the small number of OOVs in tst2010. 4 See [14] for a discussion about more potential issues with PRO. --proargs=’--smooth-brevity-penalty’ 5 Available in Moses: System Baseline (B) OSM MBR Ttable 100 PRO-fix [13] TRANSLIT Drop UNK Arabic-English (tst2010) BLEU 1-TER 24.7 25.3 24.7 24.8 24.9 24.7 24.8 Arabic-English (tst2010) BLEU 1-TER System 45.6 46.1 45.7 45.6 44.7 45.6 45.7 SEG-D0 SEG-D1 SEG-D2 SEG-D3 SEG-S2 SEG-ATB 22.4 23.6 24.1 24.4 24.5 24.7 43.0 44.2 45.2 45.5 45.7 45.6 Table 5: Using different Arabic segmentation schem"
2013.iwslt-evaluation.8,P12-1049,1,0.860515,"o best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1 word alignments from a subset of the UN bitext, and we used them to train a character-level transliteration system [18, 19] using Moses. As Table 3 shows this did not help, probably due to the small number of OOVs in tst2010. 4 See [14] for a discussion about more potential issues with PRO. --proargs=’--smooth-brevity-penalty’ 5 Available in Moses: System Baseline (B) OSM MBR Ttable 100 PRO-fix [13] TRANSLIT Drop UNK Arabic-English (tst2010) BLEU 1-TER 24.7 25.3 24.7 24.8 24.9 24.7 24.8 Arabic-English (tst2010) BLEU 1-TER System 45.6 46.1 45.7 45.6 44.7 45.6 45.7 SEG-D0 SEG-D1 SEG-D2 SEG-D3 SEG-S2 SEG-ATB 22.4 23.6 24.1 24.4 24.5 24.7 43.0 44.2 45.2 45.5 45.7 45.6 Table 5: Using different Arabic segmentation schem"
2013.iwslt-evaluation.8,P03-1021,0,0.0120716,"hierarchical cdec decoder [3]. We used its default features: forward and backward translation features, singleton features, a glue-rule probability, and a pass-through feature (to handle OOVs). We tuned the parameters using MIRA with IBM BLEU as the objective function and a k-best forest size of 250. Jane. We also used another hierarchical phrase-based decoder: Jane 2.2 [2]. We used the standard features: phrase translation probabilities and lexical smoothing in both directions, word and phrase penalties, a distance-based distortion model, and a 5-gram LM. We optimized the weights using MERT [21] on 100-best candidates with BLEU as objective. 5. Arabic Segmentation 7. Adaptation In Arabic, various clitics such as pronouns, conjunctions and articles appear concatenated to content words such as nouns and verbs. This can cause data sparseness issues, and thus clitics are typically segmented in a preprocessing step. There are various standard segmentation schemes defined in MADA [4, 5] such as D0, D1, D2, D3 and S2, for which we used the MADA+TOKAN toolkit [20], as well as ATB, which we performed using the Stanford segmenter [6]. Table 5 shows the results when training on the TED bitext o"
2013.iwslt-evaluation.8,P10-2041,0,0.0888063,"Missing"
2013.iwslt-evaluation.8,D11-1033,0,0.090854,"Missing"
2013.iwslt-evaluation.8,W08-0320,1,0.891463,"Missing"
2013.iwslt-evaluation.8,D09-1141,1,0.919233,"Missing"
2013.iwslt-evaluation.8,W09-0408,0,0.0608322,"Missing"
2013.iwslt-evaluation.8,W12-5611,0,0.0448251,"could build a strong LM through interpolation, similarly to our Arabic-to-English LM, that also used the Gigaword Arabic, UN, and News Commentary data (see Table 1). Desegmentation. Unlike the Arabic-to-English direction, where the segmentation was on the input side and thus the output was unaffected, here the segmentation had to be undone. For example, if we use an ATB-segmented target side, we end up with an ATB-segmented translation output, which we have to desegment in order to obtain proper Arabic. Desegmentation is not a trivial task since it involves some morphological adjustments, see [27] for a broader discussion. For desegmentation, we used the best approach described in [27]; in fact, we used their implementation. Normalization. Translating into Arabic is tricky because the Arabic spelling is often inconsistent in terms of punctuation (using both Arabic UTF8 and English punctuation symbols), digits (appearing as both Arabic and Indian characters), diacritics (can be used or omitted, and can often be wrong), spelling (there are many errors in the spelling of some Arabic characters, esp. Alef and Ta Marbuta; also, Waa appears sometimes separated). These problems are especially"
2013.iwslt-papers.2,2010.iwslt-evaluation.1,0,0.145712,"sed for this task, and also observe an absolute improvement of 1.6 BLEU when it is used in combination with TED data. Finally, we analyze some of the specific challenges when translating the educational content. 1. Introduction Lecture Translation has become an active field of research in the wider area of Speech Translation [1, 2]. This is demonstrated by large scale projects like the EU-funded translectures [3] and by evaluation campaigns like the one organized as part of the International Workshop on Spoken Language Translation (IWSLT), which introduced the challenge to translate TED talks [4] for the 2010 competition. However, the main limitation for the success of these projects continues to be the access to high quality training data. With the emergence of Massive Online Open Courses (MOOCs), thousands of video lectures have already been generated. Sites like Khan Academy1 , Coursera2 , Udacity3 , etc., continuously increase their repertoire of lectures, which range from basic math and science topics, to more advanced topics like machine learning, also covering history, economy, psychology, medicine, and more. Online education has bridged the geographical and financial gap, enab"
2013.iwslt-papers.2,federico-etal-2012-iwslt,0,0.0409166,"les are not available. It also can support volunteer translators, by providing an initial translation, which then can be post-edited [5]. Thus, SMT has the potential to increase the penetration of educational content, allowing it to reach a wider audience. To achieve this, an SMT system requires a large quantity of high-quality in-domain training data. Unfortunately, large data for machine translation has traditionally been constrained to domains such as legal documents, parliamentary proceedings and news. So far, the only openly accessible corpus for the lecture domain has been the TED talks [6]. In this paper, we introduce a new parallel corpus of subtitles of educational videos: the AMARA corpus for online educational content. We crawl a collection of multilingual community-generated subtitles6 . Furthermore, we explore the steps necessary to build corpora suitable for Machine Translation by processing the Arabic-English part of the multilingual collection. This yields a parallel corpus of about 2.6M Arabic and 3.9M English words. We explore different approaches to align the subtitles, and verify the quality of the generated parallel corpus by building translation models, and extri"
2013.iwslt-papers.2,2012.eamt-1.60,0,0.110653,"Loop (CHIL) [7], which consists of recordings and transcriptions of technical seminars and meetings in English. The content of the corpus includes a variety of topics: from audio and visual technologies to biology and finance. It is available through ELRA7 to its members. More recently, the IWSLT10 [4] evaluation campaign has turned its attention to the lecture and seminar domain by focusing on TED talks. To support this task, a collection of lecture translations has been automatically crawled from the TED website in a variety of languages and made publicly available through the WIT3 project [8]. In this paper, we used such data as a point of comparison. We crawl parallel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with r"
2013.iwslt-papers.2,tiedemann-2008-synchronizing,0,0.219263,"s attention to the lecture and seminar domain by focusing on TED talks. To support this task, a collection of lecture translations has been automatically crawled from the TED website in a variety of languages and made publicly available through the WIT3 project [8]. In this paper, we used such data as a point of comparison. We crawl parallel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the langu"
2013.iwslt-papers.2,P07-2045,0,0.0105846,"tional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then interpolated these mod10 We els minimizing the perplexity on the target side of the tuning dataset (IWSLT dev-2010). Finally, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On test"
2013.iwslt-papers.2,P12-1016,0,0.140562,"Missing"
2013.iwslt-papers.2,J93-2003,0,0.041392,"t on the training data 4.2. Experimental Setup Preprocessing: We tokenized the English side of all bi-texts as well as the monolingual data (GigaWord) for language modeling using the standard tokenizer of the Moses toolkit [17]. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus. For the Arabic side, we segmented the corpus following the ATB segmentation scheme with the Stanford word segmenter [18]. Training: We built separate directed word alignments for English→Arabic and for Arabic→English using IBM model 4 [19], and symmetrized them using grow-diag-final-and heuristic [20]. We extracted phrase pairs of maximum length seven. We scored these phrase pairs using maximum likelihood with Kneser-Ney smoothing,as implemented in the moses toolkit, thus obtaining a phrase table where each phrase-pair has the standard five translation model features. We also built a lexicalized reordering model : msdbidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then inter"
2013.iwslt-papers.2,tiedemann-2012-parallel,0,0.10406,"ite in a variety of languages and made publicly available through the WIT3 project [8]. In this paper, we used such data as a point of comparison. We crawl parallel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the language model perplexity. In this paper, we present the experimental results from data gathered from publicly available crowd-generated data, that has proved to be useful for the lectu"
2013.iwslt-papers.2,N03-1017,0,0.0523188,"e tokenized the English side of all bi-texts as well as the monolingual data (GigaWord) for language modeling using the standard tokenizer of the Moses toolkit [17]. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus. For the Arabic side, we segmented the corpus following the ATB segmentation scheme with the Stanford word segmenter [18]. Training: We built separate directed word alignments for English→Arabic and for Arabic→English using IBM model 4 [19], and symmetrized them using grow-diag-final-and heuristic [20]. We extracted phrase pairs of maximum length seven. We scored these phrase pairs using maximum likelihood with Kneser-Ney smoothing,as implemented in the moses toolkit, thus obtaining a phrase table where each phrase-pair has the standard five translation model features. We also built a lexicalized reordering model : msdbidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then interpolated these mod10 We els minimizing the perplexity on the tar"
2013.iwslt-papers.2,W12-3154,0,0.0338013,"allel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the language model perplexity. In this paper, we present the experimental results from data gathered from publicly available crowd-generated data, that has proved to be useful for the lecture domain, but that poses specific challenges, as it has a special focus on online education. 3. The AMARA Corpus Amara is a web-based platform for editing and"
2013.iwslt-papers.2,W11-2123,0,0.0748631,"ng IBM model 4 [19], and symmetrized them using grow-diag-final-and heuristic [20]. We extracted phrase pairs of maximum length seven. We scored these phrase pairs using maximum likelihood with Kneser-Ney smoothing,as implemented in the moses toolkit, thus obtaining a phrase table where each phrase-pair has the standard five translation model features. We also built a lexicalized reordering model : msdbidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then interpolated these mod10 We els minimizing the perplexity on the target side of the tuning dataset (IWSLT dev-2010). Finally, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing o"
2013.iwslt-papers.2,2012.iwslt-papers.7,0,0.0268125,"n with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the language model perplexity. In this paper, we present the experimental results from data gathered from publicly available crowd-generated data, that has proved to be useful for the lecture domain, but that poses specific challenges, as it has a special focus on online education. 3. The AMARA Corpus Amara is a web-based platform for editing and managing subtitles of online videos. It provides an easy-to-use interface, which allows users to collaboratively sub"
2013.iwslt-papers.2,P02-1040,0,0.091011,"uning dataset (IWSLT dev-2010). Finally, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On testing, we further used cube pruning. did not use the second test set for the experiments in this paper. For the baseline system, we trained the phrase and the reordering models on the IWSLT training dataset. The language model was trained on the English side of the IWSLT training data. We"
2013.iwslt-papers.2,J93-1004,0,0.461534,"Missing"
2013.iwslt-papers.2,D11-1125,0,0.042418,"y, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On testing, we further used cube pruning. did not use the second test set for the experiments in this paper. For the baseline system, we trained the phrase and the reordering models on the IWSLT training dataset. The language model was trained on the English side of the IWSLT training data. We tuned the weights on IWSLT-dev2010. B"
2013.iwslt-papers.2,C12-1121,1,0.888738,"odel, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On testing, we further used cube pruning. did not use the second test set for the experiments in this paper. For the baseline system, we trained the phrase and the reordering models on the IWSLT training dataset. The language model was trained on the English side of the IWSLT training data. We tuned the weights on IWSLT-dev2010. Below, we present the experimental resu"
2013.iwslt-papers.2,D09-1141,0,0.300772,"ARA only (T M1 ): Instead of using the IWSLT training data, we built the translation and reordering models using only the AMARA corpus. Concatenation (T M2 ): In this setting, we concatenated AMARA with IWSLT for training of the translation and reordering models. This generally improves word alignment, reduces OOV rate and improves translation quality if two corpora are from similar domain. However, if the added corpus is noisy or of out-of-domain, (e.g. UN data), we can observe a degradation in performance. Phrase table combination (T M3 ): We applied phrase table combination as described in [25]. We built two phrase tables and reordering models separately on the IWSLT and AMARA data. Then, we merged them by adding three additional indicator features to each entry to inform the decoder if the phrase was found in the first, second or both tables. This can be seen as a form of log-linear interpolation. SYS TM IW10 OOV AM13 OOV SYS LM IW10 AM13 B1 TM1 TM2 TM3 IWSLT AMARA IW+AM PT(IW,AM) 22.97 22.40 23.41 23.57 1.9 2.4 1.2 1.2 23.26 23.66 27.63 27.65 3.9 1.7 1.8 1.8 B1 LM1 LM2 LM3 LM4 IWSLT AMARA IWSLT+AMARA INTERPOL GW 22.97 22.83 23.69 23.59 24.24 23.26 24.05 25.90 25.62 24.79 Table 4:"
2014.iwslt-papers.1,W12-2301,0,0.0369333,"Missing"
2014.iwslt-papers.1,elmahdy-etal-2014-development,0,0.0830499,"Missing"
2014.iwslt-papers.1,J14-1006,0,0.0123809,"n excess of 15 million per day (private communication). To build a dialectal tweet corpus a multi-step procedure was used: 1) Arabic tweets were extracted by issuing the query lang:ar against the Twitter API3 . 2) Each tweet was classified as dialectal or not dialectal. 3) Dialectal tweets were mapped, if possible, to a country. If such a mapping was possible, the tweet was classified as being written in the dialect associated with that country according to Figure 2. In more detail: To perform step 2, dialectal words were extracted from the Arabic Online Commentary Dataset (AOCD) described in [20]. Examples of words used in di «  E$An, J ë hyk,  @ Ay$,ñ» @ Ako, ñJ  alects: ø X dy, àA  $nw, @ð wA$ etc. As shown in [14], many of these dialectal words are used in more than one dialect. I.e. these words do not map a tweet uniquely to a dialect. For example the word Figure 3: Dialectal Tweets Distribution Percentages. 5. Speech Recognition This section describes the details of the speech recognition system, esp. the acoustic model training and the language models used in the experiments. 5.1. Language Modeling Following [7] we wanted to test the impact of using tweets when buildin"
2014.iwslt-papers.1,N12-1006,0,0.144049,"Missing"
2014.iwslt-papers.1,W14-3601,1,0.833102,"tweets were extracted by issuing the query lang:ar against the Twitter API3 . 2) Each tweet was classified as dialectal or not dialectal. 3) Dialectal tweets were mapped, if possible, to a country. If such a mapping was possible, the tweet was classified as being written in the dialect associated with that country according to Figure 2. In more detail: To perform step 2, dialectal words were extracted from the Arabic Online Commentary Dataset (AOCD) described in [20]. Examples of words used in di «  E$An, J ë hyk,  @ Ay$,ñ» @ Ako, ñJ  alects: ø X dy, àA  $nw, @ð wA$ etc. As shown in [14], many of these dialectal words are used in more than one dialect. I.e. these words do not map a tweet uniquely to a dialect. For example the word Figure 3: Dialectal Tweets Distribution Percentages. 5. Speech Recognition This section describes the details of the speech recognition system, esp. the acoustic model training and the language models used in the experiments. 5.1. Language Modeling Following [7] we wanted to test the impact of using tweets when building the language model for the speech recognition system. This leads to a number of questions: Is it better to use all dialectal tweets"
2015.mtsummit-papers.10,abdelali-etal-2014-amara,1,0.856126,"a standard task of translating German-to-English and Arabic-to-English IWSLT TED talks, we observed statistically significant improvements of up to +0.9 BLEU points. 1 Introduction Parallel data required to train Statistical Machine Translation (SMT) systems is often inadequate, and is typically collected opportunistically from wherever it is available. The conventional wisdom is that more data improves the translation quality. Additional data however, may not be best suited for tasks such as translating TED talks (Cettolo et al., 2014) or patents (Fujii et al., 2010) or educational content (Abdelali et al., 2014), and often come with the challenges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution i"
2015.mtsummit-papers.10,D11-1033,0,0.702839,"ni et al., 2002) improvements of up to +0.6 for DE-EN and +0.9 for AR-EN. • Log-linear variant performed better in the case of NNJM giving an average improvements of +0.4 BLEU points for DE-EN and +0.5 for AR-EN. • Linear interpolation for NNJM models was slightly behind its log-linear variant. Data Selection: • OSM-based selection performed better for AR-EN task giving an average improvement of +0.7 • NNJM performed better at the DE-EN task giving an average improvement of +0.6 points. • Both OSM- and NNJM-based selection gave slightly better results than Modified-MooreLewis (MML) selection (Axelrod et al., 2011). The rest of the paper is organized as follows. Section 2 briefly describes the OSM and the NNJM models. Section 3 describes mixture model and data selection techniques that we apply using the OSM and the NNJM models to carry out adaptation. Section 4 presents the results. Section 5 discusses related work and Section 6 concludes the paper. 2 Joint Sequence Models In this section, we revisit Operation Sequence and Neural Network Joint models briefly. 2.1 Operation Sequence Model The Operation Sequence Model (OSM) is a bilingual model that couples translation and reordering by representing them"
2015.mtsummit-papers.10,2014.iwslt-evaluation.6,1,0.864662,"52K 24K 32K 28K Table 2: Statistics of the German-English and Arabic-English training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in Millions. ep = Europarl, cc = Common Crawl, un = United Nations and an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry an"
2015.mtsummit-papers.10,2011.iwslt-evaluation.18,0,0.186872,"ata, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but n"
2015.mtsummit-papers.10,P13-1141,0,0.232794,"Missing"
2015.mtsummit-papers.10,N13-1114,0,0.591829,"EM-based weighting, (ii) using log-linear model inside the SMT pipeline. Secondly, we use cross-entropy difference (Moore and Lewis, 2010) between in- and out-domain models to perform data selection for domain adaptation. The bilingual property of the OSM and NNJM models gives them an edge over traditional LM-based methods, which do not capture source and target domain relevance jointly. The embedded reordering information modeled in OSM helps it to preserve reordering characteristic of the in-domain data. Capturing reordering variation across domains have been shown to be beneficial also by Chen et al. (2013a). NNJM adds a different dimension to it by semantically generalizing the data using distributed representation of words (Bengio et al., 2003). We evaluated our systems on a standard task of translating IWSLT TED talks for Germanto-English (DE-EN) and Arabic-to-English (AR-EN) language pairs. Below is a summary of our main findings: Model Weighting: • Linearly interpolating OSM models through EM-based weighting gave average BLEU (Papineni et al., 2002) improvements of up to +0.6 for DE-EN and +0.9 for AR-EN. • Log-linear variant performed better in the case of NNJM giving an average improveme"
2015.mtsummit-papers.10,P13-1126,0,0.590988,"EM-based weighting, (ii) using log-linear model inside the SMT pipeline. Secondly, we use cross-entropy difference (Moore and Lewis, 2010) between in- and out-domain models to perform data selection for domain adaptation. The bilingual property of the OSM and NNJM models gives them an edge over traditional LM-based methods, which do not capture source and target domain relevance jointly. The embedded reordering information modeled in OSM helps it to preserve reordering characteristic of the in-domain data. Capturing reordering variation across domains have been shown to be beneficial also by Chen et al. (2013a). NNJM adds a different dimension to it by semantically generalizing the data using distributed representation of words (Bengio et al., 2003). We evaluated our systems on a standard task of translating IWSLT TED talks for Germanto-English (DE-EN) and Arabic-to-English (AR-EN) language pairs. Below is a summary of our main findings: Model Weighting: • Linearly interpolating OSM models through EM-based weighting gave average BLEU (Papineni et al., 2002) improvements of up to +0.6 for DE-EN and +0.9 for AR-EN. • Log-linear variant performed better in the case of NNJM giving an average improveme"
2015.mtsummit-papers.10,N12-1047,0,0.0324171,"l. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also trained on the concatenated data. Linear interpolation (OSMln ) based on EM-weighting shows significant improvements with average BLEU gains of +0.6 in DE-EN and +0.9 in AR-EN over the baseline system Bcat (see Table 3).6 One reason for better gains in AR-EN is the fact that the out-domain UN data 4 Training NNJM with backpropagation could be proh"
2015.mtsummit-papers.10,J81-4005,0,0.515198,"Missing"
2015.mtsummit-papers.10,P14-1129,0,0.128935,". Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as additional language model (LM) features insid"
2015.mtsummit-papers.10,P13-2119,0,0.300031,"based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was earlier done for language modeling using information retrieval techniques (Hildebrand et al., 2005) and using perplexity measure (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source side and target side language models. Duh et al. (2013) used recurrent neural network language model instead of an ngram-based language model to do the same. Translation model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards th"
2015.mtsummit-papers.10,P13-2071,1,0.880717,"g a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as additional language mod"
2015.mtsummit-papers.10,C14-1041,1,0.833548,"epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also trained on the concatenated data. Linear interpolation (OSMln ) based on EM-weighting shows significant improvements with average BLEU gains of +0.6 in DE-EN and +0.9 in AR-EN over the baseline system Bcat (see Table 3).6 One reason for better gains in AR-EN is the fact that the out-d"
2015.mtsummit-papers.10,P11-1105,1,0.931772,"y distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as additional language model (LM) features inside the SMT decoder. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 117 The diversity of the two models, i.e., OSM with embedded reordering information and NNJM with continuous space modeling, makes them interesting to be explored for domain adaptation."
2015.mtsummit-papers.10,N13-1073,0,0.0478691,"arget). Tokens are represented in Millions. ep = Europarl, cc = Common Crawl, un = United Nations and an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT sy"
2015.mtsummit-papers.10,P12-2023,0,0.0681817,"adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data select"
2015.mtsummit-papers.10,W08-0334,0,0.165313,"l cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network traini"
2015.mtsummit-papers.10,D10-1044,0,0.19196,"ion model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixtu"
2015.mtsummit-papers.10,W07-0717,0,0.155727,"enges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates r"
2015.mtsummit-papers.10,W09-0439,0,0.123872,"filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain ada"
2015.mtsummit-papers.10,D08-1089,0,0.0222621,"r training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also trained on the concatenated data. Linear interpolation (OSMln ) ba"
2015.mtsummit-papers.10,P14-1066,0,0.0237562,"his allows the model to learn very rich translation and reordering patterns. Moreover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has a D dimensional ("
2015.mtsummit-papers.10,E14-1035,0,0.0822255,"Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data selection using differences"
2015.mtsummit-papers.10,W11-2123,0,0.0290447,"Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also tr"
2015.mtsummit-papers.10,2005.eamt-1.19,1,0.837721,"be an effective way to discard poor quality or irrelevant training instances, which when included in the MT systems, hurts its performance. The idea is to score the out-domain data using model trained from the in-domain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was earlier done for language modeling using information retrieval techniques (Hildebrand et al., 2005) and using perplexity measure (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source side and target side language models. Duh et al. (2013) used recurrent neural network language model instead of an ngram-based language model to do the same. Translation model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather th"
2015.mtsummit-papers.10,C14-1182,0,0.626827,"Missing"
2015.mtsummit-papers.10,D15-1147,1,0.427467,"e out-domain data that is unknown to the in-domain OSM, gets high probability7 and is ranked higher in the search space. On the contrary, the same gets down-weighted in a linearly interpolated global model. Both linear and log-linear interpolation of the NNJM models showed improvements over the baseline system Bcat (refer to Table 4). Log-linear interpolation (NNJMlg ) performed slightly better in both cases. Notice that NNJMlg does not face the same problem as OSMlg because all NNJM models are trained using the in-domain vocabulary with a low probability assigned to the out-domain UNKs.8 See Joty et al. (2015) for more details on our novel handling 7 Due to probability mass assigned to UNK sequences. order to reduce the training time and to learn better word representations, neural models are trained on most frequent vocabulary words only and low frequency words are represented under a class of unknown words, unk. This results in a large number of n-gram sequences containing at least one unk word and thereby, makes unk a highly probable word for the model. As a result of this discrepancy, sentences with more number of unk words will be selected. To solve this problem we created a separate class for"
2015.mtsummit-papers.10,D13-1176,0,0.0465297,"tion or reordering) decisions. This allows the model to learn very rich translation and reordering patterns. Moreover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has"
2015.mtsummit-papers.10,P07-2045,0,0.00623946,"erman-English and Arabic-English training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in Millions. ep = Europarl, cc = Common Crawl, un = United Nations and an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Mo"
2015.mtsummit-papers.10,N12-1005,0,0.0261074,"t-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as a"
2015.mtsummit-papers.10,P14-2093,0,0.222498,"to reduce computational cost when training is expensive and also when memory is constrained. Data selection was earlier done for language modeling using information retrieval techniques (Hildebrand et al., 2005) and using perplexity measure (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source side and target side language models. Duh et al. (2013) used recurrent neural network language model instead of an ngram-based language model to do the same. Translation model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases ra"
2015.mtsummit-papers.10,N13-1074,0,0.30113,"ain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et a"
2015.mtsummit-papers.10,J06-4004,0,0.0704009,"Missing"
2015.mtsummit-papers.10,C14-1105,0,0.264728,"pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data selection using differences in cross entropy and showed improvements of up to +0.6 BLEU points. The code will be contributed to Mo"
2015.mtsummit-papers.10,D09-1074,0,0.345709,"tolo et al., 2014) or patents (Fujii et al., 2010) or educational content (Abdelali et al., 2014), and often come with the challenges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore"
2015.mtsummit-papers.10,P10-2041,0,0.680232,"tents (Fujii et al., 2010) or educational content (Abdelali et al., 2014), and often come with the challenges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) th"
2015.mtsummit-papers.10,D09-1141,0,0.0657375,"time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NN"
2015.mtsummit-papers.10,P02-1040,0,0.102972,"Missing"
2015.mtsummit-papers.10,C12-2104,0,0.064795,"el to learn very rich translation and reordering patterns. Moreover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has a D dimensional (continuous-valu"
2015.mtsummit-papers.10,I08-2089,0,0.0130472,"nd an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in base"
2015.mtsummit-papers.10,E12-1055,0,0.662401,"ord-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (i"
2015.mtsummit-papers.10,P13-1082,0,0.207199,"se level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data selection using differences in cross entropy and showed improvements of up to +0.6 BLEU points. The code wil"
2015.mtsummit-papers.10,P13-1045,0,0.034229,"reover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has a D dimensional (continuous-valued) Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami"
2015.mtsummit-papers.10,D13-1140,0,0.293,"models are trained by randomly selecting corpora of same size as that of the in-domain data. 4 Experiments Data: We used TED talks (Cettolo et al., 2014) as our in-domain corpus. For German-toEnglish (DE-EN), we used the data made available for WMT’14.2 This contains News, Europarl and Common Crawl as out-domain data. For Arabic-English (AR-EN), we used the UN corpus as out-domain data. We concatenated dev- and test-2010 for tuning and used test2011-2013 for evaluation. Table 2 shows the size of the training and test data used. NNJM Settings: The NNJM models were trained using NPLM3 toolkit (Vaswani et al., 2013) with the following settings. We used a target context of 5 words and an aligned source window of 9 words, forming a joint stream of 14-grams for training. We restricted source and target side vocabularies to 20K and 40K most frequent words. We used an input embedding layer of 150 2 http://www.statmt.org/wmt14/ 3 http://nlg.isi.edu/software/nplm/ Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 122 German-English Arabic-English Corpus Sent. TokDE TokEN Corpus Sent. TokAR TokEN iwslt news ep cc 177K 200K 1.9M 2.3M 3.3M 5.1M 48.7M 53.9M 3.5M 5.0M 51.0M 57"
abdelali-etal-2014-amara,J93-1004,0,\N,Missing
abdelali-etal-2014-amara,tiedemann-2008-synchronizing,0,\N,Missing
abdelali-etal-2014-amara,J93-2003,0,\N,Missing
abdelali-etal-2014-amara,P02-1040,0,\N,Missing
abdelali-etal-2014-amara,P11-1105,0,\N,Missing
abdelali-etal-2014-amara,P13-2003,1,\N,Missing
abdelali-etal-2014-amara,P07-2045,0,\N,Missing
abdelali-etal-2014-amara,N04-1022,0,\N,Missing
abdelali-etal-2014-amara,N03-1017,0,\N,Missing
abdelali-etal-2014-amara,P12-1016,0,\N,Missing
abdelali-etal-2014-amara,2013.iwslt-papers.2,1,\N,Missing
abdelali-etal-2014-amara,tiedemann-2012-parallel,0,\N,Missing
abdelali-etal-2014-amara,W11-2123,0,\N,Missing
abdelali-etal-2014-amara,D11-1125,0,\N,Missing
abdelali-etal-2014-amara,2012.eamt-1.60,0,\N,Missing
abdelali-etal-2014-amara,C12-1121,1,\N,Missing
abdelali-etal-2014-amara,2010.iwslt-evaluation.1,0,\N,Missing
ambati-etal-2010-active,2005.mtsummit-papers.30,1,\N,Missing
ambati-etal-2010-active,D08-1027,0,\N,Missing
ambati-etal-2010-active,J04-3001,0,\N,Missing
ambati-etal-2010-active,W09-4633,1,\N,Missing
ambati-etal-2010-active,P02-1040,0,\N,Missing
ambati-etal-2010-active,N09-1047,0,\N,Missing
ambati-etal-2010-active,P07-2045,0,\N,Missing
ambati-etal-2010-active,W07-0734,0,\N,Missing
ambati-etal-2010-active,D09-1030,0,\N,Missing
ambati-etal-2010-active,N06-1003,0,\N,Missing
ambati-etal-2010-active,N03-1017,0,\N,Missing
ambati-etal-2010-active,W09-1904,0,\N,Missing
ambati-etal-2010-active,P02-1016,0,\N,Missing
ambati-etal-2010-active,takezawa-etal-2002-toward,0,\N,Missing
ambati-etal-2010-active,P03-1021,0,\N,Missing
ambati-etal-2010-active,D09-1006,0,\N,Missing
C04-1059,J93-2003,0,0.00554611,"Missing"
C04-1059,eck-etal-2004-language,1,0.279764,"s query models are proposed and explained in the following sections. The paper is structured as follows: section 2 outlines the sentence retrieval approach, and three bag-of-words query models are designed and explored; structured query models are introduced in section 3. In section 4 we present translation experiments are presented for the different query. Finally, summary is given in section 5. In our sentence retrieval process, the standard tf/idf (term frequency and inverse document frequency) term weighting scheme is used. The queries are built from the translation hypotheses. We follow (Eck, et al., 2004) in considering each sentence in the monolingual corpus as a document, as they have shown that this gives better results compared to retrieving entire news stories. Both the query and the sentences in the text corpus are converted into vectors by assigning a term weight to each word. Then the cosine similarity is calculated proportional to the inner product of the two vectors. All sentences are ranked according to their similarity with the query, and the most similar sentences are used as the data for building the specific language model. In our experiments we use different numbers of similar"
C04-1059,P02-1040,0,0.101775,"Missing"
C04-1059,2003.mtsummit-papers.53,1,0.417652,"Missing"
C04-1114,P00-1004,1,0.893904,"Missing"
C04-1114,C96-2141,1,0.6513,"medical terms but the language is not very complex. Figure 3 shows some example test sentences (from the reference data). (…) The symptoms you are describing and given your recent change in diet, I believe you may be anemic. Patient: Anemic? Really? Is that serious? Doctor: Anemia can be very serious if left untreated. Being anemic means your body lacks a sufficient amount of red blood cells to carry oxygen through your body. (…) Figure 3: Example test sentences (reference) Doctor: The Baseline system uses IBM1 lexicon transducers and different types of phrase transducers (Zhang et al. 2003, Vogel et al. 1996, Vogel et al. 2003). The Language model is a trigram language model with Good-TuringSmoothing built with the SRI-Toolkit (SRI, 19952004) using only the English part of the training data. The Baseline system scores a 0.171 BLEU and 4.72 NIST. [BLEU and NIST are well known scoring methods for measuring machine translation quality. Both calculate the precision of a translation by comparing it to a reference translation and incorporating a length penalty (Doddington, 2001; Papineni et al., 2002).] 3.2 Extracting dictionaries from the UMLS The first way to exploit the UMLS database for a statistic"
C04-1114,P02-1040,0,\N,Missing
C10-1040,P06-1009,0,0.111836,"ell-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is whether we can jointly improve discrimi"
C10-1040,2009.mtsummit-papers.5,1,0.915643,"e directions. It can also make use of first-order features which model the dependency between different links, the Parts-of-Speech tagging features, the word form similarity feature and the phrase features. In this paper we use all the features mentioned above except the POS and phrase features. The aligner is trained using a beliefpropagation (BP) algorithm, and can be optimized to maximize likelihood or directly optimize towards AER on a tuning set. The aligner outputs confidence scores for alignment links, which allows us to control the precision and recall rate of the resulting alignment. Guzman et al. (2009) experimented with different alignments produced by adjusting the filtering threshold for the alignment links and showed that they could get high-precision-low-recall alignments by having a higher threshold. Therefore, we replicated the confidence filtering procedures to produce the partial alignment constraints. Afterwards we iterate by putting the partial alignments back to the constrained word alignment algorithm described in section 3. Although the discriminative aligner performs well in supplying high precision constraints, it does not model the null alignment explicitly. 352 Num. of Sent"
C10-1040,P09-1105,0,0.0170113,"have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is whether we can jointly improve discriminative models and generative models by feeding the information we get from the discriminative aligner back into the generative aligner. Examples of this line of research include Model 6 (Och and Ney, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). These approaches use labeled data to tune additional parameters to weight different components of the IBM models such as the lexical translation model, the distortion model and the"
C10-1040,H05-1012,0,0.0452004,"achine translation (SMT). From a Machine Learning perspective, the models for word alignment can be roughly categorized as generative models and discriminative models. The widely used word alignment tool, i.e. GIZA++ (Och and Ney, 2003), implements the well-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned co"
C10-1040,J93-2003,0,0.0474385,"Missing"
C10-1040,P05-1057,0,0.0193616,"nd Ney, 2003), implements the well-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is wh"
C10-1040,P04-1023,0,0.350911,"Missing"
C10-1040,H05-1011,0,0.0243153,"lements the well-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is whether we can"
C10-1040,P06-1097,0,0.058423,"(2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is whether we can jointly improve discriminative models and generative models by feeding the information we get from the discriminative aligner back into the generative aligner. Examples of this line of research include Model 6 (Och and Ney, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). These approaches use labeled data to tune additional parameters to weight different components of the IBM models such as the lexical translation model, the distortion model and the fertility model. These methods are proven to be effective in improving the quality of alignments. However, the discriminative training in these methods is restricted in using the model components of generative models, in other words, incorporating new features is difficult. Instead of using discriminative training methods to tune the weights of generat"
C10-1040,W08-0303,1,0.927935,"1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is whether we can jointly improve discriminative models and generati"
C10-1040,D07-1006,0,0.113689,"ed by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the question we address is whether we can jointly improve discriminative models and generative models by feeding the information we get from the discriminative aligner back into the generative aligner. Examples of this line of research include Model 6 (Och and Ney, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). These approaches use labeled data to tune additional parameters to weight different components of the IBM models such as the lexical translation model, the distortion model and the fertility model. These methods are proven to be effective in improving the quality of alignments. However, the discriminative training in these methods is restricted in using the model components of generative models, in other words, incorporating new features is difficult. Instead of using discriminative training methods to tune the weights of generative models, in this paper we propose to use a discriminative wo"
C10-1040,W08-0509,1,0.883934,"Missing"
C10-1040,J03-1002,0,0.20774,"version of the EM algorithm. Experiments on small-size Chinese and Arabic tasks show consistent improvements on AER. We also experimented with moderate-size Chinese machine translation tasks and got an average of 0.5 point improvement on BLEU scores across five standard NIST test sets and four other test sets. 1 Introduction Word alignment is a crucial component in statistical machine translation (SMT). From a Machine Learning perspective, the models for word alignment can be roughly categorized as generative models and discriminative models. The widely used word alignment tool, i.e. GIZA++ (Och and Ney, 2003), implements the well-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2"
C10-1040,H05-1010,0,0.0572487,"l, i.e. GIZA++ (Och and Ney, 2003), implements the well-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out manual alignments to tune weights for discriminative models, while using the model parameters, model scores or alignment links from unsupervised word aligners as features. CallisonBurch et. al. (2004) proposed a method to interpolate the parameters estimated by sentence-aligned and word-aligned corpus. Also, there are recent attempts to combine multiple alignment sources using alignment confidence measures so as to improve the alignment quality (Huang, 2009). In this paper, the questio"
C10-1040,C96-2141,1,0.783981,"improvements on AER. We also experimented with moderate-size Chinese machine translation tasks and got an average of 0.5 point improvement on BLEU scores across five standard NIST test sets and four other test sets. 1 Introduction Word alignment is a crucial component in statistical machine translation (SMT). From a Machine Learning perspective, the models for word alignment can be roughly categorized as generative models and discriminative models. The widely used word alignment tool, i.e. GIZA++ (Och and Ney, 2003), implements the well-known IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which are generative models. For language pairs such as ChineseEnglish, the word alignment quality is often unsatisfactory. There has been increasing interest on using manual alignments in word alignment tasks, which has resulted in several discriminative models. Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model, which is considered supervised. Also, a number of semi-supervised word aligners have been proposed (Taskar et al., 2005; Liu et al., 2005; Moore, 2005; Blunsom and Cohn, 2006; Niehues and Vogel, 2008). These methods use held-out man"
C10-1040,W10-1701,1,0.452524,"Missing"
C10-1040,P06-1002,0,\N,Missing
C10-1092,D09-1075,0,0.586832,"ed lattices for a subset of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the translation with respect to these references. He showed that the parameters of the model can be applied to similar languages when translating into English. However, manually creating these lattices is time-consuming and requires a bilingual person with some knowledge of the underlying statistical machine translation system. There have been some attempts to apply unsupervised methods for tokenization in machine translation (Chung and Gildea, 2009; Xu et al., 2008). The alignment model of Chung and Gildea (2009) forces every source word to align with a target word. Xu et al. (2008) modeled the source-to-null alignment as in the source word to target word model. Their models are special cases of our proposed model when the source model2 is a unigram model. Like Xu et al. (2008), we use Gibbs sampling for inference. Chung and Gildea (2009) applied efﬁcient dynamic programming-based variational inference algorithms. We beneﬁt from existing unsupervised monolingual segmentation. The source model uses the nested Pitman-Yor model as describe"
C10-1092,N10-1081,1,0.834946,"bilingual extension of what is described by Goldwater et al. (2006) for monolingual segmentation. Nonparametric models have received attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical PitmanYor process as a smoothing method for language models. Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009; Cohen et al., 2010), morphological segmentation (Goldwater et al., 2006; Snyder and Barzilay, 2008), and part-of-speech tagging (Goldwater and Grifﬁths, 2007; Snyder et al., 2 Note that “source model” here means a model of source text, not a source model in the noisy channel paradigm. 816 2008). 3 Models We start with the generative process for a source sentence and its alignment with a target sentence. Then we describe individual models employed by this generation scheme. 3.1 Generative Story A source sentence is a sequence of word tokens, and each word is either aligned or not aligned. We focus only on the seg"
C10-1092,D08-1033,0,0.391284,"rds to optimize the translation with respect to these references. He showed that the parameters of the model can be applied to similar languages when translating into English. However, manually creating these lattices is time-consuming and requires a bilingual person with some knowledge of the underlying statistical machine translation system. There have been some attempts to apply unsupervised methods for tokenization in machine translation (Chung and Gildea, 2009; Xu et al., 2008). The alignment model of Chung and Gildea (2009) forces every source word to align with a target word. Xu et al. (2008) modeled the source-to-null alignment as in the source word to target word model. Their models are special cases of our proposed model when the source model2 is a unigram model. Like Xu et al. (2008), we use Gibbs sampling for inference. Chung and Gildea (2009) applied efﬁcient dynamic programming-based variational inference algorithms. We beneﬁt from existing unsupervised monolingual segmentation. The source model uses the nested Pitman-Yor model as described by Mochihashi et al. (2009). When sampling each potential word boundary, our inference technique is a bilingual extension of what is de"
C10-1092,N09-1046,0,0.0279273,"problem of segmentation for machine translation has been studied extensively in recent literature. Most of the work used some linguistic knowledge about the source and the target languages (Nießen and Ney, 2004; Goldwater and McClosky, 2005). Sadat and Habash (2006) experimented with a wide range of tokenization schemes for Arabic-English translation. These experiments further show that even for a single language pair, different tokenizations are needed depending on the training corpus size. The experiments are very expensive to conduct and do not generalize to other language pairs. Recently, Dyer (2009) created manually crafted lattices for a subset of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the translation with respect to these references. He showed that the parameters of the model can be applied to similar languages when translating into English. However, manually creating these lattices is time-consuming and requires a bilingual person with some knowledge of the underlying statistical machine translation system. There have been some attempts to apply unsupervised methods for tokenization i"
C10-1092,P07-1094,0,0.053087,"ived attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical PitmanYor process as a smoothing method for language models. Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009; Cohen et al., 2010), morphological segmentation (Goldwater et al., 2006; Snyder and Barzilay, 2008), and part-of-speech tagging (Goldwater and Grifﬁths, 2007; Snyder et al., 2 Note that “source model” here means a model of source text, not a source model in the noisy channel paradigm. 816 2008). 3 Models We start with the generative process for a source sentence and its alignment with a target sentence. Then we describe individual models employed by this generation scheme. 3.1 Generative Story A source sentence is a sequence of word tokens, and each word is either aligned or not aligned. We focus only on the segmentation problem and not reordering source words; therefore, the model will not generate the order of the target word tokens. A sentence"
C10-1092,H05-1085,0,0.0368615,"ce text. Our experiments show that the proposed segmentation method leads to improvements on Arabic-English and Chinese-English translation tasks. In the next section we will discuss related work. Section 3 will describe our model in detail. The inference will be covered in Section 4, and decoding in Section 5. Experiments and results will be presented in Section 6. 2 Related Work The problem of segmentation for machine translation has been studied extensively in recent literature. Most of the work used some linguistic knowledge about the source and the target languages (Nießen and Ney, 2004; Goldwater and McClosky, 2005). Sadat and Habash (2006) experimented with a wide range of tokenization schemes for Arabic-English translation. These experiments further show that even for a single language pair, different tokenizations are needed depending on the training corpus size. The experiments are very expensive to conduct and do not generalize to other language pairs. Recently, Dyer (2009) created manually crafted lattices for a subset of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the translation with respect to these"
C10-1092,P06-1085,0,0.557435,"e source-to-null alignment as in the source word to target word model. Their models are special cases of our proposed model when the source model2 is a unigram model. Like Xu et al. (2008), we use Gibbs sampling for inference. Chung and Gildea (2009) applied efﬁcient dynamic programming-based variational inference algorithms. We beneﬁt from existing unsupervised monolingual segmentation. The source model uses the nested Pitman-Yor model as described by Mochihashi et al. (2009). When sampling each potential word boundary, our inference technique is a bilingual extension of what is described by Goldwater et al. (2006) for monolingual segmentation. Nonparametric models have received attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical PitmanYor process as a smoothing method for language models. Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009; Cohen et al., 2010), morphological segmentation (Goldwater et al., 2"
C10-1092,P05-1071,0,0.00573616,"ne them using the “grow-diag-ﬁnal-and” heuristic. The output of combining GIZA++ alignment for a sentence pair is a sequence of si -tj entries where i is an index of the source sentence and j is an index of the target sentence. As our model allows only one-to-one mappings between the words in the source and target sentences, we remove si -tj from the sequence if either the source word si or target word tj is already in a previous entry of the combined alignment sequence. The resulting alignment is our initial alignment for the inference. We also apply the MADA morphology segmentation toolkit (Habash and Rambow, 2005) to preprocess the Arabic corpus. We use the D3 scheme (each Arabic word is segmented into morphemes in sequence [CONJ+ [PART+ [Al+ BASE +PRON]]]), mark the morpheme boundaries, and then combine the morphemes again to have words in their original full word form. During inference, we only sample over these morpheme boundaries as potential word boundaries. In this way, we limit the search space, allowing only segmentations consistent with MADA-D3. The inference samples 150 iterations through the whole training set and uses the posterior probability distribution from the last iteration for decodi"
C10-1092,P07-2045,0,0.00835706,"nly segmentations consistent with MADA-D3. The inference samples 150 iterations through the whole training set and uses the posterior probability distribution from the last iteration for decoding. The decoding process is then applied to the entire training set as well as to the development and test sets to generate a consistent tokenization across all three data sets. We used the OpenFST toolkit (Allauzen et al., 2007) for ﬁnite-state machine implementation and operations. The output of the decoding is the preprocessed data for translation. We use the open source Moses phrase-based MT system (Koehn et al., 2007) to test the impact of the preprocessing technique on translation quality.5 6.3.1 Arabic-English Translation Results We consider the Arabic-English setting. We use two baselines: original full word form and MADA-D3 tokenization scheme for ArabicEnglish translation. Table 1 compares the translation results of our segmentation methods with these baselines. Our segmentation method shows improvement over the two baselines on both the development and test sets. According to Sadat and Habash (2006), the MADA-D3 scheme per5 The Moses translation alignment is the output of GIZA++, not from our MCMC in"
C10-1092,P09-1012,0,0.364677,"the best alignment, and thereby the best translation system. We propose an unsupervised tokenization method for machine translation by formulating a generative Bayesian model to “explain” the bilingual training data. Generation of a sentence pair is described as follows: ﬁrst a monolingual tokenization model generates the source sentence, then the alignment model generates the target sentence through the alignments with the source sentence. Breaking this generation process into two steps provides ﬂexibility to incorporate existing monolingual morphological segmentation models such as those of Mochihashi et al. (2009) or Creutz and Lagus (2007). Using nonparametric 815 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 815–823, Beijing, August 2010 models and the Bayesian framework makes it possible to incorporate linguistic knowledge as prior distributions and obtain the posterior distribution through inference techniques such as MCMC or variational inference. As new test source sentences do not have translations which can help to infer the best segmentation, we decode the source string according to the posterior distribution from the inference step. In summ"
C10-1092,J04-2003,0,0.0188815,"the preprocessed source text. Our experiments show that the proposed segmentation method leads to improvements on Arabic-English and Chinese-English translation tasks. In the next section we will discuss related work. Section 3 will describe our model in detail. The inference will be covered in Section 4, and decoding in Section 5. Experiments and results will be presented in Section 6. 2 Related Work The problem of segmentation for machine translation has been studied extensively in recent literature. Most of the work used some linguistic knowledge about the source and the target languages (Nießen and Ney, 2004; Goldwater and McClosky, 2005). Sadat and Habash (2006) experimented with a wide range of tokenization schemes for Arabic-English translation. These experiments further show that even for a single language pair, different tokenizations are needed depending on the training corpus size. The experiments are very expensive to conduct and do not generalize to other language pairs. Recently, Dyer (2009) created manually crafted lattices for a subset of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the tra"
C10-1092,J03-1002,0,0.00570158,"lish sentence pairs. The development set and the test set each have 489 Chinese sentences and each sentence has 7 English references. 6.3 Results We will report the translation results where the preprocessing of the source text are our unigram, bigram, and trigram source models and source-tonull model. The MCMC inference algorithm starts with an initial segmentation of the source text into full word forms. For Chinese, we use the original word segmentation as distributed by IWSLT. To get an initial alignment, we generate the IBM4 Viterbi alignments in both directions using the GIZA++ toolkit (Och and Ney, 2003) and combine them using the “grow-diag-ﬁnal-and” heuristic. The output of combining GIZA++ alignment for a sentence pair is a sequence of si -tj entries where i is an index of the source sentence and j is an index of the target sentence. As our model allows only one-to-one mappings between the words in the source and target sentences, we remove si -tj from the sequence if either the source word si or target word tj is already in a previous entry of the combined alignment sequence. The resulting alignment is our initial alignment for the inference. We also apply the MADA morphology segmentation"
C10-1092,P06-1001,0,0.116056,"hat the proposed segmentation method leads to improvements on Arabic-English and Chinese-English translation tasks. In the next section we will discuss related work. Section 3 will describe our model in detail. The inference will be covered in Section 4, and decoding in Section 5. Experiments and results will be presented in Section 6. 2 Related Work The problem of segmentation for machine translation has been studied extensively in recent literature. Most of the work used some linguistic knowledge about the source and the target languages (Nießen and Ney, 2004; Goldwater and McClosky, 2005). Sadat and Habash (2006) experimented with a wide range of tokenization schemes for Arabic-English translation. These experiments further show that even for a single language pair, different tokenizations are needed depending on the training corpus size. The experiments are very expensive to conduct and do not generalize to other language pairs. Recently, Dyer (2009) created manually crafted lattices for a subset of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the translation with respect to these references. He showed tha"
C10-1092,P08-1084,0,0.251677,"monolingual segmentation. Nonparametric models have received attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical PitmanYor process as a smoothing method for language models. Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009; Cohen et al., 2010), morphological segmentation (Goldwater et al., 2006; Snyder and Barzilay, 2008), and part-of-speech tagging (Goldwater and Grifﬁths, 2007; Snyder et al., 2 Note that “source model” here means a model of source text, not a source model in the noisy channel paradigm. 816 2008). 3 Models We start with the generative process for a source sentence and its alignment with a target sentence. Then we describe individual models employed by this generation scheme. 3.1 Generative Story A source sentence is a sequence of word tokens, and each word is either aligned or not aligned. We focus only on the segmentation problem and not reordering source words; therefore, the model will not"
C10-1092,D08-1109,0,0.0441549,"Missing"
C10-1092,P09-1009,0,0.0175641,"erence technique is a bilingual extension of what is described by Goldwater et al. (2006) for monolingual segmentation. Nonparametric models have received attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical PitmanYor process as a smoothing method for language models. Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009; Cohen et al., 2010), morphological segmentation (Goldwater et al., 2006; Snyder and Barzilay, 2008), and part-of-speech tagging (Goldwater and Grifﬁths, 2007; Snyder et al., 2 Note that “source model” here means a model of source text, not a source model in the noisy channel paradigm. 816 2008). 3 Models We start with the generative process for a source sentence and its alignment with a target sentence. Then we describe individual models employed by this generation scheme. 3.1 Generative Story A source sentence is a sequence of word tokens, and each word is either aligned or not aligned. We"
C10-1092,P06-1124,0,0.0575072,"onal inference algorithms. We beneﬁt from existing unsupervised monolingual segmentation. The source model uses the nested Pitman-Yor model as described by Mochihashi et al. (2009). When sampling each potential word boundary, our inference technique is a bilingual extension of what is described by Goldwater et al. (2006) for monolingual segmentation. Nonparametric models have received attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical PitmanYor process as a smoothing method for language models. Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009; Cohen et al., 2010), morphological segmentation (Goldwater et al., 2006; Snyder and Barzilay, 2008), and part-of-speech tagging (Goldwater and Grifﬁths, 2007; Snyder et al., 2 Note that “source model” here means a model of source text, not a source model in the noisy channel paradigm. 816 2008). 3 Models We start with the generative process for a source"
C10-1092,C96-2141,1,0.468209,"y existing probabilistic monolingual segmentation to generate the source sentence. For example, the source model can be the nested Pitman-Yor process as described by Mochihashi et al. (2009), the minimum description length model presented by Creutz and Lagus (2007), or something else. Also the source model can incorporate linguistic knowledge from a rule-based or statistical morphological disambiguator. The model generates the alignment after the source sentence with word boundaries already generated. Therefore, the alignment model can be any existing word alignment model (Brown et al., 1993; Vogel et al., 1996). Even though the choices of source model or alignment model can lead to different inference methods, the model we propose here is highly extensible. Note that we assume that the alignment consists of at most one-to-one mappings between source and target words, with null alignments possible on both sides. Another advantage of a separate source model lies in the segmentation of an unseen test set. In section 5 we will show how to apply the source model distribution learned from training data to ﬁnd the best segmentation of an unseen test set. Notation and Parameters We will use bold font for a"
C10-1092,C08-1128,0,0.26841,"of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the translation with respect to these references. He showed that the parameters of the model can be applied to similar languages when translating into English. However, manually creating these lattices is time-consuming and requires a bilingual person with some knowledge of the underlying statistical machine translation system. There have been some attempts to apply unsupervised methods for tokenization in machine translation (Chung and Gildea, 2009; Xu et al., 2008). The alignment model of Chung and Gildea (2009) forces every source word to align with a target word. Xu et al. (2008) modeled the source-to-null alignment as in the source word to target word model. Their models are special cases of our proposed model when the source model2 is a unigram model. Like Xu et al. (2008), we use Gibbs sampling for inference. Chung and Gildea (2009) applied efﬁcient dynamic programming-based variational inference algorithms. We beneﬁt from existing unsupervised monolingual segmentation. The source model uses the nested Pitman-Yor model as described by Mochihashi et"
C10-1092,knight-al-onaizan-1998-translation,0,0.0211093,"tences. Only the source model is used in preprocessing. ∗ The best segmentation  s of a string of characters c = c1 , . . . , c|c |according to the n-gram source model is: i=|s| ∗ s = argmax p (|s|) s from c i=1 p (si |si−n , . . . , si−1 ) We use a stochastic ﬁnite-state machine for decoding. This is possible by composition of the following two ﬁnite state machines: 820 • Acceptor Ac . The string of characters c is represented as an ﬁnite state acceptor machine where any path through the machine represents an unweighted segmentation of c. • Source model weighted ﬁnite state transducer Lc . Knight and Al-Onaizan (1998) show how to build an n-gram language model by a weighted ﬁnite state machine. The states of the transducer are (n − 1)gram history, the edges are words from the language. The arc si coming from state (si−n , . . . , si−1 ) to state (si−n+1 , . . . , si ) has weight p (si |si−n , . . . , si−1 ). The best segmentation s∗ is given as s∗ = BestPath(Ac ◦ Lc ). 6 Experiments This section presents experimental results on Arabic-English and Chinese-English translation tasks using the proposed segmentation technique. 6.1 Arabic-English As a training set we use the BTEC corpus distributed by the Intern"
C10-1092,J93-2003,0,\N,Missing
C10-1092,2005.iwslt-1.1,0,\N,Missing
C12-1063,P06-1002,0,0.0163588,"of the translation features (inverse and direct, phrasal and lexical translation probabilities) in the baseline phrase-based models, we used a variation of 1032 the conditional entropy, assuming a uniform distribution over x (i.e. p(x) = 1/|X |), For instance, the entropy for the inverse phrasal probability p( f |e) is: XX p( f |e) log p( f |e) (4) H p (Fi |Ei ) = 1/|Ei | e∈Ei f ∈Fi Translation model size For each phrase-table, we measure the number of entries (log), as well as the number of source and target singletons. Alignment density variables We use the per-phrase pair number of links (Ayan and Dorr, 2006), source and target gaps (Guzman et al., 2009), averaged over the phrase-table. Alignment distortion variables We use the per-phrase pair number of link-crossings (Lambert et al., 2009), relative link distortion, and a new distortion feature we call diagonality, which is the absolute value of Pearson’s correlation (from 0 to 1) of the positions in the source and target words of an alignment. 3.1.2 Translation hypothesis features These types of features include the translation cost for each of the features used in the Moses phrase-based decoder (Koehn et al., 2007). These include: Translation f"
C12-1063,D08-1078,0,0.0173708,"cal framework that researchers can apply to their own systems. 1030 2 Related work The work presented in this paper is related to previous analysis done in the past few years. For instance, the correlation between characteristics of the translation model and the automatic quality metrics has previously been addressed. Lopez and Resnik (2006) make a study of different phrase-based translation model (TM) features and their impact translation quality. They also analyze variations in the translation search space of the decoder by having alignments of gradually degraded quality. On the other hand, Birch et al. (2008) study different languagepair characteristics and use them as predictors of BLEU translation quality using linear regression. Furthermore, Pado et al. (2009) use linear models to build a higher-level translation quality metric that uses features from other established metrics (e.g. BLEU, METEOR, TER) as well as Textual Entailment features (Dagan et al., 2006) and that achieves higher correlation with human judgements. However, multivariate regression, has not been used as a tool to predict translation quality based on the characteristics of the translation model. Others have focused on identif"
C12-1063,W11-2107,0,0.0156232,"regression model and ǫ is the model error. On a multivariate regression model, Equation 1 represents an hyperplane that minimizes the error ǫ. In this paper, we analyze the output of several translation systems that use the same decoder, but differ in the alignment models that they use to build their respective translation model. We use different characteristics of their corresponding translation hypotheses and translation models as input features X to predict their translation performance y in terms of three popular automatic translation quality metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and TER (Snover et al., 2006). We use a regularized regression model to estimate the parameters of our prediction model. We use Spearman’s rank correlation, Pearson’s correlation and RMSE to evaluate the fitness of the regression models estimated for two different domains (News, Proceedings) and a mixed-domain, general model. Our results indicate that using a regularized linear regression, we can achieve high levels of correlation between our predicted values and the actual values of the quality metrics. We take a closer look at the most important features according to the regression coeffici"
C12-1063,N12-1059,0,0.0306433,"have many sources of information available (typically: model scores, automatic metric scores, post-editing effort scores, etc.) and the goal is to provide a model that reliably is able to distinguish good from bad translations. The proposed work differs from Quality Estimation in two aspects: First, here we are interested in contrasting the output from several translation models, to be able to learn their shared features that help predict better quality scores. Second, we are not interested in performing a local estimation (at a sentence level) but at a document level. Finally, recent work by Devlin and Matsoukas (2012), focuses in using variation in traits, or hypothesis characteristics to generate alternative hypotheses that are later used for system combination. In their work, they use null words, reordering, ngram-frequency, hypothesis length, among other features. In our view, the current study is complementary to that work, given that our framework allows to detect important features of &quot;traits&quot; which could serve as input a trait-based hypothesis selection system. Summarizing, in this paper we propose a framework for the analysis of Machine Translation performance in terms of characteristics of the tra"
C12-1063,2009.mtsummit-papers.5,1,0.886101,"ct, phrasal and lexical translation probabilities) in the baseline phrase-based models, we used a variation of 1032 the conditional entropy, assuming a uniform distribution over x (i.e. p(x) = 1/|X |), For instance, the entropy for the inverse phrasal probability p( f |e) is: XX p( f |e) log p( f |e) (4) H p (Fi |Ei ) = 1/|Ei | e∈Ei f ∈Fi Translation model size For each phrase-table, we measure the number of entries (log), as well as the number of source and target singletons. Alignment density variables We use the per-phrase pair number of links (Ayan and Dorr, 2006), source and target gaps (Guzman et al., 2009), averaged over the phrase-table. Alignment distortion variables We use the per-phrase pair number of link-crossings (Lambert et al., 2009), relative link distortion, and a new distortion feature we call diagonality, which is the absolute value of Pearson’s correlation (from 0 to 1) of the positions in the source and target words of an alignment. 3.1.2 Translation hypothesis features These types of features include the translation cost for each of the features used in the Moses phrase-based decoder (Koehn et al., 2007). These include: Translation feature costs The per-phrase cost for each of t"
C12-1063,W04-3250,0,0.0785917,"vailable for the Spanish-English translation task for the WMT competitions2 . The description of the different datasets is presented in Table 2. 4.2.1 Sub-document sampling To better appreciate the effect of a translation model into translation quality, we split each dataset into used several sub-documents long enough to provide accurate translation statistics (e.g. ngram counts for BLEU), but short enough to allow us to appreciate the differences between different translation models. Sub-document splitting is a known technique that has been used previously for confidence interval estimation (Koehn, 2004). In our study, we chose a slightly more conservative sub-document size of 100 translation sentences to get smoother results. For our experiments we used only 4 subdocuments (one hundred sentences each) from each of the 9 datasets presented in Table 2. We restricted to 4 samples to ensure that each dataset was equally represented (some datasets are shorter than others). We obtained translations for each of the 7 different translation systems. This resulted in a total set of (9x7x4) 252 different training instances for our regression models (for the cross domain set). 2 Data can be obtained dir"
C12-1063,P07-2045,0,0.00537847,"phrase pair number of links (Ayan and Dorr, 2006), source and target gaps (Guzman et al., 2009), averaged over the phrase-table. Alignment distortion variables We use the per-phrase pair number of link-crossings (Lambert et al., 2009), relative link distortion, and a new distortion feature we call diagonality, which is the absolute value of Pearson’s correlation (from 0 to 1) of the positions in the source and target words of an alignment. 3.1.2 Translation hypothesis features These types of features include the translation cost for each of the features used in the Moses phrase-based decoder (Koehn et al., 2007). These include: Translation feature costs The per-phrase cost for each of the translation probability features in the translation model averaged over the translation set t i . We used the baseline translation features in the phrase-based model (Koehn et al., 2003): inverse and direct phrasal translation probabilities and inverse and direct lexical probabilities. Lexicalized reordering costs The per phrase cost for the distance-based reordering feature and each of the three different orientations (mono, swap, discontinuous) in a bidirectional setting averaged over the translation set t i . Lan"
C12-1063,N03-1017,0,0.0210788,"a new distortion feature we call diagonality, which is the absolute value of Pearson’s correlation (from 0 to 1) of the positions in the source and target words of an alignment. 3.1.2 Translation hypothesis features These types of features include the translation cost for each of the features used in the Moses phrase-based decoder (Koehn et al., 2007). These include: Translation feature costs The per-phrase cost for each of the translation probability features in the translation model averaged over the translation set t i . We used the baseline translation features in the phrase-based model (Koehn et al., 2003): inverse and direct phrasal translation probabilities and inverse and direct lexical probabilities. Lexicalized reordering costs The per phrase cost for the distance-based reordering feature and each of the three different orientations (mono, swap, discontinuous) in a bidirectional setting averaged over the translation set t i . Language model cost The per-word language model cost for each translation hypothesis averaged over the translation set t i . Additionally, we include word-alignment based features: Word aligment variables Similarly to the translation model features, we used aligment d"
C12-1063,2009.mtsummit-posters.12,0,0.057014,"features (Dagan et al., 2006) and that achieves higher correlation with human judgements. However, multivariate regression, has not been used as a tool to predict translation quality based on the characteristics of the translation model. Others have focused on identifying characteristics of the word alignments upon which these models have been built. Fraser and Marcu (2007) study how alignment quality (AER) is related to its translation quality relative BLEU. As a result, they proposed a modified version of AER to increase the correlation between alignment quality and translation performance. Lambert et al. (2009, 2010) analyze how alignment characteristics correlate with translation quality. They analyze the effect of the number of links of different types of alignments including its repercussions on the size of phrase tables and the ambiguity of the translation model. They also propose new structural metrics for alignments such as link length, distortion and crossings. In this study, we also include alignment features to characterize our translation models. A closely related topic to this study is the task of Quality Estimation for Machine translation (Specia et al., 2009; Specia, 2011), where sente"
C12-1063,2010.eamt-1.7,0,0.0261575,"Missing"
C12-1063,2006.amta-papers.11,0,0.0191146,"tortion. Note, however that the results are dependent on the specific datasets analyzed as well as the features included in the model. Our goal is not to provide a one-hat-fits-all set of recommendations that would address every possible scenario, but rather to provide an analytical framework that researchers can apply to their own systems. 1030 2 Related work The work presented in this paper is related to previous analysis done in the past few years. For instance, the correlation between characteristics of the translation model and the automatic quality metrics has previously been addressed. Lopez and Resnik (2006) make a study of different phrase-based translation model (TM) features and their impact translation quality. They also analyze variations in the translation search space of the decoder by having alignments of gradually degraded quality. On the other hand, Birch et al. (2008) study different languagepair characteristics and use them as predictors of BLEU translation quality using linear regression. Furthermore, Pado et al. (2009) use linear models to build a higher-level translation quality metric that uses features from other established metrics (e.g. BLEU, METEOR, TER) as well as Textual Ent"
C12-1063,W10-1719,0,0.0164157,"slation models, we used different types of alignments. The aligners used for these systems were a discriminative aligner (DWA) (Niehues and Vogel, 2008) with different density thresholds (0.4, 0.5, 0.6, 0.7) to have a variety of dense and sparse alignments. The DWA aligner was trained using hand aligned data from the EPPS (Lambert et al., 2006) dataset. Additionally, we used the symmetrized GIZA++ alignments using the heuristics grow-diag, grow-diag-final and grow-diag-final-and. While these variations in alignments might seem minor, in reality, as previously observed by (Guzman et al., 2009; Niehues et al., 2010) they can have a large impact on the characteristics of the translation model. In total we experimented with 7 different translation models. Each of the systems was tuned using MERT on the WMT news2008 set. 4.2 Feature generation For our regression training, we translated and analyzed the quality of different documents. We used a variety of different test-sets publicly available for the Spanish-English translation task for the WMT competitions2 . The description of the different datasets is presented in Table 2. 4.2.1 Sub-document sampling To better appreciate the effect of a translation model"
C12-1063,W08-0303,1,0.822664,"K 1.4M 1.6M 1.4M 90.0K 4.9M 6.4M 35.1M 2.3M 129.8M 167.2M 140.0K 59.2K 330.1K 387.9K Table 1: Statistics for Raw and preprocessed data for Europarl (EU), News Commentary (NC) and UN training data. We present the total number of training examples (lines), number of tokens (tok) and the vocabulary size (voc). 4.1 Translation model training The data was lowercased and tokenized with the standard preprocessing toolkit available in Moses. To introduce variation in our translation models, we used different types of alignments. The aligners used for these systems were a discriminative aligner (DWA) (Niehues and Vogel, 2008) with different density thresholds (0.4, 0.5, 0.6, 0.7) to have a variety of dense and sparse alignments. The DWA aligner was trained using hand aligned data from the EPPS (Lambert et al., 2006) dataset. Additionally, we used the symmetrized GIZA++ alignments using the heuristics grow-diag, grow-diag-final and grow-diag-final-and. While these variations in alignments might seem minor, in reality, as previously observed by (Guzman et al., 2009; Niehues et al., 2010) they can have a large impact on the characteristics of the translation model. In total we experimented with 7 different translatio"
C12-1063,P09-1034,0,0.0240206,"e past few years. For instance, the correlation between characteristics of the translation model and the automatic quality metrics has previously been addressed. Lopez and Resnik (2006) make a study of different phrase-based translation model (TM) features and their impact translation quality. They also analyze variations in the translation search space of the decoder by having alignments of gradually degraded quality. On the other hand, Birch et al. (2008) study different languagepair characteristics and use them as predictors of BLEU translation quality using linear regression. Furthermore, Pado et al. (2009) use linear models to build a higher-level translation quality metric that uses features from other established metrics (e.g. BLEU, METEOR, TER) as well as Textual Entailment features (Dagan et al., 2006) and that achieves higher correlation with human judgements. However, multivariate regression, has not been used as a tool to predict translation quality based on the characteristics of the translation model. Others have focused on identifying characteristics of the word alignments upon which these models have been built. Fraser and Marcu (2007) study how alignment quality (AER) is related to"
C12-1063,P02-1040,0,0.084287,"ts the parameter vector for the regression model and ǫ is the model error. On a multivariate regression model, Equation 1 represents an hyperplane that minimizes the error ǫ. In this paper, we analyze the output of several translation systems that use the same decoder, but differ in the alignment models that they use to build their respective translation model. We use different characteristics of their corresponding translation hypotheses and translation models as input features X to predict their translation performance y in terms of three popular automatic translation quality metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and TER (Snover et al., 2006). We use a regularized regression model to estimate the parameters of our prediction model. We use Spearman’s rank correlation, Pearson’s correlation and RMSE to evaluate the fitness of the regression models estimated for two different domains (News, Proceedings) and a mixed-domain, general model. Our results indicate that using a regularized linear regression, we can achieve high levels of correlation between our predicted values and the actual values of the quality metrics. We take a closer look at the most important features"
C12-1063,2006.amta-papers.25,0,0.0290561,"error. On a multivariate regression model, Equation 1 represents an hyperplane that minimizes the error ǫ. In this paper, we analyze the output of several translation systems that use the same decoder, but differ in the alignment models that they use to build their respective translation model. We use different characteristics of their corresponding translation hypotheses and translation models as input features X to predict their translation performance y in terms of three popular automatic translation quality metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2011) and TER (Snover et al., 2006). We use a regularized regression model to estimate the parameters of our prediction model. We use Spearman’s rank correlation, Pearson’s correlation and RMSE to evaluate the fitness of the regression models estimated for two different domains (News, Proceedings) and a mixed-domain, general model. Our results indicate that using a regularized linear regression, we can achieve high levels of correlation between our predicted values and the actual values of the quality metrics. We take a closer look at the most important features according to the regression coefficients and discuss the results."
C12-1063,2011.eamt-1.12,0,0.0128715,"ance. Lambert et al. (2009, 2010) analyze how alignment characteristics correlate with translation quality. They analyze the effect of the number of links of different types of alignments including its repercussions on the size of phrase tables and the ambiguity of the translation model. They also propose new structural metrics for alignments such as link length, distortion and crossings. In this study, we also include alignment features to characterize our translation models. A closely related topic to this study is the task of Quality Estimation for Machine translation (Specia et al., 2009; Specia, 2011), where sentence-level prediction models are used to estimate quality of Machine Translation output. In that task, researchers have many sources of information available (typically: model scores, automatic metric scores, post-editing effort scores, etc.) and the goal is to provide a model that reliably is able to distinguish good from bad translations. The proposed work differs from Quality Estimation in two aspects: First, here we are interested in contrasting the output from several translation models, to be able to learn their shared features that help predict better quality scores. Second,"
C12-1063,2009.eamt-1.5,0,0.0561667,"d translation performance. Lambert et al. (2009, 2010) analyze how alignment characteristics correlate with translation quality. They analyze the effect of the number of links of different types of alignments including its repercussions on the size of phrase tables and the ambiguity of the translation model. They also propose new structural metrics for alignments such as link length, distortion and crossings. In this study, we also include alignment features to characterize our translation models. A closely related topic to this study is the task of Quality Estimation for Machine translation (Specia et al., 2009; Specia, 2011), where sentence-level prediction models are used to estimate quality of Machine Translation output. In that task, researchers have many sources of information available (typically: model scores, automatic metric scores, post-editing effort scores, etc.) and the goal is to provide a model that reliably is able to distinguish good from bad translations. The proposed work differs from Quality Estimation in two aspects: First, here we are interested in contrasting the output from several translation models, to be able to learn their shared features that help predict better quality"
C12-1063,J07-3002,0,\N,Missing
C12-1121,N12-1062,0,0.176306,"Lin and Och, 2004), and suffers from a length bias: the parameters it finds yield translations that are too short compared to the references (see Table 4). Exploring the reasons for this bias and proposing ways to solve it is the main focus of this paper. There are many other tuning strategies, which fall outside of the scope of the current study, but to many of which some of our general finding and conclusions should apply. This includes improved versions of some of the above-mentioned algorithms, e.g., a batch version of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al., 2012), but also many original algorithms that use a variety of machine learning methods and loss functions. We refer the interested reader to some excellent recent overviews: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). To the best of our knowledge, no prior work has tried to study the reasons for the length bias of optimizers like PRO. However, researchers have previously expressed concerns about sentence-level BLEU+1, and some have proposed improvements, e.g., He and Deng (2012) used different smoothing for higher-order n-grams, unclipped brevity penalty, and sc"
C12-1121,J93-2003,0,0.0439624,"ling the effective reference length, grounding the precision component, and unclipping the brevity penalty, which yield sizable improvements in test BLEU on two Arabic-English datasets: IWSLT (+0.65) and NIST (+0.37). KEYWORDS: Statistical machine translation, parameter optimization, MERT, PRO, MIRA. Proceedings of COLING 2012: Technical Papers, pages 1979–1994, COLING 2012, Mumbai, December 2012. 1979 1 Introduction Early work on statistical machine translation (SMT) has relied on generative training using maximum likelihood parameter estimation. This was inspired by the noisy channel model (Brown et al., 1993), which asked for calculating the product of two components, a language model and a translation model, giving them equal weights. As mainstream research has moved towards combining multiple scores, the field has switched to discriminative tuning in a log-linear fashion. The standard approach has been to maximize BLEU (Papineni et al., 2002) on a tuning dataset using a coordinate descent optimization algorithm known as minimum error rate training (MERT), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually inc"
C12-1121,N12-1047,0,0.065585,"dd-one smoothed sentence-level version of BLEU, known as BLEU+1 (Lin and Och, 2004), and suffers from a length bias: the parameters it finds yield translations that are too short compared to the references (see Table 4). Exploring the reasons for this bias and proposing ways to solve it is the main focus of this paper. There are many other tuning strategies, which fall outside of the scope of the current study, but to many of which some of our general finding and conclusions should apply. This includes improved versions of some of the above-mentioned algorithms, e.g., a batch version of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al., 2012), but also many original algorithms that use a variety of machine learning methods and loss functions. We refer the interested reader to some excellent recent overviews: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). To the best of our knowledge, no prior work has tried to study the reasons for the length bias of optimizers like PRO. However, researchers have previously expressed concerns about sentence-level BLEU+1, and some have proposed improvements, e.g., He and Deng (2012) used different smoo"
C12-1121,N09-1025,0,0.106178,"Missing"
C12-1121,D08-1024,0,0.692837,"imization algorithm known as minimum error rate training (MERT), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually increased, in some cases to hundreds and even to hundreds of thousands of scores, which has called for new tuning algorithms since MERT was unable to scale beyond just a handful of parameters. Many alternatives to MERT have been proposed over the years, but it is only recently that some of them have gained popularity in the community, most notably, the margin infused relaxed algorithm (MIRA) (Chiang et al., 2008) and pairwise ranking optimization (PRO) (Hopkins and May, 2011). While the number of parameters that an optimizer can handle has become a major concern recently, there are many other important aspects that researchers have paid attention to, e.g., the performance of parameters when translating unseen test data, the speed of convergence, the stability across multiple reruns, the objective function being optimized (e.g., BLEU vs. an approximation of BLEU), the mode of learning (e.g., online vs. batch). Here we study a different, and so far neglected, aspect: the characteristics of the translati"
C12-1121,federico-etal-2012-iwslt,0,0.0123357,"length). On testing, we further used cube pruning and minimum Bayes Risk decoding (the latter yielded slightly longer translations). 7 Still, for comparison purposes, we also report BLEU calculated with respect of the original references using NIST v13a, after detokenization and recasing of the system’s output (shown in small script in the tables). 1986 4.2 Datasets We experimented with the Arabic-English datasets from two machine translation evaluation campaigns: (1) the NIST 2012 Open Machine Translation Evaluation8 , and (2) the IWSLT 2011 Evaluation Campaign on Automatic Talk Translation (Federico et al., 2012). 1. NIST: We trained the phrase and the reordering tables on all training datasets from NIST 2012 (except for UN), we tuned on MT06 and tested on MT09. For language modeling, we built a separate LM from the English side of each training dataset, and from each year of the English GigaWord; we then interpolated them into a single LM. 2. IWSLT: We trained the phrase and the reordering tables on the TED training dataset, we tuned on dev2010, and we tested on tst2010. Since there was a small mismatch in the source/reference length ratios between dev2010 and tst2010, we also experimented with rever"
C12-1121,W09-0439,0,0.0442971,"or future work. 1 That is why the Moses toolkit has an option to run a few iterations of MERT after PRO – to get the length right. 1980 2 Related Work The dominant approach for parameters optimization in SMT is to use MERT (Och, 2003), a batch tuning algorithm that iterates between two modes: (i) generating a k-best list of translation hypotheses using the current parameters values, and (ii) parameter optimization using the k-best lists from all previous iterations. MERT optimizes expected BLEU. It works well for a small number of parameters, but suffers from scalability and stability issues (Foster and Kuhn, 2009). Most importantly for our discussion, it tends not to have length biases; this is also confirmed by our own experiments (see Table 4). Various alternatives to MERT have been proposed, motivated primarily by scalability considerations. One popular alternative is MIRA (Watanabe et al., 2007; Chiang et al., 2008, 2009), which is a perceptron-like online tuning algorithm with passive-aggressive updates. It uses an approximation to BLEU, where a sentence is scored in the context of a pseudo-document formed from the n-gram statistics for the last few updates. MIRA can scale to thousands of paramete"
C12-1121,N12-1023,0,0.132132,"paper. There are many other tuning strategies, which fall outside of the scope of the current study, but to many of which some of our general finding and conclusions should apply. This includes improved versions of some of the above-mentioned algorithms, e.g., a batch version of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al., 2012), but also many original algorithms that use a variety of machine learning methods and loss functions. We refer the interested reader to some excellent recent overviews: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). To the best of our knowledge, no prior work has tried to study the reasons for the length bias of optimizers like PRO. However, researchers have previously expressed concerns about sentence-level BLEU+1, and some have proposed improvements, e.g., He and Deng (2012) used different smoothing for higher-order n-grams, unclipped brevity penalty, and scaled reference length. However, this was not done for the purpose of studying the length bias of PRO; moreover, as we will see below, the use of BLEU+1 is not the only reason for this bias. 3 The Length Bias with PRO We explore the following hypoth"
C12-1121,P12-1016,0,0.0176715,"y, in order to avoid stability issues, we report results averaged over three runs. 4.1 Experimental Setup Preprocessing: We tokenized the English side of all bi-texts and the monolingual data for language modeling using the standard tokenizer of Moses. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus; for lines containing ALL CAPS, we did this for each word. We segmented the words on the Arabic side using the ATB segmentation scheme: we used MADA (Roth et al., 2008) for NIST, and the Stanford word segmenter (Green and DeNero, 2012) for IWSLT. Training. We built separate directed word alignments using IBM model 4 (Brown et al., 1993), we symmetrized them with the grow-diag-final-and heuristic of Moses, and we extracted phrase pairs of length up to seven. We scored these pairs using maximum likelihood with Kneser-Ney smoothing, to build a phrase table with five standard scores: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. For language modeling, we trai"
C12-1121,P12-1031,0,0.144739,"sion of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al., 2012), but also many original algorithms that use a variety of machine learning methods and loss functions. We refer the interested reader to some excellent recent overviews: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). To the best of our knowledge, no prior work has tried to study the reasons for the length bias of optimizers like PRO. However, researchers have previously expressed concerns about sentence-level BLEU+1, and some have proposed improvements, e.g., He and Deng (2012) used different smoothing for higher-order n-grams, unclipped brevity penalty, and scaled reference length. However, this was not done for the purpose of studying the length bias of PRO; moreover, as we will see below, the use of BLEU+1 is not the only reason for this bias. 3 The Length Bias with PRO We explore the following hypotheses about the length bias with PRO: • PRO’s optimization: The bias could be due to the optimization mechanism of PRO. • BLEU+1: PRO uses BLEU+1, where the add-one smoothing is applied to the precision component but does not touch the brevity penalty, which introduce"
C12-1121,D11-1125,0,0.340142,"), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually increased, in some cases to hundreds and even to hundreds of thousands of scores, which has called for new tuning algorithms since MERT was unable to scale beyond just a handful of parameters. Many alternatives to MERT have been proposed over the years, but it is only recently that some of them have gained popularity in the community, most notably, the margin infused relaxed algorithm (MIRA) (Chiang et al., 2008) and pairwise ranking optimization (PRO) (Hopkins and May, 2011). While the number of parameters that an optimizer can handle has become a major concern recently, there are many other important aspects that researchers have paid attention to, e.g., the performance of parameters when translating unseen test data, the speed of convergence, the stability across multiple reruns, the objective function being optimized (e.g., BLEU vs. an approximation of BLEU), the mode of learning (e.g., online vs. batch). Here we study a different, and so far neglected, aspect: the characteristics of the translations generated using weights found by different optimizers. More"
C12-1121,2005.iwslt-1.8,0,0.0245682,"2008) for NIST, and the Stanford word segmenter (Green and DeNero, 2012) for IWSLT. Training. We built separate directed word alignments using IBM model 4 (Brown et al., 1993), we symmetrized them with the grow-diag-final-and heuristic of Moses, and we extracted phrase pairs of length up to seven. We scored these pairs using maximum likelihood with Kneser-Ney smoothing, to build a phrase table with five standard scores: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed model on each corpus (target side of a training bi-text or monolingual dataset); we then interpolated these models minimizing the perplexity on the target side of the tuning dataset. Finally, we built a log-linear model including the language model probability, the word penalty, and the parameters from the phrase and the reordering tables. Tuning. We tuned the weights in the log-linear model by optimizing BLEU (Papineni et al., 2002) on the tuning dataset, using MERT, PRO, or MIRA. We allowed optimi"
C12-1121,P07-2045,0,0.00793493,"s found to work well in general, but many other values yielded a similar result since the BLEU score gets dominated by examples from the current iteration very quickly, making this value irrelevant. 6 We only allow up to 25 iterations, which means there could be up to 24 accumulated one-best hypotheses per sentence, while we accept up to Ξ = 50 pairs per sentence. 1985 4 Experiments and Evaluation We compare variations of three parameter optimization algorithms: MERT, PRO, and MIRA. In all experiments, we use the phrase-based SMT model (Koehn et al., 2003) as implemented in the Moses toolkit (Koehn et al., 2007), and we report evaluation results over two datasets: NIST, which has four reference translations, and IWSLT, with a single reference translation. In order to be able to directly compare the candidate/reference length ratios on the development and on the testing datasets, we need to make sure that we use the same tokenization when calculating BLEU on tuning and on testing. Such differences can arise because many standard scoring tools, e.g., those of NIST, work on detokenized text, which they retokenize again internally; this retokenization typically differs from the one used by the SMT system"
C12-1121,N03-1017,0,0.00889571,"n BLEU points on the NIST datasets. 5 The value of 0.9 was found to work well in general, but many other values yielded a similar result since the BLEU score gets dominated by examples from the current iteration very quickly, making this value irrelevant. 6 We only allow up to 25 iterations, which means there could be up to 24 accumulated one-best hypotheses per sentence, while we accept up to Ξ = 50 pairs per sentence. 1985 4 Experiments and Evaluation We compare variations of three parameter optimization algorithms: MERT, PRO, and MIRA. In all experiments, we use the phrase-based SMT model (Koehn et al., 2003) as implemented in the Moses toolkit (Koehn et al., 2007), and we report evaluation results over two datasets: NIST, which has four reference translations, and IWSLT, with a single reference translation. In order to be able to directly compare the candidate/reference length ratios on the development and on the testing datasets, we need to make sure that we use the same tokenization when calculating BLEU on tuning and on testing. Such differences can arise because many standard scoring tools, e.g., those of NIST, work on detokenized text, which they retokenize again internally; this retokenizat"
C12-1121,C04-1072,0,0.657577,"mation to BLEU, where a sentence is scored in the context of a pseudo-document formed from the n-gram statistics for the last few updates. MIRA can scale to thousands of parameters and generally has no length bias (see Table 4). Another recent, but already popular alternative to MERT is PRO (Hopkins and May, 2011), which models parameter tuning as pairwise ranking optimization. This is a batch tuning algorithm, which iterates between translation and optimization, just like MERT, but scales to thousands of parameters. It uses an add-one smoothed sentence-level version of BLEU, known as BLEU+1 (Lin and Och, 2004), and suffers from a length bias: the parameters it finds yield translations that are too short compared to the references (see Table 4). Exploring the reasons for this bias and proposing ways to solve it is the main focus of this paper. There are many other tuning strategies, which fall outside of the scope of the current study, but to many of which some of our general finding and conclusions should apply. This includes improved versions of some of the above-mentioned algorithms, e.g., a batch version of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al.,"
C12-1121,P03-1021,0,0.533365,"ative training using maximum likelihood parameter estimation. This was inspired by the noisy channel model (Brown et al., 1993), which asked for calculating the product of two components, a language model and a translation model, giving them equal weights. As mainstream research has moved towards combining multiple scores, the field has switched to discriminative tuning in a log-linear fashion. The standard approach has been to maximize BLEU (Papineni et al., 2002) on a tuning dataset using a coordinate descent optimization algorithm known as minimum error rate training (MERT), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually increased, in some cases to hundreds and even to hundreds of thousands of scores, which has called for new tuning algorithms since MERT was unable to scale beyond just a handful of parameters. Many alternatives to MERT have been proposed over the years, but it is only recently that some of them have gained popularity in the community, most notably, the margin infused relaxed algorithm (MIRA) (Chiang et al., 2008) and pairwise ranking optimization (PRO) (Hopkins and May, 2011). Whil"
C12-1121,P02-1040,0,0.107284,"Papers, pages 1979–1994, COLING 2012, Mumbai, December 2012. 1979 1 Introduction Early work on statistical machine translation (SMT) has relied on generative training using maximum likelihood parameter estimation. This was inspired by the noisy channel model (Brown et al., 1993), which asked for calculating the product of two components, a language model and a translation model, giving them equal weights. As mainstream research has moved towards combining multiple scores, the field has switched to discriminative tuning in a log-linear fashion. The standard approach has been to maximize BLEU (Papineni et al., 2002) on a tuning dataset using a coordinate descent optimization algorithm known as minimum error rate training (MERT), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually increased, in some cases to hundreds and even to hundreds of thousands of scores, which has called for new tuning algorithms since MERT was unable to scale beyond just a handful of parameters. Many alternatives to MERT have been proposed over the years, but it is only recently that some of them have gained popularity in the community, most not"
C12-1121,P08-2030,0,0.0550344,"he same models that were used for training and tuning.7 Finally, in order to avoid stability issues, we report results averaged over three runs. 4.1 Experimental Setup Preprocessing: We tokenized the English side of all bi-texts and the monolingual data for language modeling using the standard tokenizer of Moses. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus; for lines containing ALL CAPS, we did this for each word. We segmented the words on the Arabic side using the ATB segmentation scheme: we used MADA (Roth et al., 2008) for NIST, and the Stanford word segmenter (Green and DeNero, 2012) for IWSLT. Training. We built separate directed word alignments using IBM model 4 (Brown et al., 1993), we symmetrized them with the grow-diag-final-and heuristic of Moses, and we extracted phrase pairs of length up to seven. We scored these pairs using maximum likelihood with Kneser-Ney smoothing, to build a phrase table with five standard scores: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We also built a lexicalized reordering model (Koehn"
C12-1121,2006.amta-papers.25,0,0.0697215,"lting translations, which we have attributed to the use of sentence-level BLEU+1 as an objective function. We have thus suggested a number of simple modifications, which do improve the length ratio in practice, ultimately yielding better BLEU scores, while also preserving the sentence-level nature of BLEU+1, which makes optimizers simpler conceptually and implementation-wise. In future work, we plan a more systematic study of the relationship between optimizers and objective functions with respect to the target/reference length ratio, which would be extended with other optimizers such as TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009). Overall, we see two promising general directions in which the present study can be extended. First, explore the relationship between sentence-level and corpuslevel optimization and the possibility to combine them. Second, study the characteristics of translations generated using weights from different optimizers: while here we have only touched length, we believe there are many other important aspects that are worth exploring. Acknowledgments We thank the anonymous reviewers for their comments, which helped us improve the paper. 11 For example, the aver"
C12-1121,D07-1080,0,0.167475,"s: (i) generating a k-best list of translation hypotheses using the current parameters values, and (ii) parameter optimization using the k-best lists from all previous iterations. MERT optimizes expected BLEU. It works well for a small number of parameters, but suffers from scalability and stability issues (Foster and Kuhn, 2009). Most importantly for our discussion, it tends not to have length biases; this is also confirmed by our own experiments (see Table 4). Various alternatives to MERT have been proposed, motivated primarily by scalability considerations. One popular alternative is MIRA (Watanabe et al., 2007; Chiang et al., 2008, 2009), which is a perceptron-like online tuning algorithm with passive-aggressive updates. It uses an approximation to BLEU, where a sentence is scored in the context of a pseudo-document formed from the n-gram statistics for the last few updates. MIRA can scale to thousands of parameters and generally has no length bias (see Table 4). Another recent, but already popular alternative to MERT is PRO (Hopkins and May, 2011), which models parameter tuning as pairwise ranking optimization. This is a batch tuning algorithm, which iterates between translation and optimization,"
C12-3042,N09-1036,0,0.0206681,"tribution, we use blocked Gibbs sampling algorithm and sample the whole source sentence and its alignment at the same time. The inference uses the dynamic programming method to avoid explicitly considering all possible segmentations of the source sentence and their alignments with the target sentence. Our inference technique is an extension of the forward sampling-backward filtering presented by (Mochihashi et al., 2009) for monolingual word segmentation. Dynamic programming algorithm has been also employed to sample PCFG parse trees (Johnson et al., 2007) and grammar-based word segmentation (Johnson and Goldwater, 2009). 1 Inupiaq is a language spoken in Northern Alaska We use the term bilingual word segmentation for the problem we are working on to differentiate with monolingual word segmentation problem of learning to segment the text without translation reference. 2 338 In the next section we will discuss the model. Section 3 will describe the inference in detail. Experiments and results will be presented in section 4. 2 Model   A source sentence— s is a sequence of |— s |characters c1 , c2 . . . c|— s is a sentence s |. A segmentation of—   s to denote a sequence of s of |s |words si : s1 , . . . , s"
C12-3042,N07-1018,0,0.036918,"possible on both sides. To get samples of the posterior distribution, we use blocked Gibbs sampling algorithm and sample the whole source sentence and its alignment at the same time. The inference uses the dynamic programming method to avoid explicitly considering all possible segmentations of the source sentence and their alignments with the target sentence. Our inference technique is an extension of the forward sampling-backward filtering presented by (Mochihashi et al., 2009) for monolingual word segmentation. Dynamic programming algorithm has been also employed to sample PCFG parse trees (Johnson et al., 2007) and grammar-based word segmentation (Johnson and Goldwater, 2009). 1 Inupiaq is a language spoken in Northern Alaska We use the term bilingual word segmentation for the problem we are working on to differentiate with monolingual word segmentation problem of learning to segment the text without translation reference. 2 338 In the next section we will discuss the model. Section 3 will describe the inference in detail. Experiments and results will be presented in section 4. 2 Model   A source sentence— s is a sequence of |— s |characters c1 , c2 . . . c|— s is a sentence s |. A segmentation of"
C12-3042,P09-1012,0,0.0213236,"nment to the target language. The alignment consists of at most one-to-one mappings between the source and target words with null alignments possible on both sides. To get samples of the posterior distribution, we use blocked Gibbs sampling algorithm and sample the whole source sentence and its alignment at the same time. The inference uses the dynamic programming method to avoid explicitly considering all possible segmentations of the source sentence and their alignments with the target sentence. Our inference technique is an extension of the forward sampling-backward filtering presented by (Mochihashi et al., 2009) for monolingual word segmentation. Dynamic programming algorithm has been also employed to sample PCFG parse trees (Johnson et al., 2007) and grammar-based word segmentation (Johnson and Goldwater, 2009). 1 Inupiaq is a language spoken in Northern Alaska We use the term bilingual word segmentation for the problem we are working on to differentiate with monolingual word segmentation problem of learning to segment the text without translation reference. 2 338 In the next section we will discuss the model. Section 3 will describe the inference in detail. Experiments and results will be presented"
C12-3042,C10-1092,1,0.8072,"ased on the phonetic matching between two words. The bilingual word segmentation therefore benefits only when the source language and the target language belongs to the same family but does not achieve the same benefit when two languages are unrelated, such as using English translation to segment Arabic or Hebrew. We model the base distribution of target-source word alignment depends on the cooccurance of the two words in parallel corpora, independent of the language pair, the experiment results show the benefit of using English translation for Inupiaq morphology analysis. Our model inherits (Nguyen et al., 2010)’s model of joint distribution of tokenized source text and its alignment to the target language. The alignment consists of at most one-to-one mappings between the source and target words with null alignments possible on both sides. To get samples of the posterior distribution, we use blocked Gibbs sampling algorithm and sample the whole source sentence and its alignment at the same time. The inference uses the dynamic programming method to avoid explicitly considering all possible segmentations of the source sentence and their alignments with the target sentence. Our inference technique is an"
C12-3042,N09-1024,0,0.0239673,"–344, COLING 2012, Mumbai, December 2012. 337 1 Introduction The tasks of morphological analysis or word segmentation have relied on word-formation rules or on available pretokenized corpora such as a Chinese word list, the Arabic or Korean treebanks, or the Czech dependancy treebank. However, these resources are expensive to obtain and for small or endangered languages, these resources are even not available. In recent years, unsupervised methods have been developed to infer the segmentation from unlabeled data such using minimal description length (Creutz and Lagus, 2007), log-linear model (Poon et al., 2009), nonparametric Bayesian model (Goldwater et al., 2009). 1. 2. An¸un maqpi˙gaaliu˙gaa Aiviq . the man is writing the book for Aiviq . An¸un(man) maqpi˙gaa(book) liu˙g(writing) aa Aiviq(Aiviq) . An¸un maqpi˙gaaliun¸itkaa Aiviq . the man is not writing the book for Aiviq . An¸un(man) maqpi˙gaa(book) liu(writing) n¸it(not) kaa Aiviq(Aiviq) . Table 1: Two examples of Inupiaq text, their English translations and their morphology tokenizations and English alignments. A different promising approach is using information from a second language to learn the morphology analysis of the low resource langua"
C12-3042,P08-1084,0,0.206575,"formation from a second language to learn the morphology analysis of the low resource language. Look at the example of Inupiaq1 and its English translation in Table 1. Without knowing the language, let alone its morphology, we can conjecture that Inupiaq morpheme equivalent to English’s “not” must be a substring of “maqpi˙gaaliun¸itkaa” and be overlaping with“n¸itk”. This derivation is not possible without English translation. This paper presents a nonparametric model and blocked Gibbs sampling inference to automatically derive morphology analysis of a text through its English translation. 2 (Snyder and Barzilay, 2008) also applied a Bayesian model with Dirichlet Process priors to multilingual word segmentation task. Their prior distribution of souce word and target word alignment is defined based on the phonetic matching between two words. The bilingual word segmentation therefore benefits only when the source language and the target language belongs to the same family but does not achieve the same benefit when two languages are unrelated, such as using English translation to segment Arabic or Hebrew. We model the base distribution of target-source word alignment depends on the cooccurance of the two words"
C96-2141,1993.mtsummit-1.11,0,\N,Missing
C96-2141,J93-2003,0,\N,Missing
C96-2141,C94-2178,0,\N,Missing
D15-1147,abdelali-etal-2014-amara,1,0.184811,"Missing"
D15-1147,D13-1106,0,0.0123399,"rge to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . ."
D15-1147,D11-1033,0,0.0505111,"nces, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An al"
D15-1147,2014.iwslt-evaluation.6,1,0.906521,"Missing"
D15-1147,2011.iwslt-evaluation.18,0,0.0228131,"etely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasl"
D15-1147,N13-1114,0,0.33049,"domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural networ"
D15-1147,P13-1126,0,0.513472,"domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural networ"
D15-1147,N12-1047,0,0.0255873,"sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the relatedness of each domain with respect to the in-domain. We later replicated selective experiments for the English-German language pair. The first part of Table 2 summarizes the results for Arabic-English. The perplexity numbers in the second column (NNJMb ) show that NEWS is the 1265 Doma"
D15-1147,P14-1129,0,0.286949,"the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as an additional language model feature. Our aim in this paper is to advance the state-ofthe-art in SMT by extending NNJM for domain adaptation to leverage the huge amount of out1259 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1259–1270, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguist"
D15-1147,P13-2119,0,0.178533,"a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advanta"
D15-1147,E14-4029,1,0.0587384,"del (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the relatedness of each domain with respect to the in-domain. We later replicated selective experiments for the English-German language pair. The first part of Table 2 summarizes the results for Arabic-English. The perplexity numbers in the s"
D15-1147,P13-1141,0,0.0984619,"ture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In r"
D15-1147,2015.mtsummit-papers.10,1,0.422186,"ensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting"
D15-1147,N13-1073,0,0.0372083,"9/125 IWSLT CC NEWS EP 177K 2.3M 200K 1.8M Tok. 3.5/3.3 57/53 2.8/3.4 51/48 Table 1: Statistics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) f"
D15-1147,P12-2023,0,0.0241725,"2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss"
D15-1147,eisele-chen-2010-multiun,0,0.0290387,"Missing"
D15-1147,W08-0334,0,0.257478,"that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptati"
D15-1147,W07-0717,0,0.0175294,"taken from out-domain data completely changes the meaning of the sentence. In this paper, we tackle this problem by proposing domain adaptation models that make use of all the data while preserving the in-domain preferences. A significant amount of research has been carried out recently in domain adaptation. The complexity of the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as an additional language model feature. Our a"
D15-1147,W09-0439,0,0.181637,"An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidel"
D15-1147,D10-1044,0,0.0298816,"Missing"
D15-1147,D08-1089,0,0.134778,"training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used"
D15-1147,P14-1066,0,0.0175969,"s. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transfo"
D15-1147,P12-1016,0,0.0157674,"al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the"
D15-1147,2013.iwslt-papers.2,1,0.883493,"Missing"
D15-1147,E14-1035,0,0.111537,"2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptati"
D15-1147,W11-2123,0,0.0263896,"stics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transl"
D15-1147,2005.eamt-1.19,1,0.289763,"o be an effective way to discard poor quality or irrelevant training instances, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that"
D15-1147,C14-1182,0,0.475615,"Missing"
D15-1147,D13-1176,0,0.0524999,"-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) d"
D15-1147,P07-2045,0,0.00828985,"are/nplm/ Corpus AR-EN Sent. IWSLT QED NEWS UN 150k 150k 203k 3.7M Tok. Corpus EN-DE Sent. 2.8/3.0 1.4/1.5 5.6/6.3 129/125 IWSLT CC NEWS EP 177K 2.3M 200K 1.8M Tok. 3.5/3.3 57/53 2.8/3.4 51/48 Table 1: Statistics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and"
D15-1147,P14-2093,0,0.196766,"Missing"
D15-1147,C12-2104,0,0.0153129,"lized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transformations of xn"
D15-1147,N13-1074,0,0.197071,"It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Ma"
D15-1147,P13-1082,0,0.126125,"n (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data witho"
D15-1147,C14-1105,0,0.147949,"3). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability"
D15-1147,E12-1055,0,0.100976,"ss useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014),"
D15-1147,D09-1074,0,0.161736,"ice overload”. The sense of the Arabic phrase taken from out-domain data completely changes the meaning of the sentence. In this paper, we tackle this problem by proposing domain adaptation models that make use of all the data while preserving the in-domain preferences. A significant amount of research has been carried out recently in domain adaptation. The complexity of the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as"
D15-1147,P13-1045,0,0.0308553,"s the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transformations of xn through the hidden layers, and wk are the weights from the last hidd"
D15-1147,N13-1090,0,0.072173,"al network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transformations of xn through the hidden layers, and wk are the weig"
D15-1147,D13-1140,0,0.283823,"(yn = k) is an indicator variable (i.e., ynk =1 when yn =k, otherwise 0). Optimization is performed using first-order online methods, such as stochastic gradient ascent (SGA) with standard backpropagation algorithm. Unfortunately, training NNLMs are impractically slow because for each training instance (xn , yn ), the softmax output layer (see Equation 2) needs to compute a summation over all words in the output vocabulary.2 Noise contrastive estimation or NCE (Gutmann and Hyv¨arinen, 2010) provides an efficient and stable way to avoid this repetitive computation as recently applied to NNLMs (Vaswani et al., 2013; Mnih and Teh, 2012). We can re-write Equation 2 as follows: P (yn = k|xn , θ) = σ(yn = k|xn , θ) Z(φ(xn ), W) (4) where σ(.) is the un-normalized score and Z(.) is the normalization factor. In NCE, we consider 2 This would take few weeks for a modern CPU machine to train a single NNJM model on the whole data. 1261 Look-up layer Hidden layer Output layer U Source token 1 W Source token 2 C π yn Source token 3 Target token 1 ψ ynm Target token 2 xn M φ(xn ) Figure 1: A simplified neural network joint model with noise contrastive loss, where we use 3-gram target words (i.e., 2-words history) an"
D15-1147,P10-2041,0,0.0346668,"rrelevant training instances, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time c"
D15-1147,D09-1141,0,0.149564,"cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not l"
D15-1147,P02-1040,0,0.103895,"he cross entropy by regularizing the loss function with respect to the in-domain model. The regularizer gives higher weight to the training instances that are similar to the in-domain data. Our second model takes a more conservative approach by additionally penalizing data instances similar to the out-domain data. We evaluate our models on the standard task of translating Arabic-English and English-German language pairs. Our adapted models achieve better perplexities (Chen and Goodman, 1999) than the models trained on in- and in+out-domain data. Improvements are also reflected in BLEU scores (Papineni et al., 2002) as we compare these models within the SMT pipeline. We obtain gains of up to 0.5 and 0.6 on Arabic-English and EnglishGerman pairs over a competitive baseline system. The remainder of this paper is organized as follows: Section 2 gives an account on related work. Section 3 revisits NNJM model and Section 4 discusses our models. Section 5 presents the experimental setup and the results. Section 6 concludes. 2 Related Work Previous work on domain adaptation in MT can be broken down broadly into two main categories namely data selection and model adaptation. 2.1 Data Selection Data selection has"
D15-1147,P13-2071,1,\N,Missing
D15-1147,P11-1105,1,\N,Missing
D15-1147,J15-2001,1,\N,Missing
D15-1147,W13-2212,1,\N,Missing
D15-1147,W14-3302,0,\N,Missing
E03-1050,J93-2003,0,0.00933011,"Missing"
E03-1050,W02-1037,0,0.0334705,"Missing"
E03-1050,C96-2141,1,0.724331,"n-grams are actually overlapping, resulting from a very small number of very long matches was detected. And each n-gram contains m (n-m+1)-grams. The longest matching n-grams in the Xinhua news corpus were 56, 53, 43, 34, 31, 28, 24, 21 words long, each occurring once. Table 2: Number of n-grams from test sentences found in the different corpora. n 2 3 4 5 6 7 8 9 10 Clean 12621 6990 2396 810 314 123 53 29 18 XN 11503 6525 2735 1283 745 486 368 310 275 Clean + XN 13683 8663 3628 1611 884 545 395 321 281 3.4 Training the Alignment Models IBM1 alignments (Brown et al., 1993) and HMM alignments (Vogel et al., 1996) were trained for both the clean parallel corpus and for the extended corpus with the noisy Xinhua News data. The alignment models were trained for Chinese to English as well as English to Chinese. Phrase-tophrase translations were extracted from the Viterbi path of the HMM alignment. The reverse alignment, i.e. English to Chinese, was used for phrase pair extraction as this resulted in higher translation quality in our experiments. The translation probabilities, however, where calculated using the lexicon trained with the IBM1 Chinese to English alignment. Table 3: Training perplexity for cle"
E03-1050,1999.mtsummit-1.79,0,\N,Missing
E17-3016,E14-4029,1,0.83174,"f the systems.3 The results shown in Figure 3 depict the significant time gain we achieved using the pruned phrase based system. However, with a 5 BLEU point difference in translation quality, we decided to compromise and use the slower NMTCPU in our final demo. We also allow the user to switch to the phrase-based system, if translation speed is more important. We did not use NMTGPU since it is very costly to put into production with its requirement for a dedicated GPU card. Finally, we added a customized dictionary and translated unknown words by transliterating them in a post-decoding step (Durrani et al., 2014). PB-Pruned: The PB-best system is not suitable for real time translation and has high memory requirements. To increase the efficiency, we dropped the OSM and NNJM features, heavily pruned the language model and used MML-filtering to select a subset of training data. The resulting system was trained on 1.2 M sentences, 10 times less the original data. 2.4 NMT-GPU: This is our best system2 that we submitted to the IWSLT’16 campaign (Durrani et al., 2016). The advantage of Neural models is that their size does not scale linearly with the data, and hence we were able to train using all available"
E17-3016,W16-2323,0,0.0666475,"Missing"
E17-3016,P14-1129,0,0.018282,"eamlessly switch between them. We had four systems to choose from for our demo, two of which were Phrase-based systems, and the two were Neural MT systems trained using Nematus (Sennrich et al., 2016). Figure 3: Performance and Translation speed of various MT systems PB-Best: This is a competition-grade phrasebased system, also used for our participation at the IWSLT’16 campaign (Durrani et al., 2016). It was trained using all the freely available ArabicEnglish data with state-of-the-art features such as a large language model, lexical reordering, OSM (Durrani et al., 2011) and NNJM features (Devlin et al., 2014). We also computed the translation speed of each of the systems.3 The results shown in Figure 3 depict the significant time gain we achieved using the pruned phrase based system. However, with a 5 BLEU point difference in translation quality, we decided to compromise and use the slower NMTCPU in our final demo. We also allow the user to switch to the phrase-based system, if translation speed is more important. We did not use NMTGPU since it is very costly to put into production with its requirement for a dedicated GPU card. Finally, we added a customized dictionary and translated unknown words"
E17-3016,P11-1105,1,\N,Missing
E17-3029,P09-1039,0,0.0167833,"Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in n"
E17-3029,N13-1008,1,0.77865,"Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in news https://github.com/andre-martins/ TurboParser 118 Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is neural machine translation ready for deployment? A case study on 30 translation directions. CoRR, abs/1610.01108. documents are connected. 2.9 Storyline Construction and Summarization Storylines are co"
E17-3029,E17-3017,1,0.751559,"m a multilingual corpus of nearly 600k documents in 8 of the 9 SUMMA languages (all except Latvian), which were manually annotated by journalists at Deutsche Welle. The document model is a hierarchical attention network with attention at each level of the hierarchy, inspired by Yang et al. (2016), followed by a sigmoid classification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is"
E17-3029,P13-1020,0,0.025126,"Missing"
E17-3029,E17-1051,1,0.815701,"lassification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 K"
eck-etal-2008-communicating,P98-1069,0,\N,Missing
eck-etal-2008-communicating,C98-1066,0,\N,Missing
eck-etal-2008-communicating,2007.iwslt-1.1,0,\N,Missing
eck-etal-2008-communicating,N07-1046,1,\N,Missing
eck-etal-2008-communicating,N06-1003,0,\N,Missing
eck-etal-2008-communicating,D07-1092,0,\N,Missing
eck-etal-2008-communicating,P07-1092,0,\N,Missing
eck-etal-2008-communicating,takezawa-etal-2002-toward,0,\N,Missing
eck-etal-2008-communicating,2005.iwslt-1.6,1,\N,Missing
eck-etal-2008-communicating,2007.iwslt-1.27,0,\N,Missing
eck-etal-2008-communicating,2005.iwslt-1.4,0,\N,Missing
H05-1061,J93-2003,0,0.00387009,"e Chinese phrase f has J Chinese characters, f 1 , f 2 ,... f J , and the English candidate phrase e has L English words, e1 , e2 ,..., eL . The transliteration cost between a Chinese query f and an English translation candidate e is calculated as: Ctrl (e, f ) ≈ ∑ log p(ea j |y j ) = ∑∑ log p(ea( i , j ) |yi , j ). j j i where y j is the pinyin of Chinese character f j , y j ,i is the i th letter in y j , and ea and ea j ( j ,i ) Translation Model The translation model measures the semantic equivalence between a Chinese phrase and an English candidate. One widely used model is the IBM model (Brown et al. 1993). The phrase translation probability is computed using the IBM model-1 as: 1 Ptrans ( f |e ) = J L J j 3.3 Frequency-Distance Model The more often a bilingual phrase pair co-occurs, or the closer a bilingual phrase pair is within a snippet, the more likely they are translations of each other. The frequency-distance model measures this correlation. Suppose S is the set of returned snippets for query f , and a single returned snippet is si ∈ S . The source phrase occurs in si as f i , j ( j ≥ 1 since f may occur several times in a snippet). The frequency-distance weight of an English candidate e"
H05-1061,W03-1502,1,0.827904,"and location names, which are phonetically translated and whose written forms resemble their pronunciations. Therefore it is possible to discover these translation pairs through their surface strings. Surface string transliteration does not need a pronunciation lexicon to map words into phoneme sequences; thus it is especially appealing for OOV word translation. For non-Latin languages like Chinese, a romanization script called “pinyin” maps each Chinese character into Latin letter strings. This normalization makes the string alignment possible. We adopt the transliteration model proposed in (Huang, et al. 2003). This model calculates the probabilistic Levinstein distance between a romanized source string and a target string. Unlike the traditional Levinstein distance calculation, the character alignment cost is not binary (0/1); rather it is the logarithm of character alignment probability, which ensures that characters with similar pronunciations (e.g. `p` and `b`) have higher alignment probabilities and lower cost. These probabilities are automatically learned from bilingual name lists using EM. Assume the Chinese phrase f has J Chinese characters, f 1 , f 2 ,... f J , and the English candidate ph"
H05-1061,2003.mtsummit-papers.53,1,\N,Missing
H05-1061,J03-3002,0,\N,Missing
I11-1053,P07-2045,0,0.0025733,"active hiker} and a verb list SV {comes from, works for, is}. Our model constructs simple sentences such as “John comes from England” , “John comes from IMF” and “John comes from an active hiker”. The total number of simple sentences, |S|, is 48. 4.2 IMF is an active hiker Figure 2: Left-right decoding by objects This section presents a solution to the decoding problem. The solution is based on a stack decoding algorithm that finds the best S given an English sentence e. Our decoding algorithm is inspired by the decoding algorithms in speech recognition and machine translation (Jelinek, 1998; Koehn et al., 2007). For example, with a sentence e “John comes from England, works for IMF, and is an active hiker”, the stack decoding algorithm tries to find S, which is a set of three sentences: “John comes from England”, “John works for IMF” and “John is an active hiker”. Note that S is a set of k simple sentences S = {s1 , ..., si , ..., sk }. We can assume the items si are drawn from a finite set S of grammatical sentences that can be derived from e. Therefore, the first step is to construct the set S. 4.1 # John is an active hiker an active hiker Figure 1: Constructing simple sentences 4 # an active hi"
I11-1053,P06-1096,0,0.0987345,"Missing"
I11-1053,W04-1013,0,0.0157446,"incorrect output. In line 6, α can be interpreted as an update step size; when α is a large number we want to update our weights aggressively, otherwise weights are 477 updated conservatively. α is computed as follow: L(eo , eh ; εt ) = AveFN (eo , εt ) − AveFN (eh , εt ) (4) where AveFN (eo , εt ) and AveFN (eh , εt ) is the average n-gram (n=[2:N]) cooccurrence F-score of (eo , εt ) and (eh , εt ), respectively. In this case, we optimize the weights directly against the AveFN metric over the training data. AveFN can be substituted by other evaluation metrics such as the ROUGE family metric (Lin, 2004). Similar to the perceptron method, the actual weight vector during decoding is averaged across the number of iterations and training instances; and it is computed in line 11. connection. One way to possibly reduce this kind of mistake is analyze the dependency chain between S, V, and O on the original dependency tree of e. Our dependency structure features include the minimum and maximum distances of (S:O), (S:V), and (V:O). Syntactic Structures Another source of information is the syntactic parse tree of e, which can be used to extract syntactic features. The sentence-like boundary feature c"
I11-1053,de-marneffe-etal-2006-generating,0,0.00934639,"rage all scores (Flesch, 1948; Gunning, 1968; McLaughlin, 1969; Kincaid et al., We now turn to the modeling problem. Our fundamental question is: given the model in Equation 1 with M feature functions, what linguistic features can be leveraged to capture semantic information of the original sentence? We address the question in this section by describing features that cover different levels of linguistic structures. Our model incorporates 177 features based on information from the original English sentence e which contains chunks, syntactic and dependency parse trees (Ramshaw and Marcus, 1995; Marneffe et al., 2006). 6.1 Interactive simple sentence features Simple sentence level features A simplification hypothesis s contains k simple sentences. Therefore, it is crucial that our model chooses reasonable simple sentences to form a hypothesis. For each simple sentence si we incorporated the following feature functions: Word Count These features count the number word in subject (S), verb (V) and object (O), also counting the number of proper nouns in S and the number of proper nouns in O. Distance between NPs and Verbs These features focus on the number of NPs and VPs in between S, V and O. This feature gro"
I11-1053,P05-1012,0,0.0483633,"e training set (St , et ) and updates the weights so that the score of the correct simplification εt is greater than the score of all other simplifications by a margin proportional to their loss. However, given a sentence there are an exponential amount of possible simplification candidates. Therefore, the optimizer has to deal with an exponentially large number of constraints. To tackle this, we only consider K-best hypotheses and choose m-oracle hypotheses to support the weight update decision. This idea is similar to the way MIRA has been used in dependency parsing and machine translation (McDonald et al., 2005; Liang et al., 2006; Watanabe et al., 2007). Learning Since defining a log-linear sentence simplification model and decoding algorithm has been completed, this section describes a discriminative learning algorithm for the learning problem. We learn optimized weight vector w by using the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003), which is an online learner closely related to both the support vector machine and perceptron learning framework. In general, weights are updated at each step time On each update, MIRA attempts to keep the new weight vector as close as possibl"
I11-1053,W10-1001,0,0.0720269,"s. There have been readability tests such as Flesch, Gunning-Fog, SMOG, Flesch-Kincaid, etc. (Flesch, 1948; Gunning, 1968; McLaughlin, 1969; Kincaid et al., 1975). In this work, we will use FleschKincaid grade level which can be interpret as the number of years of education generally required to understand a text. Furthermore, automatic evaluation of summaries has also been explored recently. The work of Lin (2004) on the ROUGE family metric is perhaps the best known study of automatic summarization evaluation. Other methods have been proposed such as Pyramid (Nenkova et al., 2007). Recently, Aluisio et al. (2010) proposed readability assessment for sentence simplification. Our models are optimized toward AveF10 , which is the average F-score of n-gram concurrence between hypothesis and reference in which n is from 2 to 10. Besides AveF10 , we will report automatic evaluation scores on the unseen test set in Flesch-Kincaid grade level, ROUGE-2 and ROUGE-4. When we evaluate on a test set, a score will be reported as the average score per sentence. 7.3 Model behaviors How well does our system learn from the labeled corpus? To answer this question we investigate the interactions of model and decoder hyper"
I11-1053,E06-1038,0,0.0244931,"ocument summarization, which produces a sentence that conveys common information of multiple sentences based upon dependency tree structures and lexical similarity. Sentence compression generates a summary of a single sentence with minimal information loss, which can also be treated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charniak, 2005) to discriminative models (McDonald, 2006) and Integer Linear Programming (Clarke, 2008). Another line of research treats sentence compression as machine translation, in which tree-based translation models have been developed (Galley and McKeown, 2007; Cohn and Lapata, 2008; Zhu et al., 2010). Recently, Woodsend and Lapata (2011) proposed a framework to combine treebased simplification with ILP. In contrast to sentence compression, sentence simplification generates multiple sentences from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to transform sentences in complicated structur"
I11-1053,J05-3002,0,0.0260945,"to the decoding process as soft constraints in order to explore a much larger search space. Related Work Given the problematic nature of text-to-text generation that takes a sentence or a document as the input and optimizes the output toward a certain objective, we briefly review state-of-art approaches of text-to-text generation methods. Early approaches in summarization focus on extraction methods which try to isolate and then summarize the most significant sentences or paragraphs of the text. However, this has been found to be insufficient because it usually generates incoherent summaries. Barzilay and McKeown (2005) proposed sentence fusion for multi-document summarization, which produces a sentence that conveys common information of multiple sentences based upon dependency tree structures and lexical similarity. Sentence compression generates a summary of a single sentence with minimal information loss, which can also be treated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charn"
I11-1053,W10-0406,0,0.109654,"Missing"
I11-1053,C96-2183,0,0.435585,"proposed system demonstrates an improvement of 0.2, 0.6, and 4.5 points in ROUGE-2, ROUGE-4, and AveF10 , respectively. 1 Introduction Complicated sentences impose difficulties on reading comprehension. For instance, a person in 5th grade can comprehend a comic book easily but will struggle to understand New York Times articles which require at least 12th grade average reading level (Flesch, 1981). Complicated sentences also challenge natural language processing applications including, but not limited to, text summarization, question answering, information extraction, and machine translation (Chandrasekar et al., 1996). An example of this is syntactic parsing in which long and complicated sentences will generate a large number of hypotheses and usually fail in disambiguating the attachments. Therefore, it is desirable to pre-process complicated sentences and generate simpler counter parts. There are direct applications of sentence simplification. Dalemans et al. (2004) applied sentence simplification so that the automatically generated closed caption can fit into limited display area. The Facilita system generates accessible content from Brazilian Portuguese web pages for low literacy readers using both sum"
I11-1053,W95-0107,0,0.0384332,"readability index, and average all scores (Flesch, 1948; Gunning, 1968; McLaughlin, 1969; Kincaid et al., We now turn to the modeling problem. Our fundamental question is: given the model in Equation 1 with M feature functions, what linguistic features can be leveraged to capture semantic information of the original sentence? We address the question in this section by describing features that cover different levels of linguistic structures. Our model incorporates 177 features based on information from the original English sentence e which contains chunks, syntactic and dependency parse trees (Ramshaw and Marcus, 1995; Marneffe et al., 2006). 6.1 Interactive simple sentence features Simple sentence level features A simplification hypothesis s contains k simple sentences. Therefore, it is crucial that our model chooses reasonable simple sentences to form a hypothesis. For each simple sentence si we incorporated the following feature functions: Word Count These features count the number word in subject (S), verb (V) and object (O), also counting the number of proper nouns in S and the number of proper nouns in O. Distance between NPs and Verbs These features focus on the number of NPs and VPs in between S, V"
I11-1053,C08-1018,0,0.106052,"with minimal information loss, which can also be treated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charniak, 2005) to discriminative models (McDonald, 2006) and Integer Linear Programming (Clarke, 2008). Another line of research treats sentence compression as machine translation, in which tree-based translation models have been developed (Galley and McKeown, 2007; Cohn and Lapata, 2008; Zhu et al., 2010). Recently, Woodsend and Lapata (2011) proposed a framework to combine treebased simplification with ILP. In contrast to sentence compression, sentence simplification generates multiple sentences from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to transform sentences in complicated structures to a set of easy-to-read sentences, which will be easier for human to comprehend, and hopefully easier for computers to deal with. Numerous attempts have been made to tackle the sentence simplification problem. One line of resear"
I11-1053,P05-1036,0,0.185035,"d McKeown (2005) proposed sentence fusion for multi-document summarization, which produces a sentence that conveys common information of multiple sentences based upon dependency tree structures and lexical similarity. Sentence compression generates a summary of a single sentence with minimal information loss, which can also be treated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charniak, 2005) to discriminative models (McDonald, 2006) and Integer Linear Programming (Clarke, 2008). Another line of research treats sentence compression as machine translation, in which tree-based translation models have been developed (Galley and McKeown, 2007; Cohn and Lapata, 2008; Zhu et al., 2010). Recently, Woodsend and Lapata (2011) proposed a framework to combine treebased simplification with ILP. In contrast to sentence compression, sentence simplification generates multiple sentences from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to t"
I11-1053,D07-1080,0,0.0242227,"eights so that the score of the correct simplification εt is greater than the score of all other simplifications by a margin proportional to their loss. However, given a sentence there are an exponential amount of possible simplification candidates. Therefore, the optimizer has to deal with an exponentially large number of constraints. To tackle this, we only consider K-best hypotheses and choose m-oracle hypotheses to support the weight update decision. This idea is similar to the way MIRA has been used in dependency parsing and machine translation (McDonald et al., 2005; Liang et al., 2006; Watanabe et al., 2007). Learning Since defining a log-linear sentence simplification model and decoding algorithm has been completed, this section describes a discriminative learning algorithm for the learning problem. We learn optimized weight vector w by using the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003), which is an online learner closely related to both the support vector machine and perceptron learning framework. In general, weights are updated at each step time On each update, MIRA attempts to keep the new weight vector as close as possible to the old weight vector. Subject to margi"
I11-1053,N07-1023,0,0.0210113,"mmary of a single sentence with minimal information loss, which can also be treated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charniak, 2005) to discriminative models (McDonald, 2006) and Integer Linear Programming (Clarke, 2008). Another line of research treats sentence compression as machine translation, in which tree-based translation models have been developed (Galley and McKeown, 2007; Cohn and Lapata, 2008; Zhu et al., 2010). Recently, Woodsend and Lapata (2011) proposed a framework to combine treebased simplification with ILP. In contrast to sentence compression, sentence simplification generates multiple sentences from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to transform sentences in complicated structures to a set of easy-to-read sentences, which will be easier for human to comprehend, and hopefully easier for computers to deal with. Numerous attempts have been made to tackle the sentence simplification prob"
I11-1053,D11-1038,0,0.170634,"eated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charniak, 2005) to discriminative models (McDonald, 2006) and Integer Linear Programming (Clarke, 2008). Another line of research treats sentence compression as machine translation, in which tree-based translation models have been developed (Galley and McKeown, 2007; Cohn and Lapata, 2008; Zhu et al., 2010). Recently, Woodsend and Lapata (2011) proposed a framework to combine treebased simplification with ILP. In contrast to sentence compression, sentence simplification generates multiple sentences from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to transform sentences in complicated structures to a set of easy-to-read sentences, which will be easier for human to comprehend, and hopefully easier for computers to deal with. Numerous attempts have been made to tackle the sentence simplification problem. One line of research has explored simplification with linguistic rules. Jon"
I11-1053,C10-1152,0,0.4484,"on loss, which can also be treated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charniak, 2005) to discriminative models (McDonald, 2006) and Integer Linear Programming (Clarke, 2008). Another line of research treats sentence compression as machine translation, in which tree-based translation models have been developed (Galley and McKeown, 2007; Cohn and Lapata, 2008; Zhu et al., 2010). Recently, Woodsend and Lapata (2011) proposed a framework to combine treebased simplification with ILP. In contrast to sentence compression, sentence simplification generates multiple sentences from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to transform sentences in complicated structures to a set of easy-to-read sentences, which will be easier for human to comprehend, and hopefully easier for computers to deal with. Numerous attempts have been made to tackle the sentence simplification problem. One line of research has explored sim"
I11-1053,N09-2045,0,0.0309621,"from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to transform sentences in complicated structures to a set of easy-to-read sentences, which will be easier for human to comprehend, and hopefully easier for computers to deal with. Numerous attempts have been made to tackle the sentence simplification problem. One line of research has explored simplification with linguistic rules. Jonnalagadda (2006) developed a rule-based system that take into account the discourse information. This method is applied on simplification of biomedical text (Jonnalagadda et al., 2009) and protein-protein information extraction (Jonnalagadda and Gonzalez, 2010). Chandrasekar and Srinivas (1997) automatically induced simplification rules based on dependency trees. Additionally, Klebanov et al. (2004) develop a set of rules that generate a set of EAS from syntactically complicated sentences. Heilman and Smith (2010) proposed an algorithm for extracting simplified declarative sentences from syntactically complex sentences. The rule-based systems performs well on English. 3 Statistical Sentence Simplification with Log-linear Models Assume that we are given an English sentence e"
I11-1053,daelemans-etal-2004-automatic,0,\N,Missing
I17-1015,I17-1001,1,0.708774,"ng in the Neural Machine Translation Decoder Fahim Dalvi Nadir Durrani Hassan Sajjad Yonatan Belinkov∗ Stephan Vogel Qatar Computing Research Institute – HBKU, Doha, Qatar {faimaduddin, ndurrani, hsajjad, svogel}@qf.org.qa ∗ MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139, USA belinkov@mit.edu Abstract what NMT models learn about morphology (Belinkov et al., 2017a), syntax (Shi et al., 2016) and semantics (Belinkov et al., 2017b). Shi et al. (2016) used activations at various layers from the NMT encoder to predict syntactic properties on the source-side, while Belinkov et al. (2017a) and Belinkov et al. (2017b) used a similar approach to investigate the quality of word representations on the task of morphological and semantic tagging. Belinkov et al. (2017a) found that word representations learned from the encoder are rich in morphological information, while representations learned from the decoder are significantly poorer. However, the paper does not present a convincing explanation for this finding. Our first contribution in this work is to provide a more comprehensive analysis of morphological learning on the decoder side. We hypothesize that other components of the"
I17-1015,D16-1025,0,0.0176372,"d iii) multi-task learning. Our results show that explicit morphological information helps the decoder learn target language morphology and improves the translation quality by 0.2–0.6 BLEU points. 1 Introduction • What is the effect of attention on the performance of the decoder? Neural machine translation (NMT) offers an elegant end-to-end architecture, improving translation quality compared to traditional phrase-based machine translation. These improvements are attributed to more fluent output (Toral and S´anchezCartagena, 2017) and better handling of morphology and long-range dependencies (Bentivogli et al., 2016). However, systematic studies are required to understand what kinds of linguistic phenomena (morphology, syntax, semantics, etc.) are learned by these models and more importantly, which of the components is responsible for each phenomenon. A few attempts have been made to understand • How much does the encoder help the decoder in predicting the correct morphological variant of the word it generates? To answer these questions, we train NMT models for different language pairs, involving morphologically rich languages such as German and Czech. We then use the trained models to extract features fr"
I17-1015,2014.iwslt-evaluation.6,1,0.839752,"o integrate morphology into the decoder. Section 5 presents the results. Section 6 gives an account of related work and Section 7 concludes the paper. 2 Language-pair NMT Systems We used the seq2seq-attn implementation (Kim, 2016) with the following default settings: word embeddings and LSTM states with 500 dimensions, SGD with an initial learning rate of 1.0 and decay rate of 0.5 (after the 9th epoch), and dropout rate of 0.3. We use two uni-directional hidden layers for both the encoder and the decoder. 1 These have been used frequently to annotate data in the previous evaluation campaigns (Birch et al., 2014; Durrani et al., 2014a). 2 The difficulty with using these is that it is not straightforward to derive word representations out of a decoder that processes BPE-ed text, because the original words are split into subwords. We considered aggregating the representations of BPE subword units, but the choice of aggregation strategy may have an undesired impact on the analysis. For this reason we decided to leave exploration of BPE for future work. 3 Character-based models are becoming increasingly popular in Neural MT, for addressing the rare word problem – and they have been used previously also t"
I17-1015,P16-2058,0,0.023772,"Missing"
I17-1015,P17-2021,0,0.0187344,"er was tuned on a separate held out development set (test-11), and the results shown in Figure 3 are on blind test sets (test-12,13). Averages are reported in the figure. 6 Integrating Morphology Some work has also been done in injecting morphological or more general linguistic knowledge into an NMT system. Sennrich and Haddow (2016) proposed a factored model that incorporates linguistic features on the source side as additional factors. An embedding is learned for each factor, just like a source word, and then the word and factor embeddings are combined before being passed on to the encoder. Aharoni and Goldberg (2017) proposed a method to predict the target sentence along with its syntactic tree. They linearize the tree in order to use the existing sequence-to-sequence model. Nadejde et al. (2017) also evaluated several methods of incorporating syntactic knowledge on both the source and target. While they used factors on the source side, their best method for the target side was to linearize the information and interleave it between the target words. Garc´ıa-Mart´ınez et al. (2016) used a neural MT model with multiple outputs, like in our case of Multi-task learning. Their model predicts two properties at"
I17-1015,P15-1166,0,0.0240273,"Missing"
I17-1015,W14-3309,1,0.847572,"gy into the decoder. Section 5 presents the results. Section 6 gives an account of related work and Section 7 concludes the paper. 2 Language-pair NMT Systems We used the seq2seq-attn implementation (Kim, 2016) with the following default settings: word embeddings and LSTM states with 500 dimensions, SGD with an initial learning rate of 1.0 and decay rate of 0.5 (after the 9th epoch), and dropout rate of 0.3. We use two uni-directional hidden layers for both the encoder and the decoder. 1 These have been used frequently to annotate data in the previous evaluation campaigns (Birch et al., 2014; Durrani et al., 2014a). 2 The difficulty with using these is that it is not straightforward to derive word representations out of a decoder that processes BPE-ed text, because the original words are split into subwords. We considered aggregating the representations of BPE subword units, but the choice of aggregation strategy may have an undesired impact on the analysis. For this reason we decided to leave exploration of BPE for future work. 3 Character-based models are becoming increasingly popular in Neural MT, for addressing the rare word problem – and they have been used previously also to benefit MT for morph"
I17-1015,P17-1080,1,0.711845,"ng in the Neural Machine Translation Decoder Fahim Dalvi Nadir Durrani Hassan Sajjad Yonatan Belinkov∗ Stephan Vogel Qatar Computing Research Institute – HBKU, Doha, Qatar {faimaduddin, ndurrani, hsajjad, svogel}@qf.org.qa ∗ MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139, USA belinkov@mit.edu Abstract what NMT models learn about morphology (Belinkov et al., 2017a), syntax (Shi et al., 2016) and semantics (Belinkov et al., 2017b). Shi et al. (2016) used activations at various layers from the NMT encoder to predict syntactic properties on the source-side, while Belinkov et al. (2017a) and Belinkov et al. (2017b) used a similar approach to investigate the quality of word representations on the task of morphological and semantic tagging. Belinkov et al. (2017a) found that word representations learned from the encoder are rich in morphological information, while representations learned from the decoder are significantly poorer. However, the paper does not present a convincing explanation for this finding. Our first contribution in this work is to provide a more comprehensive analysis of morphological learning on the decoder side. We hypothesize that other components of the"
I17-1015,P10-1048,1,0.860476,"cesses BPE-ed text, because the original words are split into subwords. We considered aggregating the representations of BPE subword units, but the choice of aggregation strategy may have an undesired impact on the analysis. For this reason we decided to leave exploration of BPE for future work. 3 Character-based models are becoming increasingly popular in Neural MT, for addressing the rare word problem – and they have been used previously also to benefit MT for morphologically rich (Luong et al., 2010; Belinkov and Glass, 2016; Costa-juss`a and Fonollosa, 2016) and closely related languages (Durrani et al., 2010; Sajjad et al., 2013). Experimental Design Parallel Data We used the German-English and Czech-English datasets from the WIT3 TED corpus (Cettolo, 2016) made available for IWSLT 2016. We used the official training sets to analyze and evaluate the proposed methods for integrating morphology . The corpus also provides four test sets, test-11 through test-14. We used test-11 for tuning, and the other test sets for evaluation. The statistics for the sets are provided in Table 1. 143 weighted average of these hidden states from the previous decoder state (di−1 ), known as the context vector ci (Equ"
I17-1015,D10-1015,0,0.0577683,"lty with using these is that it is not straightforward to derive word representations out of a decoder that processes BPE-ed text, because the original words are split into subwords. We considered aggregating the representations of BPE subword units, but the choice of aggregation strategy may have an undesired impact on the analysis. For this reason we decided to leave exploration of BPE for future work. 3 Character-based models are becoming increasingly popular in Neural MT, for addressing the rare word problem – and they have been used previously also to benefit MT for morphologically rich (Luong et al., 2010; Belinkov and Glass, 2016; Costa-juss`a and Fonollosa, 2016) and closely related languages (Durrani et al., 2010; Sajjad et al., 2013). Experimental Design Parallel Data We used the German-English and Czech-English datasets from the WIT3 TED corpus (Cettolo, 2016) made available for IWSLT 2016. We used the official training sets to analyze and evaluate the proposed methods for integrating morphology . The corpus also provides four test sets, test-11 through test-14. We used test-11 for tuning, and the other test sets for evaluation. The statistics for the sets are provided in Table 1. 143 wei"
I17-1015,P17-2012,0,0.0111985,"raining is to learn several tasks simultaneously such that each task can benefit from the mutual information learned (Collobert and Weston, 2008). 5 With this motivation, we modified the NMT decoder to predict not only a word but also its corresponding tag. All of the layers below the output layers are shared. We have two output layers in parallel – the first to predict the target word, and the second to predict the morphological tag of the target word. Both ouput layJoint-data Learning Given the drawbacks of the first approach, we considered another data augmentation technique 5 For example, Eriguchi et al. (2017) jointly learned the tasks of parsing and translation. 146 Figure 3: Improvements from adding morphology. A y-value of zero represents the baseline ers have their own separate loss function. While training, we combine the losses from both output layers to jointly train the system. This is different from the Joint-data learning technique, where we predict entire sequences of words or tags without any dependence on each other. Formally, given a set of N tasks, sequence-tosequence multi-task learning involves an objective function minimizing the overall loss, which is a weighted combination of th"
I17-1015,D16-1079,0,0.0518352,"Missing"
I17-1015,Q17-1024,0,0.0293413,"Missing"
I17-1015,P16-1140,0,0.0270118,"Missing"
I17-1015,P13-2001,1,0.838603,"cause the original words are split into subwords. We considered aggregating the representations of BPE subword units, but the choice of aggregation strategy may have an undesired impact on the analysis. For this reason we decided to leave exploration of BPE for future work. 3 Character-based models are becoming increasingly popular in Neural MT, for addressing the rare word problem – and they have been used previously also to benefit MT for morphologically rich (Luong et al., 2010; Belinkov and Glass, 2016; Costa-juss`a and Fonollosa, 2016) and closely related languages (Durrani et al., 2010; Sajjad et al., 2013). Experimental Design Parallel Data We used the German-English and Czech-English datasets from the WIT3 TED corpus (Cettolo, 2016) made available for IWSLT 2016. We used the official training sets to analyze and evaluate the proposed methods for integrating morphology . The corpus also provides four test sets, test-11 through test-14. We used test-11 for tuning, and the other test sets for evaluation. The statistics for the sets are provided in Table 1. 143 weighted average of these hidden states from the previous decoder state (di−1 ), known as the context vector ci (Equation 2). The context"
I17-1015,D07-1091,0,0.203126,"Missing"
I17-1015,C94-1027,0,0.0784836,"logical information during training which can in turn improve the overall translation quality. In order to test this hypothesis, we experiment with three possible solutions: Sentences tokde/cz token De↔En Cz↔En 210K 122K 4M 2.1M 4.2M 2.5M Table 1: Statistics for the data used for training, tuning and testing Morphological Annotations In order to train and evaluate the external classifier on the extracted features, we required data annotated with morphological tags. We used the following tools recommended on the Moses website1 to annotate the data: LoPar (Schmid, 2000) for German, Tree-tagger (Schmid, 1994) for Czech and MXPOST (Ratnaparkhi, 1998) for English. The number of tags produced by these taggers is 214 for German and 368 for Czech. 1. Joint Generation: An NMT model is trained on the concatenation of words and morphological tags on the target side. 2. Joint-data learning: An NMT model is trained where each source sequence is used twice with an artificial token to either predict target words or morphological tags. Data preprocessing We used the standard MT pre-processing pipeline of tokenizing and truecasing the data using Moses (Koehn et al., 2007) scripts. We did not apply byte-pair enc"
I17-1015,P07-2045,0,0.0125609,"Missing"
I17-1015,W16-2209,0,0.0260724,"e at that point the model is only minimizing the tag objective function. Similarly at λ = 0, the model falls back to the baseline model with a single objective function minimizing translation error. For all language pairs, we consistently achieved the best BLEU score at λ = 0.2. The parameter was tuned on a separate held out development set (test-11), and the results shown in Figure 3 are on blind test sets (test-12,13). Averages are reported in the figure. 6 Integrating Morphology Some work has also been done in injecting morphological or more general linguistic knowledge into an NMT system. Sennrich and Haddow (2016) proposed a factored model that incorporates linguistic features on the source side as additional factors. An embedding is learned for each factor, just like a source word, and then the word and factor embeddings are combined before being passed on to the encoder. Aharoni and Goldberg (2017) proposed a method to predict the target sentence along with its syntactic tree. They linearize the tree in order to use the existing sequence-to-sequence model. Nadejde et al. (2017) also evaluated several methods of incorporating syntactic knowledge on both the source and target. While they used factors o"
I17-1015,D15-1246,0,0.0420013,"Missing"
I17-1015,N16-1005,0,0.0144274,"phological knowledge into the decoder inspired by multilingual NMT systems (Johnson et al., 2016). Instead of having multiple source and target languages, we used one source language and two target language variations. The training data consists of sequences of source→target words and source→target morphological tags. We added an artificial token in the beginning of each source sentence indicating whether we want to generate target words or morphological tags. Using an artificial token in the source sentence has been explored and shown to work well to control the style of the target language (Sennrich et al., 2016a). The objective function is the same as the one in usual sequence-to-sequence models, and is hence shared to minimize both morphological and translation error given the mixed data. encoder (Table 2) and the overall system does not learn as much about target morphology as source morphology, we investigated three ways to directly inject target morphology into the decoder, namely: i) Joint Generation, ii) Joint-data Learning, iii) Multi-task Learning. Figure 2 illustrates the approaches. 4.1 Joint Generation As our first approach, we considered a solution that uses the standard NMT architecture"
I17-1015,P16-1162,0,0.0453069,"phological knowledge into the decoder inspired by multilingual NMT systems (Johnson et al., 2016). Instead of having multiple source and target languages, we used one source language and two target language variations. The training data consists of sequences of source→target words and source→target morphological tags. We added an artificial token in the beginning of each source sentence indicating whether we want to generate target words or morphological tags. Using an artificial token in the source sentence has been explored and shown to work well to control the style of the target language (Sennrich et al., 2016a). The objective function is the same as the one in usual sequence-to-sequence models, and is hence shared to minimize both morphological and translation error given the mixed data. encoder (Table 2) and the overall system does not learn as much about target morphology as source morphology, we investigated three ways to directly inject target morphology into the decoder, namely: i) Joint Generation, ii) Joint-data Learning, iii) Multi-task Learning. Figure 2 illustrates the approaches. 4.1 Joint Generation As our first approach, we considered a solution that uses the standard NMT architecture"
I17-1015,D16-1159,0,0.118608,"sus 214 in German. We tuned the weight parameter on held-out data. 147 Figure 4: Multi-task learning: Translation vs. Morphological Tagging weight for En→De model relevant information about the input. K¨ohn (2015) and Qian et al. (2016b) analyzed linguistic information learned in word embeddings, while Qian et al. (2016a) went further and analyzed linguistic properties in the hidden states of a recurrent neural network. Adi et al. (2016) looked at the overall information learned in a sentence summary vector generated by an RNN using a similar approach. Our approach closely aligns with that of Shi et al. (2016) and Belinkov et al. (2017a), where the activations from various layers in a trained NMT system are used to predict linguistic properties. be handy if the morphological information quality is not very high. On the flip side, this additional explicit weight adjustment can also be viewed as a potential constraint that is not present in the jointdata learning approach. Multi-task Weight Hyper-Parameter As discussed, the multi-task learning approach has an additional weight hyper-parameter λ that adjusts the balance between word and tag prediction. Figure 4 shows the result of varying λ from no mo"
I17-1015,E17-1100,0,0.0361648,"Missing"
K15-1007,N12-1062,0,0.16668,"SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and"
K15-1007,N15-1106,0,0.0232565,"lar include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emer"
K15-1007,W08-0304,0,0.0236305,", 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as we"
K15-1007,P10-4002,0,0.0542157,"Missing"
K15-1007,2011.mtsummit-papers.1,0,0.0267411,"rsion of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of any previous studies of the impact of sentence len"
K15-1007,N12-1047,0,0.0364323,"), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et"
K15-1007,N04-1035,0,0.0444567,"Missing"
K15-1007,D08-1024,0,0.400262,"Missing"
K15-1007,N12-1023,0,0.102217,"MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been"
K15-1007,N09-1025,0,0.112274,"Missing"
K15-1007,W11-2123,0,0.0394859,"ll WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on GigaWord v.5 with Kneser-Ney smoothing using KenLM (Heafield, 2011). On tuning and testing, we dropped the unknown words for Arabic-English, and we used monotoneat-punctuation decoding for Spanish-English. We tuned using MERT and PRO. We used the standard implementation of MERT from the Moses toolkit, and a fixed version of PRO, as we recommended in (Nakov et al., 2013), which solves instability issues when tuning on the long sentences; we will discuss our PRO fix and the reasons it is needed in Section 5 below. In order to ensure convergence, we allowed both MERT and PRO to run for up to 25 iterations (default: 16); we further used 1000best lists (default: 1"
K15-1007,D11-1125,0,0.0434425,"Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains"
K15-1007,P05-1033,0,0.371943,"Missing"
K15-1007,N03-1017,0,0.0487953,"rences makes both MERT and PRO appear more stable, allowing them to generate hypotheses that are less spread, and closer to 1. This can be attributed to the best match reference length, which naturally dampens the effect of verbosity during optimization by selecting the reference that is closest to the respective hypothesis. Overall, we can conclude that MERT learns the tuning set’s verbosity more accurately than PRO. PRO learns verbosity that is more dependent on the source side length of the sentences in the tuning dataset. Experimental Setup We experimented with the phrase-based SMT model (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). For Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothi"
K15-1007,P11-2031,0,0.395785,"(Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of"
K15-1007,2005.iwslt-1.8,0,0.117546,"Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on GigaWord v.5 with Kneser-Ney smoothing using KenLM (Heafield, 2011). On tuning and testing, we dropped the unknown words for Arabic-English, and we used monotoneat-punctuation decoding for Spanish-English. We tuned using MERT and PRO. We used the standard implementation of MERT from the Moses toolkit, and a fixed version of PRO, as we recommended in (Nakov et al., 2013), which solves instability issues when tuning on the long sentences; we will discuss our PRO fix and the reasons it is needed in Section 5 below. In order to ens"
K15-1007,P05-1066,0,0.065072,"Missing"
K15-1007,P03-1021,0,0.0297129,"anslator; it is often a stylistic choice. and not necessarily related to fluency or adequacy. This aspect is beyond the scope of the present work. 62 Proceedings of the 19th Conference on Computational Language Learning, pages 62–72, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics 2 3 Related Work Method For the following analysis, we need to define the following four quantities: Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (C"
K15-1007,P07-2045,0,0.00687606,"able, allowing them to generate hypotheses that are less spread, and closer to 1. This can be attributed to the best match reference length, which naturally dampens the effect of verbosity during optimization by selecting the reference that is closest to the respective hypothesis. Overall, we can conclude that MERT learns the tuning set’s verbosity more accurately than PRO. PRO learns verbosity that is more dependent on the source side length of the sentences in the tuning dataset. Experimental Setup We experimented with the phrase-based SMT model (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). For Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering mo"
K15-1007,P02-1040,0,0.0928671,"ved in singlereference sets (we used the first reference), and to a lesser extent in multiple-reference sets (five references for MT04 and MT05, and four for MT06 and MT09). For Spanish-English, the story is different: here the English sentences tend to be shorter than the Spanish ones, and the verbosity decreases as the sentence length increases. Overall, in all three cases, the verbosity appears to be length-dependent. 2 For multi-reference sets, we use the length of the reference that is closest to the length of the hypothesis. This is the best match length from the original paper on BLEU (Papineni et al., 2002); it is default in the NIST scoring tool v13a, which we use in our experiments. 3 When dealing with multi-reference sets, we use the average reference length. 4 The datasets we experiment with are described in more detail in Section 4 below. 63 Source length vs. avg. verbosity Arabic−English Spanish−English 1.025 set ● Ar−En−multi Ar−En−single Average verbosity Average verbosity 1.175 1.150 1.125 ● 1.100 ● ● ● ● ● ● ● ●● ●●●● ● ● ●●● ●● ● ●● ●● ● ●●●● ● ●●●● ●● ●● ● ●● ● ● ● ●● ●●● ● ●● ● ●● ● ●● ●●● ●● ● ● ● ● ●● ●● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 1.075 1.000 0.975 ● 0.950 ● ● ● ● ● ●"
K15-1007,P05-1034,0,0.111977,"Missing"
K15-1007,W09-0424,0,0.0539164,"Missing"
K15-1007,P08-2030,0,0.0896902,"ntences in the tuning dataset. Experimental Setup We experimented with the phrase-based SMT model (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). For Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on GigaWord v.5 with Kneser-Ney smoothing using KenLM (Heafield, 2011). On tuning and testing, we dropped the unknown words for Arabic-English, and we used monotoneat-punctuation decoding for Spanish-English. We tuned using MERT and PRO. We used the standard implementation of MERT from the Moses toolkit, and a fixed version of PRO, as we recommended in (Nakov et al"
K15-1007,C10-1075,0,0.228135,"log-linear framework, and their values are optimized to maximize some automatic metric, typically BLEU, on a tuning dataset. Given this setup, it is clear that the choice of a tuning set and its characteristics, can have significant impact on the SMT system’s performance: if the experimental framework (training data, tuning set, and test set) is highly consistent, i.e., there is close similarity in terms of genre, domain and verbosity,1 then translation quality can be improved by careful selection of tuning sentences that exhibit high degree of similarity to the test set (Zheng et al., 2010; Li et al., 2010). In our recent work (Nakov et al., 2012), we have studied the relationship between optimizers such as MERT, PRO and MIRA, and we have pointed out that PRO tends to generate relatively shorter translations, which could lead to lower BLEU scores on testing. Our solution there was to fix the objective function being optimized: PRO uses sentence-level smoothed BLEU+1, as opposed to the standard dataset-level BLEU. Here we are interested in a related but different question: the relationship between properties of the tuning dataset and the optimizer’s performance. More specifically, we study how th"
K15-1007,P12-1002,0,0.0634812,"rk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of any previous studies of the impact of sentence length and dataset verbosity across optimizers. • source-side length: the number of words in the source sentence; • length ratio: the ratio of the number of words in the output hypothesis to those in the reference;2 • verbosity: the ratio of the number of words in the reference to those in the source;3 • hypothesis verbosity: the ratio of the number of words in the hypothesis to those in the source. Naturally, the verbosity varies across different tuning/testing datasets, e.g., because of style, translator choice, et"
K15-1007,D12-1037,0,0.0142596,"ere has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and"
K15-1007,2006.amta-papers.25,0,0.103422,"Missing"
K15-1007,W10-1738,0,0.02313,"Missing"
K15-1007,C08-1074,0,0.0259005,"al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer"
K15-1007,D07-1080,0,0.0742595,"–72, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics 2 3 Related Work Method For the following analysis, we need to define the following four quantities: Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We"
K15-1007,C12-1121,1,0.888761,"Missing"
K15-1007,P13-2003,1,0.728346,"Missing"
L16-1578,avramidis-etal-2014-taraxu,0,0.0184159,"(Carl et al., 2011; Vieira, 2014; Stymne et al., 2012; Doherty and O’Brien, 2009; O’Brien, 2011). Furthermore, the approach is more objective than the approaches which rely on human scores for perceived post-editing effort (Specia, 2011; De Sousa et al., 2011; Koponen, 2012; Vieira, 2014). In its essence, it is similar to other error classification approaches, such as (Flanagan, 1994; Font-Llitj´os et al., 2005; 3644 Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Blain et al., 2011; Stymne, 2011; Koponen, 2012; Koponen et al., 2012; Fishel et al., 2012; Stymne et al., 2012; Vieira, 2014; Avramidis et al., 2014). It is enriched, however by error ranking, based on information specifying which errors require more cognitive effort to be corrected, and which less. In this way, the approach only requires counting the number of errors of each type in the MT output. And thus it allows the comparison of the post-editing cost of different output texts of the same MT engine, the same text as an output of different MT engines, or for different language pairs. Temnikova (2010) tested her approach on two emergency instructions texts, one original (called “Complex”) and one manually simplified (called “Simplified”"
L16-1578,R11-1014,0,0.0530077,"Missing"
L16-1578,2009.mtsummit-posters.5,0,0.1322,"Missing"
L16-1578,2010.eamt-1.12,0,0.0241724,"cognitive evaluation approaches which are based on keystroke logging (Carl et al., 2011; Krings and Koby, 2001; Koponen et al., 2012) or eye-tracking (Carl et al., 2011; Vieira, 2014; Stymne et al., 2012; Doherty and O’Brien, 2009; O’Brien, 2011). Furthermore, the approach is more objective than the approaches which rely on human scores for perceived post-editing effort (Specia, 2011; De Sousa et al., 2011; Koponen, 2012; Vieira, 2014). In its essence, it is similar to other error classification approaches, such as (Flanagan, 1994; Font-Llitj´os et al., 2005; 3644 Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Blain et al., 2011; Stymne, 2011; Koponen, 2012; Koponen et al., 2012; Fishel et al., 2012; Stymne et al., 2012; Vieira, 2014; Avramidis et al., 2014). It is enriched, however by error ranking, based on information specifying which errors require more cognitive effort to be corrected, and which less. In this way, the approach only requires counting the number of errors of each type in the MT output. And thus it allows the comparison of the post-editing cost of different output texts of the same MT engine, the same text as an output of different MT engines, or for different language pairs. Te"
L16-1578,fishel-etal-2012-terra,0,0.0135384,"s and Koby, 2001; Koponen et al., 2012) or eye-tracking (Carl et al., 2011; Vieira, 2014; Stymne et al., 2012; Doherty and O’Brien, 2009; O’Brien, 2011). Furthermore, the approach is more objective than the approaches which rely on human scores for perceived post-editing effort (Specia, 2011; De Sousa et al., 2011; Koponen, 2012; Vieira, 2014). In its essence, it is similar to other error classification approaches, such as (Flanagan, 1994; Font-Llitj´os et al., 2005; 3644 Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Blain et al., 2011; Stymne, 2011; Koponen, 2012; Koponen et al., 2012; Fishel et al., 2012; Stymne et al., 2012; Vieira, 2014; Avramidis et al., 2014). It is enriched, however by error ranking, based on information specifying which errors require more cognitive effort to be corrected, and which less. In this way, the approach only requires counting the number of errors of each type in the MT output. And thus it allows the comparison of the post-editing cost of different output texts of the same MT engine, the same text as an output of different MT engines, or for different language pairs. Temnikova (2010) tested her approach on two emergency instructions texts, one original (called"
L16-1578,I13-2001,1,0.895716,"Missing"
L16-1578,P02-1040,0,0.106211,"6), from which the error classes were subsequently regrouped and ranked in an increasing order, so as to reflect the cognitive load post-editors experience while correcting the MT output. Error re-grouping and ranking was done on the basis of relevant psycholinguistic error correction litera1 https://translate.google.com ture (Harley, 2013; Larigauderie et al., 1998; Baddeley and Hitch, 1974). The aim of proposing such an approach was to create a better metric for the effort a post-editor faces while correcting MT texts, instead of relying on a nontransparent MT evaluation score such as BLEU (Papineni et al., 2002). Figure 1. shows the previous error ranking. The easiest errors to correct were considered those which required only a small change inside the word (CInF), followed by errors requiring replacing or adding a word (Styl, InW, etc.), while the hardest errors were considered those which required understanding of the whole sentence (e.g. InP, MissP, WoW and WoPh). Figure 1: Temnikova (2010)’s Error Ranking. The approach does not rely on using specific software, in contrast to PE cognitive evaluation approaches which are based on keystroke logging (Carl et al., 2011; Krings and Koby, 2001; Koponen"
L16-1578,pasha-etal-2014-madamira,1,0.803417,"Missing"
L16-1578,W15-3204,1,0.794797,"Missing"
L16-1578,2011.eamt-1.12,0,0.0234323,"rs were considered those which required understanding of the whole sentence (e.g. InP, MissP, WoW and WoPh). Figure 1: Temnikova (2010)’s Error Ranking. The approach does not rely on using specific software, in contrast to PE cognitive evaluation approaches which are based on keystroke logging (Carl et al., 2011; Krings and Koby, 2001; Koponen et al., 2012) or eye-tracking (Carl et al., 2011; Vieira, 2014; Stymne et al., 2012; Doherty and O’Brien, 2009; O’Brien, 2011). Furthermore, the approach is more objective than the approaches which rely on human scores for perceived post-editing effort (Specia, 2011; De Sousa et al., 2011; Koponen, 2012; Vieira, 2014). In its essence, it is similar to other error classification approaches, such as (Flanagan, 1994; Font-Llitj´os et al., 2005; 3644 Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Blain et al., 2011; Stymne, 2011; Koponen, 2012; Koponen et al., 2012; Fishel et al., 2012; Stymne et al., 2012; Vieira, 2014; Avramidis et al., 2014). It is enriched, however by error ranking, based on information specifying which errors require more cognitive effort to be corrected, and which less. In this way, the approach only requires counting the number o"
L16-1578,stymne-etal-2012-eye,0,0.0214094,"to correct were considered those which required only a small change inside the word (CInF), followed by errors requiring replacing or adding a word (Styl, InW, etc.), while the hardest errors were considered those which required understanding of the whole sentence (e.g. InP, MissP, WoW and WoPh). Figure 1: Temnikova (2010)’s Error Ranking. The approach does not rely on using specific software, in contrast to PE cognitive evaluation approaches which are based on keystroke logging (Carl et al., 2011; Krings and Koby, 2001; Koponen et al., 2012) or eye-tracking (Carl et al., 2011; Vieira, 2014; Stymne et al., 2012; Doherty and O’Brien, 2009; O’Brien, 2011). Furthermore, the approach is more objective than the approaches which rely on human scores for perceived post-editing effort (Specia, 2011; De Sousa et al., 2011; Koponen, 2012; Vieira, 2014). In its essence, it is similar to other error classification approaches, such as (Flanagan, 1994; Font-Llitj´os et al., 2005; 3644 Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Blain et al., 2011; Stymne, 2011; Koponen, 2012; Koponen et al., 2012; Fishel et al., 2012; Stymne et al., 2012; Vieira, 2014; Avramidis et al., 2014). It is enriched, however by e"
L16-1578,P11-4010,0,0.0197882,"ased on keystroke logging (Carl et al., 2011; Krings and Koby, 2001; Koponen et al., 2012) or eye-tracking (Carl et al., 2011; Vieira, 2014; Stymne et al., 2012; Doherty and O’Brien, 2009; O’Brien, 2011). Furthermore, the approach is more objective than the approaches which rely on human scores for perceived post-editing effort (Specia, 2011; De Sousa et al., 2011; Koponen, 2012; Vieira, 2014). In its essence, it is similar to other error classification approaches, such as (Flanagan, 1994; Font-Llitj´os et al., 2005; 3644 Vilar et al., 2006; Farr´us Cabeceran et al., 2010; Blain et al., 2011; Stymne, 2011; Koponen, 2012; Koponen et al., 2012; Fishel et al., 2012; Stymne et al., 2012; Vieira, 2014; Avramidis et al., 2014). It is enriched, however by error ranking, based on information specifying which errors require more cognitive effort to be corrected, and which less. In this way, the approach only requires counting the number of errors of each type in the MT output. And thus it allows the comparison of the post-editing cost of different output texts of the same MT engine, the same text as an output of different MT engines, or for different language pairs. Temnikova (2010) tested her approach"
L16-1578,temnikova-etal-2012-clcm,1,0.861661,"cifying which errors require more cognitive effort to be corrected, and which less. In this way, the approach only requires counting the number of errors of each type in the MT output. And thus it allows the comparison of the post-editing cost of different output texts of the same MT engine, the same text as an output of different MT engines, or for different language pairs. Temnikova (2010) tested her approach on two emergency instructions texts, one original (called “Complex”) and one manually simplified (called “Simplified”), according to Controlled Language (CL) text simplification rules (Temnikova et al., 2012). Both texts were translated using the web version of Google Translate into three languages: Russian, Spanish, and Bulgarian. The MT output was manually post-edited by 3-5 human translators per language and then the number of errors per category was manually counted by one annotator per language. Several researchers based their work on Temnikova (2010)’s cognitive evaluation approach. Among them, Koponen et al. (2012) have modified the error classification by adding one additional class: “Typographical, upper/lowercase or similar orthographical edits”, and splitting the “Incorrect Word” (InW)"
L16-1578,temnikova-2010-cognitive,1,0.356794,"f different difficulty to be corrected, fair compensation of post-editing should take into account the difficulty of the task, which should thus be measured in the most reliable way. The best solution for this would be to build an automatic classifier which (a) assigns each MT error into a specific correction class, (b) assigns an effort value which reflects the cognitive effort a post-editor needs to make in order to make such a correction, and (c) gives a post-editing effort score to a text. On our way of building such a classifier, we investigate whether an existing cognitive effort model (Temnikova, 2010) could provide a fairer compensation for the post-editor, by testing it on a new language which strongly differs from the previous languages on which this methodology was tested. The model made use of the Statistical Machine Translation (SMT) error classification schema proposed in Vilar et al. (2006), from which the error classes were subsequently regrouped and ranked in an increasing order, so as to reflect the cognitive load post-editors experience while correcting the MT output. Error re-grouping and ranking was done on the basis of relevant psycholinguistic error correction litera1 https:"
L16-1578,vilar-etal-2006-error,0,0.113944,"Missing"
L16-1578,zaghouani-etal-2014-large,1,0.906904,"Missing"
L16-1578,W15-1614,1,0.887222,"Missing"
L16-1578,L16-1295,1,0.826208,"Missing"
L16-1578,2012.amta-wptp.2,0,\N,Missing
L16-1578,W14-3605,1,\N,Missing
L16-1578,W12-3123,0,\N,Missing
L18-1336,E17-3016,1,0.292645,"Missing"
L18-1336,2014.iwslt-papers.13,0,0.0457047,"rences, together with their interpretations done by professional interpreters. After giving a short survey of the related work, we present the WAW Corpus and details about the collection and curation process. Next, we provide a quantitative and qualitative assessment of the collected data. We also present a pilot/case study of the use of the corpus for extracting interpretation strategies used by interpreters. 2. Related Work In (Al-Khanji et al., 2000) interpreting strategies in Arabic have been studied, but no re-usable corpus was released. There are also Arabic speech corpora used for MT2 (Kumar et al., 2014; Zaidan and Callison-Burch, 2014), but they do not include human interpretation of the original speech (only translated speech transcripts are provided). We are not aware of any other publicly available interpreting corpora for Arabic, whereas they exist for Italian, Spanish, English, French, Dutch (Bendazzoli and Sandrelli, 2005; Falbo, 2012), Brazilian Portuguese and German (House et al., 2012), Japanese and Chinese (Tohyama and Matsubara, 2006; Hu and Qing, 2009). Differing from the existing Arabic speech corpora, the WAW corpus contains recordings of the original speakers, the recordings"
L18-1336,temnikova-etal-2017-interpreting,1,0.632503,"y annotating the interpreting strategies in the corpus. The aim of this study was to reveal which strategies interpreters from English into Arabic use, and how often. The hope is that this might eventually provide some indications if our speech-to-text automatic translation system (Dalvi et al., 2017) could benefit from implementing some of these human interpreting strategies. The study was also motivated by our previous observation on the differences in the word ratios between interpretations and translations and the discrepancy in number of named entities tags. This study is a follow-up of (Temnikova et al., 2017), where we conducted a preliminary annotation of the WAW corpus for interpreting strategies by analyzing a small sample of 7500 words (English+Arabic) from the transcripts of 4 sessions, with 2 female interpreters, including W2 (the most productive interpreter in our corpus), and 2 talks for each of these interpreters. For the study reported in the current paper we expanded the sample to 8 sessions, done by 4 interpreters, 2 women (W2, W4) and 2 men (M1, M7), adding up to 16,955 words in English and 9,477 words in Arabic. We selected these specific interpreters based on the transcript ratios a"
L18-1336,tohyama-matsubara-2006-collection,0,0.03329,"i et al., 2000) interpreting strategies in Arabic have been studied, but no re-usable corpus was released. There are also Arabic speech corpora used for MT2 (Kumar et al., 2014; Zaidan and Callison-Burch, 2014), but they do not include human interpretation of the original speech (only translated speech transcripts are provided). We are not aware of any other publicly available interpreting corpora for Arabic, whereas they exist for Italian, Spanish, English, French, Dutch (Bendazzoli and Sandrelli, 2005; Falbo, 2012), Brazilian Portuguese and German (House et al., 2012), Japanese and Chinese (Tohyama and Matsubara, 2006; Hu and Qing, 2009). Differing from the existing Arabic speech corpora, the WAW corpus contains recordings of the original speakers, the recordings of the interpreters, the transcripts of both recordings, and the translations of all transcripts. 3. WAW Corpus The WAW corpus comprises recordings from three international conferences, which took place in Qatar: WISE 2013 (World Innovation Summit for Education)3 , ARC’14 (Qatar Foundation’s Annual Research and Development Conference4 , and WISH 2013 (World Innovation Summit for Health)5 . The speeches and discussions were mostly in English, some"
L18-1336,J14-1006,0,0.0330133,"h their interpretations done by professional interpreters. After giving a short survey of the related work, we present the WAW Corpus and details about the collection and curation process. Next, we provide a quantitative and qualitative assessment of the collected data. We also present a pilot/case study of the use of the corpus for extracting interpretation strategies used by interpreters. 2. Related Work In (Al-Khanji et al., 2000) interpreting strategies in Arabic have been studied, but no re-usable corpus was released. There are also Arabic speech corpora used for MT2 (Kumar et al., 2014; Zaidan and Callison-Burch, 2014), but they do not include human interpretation of the original speech (only translated speech transcripts are provided). We are not aware of any other publicly available interpreting corpora for Arabic, whereas they exist for Italian, Spanish, English, French, Dutch (Bendazzoli and Sandrelli, 2005; Falbo, 2012), Brazilian Portuguese and German (House et al., 2012), Japanese and Chinese (Tohyama and Matsubara, 2006; Hu and Qing, 2009). Differing from the existing Arabic speech corpora, the WAW corpus contains recordings of the original speakers, the recordings of the interpreters, the transcrip"
N06-2051,P03-1051,0,0.00793451,"vel morphological analysis components to improve translation quality above state-of-the-art on a limited-data Arabic to English speech translation task. 1 Lexical relationships under the standard IBM models (Brown et al., 1993) do not account for many-to-many mappings, and phrase extraction relies heavily on the accuracy of the IBM word-toword alignment. In this work, we propose an approach to bridge the inflectional gap that addresses the issues described above through a series of preprocessing steps based on the Buckwalter Arabic Morphological Analyzer (BAMA) tool (Buckwalter, 2004). While (Lee et al., 2003) develop accurate segmentation models of Arabic surface word forms using manually segmented data, we rely instead on the translated context in the target language, leveraging the manually constructed lexical gloss from BAMA to select the appropriate segmented sense for each Arabic source word. Introduction The problem of translating from a language exhibiting rich inflectional morphology to a language exhibiting relatively poor inflectional morphology presents several challenges to the existing components of the statistical machine translation (SMT) process. This inflection gap causes an abund"
N06-2051,N04-4015,0,0.538618,"rst alternative from the BAMA tool. This allows us to still translate an unseen test word correctly even if the surface form was never seen during training. 2.3 CORRMATCH: Correspondence matching The Arabic language often encodes linguistic information within the surface word form that is not present in English. Word fragments that represent this missing information are misleading in the translation process unless explicitly aligned to the NULL word on the target side. In this step we explicitly remove fragments that correspond to lexical information that is not represented in English. While (Lee, 2004) builds part of speech models to recognize such elements, we use the fact that their corresponding English translations in the BAMA lexicon are empty. Examples of such fragments are case and gender markers. As an example of CORRMATCH removal, we present the Arabic sentence ” h‘*A lA ya zAl u gayor naZiyf ” (after BAMA only) which becomes ”h‘*A lA ya zAl gayor naZiyf” after the CORRMATCH stage. The ”u” has been removed. 3 Experimental Framework We evaluate the impact of inflectional splitting on the BTEC (Takezawa et al., 2002) IWSLT05 Arabic language data track. The “Supplied” data track inclu"
N06-2051,C00-2162,0,0.044238,"BTEC IWSLT’05 Arabic-English corpus (Eck and Hori, 2005). 201 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 201–204, c New York, June 2006. 2006 Association for Computational Linguistics 2 Arabic Morphology in Recent Work Arabic-to-English machine translation exemplifies some of the issues caused by the inflection gap. Refer to (Buckwalter, 2005) and (Larkey et al., 2002) for examples that highlight morphological inflection for a simple Modern Standard Arabic (MSA) word and basic stemming operations that we use as our baseline system. (Nießen and Ney, 2000) tackle the inflection gap for German-to-English word alignment by performing a series of morphological operations on the German text. They fragment words based on a full morphological analysis of the sentence, but need to use domain specific and hand written rules to deal with ambiguous fragmentation. (Nießen and Ney, 2004) also extend the corpus by annotating each source word with morphological information and building a hierarchical lexicon. The experimental results show dramatic improvements from sentencelevel restructuring (question inversion, separated verb prefixes and merging phrases),"
N06-2051,J04-2003,0,0.213997,"ively poor inflectional morphology presents several challenges to the existing components of the statistical machine translation (SMT) process. This inflection gap causes an abundance of surface word forms 1 in the source language compared with relatively few forms in the target language. This mismatch aggravates several issues 1 We use the term surface form to refer to a series of characters separated by whitespace Our technique, applied as preprocessing to the source corpus, splits and normalizes surface words based on the target sentence context. In contrast to (Popovic and Ney, 2004) and (Nießen and Ney, 2004), we do not modify the IBM models, and we leave reordering effects to the decoder. Statistically significant improvements (Zhang and Vogel, 2004) in BLEU and NIST translation score over a lightly stemmed baseline are reported on the available and well known BTEC IWSLT’05 Arabic-English corpus (Eck and Hori, 2005). 201 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 201–204, c New York, June 2006. 2006 Association for Computational Linguistics 2 Arabic Morphology in Recent Work Arabic-to-English machine translation exemplifies some of the"
N06-2051,P02-1040,0,0.107662,"omly sampled from the complete supplied dev and test data, due to considerations noted by (Josep M.Crego, 2005) regarding the similarity of the development and test data sets. 3.1 System description Translation experiments were conducted using the (Vogel et al., 2003) system with reordering and future cost estimation. We trained translation parameters for 10 scores (language model, word and phrase count, and 6 translation model scores from (Vogel, 2005) ) with Minimum Error Rate training on the development set. We optimized separately for both the NIST (Doddington, 2002) and the BLEU metrics (Papineni et al., 2002). 4 Translation Results Table 1 and 2 shows the results of each stage of inflectional splitting on the BLEU and NIST metrics. Basic orthographic normalization serves as a baseline (merging all Alif, tar marbuta, ee forms to the base form). The test set NIST scores show steady improvements of up to 5 percent relative, as more sophisticated splitting techniques are used, ie BAMA+CONTEXT+CORRMATCH. These improvements are statistically significant over the baseline in both metrics as measured by the techniques in (Zhang and Vogel, 2004). Our NIST results for all the final stages of inflectional sp"
N06-2051,C04-1045,0,0.0551549,"a language exhibiting relatively poor inflectional morphology presents several challenges to the existing components of the statistical machine translation (SMT) process. This inflection gap causes an abundance of surface word forms 1 in the source language compared with relatively few forms in the target language. This mismatch aggravates several issues 1 We use the term surface form to refer to a series of characters separated by whitespace Our technique, applied as preprocessing to the source corpus, splits and normalizes surface words based on the target sentence context. In contrast to (Popovic and Ney, 2004) and (Nießen and Ney, 2004), we do not modify the IBM models, and we leave reordering effects to the decoder. Statistically significant improvements (Zhang and Vogel, 2004) in BLEU and NIST translation score over a lightly stemmed baseline are reported on the available and well known BTEC IWSLT’05 Arabic-English corpus (Eck and Hori, 2005). 201 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 201–204, c New York, June 2006. 2006 Association for Computational Linguistics 2 Arabic Morphology in Recent Work Arabic-to-English machine translati"
N06-2051,takezawa-etal-2002-toward,0,0.0327093,"Missing"
N06-2051,2003.mtsummit-papers.53,1,0.814142,"English sentence pair training set, as well as a development (“DevSet”) and test (“Test05”) set of 500 Arabic sentences each and 16 reference translations per Arabic sentence. Details regarding the IWSLT evaluation criteria and data topic and collection methods are available in (Eck and Hori, 2005). We also evaluate on test and development data randomly sampled from the complete supplied dev and test data, due to considerations noted by (Josep M.Crego, 2005) regarding the similarity of the development and test data sets. 3.1 System description Translation experiments were conducted using the (Vogel et al., 2003) system with reordering and future cost estimation. We trained translation parameters for 10 scores (language model, word and phrase count, and 6 translation model scores from (Vogel, 2005) ) with Minimum Error Rate training on the development set. We optimized separately for both the NIST (Doddington, 2002) and the BLEU metrics (Papineni et al., 2002). 4 Translation Results Table 1 and 2 shows the results of each stage of inflectional splitting on the BLEU and NIST metrics. Basic orthographic normalization serves as a baseline (merging all Alif, tar marbuta, ee forms to the base form). The te"
N06-2051,2005.mtsummit-papers.33,1,0.742418,"e IWSLT evaluation criteria and data topic and collection methods are available in (Eck and Hori, 2005). We also evaluate on test and development data randomly sampled from the complete supplied dev and test data, due to considerations noted by (Josep M.Crego, 2005) regarding the similarity of the development and test data sets. 3.1 System description Translation experiments were conducted using the (Vogel et al., 2003) system with reordering and future cost estimation. We trained translation parameters for 10 scores (language model, word and phrase count, and 6 translation model scores from (Vogel, 2005) ) with Minimum Error Rate training on the development set. We optimized separately for both the NIST (Doddington, 2002) and the BLEU metrics (Papineni et al., 2002). 4 Translation Results Table 1 and 2 shows the results of each stage of inflectional splitting on the BLEU and NIST metrics. Basic orthographic normalization serves as a baseline (merging all Alif, tar marbuta, ee forms to the base form). The test set NIST scores show steady improvements of up to 5 percent relative, as more sophisticated splitting techniques are used, ie BAMA+CONTEXT+CORRMATCH. These improvements are statistically"
N06-2051,2004.tmi-1.9,1,0.887129,"This inflection gap causes an abundance of surface word forms 1 in the source language compared with relatively few forms in the target language. This mismatch aggravates several issues 1 We use the term surface form to refer to a series of characters separated by whitespace Our technique, applied as preprocessing to the source corpus, splits and normalizes surface words based on the target sentence context. In contrast to (Popovic and Ney, 2004) and (Nießen and Ney, 2004), we do not modify the IBM models, and we leave reordering effects to the decoder. Statistically significant improvements (Zhang and Vogel, 2004) in BLEU and NIST translation score over a lightly stemmed baseline are reported on the available and well known BTEC IWSLT’05 Arabic-English corpus (Eck and Hori, 2005). 201 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 201–204, c New York, June 2006. 2006 Association for Computational Linguistics 2 Arabic Morphology in Recent Work Arabic-to-English machine translation exemplifies some of the issues caused by the inflection gap. Refer to (Buckwalter, 2005) and (Larkey et al., 2002) for examples that highlight morphological inflection f"
N06-2051,2006.iwslt-evaluation.17,0,\N,Missing
N06-2051,J93-2003,0,\N,Missing
N06-2051,2005.iwslt-1.1,0,\N,Missing
N07-1046,2005.iwslt-1.6,1,0.879682,"candidate block, and the features are combined in the log-linear model in Eqn. 11. Given a named-entity pair in the training data, we rank all the transliteration blocks by the scores using the log-linear model. This step is shown in the lower-part in Figure 1. 4.5 Decoding Unseen NEs The decoding of NEs is an extension to the noisy-channel scheme in Eqn. 1. In our configurations for NE transliteration, the extracted transliteration blocks are used. Our letter ngram is a standard letter-ngram model trained using the SriLM toolkit (Stolcke, 2002). To transliterate the unseen NEs, the decoder (Hewavitharana et al., 2005) is configured for monotone decoding. It loads the transliteration blocks and the letter-ngram LM, and it decodes the unseen Arabic named entities with block-based transliteration from left to right. 5 Experiments 5.1 The Data We have 74,887 bilingual geographic names from LDC2005G01-NGA, 11,212 bilingual person names from LDC2005G021 , and about 6,000 bilingual names extracted from the BAMA2 dictionary. In total, there are 92,099 NE pairs. We split them into three parts: 91,459 pairs as the training dataset, 100 pairs as the development dataset, and 540 unique NE pairs as the held-out dataset"
N07-1046,P97-1017,0,0.133066,"ntly more difficult when the language pairs are considerably different, for example, English-Arabic, English-Chinese, and English-Japanese. In this paper, we focus on forward transliteration from Arabic to English. The work in (Arbabi et al., 1994), to our knowledge, is the first work on machine transliteration of Arabic names into English, French, and Spanish. The idea is to vowelize Arabic names by adding appropriate vowels and utilizing a phonetic look-up table to provide the spelling in the target language. Their framework is strictly applicable within standard Arabic morphological rules. Knight and Graehl (1997) introduced finite state transducers that implement back-transliteration from Japanese to English, which was then extended to Arabic-English in (Stalls and Knight, 1998). Al-Onaizan and Knight (2002) transliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and reranking candidates with full-name web counts, named entities co-reference, and contextual web counts. Huang (2005) proposed a specific model for Chinese-English name transliteration with clusterings of names’ origins, and appropriate hypotheses are generated given the origins. All o"
N07-1046,P04-1021,0,0.0509484,"d in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment models (Brown et al., 1993) are used to align letter-pairs within named entity pairs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a framework allowing direct orthographical mapping of transliteration units between English and Chinese, and an extended model is presented in Ekbal et al. (2006). We propose a block-level transliteration framework, as shown in Figure 1, to model letter-level context information for transliteration at two levels. First, we propose a bi-stream HMM incorporating letter-clusters to better model the vowel and non-vowel transliterations with position-information, i.e., initial and final, to improve the letter-level alignment accuracy. Second, based on the letter-alignment, we propose let"
N07-1046,J03-1002,0,0.00916928,"e functions for inferring transliteration blocks from a named entity pair. Besides the above six feature functions, we also compute the average letter-alignment links per block. We count the number of letter-alignment links within the block, and normalize the number by the length of the source letter-ngram. Note that, we can refine the letteralignment by growing the intersections of the two direction letter-alignments from Bi-stream HMM via additional aligned letter-pairs seen in the union of the two. In a way, this approach is similar to those of refining the word-level alignment for SMT in (Och and Ney, 2003). This step is shown in the upper-part in Figure 1. Overall, our proposed feature functions cover relatively different aspects for transliteration blocks: the block level length relevance probability in Eqn. 5, lexical translation equivalence, and positions’ distortion from a gaussian distribution in Eqn. 8, in both directions; and the average number of letter-alignment links within the block. Also, these feature functions are positive and bounded within [0, 1]. Therefore, it is suitable to apply a log-linear model (in §4.3) to combine the weighted individual strengths from the proposed featur"
N07-1046,J04-4002,0,0.071872,"considered to be an acceptable match. 5.2 Comparison of Transliteration Models We compare the performance of three systems within our proposed framework in Figure.1: the baseline Block system, a system in which we use a log-linear combination of alignment features as described in §4.3, we call the the L-Block system, and finally a system, which also uses the bi-stream HMM alignment model as described in §3. This last system will be denoted LCBE system. The baseline is based on the refined letter-alignment from the two directions of IBM-Model-4, trained with a scheme of 15 h5 45 using GIZA++ (Och and Ney, 2004). The final alignment was obtained by growing the intersections between Arabic-to-English (AE) and Englishto-Arabic (EA) alignments with additional aligned letterpairs seen in the union. This is to compensate for the inherent asymmetry in alignment models. Blocks (letterngram pairs) were collected directly from the refined letter-alignment, using the same algorithm as described in §4.3 for extracting gold-standard letter blocks. There is no length restrictions to the letter-ngram extracted in our system. All the blocks were then scored using relative frequencies and lexical scores in both dire"
N07-1046,C02-1099,0,0.037172,"gs of names’ origins, and appropriate hypotheses are generated given the origins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment models (Brown et al., 1993) are used to align letter-pairs within named entity pairs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a framework allowing direct orthographical mapping of transliteration units between English and Chinese, and an extended model is presented in Ekbal et al. (2006). We propose a block-level transliteration framework, as shown in Figure 1, to model letter-level context information for transliteration at two levels. First, we propose a bi-stream HMM incorporating letter-clusters to better model the"
N07-1046,J96-1002,0,0.043228,"Missing"
N07-1046,J93-2003,0,0.0552805,"nsliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and reranking candidates with full-name web counts, named entities co-reference, and contextual web counts. Huang (2005) proposed a specific model for Chinese-English name transliteration with clusterings of names’ origins, and appropriate hypotheses are generated given the origins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment models (Brown et al., 1993) are used to align letter-pairs within named entity pairs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a framework allowing direct orthographical mapping of transliteration units between Englis"
N07-1046,P06-2025,0,0.133391,"airs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a framework allowing direct orthographical mapping of transliteration units between English and Chinese, and an extended model is presented in Ekbal et al. (2006). We propose a block-level transliteration framework, as shown in Figure 1, to model letter-level context information for transliteration at two levels. First, we propose a bi-stream HMM incorporating letter-clusters to better model the vowel and non-vowel transliterations with position-information, i.e., initial and final, to improve the letter-level alignment accuracy. Second, based on the letter-alignment, we propose letter n-gram (lettersequence) alignment models (block) to automatically learn the mappings from source letter n-grams to target letter n-grams. A few features specific for tra"
N07-1046,W02-1039,0,0.0169809,"ard projection: for the target letter-gram erl , search for its left-most fl0 and rightmost fr0 projected positions in the source NE. Now if l0 ≥j and r0 ≤j+n, i.e. flr is contained within the source letter-ngram fjj+n , then this block X = (fjj+n , erl ) is defined as coherent for the aligned pairs: (fjj+n , erl ) . We accept coherent X as gold-standard blocks. This block transliteration coherence is generally sound for extracting the gold-blocks mostly because of the the monotone leftto-right nature of the letter-alignment for transliteration. A related coherence assumption can be found in (Fox, 2002), where their assumption on phrase-pairs for statistical machine translation is shown to be somewhat restrictive for SMT. This is mainly because the word alignment is often non-monotone, especially for langaugepairs from different families such as Arabic-English and Chinese-English. 4.4 Aligning Letter-Blocks: a Local Search Aligning the blocks within NE pairs can be formulated as a local search given the heuristic function defined in Eqn. 11. To be more specific: given a Arabic letter-ngram fjj+l , our algorithm searches for the best translation candidate eii+k in the target named entities. I"
N07-1046,2003.mtsummit-papers.17,0,0.0308796,"ins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment models (Brown et al., 1993) are used to align letter-pairs within named entity pairs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a framework allowing direct orthographical mapping of transliteration units between English and Chinese, and an extended model is presented in Ekbal et al. (2006). We propose a block-level transliteration framework, as shown in Figure 1, to model letter-level context information for transliteration at two levels. First, we propose a bi-stream HMM incorporating letter-clusters to better model the vowel and non-vowel transliterations with position-information, i.e., initial"
N07-1046,P02-1040,0,0.0739352,"-English machine translation evaluation test set. The 663 sentences contain 286 unique words, which were not covered by the available training data. From this set of untranslated words, we manually labeled the entities of persons, locations and organizations, giving a total of 97 unique un-translated NEs. The BAMA toolkit was used to romanize the Arabic words. Some names from this test set are shown in Figure 1. These untranslated NEs make up only a very small fraction of all words in the test set. Therefore, having correct transliterations would give only small improvements in terms of BLEU (Papineni et al., 2002) and NIST scores. However, successfully translating these unknown NEs is very crucial for cross-lingual distillation tasks or question-answering based on the MT-output. (12) 1 In this way, the distribution of Pˆ (f |e) is sharper and more focused in the context of an entity pair. 368 The corpus is provided as FOUO (for official use only) in the DARPA-GALE project 2 LDC2004L02: Buckwalter Arabic Morphological Analyzer version 2.0 Table 1: Test Set Examples. Table 2: Transliteration accuracy for different transliteration models. System Baseline L-Block LCBE To evaluate the transliteration perfor"
N07-1046,W98-1005,0,0.188268,"rward transliteration from Arabic to English. The work in (Arbabi et al., 1994), to our knowledge, is the first work on machine transliteration of Arabic names into English, French, and Spanish. The idea is to vowelize Arabic names by adding appropriate vowels and utilizing a phonetic look-up table to provide the spelling in the target language. Their framework is strictly applicable within standard Arabic morphological rules. Knight and Graehl (1997) introduced finite state transducers that implement back-transliteration from Japanese to English, which was then extended to Arabic-English in (Stalls and Knight, 1998). Al-Onaizan and Knight (2002) transliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and reranking candidates with full-name web counts, named entities co-reference, and contextual web counts. Huang (2005) proposed a specific model for Chinese-English name transliteration with clusterings of names’ origins, and appropriate hypotheses are generated given the origins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003"
N07-1046,W03-1508,0,0.470143,"hen extended to Arabic-English in (Stalls and Knight, 1998). Al-Onaizan and Knight (2002) transliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and reranking candidates with full-name web counts, named entities co-reference, and contextual web counts. Huang (2005) proposed a specific model for Chinese-English name transliteration with clusterings of names’ origins, and appropriate hypotheses are generated given the origins. All of these approaches, however, are not based on a SMT-framework. Technologies developed for SMT are borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). Standard SMT alignment models (Brown et al., 1993) are used to align letter-pairs within named entity pairs for transliteration. Their approach are generative models for letter-to-letter translations, and the letter-alignment is augmented with heuristics. Letter-level contextual information is shown to be very helpful for transliteration. Oh and Choi (2002) used conversion units for English-Korean Transliteration; Goto et al. (2003) used conversion units, mapping English letter-sequence into Japanese Katakana character string. Li et al. (2004) presented a fr"
N07-1046,C96-2141,1,0.784892,"Missing"
N07-1046,I05-3011,1,0.825234,", we define letter n-grams, which consist of n consecutive letters, as the basic transliteration unit. A block is defined as a pair of such letter n-grams which are transliterations of each other. During decoding of unseen NEs, transliteration is performed block-by-block, rather than letter-by-letter. The goal of 366 where fjj+l is the source letter-ngram with (l + 1) letters in source language, and its projection of eii +k in the English NE with left boundary at the position of i, and right boundary at (i + k). We formulate the block extraction as a local search problem following the work in Zhao and Waibel (2005): given a source letter n-gram fjj+l , search for the projected boundaries of candidate target letter n-gram eii +k according to a weighted combination of the diverse features in a log-linear model detailed in §4.3. The log-linear model serves as a performance measure to guide the local search, which, in our setup, is randomized hill-climbing, to extract bilingual letter n-gram transliteration pairs. 4.1 Features for Block Transliteration Three features: fertility, distortion, and lexical translation are investigated for inferring transliteration blocks from the NE pairs. Each feature correspo"
N07-1046,W05-0804,1,0.850231,"in Figure 2: X = (fjj+l , ei+k ), (4) i Figure 2: Block of letters for transliteration. A block is defined by the left- and right- boundaries in the NE-pair. state, respectively. To be in accordance with the monotone nature of the NE’s alignment mentioned before, we enforce the following constraints in Eqn. 3, so that the transition can only jump forward or stay at the same state: aj −aj−1 ≥0 ∀j ∈ [1, J]. (3) Since the two streams are conditionally independent given the current state, the extended EM is straightforward, with only small modifications of the standard forward-backward algorithm (Zhao et al., 2005), for parameter estimation. 3.2 Designing Letter-Classes Pronunciation is typically highly structured. For instance, in English the pronunciation structure of “cvc” (consonant-vowel-consonant) is common. By incorporating letter classes into the proposed two-stream HMM, the models’ expressiveness and robustness can be improved. In this work, we focus on transliteration of Arabic NEs into English. We define six non-overlapping letter classes: vowel, consonant, initial, final, noclass, and unknown. Initial and final classes represent semantic markers at the beginning or end of NEs such as “Al” an"
N07-1046,W02-0505,0,\N,Missing
N07-1063,P02-1040,0,0.1132,"lexible history n-gram LMs to drive the search process, rather than simply using such models for hypothesis rescoring. We begin by discussing the PSCFG model for statistical machine translation, motivating the need 500 Proceedings of NAACL HLT 2007, pages 500–507, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics for effective n-gram LM integration during decoding. We then present our two-pass approach and discuss Cube Pruning as a state-of-the-art baseline. We present results in the form of search error analysis and translation quality as measured by the BLEU score (Papineni et al., 2002) on the IWSLT 06 text translation task (Eck and Hori, 2005)1 , comparing Cube Pruning with our two-pass approach. 2 Synchronous Parsing for SMT Probabilistic Synchronous Context Free Grammar (PSCFG) approaches to statistical machine translation use a source terminal set (source vocabulary) TS , a target terminal set (target vocabulary) TT and a shared nonterminal set N and induce rules of the form X → hγ, α, ∼, wi where (i) X ∈ N is a nonterminal, (ii) γ ∈ (N ∪ TS )∗ is a sequence of nonterminals and source terminals, (iii) α ∈ (N ∪ TT )∗ is a sequence of nonterminals and target terminals, (iv"
N07-1063,P05-1033,0,0.545823,"art item approximations, generating a hypergraph of sentence spanning target language derivations. In the second stage, we instantiate specific alternative derivations from this hypergraph, using the LM to drive this search process, recovering from search errors made in the first pass. Model search errors in our approach are comparable to those made by the state-of-the-art “Cube Pruning” approach in (Chiang, 2007) under comparable pruning conditions evaluated on both hierarchical and syntax-based grammars. 1 Introduction Syntax-driven (Galley et al., 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process. These models extend the domain of locality (over phrase-based models) during decoding, representing a significantly larger search space of possible translation derivations. While PSCFG models are often induced with the goal of producing grammatically correct target translations as an implicit syntaxstructured language model, we acknowledge the value of n-gram language models (LM) in phrasebased approaches. Integrating n-gram LMs into P"
N07-1063,J07-2003,0,0.41789,"ction of an n-gram language model (LM) and a probabilistic synchronous context-free grammar (PSCFG) for statistical machine translation. In first pass CYK-style decoding, we consider first-best chart item approximations, generating a hypergraph of sentence spanning target language derivations. In the second stage, we instantiate specific alternative derivations from this hypergraph, using the LM to drive this search process, recovering from search errors made in the first pass. Model search errors in our approach are comparable to those made by the state-of-the-art “Cube Pruning” approach in (Chiang, 2007) under comparable pruning conditions evaluated on both hierarchical and syntax-based grammars. 1 Introduction Syntax-driven (Galley et al., 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process. These models extend the domain of locality (over phrase-based models) during decoding, representing a significantly larger search space of possible translation derivations. While PSCFG models are often induced with the goal of producing gramma"
N07-1063,P03-1054,0,0.0332212,"mework We present results on the IWSLT 2006 Chinese to English translation task, based on the Full BTEC corpus of travel expressions with 120K parallel sentences (906K source words and 1.2m target words). The evaluation test set contains 500 sentences with an average length of 10.3 source words. Grammar rules were induced with the syntaxbased SMT system “SAMT” described in (Zollmann and Venugopal, 2006), which requires initial phrase alignments that we generated with “GIZA++” (Koehn et al., 2003), and syntactic parse trees of the target training sentences, generated by the Stanford Parser (D. Klein, 2003) pre-trained on the Penn Treebank. All these systems are freely available on the web. We experiment with 2 grammars, one syntaxbased (3688 nonterminals, 0.3m rules), and one purely hierarchical (1 generic nonterminal, 0.05m rules) as in (Chiang, 2005). The large number of nonterminals in the syntax based systems is due to the CCG extension over the original 75 Penn Treebank nonterminals. Parameters λ used to calculate P (D) are trained using MER training (Och, 2003) on development data. 6 Comparison of Approaches We evaluate each approach by considering both search errors made on the developme"
N07-1063,W05-1506,0,0.228904,"ng for each nonterminal X a PSCFG rule S → hX, hsig−1 Xhsig−1 , 1i We are only searching for the derivation of highest probability, so we can discard identical chart items that have lower weight. Since chart items are defined by their left-hand side nonterminal production, span, and the LM contexts e, we can safely discard these identical items since q has retained all context that could possibly impact the LM calculation. This process is commonly referred to as item recombination. Backpointers to antecedent cells are typically retained to allow N -Best extraction using an algorithm such as (Huang and Chiang, 2005). The impact of g-gram LM intersection during decoding is apparent in the final deduction step. Generating the set of consequent Z chart items involves combining m previously produced chart cells. Since each of these chart cells with given source span [i, j] is identified by nonterminal symbol X and LM context e, we have at worst |N |∗ |TT |2(g−1) such chart cells in a span. The runtime of this algorithm is thus  h iK  3 2(g−1) O n |N ||TT | where K is the maximum number of NT pairs per rule and n the source sentence length. Without severe pruning, this runtime is prohibitive for even the 50"
N07-1063,N03-1017,0,0.00882678,"P (selecting e labeled items) vs H.Search (selecting rules since items are not labeled with e). 505 5 Experimental Framework We present results on the IWSLT 2006 Chinese to English translation task, based on the Full BTEC corpus of travel expressions with 120K parallel sentences (906K source words and 1.2m target words). The evaluation test set contains 500 sentences with an average length of 10.3 source words. Grammar rules were induced with the syntaxbased SMT system “SAMT” described in (Zollmann and Venugopal, 2006), which requires initial phrase alignments that we generated with “GIZA++” (Koehn et al., 2003), and syntactic parse trees of the target training sentences, generated by the Stanford Parser (D. Klein, 2003) pre-trained on the Penn Treebank. All these systems are freely available on the web. We experiment with 2 grammars, one syntaxbased (3688 nonterminals, 0.3m rules), and one purely hierarchical (1 generic nonterminal, 0.05m rules) as in (Chiang, 2005). The large number of nonterminals in the syntax based systems is due to the CCG extension over the original 75 Penn Treebank nonterminals. Parameters λ used to calculate P (D) are trained using MER training (Och, 2003) on development dat"
N07-1063,P99-1039,0,0.434798,"terminal occurrences in α, and (vi) w ∈ [0, ∞) is a non-negative real-valued weight assigned to the rule. We will assume ∼ to be implicitly defined by indexing the NT occurrences in γ from left to right starting with 1, and by indexing the NT occurrences in α by the indices of their corresponding counterparts in γ. Syntax-oriented PSCFG approaches typically ignore source structure, instead focussing on generating syntactically well formed target derivations. (Galley et al., 2006) use syntactic constituents for the PSCFG nonterminal set and (Zollmann and Venugopal, 2006) take advantage of CCG (Steedman, 1999) categories, while (Chiang, 2005) uses a single generic nonterminal. PSCFG derivations function analogously to CFG derivations. Given a source sentence f , the translation task under a PSCFG grammar can be expressed as 1 While IWSLT represents a limited resource translation task (120K sentences of training data for Chinese-English), the problem of efficient n-gram LM integration is still critically important to efficient decoding, and our contributions can be expected to have an even more significant impact when decoding with grammars induced from larger corpora. 501 eˆ = arg max P (D) {e |∃D."
N07-1063,P06-1098,0,0.070274,"|N ||TT | where K is the maximum number of NT pairs per rule and n the source sentence length. Without severe pruning, this runtime is prohibitive for even the 502 smallest induced grammars. Traditional pruning approaches that limit the number of consequents after they are produced are not effective since they first require that the cost of each consequent be computed (which requires calls to the g-gram LM). Restrictions to the grammar afford alternative decoding strategies to reduce the runtime cost of synchronous parsing. (Zhang et al., 2006) “binarize” grammars into CNF normal form, while (Watanabe et al., 2006) allow only Griebach-Normal form grammars. (Wellington et al., 2006) argue that these restrictions reduce our ability to model translation equivalence effectively. We take an agnostic view on the issue; directly addressing the question of efficient LM intersection rather than grammar construction. 3 Two-pass LM Intersection We propose a two-pass solution to the problem of online g-gram LM intersection. A naive two-pass approach would simply ignore the LM interactions during parsing, extract a set of N derivations from the sentence spanning hypergraph and rescore these derivations with the g-gr"
N07-1063,P06-1123,0,0.00488817,"the source sentence length. Without severe pruning, this runtime is prohibitive for even the 502 smallest induced grammars. Traditional pruning approaches that limit the number of consequents after they are produced are not effective since they first require that the cost of each consequent be computed (which requires calls to the g-gram LM). Restrictions to the grammar afford alternative decoding strategies to reduce the runtime cost of synchronous parsing. (Zhang et al., 2006) “binarize” grammars into CNF normal form, while (Watanabe et al., 2006) allow only Griebach-Normal form grammars. (Wellington et al., 2006) argue that these restrictions reduce our ability to model translation equivalence effectively. We take an agnostic view on the issue; directly addressing the question of efficient LM intersection rather than grammar construction. 3 Two-pass LM Intersection We propose a two-pass solution to the problem of online g-gram LM intersection. A naive two-pass approach would simply ignore the LM interactions during parsing, extract a set of N derivations from the sentence spanning hypergraph and rescore these derivations with the g-gram LM. In practice, this approach performs poorly (Chiang, 2007; Zol"
N07-1063,P96-1021,0,0.207138,"g to the rules r used in D, is: YY 1 φi (r)λi P (D) = PLM (tgt(D))λLM × Z i r∈D where φi refers to features defined on each rule, and PLM is a g-gram LM probability applied to the target terminal symbols generated by the derivation D. Introducing the LM feature defines dependencies across adjacent rules used in each derivation, and requires modifications to the decoding strategy. Viewing the LM as a finite-state machine, the decoding process involves performing an intersection between the PSCFG grammar and the g-gram LM (Bar-Hillel et al., 1964). We present our work under the construction in (Wu, 1996), following notation from (Chiang, 2007), extending the formal description to reflect grammars with an arbitrary number of nonterminals in each rule. 2.1 Decoding Strategies In Figure 1, we reproduce the decoding algorithm from (Chiang, 2007) that applies a PSCFG to translate a source sentence in the same notation (as a deductive proof system (Shieber et al., 1995)), generalized to handle more than two non-terminal pairs. Chart items [X, i, j, e] : w span j − i words in the source sentence f1 · · · fn , starting at position i + 1, and have weight w (equivalent to P (D)), and e ∈ (TT ∪ {?})∗ is"
N07-1063,N06-1033,0,0.038709,"a span. The runtime of this algorithm is thus  h iK  3 2(g−1) O n |N ||TT | where K is the maximum number of NT pairs per rule and n the source sentence length. Without severe pruning, this runtime is prohibitive for even the 502 smallest induced grammars. Traditional pruning approaches that limit the number of consequents after they are produced are not effective since they first require that the cost of each consequent be computed (which requires calls to the g-gram LM). Restrictions to the grammar afford alternative decoding strategies to reduce the runtime cost of synchronous parsing. (Zhang et al., 2006) “binarize” grammars into CNF normal form, while (Watanabe et al., 2006) allow only Griebach-Normal form grammars. (Wellington et al., 2006) argue that these restrictions reduce our ability to model translation equivalence effectively. We take an agnostic view on the issue; directly addressing the question of efficient LM intersection rather than grammar construction. 3 Two-pass LM Intersection We propose a two-pass solution to the problem of online g-gram LM intersection. A naive two-pass approach would simply ignore the LM interactions during parsing, extract a set of N derivations from the"
N07-1063,W06-3119,1,0.87992,"model, we acknowledge the value of n-gram language models (LM) in phrasebased approaches. Integrating n-gram LMs into PSCFGs based decoding can be viewed as online intersection of the PSCFG grammar with the finite state machine represented by the n-gram LM, dramatically increasing the effective number of nonterminals in the decoding grammar, rendering the decoding process essentially infeasible without severe, beam-based lossy pruning. The alternative, simply decoding without the n-gram LM and rescoring N-best alternative translations, results in substantially more search errors, as shown in (Zollmann and Venugopal, 2006). Our two-pass approach involves fast, approximate synchronous parsing in a first stage, followed by a second, detailed exploration through the resulting hypergraph of sentence spanning derivations, using the n-gram LM to drive that search. This achieves search errors comparable to a strong “Cube Pruning” (Chiang, 2007), single-pass baseline. The first pass corresponds to a severe parameterization of Cube Pruning considering only the first-best (LM integrated) chart item in each cell while maintaining unexplored alternatives for second-pass consideration. Our second stage allows the integratio"
N07-1063,2005.iwslt-1.1,0,\N,Missing
N07-1063,P03-1021,0,\N,Missing
N07-2006,koen-2004-pharaoh,0,0.0772576,"adation as much as possible. We will not specifically address the computing power limitations of the portable devices in this paper. 1 A “phrase” here can also refer to a single word. Small language models are also desirable and the approaches could be applied as well but this was not investigated yet. 2 21 Proceedings of NAACL HLT 2007, Companion Volume, pages 21–24, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics 2 Previous work Previous work mainly introduced two natural ideas to prune phrase pairs. Both are for example directly available in the Pharaoh decoder (Koehn, 2004). Probability threshold A very simple way to prune phrase pairs from a translation model is to use a probability threshold and remove all pairs for which the translation probability is below the threshold. The reasoning for this is that it is very unlikely that a translation with a very low probability will be chosen (over another translation candidate with a higher probability). Translation variety threshold Another way to prune phrase pairs is to impose a limit on the number of translation candidates for a certain phrase. That means the pruned translation model can only have equal or fewer p"
N07-2006,P00-1056,0,0.130907,"Generally statistical machine translation systems have recently outperformed other translation approaches so it seems natural to also apply them in these scenarios. A main component of every statistical machine translation system is the translation model. The translation model assigns translation probabilities to phrase1 pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied during the decoding process and their target sides are combined to form the final translation. A variety of algorithms to extract phrase pairs has been proposed. (e.g. Och and Ney, 2000 and Vogel, 2005). Our proposed approach now tries to remove phrase pairs, which have little influence on the final translation performance, from a translation system (pruning of the translation model2). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the translation performance too heavily. The approach does not depend on the actual algorithm used to extract the phrase pairs and can be applied to every imaginable method that assigns probabilities to phrase pairs. We assume that the phrase pairs were pre-e"
N07-2006,P02-1040,0,0.0713786,"Missing"
N07-2006,takezawa-etal-2002-toward,0,0.0273506,"Missing"
N07-2006,2005.mtsummit-papers.33,1,0.877773,"machine translation systems have recently outperformed other translation approaches so it seems natural to also apply them in these scenarios. A main component of every statistical machine translation system is the translation model. The translation model assigns translation probabilities to phrase1 pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied during the decoding process and their target sides are combined to form the final translation. A variety of algorithms to extract phrase pairs has been proposed. (e.g. Och and Ney, 2000 and Vogel, 2005). Our proposed approach now tries to remove phrase pairs, which have little influence on the final translation performance, from a translation system (pruning of the translation model2). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the translation performance too heavily. The approach does not depend on the actual algorithm used to extract the phrase pairs and can be applied to every imaginable method that assigns probabilities to phrase pairs. We assume that the phrase pairs were pre-extracted before d"
N07-2006,2005.eamt-1.39,1,0.873456,"uence on the final translation performance, from a translation system (pruning of the translation model2). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the translation performance too heavily. The approach does not depend on the actual algorithm used to extract the phrase pairs and can be applied to every imaginable method that assigns probabilities to phrase pairs. We assume that the phrase pairs were pre-extracted before decoding. (in contrast to the proposed approaches to “online phrase extraction” (Zhang and Vogel, 2005; Callison-Burch et al., 2005)). The task now is to remove enough pre-extracted phrase pairs in order to accommodate the possibly strict memory limitations of a portable device while restricting performance degradation as much as possible. We will not specifically address the computing power limitations of the portable devices in this paper. 1 A “phrase” here can also refer to a single word. Small language models are also desirable and the approaches could be applied as well but this was not investigated yet. 2 21 Proceedings of NAACL HLT 2007, Companion Volume, pages 21–24, c Rochester, NY, A"
N07-2006,2004.iwslt-evaluation.1,0,\N,Missing
N07-2006,P05-1032,0,\N,Missing
N07-2006,2005.iwslt-1.6,1,\N,Missing
N09-1027,P05-1033,0,0.717992,"le labeled derivation, reducing the fragmentation problem. Solving this problem exactly is still an NP-hard consensus problem, but we provide approximations that build on well-known PSCFG decoding methods. Our model falls somewhere between PSCFGs that extract nonterminal symbols from parse trees and treat them as part of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 236–244, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics the derivation (Zollmann and Venugopal, 2006) and unlabeled hierarchical structures (Chiang, 2005); we treat nonterminal labels as random variables chosen at each node, with each (unlabeled) rule expressing “preferences” for particular nonterminal labels, learned from data. The paper is organized as follows. In Section 2, we summarize the use of PSCFG grammars for translation. We describe our model (Section 3). Section 4 explains the preference-related calculations, and Section 5 addresses decoding. Experimental results using preference grammars in a loglinear translation model are presented for two standard Chinese-to-English tasks in Section 6. We review related work (Section 7) and conc"
N09-1027,P07-1019,0,0.0212625,"h0 = VPi |r)u(VP) = (0.4 × 0.8) + (0.3 × 0.1) = 0.35 v˜(SBAR) = p(hh = SBAR, h0 = VPi |r)u(VP) = (0.2 × 0.1) = 0.02 v = hv(S) = 0.35/(˜ v (S) + v˜(SBAR)), v(SBAR) = 0.02/˜ v (S) + v˜(SBAR)i = hv(S) = 0.35/0.37, v(SBAR) = 0.02/0.37i φ2 = u(VB) + u(VP) = 0.8 + 0.1 = 0.9 Figure 1: Calculating v and φ2 for the running example. search space further. To prevent this partitioning, we follow the approach of Venugopal et al. (2007). We keep track of u for the best performing derivation from the set of derivations that share [X, i, j, q(α)] in a first-pass decoding. In a second top-down pass similar to Huang and Chiang (2007), we can recalculate psyn (d) for alternative derivations in the hypergraph; potentially correcting search errors made in the first pass. We face another significant practical challenge during decoding. In real data conditions, the size of the preference vector for a single rule can be very high, especially for rules that include multiple nonterminal symbols that are located on the left and right boundaries of γ. For example, the Chineseto-English rule X → h X1 „ X2 # X1 ’s X2 i has over 24K elements in hargs(r) when learned for the medium-sized NIST task used below. In order to limit the expl"
N09-1027,J99-4005,0,0.0433364,"ules. A PSCFG derivation is a synchronous parse tree. Defining the translation function as finding the best derivation has the unfortunate side effect of forcing differently-derived versions of the same target sentence to compete with each other. In other words, the true score of each translation is “fragmented” across many derivations, so that each translation’s most probable derivation is the only one that matters. The more Bayesian approach of finding the most probable translation (integrating out the derivations) instantiates an NP-hard inference problem even for simple word-based models (Knight, 1999); for grammar-based translation it is known as the consensus problem (Casacuberta and de la Higuera, 2000; Sima’an, 2002). With weights interpreted as probabilities, the maximum-weighted derivation is the maximum a posteriori (MAP) derivation: We propose a novel probabilistic synchoronous context-free grammar formalism for statistical machine translation, in which syntactic nonterminal labels are represented as “soft” preferences rather than as “hard” matching constraints. This formalism allows us to efficiently score unlabeled synchronous derivations without forgoing traditional syntactic con"
N09-1027,N04-1022,0,0.0238305,"ating the selection of the most likely unlabeled derivation during search, rather than as a post-processing operation; the methods described above might improve this approximation, at some computational expense. Related Work There have been significant efforts in the both the monolingual parsing and machine translation literature to address the impact of the MAP approximation and the choice of labels in their respective models; we survey the work most closely related to our approach. May and Knight (2006) extract nbest lists containing unique translations rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search. Matsusaki et al. (2005) and Petrov et al. (2006) propose automatically learning annotations"
N09-1027,P05-1010,0,0.0447155,"ith the arguments of this higherup rule. We design psyn (d) to reflect compatibility between two rules (one expanding a right-hand side nonterminal in the other), based on label preference distributions. 3.2 Formal definition Probabilistic synchronous context-free preference grammars are defined as PSCFGs with the following additional elements: • H: a set of implicit labels, not to be confused 239 with the explicit label set N . • π: H → N , a function that associates each implicit label with a single explicit label. We can therefore think of H symbols as refinements of the nonterminals in N (Matsusaki et al., 2005). • For each rule r, we define a probability distribution over vectors ~h of implicit label bindings for its nonterminals, denoted ppref (~h |r). ~h includes bindings for the left-hand side nonterminal (h0 ) as well as each right-hand side nonterminal (h1 , ..., h|~h |). Each hi ∈ H. When N , H are defined to include just a single generic symbol as in (Chiang, 2005), we produce the unlabeled grammar discussed above. In this work, we define • N = {S, X} • H = {NP, DT, NN · · · } = NSAMT where N corresponds to the generic labels of Chiang (2005) and H corresponds to the syntactically motivated S"
N09-1027,N06-1045,0,0.026388,"tree rather than simply stripping annotations from the MAP annotated tree. In our work, we focused on approximating the selection of the most likely unlabeled derivation during search, rather than as a post-processing operation; the methods described above might improve this approximation, at some computational expense. Related Work There have been significant efforts in the both the monolingual parsing and machine translation literature to address the impact of the MAP approximation and the choice of labels in their respective models; we survey the work most closely related to our approach. May and Knight (2006) extract nbest lists containing unique translations rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translati"
N09-1027,P03-1021,0,0.0280794,"(redundantly) requires every nonterminal token to be expanded by a rule with that nonterminal on its left-hand side. freq(r; d) denotes the frequency of the rule r in the derivation d. Note that λm+1 can be effectively ignored when psyn is defined as in Equation 3. Z(~λ) is a normalization constant that does not need to be computed during search under the argmax search criterion in Equation 1. Feature weights ~λ are trained discriminatively in concert with the language model weight to maximize the BLEU (Papineni et al., 2002) automatic evaluation metric via Minimum Error Rate Training (MERT) (Och, 2003). We use the open-source PSCFG rule extraction framework and decoder from Zollmann et al. (2008) as the framework for our experiments. The asymptotic runtime of this decoder is:  h iK  3 2(n−1) O |f ||N ||TT | (4) where K is the maximum number of nonterminal symbols per rule, |f |the source sentence length, and n is the order of the n-gram LM that is used to compute pLM . This constant factor in Equation 4 arises from the dynamic programming item structure used to perform search under this model. Using notation from Chiang (2007), the corresponding item structure is: [X, i, j, q(α)] : w (5)"
N09-1027,P02-1040,0,0.105687,"uence, a collection of m rule feature functions hi : R → R≥0 , and a “syntax” feature that (redundantly) requires every nonterminal token to be expanded by a rule with that nonterminal on its left-hand side. freq(r; d) denotes the frequency of the rule r in the derivation d. Note that λm+1 can be effectively ignored when psyn is defined as in Equation 3. Z(~λ) is a normalization constant that does not need to be computed during search under the argmax search criterion in Equation 1. Feature weights ~λ are trained discriminatively in concert with the language model weight to maximize the BLEU (Papineni et al., 2002) automatic evaluation metric via Minimum Error Rate Training (MERT) (Och, 2003). We use the open-source PSCFG rule extraction framework and decoder from Zollmann et al. (2008) as the framework for our experiments. The asymptotic runtime of this decoder is:  h iK  3 2(n−1) O |f ||N ||TT | (4) where K is the maximum number of nonterminal symbols per rule, |f |the source sentence length, and n is the order of the n-gram LM that is used to compute pLM . This constant factor in Equation 4 arises from the dynamic programming item structure used to perform search under this model. Using notation fr"
N09-1027,P06-1055,0,0.0156806,"ons rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search. Matsusaki et al. (2005) and Petrov et al. (2006) propose automatically learning annotations that add information to categories to improve monolingual parsing quality. Since the parsing task requires selecting the most non-annotated tree, the an243 Conclusions and Future Work We have proposed a novel grammar formalism that replaces hard syntactic constraints with “soft” preferences. These preferences are used to compute a machine translation feature (psyn (d)) that scores unlabeled derivations, taking into account traditional syntactic constraints. Representing syntactic constraints as a feature allows MERT to train the corresponding weight"
N09-1027,D08-1065,0,0.0118051,"pproximation, at some computational expense. Related Work There have been significant efforts in the both the monolingual parsing and machine translation literature to address the impact of the MAP approximation and the choice of labels in their respective models; we survey the work most closely related to our approach. May and Knight (2006) extract nbest lists containing unique translations rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search. Matsusaki et al. (2005) and Petrov et al. (2006) propose automatically learning annotations that add information to categories to improve monolingual parsing quality. Since the parsing task requires selecting the most non-annotated tree, the an243 Conclusion"
N09-1027,N07-1063,1,0.86028,"over the longer sequences in hargs(r) and include appropriate values from the additional “child” items’ preference vectors in the product. v˜(S) = ppref (hh = S, h0 = VBi |r)u(VB) + ppref (hh = S, h0 = VPi |r)u(VP) = (0.4 × 0.8) + (0.3 × 0.1) = 0.35 v˜(SBAR) = p(hh = SBAR, h0 = VPi |r)u(VP) = (0.2 × 0.1) = 0.02 v = hv(S) = 0.35/(˜ v (S) + v˜(SBAR)), v(SBAR) = 0.02/˜ v (S) + v˜(SBAR)i = hv(S) = 0.35/0.37, v(SBAR) = 0.02/0.37i φ2 = u(VB) + u(VP) = 0.8 + 0.1 = 0.9 Figure 1: Calculating v and φ2 for the running example. search space further. To prevent this partitioning, we follow the approach of Venugopal et al. (2007). We keep track of u for the best performing derivation from the set of derivations that share [X, i, j, q(α)] in a first-pass decoding. In a second top-down pass similar to Huang and Chiang (2007), we can recalculate psyn (d) for alternative derivations in the hypergraph; potentially correcting search errors made in the first pass. We face another significant practical challenge during decoding. In real data conditions, the size of the preference vector for a single rule can be very high, especially for rules that include multiple nonterminal symbols that are located on the left and right bou"
N09-1027,W06-3119,1,0.416619,"obable equivalence class of unlabeled derivations, rather than a single labeled derivation, reducing the fragmentation problem. Solving this problem exactly is still an NP-hard consensus problem, but we provide approximations that build on well-known PSCFG decoding methods. Our model falls somewhere between PSCFGs that extract nonterminal symbols from parse trees and treat them as part of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 236–244, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics the derivation (Zollmann and Venugopal, 2006) and unlabeled hierarchical structures (Chiang, 2005); we treat nonterminal labels as random variables chosen at each node, with each (unlabeled) rule expressing “preferences” for particular nonterminal labels, learned from data. The paper is organized as follows. In Section 2, we summarize the use of PSCFG grammars for translation. We describe our model (Section 3). Section 4 explains the preference-related calculations, and Section 5 addresses decoding. Experimental results using preference grammars in a loglinear translation model are presented for two standard Chinese-to-English tasks in S"
N09-1027,C08-1144,1,0.729917,"nterminal on its left-hand side. freq(r; d) denotes the frequency of the rule r in the derivation d. Note that λm+1 can be effectively ignored when psyn is defined as in Equation 3. Z(~λ) is a normalization constant that does not need to be computed during search under the argmax search criterion in Equation 1. Feature weights ~λ are trained discriminatively in concert with the language model weight to maximize the BLEU (Papineni et al., 2002) automatic evaluation metric via Minimum Error Rate Training (MERT) (Och, 2003). We use the open-source PSCFG rule extraction framework and decoder from Zollmann et al. (2008) as the framework for our experiments. The asymptotic runtime of this decoder is:  h iK  3 2(n−1) O |f ||N ||TT | (4) where K is the maximum number of nonterminal symbols per rule, |f |the source sentence length, and n is the order of the n-gram LM that is used to compute pLM . This constant factor in Equation 4 arises from the dynamic programming item structure used to perform search under this model. Using notation from Chiang (2007), the corresponding item structure is: [X, i, j, q(α)] : w (5) (4) (3) (2) (1) where X is the nonterminal label of a derivation, i, j define a span in the sour"
N09-1027,P06-1121,0,\N,Missing
N09-1027,2006.iwslt-evaluation.1,0,\N,Missing
N09-1027,J07-2003,0,\N,Missing
N09-1027,P08-1024,0,\N,Missing
N09-2001,P08-1009,1,0.824989,"the source phrase to be translated does not immediately follow the previously translated phrase. This is penalized with a discriminatively-trained distortion penalty. In order to calculate the current translation score, each state can be represented by a triple: • A coverage vector HC indicates which source words have already been translated. 1 As cohesion concerns only movement in the source, we can completely ignore the language model context, making state effectively an (f¯, HC ) tuple. To enforce cohesion during the state expansion process, cohesive phrasal decoding has been proposed in (Cherry, 2008; Yamamoto et al., 2008). The cohesionenhanced decoder enforces the following constraint: once the decoder begins translating any part of a source subtree, it must cover all the words under that subtree before it can translate anything outside of it. This notion can be applied to any projective tree structure, but we use dependency trees, which have been shown to demonstrate greater cross-lingual cohesion than other structures (Fox, 2002). We use a tree data structure to store the dependency tree. Each node in the tree contains surface word form, word position, parent position, dependency type"
N09-2001,W02-1039,0,0.180787,"ntext, making state effectively an (f¯, HC ) tuple. To enforce cohesion during the state expansion process, cohesive phrasal decoding has been proposed in (Cherry, 2008; Yamamoto et al., 2008). The cohesionenhanced decoder enforces the following constraint: once the decoder begins translating any part of a source subtree, it must cover all the words under that subtree before it can translate anything outside of it. This notion can be applied to any projective tree structure, but we use dependency trees, which have been shown to demonstrate greater cross-lingual cohesion than other structures (Fox, 2002). We use a tree data structure to store the dependency tree. Each node in the tree contains surface word form, word position, parent position, dependency type and POS tag. We use T to stand for our dependency tree, and T (n) to stand for the subtree rooted at node n. Each subtree T (n) covers a span of contiguous source words; for subspan f¯ covered by T (n), we say f¯ ∈ T (n). Cohesion is checked as we extend a state (f¯h , HC h ) with the translation of f¯h+1 , creating a new state (f¯h+1 , HC h+1 ). Algorithm 1 presents the cohesion check described by Cherry (2008). Line 2 selects focal poi"
N09-2001,P07-2045,0,0.00803084,"have not been covered. For example, we want to translate the English sentence “the presidential election of the united states begins tomorrow” to French with the dependency structure as in Figure 1. We consider f¯h = “the united states”, f¯h+1 = “begins”. The coverage bit vector HC h+1 is “0 0 0 0 1 1 1 1 0”. Algorithm 5 will return true for Interruption, 4 for ICount (“the”; “presidential”; “election”; “of”), 0 for V erbCount and 1 for N ounCount (“election”). 3 Experiments We built baseline systems using GIZA++ (Och and Ney, 2003), Moses’ phrase extraction with grow-diag-finalend heuristic (Koehn et al., 2007), a standard phrasebased decoder (Vogel, 2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a distance-based word reordering model 3 Algorithm 5 Rich Interruption Constraints (Coh5) Input: Source tree T , previous phrase f¯h , current phrase f¯h+1 , coverage vector HC 1: Interruption ← F alse 2: ICount, V erbCount, N ounCount ← 0 3: F ← the left and right-most tokens of f¯h 4: for each of f ∈ F do Climb the dependency tree from f until you reach 5: the highest node n such that f¯h+1 ∈ / T (n). 6: if n exists then 7: for each of e ∈ T (n) and HCh"
N09-2001,de-marneffe-etal-2006-generating,0,0.012338,"+ 1 12: else if POS of e is “NN” then 13: N ounCount ← N ounCount + 1 14: end if 15: end for 16: end if 17: end for 18: Return Interruption, ICount, V erbCount, N ounCount with a window of 3, and the maximum number of target phrases restricted to 10. Results are reported using lowercase BLEU (Papineni et al., 2002). All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. We used the MALT parser (Nivre et al., 2006) to obtain source English dependency trees and the Stanford parser for Arabic (Marneffe et al., 2006). In order to decide whether the translation output of one MT engine is significantly better than another one, we used the bootstrap method (Zhang et al., 2004) with 1000 samples (p &lt; 0.05). We perform experiments on English→Iraqi and English→Spanish. Detailed corpus statistics are shown in Table 1. Table 2 shows results in lowercase BLEU and bold type is used to indicate highest scores. An italic text indicates the score is statistically significant better than the baseline. sentence pairs unique sent. pairs avg. sentence length # words vocabulary English→Iraqi English Iraqi 654,556 510,314 8"
N09-2001,nivre-etal-2006-maltparser,0,0.0395762,"erruption ← T rue ICount = ICount + 1 9: 10: if POS of e is “VB” then 11: V erbCount ← V erbCount + 1 12: else if POS of e is “NN” then 13: N ounCount ← N ounCount + 1 14: end if 15: end for 16: end if 17: end for 18: Return Interruption, ICount, V erbCount, N ounCount with a window of 3, and the maximum number of target phrases restricted to 10. Results are reported using lowercase BLEU (Papineni et al., 2002). All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. We used the MALT parser (Nivre et al., 2006) to obtain source English dependency trees and the Stanford parser for Arabic (Marneffe et al., 2006). In order to decide whether the translation output of one MT engine is significantly better than another one, we used the bootstrap method (Zhang et al., 2004) with 1000 samples (p &lt; 0.05). We perform experiments on English→Iraqi and English→Spanish. Detailed corpus statistics are shown in Table 1. Table 2 shows results in lowercase BLEU and bold type is used to indicate highest scores. An italic text indicates the score is statistically significant better than the baseline. sentence pairs uni"
N09-2001,J03-1002,0,0.00592023,"ation will be penalized more in terms of the number of verb and noun words that have not been covered. For example, we want to translate the English sentence “the presidential election of the united states begins tomorrow” to French with the dependency structure as in Figure 1. We consider f¯h = “the united states”, f¯h+1 = “begins”. The coverage bit vector HC h+1 is “0 0 0 0 1 1 1 1 0”. Algorithm 5 will return true for Interruption, 4 for ICount (“the”; “presidential”; “election”; “of”), 0 for V erbCount and 1 for N ounCount (“election”). 3 Experiments We built baseline systems using GIZA++ (Och and Ney, 2003), Moses’ phrase extraction with grow-diag-finalend heuristic (Koehn et al., 2007), a standard phrasebased decoder (Vogel, 2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a distance-based word reordering model 3 Algorithm 5 Rich Interruption Constraints (Coh5) Input: Source tree T , previous phrase f¯h , current phrase f¯h+1 , coverage vector HC 1: Interruption ← F alse 2: ICount, V erbCount, N ounCount ← 0 3: F ← the left and right-most tokens of f¯h 4: for each of f ∈ F do Climb the dependency tree from f until you reach 5: the highest node"
N09-2001,P03-1021,0,0.0622502,"res context needed by the target language model. Introduction Phrase-based machine translation is driven by a phrasal translation model, which relates phrases (contiguous segments of words) in the source to phrases in the target. This translation model can be derived from a wordaligned bitext. Translation candidates are scored according to a linear model combining several informative feature functions. Crucially, this model incorporates translation model scores and n-gram language model scores. The component features are weighted to minimize a translation error criterion on a development set (Och, 2003). Decoding the source sentence takes the form of a beam search through the translation space, with intermediate states corresponding to partial translations. The decoding process advances by extending a state with the translation of a source phrase, until each source word has been translated exactly once. Re-ordering occurs when the source phrase to be translated does not immediately follow the previously translated phrase. This is penalized with a discriminatively-trained distortion penalty. In order to calculate the current translation score, each state can be represented by a triple: • A co"
N09-2001,P02-1040,0,0.0778749,"tokens of f¯h 4: for each of f ∈ F do Climb the dependency tree from f until you reach 5: the highest node n such that f¯h+1 ∈ / T (n). 6: if n exists then 7: for each of e ∈ T (n) and HCh+1 (e) = 0 do 8: Interruption ← T rue ICount = ICount + 1 9: 10: if POS of e is “VB” then 11: V erbCount ← V erbCount + 1 12: else if POS of e is “NN” then 13: N ounCount ← N ounCount + 1 14: end if 15: end for 16: end if 17: end for 18: Return Interruption, ICount, V erbCount, N ounCount with a window of 3, and the maximum number of target phrases restricted to 10. Results are reported using lowercase BLEU (Papineni et al., 2002). All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. We used the MALT parser (Nivre et al., 2006) to obtain source English dependency trees and the Stanford parser for Arabic (Marneffe et al., 2006). In order to decide whether the translation output of one MT engine is significantly better than another one, we used the bootstrap method (Zhang et al., 2004) with 1000 samples (p &lt; 0.05). We perform experiments on English→Iraqi and English→Spanish. Detailed corpus statistics are shown in Ta"
N09-2001,W08-0401,0,0.104581,"rase to be translated does not immediately follow the previously translated phrase. This is penalized with a discriminatively-trained distortion penalty. In order to calculate the current translation score, each state can be represented by a triple: • A coverage vector HC indicates which source words have already been translated. 1 As cohesion concerns only movement in the source, we can completely ignore the language model context, making state effectively an (f¯, HC ) tuple. To enforce cohesion during the state expansion process, cohesive phrasal decoding has been proposed in (Cherry, 2008; Yamamoto et al., 2008). The cohesionenhanced decoder enforces the following constraint: once the decoder begins translating any part of a source subtree, it must cover all the words under that subtree before it can translate anything outside of it. This notion can be applied to any projective tree structure, but we use dependency trees, which have been shown to demonstrate greater cross-lingual cohesion than other structures (Fox, 2002). We use a tree data structure to store the dependency tree. Each node in the tree contains surface word form, word position, parent position, dependency type and POS tag. We use T t"
N09-2001,2005.eamt-1.39,1,0.877337,"orrow” to French with the dependency structure as in Figure 1. We consider f¯h = “the united states”, f¯h+1 = “begins”. The coverage bit vector HC h+1 is “0 0 0 0 1 1 1 1 0”. Algorithm 5 will return true for Interruption, 4 for ICount (“the”; “presidential”; “election”; “of”), 0 for V erbCount and 1 for N ounCount (“election”). 3 Experiments We built baseline systems using GIZA++ (Och and Ney, 2003), Moses’ phrase extraction with grow-diag-finalend heuristic (Koehn et al., 2007), a standard phrasebased decoder (Vogel, 2003), the SRI LM toolkit (Stolcke, 2002), the suffix-array language model (Zhang and Vogel, 2005), a distance-based word reordering model 3 Algorithm 5 Rich Interruption Constraints (Coh5) Input: Source tree T , previous phrase f¯h , current phrase f¯h+1 , coverage vector HC 1: Interruption ← F alse 2: ICount, V erbCount, N ounCount ← 0 3: F ← the left and right-most tokens of f¯h 4: for each of f ∈ F do Climb the dependency tree from f until you reach 5: the highest node n such that f¯h+1 ∈ / T (n). 6: if n exists then 7: for each of e ∈ T (n) and HCh+1 (e) = 0 do 8: Interruption ← T rue ICount = ICount + 1 9: 10: if POS of e is “VB” then 11: V erbCount ← V erbCount + 1 12: else if POS o"
N09-2001,zhang-etal-2004-interpreting,1,0.825322,"ounCount with a window of 3, and the maximum number of target phrases restricted to 10. Results are reported using lowercase BLEU (Papineni et al., 2002). All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. We used the MALT parser (Nivre et al., 2006) to obtain source English dependency trees and the Stanford parser for Arabic (Marneffe et al., 2006). In order to decide whether the translation output of one MT engine is significantly better than another one, we used the bootstrap method (Zhang et al., 2004) with 1000 samples (p &lt; 0.05). We perform experiments on English→Iraqi and English→Spanish. Detailed corpus statistics are shown in Table 1. Table 2 shows results in lowercase BLEU and bold type is used to indicate highest scores. An italic text indicates the score is statistically significant better than the baseline. sentence pairs unique sent. pairs avg. sentence length # words vocabulary English→Iraqi English Iraqi 654,556 510,314 8.4 5.9 5.5 M 3.8 M 34 K 109 K English→Spanish English Spanish 1,310,127 1,287,016 27.4 28.6 35.8 M 37.4 M 117 K 173 K Table 1: Corpus statistics Our English-Ira"
N09-2001,N04-1035,0,\N,Missing
N09-2001,N04-4026,0,\N,Missing
N09-2001,C04-1030,0,\N,Missing
N09-2001,C04-1073,0,\N,Missing
N09-2001,D08-1089,0,\N,Missing
N09-2001,P06-1067,0,\N,Missing
N09-2001,W06-1608,0,\N,Missing
N09-2001,P05-1033,0,\N,Missing
N09-2001,P05-1034,1,\N,Missing
N09-2001,J97-3002,0,\N,Missing
N09-2001,P05-1066,0,\N,Missing
N09-2001,D07-1077,0,\N,Missing
N09-2001,2005.iwslt-1.8,0,\N,Missing
N09-2001,W08-0509,1,\N,Missing
N09-2001,N06-1004,0,\N,Missing
N09-2038,2007.iwslt-1.4,1,0.789571,"adapt the system based on its usage automatically without having to ship data back to the laboratory for retraining. This paper investigates the scenario of a two-day event. We wish to improve the system for the second day based on the data collected on the first day. Our system is designed for eyes-free use and hence provides no graphical user interface. This allows the user to concentrate on his surrounding environment during an operation. The system only provides audio control and feedback. Additionally the system operates on a push-totalk method. Previously the system (Hsiao et al., 2006; Bach et al., 2007) needed 2 buttons to operate, one for the English speaker and the other one for the Iraqi speaker. To make the system easier and faster to use, we propose to use a single button which can be controlled by the English speaker. We mounted a microphone and a Wii remote controller together as shown in 1. Since the Wii controller has an accelerometer which can be used to detect the orientation of the controller, this feature can be applied to identify who is speaking. When the English speaker points towards himself, the system will switch to English-Iraqi translation. However, when the Wii is point"
N09-2038,P08-2040,0,0.0125219,"0K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translation references; 2) re-tune the translation system on day 1 via minimum error rate training (MERT) (Venugopal and Vogel, 2005). Use 500 Best MT Hypos Day 1 Day 2 Baseline 29.39 27.41 1gramLM 2gramLM 3gramLM 29.18 29.53 29.36 27.23 27.50 27.23 The first question we would like to address is whether our adaptation obtains improvements via an unsupervised manner. We take day 1 baseline ASR hypothesis and use the baseline SMT to get the MT hypothesis and a 500bes"
N09-2038,eck-etal-2004-language,1,0.856377,"d and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translation references; 2) re-tune the translation system on day 1 via minimum error rate training (MERT) (Venugopal and Vogel, 2005). Use 500 Best MT Hypos Day 1 Day 2 Baseline 29.39 27.41 1gramLM 2gramLM 3gra"
N09-2038,P02-1040,0,0.0766368,"FCC and we concatenate adjacent 15 frames and perform LDA to reduce the dimension to 42 for the final feature vectors. The language model of the ASR system is a trigram LM trained on the audio transcripts with around three million words with Kneser-Ney smoothing (Stolcke, 2002). To perform LM adaptation for the ASR system, we use the ASR hypotheses from day 1 to build a LM. This LM is then interpolated with the original trigram LM to produce an adapted LM for day 2. We also evaluate the effect 150 Table 2 shows the impact of ASR adaptation on the performance of the translation system in BLEU (Papineni et al., 2002). In these experiments we only performed adaptation on ASR and still using the baseline SMT component. There is no obvious difference between unsupervised and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one a"
N09-2038,2005.eamt-1.36,1,0.647983,"exity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translation references; 2) re-tune the translation system on day 1 via minimum error rate training (MERT) (Venugopal and Vogel, 2005). Use 500 Best MT Hypos Day 1 Day 2 Baseline 29.39 27.41 1gramLM 2gramLM 3gramLM 29.18 29.53 29.36 27.23 27.50 27.23 The first question we would like to address is whether our adaptation obtains improvements via an unsupervised manner. We take day 1 baseline ASR hypothesis and use the baseline SMT to get the MT hypothesis and a 500best list. We train a domain LM using the 500-best list and use the MT hypotheses as the reference in MERT. We treat day 1 as a development set and day 2 as an unseen test set. In Table 3 we compare the performance of four systems: the baseline which does not have an"
N09-2038,2005.mtsummit-papers.33,1,0.693382,"impact of ASR adaptation on the performance of the translation system in BLEU (Papineni et al., 2002). In these experiments we only performed adaptation on ASR and still using the baseline SMT component. There is no obvious difference between unsupervised and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list o"
N09-2038,2005.eamt-1.39,1,0.826207,"he translation system in BLEU (Papineni et al., 2002). In these experiments we only performed adaptation on ASR and still using the baseline SMT component. There is no obvious difference between unsupervised and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translati"
N16-1125,N16-3004,1,0.864659,"Missing"
N16-1125,W07-0718,0,0.0456341,"res given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 1998; Garrod, 2006; Hansen and Ji, 2010). The overall difficulty of a sentence and its syntactic complexity"
N16-1125,W12-3102,0,0.0857719,"Missing"
N16-1125,P11-1105,1,0.882545,"and distance between the start and end words. For subsequent words n, n + 1, this would mean a forward jump of distance equal to 1. All jumps with distance greater than 4 were sorted into a 5+ bucket. Additionally, we separate the features for reference and translation jumps. We also count the total number of jumps. Total jump distance We additionally aggregate jump distances2 to count the total distance covered while evaluating a sentence. We have reference distance and translation distance features. Again, the 2 Jump count and distance features have also shown to be useful in SMT decoders (Durrani et al., 2011). 1083 idea is that for a well-formed sentence, gaze distance should be less, compared to a poorly-formed one. Inter-region jumps While reading a translation, evaluators can jump between the translation and a reference to compare them. Intuitively, more jumps of this type could signify that the translation is harder to evaluate. Here we count the number of transitions between reference and translation. Dwell time The amount of time a person fixates on a region is a crucial marker for processing difficulty in sentence comprehension (Clifton et al., 2007) and moderately correlates with the quali"
N16-1125,W13-2305,0,0.0177286,"was performed by 6 different evaluators, resulting in 720 evaluations. The annotators were presented with a translationreference pair at a time. The two evaluation tasks corresponding to the same reference were presented at two different times with at least 40 other tasks in-between. This was done to prevent any possible spurious effects that may arise from remembering the content of a first translation, when evaluating the second translation of the same sentence. During each evaluation task, the evaluators were asked to assess the quality of a translation by providing a score between 0–100 (Graham et al., 2013). The observed inter-annotator agreement (Cohen’s kappa) among our annotators was 0.321. This is slightly higher than the overall inter-annotator agreement of 0.284 reported in WMT’12 for the Spanish-English.3 For reading patterns we use the EyeTribe eye-tracker at 3 For a rough comparison only. Note that these two numbers are not exactly comparable given that they are calculated on different subsets of the same data. Still, there is a fair agreement between the our evaluators and the expected wins from WMT’12 (avg. pairwise kappa of 0.381) 1084 Evaluation In our evaluation, we used eye-tracki"
N16-1125,W15-3059,1,0.875061,"Missing"
N16-1125,P02-1040,0,0.0970473,"n evaluation metric So far, we’ve shown that the individual sets of features based on reading patterns can help to predict translation quality, and that this goes beyond simple fluency. One question that remains to be answered is whether these features could be used as a whole to evaluate the quality of a translation semi-automatically. That is, whether we can use the gaze information, and other lexical information to anticipate the score that an evaluator will assign to a translation. Here, we present evaluation results combining several of these gaze features, and compare them against BLEU (Papineni et al., 2002), which uses lexical information and is designed to measure not only fluency but also adequacy. In Table 2, we present results in the following way: in (I) we present the best non-lexicalized feature combinations that improve the predictive power of the model. In (II) we re-introduce the results of lexicalized jumps feature. In (III) we present results of BLEU and the combination of eye-tracking features with it. Finally in (IV) we present the humanto-human agreement measured in average Kendall’s tau and in max human-to-human Kendall’s tau. Combinations of translation jumps In section I we pre"
N16-1125,2006.amta-papers.25,0,0.0363431,"patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 1998; Garrod, 2006; Hans"
N16-1125,stymne-etal-2012-eye,0,0.322545,"enomena remain to be explored in future work. Human performance On average, evaluators agreements with each other are fair (τ = 0.33) and below the best combination (CB3 ), while the maximum agreement of any two evaluators is relatively higher (τ = 0.53). This tells us that on average the semi-automatic approach to evaluation that we propose here is already competitive to predictions done by another (average) human. However, there is still room for improvement with respect to the mostagreeing pair of evaluators. 5 Related Work Eye-tracking devices have been used previously in the MT research. Stymne et al. (2012) used eye-tracking to identify and classify MT errors. 1086 SYS Feature Sets τ I. Combination of translation jumps EyeTrabj Backward jumps CTJ1 Backward jumps, total jumps CTJ2 Backward jumps, total jumps, distance 0.22 0.25 0.27 II. Eye-tracking: Best Lexicalized EyeLexall Lexicalized gaze jumps 0.22 III. Combinations with BLEU Bbleu BLEU CB1 Bbleu + EyeTrabj CB2 Bbleu + CTJ2 CB3 Bbleu + EyeLexall 0.34 0.38 0.39 0.42 IV. Human performance Avg Avg. human-to-human agreement Max Max. human-to-human agreement 0.33 0.53 Table 2: Result of combining several jump and lexicalized features with BLEU."
N16-1125,2003.mtsummit-papers.51,0,0.111887,"ts show that reading patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 19"
N16-1125,1994.amta-1.25,0,0.757488,"Missing"
N18-2079,N16-3003,1,0.868742,"Missing"
N18-2079,P07-2045,0,0.00773552,"Missing"
N18-2079,N12-1048,0,0.0553586,", or the end of sentence is not clearly marked, the system must operate on a buffered sequence. Generating translations for such incomplete sequences presents a considerable challenge for machine translation, more so in the case of syntactically divergent language pairs (such as German-English), where the context required to correctly translate a sentence, appears much later in the sequence, and prematurely committing to a translation leads to significant loss in quality. Various strategies to select appropriate segmentation points in a streaming input have been proposed (F¨ugen et al., 2007; Bangalore et al., 2012; Sridhar et al., 2013; Yarmohammadi et al., 2013; Oda et al., 2014). A downside of this approach is that the MT system translates sequences independent of each other, ignoring the context. Even if the segmenter decides perfect points to segment the input stream, an MT system requires lexical history to make the correct decision. The remaining paper is organized as follow: Section 2 describes modifications to the NMT decoder to enable stream decoding. Section 3 describes various agents to learn a READ/WRITE strategy. Section 4 presents evaluation and results. Section 5 describes modifications"
N18-2079,2015.iwslt-evaluation.11,0,0.0460089,"an agent introduced by Gu et al. (2017) and Satija and Pineau (2016), but without the overhead of expensive training for the agent. Figure 4: Averaged results on test-sets (2011-2014) using the models trained on small and large datasets using AP  0.75. Detailed test-wise results are available in the supplementary material. Scalability: The preliminary results were obtained using models trained on the TED corpus only. We conducted further experiments by training models on larger data-sets (See the supplementary section again for data sizes) to see if our findings are scalable. We fine-tuned (Luong and Manning, 2015; Sajjad et al., 2017b) our models with the in-domain data to avoid domain disparity. We then re-ran our agents with the best S,RW values (with an AP under 0.75) for each language pair. Figure 4 (“large” models) shows that the BLEU loss from the respective oracle increased when the models were trained with bigger data sizes. This could be attributed to the increased lexical ambiguity from the large amount of out-domain data, which can only be resolved with additional contextual information. However our results were still better than the WIW agent, which also has an AP value above 0.8. Allowing"
N18-2079,P14-2090,0,0.431005,"n a buffered sequence. Generating translations for such incomplete sequences presents a considerable challenge for machine translation, more so in the case of syntactically divergent language pairs (such as German-English), where the context required to correctly translate a sentence, appears much later in the sequence, and prematurely committing to a translation leads to significant loss in quality. Various strategies to select appropriate segmentation points in a streaming input have been proposed (F¨ugen et al., 2007; Bangalore et al., 2012; Sridhar et al., 2013; Yarmohammadi et al., 2013; Oda et al., 2014). A downside of this approach is that the MT system translates sequences independent of each other, ignoring the context. Even if the segmenter decides perfect points to segment the input stream, an MT system requires lexical history to make the correct decision. The remaining paper is organized as follow: Section 2 describes modifications to the NMT decoder to enable stream decoding. Section 3 describes various agents to learn a READ/WRITE strategy. Section 4 presents evaluation and results. Section 5 describes modifications to the NMT training to mimic corresponding decoding strategy, and Se"
N18-2079,P17-2095,1,0.839756,"u et al. (2017) and Satija and Pineau (2016), but without the overhead of expensive training for the agent. Figure 4: Averaged results on test-sets (2011-2014) using the models trained on small and large datasets using AP  0.75. Detailed test-wise results are available in the supplementary material. Scalability: The preliminary results were obtained using models trained on the TED corpus only. We conducted further experiments by training models on larger data-sets (See the supplementary section again for data sizes) to see if our findings are scalable. We fine-tuned (Luong and Manning, 2015; Sajjad et al., 2017b) our models with the in-domain data to avoid domain disparity. We then re-ran our agents with the best S,RW values (with an AP under 0.75) for each language pair. Figure 4 (“large” models) shows that the BLEU loss from the respective oracle increased when the models were trained with bigger data sizes. This could be attributed to the increased lexical ambiguity from the large amount of out-domain data, which can only be resolved with additional contextual information. However our results were still better than the WIW agent, which also has an AP value above 0.8. Allowing similar AP, our STAT"
N18-2079,P11-1105,1,0.811515,"escribed in Cho and Esipova (2016). The Wait-if-Worse (WIW) agent WRITES 495 Figure 3: Results for various streaming AGENTS (WID, WIW, WUE, C6 (Chunk decoding with a N=6) and S,RW for STATIC-RW) on the tune-set. For each AP bucket, we only show the Agents with the top 3 BLEU scores in that bucket, with remaining listed in descending order of their BLEU scores. bigger challenge and requires larger context than other language pairs. For example the conjugated verb in a German verb complex appears in the second position, while the main verb almost always occurs at the end of the sentence/phrase (Durrani et al., 2011). Our methods are also comparable to the more sophisticated techniques involving Reinforcement Learning to learn an agent introduced by Gu et al. (2017) and Satija and Pineau (2016), but without the overhead of expensive training for the agent. Figure 4: Averaged results on test-sets (2011-2014) using the models trained on small and large datasets using AP  0.75. Detailed test-wise results are available in the supplementary material. Scalability: The preliminary results were obtained using models trained on the TED corpus only. We conducted further experiments by training models on larger dat"
N18-2079,J15-2001,1,0.906195,"Missing"
N18-2079,E17-2045,0,0.0284211,"u et al. (2017) and Satija and Pineau (2016), but without the overhead of expensive training for the agent. Figure 4: Averaged results on test-sets (2011-2014) using the models trained on small and large datasets using AP  0.75. Detailed test-wise results are available in the supplementary material. Scalability: The preliminary results were obtained using models trained on the TED corpus only. We conducted further experiments by training models on larger data-sets (See the supplementary section again for data sizes) to see if our findings are scalable. We fine-tuned (Luong and Manning, 2015; Sajjad et al., 2017b) our models with the in-domain data to avoid domain disparity. We then re-ran our agents with the best S,RW values (with an AP under 0.75) for each language pair. Figure 4 (“large” models) shows that the BLEU loss from the respective oracle increased when the models were trained with bigger data sizes. This could be attributed to the increased lexical ambiguity from the large amount of out-domain data, which can only be resolved with additional contextual information. However our results were still better than the WIW agent, which also has an AP value above 0.8. Allowing similar AP, our STAT"
N18-2079,N13-1073,0,0.133373,"Missing"
N18-2079,eisele-chen-2010-multiun,0,0.0193242,"Missing"
N18-2079,P16-1162,0,0.318923,"Missing"
N18-2079,E17-1099,0,0.103077,"g with a N=6) and S,RW for STATIC-RW) on the tune-set. For each AP bucket, we only show the Agents with the top 3 BLEU scores in that bucket, with remaining listed in descending order of their BLEU scores. bigger challenge and requires larger context than other language pairs. For example the conjugated verb in a German verb complex appears in the second position, while the main verb almost always occurs at the end of the sentence/phrase (Durrani et al., 2011). Our methods are also comparable to the more sophisticated techniques involving Reinforcement Learning to learn an agent introduced by Gu et al. (2017) and Satija and Pineau (2016), but without the overhead of expensive training for the agent. Figure 4: Averaged results on test-sets (2011-2014) using the models trained on small and large datasets using AP  0.75. Detailed test-wise results are available in the supplementary material. Scalability: The preliminary results were obtained using models trained on the TED corpus only. We conducted further experiments by training models on larger data-sets (See the supplementary section again for data sizes) to see if our findings are scalable. We fine-tuned (Luong and Manning, 2015; Sajjad et al.,"
N18-2079,J81-4005,0,0.707264,"Missing"
nakov-vogel-2017-robust,D09-1141,1,\N,Missing
nakov-vogel-2017-robust,P02-1040,0,\N,Missing
nakov-vogel-2017-robust,D08-1024,0,\N,Missing
nakov-vogel-2017-robust,P13-2003,1,\N,Missing
nakov-vogel-2017-robust,W12-3136,1,\N,Missing
nakov-vogel-2017-robust,P07-2045,0,\N,Missing
nakov-vogel-2017-robust,P08-2030,0,\N,Missing
nakov-vogel-2017-robust,N09-1025,0,\N,Missing
nakov-vogel-2017-robust,P11-2031,0,\N,Missing
nakov-vogel-2017-robust,N06-1003,0,\N,Missing
nakov-vogel-2017-robust,R13-1066,1,\N,Missing
nakov-vogel-2017-robust,N03-1017,0,\N,Missing
nakov-vogel-2017-robust,K15-1007,1,\N,Missing
nakov-vogel-2017-robust,2005.iwslt-1.8,0,\N,Missing
nakov-vogel-2017-robust,D07-1080,0,\N,Missing
nakov-vogel-2017-robust,C10-1075,0,\N,Missing
nakov-vogel-2017-robust,W07-0716,0,\N,Missing
nakov-vogel-2017-robust,P12-1002,0,\N,Missing
nakov-vogel-2017-robust,W11-2123,0,\N,Missing
nakov-vogel-2017-robust,P03-1021,0,\N,Missing
nakov-vogel-2017-robust,N12-1047,0,\N,Missing
nakov-vogel-2017-robust,D11-1125,0,\N,Missing
nakov-vogel-2017-robust,N12-1062,0,\N,Missing
nakov-vogel-2017-robust,C12-1121,1,\N,Missing
nakov-vogel-2017-robust,N12-1023,0,\N,Missing
P00-1004,W99-0604,1,\N,Missing
P00-1004,niessen-etal-2000-evaluation,1,\N,Missing
P00-1004,C00-2172,0,\N,Missing
P00-1004,C96-1030,0,\N,Missing
P00-1004,P96-1021,0,\N,Missing
P00-1004,P97-1047,0,\N,Missing
P03-1041,J93-2003,0,0.0192708,"e facto standard. Direct translation approaches (Fos ! "" directly, and ter, 2000) consider estimating  work by (Och and Ney, 2002) show that similar ! '  or improved results are achieved by replacing #!  in the optimization with  , at the cost of deviating from the Bayesian framework. Regardless of the approach, the question of accurately estimating a model of translation from a large parallel or comparable corpus is one of the defining components within statistical machine translation. Re-ordering effects across languages have been modeled in several ways, including word-based (Brown et al., 1993), template-based (Och et al., 1999) and syntax-based (Yamada, Knight, 2001). Analyzing these models from a generative mindset, they all assume that the atomic unit of lexical content is the word, and re-ordering effects are applied above that level. (Marcu, Wong, 2002) illustrate the effects of assuming that lexical correspondence can only be modeled at the word level, and motivate a joint probability model that explicitly generates phrase level lexical content across both languages. (Wu, 1995) presents a bracketing method that models re-ordering at the sentence level. Both (Marcu, Wong, 2002;"
P03-1041,P02-1038,0,0.0491004,"tion problem where we need to separate the distribution of the incorrectly translated hypothesis from the distribution of the likely translations. Instead of using the maximum likelihood criteria, we use the maximal separation criteria ie. selecting a splitting point within the scores to maximize the difference of the mean score between distributions as shown below. 8: .,¤ 8X Z   *  ""¥ * P¦ B§ * (9)  ¨E M ! w©  . <W 8YX[ZB M . V ,  ª ¬ <W 8YX[ZB  « V  c , M  (10) (10) calculates direct translation probabilities, ie #!  . As mentioned earlier, (Och and Ney, 2002), show that using direction translation estimates in the decoding process as compared with calculating &!   as prescribed by the Bayesian framework does not reduce translation quality. Our results corroborate these findings and we use (10) as the phrase level translation model estimate within our decoder.  6 Integration Phrase translation pairs that are generated by the method described in this paper are finally scored with estimates of translation probability, which can be conditioned on the target language if necessary. These estimates fit cleanly into the decoding process, except for"
P03-1041,W99-0604,0,\N,Missing
P03-1041,C00-2163,0,\N,Missing
P03-1041,C96-2141,1,\N,Missing
P03-1041,P02-1040,0,\N,Missing
P03-1041,P01-1067,0,\N,Missing
P03-1041,P00-1006,0,\N,Missing
P08-2020,P07-2045,0,0.0110783,"a for the models, e.g. the co-occurrence or fertility model. A master process collects the counts from the nodes, normalizes them and outputs the intermediate model for each iteration. This distributed GIZA++ version finished training the word alignment up to IBM Model 4 for both language directions on the full bilingual corpus (260 million words, English) in 39 hours. On average about 11 CPUs were running concurrently. In comparison the standard GIZA++ implementation finished the same training in 169 hours running on 2 CPUs, one for each language direction. We used the Pharaoh/Moses package (Koehn et al., 2007) to extract and score phrase pairs using the grow-diag-final extraction method. 4 POS-based Reordering As Chinese and English have very different word order, reordering over a rather limited distance during decoding is not sufficient. Also using a simple distance based distortion probability leaves it essentially to the language model to select among different reorderings. An alternative is to apply automatically learned reordering rules to the test sentences before decoding (Crego and Marino, 2006). We create a word lattice, which encodes many reorderings and allows long distance reordering."
P08-2020,J03-1002,0,0.00744539,"Missing"
P08-2020,P02-1040,0,0.0759876,"ible to use larger and larger amounts of data in training. In Section 3 we show how parallelizing model training can reduce training time by an order of magnitude and how using larger training data as well as more extensive models improve translation quality. Word reordering is still a difficult problem in SMT. In Section 4 we apply a Part Of Speech (POS) based syntactic reordering model successfully to our large Chinese system. 1.1 Decoder Our translation system is based on the CMU SMT decoder as described in (Hewavitharana et Evaluation In this paper we report results using the BLEU metric (Papineni et al., 2002), however as the evaluation criterion in GALE is HTER (Snover et al., 2006), we also report in TER (Snover et al., 2005). We used the test sets from the NIST MT evaluations from the years 2003 and 2006 as development and unseen test data. 1.3 Training Data In translation model training we used the ChineseEnglish bilingual corpora relevant to GALE available through the LDC1 . After sentence alignment these sources add up to 10.7 million sentences with 301 million running words on the English side. Our preprocessing steps include tokenization on the English side and for Chinese: automatic word s"
P08-2020,2007.tmi-papers.21,1,0.748539,"r to 410, but did not effect the translation score. 79 Learning Reordering Rules Applying Reordering Rules To avoid hard decisions, we build a lattice structure for each source sentence as input for our decoder, which contains reordering alternatives consistent with the previously extracted rules. Longer reordering patterns are applied first. Thereby shorter patterns can match along new paths, creating short distance reordering on top of long distance reordering. Every outgoing edge of a node is scored with the relative frequency of the pattern used on the following sub path (For details see (Rottmann and Vogel, 2007)). These model scores give this re3 http://nlp.stanford.edu/software/lex-parser.shtml ordering approach an advantage over a simple jump model with a sliding window. System 260M, standard 260M, lattice MT03 32.20/60.59 33.53/59.74 MT06 30.22/60.81 31.74/59.59 Table 5: Reordering lattice decoding in BLEU/TER The system with reordering lattice input outperforms the system with a reordering window of 4 words by 1.5 BLEU (see Table 5). 5 Summary The recent improvements to our Chinese-English SMT system (see Fig. 1) can be mainly attributed to a POS based word reordering method and the possibility t"
P08-2020,W05-0836,0,0.0350695,"provements in the CMU Large Scale Chinese-English SMT System Almut Silja Hildebrand, Kay Rottmann, Mohamed Noamany, Qin Gao, Sanjika Hewavitharana, Nguyen Bach and Stephan Vogel Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA silja, kayrm, mfn, qing, sanjika, nbach, vogel+@cs.cmu.edu Abstract al., 2005). Our decoder is a phrase-based beam search decoder, which combines multiple models e.g. phrase tables, several language models, a distortion model ect. in a log-linear fashion. In order to find an optimal set of weights, we use MER training as described in (Venugopal et al., 2005), which uses rescoring of the top n hypotheses to maximize an evaluation metric like BLEU or TER. In this paper we describe recent improvements to components and methods used in our statistical machine translation system for ChineseEnglish used in the January 2008 GALE evaluation. Main improvements are results of consistent data processing, larger statistical models and a POS-based word reordering approach. 1.2 1 Introduction Building a full scale Statistical Machine Translation (SMT) system involves many preparation and training steps and it consists of several components, each of which contr"
P08-2020,2005.iwslt-1.6,1,\N,Missing
P08-2020,I05-3027,0,\N,Missing
P10-2027,W07-0702,0,0.031341,"Missing"
P10-2027,D07-1091,0,0.0943925,"Missing"
P10-2067,N03-1017,0,0.00497929,"rporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner. 1 Introduction Corpus-based approaches to machine translation have become predominant, with phrase-based statistical machine translation (PB-SMT) (Koehn et al., 2003) being the most actively progressing area. The success of statistical approaches to MT can be attributed to the IBM models (Brown et al., 1993) that characterize word-level alignments in parallel corpora. Parameters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. While the ease of automatically aligning sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of SMT systems for various language pairs, the quality of alignment is typically quite low for language pairs l"
P10-2067,C04-1046,0,0.0442399,"Missing"
P10-2067,P07-2045,0,0.00770914,"uncertainty. We will be exploring alternative formulations to this strategy. We observe that confidence based metrics perform significantly better than the baseline. From the scatter plots in Figure 1 1 we can say that using our best selection strategy one achieves similar performance to the baseline, but at a much lower cost of elicitation assuming cost per link is uniform. We also perform end-to-end machine translation experiments to show that our improvement of alignment quality leads to an improvement of translation scores. For this experiment, we train a standard phrase-based SMT system (Koehn et al., 2007) over the entire parallel corpus. We tune on the MT-Eval 2004 dataset and test on a subset of MT-Eval 2004 dataset consisting of 631 sentences. We first obtain the baseline score where no manual alignment was used. We also train a configuration using gold standard manual alignment data for the parallel corpus. This is the maximum translation accuracy that we can achieve by any link selection algorithm. We now take the best link selection criteria, which is the confidence M argin(i) = ˆ ij /S, T ) −Conf 1(a2 ˆ ij /S, T ) Conf 1(a1 5 5.1 Experiments Data Setup Our aim in this paper is to show th"
P10-2067,J93-2003,0,0.0116582,"ry-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner. 1 Introduction Corpus-based approaches to machine translation have become predominant, with phrase-based statistical machine translation (PB-SMT) (Koehn et al., 2003) being the most actively progressing area. The success of statistical approaches to MT can be attributed to the IBM models (Brown et al., 1993) that characterize word-level alignments in parallel corpora. Parameters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. While the ease of automatically aligning sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of SMT systems for various language pairs, the quality of alignment is typically quite low for language pairs like ChineseEnglish, Arabic-English that diverge from the independence assumptions made by the generative models. Increased parallel data enable"
P10-2067,W07-0734,0,0.0352709,"Missing"
P10-2067,P04-1023,0,0.0200747,"scored high by our query strategy. We seek manual corrections for the selected links and add the alignment data to the current labeled data set. The word-level aligned labeled data is provided to our semi-supervised word alignment algorithm for training an alignment model Mt+1 over U . linear model is trained on available labeled data to improve performance. They propose a semisupervised training algorithm which alternates between discriminative error training on the labeled data to learn the weighting parameters and maximum-likelihood EM training on unlabeled data to estimate the parameters. Callison-Burch et al. (2004) also improve alignment by interpolating human alignments with automatic alignments. They observe that while working with such data sets, alignments of higher quality should be given a much higher weight than the lower-quality alignments. Wu et al. (2006) learn separate models from labeled and unlabeled data using the standard EM algorithm. The two models are then interpolated to use as a learner in the semi-supervised algorithm to improve word alignment. To our knowledge, there is no prior work that has looked at reducing human effort by selective elicitation of partial word alignment using a"
P10-2067,P06-2014,0,0.0339173,"Missing"
P10-2067,J03-1002,0,0.00582919,"duction Corpus-based approaches to machine translation have become predominant, with phrase-based statistical machine translation (PB-SMT) (Koehn et al., 2003) being the most actively progressing area. The success of statistical approaches to MT can be attributed to the IBM models (Brown et al., 1993) that characterize word-level alignments in parallel corpora. Parameters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. While the ease of automatically aligning sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of SMT systems for various language pairs, the quality of alignment is typically quite low for language pairs like ChineseEnglish, Arabic-English that diverge from the independence assumptions made by the generative models. Increased parallel data enables better estimation of the model parameters, but a large number of language pairs still lack such resources. 2 Related Work Researchers have begun to explore models that use both labeled and unlabeled data to build word-alignment models for MT. Fraser and Marcu (2006) pose the problem of alignment as a search probl"
P10-2067,P02-1040,0,0.0783219,"Missing"
P10-2067,P06-1097,0,0.0240295,"sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of SMT systems for various language pairs, the quality of alignment is typically quite low for language pairs like ChineseEnglish, Arabic-English that diverge from the independence assumptions made by the generative models. Increased parallel data enables better estimation of the model parameters, but a large number of language pairs still lack such resources. 2 Related Work Researchers have begun to explore models that use both labeled and unlabeled data to build word-alignment models for MT. Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. The log365 Proceedings of the ACL 2010 Conference Short Papers, pages 365–370, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics model Mt from current iteration t for scoring the links. Re-training and re-tuning an SMT system for each link at a time is computationally infeasible. We therefore perform batch learning by selecting a set of N links scored high by our query strategy. We seek manual corrections for the selected links and add the"
P10-2067,D07-1006,0,0.0187025,"ethod on the newly-labeled and previously-labeled instances to minimize prediction or translation error, repeating until either the maximal number of external queries is reached or a desired accuracy level is achieved. Several studies (Tong and Koller, 2002; Nguyen and Smeulders, 2004; Donmez and Carbonell, 2008) show that active learning greatly helps to reduce the labeling effort in various classification tasks. 3.1 We can iteratively perform the algorithm for a defined number of iterations T or until a certain desired performance is reached, which is measured by alignment error rate (AER) (Fraser and Marcu, 2007b) in the case of word alignment. In a more typical scenario, since reducing human effort or cost of elicitation is the objective, we iterate until the available budget is exhausted. 3.2 Semi-Supervised Word Alignment We use an extended version of MGIZA++ (Gao and Vogel, 2008) to perform the constrained semisupervised word alignment. Manual alignments are incorporated in the EM training phase of these models as constraints that restrict the summation over all possible alignment paths. Typically in the EM procedure for IBM models, the training procedure requires for each source sentence positio"
P10-2067,J07-1003,0,0.0799212,"ths reduces to restricting the summation in EM. 4 links that the initial word aligner is least confident according to our metric and seek manual correction of the links. We use t2s to denote computation using higher order (IBM4) target-givensource models and s2t to denote source-giventarget models. Targeting some of the uncertain parts of word alignment has already been shown to improve translation quality in SMT (Huang, 2009). We use confidence metrics as an active learning sampling strategy to obtain most informative links. We also experimented with other confidence metrics as discussed in (Ueffing and Ney, 2007), especially the IBM 1 model score metric, but it did not show significant improvement in this task. Query Strategies for Link Selection We propose multiple query selection strategies for our active learning setup. The scoring criteria is designed to select alignment links across sentence pairs that are highly uncertain under current automatic translation models. These links are difficult to align correctly by automatic alignment and will cause incorrect phrase pairs to be extracted in the translation model, in turn hurting the translation quality of the SMT system. Manual correction of such l"
P10-2067,J07-3002,0,0.0158945,"ethod on the newly-labeled and previously-labeled instances to minimize prediction or translation error, repeating until either the maximal number of external queries is reached or a desired accuracy level is achieved. Several studies (Tong and Koller, 2002; Nguyen and Smeulders, 2004; Donmez and Carbonell, 2008) show that active learning greatly helps to reduce the labeling effort in various classification tasks. 3.1 We can iteratively perform the algorithm for a defined number of iterations T or until a certain desired performance is reached, which is measured by alignment error rate (AER) (Fraser and Marcu, 2007b) in the case of word alignment. In a more typical scenario, since reducing human effort or cost of elicitation is the objective, we iterate until the available budget is exhausted. 3.2 Semi-Supervised Word Alignment We use an extended version of MGIZA++ (Gao and Vogel, 2008) to perform the constrained semisupervised word alignment. Manual alignments are incorporated in the EM training phase of these models as constraints that restrict the summation over all possible alignment paths. Typically in the EM procedure for IBM models, the training procedure requires for each source sentence positio"
P10-2067,P06-2117,0,0.0204919,"model Mt+1 over U . linear model is trained on available labeled data to improve performance. They propose a semisupervised training algorithm which alternates between discriminative error training on the labeled data to learn the weighting parameters and maximum-likelihood EM training on unlabeled data to estimate the parameters. Callison-Burch et al. (2004) also improve alignment by interpolating human alignments with automatic alignments. They observe that while working with such data sets, alignments of higher quality should be given a much higher weight than the lower-quality alignments. Wu et al. (2006) learn separate models from labeled and unlabeled data using the standard EM algorithm. The two models are then interpolated to use as a learner in the semi-supervised algorithm to improve word alignment. To our knowledge, there is no prior work that has looked at reducing human effort by selective elicitation of partial word alignment using active learning techniques. 3 Algorithm 1 AL FOR W ORD A LIGNMENT 1: Unlabeled Data Set: U = {(Sk , Tk )} 2: Manual Alignment Set : A0 = {akij , ∀si ∈ Sk , tj ∈ Tk } 3: Train Semi-supervised Word Alignment using (U , A0 ) → M0 4: N : batch size 5: for t ="
P10-2067,W08-0509,1,0.787331,", 2004; Donmez and Carbonell, 2008) show that active learning greatly helps to reduce the labeling effort in various classification tasks. 3.1 We can iteratively perform the algorithm for a defined number of iterations T or until a certain desired performance is reached, which is measured by alignment error rate (AER) (Fraser and Marcu, 2007b) in the case of word alignment. In a more typical scenario, since reducing human effort or cost of elicitation is the objective, we iterate until the available budget is exhausted. 3.2 Semi-Supervised Word Alignment We use an extended version of MGIZA++ (Gao and Vogel, 2008) to perform the constrained semisupervised word alignment. Manual alignments are incorporated in the EM training phase of these models as constraints that restrict the summation over all possible alignment paths. Typically in the EM procedure for IBM models, the training procedure requires for each source sentence position, the summation over all positions in the target sentence. The manual alignments allow for one-tomany alignments and many-to-many alignments in both directions. For each position i in the source sentence, there can be more than one manually aligned target word. The restricted"
P10-2067,P09-1021,0,0.0244571,"rcu, 2007a). The second is to use extra annotation, typically word-level human alignment for some sentence pairs, in conjunction with the parallel data to learn alignment in a semi-supervised manner. Our research is in the direction of the latter, and aims to reduce the effort involved in hand-generation of word alignments by using active learning strategies for careful selection of word pairs to seek alignment. Active learning for MT has not yet been explored to its full potential. Much of the literature has explored one task – selecting sentences to translate and add to the training corpus (Haffari and Sarkar, 2009). In this paper we explore active learning for word alignment, where the input to the active learner is a sentence pair (S, T ) and the annotation elicited from human is a set of links {aij , ∀si ∈ S, tj ∈ T }. Unlike previous approaches, our work does not require elicitation of full alignment for the sentence pair, which could be effort-intensive. We propose active learning query strategies to selectively elicit partial alignment information. Experiments in Section 5 show that our selection strategies reduce alignment error rates significantly over baseline. Semi-supervised word alignment aim"
P10-2067,P09-1105,0,0.0174733,"nks based on our active learning query strategy. The query strategy uses the automatically trained alignment 366 the manual alignments. Therefore, the restriction of the alignment paths reduces to restricting the summation in EM. 4 links that the initial word aligner is least confident according to our metric and seek manual correction of the links. We use t2s to denote computation using higher order (IBM4) target-givensource models and s2t to denote source-giventarget models. Targeting some of the uncertain parts of word alignment has already been shown to improve translation quality in SMT (Huang, 2009). We use confidence metrics as an active learning sampling strategy to obtain most informative links. We also experimented with other confidence metrics as discussed in (Ueffing and Ney, 2007), especially the IBM 1 model score metric, but it did not show significant improvement in this task. Query Strategies for Link Selection We propose multiple query selection strategies for our active learning setup. The scoring criteria is designed to select alignment links across sentence pairs that are highly uncertain under current automatic translation models. These links are difficult to align correct"
P11-1001,P09-1088,0,0.0502115,"table, resulting in generalized phrases with placeholders in them. The supertags are also injected into the language model. Our approach also generates phrase labels and placeholders based on word tags (albeit in a different manner and without the use of subcategorization information), but produces PSCFG rules for use in a parsing-based decoding system. Unsupervised synchronous grammar induction, apart from the contribution of Chiang (2005) discussed earlier, has been proposed by Wu (1997) for inversion transduction grammars, but as Chiang’s model only uses a single generic nonterminal label. Blunsom et al. (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and Cohn and Blunsom (2009) achieve the same for tree-to-string grammars, with encouraging results on small data. Our more humble approach treats the training sentences’ word alignments and phrase pairs, obtained from external modules, as ground truth and employs a straight-forward generalization of Chiang’s popular rule extraction approach to labeled phrase pairs, resulting in a PSCFG with multiple nonterminal labels. Our phras"
P11-1001,J93-2003,0,0.0261386,"and imply conditional independence assumptions in the translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). 1 While all of these techniques rely on wordalignments to suggest lexical relationships, they differ in the way in which they assign labels to nonterminal symbols of PSCFG rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006) directly extend the rule extraction procedure from Chiang (2005) to heuristically label any phrase pair based on target language parse trees. Label-based approaches have resulted in improvements in translation quality over the single X label approach (Zollmann et al., 2008; Mi and Huang, 2008); how"
P11-1001,D08-1024,0,0.0471678,"Missing"
P11-1001,P05-1033,0,0.588529,"lmann and Stephan Vogel Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {zollmann,vogel+}@cs.cmu.edu Abstract In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chineseto-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available. 1 Introduction The Probabilistic Synchronous Context Free Grammar (PSCFG) formalism suggests an intuitive approach to model the long-distance and lexically sensitive reordering phenomena that often occur across language pairs considered for statistical mac"
P11-1001,J07-2003,0,0.475148,"We evaluate our approach by comparing translation quality, as evaluated by the IBM-BLEU (Papineni et al., 2002) metric on the NIST Chinese-to-English translation task using MT04 as development set to train the model parameters λ, and MT05, MT06 and MT08 as test sets. Even though a key advantage of our method is its applicability to resource-poor languages, we used a language pair for which lin5 guistic resources are available in order to determine how close translation performance can get to a fully syntax-based system. Accordingly, we use Chiang’s hierarchical phrase based translation model (Chiang, 2007) as a base line, and the syntax-augmented MT model (Zollmann and Venugopal, 2006) as a ‘target line’, a model that would not be applicable for language pairs without linguistic resources. We perform PSCFG rule extraction and decoding using the open-source “SAMT” system (Venugopal and Zollmann, 2009), using the provided implementations for the hierarchical and syntax-augmented grammars. Apart from the language model, the lexical, phrasal, and (for the syntax grammar) labelconditioned features, and the rule, target word, and glue operation counters, Venugopal and Zollmann (2009) also provide bot"
P11-1001,E03-1009,0,0.0316051,"change algorithm (Kneser and Ney, 1993). Its objective function is maximizing the likelihood n Y P (wi |w1 , . . . , wi−1 ) i=1 P (wi |w1 , . . . , wi−1 ) ≈ p(c(wi )|wi−1 ) · p(wi |c(wi )) where c : V → {1, . . . , N } maps a word (type, not token) w to its class c(w), V is the vocabulary, and N the fixed number of classes, which has to be chosen a priori. We use the publicly available implementation MKCLS (Och, 1999) to train this model. As training data we use the respective side of the parallel training data for the translation system. We also experiment with the extension of this model by Clark (2003), who incorporated morphological information by imposing a Bayesian prior on the class mapping c, based on N individual distributions over strings, one for each word class. Each such distribution is a character-based hidden Markov model, thus encouraging the grouping of morphologically similar words into the same class. 4 Clustering phrase pairs directly using the K-means algorithm Even though we have only made use of the first and last words’ classes in the labeling methods described so far, the number of resulting grammar nonterminals quickly explodes. Using a scheme based on source and targ"
P11-1001,D09-1037,0,0.163162,"ord tags (albeit in a different manner and without the use of subcategorization information), but produces PSCFG rules for use in a parsing-based decoding system. Unsupervised synchronous grammar induction, apart from the contribution of Chiang (2005) discussed earlier, has been proposed by Wu (1997) for inversion transduction grammars, but as Chiang’s model only uses a single generic nonterminal label. Blunsom et al. (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and Cohn and Blunsom (2009) achieve the same for tree-to-string grammars, with encouraging results on small data. Our more humble approach treats the training sentences’ word alignments and phrase pairs, obtained from external modules, as ground truth and employs a straight-forward generalization of Chiang’s popular rule extraction approach to labeled phrase pairs, resulting in a PSCFG with multiple nonterminal labels. Our phrase pair clustering approach is similar in spirit to the work of Lin and Wu (2009), who use Kmeans to cluster (monolingual) phrases and use the resulting clusters as features in discriminative clas"
P11-1001,N04-1035,0,0.1701,"ive reordering phenomena that often occur across language pairs considered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). 1 While all of these techniques rely on wordalignments to suggest lexical relationships, they differ in the way in which they assign labels to nonterminal symbols of PSCFG rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use sourc"
P11-1001,P07-1037,0,0.0915198,"Missing"
P11-1001,P03-1054,0,0.00302809,"ly single-occurrences were removed. For parameter tuning, we use the L0 -regularized minimum-error-rate training tool provided by the SAMT toolkit. Each system is trained separately to adapt the parameters to its specific properties (size of nonterminal set, grammar complexity, features sparseness, reliance on the language model, etc.). The parallel training data comprises of 9.6M sentence pairs (206M Chinese and 228M English words). The source and target language parses for the syntax-augmented grammar, as well as the POS tags for our POS-based grammars were generated by the Stanford parser (Klein and Manning, 2003). The results are given in Table 1. Results for the Syntax system are consistent with previous results (Zollmann et al., 2008), indicating improvements over the hierarchical system. Our approach, using target POS tags (‘POS-tgt (no phr. s.)’), outperforms the hierarchical system on all three tests sets, and gains further improvements when accounting for phrase size (‘POS-tgt’). The latter approach is roughly on par with the corresponding Syntax system, slightly outperforming it on average, but not consistently across all test sets. The same is true for the ‘slow’ version (‘POS-tgt-slow’). The"
P11-1001,N03-1017,0,0.0389931,"bulary) TS , a target terminal set (target vocabulary) TT , a shared nonterminal set N and rules of the form: A → hγ, α, wi where • A ∈ N is a labeled nonterminal referred to as the left-hand-side of the rule, • γ ∈ (N ∪ TS )∗ is the source side of the rule, • α ∈ (N ∪ TT )∗ is the target side of the rule, • w ∈ [0, ∞) is a non-negative real-valued weight assigned to the rule; in our model, w is the product of features φi raised to the power of weight λi . Chiang (2005) learns a single-nonterminal PSCFG from a bilingual corpus by first identifying initial phrase pairs using the technique from Koehn et al. (2003), and then performing a generalization operation to generate phrase pairs with gaps, which can be viewed as PSCFG rules with generic ‘X’ nonterminal left-hand-sides and substitution sites. Bilingual features φi that judge the quality of each rule are estimated based on rule extraction frequency counts. 3 t1 · · · tn its corresponding target tag sequence. We convert each extracted phrase pair, represented by its source span hi, ji and target span hk, `i, into an initial rule Hard rule labeling from word classes We now describe a simple method of inducing a multi-nonterminal PSCFG from a paralle"
P11-1001,C10-1069,0,0.0121394,"feature space, allowing for the discovery of subtle semantic similarities in the phrases, but at much greater computational expense. Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type’s contexts by averaging the contexts of all its phrase instances. Nagata et al. (2006) present a reordering model for machine translation, and make use of clustered phrase pairs to cope with data sparseness in the model. They achieve the clustering by reducing phrases to their head words and then applying the MKCLS tool to these pseudo-words. Kuhn et al. (2010) cluster the phrase pairs of an SMT phrase table based on their co-occurrence counts and edit distances in order to arrive at semantically similar phrases for the purpose of phrase table smoothing. The clustering proceeds in a bottom-up fashion, gradually merging similar phrases while alternating back and forth between the two languages. 7 Conclusion and discussion In this work we proposed methods of labeling phrase pairs to create automatically learned PSCFG rules for machine translation. Crucially, our methods only rely on “shallow” lexical tags, either generated by POS taggers or by automat"
P11-1001,P09-1116,0,0.0282268,"induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and Cohn and Blunsom (2009) achieve the same for tree-to-string grammars, with encouraging results on small data. Our more humble approach treats the training sentences’ word alignments and phrase pairs, obtained from external modules, as ground truth and employs a straight-forward generalization of Chiang’s popular rule extraction approach to labeled phrase pairs, resulting in a PSCFG with multiple nonterminal labels. Our phrase pair clustering approach is similar in spirit to the work of Lin and Wu (2009), who use Kmeans to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task. Phrases are represented in terms of their contexts, which can be more than one word long; words within the phrase are not considered. Further, each context contributes one dimension per vocabulary word (not per word class as in our approach) to the feature space, allowing for the discovery of subtle semantic similarities in the phrases, but at much greater computational expense. Another distinction is that Lin"
P11-1001,P06-1077,0,0.0367147,"onsidered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). 1 While all of these techniques rely on wordalignments to suggest lexical relationships, they differ in the way in which they assign labels to nonterminal symbols of PSCFG rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006)"
P11-1001,W06-1606,0,0.0570476,"istical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). 1 While all of these techniques rely on wordalignments to suggest lexical relationships, they differ in the way in which they assign labels to nonterminal symbols of PSCFG rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006) directly extend the r"
P11-1001,D08-1022,0,0.0295268,"ned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006) directly extend the rule extraction procedure from Chiang (2005) to heuristically label any phrase pair based on target language parse trees. Label-based approaches have resulted in improvements in translation quality over the single X label approach (Zollmann et al., 2008; Mi and Huang, 2008); however, all the works cited here rely on stochastic parsers that have been trained on manually created syntactic treebanks. These treebanks are difficult and expensive to produce and exist for a limited set of languages only. In this work, we propose a labeling approach that is based merely on part-of-speech analysis of the source or target language (or even both). Towards the ultimate goal of building end-to-end machine translation systems without any human annotations, we also experiment with automatically inferred word classes using distributional clustering (Kneser and Ney, 1993). Since"
P11-1001,P06-1090,0,0.0191654,"a query classification task. Phrases are represented in terms of their contexts, which can be more than one word long; words within the phrase are not considered. Further, each context contributes one dimension per vocabulary word (not per word class as in our approach) to the feature space, allowing for the discovery of subtle semantic similarities in the phrases, but at much greater computational expense. Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type’s contexts by averaging the contexts of all its phrase instances. Nagata et al. (2006) present a reordering model for machine translation, and make use of clustered phrase pairs to cope with data sparseness in the model. They achieve the clustering by reducing phrases to their head words and then applying the MKCLS tool to these pseudo-words. Kuhn et al. (2010) cluster the phrase pairs of an SMT phrase table based on their co-occurrence counts and edit distances in order to arrive at semantically similar phrases for the purpose of phrase table smoothing. The clustering proceeds in a bottom-up fashion, gradually merging similar phrases while alternating back and forth between th"
P11-1001,E99-1010,0,0.0497547,"→ habe PRP+PRP1 gesehen |saw PRP+PRP1 Unsupervised word class assignment by clustering As an alternative to POS tags, we experiment with unsupervised word clustering methods based on the exchange algorithm (Kneser and Ney, 1993). Its objective function is maximizing the likelihood n Y P (wi |w1 , . . . , wi−1 ) i=1 P (wi |w1 , . . . , wi−1 ) ≈ p(c(wi )|wi−1 ) · p(wi |c(wi )) where c : V → {1, . . . , N } maps a word (type, not token) w to its class c(w), V is the vocabulary, and N the fixed number of classes, which has to be chosen a priori. We use the publicly available implementation MKCLS (Och, 1999) to train this model. As training data we use the respective side of the parallel training data for the translation system. We also experiment with the extension of this model by Clark (2003), who incorporated morphological information by imposing a Bayesian prior on the class mapping c, based on N individual distributions over strings, one for each word class. Each such distribution is a character-based hidden Markov model, thus encouraging the grouping of morphologically similar words into the same class. 4 Clustering phrase pairs directly using the K-means algorithm Even though we have only"
P11-1001,P02-1040,0,0.0849363,"om Partition. The Forgy method randomly chooses K observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds straight to step (ii). Forgy tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. As the resulting clusters looked similar, and Random Partition sometimes led to a high rate of empty clusters, we settled for Forgy. 5 Experiments We evaluate our approach by comparing translation quality, as evaluated by the IBM-BLEU (Papineni et al., 2002) metric on the NIST Chinese-to-English translation task using MT04 as development set to train the model parameters λ, and MT05, MT06 and MT08 as test sets. Even though a key advantage of our method is its applicability to resource-poor languages, we used a language pair for which lin5 guistic resources are available in order to determine how close translation performance can get to a fully syntax-based system. Accordingly, we use Chiang’s hierarchical phrase based translation model (Chiang, 2007) as a base line, and the syntax-augmented MT model (Zollmann and Venugopal, 2006) as a ‘target lin"
P11-1001,J97-3002,0,0.113885,"ertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. The supertags are also injected into the language model. Our approach also generates phrase labels and placeholders based on word tags (albeit in a different manner and without the use of subcategorization information), but produces PSCFG rules for use in a parsing-based decoding system. Unsupervised synchronous grammar induction, apart from the contribution of Chiang (2005) discussed earlier, has been proposed by Wu (1997) for inversion transduction grammars, but as Chiang’s model only uses a single generic nonterminal label. Blunsom et al. (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and Cohn and Blunsom (2009) achieve the same for tree-to-string grammars, with encouraging results on small data. Our more humble approach treats the training sentences’ word alignments and phrase pairs, obtained from external modules, as ground truth and employs a straight-forward generalization of Chia"
P11-1001,W06-3119,1,0.946757,"ce Carnegie Mellon University Pittsburgh, PA 15213, USA {zollmann,vogel+}@cs.cmu.edu Abstract In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chineseto-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available. 1 Introduction The Probabilistic Synchronous Context Free Grammar (PSCFG) formalism suggests an intuitive approach to model the long-distance and lexically sensitive reordering phenomena that often occur across language pairs considered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used t"
P11-1001,W10-3814,1,0.791321,"ounters, Venugopal and Zollmann (2009) also provide both the hierarchical and syntax-augmented grammars with a rareness penalty 1/ cnt(r), where cnt(r) is the occurrence count of rule r in the training corpus, allowing the system to learn penalization of low-frequency rules, as well as three indicator features firing if the rule has one, two unswapped, and two swapped nonterminal pairs, respectively.2 Further, to mitigate badly estimated PSCFG derivations based on low-frequency rules of the much sparser syntax model, the syntax grammar also contains the hierarchical grammar as a backbone (cf. Zollmann and Vogel (2010) for details and empirical analysis). We implemented our rule labeling approach within the SAMT rule extraction pipeline, resulting in comparable features across all systems. For all systems, we use the bottom-up chart parsing decoder implemented in the SAMT toolkit with a reordering limit of 15 source words, and correspondingly extract rules from initial phrase pairs of maximum source length 15. All rules have at most two nonterminal symbols, which must be non-consecutive on the source side, and rules must contain at least one source-side terminal symbol. The beam settings for the hierarchica"
P11-1001,C08-1144,1,0.957657,"FG rules from word-aligned (Brown et al., 1993) corpora, where all nonterminals share the same generic label X. In Galley et al. (2004) and Marcu et al. (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. (2006) use source language parse trees instead. Zollmann and Venugopal (2006) directly extend the rule extraction procedure from Chiang (2005) to heuristically label any phrase pair based on target language parse trees. Label-based approaches have resulted in improvements in translation quality over the single X label approach (Zollmann et al., 2008; Mi and Huang, 2008); however, all the works cited here rely on stochastic parsers that have been trained on manually created syntactic treebanks. These treebanks are difficult and expensive to produce and exist for a limited set of languages only. In this work, we propose a labeling approach that is based merely on part-of-speech analysis of the source or target language (or even both). Towards the ultimate goal of building end-to-end machine translation systems without any human annotations, we also experiment with automatically inferred word classes using distributional clustering (Kneser"
P11-2051,N06-1003,0,0.0423797,"ed sentence pairs. The filtered corpus is used for training phrase-based translation models, which can be used directly in translation tasks or combined with baseline models. Experimental results on ChineseEnglish machine translation tasks show an average improvement of 0.45 BLEU and 1.22 TER points across 5 different NIST test sets. 1 Introduction Statistical machine translation (SMT) relies on parallel corpus. Aside from collecting parallel corpus, we have seen interesting research on automatically generating corpus from existing resources. Typical examples are paraphrasing using bilingual (Callison-Burch et al., 2006) or monolingual (Quirk et al., 2004) data. In this paper, we propose a different methodology of generating additional parallel corpus. The basic idea of paraphrasing is to find alternative ways that convey the same information. In contrast, we propose to build new parallel sentences that convey different information, yet retain correct grammatical and semantic structures. The basic idea of the proposed method is to substitute source and target phrase pairs in a sentence pair with phrase pairs from other sentences. The problem is how to identify where a substitution should happen and which phra"
P11-2051,D08-1021,0,0.0234575,"ea of paraphrasing is to find alternative ways that convey the same information. In contrast, we propose to build new parallel sentences that convey different information, yet retain correct grammatical and semantic structures. The basic idea of the proposed method is to substitute source and target phrase pairs in a sentence pair with phrase pairs from other sentences. The problem is how to identify where a substitution should happen and which phrase pairs are valid candidates for the substitution. While syntactical constraints have been proven to helpful in identifying 294 good paraphrases (Callison-Burch, 2008), it is insufficient in our task because it cannot properly filter the candidates for the replacement. If we allow all the NPs to be replaced with other NPs, each sentence pair can generate huge number of new sentences. Instead, we resort to Semantic Role Labeling (Palmer et al., 2005) to provide more lexicalized and semantic constraints to select the candidates. The method only requires running SRL labeling on either side of the language pair, and that enables applications on low resource languages. Even with the SRL constraints, the generated corpus may still be large and noisy. Hence, we ap"
P11-2051,W08-0509,1,0.802428,"Missing"
P11-2051,P07-2045,0,0.00405796,"Missing"
P11-2051,J04-4002,0,0.0669779,"role constituent. SRL Substitution Rules Swapping phrase pairs that serve as the same semantic role of the same semantic frame can provide more combinations of words and phrases. Figure 1 shows an example. The phrase pair “新疆 伊犁 将 举行 → Xinjiang’s Yili will hold” would not be observed in the original corpus without substitution. In this paper, we call a tuple of semantic frame and semantic role a semantic signature. Two phrase pairs with the same semantic signature are considered valid substitutions of each other. The extraction of SSRs is similar to the wellknown phrase extraction algorithm (Och and Ney, 2004). The criteria of a phrase pair to be included in the SSR set are1 : • The phrase on side A must cover a whole semantic role constituent, and it must not contain 1 We call the language which has SRL labels side A, and the other language side B. 295 Utilizing these rules, we can perform the sentence generation process. For each semantic structure of each sentence,2 we determine the phrase pair to be replaced by the same criteria as mention above, and search for suitable SSRs with the same semantic signature. Finally, we replace the original phrases with the source and target side phrases given"
P11-2051,J05-1004,0,0.258654,"ce and target phrase pairs in a sentence pair with phrase pairs from other sentences. The problem is how to identify where a substitution should happen and which phrase pairs are valid candidates for the substitution. While syntactical constraints have been proven to helpful in identifying 294 good paraphrases (Callison-Burch, 2008), it is insufficient in our task because it cannot properly filter the candidates for the replacement. If we allow all the NPs to be replaced with other NPs, each sentence pair can generate huge number of new sentences. Instead, we resort to Semantic Role Labeling (Palmer et al., 2005) to provide more lexicalized and semantic constraints to select the candidates. The method only requires running SRL labeling on either side of the language pair, and that enables applications on low resource languages. Even with the SRL constraints, the generated corpus may still be large and noisy. Hence, we apply an additional filtering stage on the generated corpus. We used an SVM classifier with features derived from standard phrase based translation models and bilingual language models to identify high quality sentence pairs, and use these sentence pairs in the SMT training. In the remai"
P11-2051,N04-1030,0,0.158162,"Missing"
P11-2051,W04-3219,0,0.0931502,"Missing"
P11-2066,J93-2003,0,0.0316767,"nd discriminative learning. We also propose a variant of the grammar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments. Figure 1: BTG rules. [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar 379 (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment."
P11-2066,W07-0403,0,0.0698784,"nd real-world experiments. Figure 1: BTG rules. [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar 379 (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity problem. Calculating and saving those derivations will slow down the parsing speed significantly. Furthermore, spurious deriv"
P11-2066,P96-1011,0,0.587138,"tion experiments. Without initializing by phrases extracted from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al., 2009), we further reduced AER on the test set to 12.25. An average improvement of 0.52 BLEU (Papineni et al., 2002) score and 2.05 TER (Snover et al., 2006) score over 5 test sets for a typical phrase-based translation system, Moses (Koehn et al., 2003), validated the effectiveness of our experiments. 5 Conclusion Great efforts have been made in reducing spurious ambiguities in parsing combinatory categorial grammar (Karttunen, 1986; Eisner, 1996). However, to our knowledge, we give the first detailed analysis on spurious ambiguity of word alignment. Empirical comparisons between different grammars also validates our analysis. This paper makes its own contribution in demonstrating that spurious ambiguity has a negative impact on discriminative learning. We will continue working on this line of research and improve our discriminative learning model in the future, for example, by adding more phrase level features. It is worth noting that the definition of spurious ambiguity actually varies for different tasks. In some cases, e.g. bilingu"
P11-2066,P09-1104,0,0.330817,"nts. Figure 1: BTG rules. [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar 379 (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity problem. Calculating and saving those derivations will slow down the parsing speed significantly. Furthermore, spurious derivations may fill up the"
P11-2066,N03-1017,0,0.0971962,"Missing"
P11-2066,P10-1033,0,0.0590548,"s. [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar 379 (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity problem. Calculating and saving those derivations will slow down the parsing speed significantly. Furthermore, spurious derivations may fill up the n-best list and sup"
P11-2066,J03-1002,0,0.0152579,"Missing"
P11-2066,P02-1040,0,0.0799889,"Missing"
P11-2066,2006.amta-papers.25,0,0.0393175,"Missing"
P11-2066,J97-3002,0,0.84735,"mmar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments. Figure 1: BTG rules. [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar 379 (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity pr"
P11-2066,P05-1059,0,0.0212389,"mars in both synthetic and real-world experiments. Figure 1: BTG rules. [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al., 1993). Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al., 2009; Liu et al., 2010). ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. The most general case of the ITG family is the bracketing transduction grammar 379 (BTG, Figure 1), which has only one nonterminal symbol. Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. This is often referred to as the spurious ambiguity problem. Calculating and saving those derivations will slow down the parsing speed significantly. Furth"
P11-2066,N06-1033,0,0.0278782,"will contain at least one aligned word-pair. Comparatively, the grammar in Liu et al. (2010) uses a leftbranching manner. It may generate more spans that only contain null-aligned words, which makes it less efficient than ours. Theorem 2. LGFN has a unique derivation for each ITG alignment, i.e. LGFN is non-spurious. Proof: Derived directly from Definition 4, Theorem 1 and Lemma 1. 4 4.1 Experiments Synthetic Experiments We automatically generated 1000 fully aligned ITG alignments of length 20 by generating random permutations first and checking ITG constraints using a linear time algorithm (Zhang et al., 2006). Sparser alignments were generated by random removal of alignment links according to a given null-aligned word ratio. Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al., 2009), LiuG (Liu et al., 2010) and LGFN (Section 3.3). Table 1 shows the average number of derivations per alignment generated under LG and HaG. The number of derivations produced by LG increased dramatically because LG has no restrictions on nullaligned word attachment. HaG also produced a large number of spurious derivations as the number of null-aligned words increased. Both LiuG"
P13-1156,W09-0434,0,0.0577101,"rd count, and translation model features. Phrase-based systems often use a lexical reordering model in addition to the distance cost feature. Introduction Phrase-based and tree-based translation model are the two main streams in state-of-the-art machine translation. The tree-based translation model, by using a synchronous context-free grammar formalism, can capture longer reordering between source and target language. Yet, tree-based translation often underperforms phrase-based translation in language pairs with short range reordering such as Arabic-English translation (Zollmann et al., 2008; Birch et al., 2009). We follow Koehn et al. (2003) for our phrasebased system and Chiang (2005) for our Hiero system. In both systems, the translation of a source sentence f is the target sentence e∗ that maximizes a linear combination of features and weights: X he∗ , a∗ i = argmax λm hm (e, f , a) . (1) he,ai∈H(f ) m∈M The biggest difference in a Hiero system and a phrase-based system is in how the reordering is modeled. In the Hiero system, the reordering decision is encoded in weighted translation rules, determined by nonterminal mappings. For example, the rule X → ne X1 pas ; not X1 : w indicates the transla"
P13-1156,D09-1021,0,0.0189329,"o English i don’t speak french in Figure 1. The Hiero decoder translates the sentence using a derivation of three rules: • r1 = X → parle ; speak. 1. A limited hypothesis space because the synchronous context-free grammar is not applicable to non-projective dependencies. 2. It does not have the expressive lexicalized reordering model and distance cost features of the phrase-based system. When comparing phrase-based and Hiero translation models, most of previous work on treebased translation addresses its limited hypothesis space problem. Huck et al. (2012) add new rules into the Hiero system, Carreras and Collins (2009) apply the tree adjoining grammar formalism to allow highly flexible reordering. On the other hand, the Hiero model has the advantage of capturing long distance and structure reordering. Galley and Manning (2010) extend phrase-based translation by allowing gaps within phrases such as hne . . . pas, noti, so the decoder still has the discriminative reordering features of phrase-based, but also uses on average longer phrases. However, these phrase pairs with gaps do not capture structure reordering as do Hiero rules with nonterminal mappings. For example, the rule X → ne X1 pas ; not X1 explicit"
P13-1156,N10-2003,0,0.0374603,"Missing"
P13-1156,D08-1024,0,0.0724655,"of features in equation 1. We denote ph(a) as the corresponding phrase-based path of a Hiero derivation a, and MP hH as the indexes of phrase-based features currently not applicable to the Hiero decoder. Our Phrasal-Hiero decoder seeks to find the translation, which optimizes:  X he∗ , a∗ i = argmax λm hm (e, f , a) + he,ai∈Htr (f ) + X m∈MH m0 ∈MP hH  λm0 hm0 (e, f , ph(a)) . We focus on improving the modelling of reordering within Hiero and include discriminative reordering features (Tillmann, 2004) and a distance cost feature, both of which are not modeled in the original Hiero system. Chiang et al. (2008) added structure distortion features into their decoder and showed improvements in their ChineseEnglish experiment. To our knowledge, PhrasalHiero is the first system, which directly integrates phrase-based and Hiero features into one model. 1588 Figure 1: Example of French-English Hiero Translation on the left and its corresponding discontinuous phrase-based translation on the right. Rules Alignments r1 = X → parle ; speak. 0-0 r3 = X → Je X1 le F rancais ; I X1 F rench 0-0 1-1 3-2 r2 = X → ne X1 pas ; don0 t X1 . 0-0 1-1 2-0 r4 = X → je X1 le X2 ; i X1 X2 0-0 1-1 3-2 Phrase pairs & nontermin"
P13-1156,P05-1033,0,0.840209,"reordering model in addition to the distance cost feature. Introduction Phrase-based and tree-based translation model are the two main streams in state-of-the-art machine translation. The tree-based translation model, by using a synchronous context-free grammar formalism, can capture longer reordering between source and target language. Yet, tree-based translation often underperforms phrase-based translation in language pairs with short range reordering such as Arabic-English translation (Zollmann et al., 2008; Birch et al., 2009). We follow Koehn et al. (2003) for our phrasebased system and Chiang (2005) for our Hiero system. In both systems, the translation of a source sentence f is the target sentence e∗ that maximizes a linear combination of features and weights: X he∗ , a∗ i = argmax λm hm (e, f , a) . (1) he,ai∈H(f ) m∈M The biggest difference in a Hiero system and a phrase-based system is in how the reordering is modeled. In the Hiero system, the reordering decision is encoded in weighted translation rules, determined by nonterminal mappings. For example, the rule X → ne X1 pas ; not X1 : w indicates the translation of the phrase between ne and pas to be after the English word not with"
P13-1156,J07-2003,0,0.355153,"different rules can have the same lexical phrase pairs. In Phrasal-Hiero, each lexical phrase pair is only generated once for a sentence. Look at the example of the training sentence pair in Figure 2, the rule X → je ; I spanning (0 . . . 1, 0 . . . 1) and the rule X → je X1 ; I X1 spanning (0 . . . 3, 0 . . . 2) are both sharing the same lexical phrase pair (je, i) spanning (0 . . . 1, 0 . . . 1). But Phrasal-Hiero only gen1590 erates (je, i) once for the sentence. Phrase pairs are generated together with phrase-based reordering orientations to build lexicalized reordering table. 3 Decoding Chiang (2007) applied bottom up chart parsing to parse the source sentence and project on the target side for the best translation. Each chart cell [X, i, j, r] indicates a subtree with rule r at the root covers the translation of the i-th word upto the j-th word of the source sentence. We extend the chart parsing, mapping the subtree to the equivalent discontinuous phrase-based path and includes phrasebased features to the log-linear model. In Phrasal-Hiero, each chart cell [X, i, j, r] also stores the first phrase pair and the last phrase pair of the phrase-based translation path covered the ith to the j"
P13-1156,D08-1089,0,0.137674,"first phrase pair and the last phrase pair of cell X2 . The first phrase pair of X2 is (ne . . . pas, don0 t), the last phrase pair is also the last phrase pair of cell X1 which is (parle, speak). Similarly, finding the phrase-based path and calculate its distortion features in the chart cell X3 include calculate the feature values for moving from the phrase pair (je, I) to the first phrase pair of chart cell X2 and also from last phrase pair of chart cell X2 to the phrase pair (le f ranc¸aise, f rench). 4 Experiment Results In all experiments we use phrase-orientation lexicalized reordering (Galley and Manning, 2008)2 which models monotone, swap, discontinuous orientations from both reordering with previous phrase pair and with the next phrase pair. There are total six features in lexicalized reordering model. We will report the impact of integrating phrasebased features into Hiero systems for three language pairs: Arabic-English, Chinese-English and German-English. 4.1 System Setup We are using the following three baselines: • Phrase-based without lexicalized reodering features. (PB+nolex) • Phrase-based with lexicalized reordering features.(PB+lex) • Hiero system with all rules extracted from training d"
P13-1156,N10-1140,0,0.321369,"distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, find its corresponding discontinuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for ArabicEnglish, Chinese-English and GermanEnglish translation. 1 • H (f ) is the hypothesis space of the sentence f . We denote Hph (f ) as the phrase-based hypothesis space of f and Htr (f ) as its treebased hypothesis space. Galley and Manning (2010) point out that due to the hard constraints of rule combination, the tree-based system does not have the same excessive hypothesis space as the phrase-based system. • M is the set of feature indexes used in the decoder. Many features are shared between phrase-based and tree-based systems including language model, word count, and translation model features. Phrase-based systems often use a lexical reordering model in addition to the distance cost feature. Introduction Phrase-based and tree-based translation model are the two main streams in state-of-the-art machine translation. The tree-based t"
P13-1156,2012.eamt-1.66,0,0.0165405,"of the French sentence je ne parle pas le franc¸aise into English i don’t speak french in Figure 1. The Hiero decoder translates the sentence using a derivation of three rules: • r1 = X → parle ; speak. 1. A limited hypothesis space because the synchronous context-free grammar is not applicable to non-projective dependencies. 2. It does not have the expressive lexicalized reordering model and distance cost features of the phrase-based system. When comparing phrase-based and Hiero translation models, most of previous work on treebased translation addresses its limited hypothesis space problem. Huck et al. (2012) add new rules into the Hiero system, Carreras and Collins (2009) apply the tree adjoining grammar formalism to allow highly flexible reordering. On the other hand, the Hiero model has the advantage of capturing long distance and structure reordering. Galley and Manning (2010) extend phrase-based translation by allowing gaps within phrases such as hne . . . pas, noti, so the decoder still has the discriminative reordering features of phrase-based, but also uses on average longer phrases. However, these phrase pairs with gaps do not capture structure reordering as do Hiero rules with nontermina"
P13-1156,N03-1017,0,0.015592,"features. Phrase-based systems often use a lexical reordering model in addition to the distance cost feature. Introduction Phrase-based and tree-based translation model are the two main streams in state-of-the-art machine translation. The tree-based translation model, by using a synchronous context-free grammar formalism, can capture longer reordering between source and target language. Yet, tree-based translation often underperforms phrase-based translation in language pairs with short range reordering such as Arabic-English translation (Zollmann et al., 2008; Birch et al., 2009). We follow Koehn et al. (2003) for our phrasebased system and Chiang (2005) for our Hiero system. In both systems, the translation of a source sentence f is the target sentence e∗ that maximizes a linear combination of features and weights: X he∗ , a∗ i = argmax λm hm (e, f , a) . (1) he,ai∈H(f ) m∈M The biggest difference in a Hiero system and a phrase-based system is in how the reordering is modeled. In the Hiero system, the reordering decision is encoded in weighted translation rules, determined by nonterminal mappings. For example, the rule X → ne X1 pas ; not X1 : w indicates the translation of the phrase between ne a"
P13-1156,P07-2045,0,0.00677607,"from both reordering with previous phrase pair and with the next phrase pair. There are total six features in lexicalized reordering model. We will report the impact of integrating phrasebased features into Hiero systems for three language pairs: Arabic-English, Chinese-English and German-English. 4.1 System Setup We are using the following three baselines: • Phrase-based without lexicalized reodering features. (PB+nolex) • Phrase-based with lexicalized reordering features.(PB+lex) • Hiero system with all rules extracted from training data. (Hiero) We use Moses phrase-based and chart decoder (Koehn et al., 2007) for the baselines. The score difference between PB+nolex and PB+lex results indicates the impact of lexicalized reordering features on phrase-based system. In Phrasal-Hiero we 2 Galley and Manning (2008) introduce three orientation models for lexicalized reordering: word-based, phrase-based and hierarchical orientation model. We apply phrase-based orientation in all experiment using lexicalized reordering. 1591 Chart Cell X1 : 2 . . . 2 Rule’s phrase pairs & NTs (parle, speak) X2 : 1 . . . 3 (ne . . . pas, don0 t) ; X1 X3 : 0 . . . 5 (Je ; I) ; X2 ; (le F ranc¸ais; f rench) Distance ∅ 2 + dis"
P13-1156,N04-4026,0,0.321477,"by incorporating phrase-based features. Let us revisit machine translation’s loglinear model combination of features in equation 1. We denote ph(a) as the corresponding phrase-based path of a Hiero derivation a, and MP hH as the indexes of phrase-based features currently not applicable to the Hiero decoder. Our Phrasal-Hiero decoder seeks to find the translation, which optimizes:  X he∗ , a∗ i = argmax λm hm (e, f , a) + he,ai∈Htr (f ) + X m∈MH m0 ∈MP hH  λm0 hm0 (e, f , ph(a)) . We focus on improving the modelling of reordering within Hiero and include discriminative reordering features (Tillmann, 2004) and a distance cost feature, both of which are not modeled in the original Hiero system. Chiang et al. (2008) added structure distortion features into their decoder and showed improvements in their ChineseEnglish experiment. To our knowledge, PhrasalHiero is the first system, which directly integrates phrase-based and Hiero features into one model. 1588 Figure 1: Example of French-English Hiero Translation on the left and its corresponding discontinuous phrase-based translation on the right. Rules Alignments r1 = X → parle ; speak. 0-0 r3 = X → Je X1 le F rancais ; I X1 F rench 0-0 1-1 3-2 r2"
P13-1156,W06-3119,0,0.158584,"ng discontinuous phrase-based translation on the right. Rules Alignments r1 = X → parle ; speak. 0-0 r3 = X → Je X1 le F rancais ; I X1 F rench 0-0 1-1 3-2 r2 = X → ne X1 pas ; don0 t X1 . 0-0 1-1 2-0 r4 = X → je X1 le X2 ; i X1 X2 0-0 1-1 3-2 Phrase pairs & nonterminals (parle ; speak) (ne . . . pas ; don0 t) ; X1 (Je ; I) ; X1 ; (le F rancais; f rench) Not Applicable Table 1: Rules and their sequences of phrase pairs and nonterminals Previous work has attempted to weaken the context free assumption of the synchronous context free grammar formalism, for example using syntactic non-terminals (Zollmann and Venugopal, 2006). Our approach can be viewed as applying soft context constraint to make the probability of substituting a nonterminal by a subtree depending on the corresponding phrase-based reordering features. In the next section, we explain the model in detail. 2 Phrasal-Hiero Model Phrasal-Hiero maps a Hiero derivation into a discontinuous phrase-based translation path by the following two steps: 1. Training: Represent each rule as a sequence of phrase pairs and nonterminals. 2. Decoding: Use the rules’ sequences of phrase pairs and nonterminals to find the corresponding phrase-based path of a Hiero deri"
P13-1156,C08-1144,0,0.0395512,"ding language model, word count, and translation model features. Phrase-based systems often use a lexical reordering model in addition to the distance cost feature. Introduction Phrase-based and tree-based translation model are the two main streams in state-of-the-art machine translation. The tree-based translation model, by using a synchronous context-free grammar formalism, can capture longer reordering between source and target language. Yet, tree-based translation often underperforms phrase-based translation in language pairs with short range reordering such as Arabic-English translation (Zollmann et al., 2008; Birch et al., 2009). We follow Koehn et al. (2003) for our phrasebased system and Chiang (2005) for our Hiero system. In both systems, the translation of a source sentence f is the target sentence e∗ that maximizes a linear combination of features and weights: X he∗ , a∗ i = argmax λm hm (e, f , a) . (1) he,ai∈H(f ) m∈M The biggest difference in a Hiero system and a phrase-based system is in how the reordering is modeled. In the Hiero system, the reordering decision is encoded in weighted translation rules, determined by nonterminal mappings. For example, the rule X → ne X1 pas ; not X1 : w"
P13-2003,W08-0304,0,0.162532,"8 Lengths Neg Ref 229.0 52.5 48.5 48.70 48.4 48.9 47.6 48.4 47.8 48.6 48.0 48.7 48.0 48.7 47.9 48.6 TEST(tune:full) BLEU+1 Avg. for 3 reruns Pos Neg BLEU StdDev 52.2 2.8 47.80 0.052 47.7 42.9 47.59 0.114 47.5 43.6 47.62 0.091 47.8 43.6 47.44 0.070 47.9 43.6 47.48 0.046 47.7 43.1 47.64 0.090 47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Larg"
P13-2003,2005.iwslt-1.8,0,0.132545,"Missing"
P13-2003,2011.mtsummit-papers.1,0,0.292832,"3 reruns Pos Neg BLEU StdDev 52.2 2.8 47.80 0.052 47.7 42.9 47.59 0.114 47.5 43.6 47.62 0.091 47.8 43.6 47.44 0.070 47.9 43.6 47.48 0.046 47.7 43.1 47.64 0.090 47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et al., 2012). However, none of this work has focuse"
P13-2003,P07-2045,0,0.0178806,"Missing"
P13-2003,N12-1047,0,0.420009,"e of Figure 2: Example reference translation and hyto the affect that the of some is the with ] us our to the affect that the with ] us our of the in baker , the cook , the on and the , the we know , pothesis translations after iterations 1, 3 and 4. has are in the heaven of to the affect that the of weakness of @-@ Ittihad @-@ Al the force , to The last two hypotheses are monsters. Figure 2 shows the translations after iterations 1, 3 and 4; the last two are monsters. The monster at iteration 3 is potentially useful, but that at iteration 4 is clearly unsuitable as a negative example. 1 See (Cherry and Foster, 2012) for details on objectives. Also, using PRO to initialize MERT, as implemented in Moses, yields 46.52 BLEU and monsters, but using MERT to initialize PRO yields 47.55 and no monsters. 2 13 3 Slaying Monsters: Theory Cut-offs. A cut-off is a deterministic rule that filters out pairs that do not comply with some criteria. We experiment with a maximal cut-off on (a) the difference in BLEU+1 scores and (b) the difference in lengths. These are relative cut-offs because they refer to the pair, but absolute cut-offs that apply to each of the elements in the pair are also possible (not explored here)."
P13-2003,D08-1024,0,0.694161,"itable for learning: they are (i) much longer than the respective positive examples and the references, and (ii) have very low BLEU+1 scores compared to the positive examples and in absolute terms. The low BLEU+1 means that PRO effectively has to learn from positive examples only. Once Upon a Time... For years, the standard way to do statistical machine translation parameter tuning has been to use minimum error-rate training, or MERT (Och, 2003). However, as researchers started using models with thousands of parameters, new scalable optimization algorithms such as MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have emerged. As these algorithms are relatively new, they are still not quite well understood, and studying their properties is an active area of research. For example, Nakov et al. (2012) have pointed out that PRO tends to generate translations that are consistently shorter than desired. They have blamed this on inadequate smoothing in PRO’s optimization objective, namely sentencelevel BLEU+1, and they have addressed the problem using more sensible smoothing. We wondered whether the issue could be partially relieved simply by tuning on longer sentences, for w"
P13-2003,N04-1022,0,0.20397,"Missing"
P13-2003,N09-1025,0,0.176433,"47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et al., 2012). However, none of this work has focused on monsters. Reasons (i) and (ii) arguably also apply to stochastic sampling of differentials (for BLEU+1 or for length), which fails to kill the monsters, m"
P13-2003,C08-1074,0,0.152708,".70 48.4 48.9 47.6 48.4 47.8 48.6 48.0 48.7 48.0 48.7 47.9 48.6 TEST(tune:full) BLEU+1 Avg. for 3 reruns Pos Neg BLEU StdDev 52.2 2.8 47.80 0.052 47.7 42.9 47.59 0.114 47.5 43.6 47.62 0.091 47.8 43.6 47.44 0.070 47.9 43.6 47.48 0.046 47.7 43.1 47.64 0.090 47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained w"
P13-2003,P11-2031,0,0.0884574,"0 48.7 47.9 48.6 TEST(tune:full) BLEU+1 Avg. for 3 reruns Pos Neg BLEU StdDev 52.2 2.8 47.80 0.052 47.7 42.9 47.59 0.114 47.5 43.6 47.62 0.091 47.8 43.6 47.44 0.070 47.9 43.6 47.48 0.046 47.7 43.1 47.64 0.090 47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et"
P13-2003,C12-1121,1,0.824561,"Missing"
P13-2003,P03-1021,0,0.107668,"examples that are comparable to the positive ones. Instead, tuning on long sentences quickly introduces monsters, i.e., corrupted negative examples that are unsuitable for learning: they are (i) much longer than the respective positive examples and the references, and (ii) have very low BLEU+1 scores compared to the positive examples and in absolute terms. The low BLEU+1 means that PRO effectively has to learn from positive examples only. Once Upon a Time... For years, the standard way to do statistical machine translation parameter tuning has been to use minimum error-rate training, or MERT (Och, 2003). However, as researchers started using models with thousands of parameters, new scalable optimization algorithms such as MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have emerged. As these algorithms are relatively new, they are still not quite well understood, and studying their properties is an active area of research. For example, Nakov et al. (2012) have pointed out that PRO tends to generate translations that are consistently shorter than desired. They have blamed this on inadequate smoothing in PRO’s optimization objective, namely sentencelevel BLEU+"
P13-2003,W09-0439,0,0.0977443,"Missing"
P13-2003,P08-2030,0,0.146958,"Missing"
P13-2003,N12-1023,0,0.695369,"Missing"
P13-2003,P12-1002,0,0.292614,"al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et al., 2012). However, none of this work has focused on monsters. Reasons (i) and (ii) arguably also apply to stochastic sampling of differentials (for BLEU+1 or for length), which fails to kill the monsters, maybe because it gives them some probability of being selected by design. To alleviate this, we test the above settings with random acceptance. 4.3 Random Acceptance Table 4 shows the results for accepting training pairs for PRO uniformly at random. To eliminate possible biases, we also removed the min=0.05 BLEU+1 selection criterion. Surprisingly, this setup effectively eliminated the monster proble"
P13-2003,W11-2123,0,0.0476342,"Missing"
P13-2003,D11-1125,0,0.655498,"and can cause testing BLEU to drop by several points absolute. We propose several effective ways to address the problem, using length- and BLEU+1based cut-offs, outlier filters, stochastic sampling, and random acceptance. The best of these fixes not only slay and protect against monsters, but also yield higher stability for PRO as well as improved testtime BLEU scores. Thus, we recommend them to anybody using PRO, monsterbeliever or not. 1 2 Monsters, Inc. PRO uses pairwise ranking optimization, where the learning task is to classify pairs of hypotheses into correctly or incorrectly ordered (Hopkins and May, 2011). It searches for a vector of weights w such that higher evaluation metric scores correspond to higher model scores and vice versa. More formally, PRO looks for weights w such that g(i, j) &gt; g(i, j 0 ) ⇔ hw (i, j) &gt; hw (i, j 0 ), where g is a local scoring function (typically, sentencelevel BLEU+1) and hw are the model scores for a given input sentence i and two candidate hypotheses j and j 0 that were obtained using w. If g(i, j) &gt; g(i, j 0 ), we will refer to j and j 0 as the positive and the negative example in the pair. Learning good parameter values requires negative examples that are com"
P13-2003,D07-1080,0,0.412307,"examples that are unsuitable for learning: they are (i) much longer than the respective positive examples and the references, and (ii) have very low BLEU+1 scores compared to the positive examples and in absolute terms. The low BLEU+1 means that PRO effectively has to learn from positive examples only. Once Upon a Time... For years, the standard way to do statistical machine translation parameter tuning has been to use minimum error-rate training, or MERT (Och, 2003). However, as researchers started using models with thousands of parameters, new scalable optimization algorithms such as MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have emerged. As these algorithms are relatively new, they are still not quite well understood, and studying their properties is an active area of research. For example, Nakov et al. (2012) have pointed out that PRO tends to generate translations that are consistently shorter than desired. They have blamed this on inadequate smoothing in PRO’s optimization objective, namely sentencelevel BLEU+1, and they have addressed the problem using more sensible smoothing. We wondered whether the issue could be partially relieved simply by tuning on l"
P13-2003,N03-1017,0,0.0496158,"Missing"
P17-2095,P10-1048,1,0.856786,"ub-word segmentation based on BPE, and iii) two variants of character-based segmentation. We first map each source word to its corresponding segments (depending on the segmentation scheme), embed all segments of a word in vector space and feed them one-by-one to an encoder-decoder model. See Figure 1 for illustration. 2.1 Figure 1: Segmentation approaches for the word “b$rhm” “ ÑëQå.”; the blue vectors indicate the embedding(s) used before the encoding layer. 2.3 Morphological Segmentation Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al., 2014). Ling et al. (2016) used character embeddings to address the OOV word problem. We explored them as an alternative to morphological segmentation. Their advantage is that character embeddings do not require any complicated pre- and post-processing step other than segmenting words into characters. The fully character-level encoder treats the source sentence as a sequence of letters, encoding each letter (including white-space) in the LSTM encoder (see Figure 1). The decoding may follow identical settings. We restricted the characte"
P17-2095,D11-1033,0,0.0328528,"ng several segmentation strategies. 3 Experiments In the following, we describe the data and system settings and later present the results of machine translation and POS tagging. LSTM in the (bidirectional) encoder and the decoder, with a size of 500. We limit the sentence length to 100 for MORPH, UNSEG, BPE, cCNN, and 500 for CHAR experiments. The source and target vocabularies are limited to 50k each. 3.1 3.2 Settings Data The MT systems were trained on 1.2 Million sentences, a concatenation of TED corpus (Cettolo et al., 2012), LDC NEWS data, QED (Guzm´an et al., 2013) and an MML-filtered (Axelrod et al., 2011) UN corpus.1 We used dev+test10 for tuning and tst11-14 for testing. For EnglishArabic, outputs were detokenized using MADA detokenizer. Before scoring the output, we normalized them and reference translations using the QCRI normalizer (Sajjad et al., 2013). Machine Translation Results Table 1 presents MT results using various segmentation strategies. Compared to the UNSEG system, the MORPH system2 improved translation quality by 4.6 and 1.6 BLEU points in Ar-to-En and Ento-Ar systems, respectively. The results also improved by up to 3 BLEU points for cCNN and CHAR systems in the Ar-to-En dire"
P17-2095,E14-4029,1,0.887411,"acter-based segmentation. We first map each source word to its corresponding segments (depending on the segmentation scheme), embed all segments of a word in vector space and feed them one-by-one to an encoder-decoder model. See Figure 1 for illustration. 2.1 Figure 1: Segmentation approaches for the word “b$rhm” “ ÑëQå.”; the blue vectors indicate the embedding(s) used before the encoding layer. 2.3 Morphological Segmentation Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al., 2014). Ling et al. (2016) used character embeddings to address the OOV word problem. We explored them as an alternative to morphological segmentation. Their advantage is that character embeddings do not require any complicated pre- and post-processing step other than segmenting words into characters. The fully character-level encoder treats the source sentence as a sequence of letters, encoding each letter (including white-space) in the LSTM encoder (see Figure 1). The decoding may follow identical settings. We restricted the character-level representation to the Arabic side of the parallel corpus"
P17-2095,C96-1017,0,0.0492124,"ranslation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance. 1 Introduction Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, ‘‘wktAbnA” “ AJK . AJ»ð” (gloss: and our book) is decomposed into its stem H AJ» + ð”. and affixes as: “w+ ktAb +nA” “ AK+ . 601 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 601–607 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2095 2 Segmentation Approaches We experimente"
P17-2095,P17-1080,1,0.841253,"ing a CNN over characters. The embedding are then provided to the encoder as input. The intuition is that the character-based word embedding should be able to learn the morphological phenomena a word inherits. Compared to fully characterlevel encoding, the encoder gets word-level embeddings as in the case of unsegmented words (see Figure 1). However, the word embedding is intuitively richer than the embedding learned over unsegmented words because of the convolution over characters. The method was previously shown to help neural MT (Belinkov and Glass, 2016; Costa-juss`a and Fonollosa, 2016). Belinkov et al. (2017) also showed character-based representations learned using a CNN to be superior, at learning word morphology, than their word-based counter-parts. However, they did not compare these against BPE-based segmentation. We use character-CNN to aid Arabic word segmentation. 602 # SEG tst11 Arabic-to-English tst12 tst13 tst14 AVG. tst11 English-to-Arabic tst12 tst13 tst14 UNSEG 25.7 28.2 27.3 23.9 26.3 15.8 17.1 18.1 15.5 16.6 MORPH cCNN CHAR BPE 29.2 29.0 28.8 29.7 33.0 32.0 31.8 32.5 32.9 32.5 32.5 33.6 28.3 28.0 27.8 28.4 30.9 30.3 30.2 31.1 16.5 14.3 15.3 17.5 18.8 12.8 17.1 18.0 20.4 13.6 18.0 2"
P17-2095,fishel-kirik-2010-linguistically,0,0.0293358,"ntext). The analyses are provided with the original text to a Feature Modeling component that applies an SVM and a language model to make predictions, which are scored by an Analysis Ranking component. Farasa on the other hand is a light weight segmenter, which ignores context and instead uses a variety of features and lexicons for segmentation. 2.2 Character-level Encoding Data Driven Sub-word Units A number of data-driven approaches have been proposed that learn to segment words into smaller units from data (Demberg, 2007; Sami Virpioja and Kurimo, 2013) and shown to improve phrasebased MT (Fishel and Kirik, 2010; Stallard et al., 2012). Recently, with the advent of neural MT, a few sub-word-based techniques have been proposed that segment words into smaller units to tackle the limited vocabulary and unknown word problems (Sennrich et al., 2016; Wu et al., 2016). In this work, we explore Byte-Pair Encoding (BPE), a data compression algorithm (Gage, 1994) as an alternative to morphological segmentation of Arabic. BPE splits words into symbols (a sequence of characters) and then iteratively replaces the most frequent symbols with their merged variants. In essence, frequent character n-gram sequences wil"
P17-2095,A00-1031,0,0.0614725,"this problem is; at test time, BPE is applied to those words only which were known to the full vocabulary of the training corpus. In this way, the sub-word units created by BPE for the word are already seen in a similar context during training and the model has learned to translate them correctly. The downside of this method is that it limits BPE’s power to segment unknown words to their correct sub-word units and outputs them as UNK in translation. 3.3 We also experimented with the aforementioned segmentation strategies for the task of Arabic POS tagging. Probabilistic taggers like HMMbased (Brants, 2000) and sequence learning models like CRF (Lafferty et al., 2001) consider previous words and/or tags to predict the tag of the current word. We mimic a similar setting but in a sequence-to-sequence learning framework. Figure 3 describes a step by step procedure to train a neural encoder-decoder tagger. Consider an Arabic phrase “klm >SdqA}k b$rhm” Discussion: Though BPE performed well for machine translation, there are a few reservations that we would like to discuss here. Since the main goal of the algorithm is to compress data and segmentation comes as a by-product, it often produces different"
P17-2095,2013.iwslt-papers.2,1,0.905117,"Missing"
P17-2095,2012.eamt-1.60,0,0.0196508,"3.6 18.0 20.0 17.2 12.6 15.3 16.6 18.2 13.3 16.4 18.0 AVG. Table 1: Results of comparing several segmentation strategies. 3 Experiments In the following, we describe the data and system settings and later present the results of machine translation and POS tagging. LSTM in the (bidirectional) encoder and the decoder, with a size of 500. We limit the sentence length to 100 for MORPH, UNSEG, BPE, cCNN, and 500 for CHAR experiments. The source and target vocabularies are limited to 50k each. 3.1 3.2 Settings Data The MT systems were trained on 1.2 Million sentences, a concatenation of TED corpus (Cettolo et al., 2012), LDC NEWS data, QED (Guzm´an et al., 2013) and an MML-filtered (Axelrod et al., 2011) UN corpus.1 We used dev+test10 for tuning and tst11-14 for testing. For EnglishArabic, outputs were detokenized using MADA detokenizer. Before scoring the output, we normalized them and reference translations using the QCRI normalizer (Sajjad et al., 2013). Machine Translation Results Table 1 presents MT results using various segmentation strategies. Compared to the UNSEG system, the MORPH system2 improved translation quality by 4.6 and 1.6 BLEU points in Ar-to-En and Ento-Ar systems, respectively. The resul"
P17-2095,P05-1071,0,0.072442,"racter CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance. 1 Introduction Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, ‘‘wktAbnA” “ AJK . AJ»ð” (gloss: and our book) is decomposed into its stem H AJ» + ð”. and affixes as: “w+ ktAb +nA” “ AK+ . 601 Proceedings of the 55th Annual Meeting of the Association"
P17-2095,P16-2058,0,0.109475,"Missing"
P17-2095,N13-1044,0,0.0576645,"Missing"
P17-2095,P07-1116,0,0.100776,"al analyzer that generates a list of possible word-level analyses (independent of context). The analyses are provided with the original text to a Feature Modeling component that applies an SVM and a language model to make predictions, which are scored by an Analysis Ranking component. Farasa on the other hand is a light weight segmenter, which ignores context and instead uses a variety of features and lexicons for segmentation. 2.2 Character-level Encoding Data Driven Sub-word Units A number of data-driven approaches have been proposed that learn to segment words into smaller units from data (Demberg, 2007; Sami Virpioja and Kurimo, 2013) and shown to improve phrasebased MT (Fishel and Kirik, 2010; Stallard et al., 2012). Recently, with the advent of neural MT, a few sub-word-based techniques have been proposed that segment words into smaller units to tackle the limited vocabulary and unknown word problems (Sennrich et al., 2016; Wu et al., 2016). In this work, we explore Byte-Pair Encoding (BPE), a data compression algorithm (Gage, 1994) as an alternative to morphological segmentation of Arabic. BPE splits words into symbols (a sequence of characters) and then iteratively replaces the most fre"
P17-2095,N06-2013,0,0.0488146,"rd units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance. 1 Introduction Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, ‘‘wktAbnA” “ AJK . AJ»ð” (gloss: and our book) is decomposed into its stem H AJ» + ð”. and af"
P17-2095,C16-2047,0,0.0156449,"ater, gives optimal performance. 1 Introduction Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, ‘‘wktAbnA” “ AJK . AJ»ð” (gloss: and our book) is decomposed into its stem H AJ» + ð”. and affixes as: “w+ ktAb +nA” “ AK+ . 601 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 601–607 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2095 2 Segmentation Approaches We experimented with three data-driven segmentation schemes: i) morphological segmentation, ii) sub-word segmentation based"
P17-2095,P16-1162,0,0.504845,"bic translation (El Kholy and Habash, 2012)). More importantly, these tools are dialect- and domain-specific. A segmenter trained for modern standard Arabic (MSA) performs significantly worse on dialectal Arabic (Habash et al., 2013), or when it is applied to a new domain. In this work, we explore whether we can avoid the language-dependent pre/post-processing components and learn segmentation directly from the training data being used for a given task. We investigate data-driven alternatives to morphological segmentation using i) unsupervised sub-word units obtained using byte-pair encoding (Sennrich et al., 2016), ii) purely character-based segmentation (Ling et al., 2015), and iii) a convolutional neural network over characters (Kim et al., 2016). We evaluate these techniques on the tasks of machine translation (MT) and part-of-speech (POS) tagging and compare them against morphological segmenters MADAMIRA (Pasha et al., 2014) and Farasa (Abdelali et al., 2016). On the MT task, byte-pair encoding (BPE) performs the best among the three methods, achieving very similar performance to morphological segmentation in the Arabic-to-English direction and slightly worse in the other direction. Character-based"
P17-2095,P12-2063,0,0.0265535,"provided with the original text to a Feature Modeling component that applies an SVM and a language model to make predictions, which are scored by an Analysis Ranking component. Farasa on the other hand is a light weight segmenter, which ignores context and instead uses a variety of features and lexicons for segmentation. 2.2 Character-level Encoding Data Driven Sub-word Units A number of data-driven approaches have been proposed that learn to segment words into smaller units from data (Demberg, 2007; Sami Virpioja and Kurimo, 2013) and shown to improve phrasebased MT (Fishel and Kirik, 2010; Stallard et al., 2012). Recently, with the advent of neural MT, a few sub-word-based techniques have been proposed that segment words into smaller units to tackle the limited vocabulary and unknown word problems (Sennrich et al., 2016; Wu et al., 2016). In this work, we explore Byte-Pair Encoding (BPE), a data compression algorithm (Gage, 1994) as an alternative to morphological segmentation of Arabic. BPE splits words into symbols (a sequence of characters) and then iteratively replaces the most frequent symbols with their merged variants. In essence, frequent character n-gram sequences will be merged to form one"
P17-2095,P12-2059,0,0.0607745,"ased on BPE, and iii) two variants of character-based segmentation. We first map each source word to its corresponding segments (depending on the segmentation scheme), embed all segments of a word in vector space and feed them one-by-one to an encoder-decoder model. See Figure 1 for illustration. 2.1 Figure 1: Segmentation approaches for the word “b$rhm” “ ÑëQå.”; the blue vectors indicate the embedding(s) used before the encoding layer. 2.3 Morphological Segmentation Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al., 2014). Ling et al. (2016) used character embeddings to address the OOV word problem. We explored them as an alternative to morphological segmentation. Their advantage is that character embeddings do not require any complicated pre- and post-processing step other than segmenting words into characters. The fully character-level encoder treats the source sentence as a sequence of letters, encoding each letter (including white-space) in the LSTM encoder (see Figure 1). The decoding may follow identical settings. We restricted the character-level representation to th"
P17-2095,pasha-etal-2014-madamira,0,0.112051,"Missing"
P17-2095,N04-4038,0,\N,Missing
P17-2095,N16-3003,1,\N,Missing
P17-2095,2013.iwslt-evaluation.8,1,\N,Missing
P97-1037,H94-1028,0,0.380274,"Missing"
P97-1037,J93-2003,0,0.0588627,"Missing"
P97-1037,W93-0301,0,0.126761,"Missing"
P97-1037,C94-2178,0,0.0465429,"Missing"
P97-1037,C96-2141,1,0.821772,"issue in modeling the string translation probability Pr(f(le I) is the question of how we define the correspondence between the words of the target sentence and the words of the source sentence. In typical cases, we can assume a sort of pairwise dependence by considering all word pairs (fj,ei) for a given sentence pair [f(; el]. We further constrain this model by assigning each source word to exactly one target word. Models describing these types of dependencies are referred to as alignrnen.t models (Brown e t al., 1993), (Dagan eta].. 1993). (Kay & R6scheisen, 1993). (Fung & Church. 1994), (Vogel et al., 1996). In this section, we introduce a monotoue HMM based alignment and an associated DP based search algorithm for translation. Another approach to statistical machine translation using DP was presented in (Wu, 1996). The notational convention will be a,s follows. We use the symbol Pr(.) to denote general SourceLanguageText 1 problem in speech recognition, where the so-called Hidden Markov models have been successfully used for a long time (Jelinek. 1976). Using the same basic principles, we can rewrite the probability by introducing the 'hidden&quot; aligmnents a~ := a l...aj...aa for a sentence pair"
P97-1037,P96-1021,0,0.307476,"ssume a sort of pairwise dependence by considering all word pairs (fj,ei) for a given sentence pair [f(; el]. We further constrain this model by assigning each source word to exactly one target word. Models describing these types of dependencies are referred to as alignrnen.t models (Brown e t al., 1993), (Dagan eta].. 1993). (Kay & R6scheisen, 1993). (Fung & Church. 1994), (Vogel et al., 1996). In this section, we introduce a monotoue HMM based alignment and an associated DP based search algorithm for translation. Another approach to statistical machine translation using DP was presented in (Wu, 1996). The notational convention will be a,s follows. We use the symbol Pr(.) to denote general SourceLanguageText 1 problem in speech recognition, where the so-called Hidden Markov models have been successfully used for a long time (Jelinek. 1976). Using the same basic principles, we can rewrite the probability by introducing the 'hidden&quot; aligmnents a~ := a l...aj...aa for a sentence pair [f~; c/]: 1 I Transformation ¢~ GlobalSearch: j~ maximize Pr(el). pr(f~lell} I ovor LexiconModel P,,(s 'lcI = I AllgnmentModel J j. pc(e~) [ MLanguage odel, [;.. .,!. ., on] ~i' j=1 To avoid any confnsion with th"
P97-1037,P93-1001,0,\N,Missing
R13-1066,C04-1072,0,0.0239716,"nces are often available for the English (target) side of the tuning and the evaluation dataset, but not for the source language, e.g., Arabic, Chinese. 1 One could hire translators, but this would be costly. 504 Proceedings of Recent Advances in Natural Language Processing, pages 504–510, Hissar, Bulgaria, 7-13 September 2013. 3 3.1 Method We will do the selection with respect to some English reference, e.g., backtranslation of the Arabic reference generated by our own system or by Google translate. Below, we present the similarity measures that we use for the selection. BLEU+1 (B1). BLEU+1 (Lin and Och, 2004) is a smoothed version of BLEU (Papineni et al., 2002) used to address sparseness problems with n-gram matches when comparing sentences. BLEU+1 BP smooth (B1-BP). The BLEU+1 approximation of BLEU smooths the n-gram counts but not the brevity penalty, thus destroying the balance between the two; it also assigns a non-zero precision to cases with zero matches. Thus, we experiment with a version of BLEU+1 from (Nakov et al., 2012) that smooths the brevity penalty and also uses a “grounding” factor. BLEU+1 Sigmoid LP (B1-SG). Note that the brevity penalty of BLEU/BLEU+1 penalizes shorter but not l"
R13-1066,D12-1037,0,0.0156699,"inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SMT) systems are data-driven, and thus critically depend on the available resources for training, tuning and evaluation. These resources are hard to ob"
R13-1066,D11-1033,0,0.021347,"given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SMT) systems are data-driven, and thus crit"
R13-1066,D09-1074,0,0.0212759,"s of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SMT) systems are data-driven, and thus critically depend on the avai"
R13-1066,W09-0439,0,0.0212717,"-phrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on the GigaWord v.5 with KneserNey smoothing using KenLM (Heafield, 2011). For optimization, we used MERT. For evaluation, we used NIST’s BLEU scoring tool v13a, which we ran on a desegmented Arabic output, where conjunctions are attached to the following word. In order to ensure stability, we performed three reruns of MERT for each experiment, and we report evaluation results averaged over the three reruns, as suggested by Foster and Kuhn (2009). 2 Tuning on MT04, testing on MT05 AVERAGE BLEU len 29.41 1.014 30.13 0.993 30.07 0.991 30.03 0.983 30.14 0.986 AVG, no self BLEU len 30.30 1.020 30.18 0.993 30.14 0.990 29.36 0.981 29.32 0.982 Table 2: Tuning and testing on MT04. We tune on the English input in the first column, then we translate all MT04x inputs. We report BLEU and hyp/ref length ratios averaged over (a) all MT04 datasets, and (b) all but the one used for tuning. Table 1 shows the results when tuning on MT04 and testing on MT05. There are several interesting observations we can make. First, the choice of test dataset has a"
R13-1066,D10-1044,0,0.0220994,"on, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SM"
R13-1066,E06-1005,0,0.0303122,"ields the highest BLEU score. This finding has implications on how to pick good translators and how to select useful data for parameter optimization in SMT. 1 2 Related Work One relevant line of research is on multi-source translation, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions"
R13-1066,P10-2041,0,0.0333882,"multi-source translation, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical m"
R13-1066,W11-2123,0,0.0332866,"we normalized the Arabic training, development and test data using MADA (Roth et al., 2008), fixing automatically all wrong instances of alef, ta marbuta and alef maqsura. We segmented the Arabic words by splitting out conjunctions (MADA scheme D1). For English, we converted all words to lowercase. We built our phrase tables using the standard Moses pipeline with max-phrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on the GigaWord v.5 with KneserNey smoothing using KenLM (Heafield, 2011). For optimization, we used MERT. For evaluation, we used NIST’s BLEU scoring tool v13a, which we ran on a desegmented Arabic output, where conjunctions are attached to the following word. In order to ensure stability, we performed three reruns of MERT for each experiment, and we report evaluation results averaged over the three reruns, as suggested by Foster and Kuhn (2009). 2 Tuning on MT04, testing on MT05 AVERAGE BLEU len 29.41 1.014 30.13 0.993 30.07 0.991 30.03 0.983 30.14 0.986 AVG, no self BLEU len 30.30 1.020 30.18 0.993 30.14 0.990 29.36 0.981 29.32 0.982 Table 2: Tuning and testing"
R13-1066,C12-1121,1,0.90772,"Missing"
R13-1066,2005.eamt-1.19,1,0.791718,"nd Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SMT) systems are data-driven, and thus critically depend on the available resources for training, tuning and evalua"
R13-1066,2001.mtsummit-papers.46,0,0.0699721,"puts: (a) select one of the datasets, (b) select the best input for each sentence, and (c) synthesize an input for each sentence by fusing the available inputs. Surprisingly, we find out that it is best to tune on the hardest available input, not on the one that yields the highest BLEU score. This finding has implications on how to pick good translators and how to select useful data for parameter optimization in SMT. 1 2 Related Work One relevant line of research is on multi-source translation, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et"
R13-1066,W12-5611,0,0.0298896,"Missing"
R13-1066,P02-1040,0,0.092353,"side of the tuning and the evaluation dataset, but not for the source language, e.g., Arabic, Chinese. 1 One could hire translators, but this would be costly. 504 Proceedings of Recent Advances in Natural Language Processing, pages 504–510, Hissar, Bulgaria, 7-13 September 2013. 3 3.1 Method We will do the selection with respect to some English reference, e.g., backtranslation of the Arabic reference generated by our own system or by Google translate. Below, we present the similarity measures that we use for the selection. BLEU+1 (B1). BLEU+1 (Lin and Och, 2004) is a smoothed version of BLEU (Papineni et al., 2002) used to address sparseness problems with n-gram matches when comparing sentences. BLEU+1 BP smooth (B1-BP). The BLEU+1 approximation of BLEU smooths the n-gram counts but not the brevity penalty, thus destroying the balance between the two; it also assigns a non-zero precision to cases with zero matches. Thus, we experiment with a version of BLEU+1 from (Nakov et al., 2012) that smooths the brevity penalty and also uses a “grounding” factor. BLEU+1 Sigmoid LP (B1-SG). Note that the brevity penalty of BLEU/BLEU+1 penalizes shorter but not longer sentences. Thus, we also experiment with a versi"
R13-1066,N03-1017,0,0.023922,".001 0.51 MT054 BLEU len 35.46 0.988 35.31 0.972 35.12 0.970 34.81 0.960 34.82 0.965 35.15 0.973 0.65 AVERAGE BLEU len 34.24 0.989 34.12 0.972 33.95 0.970 33.74 0.960 33.67 0.964 34.03 0.974 0.57 Table 1: Tuning on MT04 and testing on MT05. Shown are BLEU scores and hypothesis/reference length ratios. The best and the worst BLEU scores for each test MT05 dataset are in bold and stroke out, respectively; the last row shows the absolute difference between them. 4 Experiments and Evaluation 4.1 4.2 Experimental Setup TEST ⇒ TUNE ⇓ MT040 MT041 MT042 MT043 MT044 We used the phrase-based SMT model (Koehn et al., 2003), as implemented in the Moses toolkit (Koehn et al., 2007), to train an SMT system translating from English to Arabic. For tuning and evaluation, we used two multireference datasets, MT04 and MT05, from the NIST 2012 OpenMT Evaluation,2 each with a single Arabic input and five English reference translations, which we inverted, ending up with five English inputs and one Arabic reference for each one. We trained the English-Arabic system (translation, reordering, and language models) on all training data from NIST 2012 except for UN data. Following Kholy and Habash (2012), we normalized the Arab"
R13-1066,P08-2030,0,0.123937,", 2007), to train an SMT system translating from English to Arabic. For tuning and evaluation, we used two multireference datasets, MT04 and MT05, from the NIST 2012 OpenMT Evaluation,2 each with a single Arabic input and five English reference translations, which we inverted, ending up with five English inputs and one Arabic reference for each one. We trained the English-Arabic system (translation, reordering, and language models) on all training data from NIST 2012 except for UN data. Following Kholy and Habash (2012), we normalized the Arabic training, development and test data using MADA (Roth et al., 2008), fixing automatically all wrong instances of alef, ta marbuta and alef maqsura. We segmented the Arabic words by splitting out conjunctions (MADA scheme D1). For English, we converted all words to lowercase. We built our phrase tables using the standard Moses pipeline with max-phrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on the GigaWord v.5 with KneserNey smoothing using KenLM (Heafield, 2011). For optimization, we used MERT. For evaluation, we used NIST’s BLEU scoring"
R13-1066,2005.iwslt-1.8,0,0.156577,"tem (translation, reordering, and language models) on all training data from NIST 2012 except for UN data. Following Kholy and Habash (2012), we normalized the Arabic training, development and test data using MADA (Roth et al., 2008), fixing automatically all wrong instances of alef, ta marbuta and alef maqsura. We segmented the Arabic words by splitting out conjunctions (MADA scheme D1). For English, we converted all words to lowercase. We built our phrase tables using the standard Moses pipeline with max-phrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on the GigaWord v.5 with KneserNey smoothing using KenLM (Heafield, 2011). For optimization, we used MERT. For evaluation, we used NIST’s BLEU scoring tool v13a, which we ran on a desegmented Arabic output, where conjunctions are attached to the following word. In order to ensure stability, we performed three reruns of MERT for each experiment, and we report evaluation results averaged over the three reruns, as suggested by Foster and Kuhn (2009). 2 Tuning on MT04, testing on MT05 AVERAGE BLEU len 29.41 1.014 30.13 0.993 30.07 0.9"
R13-1066,E09-1082,0,0.0210994,"t available input, not on the one that yields the highest BLEU score. This finding has implications on how to pick good translators and how to select useful data for parameter optimization in SMT. 1 2 Related Work One relevant line of research is on multi-source translation, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at t"
R13-1066,P07-2045,0,0.00956357,"70 34.81 0.960 34.82 0.965 35.15 0.973 0.65 AVERAGE BLEU len 34.24 0.989 34.12 0.972 33.95 0.970 33.74 0.960 33.67 0.964 34.03 0.974 0.57 Table 1: Tuning on MT04 and testing on MT05. Shown are BLEU scores and hypothesis/reference length ratios. The best and the worst BLEU scores for each test MT05 dataset are in bold and stroke out, respectively; the last row shows the absolute difference between them. 4 Experiments and Evaluation 4.1 4.2 Experimental Setup TEST ⇒ TUNE ⇓ MT040 MT041 MT042 MT043 MT044 We used the phrase-based SMT model (Koehn et al., 2003), as implemented in the Moses toolkit (Koehn et al., 2007), to train an SMT system translating from English to Arabic. For tuning and evaluation, we used two multireference datasets, MT04 and MT05, from the NIST 2012 OpenMT Evaluation,2 each with a single Arabic input and five English reference translations, which we inverted, ending up with five English inputs and one Arabic reference for each one. We trained the English-Arabic system (translation, reordering, and language models) on all training data from NIST 2012 except for UN data. Following Kholy and Habash (2012), we normalized the Arabic training, development and test data using MADA (Roth et"
R13-1066,2006.amta-papers.25,0,0.0233586,"e easiest). We believe that this finding has implications on how we should pick good translators and how we should select useful data for parameter optimization. On the other hand, it might also indicate a problem with BLEU as an evaluation measure. In future work, we plan to test our methods on other Arabic-English datasets that have multiple English references. We further plan experiments with other language pairs, e.g., ChineseEnglish, which are available from NIST and IWSLT. We also want to study the effect of the tuning dataset selection on evaluation measures other than BLEU, e.g., TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009). Looking at tuning dataset selection that takes the test data into account is another promising direction for future work. Features from quality estimation (Specia et al., 2010) might be also helpful to determine the best input to tune on. Another related, but different, research direction is about how to best evaluate (as opposed to tune, which we have explored above) an SMT system in case multiple possible versions of the input sentences are available. Choosing the hardest dataset A closer look at the strategies for backtranslate and X-vs-all-but-X rev"
temnikova-etal-2017-interpreting,C10-2010,0,\N,Missing
temnikova-etal-2017-interpreting,N16-1111,0,\N,Missing
temnikova-etal-2017-interpreting,tohyama-matsubara-2006-collection,0,\N,Missing
temnikova-etal-2017-interpreting,ma-2006-champollion,0,\N,Missing
vogel-monson-2004-augmenting,J93-2003,0,\N,Missing
vogel-monson-2004-augmenting,zhang-etal-2004-interpreting,1,\N,Missing
W03-0303,J93-2003,0,0.00758813,"re words in the target vocabulary Vf and source vocabulary Ve respectively. A is the alignment of texts. There are two operators for bracketing: direct bracketing denoted by [ ], and inverse bracketing, denoted by <>. The A-productions are divided into two classes: syntactic {(1),(2)}and lexical rules {(3),(4),(5)}. Each A-production rule has a probability. In our algorithm, we use the same PCFG. However, instead of estimating the probabilities for the production rules via EM as described in [Wu 1997], we assign the probabilities to the rules using the Model-1 statistical translation lexicon [Brown et al. 1993]. Because the syntactic A-production rules do not compete with the lexical rules, we can set them some default values. Also we make no assumptions which bracketing direction is more likely to occur, thus the probabilities for [ ] and <> are set to be equal. As for the lexical rules, we experimented with the conditional probabilities p(e|f ), p(f |e) and the interpolation of p(f |e, epos ) and p(f |e) (described in section 4.1). As for these probabilities of aligning a word to the null word or to unknown words, they are set to be 1e-7, which is the default small value used in training Model-1."
W03-0303,P00-1056,0,0.25751,"Missing"
W03-0303,W95-0107,0,0.069333,"Missing"
W03-0303,J97-3002,0,0.89333,"fficult. The goal is to extract structure information from parallel sentences, and thereby improve word/phrase alignment via bilingual constraint transfer. This approach can be generalized to the automatic acquisition of a translation lexicon and phrase translations esp. for languages for which resources are relatively scarce compared with English. The parallel sentences in building Statistical Machine Translation (SMT) systems are mostly unrestricted text where full parsing often fails, and robustness with respect to the inherent noise of the parallel data is important. Bilingual Bracketing [Wu 1997] is one of the bilingual shallow parsing approaches studied for Chinese-English word alignment. It uses a translation lexicon within a probabilistic context free grammar (PCFG) as a generative model to analyze the parallel sentences with weak order constraints. This provides a framework to incorporate knowledge from the English side such as POS, phrase structure and potentially more detailed parsing results. In this paper, we use a simplified bilingual bracketing grammar together with a statistical translation lexicon Stephan Vogel Language Technologies Institute Carnegie Mellon University vo"
W03-1502,J93-2003,0,\N,Missing
W03-1502,C96-2141,1,\N,Missing
W03-1502,W98-1005,0,\N,Missing
W03-1502,P00-1004,1,\N,Missing
W03-1502,J00-2004,0,\N,Missing
W03-1502,P02-1051,0,\N,Missing
W03-1502,A97-1029,0,\N,Missing
W04-3227,J97-3002,0,0.341869,"h allows for local word reordering. 2.1 Translation Model The phrase-based statistical translation systems use not only word-to-word translation, extracted from bilingual data, but also phrase-to phrase translations. . Different types of extraction approaches have been described in the literature: syntax-based, word-alignment-based, and genuine phrase alignment models. The syntax-based approach has the advantage to model the grammar structures using models of more or less structural richness, such as the syntax-based alignment model in (Yamada and Knight, 2001) or the Bilingual Bracketing in (Wu, 1997). Popular word-alignment-based approaches usually rely on initial word alignments from the IBM and HMM alignment models (Och and Ney, 2000), from which the phrase pairs are then extracted. (Marcu and Wong 2002) and (Zhang et al. 2003) do not rely on word alignment but model directly the phrase alignment. Because all statistical machine translation systems search for a globally optimal translation using the language and translation model, a translation probability has to be assigned to each phrase translation pair. This score should be meaningful in that better translations have a higher probab"
W04-3227,P01-1067,0,0.085347,"based translation models and the decoding algorithm, which allows for local word reordering. 2.1 Translation Model The phrase-based statistical translation systems use not only word-to-word translation, extracted from bilingual data, but also phrase-to phrase translations. . Different types of extraction approaches have been described in the literature: syntax-based, word-alignment-based, and genuine phrase alignment models. The syntax-based approach has the advantage to model the grammar structures using models of more or less structural richness, such as the syntax-based alignment model in (Yamada and Knight, 2001) or the Bilingual Bracketing in (Wu, 1997). Popular word-alignment-based approaches usually rely on initial word alignments from the IBM and HMM alignment models (Och and Ney, 2000), from which the phrase pairs are then extracted. (Marcu and Wong 2002) and (Zhang et al. 2003) do not rely on word alignment but model directly the phrase alignment. Because all statistical machine translation systems search for a globally optimal translation using the language and translation model, a translation probability has to be assigned to each phrase translation pair. This score should be meaningful in tha"
W04-3227,2002.tmi-tutorials.2,0,0.0208216,"language. The advantages are obvious: It has built-in local context modeling, and provides reliable local word reordering. It has multi-word translations, and models a word’s conditional fertility given a local context. It captures idiomatic phrase translations and can be easily enriched with bilingual dictionaries. In addition, it can compensate for the segmentation errors made during preprocessing, i.e. word segmentation errors of Chinese. The advantage of using phrase-based translation in a statistical framework has been shown in many studies such as (Koehn et al. 2003; Vogel et al. 2003; Zens et al. 2002; Marcu and Wong, 2002). However, the phrase translation pairs are typically extracted from a parallel corpus based on the Viterbi alignment of some word alignment models. The leads to the question what probability should be assigned to those phrase translations. Different approaches have been suggested as using relative frequencies (Zens et al. 2002), calculate probabilities based on a statistical word-to-word dictionary (Vogel et al. 2003) or use a linear interpolation of these scores (Koehn et al. 2003). In this paper we investigate a different approach with takes the information content of"
W04-3227,C96-2141,1,0.864147,"Missing"
W04-3227,P91-1023,0,0.0595274,"statistical lexicon Pr(s|t) is non-symmetric. One can easily re-write all the distances by using Pr(t|s). But in our experiments this reverse direction of using Pr(t|s) gives trivially difference. So in all the experimental results reported in this paper, the distances defined in (1) and (11) are used. 5 Length Regularization Phrase pair extraction does not work perfectly and sometimes a short source phrase is aligned to a long target phrase or vice versa. Length regularization can be applied to penalize too long or too short candidate translations. Similar to the sentence alignment work in (Gale and Church, 1991), the phrase length ratio is assumed to be a Gaussian distribution as given in Equation (12): v v v v (l (t ) / l ( s ) − µ ) 2 (12) l (t , s ) ∝ exp(−0.5 ⋅ ) σ2 where l(t) is the target sentence length. Mean µ and variance σ can be estimated using a parallel corpus using a Maximum Likelihood criteria. The regularized score is the product of (11) and (12). 6 Experiments Experiments were carried out on the so-called large data track Chinese-English TIDES translation task, using the June 2002 test data. The training data used to train the statistical lexicon and to extract the phrase translation"
W04-3227,vogel-monson-2004-augmenting,1,0.813965,"ilt on 20 million words of general newswire text, using the SRILM toolkit (Stolcke, 2002). Decoding was carried out as described in section 2.2. The test data consists of 878 Chinese sentences or 24,337 words after word segmentation. There are four human translations per Chinese sentence as references. Both NIST score and Bleu score (in percentage) are reported for adequacy and fluency aspects of the translation quality. 6.1 ond, a large monolingual English corpus was used to filter out the new word forms. If they did not appear in the corpus, the new entries were not added to the transducer (Vogel, 2004). BiBr extracts sub-tree mappings from Bilingual Bracketing alignments (Wu, 1997); HMM extracts partial path mappings from the Viterbi path in the Hidden Markov Model alignments (Vogel et. al., 1996). ISA is an integrated segmentation and alignment for phrases (Zhang et.al, 2003), which is an extension of (Marcu and Wong, 2002). LDC 425K HMM 349K ISA 263K 1.80 1.11 1.09 Table-1 statistics of transducers 1.20 N (K ) avg (ltgt / l src ) Table-1 shows some statistics of the four transducers extracted for the translation task. N is the total number of phrase pairs in the transducer. LDC is the lar"
W04-3227,zhang-etal-2004-interpreting,1,0.835189,"Missing"
W04-3227,N03-1017,0,0.102462,"e language into an mgram in the target language. The advantages are obvious: It has built-in local context modeling, and provides reliable local word reordering. It has multi-word translations, and models a word’s conditional fertility given a local context. It captures idiomatic phrase translations and can be easily enriched with bilingual dictionaries. In addition, it can compensate for the segmentation errors made during preprocessing, i.e. word segmentation errors of Chinese. The advantage of using phrase-based translation in a statistical framework has been shown in many studies such as (Koehn et al. 2003; Vogel et al. 2003; Zens et al. 2002; Marcu and Wong, 2002). However, the phrase translation pairs are typically extracted from a parallel corpus based on the Viterbi alignment of some word alignment models. The leads to the question what probability should be assigned to those phrase translations. Different approaches have been suggested as using relative frequencies (Zens et al. 2002), calculate probabilities based on a statistical word-to-word dictionary (Vogel et al. 2003) or use a linear interpolation of these scores (Koehn et al. 2003). In this paper we investigate a different approach"
W04-3227,W02-1018,0,0.188015,"antages are obvious: It has built-in local context modeling, and provides reliable local word reordering. It has multi-word translations, and models a word’s conditional fertility given a local context. It captures idiomatic phrase translations and can be easily enriched with bilingual dictionaries. In addition, it can compensate for the segmentation errors made during preprocessing, i.e. word segmentation errors of Chinese. The advantage of using phrase-based translation in a statistical framework has been shown in many studies such as (Koehn et al. 2003; Vogel et al. 2003; Zens et al. 2002; Marcu and Wong, 2002). However, the phrase translation pairs are typically extracted from a parallel corpus based on the Viterbi alignment of some word alignment models. The leads to the question what probability should be assigned to those phrase translations. Different approaches have been suggested as using relative frequencies (Zens et al. 2002), calculate probabilities based on a statistical word-to-word dictionary (Vogel et al. 2003) or use a linear interpolation of these scores (Koehn et al. 2003). In this paper we investigate a different approach with takes the information content of words better into acco"
W04-3227,P00-1056,0,0.0991688,"rd translation, extracted from bilingual data, but also phrase-to phrase translations. . Different types of extraction approaches have been described in the literature: syntax-based, word-alignment-based, and genuine phrase alignment models. The syntax-based approach has the advantage to model the grammar structures using models of more or less structural richness, such as the syntax-based alignment model in (Yamada and Knight, 2001) or the Bilingual Bracketing in (Wu, 1997). Popular word-alignment-based approaches usually rely on initial word alignments from the IBM and HMM alignment models (Och and Ney, 2000), from which the phrase pairs are then extracted. (Marcu and Wong 2002) and (Zhang et al. 2003) do not rely on word alignment but model directly the phrase alignment. Because all statistical machine translation systems search for a globally optimal translation using the language and translation model, a translation probability has to be assigned to each phrase translation pair. This score should be meaningful in that better translations have a higher probability assigned to them, and balanced with respect to word translations. Bad phrase translations should not win over better word for word tr"
W04-3227,J93-1004,0,\N,Missing
W04-3227,J93-2003,0,\N,Missing
W05-0825,J93-2003,0,0.0209022,"Missing"
W05-0825,J03-1002,0,0.0231244,"hrase pairs. We presented the results and our experience in the shared tasks on FrenchEnglish. 1 Introduction Phrase extraction becomes a key component in today’s state-of-the-art statistical machine translation systems. With a longer context than unigram, phrase translation models have flexibilities of modelling local word-reordering, and are less sensitive to the errors made from preprocessing steps including word segmentations and tokenization. However, most of the phrase extraction algorithms rely on good word alignments. A widely practiced approach explained in details in (Koehn, 2004), (Och and Ney, 2003) and (Tillmann, 2003) is to get word alignments from two directions: source to target and target to source; the intersection or union operation is applied to get refined word alignment with pre-designed heuristics fixing the unaligned words. With this refined word alignment, the phrase extraction for a given source phrase is essentially to extract the target candidate phrases in the target sentence by searching the left and right projected boundaries. Stephan Vogel Language Technologies Institute Carnegie Mellon University Pittsburgh, PA-15213 vogel+@cs.cmu.edu In (Vogel et al., 2004), they tr"
W05-0825,W03-1001,0,0.0192232,"d the results and our experience in the shared tasks on FrenchEnglish. 1 Introduction Phrase extraction becomes a key component in today’s state-of-the-art statistical machine translation systems. With a longer context than unigram, phrase translation models have flexibilities of modelling local word-reordering, and are less sensitive to the errors made from preprocessing steps including word segmentations and tokenization. However, most of the phrase extraction algorithms rely on good word alignments. A widely practiced approach explained in details in (Koehn, 2004), (Och and Ney, 2003) and (Tillmann, 2003) is to get word alignments from two directions: source to target and target to source; the intersection or union operation is applied to get refined word alignment with pre-designed heuristics fixing the unaligned words. With this refined word alignment, the phrase extraction for a given source phrase is essentially to extract the target candidate phrases in the target sentence by searching the left and right projected boundaries. Stephan Vogel Language Technologies Institute Carnegie Mellon University Pittsburgh, PA-15213 vogel+@cs.cmu.edu In (Vogel et al., 2004), they treat phrase alignment"
W05-0825,2004.iwslt-evaluation.11,1,0.93713,", 2004), (Och and Ney, 2003) and (Tillmann, 2003) is to get word alignments from two directions: source to target and target to source; the intersection or union operation is applied to get refined word alignment with pre-designed heuristics fixing the unaligned words. With this refined word alignment, the phrase extraction for a given source phrase is essentially to extract the target candidate phrases in the target sentence by searching the left and right projected boundaries. Stephan Vogel Language Technologies Institute Carnegie Mellon University Pittsburgh, PA-15213 vogel+@cs.cmu.edu In (Vogel et al., 2004), they treat phrase alignment as a sentence splitting problem: given a source phrase, find the boundaries of the target phrase such that the overall sentence alignment lexicon probability is optimal. We generalize it in various ways, esp. by using a fertility model to get a better estimation of phrase lengths, and a phrase level distortion model. In our proposed algorithm, we do not need explicit word alignment for phrase extraction. Thereby it avoids the burden of testing and comparing different heuristics especially for some language specific ones. On the other hand, the algorithm has such f"
W05-0829,J93-2003,0,0.00797865,"scribes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach. 1 Introduction In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al., 1999; Koehn et al., 2003) have been shown to outperform word-to-word translation models (Brown et al., 1993). Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase alignment strategies which require complicated statistical model training. In this paper, we describe the key idea behind this"
W05-0829,N03-1017,0,0.0246276,"University Pittsburgh, PA 15213 {joy+,vogel+}@cs.cmu.edu 2 Abstract This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach. 1 Introduction In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al., 1999; Koehn et al., 2003) have been shown to outperform word-to-word translation models (Brown et al., 1993). Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase alignment strategies which require complicat"
W05-0829,J90-1003,0,0.0280343,"a bilingual corpus of language pair F (Foreign, source language) and E (English, target language), if we know the word alignment for each sentence pair we can calculate the co-occurrence frequency for each source/target word pair type C(f, e) P and the marginal frequency C(f ) = e C(f, e) and P C(e) = f C(f, e). We can apply various statistical tests (Manning and Sch¨utze, 1999) to measure how likely is the association between f and e, in other words how likely they are mutual translations. In the following sections, we will use χ2 statistics to measure the the mutual translation likelihood (Church and Hanks, 1990). 3 The Core of the Integrated Phrase Segmentation and Alignment The competitive linking algorithm (CLA) (Melamed, 1997) is a greedy word alignment algorithm. It was designed to overcome the problem of indirect associations using a simple heuristic: whenever several word tokens fi in one half of the bilingual corpus co-occur with a particular word token e in the other half of the corpus, the word that is most likely to be e’s translation is the one for which the likelihood L(f, e) of translational equivalence is highest. The simplicity of this algorithm depends on a one-to-one alignment assump"
W05-0829,P97-1063,0,0.131269,"alculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase alignment strategies which require complicated statistical model training. In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment. Translation Likelihood as a Statistical Test Given a bilingual corpus of language pair F (Foreign, source language) and E (English, target language), if we know the word alignment for each sentence pair we can calculate the co-occurrence frequency for each source/target word pair type C(f, e) P and the marginal frequency C(f ) = e C(f, e) and P C(e) = f C(f, e). We can apply various statistical tests (Manning and Sch¨utze, 1999) to measure how likely is the association between f and e, in other words how likely they are mutual translations. In t"
W05-0829,W99-0604,0,0.0344042,"e Carnegie Mellon University Pittsburgh, PA 15213 {joy+,vogel+}@cs.cmu.edu 2 Abstract This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach. 1 Introduction In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al., 1999; Koehn et al., 2003) have been shown to outperform word-to-word translation models (Brown et al., 1993). Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase alignment strategies wh"
W06-1626,J93-2003,0,0.00531507,"han e(1) according to the reference (Ref) although its model score is lower. SMT models are not perfect, it is unavoidable to have a sub-optimal translation output as the model-best by the decoder. The objective of N best list re-ranking is then to re-rank the translation hypotheses using features which are not used during decoding so that better translations can emerge as “optimal” translations. Our experIntroduction Statistical language modeling has been widely used in natural language processing applications such as Automatic Speech Recognition (ASR), Statistical Machine Translation (SMT) (Brown et al., 1993) and Information Retrieval (IR) (Ponte and Croft, 1998). Conventional n-gram language modeling counts the frequency of all the n-grams in a corpus and calculates the conditional probabilities of a word given its history of n − 1 words i−1 P (wi |wi−n+1 ). As the corpus size increases, building a high order language model offline becomes very expensive if it is still possible (Goodman, 2000). In this paper, we describe a new approach of language modeling using a distributed computing paradigm. Distributed language modeling can 216 Proceedings of the 2006 Conference on Empirical Methods in Natur"
W06-1626,P05-1033,0,0.0112895,"1 Oracle score of the N -best list Because of the spurious ambiguity, there are only 24,612 unique hypotheses in the 1000-best list, on average 27 per source sentence. This limits the potential of N -best re-ranking. Spurious ambiguity is created by the decoder where two hypotheses generated from different decoding path are considered different even though they have identical word sequences. For example, “the terrorist attacks on the united states” could be the output of decoding path [the terrorist attacks][on the united Experiments We used the N -best list generated by the Hiero SMT system (Chiang, 2005). Hiero is a statistical phrase-based translation model that uses hierarchical phrases. The decoder uses a trigram 219 each sentence on average. This confirms our observation that there are indeed better translations in the N -best list. states] and [the terrorist attacks on] [the united states]. We first calculate the oracle score from the N best list to verify that there are alternative hypotheses better than the model-best translation. The oracle best translations are created by selecting the hypothesis which has the highest sentence BLEU score for each source sentence. Yet a critical probl"
W06-1626,P05-1066,0,0.0138971,"ation that there are indeed better translations in the N -best list. states] and [the terrorist attacks on] [the united states]. We first calculate the oracle score from the N best list to verify that there are alternative hypotheses better than the model-best translation. The oracle best translations are created by selecting the hypothesis which has the highest sentence BLEU score for each source sentence. Yet a critical problem with BLEU score is that it is a function of the entire test set and does not give meaningful scores for single sentences. We followed the approximation described in (Collins et al., 2005) to get around this problem. Given a test set with T sentences, N hypotheses are generated for each (r) source sentence ft . Denote et as the r-th ranked (1) hypothesis for ft . et is the model-best hypothesis for this sentence. The baseline BLEU scores are calculated based on the model-best translation (1) set {et |t = 1, . . . , T }. (r) Define the BLEU sentence-level gain for et as: (r) GBLEU et 5.2 Besides comparing the distributed language model re-ranked translations with the model-best translations, we also want to compare the distributed LM with the the standard 3-gram and 4-gram langu"
W06-1626,2005.eamt-1.19,1,0.780742,"Missing"
W06-1626,W05-0821,0,0.0429645,"by this FSA and those accepted by the FSA are considered as possible translations for the source sentence. The corpus is split into hundreds of chunks for parallel processing. All the sentences in one chunk are scanned by the FSA on one processor. Matched sentences from all chunks are then used together as possible translations. The assumption of this work that possible translations of a source sentence can be found as exact match in a big monolingual corpus is weak even for very large corpus. This method can easily fail to find any possible translation and return zero proposed translations. Kirchhoff and Yang (2005) used a factored 3gram model and a 4-gram LM (modified KN smoothing) together with seven system scores to re-rank an SMT N -best. They improved the translation quality of their best baseline (Spanish221 # of Relevant Chunks per. Sent 3-gram KN 4-gram KN L0 L31 L41 L51 L61 L71 L2 1 32.22 32.22 32.27 32.00 32.18 32.21 32.19 32.22 32.29 2 5 10 20 32.38 32.14 32.36 32.33 32.22 32.29 32.52 32.40 32.14 32.28 32.35 32.37 32.37 32.61 32.47 32.15 32.44 32.41 32.45 32.44 32.55 32.51 32.16 32.41 32.37 32.40 32.40 32.64 150 32.08 32.53 32.48 32.41 32.56 Table 2: BLEU scores of the re-ranked translations."
W06-1626,2004.tmi-1.9,1,0.741215,"Missing"
W06-1626,2005.eamt-1.39,1,0.0810552,"f wi−n+1 in D X i i C(wi−n+1 )|D = C(wi−n+1 )|Dd (10) on the fly.   min I(wk ; wj ) i k+1 d 3.1 Calculate n-gram frequency using suffix array Each server3 loads one chunk of the corpus with its suffix array index. The client sends an English sentence w1 . . . wm to each of the servers and requests for the count information of all the n-grams in the sentence. The client collects the count information from all the servers, sums up the counts for each n-gram and then calculates the likelihood of the sentence. For a corpus D with N words, locating all the oci currences of wi−n+1 takes O(logN ). Zhang and Vogel (2005) introduce a search algorithm which locates all the m(m + 1)/2 embedded n-grams in a sentence of m words within O(m · logN ) time. Figure 2 shows the frequencies of all the embedded n-grams in sentence “since 2001 after the incident of the terrorist attacks on the united states” matched against a 26 million words corpus. For example, unigram “after” occurs 4.43 × 104 times, trigram “after the incident” occurs 106 times. The longest n-gram that can be matched is the 8-gram “of the terrorist attacks on the united states” which occurs 7 times in the corpus. The client communicates with the server"
W06-1626,W04-3250,0,0.0413788,"Missing"
W06-1626,N04-1021,0,0.076486,"Missing"
W06-1626,2001.mtsummit-papers.68,0,0.0172153,"Missing"
W06-1626,soricut-etal-2002-using,0,0.0727761,"ize used) 160 Figure 3: BLEU score of the re-ranked best hypothesis vs. the number of the most relevant corpus chunks used to re-rank the n-best list for each sentences. L0 : number of n-grams matched; L1 : average interpolated n-gram conditional probability; L2 : sum of n-grams’ non-compositionality. would expect to see the curves in figure 3 to be much smoother. 6 Related work and discussion Yamamoto and Church (2001) used suffix arrays to compute the frequency and location of an ngram in a corpus. The frequencies are used to find “interesting” substrings which have high mutual information. Soricut et al. (2002) build a Finite State Acceptor (FSA) to compactly represent all possible English translations of a source sentence according to the translation model. All sentences in a big monolingual English corpus are then scanned by this FSA and those accepted by the FSA are considered as possible translations for the source sentence. The corpus is split into hundreds of chunks for parallel processing. All the sentences in one chunk are scanned by the FSA on one processor. Matched sentences from all chunks are then used together as possible translations. The assumption of this work that possible translati"
W06-1626,J01-1001,0,0.106381,"nto (“the”, “terrorist attacks on the united states”) or (“the terrorist”, “attacks on the united states”), ... , or (“the terrorist attacks on the united”, “states”). For each cut, calculate the point-wise mutual information (PMI) between the two short ngrams. The one with the minimal PMI is the most “natural” cut for this n-gram. The PMI over the natural cut quantifies the non-compositionality Inc of an n-gram wij . The higher the value of Inc (wij ) the more likely wij is a meaningful constituent, in other words, it is less likely that wij is composed from two short n-grams just by chance (Yamamoto and Church, 2001). i Under the Markov or higher order Markov process assumption that only the closest n − 1 words have real impact on the choice of wi , equation 1 is approximated to: (2) i The probability of a word given its history can be approximated with the maximum likelihood estimate (MLE) without any smoothing: i−1 C(wi−n+1 ) Ãm n YX (6) The goal of a language model is to determine the probability, or in general the “likelihood” of a word sequence w1 . . . wm (w1m for short) given some training data. The standard language modeling approach breaks the sentence probability down into: Y P (w1m ) = P (wi |w"
W06-1626,P02-1040,0,\N,Missing
W07-0410,P05-1033,0,0.01558,"is available. Another reason is that the performance of statistical translation systems has dramatically improved over the last 5 to 10 years. Does this mean that work on grammar-based systems should be stopped? Should all the insight into the structure of languages be neglected? This might be too drastic a reaction. Actually, now that SMT has reached some maturity, we see several Stephan Vogel Carnegie Mellon University 5000 Forbes Ave. Pittsburgh, PA, 15213 vogel+@cs.cmu.edu attempts to integrate more structure into these systems, ranging from simple hierarchical alignment models (Wu 1997, Chiang 2005) to syntax-based statistical systems (Yamada and Knight 2001, Zollmann and Venugopal 2006). What can traditional Rule-Based translation systems learn from these approaches? And would it not make sense to work from both sides towards that common goal: structurally rich statistical translation models. In this paper we study some enhancements for a Transfer-Based translation system, using techniques and even components developed for statistical machine translation. While the core engine remains virtually untouched, additional features are added to re-score the n-best list generated by the transfe"
W07-0410,2006.iwslt-papers.8,0,0.0461306,"Missing"
W07-0410,takezawa-etal-2002-toward,0,0.0162611,"r a New Domain A major bottleneck in developing a RBMT system for a new translation task (a new language pair or a new domain) is writing the grammar and building the lexicon. Automatic grammar induction using statistical alignments has been studied in (Probst 2005). Here, we start with an existing grammar and augment the baseline lexicon with entries to cover the new domain. We explore semi-automatic lexicon generation for fast adaptation to the travel domain (Section 3.2). 74 Test Data: BTEC Corpus For initial evaluation on unseen data, we selected the Basic Travel Expression Corpus (BTEC) (Takezawa et al. 2002), which has been used in the evaluation campaigns in connection with the International Workshop on Spoken Language Translation (IWSLT 2006). Besides still being currently used to build real systems (Shimizu et al. 2006; Nakamura, et al. 2006), this corpus contains relatively simple sentences that are comparable to the ones initially corrected by users, and which are covered by the baseline manual grammar. As our test set, we used 506 English sentences for which two sets of Spanish reference translations were available. Table 1 shows corpus statistics for the BTEC data. Data Train BTEC Test Pru"
W07-0410,P01-1067,0,0.037157,"nce of statistical translation systems has dramatically improved over the last 5 to 10 years. Does this mean that work on grammar-based systems should be stopped? Should all the insight into the structure of languages be neglected? This might be too drastic a reaction. Actually, now that SMT has reached some maturity, we see several Stephan Vogel Carnegie Mellon University 5000 Forbes Ave. Pittsburgh, PA, 15213 vogel+@cs.cmu.edu attempts to integrate more structure into these systems, ranging from simple hierarchical alignment models (Wu 1997, Chiang 2005) to syntax-based statistical systems (Yamada and Knight 2001, Zollmann and Venugopal 2006). What can traditional Rule-Based translation systems learn from these approaches? And would it not make sense to work from both sides towards that common goal: structurally rich statistical translation models. In this paper we study some enhancements for a Transfer-Based translation system, using techniques and even components developed for statistical machine translation. While the core engine remains virtually untouched, additional features are added to re-score the n-best list generated by the transfer engine. Statistical alignment techniques are used to lower"
W07-0410,W06-1626,1,0.881549,"Missing"
W07-0410,P03-1021,0,0.00548075,"well as the n-gram LM). 5 MER Training Like in SMT systems, in the Xfer engine translations are ranked to their total cost, which is a weighted linear combination of the individual costs. When adding more features to the translation system, a careful balancing of the individual contributions can make a significant difference. However, with each feature added, manually tuning the system becomes less and less practical, and automatic optimization becomes necessary. Different optimization techniques are available, like the Simplex algorithm or the special Minimum Error Training as described in (Och 2003). In Minimum Error Rate (MER) training, the n-best list generated by the translation system is used to find feature weight, thereby re-ranking the n-best list. This improves the match between the 1-best Rule Based + Stat. Comp. Optimizing weights with MER training translation and given reference translations. Optimization can use any metric as objective function. Typically, systems are tuned towards high BLEU or high NIST scores, more recently also towards METEOR or TER (Snover et al. 2006). We used a MER training module (Venugopal), originally developed for an SMT system, to run MER training"
W07-0410,W06-3119,0,0.0164624,"lation systems has dramatically improved over the last 5 to 10 years. Does this mean that work on grammar-based systems should be stopped? Should all the insight into the structure of languages be neglected? This might be too drastic a reaction. Actually, now that SMT has reached some maturity, we see several Stephan Vogel Carnegie Mellon University 5000 Forbes Ave. Pittsburgh, PA, 15213 vogel+@cs.cmu.edu attempts to integrate more structure into these systems, ranging from simple hierarchical alignment models (Wu 1997, Chiang 2005) to syntax-based statistical systems (Yamada and Knight 2001, Zollmann and Venugopal 2006). What can traditional Rule-Based translation systems learn from these approaches? And would it not make sense to work from both sides towards that common goal: structurally rich statistical translation models. In this paper we study some enhancements for a Transfer-Based translation system, using techniques and even components developed for statistical machine translation. While the core engine remains virtually untouched, additional features are added to re-score the n-best list generated by the transfer engine. Statistical alignment techniques are used to lower the burden in building a lexi"
W07-0410,P02-1040,0,0.0789238,"er, oracle scores provide the margin that we can gain when improving on the re-ranking of the n-best list produced by the Xfer engine. Table 2: Automatic metric scores for a purely Rule-Based MT System. 3.5 Table 2 shows that, in this crude setting, different automatic metrics do not agree on the translation accuracy of both systems. On one hand, METEOR (Lavie et al. 2004), which has been shown to correlate well with human judgments (Snover et al. 2006), indicates that the refined system outperforms the baseline system (as measured by the latest version v0.5.1,). On the other hand, both BLEU (Papineni et al., 2002) and NIST (Doddington 2002) scores are higher for the baseline system (mteval-v11b.pl). However, human inspection revealed that the refined grammar is able to augment the n-best list with correct translations that the baseline system was not able to generate. This suggests that these results reflect poor re-ranking and not n-best list quality. In the next section, we describe an oracle experiment to measure n-best list quality of both systems. Xfer Results with Initial Ranking As expected, when the Xfer system is run in combination with a LM1 as well as the fragmentation penalty, automatic met"
W07-0727,2005.mtsummit-papers.11,0,0.0174991,"3 2 have 1.0000 Members 0.6701 , 1.0000 4 we 1.0000 honourable 0.3299 6 5 a 1.0000 a 0.0825 8 7 … have 0.9175 Figure 2: Example for a source sentence lattice from the POS-based reordering scheme. sentence pairs unique sent. pairs sentence length words vocabulary English Spanish 1259914 1240151 25.3 26.3 31.84 M 33.16 M 266.9 K 346.3 K Table 1: Corpus statistics for the English/Spanish Europarl corpus. 3 3.1 Spanish ↔ English Europarl and News Commentary Task Data and Translation Tasks The systems for the English ↔ Spanish translation tasks were trained on the sentence-aligned Europarl corpus (Koehn, 2005). Detailed corpus statistics can be found in Table 1. The available parallel News Commentary training data of approximately 1 million running words for both languages was only used as additional language model training data, to adapt our in-domain (Europarl) system to the out-ofdomain (News Commentary) task. The development sets consist of 2000 Europarl sentences (dev-EU) and 1057 News Commentary sentences (dev-NC). The available developmenttest data consists of 2 x 2000 Europarl sentences (devtest-EU and test06-EU) and 1064 News Commentary sentences (test06-NC). All development and developmen"
W07-0727,P00-1056,0,0.0622652,"↔ Spanish Europarl and News Commentary tasks, along with corresponding performance numbers. Section 4 shows the data, final systems and results for the English ↔ German Europarl task. In Section 5, we present our experiments involving a combination of the syntax-augmented MT system with the phrase-based MT system and a combination of the Spanish → English and German → English phrase-based systems. 2 2.1 The ISL Phrase-Based MT System Word and Phrase Alignment Phrase-to-phrase translation pairs are extracted by training IBM Model-4 word alignments in both directions, using the GIZA++ toolkit (Och and Ney, 2000), and then extracting phrase pair candidates which are consistent with these alignments, starting from the intersection of both alignments. This is done with the help of phrase model training code provided by University of Edinburgh during the NAACL 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). The raw rel197 Proceedings of the Second Workshop on Statistical Machine Translation, pages 197–202, c Prague, June 2007. 2007 Association for Computational Linguistics ative frequency estimates found in the phrase translation tables are then smoothed by applying modified Knes"
W07-0727,2001.mtsummit-papers.46,0,0.169661,"201 PHRA 31.77 31.76 SYNT 32.48 32.15 COMB 32.77 32.27 Table 6: Results for combining the syntaxaugmented system (SYNT) with the phrase-based system (PHRA). augmented system was trained on the same normalized data as the phrase-based system. However, it was optimized on the in-domain development set only. More details on the syntax-augmented system can be found in (Zollmann et al., 2007). Table 6 lists the respective BLEU scores of both systems as well as the BLEU score achieved by combining and rescoring the individual 500-best lists. 5.3 Combining MT Systems with Different Source Languages (Och and Ney, 2001) describes methods for translating text given in multiple source languages into a single target language. The ultimate goal is to improve the translation quality when translating from one source language, for example English into multiple target languages, such as Spanish and German. This can be done by first translating the English document into German and then using the translation as an additional source, when translating to Spanish. Another scenario where a multi-source translation becomes desirable was described in (Paulik et al., 2005). The goal was to improve the quality of automatic sp"
W07-0727,W06-1607,0,0.0215739,"ndidates which are consistent with these alignments, starting from the intersection of both alignments. This is done with the help of phrase model training code provided by University of Edinburgh during the NAACL 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). The raw rel197 Proceedings of the Second Workshop on Statistical Machine Translation, pages 197–202, c Prague, June 2007. 2007 Association for Computational Linguistics ative frequency estimates found in the phrase translation tables are then smoothed by applying modified Kneser-Ney discounting as explained in (Foster et al., 2006). The resulting phrase translation tables are pruned by using the combined translation model score as determined by Minimum Error Rate (MER) optimization on the development set. 2.2 ⇒ PRP DT VB IN DT : 4 – 5 – 1 – 2 – 3 ⇒ PRP DT VB: 2 – 3 – 1 ⇒ PRP DT VB IN: 3 – 4 – 1 – 2 Figure 1: Rule extraction for the POS-based reordering scheme. Word Reordering We apply a part-of-speech (POS) based reordering scheme (J. M. Crego et al., 2006) to the POS-tagged source sentences before decoding. For this, we use the GIZA++ alignments and the POS-tagged source side of the training corpus to learn reordering"
W07-0727,W06-3125,0,0.0659682,"Missing"
W07-0727,E03-1076,0,0.0276122,"to the training data as for the English ↔ Spanish system. The main difference was that we did not replace numbers and that we removed all document references. In the translation process, the document references were treated as unknown words and therefore left unchanged. As above, we trained and optimized a first baseline system on the normalized source and reference sentences. However, we used only the Europarl task development set during optimization. To achieve further improvements on the German → English task, we applied a compound splitting technique. The compound splitting was based on (Koehn and Knight, 2003) and was applied on the lowercased source sentences. The words generated by the compound splitting were afterwards true-cased. Instead of replacing a compound by its separate parts, we added a parallel path into the source sentence lattices used for translation. The source sentence lattices were augmented with scores on their edges indicating whether each edge represents a word of the original text or if it was generated during compound splitting. Table 5 shows the case-sensitive BLEU scores for the final German ↔ English systems. In contrast to the English ↔ Spanish systems, we used only mono"
W07-0727,W06-3114,0,0.0553151,"d a combination of the Spanish → English and German → English phrase-based systems. 2 2.1 The ISL Phrase-Based MT System Word and Phrase Alignment Phrase-to-phrase translation pairs are extracted by training IBM Model-4 word alignments in both directions, using the GIZA++ toolkit (Och and Ney, 2000), and then extracting phrase pair candidates which are consistent with these alignments, starting from the intersection of both alignments. This is done with the help of phrase model training code provided by University of Edinburgh during the NAACL 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). The raw rel197 Proceedings of the Second Workshop on Statistical Machine Translation, pages 197–202, c Prague, June 2007. 2007 Association for Computational Linguistics ative frequency estimates found in the phrase translation tables are then smoothed by applying modified Kneser-Ney discounting as explained in (Foster et al., 2006). The resulting phrase translation tables are pruned by using the combined translation model score as determined by Minimum Error Rate (MER) optimization on the development set. 2.2 ⇒ PRP DT VB IN DT : 4 – 5 – 1 – 2 – 3 ⇒ PRP DT VB: 2 – 3 – 1 ⇒ PRP DT VB IN: 3 – 4"
W07-0727,H05-1096,0,0.0227669,"n-best list rescoring we used unique 500-best lists, which may have less than 500 entries for some sentences. In this evaluation, we used several features computed from different information sources such as features from the translation system, additional language models, IBM-1 word lexica and the n-best list itself. We calculated 4 features from the IBM-1 word lexica: the word probability sum as well as the maximum word probability in both language directions. From the n-best list itself, we calculated three different sets of scores. A position-dependent word agreement score as described in (Ueffing and Ney, 2005) with a position window instead of the Levenshtein alignment, the n-best list n-gram probability as described in (Zens and Ney, 2006) and a position-independent n-gram agreement, which is a variation on the first two. To tune the feature combination weights, we used MER optimization. Rescoring the n-best lists from our individual systems did not give significant improvements on the available unseen development-test data. For this reason, we did not apply n-best list rescoring to the individual systems. However, we investigated the feasibility of combining two different systems by rescoring the"
W07-0727,W07-0731,1,\N,Missing
W07-0727,W06-3110,0,\N,Missing
W07-0727,P03-1021,0,\N,Missing
W07-0731,P05-1033,0,0.610041,"ranslation results for the Spanish to English in-domain track of the shared task and discuss relative performance against our phrase-based submission. we used in our shared task submission. Finally, we compare our translation results to the CMU-UKA phrase-based SMT system and discuss relative performance. 2 Synchronous Grammars for SMT Probabilistic synchronous context-free grammars (PSCFGs) are defined by a source terminal set (source vocabulary) TS , a target terminal set (target vocabulary) TT , a shared nonterminal set N and production rules of the form X → hγ, α, ∼, wi 1 where following (Chiang, 2005) Introduction As Chiang (2005) and Koehn et al. (2003) note, purely lexical “phrase-based” translation models suffer from sparse data effects when translating conceptual elements that span or skip across several source language words. Phrase-based models also rely on distance and lexical distortion models to represent the reordering effects across language pairs. However, such models are typically applied over limited source sentence ranges to prevent errors introduced by these models and to maintain efficient decoding (Och and Ney, 2004). To address these concerns, hierarchically structured m"
W07-0731,N04-1035,0,0.174701,"probabilistic model of translation applied by the SAMT toolkit. We then present settings for the pipeline of SAMT tools that • X ∈ N is a nonterminal • γ ∈ (N ∪ TS )∗ : sequence of source nonterminals and terminals • α ∈ (N ∪ TT )∗ : sequence of target nonterminals and terminals • the count #NT(γ) of nonterminal tokens in γ is equal to the count #NT(α) of nonterminal tokens in α, • ∼: {1, . . . , #NT(γ)} → {1, . . . , #NT(α)} oneto-one mapping from nonterminal tokens in γ to nonterminal tokens in α • w ∈ [0, ∞) : nonnegative real-valued weight Chiang (2005) uses a single nonterminal category, Galley et al. (2004) use syntactic constituents for the PSCFG nonterminal set, and Zollmann and Venugopal (2006) take advantage of CCG (Combinatorial Categorical Grammar) (Steedman, 1999) inspired “slash” and “plus” categories, focusing on target (rather than source side) categories to generate well formed translations. We now describe the identification and estimation of PSCFG rules from parallel sentence aligned corpora under the framework proposed by Zollmann and Venugopal (2006). 216 Proceedings of the Second Workshop on Statistical Machine Translation, pages 216–219, c Prague, June 2007. 2007 Association for"
W07-0731,N03-1017,0,0.0294688,"-domain track of the shared task and discuss relative performance against our phrase-based submission. we used in our shared task submission. Finally, we compare our translation results to the CMU-UKA phrase-based SMT system and discuss relative performance. 2 Synchronous Grammars for SMT Probabilistic synchronous context-free grammars (PSCFGs) are defined by a source terminal set (source vocabulary) TS , a target terminal set (target vocabulary) TT , a shared nonterminal set N and production rules of the form X → hγ, α, ∼, wi 1 where following (Chiang, 2005) Introduction As Chiang (2005) and Koehn et al. (2003) note, purely lexical “phrase-based” translation models suffer from sparse data effects when translating conceptual elements that span or skip across several source language words. Phrase-based models also rely on distance and lexical distortion models to represent the reordering effects across language pairs. However, such models are typically applied over limited source sentence ranges to prevent errors introduced by these models and to maintain efficient decoding (Och and Ney, 2004). To address these concerns, hierarchically structured models as in Chiang (2005) define weighted transduction"
W07-0731,J04-4002,0,0.442706,"uction rules of the form X → hγ, α, ∼, wi 1 where following (Chiang, 2005) Introduction As Chiang (2005) and Koehn et al. (2003) note, purely lexical “phrase-based” translation models suffer from sparse data effects when translating conceptual elements that span or skip across several source language words. Phrase-based models also rely on distance and lexical distortion models to represent the reordering effects across language pairs. However, such models are typically applied over limited source sentence ranges to prevent errors introduced by these models and to maintain efficient decoding (Och and Ney, 2004). To address these concerns, hierarchically structured models as in Chiang (2005) define weighted transduction rules, interpretable as components of a probabilistic synchronous grammar (Aho and Ullman, 1969) that represent translation and reordering operations. In this work, we describe results from the open-source Syntax Augmented Machine Translation (SAMT) toolkit (Zollmann and Venugopal, 2006) applied to the Spanish-to-English in-domain translation task of the ACL’07 workshop on statistical machine translation. We begin by describing the probabilistic model of translation applied by the SAM"
W07-0731,P03-1021,0,0.00919098,"ined in terms of the rules r that are used in D: Q Q pLM (tgt(D))λLM r∈D i φi (r)λi p(D) = (2) Z(λ) where φi refers to features defined on each rule, pLM is a language model (LM) probability applied to the target terminal symbols generated by the derivation D, and Z(λ) is a normalization constant chosen such that the probabilities sum up to one. The computational challenges of this search task (compounded by the integration of the LM) are addressed in (Chiang, 2007; Venugopal et al., 2007). The feature weights λi are trained in concert with the LM weight via minimum error rate (MER) training (Och, 2003). We now describe the parameters for the SAMT implementation of the model described above. 3 SAMT Components SAMT provides tools to perform grammar induction ( “extractrules”, “filterrules”), from bilingual phrase pairs and target language parse trees, as well as translation (“FastTranslateChart”) of source sentences given an induced grammar. 3.1 extractrules extractrules is the first step of the grammar induction pipeline, where rules are identified based on the process described in section 2.1. This tool works on a per sentence basis, considering phrases extracted for the training sentence p"
W07-0731,P99-1039,0,0.0122088,"uence of source nonterminals and terminals • α ∈ (N ∪ TT )∗ : sequence of target nonterminals and terminals • the count #NT(γ) of nonterminal tokens in γ is equal to the count #NT(α) of nonterminal tokens in α, • ∼: {1, . . . , #NT(γ)} → {1, . . . , #NT(α)} oneto-one mapping from nonterminal tokens in γ to nonterminal tokens in α • w ∈ [0, ∞) : nonnegative real-valued weight Chiang (2005) uses a single nonterminal category, Galley et al. (2004) use syntactic constituents for the PSCFG nonterminal set, and Zollmann and Venugopal (2006) take advantage of CCG (Combinatorial Categorical Grammar) (Steedman, 1999) inspired “slash” and “plus” categories, focusing on target (rather than source side) categories to generate well formed translations. We now describe the identification and estimation of PSCFG rules from parallel sentence aligned corpora under the framework proposed by Zollmann and Venugopal (2006). 216 Proceedings of the Second Workshop on Statistical Machine Translation, pages 216–219, c Prague, June 2007. 2007 Association for Computational Linguistics 2.1 Grammar Induction Zollmann and Venugopal (2006) describe a process to generate a PSCFG given parallel sentence pairs hf, ei, a parse tre"
W07-0731,N07-1063,1,0.794856,"d by derivation D. Our distribution p over derivations is defined by a log-linear model. The probability of a derivation D is defined in terms of the rules r that are used in D: Q Q pLM (tgt(D))λLM r∈D i φi (r)λi p(D) = (2) Z(λ) where φi refers to features defined on each rule, pLM is a language model (LM) probability applied to the target terminal symbols generated by the derivation D, and Z(λ) is a normalization constant chosen such that the probabilities sum up to one. The computational challenges of this search task (compounded by the integration of the LM) are addressed in (Chiang, 2007; Venugopal et al., 2007). The feature weights λi are trained in concert with the LM weight via minimum error rate (MER) training (Och, 2003). We now describe the parameters for the SAMT implementation of the model described above. 3 SAMT Components SAMT provides tools to perform grammar induction ( “extractrules”, “filterrules”), from bilingual phrase pairs and target language parse trees, as well as translation (“FastTranslateChart”) of source sentences given an induced grammar. 3.1 extractrules extractrules is the first step of the grammar induction pipeline, where rules are identified based on the process describe"
W07-0731,W06-3119,1,0.911536,"esent the reordering effects across language pairs. However, such models are typically applied over limited source sentence ranges to prevent errors introduced by these models and to maintain efficient decoding (Och and Ney, 2004). To address these concerns, hierarchically structured models as in Chiang (2005) define weighted transduction rules, interpretable as components of a probabilistic synchronous grammar (Aho and Ullman, 1969) that represent translation and reordering operations. In this work, we describe results from the open-source Syntax Augmented Machine Translation (SAMT) toolkit (Zollmann and Venugopal, 2006) applied to the Spanish-to-English in-domain translation task of the ACL’07 workshop on statistical machine translation. We begin by describing the probabilistic model of translation applied by the SAMT toolkit. We then present settings for the pipeline of SAMT tools that • X ∈ N is a nonterminal • γ ∈ (N ∪ TS )∗ : sequence of source nonterminals and terminals • α ∈ (N ∪ TT )∗ : sequence of target nonterminals and terminals • the count #NT(γ) of nonterminal tokens in γ is equal to the count #NT(α) of nonterminal tokens in α, • ∼: {1, . . . , #NT(γ)} → {1, . . . , #NT(α)} oneto-one mapping from"
W07-0731,W07-0727,1,\N,Missing
W07-0731,J07-2003,0,\N,Missing
W08-0303,atserias-etal-2006-freeling,0,0.00882796,"Missing"
W08-0303,P06-1009,0,0.580374,"d a given target sentence eI1 a set of links (j, i) has to be found, which describes which source word fj is translated into which target word ei . Most SMT systems use the freely available GIZA++-Toolkit to generate the word alignment. This toolkit implements the IBM- and HMMmodels introduced in (Brown et al., 1993; Vogel et al., 1996). They have the advantage that they are trained unsupervised and are well suited for a noisychannel approach. But it is difficult to include additional features into these models. In recent years several authors (Moore et al., 2006; Lacoste-Julien et al., 2006; Blunsom and Cohn, 2006) proposed discriminative word alignment frameworks and showed that this leads to improved alignment quality. In contrast to generative models, these models need a small amount of handaligned data. But it is easy to add features to these models, so all available knowledge sources can be used to find the best alignment. The discriminative model presented in this paper uses a conditional random field (CRF) to model the alignment matrix. By modeling the matrix no restrictions to the alignment are required and even n:m alignments can be generated. Furthermore, this makes the model symmetric, so the"
W08-0303,J95-4004,0,0.174901,"weights constant. Initial results using a Gaussian prior showed no improvement. 4 Evaluation The word alignment quality of this approach was tested on three different language pairs. On the 22 Spanish-English task the hand-aligned data provided by the TALP Research Center (Lambert et al., 2005) was used. As proposed, 100 sentences were used as development data and 400 as test data. The so called “Final Text Edition of the European Parliament Proceedings” consisting of 1.4 million sentences and this hand-aligned data was used as training corpus. The POS-tags were generated by the Brill-Tagger (Brill, 1995) and the FreeLing-Tagger (Asterias et al., 2006) for the English and the Spanish text respectively. To limit the number of different tags for Spanish we grouped them according to the first 2 characters in the tag names. A second group of experiments was done on an English-French text. The data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003) was used. This data consists of 1.1 million sentences, a validation set of 37 sentences and a test set of 447 sentences, which have been hand-aligned (Och and Ney, 2003). For the English POS-tags again the Brill Tagger was used. For the French"
W08-0303,N03-1017,0,0.00430919,"nly the first 200 sentences of the development data were used to speed up the training process. The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al., 2005). The POS-tags for both sides were generated with the Stanford Parser (Klein and Manning, 2003). 4.1 Word alignment quality The GIZA++-toolkit was used to train a baseline system. The models and alignment information were then used as additional knowledge source for the discriminative word alignment. For the first two tasks, all heuristics of the Pharaoh-Toolkit (Koehn et al., 2003) as well as the refined heuristic (Och and Ney, 2003) to combine both IBM4-alignments were tested and the best ones are shown in the tables. For the Chinese task only the grow-diag-final heuristic was used. Table 1: AER-Results on EN-ES task Name IBM4 Source-Target IBM4 Target-Source IBM4 grow-diag DWA IBM1 + IBM4 + GIZA-fert. + Link feature + POS + Phrase feature Dev 15.26 14.23 13.28 12.26 9.21 8.84 Table 3: AER-Results on CH-EN task Test 21.49 19.23 16.48 20.82 18.67 18.02 15.97 15.36 14.77 Name IBM4 Source-target IBM4 Target-source IBM4 Grow-diag-final DWA IBM4 - similarity + Add. directio"
W08-0303,N06-1015,0,0.0622111,"given source sentence f1J and a given target sentence eI1 a set of links (j, i) has to be found, which describes which source word fj is translated into which target word ei . Most SMT systems use the freely available GIZA++-Toolkit to generate the word alignment. This toolkit implements the IBM- and HMMmodels introduced in (Brown et al., 1993; Vogel et al., 1996). They have the advantage that they are trained unsupervised and are well suited for a noisychannel approach. But it is difficult to include additional features into these models. In recent years several authors (Moore et al., 2006; Lacoste-Julien et al., 2006; Blunsom and Cohn, 2006) proposed discriminative word alignment frameworks and showed that this leads to improved alignment quality. In contrast to generative models, these models need a small amount of handaligned data. But it is easy to add features to these models, so all available knowledge sources can be used to find the best alignment. The discriminative model presented in this paper uses a conditional random field (CRF) to model the alignment matrix. By modeling the matrix no restrictions to the alignment are required and even n:m alignments can be generated. Furthermore, this makes th"
W08-0303,N06-1014,0,0.0353428,"d Cohn (2006). They also used CRFs, but they used two linear-chain CRFs, one for every directions. Consequently, they could find the optimal solution for each individual CRF, but they still needed the heuristics to combine both alignments. They reached an AER of 5.29 using the IBM4-alignment on the English-French task (compared to 4.30 of our approach). Lacoste-Julien et al. (2006) enriched the bipartite matching problem to model also larger fertilities and first-or der dependencies. They could reach an AER of 3.8 on the same task, but only if they also included the posteriors of the model of Liang et al. (2006). Using only the IBM4-alignment they generated an alignment with an AER of 4.5. But they did not use any POS-based features in their experiments. Finally, Moore et al. (2006) used a log-linear model for the features and performed a beam search. They could reach an AER as low as 3.7 with both types of alignment information. But they presented no results using only the IBM4-alignment features. 6 Conclusion In this paper a new discriminative word alignment model was presented. It uses a conditional random field to model directly the alignment matrix. Therefore, the algorithms used in the CRFs had"
W08-0303,W03-0301,0,0.0259111,"ed as development data and 400 as test data. The so called “Final Text Edition of the European Parliament Proceedings” consisting of 1.4 million sentences and this hand-aligned data was used as training corpus. The POS-tags were generated by the Brill-Tagger (Brill, 1995) and the FreeLing-Tagger (Asterias et al., 2006) for the English and the Spanish text respectively. To limit the number of different tags for Spanish we grouped them according to the first 2 characters in the tag names. A second group of experiments was done on an English-French text. The data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003) was used. This data consists of 1.1 million sentences, a validation set of 37 sentences and a test set of 447 sentences, which have been hand-aligned (Och and Ney, 2003). For the English POS-tags again the Brill Tagger was used. For the French side, the TreeTagger (Schmid, 1994) was used. Finally, to test our alignment approach with languages that differ more in structure a ChineseEnglish task was selected. As hand-aligned data 3160 sentences aligned only with sure links were used (LDC2006E93). This was split up into 2000 sentences of test data and 1160 sentences of development data. In some"
W08-0303,P06-1065,0,0.120607,"ed. Therefore, for a given source sentence f1J and a given target sentence eI1 a set of links (j, i) has to be found, which describes which source word fj is translated into which target word ei . Most SMT systems use the freely available GIZA++-Toolkit to generate the word alignment. This toolkit implements the IBM- and HMMmodels introduced in (Brown et al., 1993; Vogel et al., 1996). They have the advantage that they are trained unsupervised and are well suited for a noisychannel approach. But it is difficult to include additional features into these models. In recent years several authors (Moore et al., 2006; Lacoste-Julien et al., 2006; Blunsom and Cohn, 2006) proposed discriminative word alignment frameworks and showed that this leads to improved alignment quality. In contrast to generative models, these models need a small amount of handaligned data. But it is easy to add features to these models, so all available knowledge sources can be used to find the best alignment. The discriminative model presented in this paper uses a conditional random field (CRF) to model the alignment matrix. By modeling the matrix no restrictions to the alignment are required and even n:m alignments can be generate"
W08-0303,J03-1002,0,0.02279,"a was used as training corpus. The POS-tags were generated by the Brill-Tagger (Brill, 1995) and the FreeLing-Tagger (Asterias et al., 2006) for the English and the Spanish text respectively. To limit the number of different tags for Spanish we grouped them according to the first 2 characters in the tag names. A second group of experiments was done on an English-French text. The data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003) was used. This data consists of 1.1 million sentences, a validation set of 37 sentences and a test set of 447 sentences, which have been hand-aligned (Och and Ney, 2003). For the English POS-tags again the Brill Tagger was used. For the French side, the TreeTagger (Schmid, 1994) was used. Finally, to test our alignment approach with languages that differ more in structure a ChineseEnglish task was selected. As hand-aligned data 3160 sentences aligned only with sure links were used (LDC2006E93). This was split up into 2000 sentences of test data and 1160 sentences of development data. In some experiments only the first 200 sentences of the development data were used to speed up the training process. The FBIS-corpus was used as training corpus and all Chinese s"
W08-0303,N03-1028,0,0.018501,"th the weights Θ. Then the probability of an assignment of the random variables, which corresponds to a word alignment, can be expressed as: pΘ (y|e, f ) = Y 1 Φc (Vc ) Z(e, f ) c∈V (2) FN with VF N the set of all factored nodes in the graph, and the normalization factor Z(e, f ) defined as: 2 Z(e, f ) = The Model X Y Φc (Vc ) (3) Y c∈VF N In the approach presented here the word alignment matrix is modeled by a conditional random field (CRF). A CRF is an unidirectional graphical model. It models the conditional distribution over random variables. In most applications like (Tseng et al., 2005; Sha and Pereira, 2003), a sequential model is used. But to model the alignment matrix the graphical structure of the model is more complex. The alignment matrix is described by a random variable yji for every source and target word pair (fj , ei ). These variables can have two values, 0 and 1, indicating whether these words are translations of each other or not. An example is shown in Figure 1. Gray circles represent variables with value 1, white circles stand for variables with value 0. Consequently, a word with zero fertility is indirectly modeled by setting all associated random variables to a value of 0. The st"
W08-0303,P06-1028,0,0.00658086,". So we developed a method to optimize the CRFs towards the alignment error rate (AER) or the F-score with sure and possible links as introduced in (Fraser and Marcu, 2007). The advantage of the F-score is, that there is an additional parameter α, which allows to bias the metric more towards precision or more towards recall. To be able to use a gradient descent method to optimize the weights, the derivation of the word alignment metric with respect to these weights must be computed. This cannot be done for the mentioned metrics since they are not smooth functions. We follow (Gao et al., 2006; Suzuki et al., 2006) and approximate the metrics using the sigmoid function. The sigmoid function uses the probabilities for every link calculated by the belief propagation algorithm. In our experiments we compared the maximum likelihood method and the optimization towards the AER. We also tested combinations of both. The best results were obtained when the weights were first trained using the ML method and the resulting factors were used as initial values for the AER optimization. Another problem is that the POS-based features and high frequency word features have a lot more parameters than all other features an"
W08-0303,C96-2141,1,0.496512,"nificantly. 1 Introduction In machine translation parallel corpora are one very important knowledge source. These corpora are often aligned at the sentence level, but to use them in the systems in most cases a word alignment is needed. Therefore, for a given source sentence f1J and a given target sentence eI1 a set of links (j, i) has to be found, which describes which source word fj is translated into which target word ei . Most SMT systems use the freely available GIZA++-Toolkit to generate the word alignment. This toolkit implements the IBM- and HMMmodels introduced in (Brown et al., 1993; Vogel et al., 1996). They have the advantage that they are trained unsupervised and are well suited for a noisychannel approach. But it is difficult to include additional features into these models. In recent years several authors (Moore et al., 2006; Lacoste-Julien et al., 2006; Blunsom and Cohn, 2006) proposed discriminative word alignment frameworks and showed that this leads to improved alignment quality. In contrast to generative models, these models need a small amount of handaligned data. But it is easy to add features to these models, so all available knowledge sources can be used to find the best alignm"
W08-0303,2004.tmi-1.9,1,0.731318,"Since the main application of the word alignment is statistical machine translation, the aim was not only to generate better alignments measured in AER, but also to generate better translations. Therefore, the word alignment was used to extract phrases and use them then in the translation system. In all translation experiments the beam decoder as described in (Vogel, 2003) was used together with a 3-gram language model and the results are reported in the BLUE metric. For test set translations the statistical significance of the results was tested using the bootstrap technique as described in (Zhang and Vogel, 2004). The baseline system used the phrases build with the Pharaoh-Toolkit. The new word alignment was tested on the English-Spanish translation task using the TC-Star 07 development and test data. The discriminative word alignment (DWA) used the configuration denoted by +POS system in Table 1. With this configuration it took around 4 hours to align 100K sentences. But, of course, generating the alignment can be parallelized to speed up the process. As shown in Table 4 the new word alignment could generate better translations as measured in BLEU scores. 24 Comparison to other work Several discrimin"
W08-0303,J93-2003,0,\N,Missing
W08-0303,J07-3002,0,\N,Missing
W08-0303,I05-3027,0,\N,Missing
W08-0321,J93-2003,0,0.0249804,"s are observed on the NewsCommentary test sets. Genre-dependent sentence pair confidence score and integration of sentence pair confidence score into phrase table are also investigated. 1 Introduction Word alignment models are a crucial component in statistical machine translation systems. When estimating the parameters of the word alignment models, the sentence pair probability is an important factor in the objective function and is approximated by the empirical probability. The empirical probability for each sentence pair is estimated by maximum likelihood estimation over the training data (Brown et al., 1993). Due to the limitation of training data, most sentence pairs occur only once, which makes the empirical probability almost uniform. This is a rather weak approximation of the true distribution. In this paper, we investigate the methods of weighting sentence pairs using language models, and extended the general weighting method to genre-dependent weight. A method of integrating the weight directly into the phrase table is also explored. 2 The Baseline Phrase-Based MT System The ACL-WMT08 organizers provided Europarl and News-Commentary parallel corpora for English ↔ Spanish. Detailed corpus st"
W08-0321,W07-0718,0,0.0197935,"rs (Gao and Vogel, 2008). Other parameters were the same as the baseline system. Table 2 shows results in lowercase BLEU (Papineni et al., 2002) for both the baseline (B) and the improved baseline systems (B5) on development and held151 Proceedings of the Third Workshop on Statistical Machine Translation, pages 151–154, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics out evaluation sets. We observed significant gains for the News-Commentary test sets. Our improved baseline systems obtained a comparable performance with the best English↔Spanish systems in 2007 (Callison-Burch et al., 2007). Pairs En→Es Es→En B B5 B B5 Europarl E06 E07 33.00 32.21 33.33 32.25 33.08 33.23 33.26 33.23 NC NCd 31.84 35.10 31.18 36.06 NCt 30.56 34.08 31.34 35.56 3.2 Sentence Pair Confidence Table 2: NIST-BLEU scores of baseline and improved baseline systems experiments on English↔Spanish 3 Weighting Sentence Pairs 3.1 Problem Definition The quality of word alignment is crucial for the performance of the machine translation system. In the well-known so-called IBM word alignment models (Brown et al., 1993), re-estimating the model parameters depends on the empirical probability Pˆ (ek , f k ) for each"
W08-0321,W07-0722,0,0.0159228,"ek ) + L(f k )]/2 fjk ∈f k (3) where P (eki |h) and P (fjk |h) are ngram probabilities. The sentence pair confidence score is then given by: sc(ek , f k ) = exp(L(ek , f k )). (4) 3.3 Genre-Dependent Sentence Pair Confidence Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007). To overcome these problems previous works have focused on explicitly modeling topics and on using multiple language and translation models. Using a mixture of topicdependent Viterbi alignments was proposed in (Civera and Juan, 2007). Language and translation model adaptation to Europarl and News-Commentary have been explored in (Paulik et al., 2007). Given the sentence pair weighting method, it is possible to adopt genre-specific language models into the k k gdsc(e , f ) = sc(e , f |g) (5) where P (eki |h) and P (fjk |h) are estimated by genrespecific language models. The score generally represents the likelihood of the sentence pair to be in a specific genre. Thus, if both sides of the sentence pair show a high probability according to the genre-specific language models, alignments in the pair should be more possible to"
W08-0321,W08-0509,1,0.756072,"56.9 K Table 1: Statistics of English↔Spanish Europarl and NewsCommentary corpora To improve the baseline performance we trained systems on all true-cased training data with sentence length up to 100. We used two language models, a 5-gram LM build from the Europarl corpus and a 3-gram LM build from the News-Commentary data. Instead of interpolating the two language models, we explicitly used them in the decoder and optimized their weights via minimumerror-rate (MER) training (Och, 2003). To shorten the training time, a multi-threaded GIZA++ version was used to utilize multi-processor servers (Gao and Vogel, 2008). Other parameters were the same as the baseline system. Table 2 shows results in lowercase BLEU (Papineni et al., 2002) for both the baseline (B) and the improved baseline systems (B5) on development and held151 Proceedings of the Third Workshop on Statistical Machine Translation, pages 151–154, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics out evaluation sets. We observed significant gains for the News-Commentary test sets. Our improved baseline systems obtained a comparable performance with the best English↔Spanish systems in 2007 (Callison-Burch et al., 2"
W08-0321,W07-0733,0,0.0260283,"ch sentence pair was calculated by applying the corresponding language model. For each sentence pair (ek , f k ), the AVG-LL L(ek , f k ) is L(ek ) = 1 |ek | 1 |f k | P ek ∈ek i P log P (eki |h) log P (fjk |h) L(f k ) = L(ek , f k ) = [L(ek ) + L(f k )]/2 fjk ∈f k (3) where P (eki |h) and P (fjk |h) are ngram probabilities. The sentence pair confidence score is then given by: sc(ek , f k ) = exp(L(ek , f k )). (4) 3.3 Genre-Dependent Sentence Pair Confidence Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007). To overcome these problems previous works have focused on explicitly modeling topics and on using multiple language and translation models. Using a mixture of topicdependent Viterbi alignments was proposed in (Civera and Juan, 2007). Language and translation model adaptation to Europarl and News-Commentary have been explored in (Paulik et al., 2007). Given the sentence pair weighting method, it is possible to adopt genre-specific language models into the k k gdsc(e , f ) = sc(e , f |g) (5) where P (eki |h) and P (fjk |h) are estimated by genrespecific language models. The score generally rep"
W08-0321,P07-2045,0,0.0235016,"Missing"
W08-0321,J03-1002,0,0.00938956,"English↔Spanish 3 Weighting Sentence Pairs 3.1 Problem Definition The quality of word alignment is crucial for the performance of the machine translation system. In the well-known so-called IBM word alignment models (Brown et al., 1993), re-estimating the model parameters depends on the empirical probability Pˆ (ek , f k ) for each sentence pair (ek , f k ). During the EM training, all counts of events, e.g. word pair counts, distortion model counts, etc., are weighted by Pˆ (ek , f k ). For example, in IBM Model 1 the lexicon probability of source word f given target word e is calculated as (Och and Ney, 2003): p(f |e) = P c(f |e; ek , f k ) Pk k k k,f c(f |e; ek , f k ) = X ek ,f k X (1) c(f |e; e , f ) Pˆ (ek , f k ) X To get a more informative Pˆ (ek , f k ), we explored methods of weighting sentence pairs. We investigated three sets of features: sentence pair confidence (sc), genre-dependent sentence pair confidence (gdsc) and phrase alignment confidence (pc) scores. These features were calculated over an entire training corpus and could be easily integrated into the phrase-based machine translation system. P (a|ek , f k ) · (2) a δ(f , fjk )δ(e, ekaj ) j Therefore, the distribution of Pˆ (ek ,"
W08-0321,P03-1021,0,0.0143332,"sentence pairs 64,308 unique sent. pairs 64,205 avg. sentence length 24.0 27.4 # words 1.54 M 1.76 M vocabulary 44.2 K 56.9 K Table 1: Statistics of English↔Spanish Europarl and NewsCommentary corpora To improve the baseline performance we trained systems on all true-cased training data with sentence length up to 100. We used two language models, a 5-gram LM build from the Europarl corpus and a 3-gram LM build from the News-Commentary data. Instead of interpolating the two language models, we explicitly used them in the decoder and optimized their weights via minimumerror-rate (MER) training (Och, 2003). To shorten the training time, a multi-threaded GIZA++ version was used to utilize multi-processor servers (Gao and Vogel, 2008). Other parameters were the same as the baseline system. Table 2 shows results in lowercase BLEU (Papineni et al., 2002) for both the baseline (B) and the improved baseline systems (B5) on development and held151 Proceedings of the Third Workshop on Statistical Machine Translation, pages 151–154, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics out evaluation sets. We observed significant gains for the News-Commentary test sets. Our im"
W08-0321,P02-1040,0,0.0769983,"we trained systems on all true-cased training data with sentence length up to 100. We used two language models, a 5-gram LM build from the Europarl corpus and a 3-gram LM build from the News-Commentary data. Instead of interpolating the two language models, we explicitly used them in the decoder and optimized their weights via minimumerror-rate (MER) training (Och, 2003). To shorten the training time, a multi-threaded GIZA++ version was used to utilize multi-processor servers (Gao and Vogel, 2008). Other parameters were the same as the baseline system. Table 2 shows results in lowercase BLEU (Papineni et al., 2002) for both the baseline (B) and the improved baseline systems (B5) on development and held151 Proceedings of the Third Workshop on Statistical Machine Translation, pages 151–154, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics out evaluation sets. We observed significant gains for the News-Commentary test sets. Our improved baseline systems obtained a comparable performance with the best English↔Spanish systems in 2007 (Callison-Burch et al., 2007). Pairs En→Es Es→En B B5 B B5 Europarl E06 E07 33.00 32.21 33.33 32.25 33.08 33.23 33.26 33.23 NC NCd 31.84 35.10 31"
W08-0321,W07-0727,1,\N,Missing
W08-0509,P07-2045,0,0.0422486,"Missing"
W08-0509,J93-2003,0,\N,Missing
W08-0509,C96-2141,1,\N,Missing
W08-0509,P02-1040,0,\N,Missing
W08-0509,J03-1002,0,\N,Missing
W08-2118,N04-4015,0,0.065123,"achine translation in order to reduce unknown words of highly inflected languages. Nießen and Ney (2004) represented a word as a vector of morphemes and gained improvement over word-based system for c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. German-English translation. Goldwater and Mcclosky (2005) improved Czech-English translation by applying different heuristics to increase the equivalence of Czech and English text. Specially for Arabic-English translation, Lee (2004) used the Arabic part of speech and English parts of speech (POS) alignment probabilities to retain an Arabic affix, drop it from the corpus or merge it back to a stem. The resulting system outperformed the original Arabic system trained on 3.3 million sentence pairs corpora when using monotone decoding. However, an improvement in monotone decoding is no guarantee for an improvement over the best baseline achievable with full word forms. Our experiments showed that an SMT phrase-based translation using 4 words distance reordering could gain four BLEU points over monotone decoding. Sadat and Ha"
W08-2118,J04-2003,0,0.0151659,"irs, and showed a potential improvement for a large-scale SMT system trained on 5 million sentence pairs. 1 Introduction Statistical machine translation (SMT) relies heavily on the word alignment model of the source and the target language. However, there is a mismatch between a rich morphology language (e.g Arabic, Czech) and a poor morphology language (e.g English). An Arabic source word often corresponds to several English words. Previous research has focused on attempting to apply morphological analysis to machine translation in order to reduce unknown words of highly inflected languages. Nießen and Ney (2004) represented a word as a vector of morphemes and gained improvement over word-based system for c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. German-English translation. Goldwater and Mcclosky (2005) improved Czech-English translation by applying different heuristics to increase the equivalence of Czech and English text. Specially for Arabic-English translation, Lee (2004) used the Arabic part of speech and English parts of speech (POS) alignment probabilities to r"
W08-2118,J03-1002,0,0.00381826,"on of an original Arabic word, we considered a morpheme ai as a stem if its parts of speech pi is either a noun, pronoun, verb, adjective, question, punctuation, number or abbreviation. A morpheme on the left of its word’s stem is a prefix and it is a suffix if otherwise. We removed case marker morphemes and got the full morphology corpus. 2.2 • Annotate a prefix or a suffix as “Retained” (R) if it is aligned to an English word. Annotate Morphemes To extract the Arabic morphemes that align to English text, we use English as the source corpus and aligned to Arabic morpheme corpus using GIZA++ (Och and Ney, 2003) toolkit. The IBM3 and IBM4 (Brown et al., 1994) word alignment models select each word in the source sentence, generate fertility and a list of target words that connect to it. This generative process would constrain source words to find alignments in the target sentence. Using English as source corpus, the alignment models force English words to generate their alignments in the Arabic morphemes. Note that the model does not assume that GIZA++ outputs accurate word alignments. We lessen the impact of the GIZA++ errors by only using the word alignment output of prefix and suffix morphemes. Fur"
W08-2118,P02-1040,0,0.0767425,"also increased word tokens quite substantially. By removing nonaligned morphemes, the reduced corpus is well balanced with the English corpus. The BTEC experiments used the 2004 IWSLT Evaluation Test set as development set and 2005 IWSLT Evaluation Test set as unseen test data. Table 2 gives the details of the two test sets. Both of them had 16 reference translations per source sentence. The English side of the training corpus was used to build the language model. To optimize the parameters of the decoder, we performed minimum error rate training on IWSLT04 optimizing for the IBM-BLEU metric (Papineni et al., 2002). 4.1.2 Newswire Corpora We also tested the impact of our morphology technique on parallel corpus in the news domain. The corpora were collected from LDC’s full Arabic news translation corpora and a small portion of UN data. The details of the data are give in Table 3. The data consists of 177K sentence pairs, 5.2M words on the Arabic and 6M words on the English side. Ori Sentences Tokens Types 5.2M 155K Arabic Full Reduced 177035 9.3M 6.2M 47K 47K Eng 6.2M 68K Table 3: Newswire corpus statistics We used two test sets from past NIST evaluations as test data. NIST MT03 was used as development s"
W08-2118,P06-1001,0,0.0720561,", Lee (2004) used the Arabic part of speech and English parts of speech (POS) alignment probabilities to retain an Arabic affix, drop it from the corpus or merge it back to a stem. The resulting system outperformed the original Arabic system trained on 3.3 million sentence pairs corpora when using monotone decoding. However, an improvement in monotone decoding is no guarantee for an improvement over the best baseline achievable with full word forms. Our experiments showed that an SMT phrase-based translation using 4 words distance reordering could gain four BLEU points over monotone decoding. Sadat and Habash (2006) explored a wide range of Arabic word-level preprocessing and produced better translation results for a system trained on 5 million Arabic words. What all the above methodologies do not provide is a means to disambiguate morphological analysis for machine translation based on the words’ contexts. That is, for an Arabic word analysis of the form prefix*-stem-suffix* a morpheme only is either always retained, always dropped off or always merged to the stem regardless of its surrounding text. In the example in Figure (1), the Arabic word “AlnAfi*h”(“window” in English) was segmented as “Al nAfi*"
W08-2118,N03-1028,0,0.0412952,"emes of a test sentence without the English reference based on Viterbi word alignment, we need to learn a morpheme tagging model. The model estimates the distributions of tagging sequences given a morphologically analysed sentence using the previous step’s annotated training data. The task of tagging morphemes to be either “Deleted” or “Retained” belongs to the set of sequence labelling problems. The conditional random fields (CRF) (Lafferty et al., 2001) model has shown great benefits in similar applications of natural language processing such as part-of-speech tagging, noun phrase chunking (Sha and Pereira, 2003), morphology disambiguation(Smith et al., 2005). We apply the CRF model to our morpheme tagging problem. 137 Let A = {(A, T)} be the full morphology training corpus where A = a1 |p1 a2 |p2 . . . am |pm is a morphology Arabic sentence, ai is a morpheme in the sentence and pi is its POS; T = t1 t2 . . . tm is the tag sequence of A, each ti is either “Deleted” or “Retained” . The CRF model estimates param∗ eter θ maximizing the conditional probability of the sequences of tags given the observed data: X ∗ (1) θ = argmax θ (A,T)∈A pe ((A, T)) log p T|A, θ  where pe ((A, T)) is the empirical distri"
W08-2118,H05-1060,0,0.128224,"s toolkit— words are separated by ‘ ’, (c) English translation and its alignment with full morphological analysis (d) Morphological analysis after removing unaligned morphemes. ing morpheme “nAfi*” aligned to the word “window” of the English translation. Thus an appropriate preprocessing technique should be guided by English translation and bring the word context into account. In this paper we describe a context-based morphological analysis for Arabic-English translation that take full account morphemes alignment to English text. The preprocessing uses the Arabic morphology disambiguation in (Smith et al., 2005) for full morphological analysis and learns the removing morphemes model based on the Viterbi alignment of English to full morphological analysis. We tested the model with two training corpora of 5.2 millions Arabic words(177K sentences) in news domain and 159K Arabic words (20K sentences) in travel conversation domain and gain improvement over the original Arabic translation in both experiments. The system that trained on a subsample corpora of 5 millions sentence pairs corpora also showed one BLEU score improvement over the original Arabic system on unseen test set. We will explain our techn"
W08-2118,takezawa-etal-2002-toward,0,0.0128106,"of phrase pairs up to seven words long. The system used a tri-gram language model built from SRI (Stolcke, 2002) toolkit with modified Kneser-Ney interpolation smoothing technique (Chen and Goodman, 1996). By default, the Moses decoder uses 6 tokens distance reordering windows. 4 Experiment Results In this section we present experiment results using our Arabic morphology preprocessing technique. 4.1 Data Sets We tested our morphology technique on a small data set of 20K sentence pairs and a medium size data set of 177K sentence pairs. 4.1.1 As small training data set we used the BTEC corpus (Takezawa et al., 2002) distributed by the International Workshop on Spoken Language Translation (IWSLT) (Eck and Hori, 2005). The corpus is a collection of conversation transcripts from the travel domain. Table 1 gives some deGiven a full morphology sentence A, we use the morpheme tagging model learnt as described in the previous section to decode A into the most probable sequence of tags T∗ = t1 t2 . . . tm .   ∗ T∗ = argmax Pr T|A, θ (3) Ori Sentences Tokens Types T If a ti is “Deleted”, the morpheme ai is removed from the morphology sentence A. The same procedure is applied to both training Arabic corpus and t"
W08-2118,P96-1041,0,0.0621296,"smoothed the parameters by assigning them Gaussian prior distributions. 3 Phrase-based SMT System We used the open source Moses (Koehn, 2007) phrase-based MT system to test the impact of the preprocessing technique on translation results. We kept the default parameter settings of Moses for translation model generation. The system used the “grow-diag-final” alignment combination heuristic. The phrase table consisted of phrase pairs up to seven words long. The system used a tri-gram language model built from SRI (Stolcke, 2002) toolkit with modified Kneser-Ney interpolation smoothing technique (Chen and Goodman, 1996). By default, the Moses decoder uses 6 tokens distance reordering windows. 4 Experiment Results In this section we present experiment results using our Arabic morphology preprocessing technique. 4.1 Data Sets We tested our morphology technique on a small data set of 20K sentence pairs and a medium size data set of 177K sentence pairs. 4.1.1 As small training data set we used the BTEC corpus (Takezawa et al., 2002) distributed by the International Workshop on Spoken Language Translation (IWSLT) (Eck and Hori, 2005). The corpus is a collection of conversation transcripts from the travel domain."
W08-2118,W02-1001,0,0.0117706,"Missing"
W08-2118,H05-1085,0,0.0638751,"h) and a poor morphology language (e.g English). An Arabic source word often corresponds to several English words. Previous research has focused on attempting to apply morphological analysis to machine translation in order to reduce unknown words of highly inflected languages. Nießen and Ney (2004) represented a word as a vector of morphemes and gained improvement over word-based system for c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. German-English translation. Goldwater and Mcclosky (2005) improved Czech-English translation by applying different heuristics to increase the equivalence of Czech and English text. Specially for Arabic-English translation, Lee (2004) used the Arabic part of speech and English parts of speech (POS) alignment probabilities to retain an Arabic affix, drop it from the corpus or merge it back to a stem. The resulting system outperformed the original Arabic system trained on 3.3 million sentence pairs corpora when using monotone decoding. However, an improvement in monotone decoding is no guarantee for an improvement over the best baseline achievable with"
W08-2118,J93-2003,0,\N,Missing
W08-2118,P07-2045,0,\N,Missing
W08-2118,2005.iwslt-1.1,0,\N,Missing
W09-0406,2008.amta-srw.3,1,0.93704,"n method is hypothesis selection, which uses information from n-best lists from several MT systems. The sentence level features are independent from the MT systems involved. To compensate for various n-best list sizes in the workshop shared task including firstbest-only entries, we normalize one of our high-impact features for varying sub-list size. We combined restricted data track entries in French - English, German - English and Hungarian - English using provided data only. 1 • Rank feature • Normalized n-gram agreement The details on language model and word lexicon scores can be found in (Hildebrand and Vogel, 2008). We use two sentence length features, which are the ratio of the hypothesis length to the length of the source sentence and the difference between the hypothesis length and the average length of the hypotheses in the n-best list for the respective source sentence. We also use the rank of the hypothesis in the original system’s n-best list as a feature. 2.1 Introduction The participants of the WMT’09 shared translation task provided output from their translation systems in various sizes. Most submission were 1st-best translation only, some submitted 10-best up to 300-best lists. In preliminary"
W09-0406,P08-2021,0,0.0992928,"n-gram matches within their sub-list, which ”supports” them the more the larger it is. Previous experiments on Chinese-English showed, that the two feature groups with the highest impact on the combination result are the language model and the n-best list based n-gram agreement. Therefore we decided to focus on the n-best list n-gram agreement for exploring sub-list For the combination of machine translation systems there have been two main approaches described in recent publications. One uses confusion network decoding to combine translation systems as described in (Rosti et al., 2008) and (Karakos et al., 2008). The other approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the approach described in (Hildebrand and Vogel, 2008). We combine the output from the available translation systems into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on a development set to determine feature weights and re-rank the joint n-best list. 2 Normalized N-gram Agreement Features For our entries to the WMT’09 we used the following feature groups: • Language model score • Word lexicon scores Proceeding"
W09-0406,W08-0329,0,0.0814194,"because they collect more n-gram matches within their sub-list, which ”supports” them the more the larger it is. Previous experiments on Chinese-English showed, that the two feature groups with the highest impact on the combination result are the language model and the n-best list based n-gram agreement. Therefore we decided to focus on the n-best list n-gram agreement for exploring sub-list For the combination of machine translation systems there have been two main approaches described in recent publications. One uses confusion network decoding to combine translation systems as described in (Rosti et al., 2008) and (Karakos et al., 2008). The other approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the approach described in (Hildebrand and Vogel, 2008). We combine the output from the available translation systems into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on a development set to determine feature weights and re-rank the joint n-best list. 2 Normalized N-gram Agreement Features For our entries to the WMT’09 we used the following feature groups: • Language model score • Wor"
W10-0102,C04-1046,0,0.0225497,"he bidirectional probabilities. The selection strategy selects the least scoring links according to the formula below which corresponds to links with maximum uncertainty: Score(aij /sI1 , tJ1 ) = 5.2 We can iteratively perform the algorithm for a defined number of iterations T or until a certain desired performance is reached, which is measured by alignQuery Strategies for Link Selection 2 ∗ P (tj /si ) ∗ P (si /tj ) (10) P (tj /si ) + P (si /tj ) Confidence Based: Posterior Alignment probabilities Confidence estimation for MT output is an interesting area with meaningful initial exploration (Blatz et al., 2004; Ueffing and Ney, 2007). Given a sentence pair (sI1 , tJ1 ) and its word alignment, we compute two confidence metrics at alignment link level – based on the posterior link probability and a simple IBM Model 1 as seen in Equation 13. We select the alignment links that the initial word aligner is least confident according to our metric and seek manual correction of the links. We use t2s to denote computation using higher order (IBM4) target-givensource models and s2t to denote source-given-target models. Targeting some of the uncertain parts of word alignment has already been shown to improve t"
W10-0102,J93-2003,0,0.0865446,"information, usually syntactic, from the language pairs (Cherry and Lin, 2006). The second is to use extra annotation, typically word-level human alignment for some sentence pairs, in conjunction with the parallel data to learn alignment in a semi-supervised manner. Our research is in the direction of the latter, and aims to reduce the effort involved in hand-generation of word alignments by using active learning strategies for careful selection of word pairs to seek alignment. Introduction The success of statistical approaches to Machine Translation (MT) can be attributed to the IBM models (Brown et al., 1993) that characterize wordlevel alignments in parallel corpora. Parameters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. While the ease of automatically aligning sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of statistical machine translation (SMT) systems for various language pairs, the quality of alignment is typically quite low for language Active learning for MT has not yet been explored to its full potential. Much of the literature has explored one task"
W10-0102,P04-1023,0,0.801476,"Missing"
W10-0102,P06-2014,0,0.0208565,"ghly uncertain or most informative alignment links that are proposed under an unsupervised word alignment model. Manual correction of such informative links can then be applied to create a labeled dataset used by a semi-supervised word alignment model. Our experiments show that using active learning leads to maximal reduction of alignment error rates with reduced human effort. 1 Two directions of research have been pursued for improving generative word alignment. The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). The second is to use extra annotation, typically word-level human alignment for some sentence pairs, in conjunction with the parallel data to learn alignment in a semi-supervised manner. Our research is in the direction of the latter, and aims to reduce the effort involved in hand-generation of word alignments by using active learning strategies for careful selection of word pairs to seek alignment. Introduction The success of statistical approaches to Machine Translation (MT) can be attributed to the IBM models (Brown et al., 1993) that characterize wordlevel alignments in parallel corpora."
W10-0102,P06-1097,0,0.239443,"an interesting method that has been applied to clustering problems. Tomanek and Hahn (2009) applied active semi supervised learning to the sequence-labeling problem. Tur et al. (2005) describe active and semi-supervised learning methods for reducing labeling effort for spoken language understanding. They train supervised classification algorithms for the task of call classification and apply it to a large unlabeled dataset to select the least confident instances for human labeling. Researchers have begun to explore semisupervised word alignment models that use both labeled and unlabeled data. Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. The log-linear model is trained on the available labeled data to improve performance. They propose a semisupervised training algorithm which alternates between discriminative error training on the labeled data to learn the weighting parameters and maximum-likelihood EM training on unlabeled data to estimate the parameters. Callison-Burch et 11 al. (2004) also improve alignment by interpolating human alignments with automatic alignments. They observe that while working with"
W10-0102,J07-3002,0,0.0187241,"ed dataset. The word-level aligned labeled dataset is then provided to our semi-supervised word alignment algorithm, which uses it to produces the alignment model θt+1 for U . Algorithm 1 AL FOR W ORD A LIGNMENT 1: Unlabeled Data Set: U = {(sk , tk )} 2: Manual Alignment Set : A0 = {akij , ∀si ∈ Sk , tj ∈ Tk } 3: Train Semi-supervised Word Alignment using (U , A0 ) → θ0 4: N : batch size 5: for t = 0 to T do 6: Lt = LinkSelection(U ,At ,θt ,N ) 7: Request Human Alignment for Lt 8: At+1 = At + Lt 9: Re-train Semi-Supervised Word Alignment on (U, At+1 ) → θt+1 10: end for ment error rate (AER) (Fraser and Marcu, 2007) in the case of word alignment. In a more typical scenario, since reducing human effort or cost of elicitation is the objective, we iterate until the available budget is exhausted. 5 We propose multiple query selection strategies for our active learning setup. The scoring criteria is designed to select alignment links across sentence pairs that are highly uncertain under current automatic translation models. These links are difficult to align correctly by automatic alignment and will cause incorrect phrase pairs to be extracted in the translation model, in turn hurting the translation quality"
W10-0102,W08-0509,1,0.833421,"stimation for both the directions. 12 As we will discuss in Section 5, the alignments and the computed lexicons form an important part of our link selection strategies. P ˆ count(ti , sj ; A) sP P (sj /ti ) = (8) s count(ti ) P ˆ count(ti , sj ; A) sP P (ti /sj ) = (9) s count(sj ) We perform all our experiments on a symmetrized alignment that combines the bidirectional alignments using heuristics as discussed in (Koehn et al., 2007). We represent this alignment as A = {aij : i = 0 · · · J ∈ sJ1 ; j = 0 · · · I ∈ tI1 }. 3.2 Semi-Supervised Word Alignment We use an extended version of MGIZA++ (Gao and Vogel, 2008) to perform the constrained semisupervised word alignment. To get full benefit from the manual alignments, MGIZA++ modifies all alignment models used in the standard training procedure, i.e. the IBM1, HMM, IBM3 and IBM4 models. Manual alignments are incorporated in the EM training phase of these models as constraints that restrict the summation over all possible alignment paths. Typically in the EM procedure for IBM models, the training procedure requires for each source sentence position, the summation over all positions in the target sentence. The manual alignments allow for one-to-many alig"
W10-0102,N09-1047,0,0.0940585,"ters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. While the ease of automatically aligning sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of statistical machine translation (SMT) systems for various language pairs, the quality of alignment is typically quite low for language Active learning for MT has not yet been explored to its full potential. Much of the literature has explored one task – selecting sentences to translate and add to the training corpus (Haffari et al., 2009). In this paper we explore active learning for word alignment, where the input to the active learner is a sentence pair (sJ1 , tI1 ), present in two different languages S = {s∗ } and T = {t∗ }, and the annotation elicited from human is a set of links {(j, i) : j = 0 · · · J; i = 0 · · · I}. Unlike previous approaches, our work does not require elicitation of full alignment for the sentence pair, which could be effortintensive. We use standard active learning query strategies to selectively elicit partial alignment information. This partial alignment information is then fed into a semi-supervis"
W10-0102,P09-1105,0,0.0909898,"iven a sentence pair (sI1 , tJ1 ) and its word alignment, we compute two confidence metrics at alignment link level – based on the posterior link probability and a simple IBM Model 1 as seen in Equation 13. We select the alignment links that the initial word aligner is least confident according to our metric and seek manual correction of the links. We use t2s to denote computation using higher order (IBM4) target-givensource models and s2t to denote source-given-target models. Targeting some of the uncertain parts of word alignment has already been shown to improve translation quality in SMT (Huang, 2009). In our current work, we use confidence metrics as an active learning sampling strategy to obtain most informative links. We also experiment with other confidence metrics as discussed in (Ueffing and Ney, 2007), especially the IBM 1 model score metric which showed some improvement as well. Ps2t (aij , sI1 /tJ1 ) = Conf (aij /S, T ) = 5.3 pt2s (tj /si , aij ∈ A) (11) PM i pt2s (tj /si ) ps2t (si /tj , aij ∈ A) (12) PN i pt2s (tj /si ) 2 ∗ Pt2s ∗ Ps2t (13) Pt2s + Ps2t Agreement Based: Query by Committee The generative alignments produced differ based on the choice of direction of the language p"
W10-0102,J04-3001,0,0.0195447,"lso improve alignment by interpolating human alignments with automatic alignments. They observe that while working with such datasets, alignments of higher quality should be given a much higher weight than the lower-quality alignments. Wu et al. (2006) learn separate models from labeled and unlabeled data using the standard EM algorithm. The two models are then interpolated as a learner in the semi-supervised AdaBoost algorithm to improve word alignment. Active learning has been applied to various fields of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Tang et al., 2001; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For Statistical Machine Translation, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting"
W10-0102,P07-2045,0,0.00547955,"1 ) A (6) (7) Given the Viterbi alignment for each sentence pair in the parallel corpus, we can also compute the word-level alignment probabilities using simple relative likelihood estimation for both the directions. 12 As we will discuss in Section 5, the alignments and the computed lexicons form an important part of our link selection strategies. P ˆ count(ti , sj ; A) sP P (sj /ti ) = (8) s count(ti ) P ˆ count(ti , sj ; A) sP P (ti /sj ) = (9) s count(sj ) We perform all our experiments on a symmetrized alignment that combines the bidirectional alignments using heuristics as discussed in (Koehn et al., 2007). We represent this alignment as A = {aij : i = 0 · · · J ∈ sJ1 ; j = 0 · · · I ∈ tI1 }. 3.2 Semi-Supervised Word Alignment We use an extended version of MGIZA++ (Gao and Vogel, 2008) to perform the constrained semisupervised word alignment. To get full benefit from the manual alignments, MGIZA++ modifies all alignment models used in the standard training procedure, i.e. the IBM1, HMM, IBM3 and IBM4 models. Manual alignments are incorporated in the EM training phase of these models as constraints that restrict the summation over all possible alignment paths. Typically in the EM procedure for I"
W10-0102,W07-0734,0,0.0547652,"Missing"
W10-0102,J03-1002,0,0.0274685,"and aims to reduce the effort involved in hand-generation of word alignments by using active learning strategies for careful selection of word pairs to seek alignment. Introduction The success of statistical approaches to Machine Translation (MT) can be attributed to the IBM models (Brown et al., 1993) that characterize wordlevel alignments in parallel corpora. Parameters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. While the ease of automatically aligning sentences at the word-level with tools like GIZA++ (Och and Ney, 2003) has enabled fast development of statistical machine translation (SMT) systems for various language pairs, the quality of alignment is typically quite low for language Active learning for MT has not yet been explored to its full potential. Much of the literature has explored one task – selecting sentences to translate and add to the training corpus (Haffari et al., 2009). In this paper we explore active learning for word alignment, where the input to the active learner is a sentence pair (sJ1 , tI1 ), present in two different languages S = {s∗ } and T = {t∗ }, and the annotation elicited from"
W10-0102,P02-1040,0,0.0833163,"Missing"
W10-0102,P04-1075,0,0.0213902,"polating human alignments with automatic alignments. They observe that while working with such datasets, alignments of higher quality should be given a much higher weight than the lower-quality alignments. Wu et al. (2006) learn separate models from labeled and unlabeled data using the standard EM algorithm. The two models are then interpolated as a learner in the semi-supervised AdaBoost algorithm to improve word alignment. Active learning has been applied to various fields of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Tang et al., 2001; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For Statistical Machine Translation, application of active learning has been focused on the task of selecting the most informative sentences to train the model, in order to reduce cost of data acquisition. Recent work in this area discussed multiple query selection strategies for a Statistical Phrase Based Translation system (Haffari et al., 2009). Their framework requires source text to be translated by the system and the translated data is used in a self-training setting to train MT models. To our knowledge, w"
W10-0102,P09-1117,0,0.0214544,"k. 2 Related Work Semi-supervised learning is a broader area of Machine Learning, focusing on improving the learning process by usage of unlabeled data in conjunction with labeled data (Chapelle et al., 2006). Many semi-supervised learning algorithms use co-training framework, which assumes that the dataset has multiple views, and training different classifiers on a non-overlapping subset of these features provides additional labeled data (Zhu, 2005). Active query selection for training a semi-supervised learning algorithm is an interesting method that has been applied to clustering problems. Tomanek and Hahn (2009) applied active semi supervised learning to the sequence-labeling problem. Tur et al. (2005) describe active and semi-supervised learning methods for reducing labeling effort for spoken language understanding. They train supervised classification algorithms for the task of call classification and apply it to a large unlabeled dataset to select the least confident instances for human labeling. Researchers have begun to explore semisupervised word alignment models that use both labeled and unlabeled data. Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear spa"
W10-0102,J07-1003,0,0.0155819,"babilities. The selection strategy selects the least scoring links according to the formula below which corresponds to links with maximum uncertainty: Score(aij /sI1 , tJ1 ) = 5.2 We can iteratively perform the algorithm for a defined number of iterations T or until a certain desired performance is reached, which is measured by alignQuery Strategies for Link Selection 2 ∗ P (tj /si ) ∗ P (si /tj ) (10) P (tj /si ) + P (si /tj ) Confidence Based: Posterior Alignment probabilities Confidence estimation for MT output is an interesting area with meaningful initial exploration (Blatz et al., 2004; Ueffing and Ney, 2007). Given a sentence pair (sI1 , tJ1 ) and its word alignment, we compute two confidence metrics at alignment link level – based on the posterior link probability and a simple IBM Model 1 as seen in Equation 13. We select the alignment links that the initial word aligner is least confident according to our metric and seek manual correction of the links. We use t2s to denote computation using higher order (IBM4) target-givensource models and s2t to denote source-given-target models. Targeting some of the uncertain parts of word alignment has already been shown to improve translation quality in SM"
W10-0102,P06-2117,0,0.0207555,"dels. The log-linear model is trained on the available labeled data to improve performance. They propose a semisupervised training algorithm which alternates between discriminative error training on the labeled data to learn the weighting parameters and maximum-likelihood EM training on unlabeled data to estimate the parameters. Callison-Burch et 11 al. (2004) also improve alignment by interpolating human alignments with automatic alignments. They observe that while working with such datasets, alignments of higher quality should be given a much higher weight than the lower-quality alignments. Wu et al. (2006) learn separate models from labeled and unlabeled data using the standard EM algorithm. The two models are then interpolated as a learner in the semi-supervised AdaBoost algorithm to improve word alignment. Active learning has been applied to various fields of Natural Language Processing like statistical parsing, entity recognition among others (Hwa, 2004; Tang et al., 2001; Shen et al., 2004). In case of MT, the potential of active learning has remained largely unexplored. For Statistical Machine Translation, application of active learning has been focused on the task of selecting the most in"
W10-0102,P02-1016,0,\N,Missing
W10-0704,P06-1009,0,0.0758213,"Missing"
W10-0704,J93-2003,0,0.060134,"Missing"
W10-0704,P04-1023,0,0.0386181,"Missing"
W10-0704,D09-1030,0,0.131713,"Missing"
W10-0704,P06-1097,0,0.0285576,"Missing"
W10-0704,P05-1057,0,0.0372445,"Missing"
W10-0704,2006.amta-papers.11,0,0.0388701,"Missing"
W10-0704,H05-1011,0,0.0456578,"Missing"
W10-0704,W08-0303,1,0.893435,"Missing"
W10-0704,J03-1002,0,0.0116363,"trivial but they have completely different underlying assumptions. Figure 2 shows the comparison of partial alignments (the bold link) and full alignments (the dashed and the bold links). In the example, if full alignment is given, we can assert 2005 is only aligned to 2005d, not to dor dd, but we cannot do that if only partial alignment is given. In this paper we experiment with a novel method which uses the partial alignment to constraint the EM algorithm in the parameter estimation of IBM models. IBM Models (Brown et. al., 1993) are a series of generative models for word alignment. GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al., 1996) where EM algorithm is employed to estimate the model parameters. In the E-step, it is possible to obtain sufficient statistics from all possible alignments for simple models such as Model 1 and Model 2. Meanwhile for fertility-based models such as Model 3, 4, 5, enumerating all possible alignments is NP-complete. In practice, we use simpler models such as HMM or Model 2 to generate a “center alignment” and then try to find better alignments among the neighbors of it. The neighbors of an alignment aJ1 = [a1 , a2"
W10-0704,H05-1010,0,0.0510535,"Missing"
W10-0704,C96-2141,1,0.569801,"the comparison of partial alignments (the bold link) and full alignments (the dashed and the bold links). In the example, if full alignment is given, we can assert 2005 is only aligned to 2005d, not to dor dd, but we cannot do that if only partial alignment is given. In this paper we experiment with a novel method which uses the partial alignment to constraint the EM algorithm in the parameter estimation of IBM models. IBM Models (Brown et. al., 1993) are a series of generative models for word alignment. GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al., 1996) where EM algorithm is employed to estimate the model parameters. In the E-step, it is possible to obtain sufficient statistics from all possible alignments for simple models such as Model 1 and Model 2. Meanwhile for fertility-based models such as Model 3, 4, 5, enumerating all possible alignments is NP-complete. In practice, we use simpler models such as HMM or Model 2 to generate a “center alignment” and then try to find better alignments among the neighbors of it. The neighbors of an alignment aJ1 = [a1 , a2 , · · · , aJ ], aj ∈ [0, I] is defined as alignments that can be generated from aJ"
W10-0710,ambati-etal-2010-active,1,0.369711,"Missing"
W10-0710,D09-1030,0,0.173354,"based on number of speakers in the language and Internet presence of the population. Languages like Spanish, Chinese, English, Arabic are spoken by many and have a large presence of users on the Internet. Those like Urdu, Tamil, Telugu although spoken by many are not well represented on the Web. Languages like Swahili, Zulu, Haiti are neither spoken by many nor have a great presence on the Web. For this pilot study we selected Spanish, Chinese, English, Urdu, Telugu, Hindi, Haitian Creole languages. We do not select German, French and other language pairs as they have already been explored by Callison-Burch (2009). Our pilot study helped us calibrate the costs for different language pairs as well as helped us select the languages to pursue further experiments. We found that at lower pay rates like 1 cent, it is difficult to find a sufficient number of translators to complete the task. For example, we could not find turkers to complete the translation from English to HaitianCreole even after a period of 10 days. Haitian creole is spoken by a small population and it seems that only a very small portion of that was on MTurk. For a few other languages pairs, while we could find a 62 Proceedings of the NAAC"
W10-0710,P07-2045,0,0.00248585,"Missing"
W10-0710,W07-0734,0,0.0156397,"some gold-standard to identify genuine matches with automatic translation services. 3.2 Output Space and Fuzzy Matching Due to the natural variability in style of turkers, there could be multiple different, but perfectly valid translations for a given sentence. Therefore it is difficult to match translation outputs from two turkers or even with gold standard data. We therefore need a fuzzy matching algorithm to account for lexical choices, synonymy, word ordering and morphological variations. This problem is similar to the task of automatic translation output evaluation and so we use METEOR (Lavie and Agarwal, 2007), an automatic MT evaluation metric for comparing two sentences. METEOR has an internal aligner that matches words in the sentences given and scores them separately based on whether the match was supported by synonymy, exact match or fuzzy match. The scores are then combined to provide a global matching score. If the score is above a threshold δ, we treat the sentences to be equivalent translations of the source sentence. We can set the δ parameter to different values, based on what is acceptable to the application. In our experiments, we set δ = 0.7. We did not choose BLEU scoring metric as i"
W10-0710,D08-1027,0,0.0392021,"Missing"
W10-0710,takezawa-etal-2002-toward,0,0.05371,"Missing"
W10-1701,P09-1105,0,0.0313828,"w the actual improvement brought in by the algorithm instead of the manual alignment links themselves, we compare the alignment results of the proposed method with directly fixing the alignments from original GIZA++ training. By fixing alignments we mean that first the conventional Using heuristics on unlabelled data Another possible way of getting alignment links is to make use of heuristics to generate highprecision-low-recall links and feed them into the aligner. The heuristics can be number mapping, person name translator or more sophisticated methods such as alignment confidence measure (Huang, 2009). In this paper we propose to use manual dictionaries to generate alignment links. First we filter out from the dictionary the entries with high frequency in the source side, and then build an aligner based on it. The aligner output links between words if them match an entry in the dictionary. The method can be applied on large unlabelled corpus and generate large number of links, after that we use the links as manual alignment links in proposed method. The readers may notice that GIZA++ supports utilizing manual dictionary as well, however it is different from our method. The dictionary is us"
W10-1701,W10-0102,1,0.828379,"use both the model parameters and the alignment links (Niehues and Vogel, 2008). Therefore, improving the generative aligner is still important even when using discriminative aligners. Third, these methods require full alignment of sentences to provide positive (aligned) and negative (nonaligned) information, which limits the availability of data (Niehues and Vogel, 2008). The proposed method has been successfully applied on various tasks, such as utilizing manual alignments harvested from Amazon Mechanical Turk (Gao and Vogel, 2010), and active learning methods for improving word alignment (Ambati et al., 2010). This paper provides the detailed algorithm of the method and controlled experiments to demonstrate its behavior. The paper is organized as follows, in section 2 we describe the proposed model as well as the modified training algorithm. Section 3 presents two approaches of obtaining manual alignment links, The experimental results will be shown in section 4. We conclude the paper in section 5. terms, by rule-based alignment systems that have high accuracy but low recall rate. The functionality is considered useful in many scenarios. For example, the researchers can analyse the alignments gene"
W10-1701,H05-1012,0,0.153527,"Missing"
W10-1701,W09-1903,0,0.0335551,"Missing"
W10-1701,P07-2045,0,0.00837407,"Missing"
W10-1701,P06-1009,0,0.187006,"power of the generative model in which the algorithm utilizes large amount of unlabeled data. More importantly, the additional information can propagate over the whole corpus through better estimation of model parameters. In contrast, if we use the alignment links in discriminative aligners as a feature, one link can only affect the particular word, or at most the sentence. Second, although the discriminative word alignment methods provide flexibility to utilize labeled data, most of them still rely on generative aligners. Some rely on the model parameters of the IBM Models (Liu et al., 2005; Blunsom and Cohn, 2006), others rely on the alignment links from GIZA++ as features or as training data (Taskar et al., 2005), or use both the model parameters and the alignment links (Niehues and Vogel, 2008). Therefore, improving the generative aligner is still important even when using discriminative aligners. Third, these methods require full alignment of sentences to provide positive (aligned) and negative (nonaligned) information, which limits the availability of data (Niehues and Vogel, 2008). The proposed method has been successfully applied on various tasks, such as utilizing manual alignments harvested fro"
W10-1701,P05-1057,0,0.0116785,"hod preserves the power of the generative model in which the algorithm utilizes large amount of unlabeled data. More importantly, the additional information can propagate over the whole corpus through better estimation of model parameters. In contrast, if we use the alignment links in discriminative aligners as a feature, one link can only affect the particular word, or at most the sentence. Second, although the discriminative word alignment methods provide flexibility to utilize labeled data, most of them still rely on generative aligners. Some rely on the model parameters of the IBM Models (Liu et al., 2005; Blunsom and Cohn, 2006), others rely on the alignment links from GIZA++ as features or as training data (Taskar et al., 2005), or use both the model parameters and the alignment links (Niehues and Vogel, 2008). Therefore, improving the generative aligner is still important even when using discriminative aligners. Third, these methods require full alignment of sentences to provide positive (aligned) and negative (nonaligned) information, which limits the availability of data (Niehues and Vogel, 2008). The proposed method has been successfully applied on various tasks, such as utilizing manual"
W10-1701,J93-2003,0,0.0558007,"Missing"
W10-1701,H05-1011,0,0.0778364,"Missing"
W10-1701,P04-1023,0,0.313946,"Missing"
W10-1701,W08-0303,1,0.886503,"tter estimation of model parameters. In contrast, if we use the alignment links in discriminative aligners as a feature, one link can only affect the particular word, or at most the sentence. Second, although the discriminative word alignment methods provide flexibility to utilize labeled data, most of them still rely on generative aligners. Some rely on the model parameters of the IBM Models (Liu et al., 2005; Blunsom and Cohn, 2006), others rely on the alignment links from GIZA++ as features or as training data (Taskar et al., 2005), or use both the model parameters and the alignment links (Niehues and Vogel, 2008). Therefore, improving the generative aligner is still important even when using discriminative aligners. Third, these methods require full alignment of sentences to provide positive (aligned) and negative (nonaligned) information, which limits the availability of data (Niehues and Vogel, 2008). The proposed method has been successfully applied on various tasks, such as utilizing manual alignments harvested from Amazon Mechanical Turk (Gao and Vogel, 2010), and active learning methods for improving word alignment (Ambati et al., 2010). This paper provides the detailed algorithm of the method a"
W10-1701,D09-1030,0,0.0586442,"conclude the paper in section 5. terms, by rule-based alignment systems that have high accuracy but low recall rate. The functionality is considered useful in many scenarios. For example, the researchers can analyse the alignments generated by GIZA++ and fix common error patterns, and perform training again. On another way, an application can combine active learning (Arora et al., 2009) and crowdsourcing, asking non-expertise such as workers of Amazon Mechanical Turk to label crucial alignment links that can improve the system with low cost, which is now a promising methodology in NLP areas (Callison-Burch, 2009). In this paper, we propose a semi-supervised extension of the IBM Models that can utilize partial alignment links. More specifically, we are seeking answers for the following questions: • Given the partial alignment of a sentence, how to find the most probable alignment that is consistent with the partial alignment. • Given a set of partially aligned sentences, how to get the parameters that maximize the likelihood of the sentence pairs with alignments consistent with the partial alignments • Given a set of partially aligned sentences, with conflicting partial alignments, how to answer the tw"
W10-1701,J03-1002,0,0.162676,"Ittycheriah and Roukos (2005) proposed to use only manual alignment links in a maximum entropy model. A number of semi-supervised word aligners are proposed (Blunsom and Cohn, 2006; Niehues and Vogel, 2008; Taskar et al., 2005; Liu et al., 2005; Moore, 2005). These approaches use held-out manual alignments to tune the weights for discriminative models, with the model parameters, model scores or alignment links from unsupervised word aligners as features. Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). The approaches use labelled data to tune parameters to combine different components of the IBM Models. We present a word alignment framework that can incorporate partial manual alignments. The core of the approach is a novel semi-supervised algorithm extending the widely used IBM Models with a constrained EM algorithm. The partial manual alignments can be obtained by human labelling or automatically by high-precision-low-recall heuristics. We demonstrate the usages of both methods"
W10-1701,P06-1097,0,0.250039,"only manual alignment links in a maximum entropy model. A number of semi-supervised word aligners are proposed (Blunsom and Cohn, 2006; Niehues and Vogel, 2008; Taskar et al., 2005; Liu et al., 2005; Moore, 2005). These approaches use held-out manual alignments to tune the weights for discriminative models, with the model parameters, model scores or alignment links from unsupervised word aligners as features. Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). The approaches use labelled data to tune parameters to combine different components of the IBM Models. We present a word alignment framework that can incorporate partial manual alignments. The core of the approach is a novel semi-supervised algorithm extending the widely used IBM Models with a constrained EM algorithm. The partial manual alignments can be obtained by human labelling or automatically by high-precision-low-recall heuristics. We demonstrate the usages of both methods by selecting alignment links from manually aligne"
W10-1701,J04-4002,0,0.0439526,"AER when applying different number of manual alignment links. The three rows are for Arabic-English, English-Arabic and heuristically symmetrized alignments (grow-diagfinal-and) accordingly. of alignment qualities are shown in 5. As we can see, the AER reduced by 1.64 from 37.23 to 35.61 on symmetrized alignment. We also experimented with translation tasks with moderate-size corpus. We used the corpus LDC2006G05 with 25 million words. The training scheme is the same as previous experiments, where the filtered LDC dictionary is used. After word alignment, standard Moses phrase extraction tool (Och and Ney, 2004) is used to build the translation models and finally Moses (Koehn et. al., 2007) is used to tune and decode. We tune the system on the NIST MT06 test set (1664 sentences), and test on the MT08 (1357 sentences) and the DEV075 (1211 sentences) test sets, which are further divided into two sources (newswire and web data). A trigram language of the curves, with a small number of manual alignment links, we can already improve the alignment quality by a large gap. This observation can benefit low-resource word alignment tasks. 4.2 Experiment on using heuristics The previous experiment shows the pote"
W10-1701,P02-1040,0,0.095643,"links showed an average improvement of 0.35 BLEU points across 8 test sets. The algorithm has small impact on the speed of GIZA++, and can easily be added to current multithread implementation of GIZA++. Therefore it is suitable for large scale training. Future work includes applying the proposed approach on low resource language pairs and integrating the algorithm with other rule-based or discriminative aligners that can generate highprecision-low-recall partial alignments. model trained from GigaWord V1 and V2 corpora is used. Table 4 shows the comparison of the performances on BLEU metric (Papineni et al., 2002). As we can observe from the results, the proposed method outperforms the baseline on all test sets except MT03, and has significant6 improvement on MT02 (+0.72), MT04 (+0.93), and Dev07NW(+0.63). The average improvement across all test sets is 0.35 BLEU points. As a summary, the purpose of the this experiment is to demonstrate an important characteristic of the proposed method. Even with imperfect manual alignment links, we can get better alignment by applying our method. This characteristic opens a possibility to integrate other more sophisticated aligners. 5 AER 44.43 42.88 Conclusion Ackno"
W10-1701,D07-1006,0,0.087741,"er of semi-supervised word aligners are proposed (Blunsom and Cohn, 2006; Niehues and Vogel, 2008; Taskar et al., 2005; Liu et al., 2005; Moore, 2005). These approaches use held-out manual alignments to tune the weights for discriminative models, with the model parameters, model scores or alignment links from unsupervised word aligners as features. Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). The approaches use labelled data to tune parameters to combine different components of the IBM Models. We present a word alignment framework that can incorporate partial manual alignments. The core of the approach is a novel semi-supervised algorithm extending the widely used IBM Models with a constrained EM algorithm. The partial manual alignments can be obtained by human labelling or automatically by high-precision-low-recall heuristics. We demonstrate the usages of both methods by selecting alignment links from manually aligned corpus and apply links generated from bilingual dictionary on"
W10-1701,H05-1010,0,0.0687758,"antly, the additional information can propagate over the whole corpus through better estimation of model parameters. In contrast, if we use the alignment links in discriminative aligners as a feature, one link can only affect the particular word, or at most the sentence. Second, although the discriminative word alignment methods provide flexibility to utilize labeled data, most of them still rely on generative aligners. Some rely on the model parameters of the IBM Models (Liu et al., 2005; Blunsom and Cohn, 2006), others rely on the alignment links from GIZA++ as features or as training data (Taskar et al., 2005), or use both the model parameters and the alignment links (Niehues and Vogel, 2008). Therefore, improving the generative aligner is still important even when using discriminative aligners. Third, these methods require full alignment of sentences to provide positive (aligned) and negative (nonaligned) information, which limits the availability of data (Niehues and Vogel, 2008). The proposed method has been successfully applied on various tasks, such as utilizing manual alignments harvested from Amazon Mechanical Turk (Gao and Vogel, 2010), and active learning methods for improving word alignme"
W10-1701,W08-0509,1,0.85008,"nd the count collection, the Viterbi alignment is guaranteed to be consistent with the fixed alignment links, and the sufficient statistics is guaranteed to contain no statistics from inconsistent alignments. 2.4 Obtaining alignment links • C1: fj aligns to ei , i > 0 in e → f ,1 but in reversed direction ei does not align to fj but to another word. • C2: fj aligns to ei , i > 0, in f → e, but in reversed direction (e → f ), fj aligns to the empty word. • C3: no word aligns to fj , in f → e, but in reversed direction fj aligns to ei , i > 0.2 Training scheme We extend the multi-thread GIZA++ (Gao and Vogel, 2008) to load the alignments from a modified corpus file. The links are appended to the end of each sentence in the corpus file in the form of indices pairs, which will be read by the aligner during training. In practice, we first training unconstrained models up to Model 4, and then switch The criteria of ei are the same as fj after swapping the definitions of “source” and “target”. We prioritize the links αIJ = (i, j) by looking at the classes of the source/target words. The order of 1 Recall that fj can align to only one word. This class is different from C1 that whether ei aligns to concrete wo"
W10-1701,C96-2141,1,0.875796,"ch assertion. Partial alignments can be obtained from various sources, for example, we can fetch them by manually correcting unsupervised alignments, by simple heuristics such as dictionaries of technical Introduction Word alignment is used in various natural language processing applications, and most statistical machine translation systems rely on word alignment as a preprocessing step. Traditionally the word alignment model is trained in an unsupervised manner, e.g. the most widely used tool GIZA++ (Och and Ney, 2003), which implements the IBM Models (Brown et. al., 1993) and the HMM model (Vogel et al., 1996). However, for language pairs such as Chinese-English, the word alignment quality is often unsatisfactory (Guzman et al., 2009). There has been increasing interest on using manual alignments in word alignment tasks. 1 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 1–10, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics additional links to tune additional parameters to combine model components, as a result, it is not limited to fully aligned corpus. A question may raise why the proposed method is superior over usi"
W10-1701,2004.tmi-1.9,1,0.832174,"Missing"
W10-1701,2009.mtsummit-papers.5,1,0.864883,"nsupervised alignments, by simple heuristics such as dictionaries of technical Introduction Word alignment is used in various natural language processing applications, and most statistical machine translation systems rely on word alignment as a preprocessing step. Traditionally the word alignment model is trained in an unsupervised manner, e.g. the most widely used tool GIZA++ (Och and Ney, 2003), which implements the IBM Models (Brown et. al., 1993) and the HMM model (Vogel et al., 1996). However, for language pairs such as Chinese-English, the word alignment quality is often unsatisfactory (Guzman et al., 2009). There has been increasing interest on using manual alignments in word alignment tasks. 1 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 1–10, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics additional links to tune additional parameters to combine model components, as a result, it is not limited to fully aligned corpus. A question may raise why the proposed method is superior over using the partial alignment links as features in discriminative aligners? There are three possible explanations. First, the method"
W10-1745,W09-0408,0,0.0160027,"together with the other feature weights during MERT using a binary feature per system. To the feature vector for each hypothesis one feature per input system is added; for each hypothesis one of the features is one, indicating which system it came from, all others are zero. Introduction For the combination of machine translation systems there have been several approaches described in recent publications. One uses confusion networks formed along a skeleton sentence to combine translation systems as described in (Rosti et al., 2008) and (Karakos et al., 2008). A different approach described in (Heafield et al., 2009) is not keeping the skeleton fixed when aligning the systems. Another approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the latter approach. We combine the output from the submitted translation systems, including n-best lists where available, into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on the provided development data to determine feature weights and re-rank the joint nbest list. We train to maximize BLEU. 2 2.1 Source-Target Word Alignment Features We trained the I"
W10-1745,2008.amta-srw.3,1,0.757547,"put MT systems, where available. The sentence level features used are independent from the MT systems involved. Compared to the baseline we added source-to-target word alignment based features and trained system weights to our feature set. We combined MT systems for French - English and German - English using provided data only. 1 • Word lexicon scores (6) • Sentence length features (3) • Rank feature (1) • Normalized n-gram agreement (6) • Source-target word alignment features (6) • Trained system weights (no. of systems) The details on language model and word lexicon scores can be found in (Hildebrand and Vogel, 2008) and details on the rank feature and the normalized n-gram agreement can be found in (Hildebrand and Vogel, 2009). We use three sentence length features, which are the ratio of the hypothesis length to the length of the source sentence, the diversion of this ratio from the overall length ratio of the bilingual training data and the difference between the hypothesis length and the average length of the hypotheses in the n-best list for the respective source sentence. The system weights are trained together with the other feature weights during MERT using a binary feature per system. To the feat"
W10-1745,W09-0406,1,0.67462,"ompared to the baseline we added source-to-target word alignment based features and trained system weights to our feature set. We combined MT systems for French - English and German - English using provided data only. 1 • Word lexicon scores (6) • Sentence length features (3) • Rank feature (1) • Normalized n-gram agreement (6) • Source-target word alignment features (6) • Trained system weights (no. of systems) The details on language model and word lexicon scores can be found in (Hildebrand and Vogel, 2008) and details on the rank feature and the normalized n-gram agreement can be found in (Hildebrand and Vogel, 2009). We use three sentence length features, which are the ratio of the hypothesis length to the length of the source sentence, the diversion of this ratio from the overall length ratio of the bilingual training data and the difference between the hypothesis length and the average length of the hypotheses in the n-best list for the respective source sentence. The system weights are trained together with the other feature weights during MERT using a binary feature per system. To the feature vector for each hypothesis one feature per input system is added; for each hypothesis one of the features is"
W10-1745,P08-2021,0,0.0315649,"espective source sentence. The system weights are trained together with the other feature weights during MERT using a binary feature per system. To the feature vector for each hypothesis one feature per input system is added; for each hypothesis one of the features is one, indicating which system it came from, all others are zero. Introduction For the combination of machine translation systems there have been several approaches described in recent publications. One uses confusion networks formed along a skeleton sentence to combine translation systems as described in (Rosti et al., 2008) and (Karakos et al., 2008). A different approach described in (Heafield et al., 2009) is not keeping the skeleton fixed when aligning the systems. Another approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the latter approach. We combine the output from the submitted translation systems, including n-best lists where available, into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on the provided development data to determine feature weights and re-rank the joint nbest list. We train to maximize BLEU. 2"
W10-1745,J03-1002,0,0.00518814,"ther approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the latter approach. We combine the output from the submitted translation systems, including n-best lists where available, into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on the provided development data to determine feature weights and re-rank the joint nbest list. We train to maximize BLEU. 2 2.1 Source-Target Word Alignment Features We trained the IBM word alignment models up to model 4 using the GIZA++ toolkit (Och and Ney, 2003) on the bilingual training corpus. Then a forced alignment algorithm utilizes the trained models to align each source sentence to each translation hypothesis in its respective n-best list. We use the alignment score given by the word alignment models, the number of unaligned words Features For our entries to the WMT’09 we used the following feature groups (in parenthesis are the number 307 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 307–310, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 3.1 and the number"
W10-1745,W08-0329,0,0.0236678,"the n-best list for the respective source sentence. The system weights are trained together with the other feature weights during MERT using a binary feature per system. To the feature vector for each hypothesis one feature per input system is added; for each hypothesis one of the features is one, indicating which system it came from, all others are zero. Introduction For the combination of machine translation systems there have been several approaches described in recent publications. One uses confusion networks formed along a skeleton sentence to combine translation systems as described in (Rosti et al., 2008) and (Karakos et al., 2008). A different approach described in (Heafield et al., 2009) is not keeping the skeleton fixed when aligning the systems. Another approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the latter approach. We combine the output from the submitted translation systems, including n-best lists where available, into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on the provided development data to determine feature weights and re-rank the joint nbest list. W"
W10-3814,P08-1024,0,0.0142334,"rget-side syntax, to a corresponding variant based on source-side syntax, showing that target syntax is more benefitial, and unsuccessfully experimented with a model extension that jointly takes source and target syntax into account. Hierarchical phrase-based MT suffers from spurious ambiguity: A single translation for a given source sentence can usually be accomplished by many different PSCFG derivations. This problem is exacerbated by syntax-augmented MT with its thousands of nonterminals, and made even worse by its joint source-and-target extension. Future research should apply the work of Blunsom et al. (2008) and Blunsom and Osborne (2008), who marginalize over derivations to find the most probable translation rather than the most probable derivation, to these multi-nonterminal grammars. All source code underlying this work is available under the GNU Lesser General Public License as part of the ‘SAMT’ system at: www.cs.cmu.edu/˜zollmann/samt Acknowledgements This work is in part supported by NSF under the Cluster Exploratory program (grant NSF 0844507), and in part by the US DARPA GALE program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the a"
W10-3814,J93-2003,0,0.0173184,"Missing"
W10-3814,2010.eamt-1.33,0,0.0274268,"m Chiang et al. (2008) in that we estimate one source span length distribution for each substitution site of each grammar rule, resulting in unique distributions for each rule, estimated from all instances of the rule in the training data. This enables our model to condition reordering range on the individual rules used in a derivation, and even allows to distinguish between two rules r1 and r2 that both reorder arguments with identical mean span lengths `, but where the span lengths encountered in extracted instances of r1 are all close to `, whereas span length instances for r2 vary widely. Chen and Eisele (2010) propose a hypbrid approach between hierarchical phrase based MT and a rule based MT system, reporting improvement over each individual model on an Englishto-German translation task. Essentially, the rule based system is converted to a single-nonterminal PSCFG, and hence can be combined with the hierarchical model, another single-nonterminal PSCFG, by taking the union of the rule sets and augmenting the feature vectors, adding zerovalues for rules that only exist in one of the two grammars. We face the challenge of combining the single-nonterminal hierarchical grammar with a multi-nonterminal"
W10-3814,D08-1024,0,0.0629433,"ng hierarchical and syntax-based PSCFG models, by merging the grammars as well as by interpolating the translation models. Finally, we compare syntax-augmented MT, which extracts rules based on target-side syntax, to a corresponding variant based on source-side syntax, and experiment with a model extension based on source and target syntax. We evaluate the different models on the NIST large resource Chinese-to-English translation task. 110 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 110–117, COLING 2010, Beijing, August 2010. 2 Related work Chiang et al. (2008) introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length, by estimating for each possible source span length ` a Bernoulli distribution p(R|`) where R takes value one if reordering takes place and zero otherwise. Maximumlikelihood estimation of the distribution amounts to simply counting the relative frequency of nonterminal reorderings over all extracted rule instances that incurred a substitution of span length `. In a more fine-grained approach they add a separate binary feature hR, `i for each combin"
W10-3814,P05-1033,0,0.861046,"en occur across language pairs considered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the statistical translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). In this work, we propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We add a source span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training (i.e., rule extra"
W10-3814,J07-2003,0,0.242401,"automatic metrics) on held out data, e.g., using minimum-error-rate training (MERT) (Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et al., 2008). In this work we experiment with PSCFGs that have been automatically learned from wordaligned parallel corpora. PSCFGs are defined by a source terminal set (source vocabulary) TS , a target terminal set (target vocabulary) TT , a shared nonterminal set N and rules of the form: X → hγ, α, wi where • X ∈ N is a labeled nonterminal referred to as the left-hand-side of the rule. • γ ∈ (N ∪ TS )∗ is the source side of the rule. • α ∈ (N ∪ TT )∗ is the target side of the rule. • w ∈ [0, ∞) is a non-negative"
W10-3814,P10-1146,0,0.0213994,"nted grammar, resulting in a backbone grammar of well-estimated hierarchical rules supporting the sparser syntactic rules. They allow the model preference between hierarchical and syntax rules to be learned from development data by adding an indicator feature to all rules, which is one for hierarchical rules and zero for syntax rules. However, no empirical comparison is given between the purely syntax-augmented and the hybrid grammar. We aim to fill this gap by experimenting with both models, and further refine the hybrid approach by adding interpolated probability models to the syntax rules. Chiang (2010) augments a hierarchical phrasebased MT model with binary syntax features representing the source and target syntactic constituents of a given rule’s instantiations during training, thus taking source and target syntax into account while avoiding the data-sparseness and decoding-complexity problems of multinonterminal PSCFG models. In our approach, the source- and target-side syntax directly determines the grammar, resulting in a nonterminal set derived from the labels underlying the source- and target-language treebanks. 3 PSCFG-based translation Given a source language sentence f , statistic"
W10-3814,N04-1035,0,0.531519,"ng phenomena that often occur across language pairs considered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the statistical translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). In this work, we propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We add a source span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training (i."
W10-3814,P07-1019,0,0.0218644,".g., using minimum-error-rate training (MERT) (Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et al., 2008). In this work we experiment with PSCFGs that have been automatically learned from wordaligned parallel corpora. PSCFGs are defined by a source terminal set (source vocabulary) TS , a target terminal set (target vocabulary) TT , a shared nonterminal set N and rules of the form: X → hγ, α, wi where • X ∈ N is a labeled nonterminal referred to as the left-hand-side of the rule. • γ ∈ (N ∪ TS )∗ is the source side of the rule. • α ∈ (N ∪ TT )∗ is the target side of the rule. • w ∈ [0, ∞) is a non-negative real-valued weight assigned to the rule; in our"
W10-3814,P03-1054,0,0.00830674,"of 15 source words, and correspondingly extract rules from initial phrase pairs of maximum source length 15. All rules have at most two nonterminal symbols, which must be non-consecutive on the source side, and rules must contain at least 114 one source-side terminal symbol. For parameter tuning, we use the L0 regularized minimum-error-rate training tool provided by the SAMT toolkit. The parallel training data comprises of 9.6M sentence pairs (206M Chinese Words, 228M English words). The source and target language parses for the syntax-augmented grammar were generated by the Stanford parser (Klein and Manning, 2003). The results are given in Table 1. The source span models (indicated by +span) achieve small test set improvements of 0.15 BLEU points on average for the hierarchical and 0.26 BLEU points for the syntax-augmented system, but these are not statistically significant. Augmenting a syntax-augmented grammar with hierarchical features (“Syntax+hiermodels”) results in average test set improvements of 0.5 BLEU points. These improvements are not statistically significant either, but persist across all three test sets. This demonstrates the benefit of more reliable feature estimation. Further augmentin"
W10-3814,N03-1017,0,0.0181244,"nonterminal X that indicates the one-to-one correspondence between the new X tokens on the two sides (it is not in the space of word indices like i, j, u, v, m, n). The recursive form of this generalization operation allows the generation of rules with multiple nonterminal pairs. Chiang (2005) uses features analogous to the ones used in phrase-based translation: a language model neg-log probability, a ‘rule given source-side’ neg-log-probability, a ‘rule given target-side’ neg-log-probability, source- and target conditioned ‘lexical’ neg-log-probabilities based on word-to-word co-occurrences (Koehn et al., 2003), as well as rule, target word, and glue operation counters. We follow Venugopal and Zollmann (2009) to further add a rareness penalty, 1/ count(r) where count(r) is the occurrence count of rule r in the training corpus, allowing the system to learn penalization of low-frequency rules, as well as three indicator features firing if the rule has one, two unswapped, and two swapped nonterminal pairs, respectively.1 3.2 Syntax Augmented MT Syntax Augmented MT (SAMT) (Zollmann and Venugopal, 2006) extends Chiang (2005) to include nonterminal symbols from target language phrase structure parse trees"
W10-3814,P06-1077,0,0.386225,"r statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the statistical translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). In this work, we propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We add a source span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training (i.e., rule extraction) time. We further propose different method"
W10-3814,W06-1606,0,0.0968441,"ine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the statistical translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). In this work, we propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We add a source span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training (i.e., rule extraction) time. We further propose different methods of combining hierar"
W10-3814,P03-1021,0,0.0211708,"anguage sentence f , statistical machine translation defines the translation task as selecting the most likely target translation e under a model P (e|f ), i.e.: ˆ(f ) = arg max P (e|f ) = arg max e e e m X hi (e, f )λi i=1 where the arg max operation denotes a search through a structured space of translation outputs in the target language, hi (e, f ) are bilingual features of e and f and monolingual features of e, and weights λi are typically trained discriminatively to maximize translation quality (based on automatic metrics) on held out data, e.g., using minimum-error-rate training (MERT) (Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et"
W10-3814,P02-1040,0,0.0805115,"onstraints on the eligible substitutions simultaneously. Let Nf be the nonterminal label that would be assigned to a given initial rule when utilizing the source-side parse tree, and Ne the assigned label according to the target-side parse. Then our bilingual model assigns ‘Nf + Ne ’ to the initial rule. The extraction of complex rules proceeds as before. The number of nonterminals in this model, based on a source-model label set of size s and a target label set of size t, is thus given by st. 7 Experiments We evaluate our approaches by comparing translation quality according to the IBM-BLEU (Papineni et al., 2002) metric on the NIST Chineseto-English translation task using MT04 as development set to train the model parameters λ, and MT05, MT06 and MT08 as test sets. We perform PSCFG rule extraction and decoding using the open-source “SAMT” system (Venugopal and Zollmann, 2009), using the provided implementations for the hierarchical and syntax-augmented grammars. For all systems, we use the bottom-up chart parsing decoder implemented in the SAMT toolkit with a reordering limit of 15 source words, and correspondingly extract rules from initial phrase pairs of maximum source length 15. All rules have at"
W10-3814,D08-1012,0,0.0166,"Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et al., 2008). In this work we experiment with PSCFGs that have been automatically learned from wordaligned parallel corpora. PSCFGs are defined by a source terminal set (source vocabulary) TS , a target terminal set (target vocabulary) TT , a shared nonterminal set N and rules of the form: X → hγ, α, wi where • X ∈ N is a labeled nonterminal referred to as the left-hand-side of the rule. • γ ∈ (N ∪ TS )∗ is the source side of the rule. • α ∈ (N ∪ TT )∗ is the target side of the rule. • w ∈ [0, ∞) is a non-negative real-valued weight assigned to the rule; in our model, w is the exponential function of the"
W10-3814,N07-1063,1,0.796092,"ics) on held out data, e.g., using minimum-error-rate training (MERT) (Och, 2003). In PSCFG-based systems, the search space is structured by automatically extracted rules that model both translation and re-ordering operations. 111 Most large scale systems approximate the search above by simply searching for the most likely derivation of rules, rather than searching for the most likely translated output. There are efficient algorithms to perform this search (Kasami, 1965; Chappelier and Rajman, 1998) that have been extended to efficiently integrate n-gram language model features (Chiang, 2007; Venugopal et al., 2007; Huang and Chiang, 2007; Zollmann et al., 2008; Petrov et al., 2008). In this work we experiment with PSCFGs that have been automatically learned from wordaligned parallel corpora. PSCFGs are defined by a source terminal set (source vocabulary) TS , a target terminal set (target vocabulary) TT , a shared nonterminal set N and rules of the form: X → hγ, α, wi where • X ∈ N is a labeled nonterminal referred to as the left-hand-side of the rule. • γ ∈ (N ∪ TS )∗ is the source side of the rule. • α ∈ (N ∪ TT )∗ is the target side of the rule. • w ∈ [0, ∞) is a non-negative real-valued weight assi"
W10-3814,W06-3119,1,0.941047,"s language pairs considered for statistical machine translation. As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the statistical translation model. Several techniques have been recently proposed to automatically identify and estimate parameters for PSCFGs (or related synchronous grammars) from parallel corpora (Galley et al., 2004; Chiang, 2005; Zollmann and Venugopal, 2006; Liu et al., 2006; Marcu et al., 2006). In this work, we propose several improvements to the hierarchical phrase-based MT model of Chiang (2005) and its syntax-based extension by Zollmann and Venugopal (2006). We add a source span variance model that, for each rule utilized in a probabilistic synchronous context-free grammar (PSCFG) derivation, gives a confidence estimate in the rule based on the number of source words spanned by the rule and its substituted child rules, with the distributions of these source span sizes estimated during training (i.e., rule extraction) time. We further propos"
W10-3814,C08-1144,1,0.947554,"dual model on an Englishto-German translation task. Essentially, the rule based system is converted to a single-nonterminal PSCFG, and hence can be combined with the hierarchical model, another single-nonterminal PSCFG, by taking the union of the rule sets and augmenting the feature vectors, adding zerovalues for rules that only exist in one of the two grammars. We face the challenge of combining the single-nonterminal hierarchical grammar with a multi-nonterminal syntax-augmented grammar. Thus one hierarchical rule typically corresponds to many syntax-augmented rules. The SAMT system used by Zollmann et al. (2008) adds hierarchical rules separately to the syntax-augmented grammar, resulting in a backbone grammar of well-estimated hierarchical rules supporting the sparser syntactic rules. They allow the model preference between hierarchical and syntax rules to be learned from development data by adding an indicator feature to all rules, which is one for hierarchical rules and zero for syntax rules. However, no empirical comparison is given between the purely syntax-augmented and the hybrid grammar. We aim to fill this gap by experimenting with both models, and further refine the hybrid approach by addin"
W10-3814,D08-1023,0,\N,Missing
W10-4127,W03-1728,0,0.0323518,"performance. Experimental results show consistent improvement on F1 scores and OOV recall rates by applying the approach. 1 Introduction The Chinese word segmentation problem has been intensively investigated in the past two decades. From lexicon-based methods such as Bi-Directed Maximum Match (BDMM) (Chen et al., 2005) to statistical models such as Hidden Markove Model (HMM) (Zhang et al., 2003), a broad spectrum of approaches have been experimented. By casting the problem as a character labeling task, sequence labeling models such as Conditional Random Fields can be applied on the problem (Xue and Shen, 2003). State-of-the-art CRF-based systems have achieved good performance. However, like many machine learning problems, generalizability is crucial for a domain-independent segmentation system. Because the training data usually come from limited domains, when the domain of test data is different from the training data, the results are still not satisfactory. A straight-forward solution is to obtain more labeled data in the domain we want to test. However this is not easily achievable because the amount of data needed to train a segmentation system are large. In this paper, we focus on improving the"
W10-4127,W03-1709,0,0.027439,"gmentation systems. By training a second layer of large margin classifier on top of the outputs from several Conditional Random Fields classifiers, it can utilize a small amount of in-domain training data to improve the performance. Experimental results show consistent improvement on F1 scores and OOV recall rates by applying the approach. 1 Introduction The Chinese word segmentation problem has been intensively investigated in the past two decades. From lexicon-based methods such as Bi-Directed Maximum Match (BDMM) (Chen et al., 2005) to statistical models such as Hidden Markove Model (HMM) (Zhang et al., 2003), a broad spectrum of approaches have been experimented. By casting the problem as a character labeling task, sequence labeling models such as Conditional Random Fields can be applied on the problem (Xue and Shen, 2003). State-of-the-art CRF-based systems have achieved good performance. However, like many machine learning problems, generalizability is crucial for a domain-independent segmentation system. Because the training data usually come from limited domains, when the domain of test data is different from the training data, the results are still not satisfactory. A straight-forward soluti"
W10-4127,I08-4017,0,0.186543,"em combination method in section 3. Finally, in section 4 the experimental results are presented. 2 CRF-based sub-systems In this section we describe the sub-systems we used in system. All of the sub-systems are based on CRF with different features. The tag set we use is the 6-tag (B1, B2, B3, M, E, S) set proposed by Zhao et al (2006). All of the sub-systems use the same tag set, however as we will see later, the second-layer classifier in our system does not require the sub-systems to have a common tag set. Also, all of the sub-systems include a common set of character features proposed in (Zhao and Kit, 2008). The offsets and concatenations of the six n-gram features (the feature template) are: C−1 , C0 , C1 , C−1 C0 , C0 C1 , C−1 C1 . In the remaining part of the section we will introduce other features that we employed in different subsystems. 2.1 Character type features By simply classify the characters into four types: Punctuation (P), Digits (D), Roman Letters (L) and Chinese characters (C), we can assign character type tags to every character. The idea is straight-forward. We denote the feature as CT F . Similar to character feature, we also use different offsets and concatenations for chara"
W10-4127,Y06-1012,0,0.0559027,"idation-like method to train sequential stacking models, while we directly use small amount of indomain data to train the second-layer classifiers. The paper is organized as follows, first we will discuss the CRF-based sub-systems we used in section 2, and then the SVM-based system combination method in section 3. Finally, in section 4 the experimental results are presented. 2 CRF-based sub-systems In this section we describe the sub-systems we used in system. All of the sub-systems are based on CRF with different features. The tag set we use is the 6-tag (B1, B2, B3, M, E, S) set proposed by Zhao et al (2006). All of the sub-systems use the same tag set, however as we will see later, the second-layer classifier in our system does not require the sub-systems to have a common tag set. Also, all of the sub-systems include a common set of character features proposed in (Zhao and Kit, 2008). The offsets and concatenations of the six n-gram features (the feature template) are: C−1 , C0 , C1 , C−1 C0 , C0 C1 , C−1 C1 . In the remaining part of the section we will introduce other features that we employed in different subsystems. 2.1 Character type features By simply classify the characters into four type"
W11-1012,P05-1033,0,0.251288,"ing the meaning of the sentence during the translation process. However, using semantic role representation in machine translation has its own set of problems. In section 2 we briefly review SCFG-based machine translation and SRL. In section 3, we describe the SRL-aware SCFG rules. Section 4 provides the detail of the rule extraction algorithm. Section 5 presents two alternative methods how to utilize the SRL information. The experimental results are given in Section 6, followed by analysis and conclusion in Section 7. 2 2.1 Background Hierarchical Phrase-based Machine Translation Proposed by Chiang (2005), the Hierarchical Phrase-based Machine Translation model (com108 Semantic Role Labeling and Machine Translation First, we face the coverage problem. Some sentences might not have semantic structure at all, if, for instance they consist of single noun phrases or contain only rare predicates that are not covered by the semantic role labeler. Moreover, the PA structures are not guaranteed to cover the whole sentence. This is especially true when two or more predicates are presented in a coordinated structure. In this case, the arguments of other predicates will not be covered in the PA structure"
W11-1012,J07-2003,0,0.0956512,"s will also be extracted in the same framework. Special conversion rules are applied to ensure that when SRL-aware SCFG rules are used in derivation, the decoder only generates hypotheses with complete semantic structures. We perform machine translation experiments using 9 different Chinese-English test-sets. Our approach achieved an average BLEU score improvement of 0.49 as well as 1.21 point reduction in TER. 1 Introduction Syntax-based Machine Translation methods have achieved comparable performance to Phrase-based systems. Hierarchical Phrase-based Machine Translation, proposed by Chiang (Chiang, 2007), uses a general non-terminal label X but does not use linguistic information from the source or the target language. There have been efforts to include linguistic information into machine translation. Liu et al (2006) experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the Packed Forest data structure to reduce the impact of parsing errors (Liu and Huang, 2010). The string-to-tree (Galley et al, 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as 107 well as other formalism"
W11-1012,P10-1146,0,0.0224154,"ase-based systems. Hierarchical Phrase-based Machine Translation, proposed by Chiang (Chiang, 2007), uses a general non-terminal label X but does not use linguistic information from the source or the target language. There have been efforts to include linguistic information into machine translation. Liu et al (2006) experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the Packed Forest data structure to reduce the impact of parsing errors (Liu and Huang, 2010). The string-to-tree (Galley et al, 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as 107 well as other formalisms such as Dependency Trees (Shen et al., 2008). One problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules. SAMT (Zollmann and Venugopal, 2006) and Tree Sequence Alignment (Zhang et al., 2008) are proposed to amend this problem by allowing non-constituent phrases to be extracted. The reported results show that while utilizing linguistic information helps, the coverage is more importan"
W11-1012,P10-4002,0,0.0336701,"Missing"
W11-1012,P06-1121,0,0.0636449,"Missing"
W11-1012,W08-0509,1,0.772805,"tune the feature weights. The minimum modifications for the decoder make the proposed method an easy replacement for Hiero rule extractors. 6 Experiments and discussion We performed experiments on Chinese to English translation tasks. The data set we used in the experiments is a subset of the FBIS corpus. We filter the corpus with maximum sentence length be 30. The corpus has 2.5 million words in Chinese side and 3.1 million on English side. We adopted the ASSERT semantic role labeler (Pradhan et al., 2004) to label the English side sentences. The parallel sentences are aligned using MGIZA++ (Gao and Vogel, 2008) and then the proposed rule extraction algorithm was used in extracting the SRL-aware SCFG rules. We used the MosesChart decoder (Hoang and Koehn, 2008) and the Moses toolkit (Koehn et al, 2007) for tuning and decoding. The language model is a trigram language model trained on English GIGAWord corpus (V1V3) using the SRILM toolkit. We used the NIST MT06 test set for tuning, and experimented with an additional 9 test sets, including MT02, 03, 04, 05, 08, and GALE test sets DEV07-dev and DEV07-blind. DEV07-dev and DEV07-blind are further divided into newswire and 113 weblog parts. We experimente"
W11-1012,W08-0510,0,0.18367,"d translation in Figure 3a) using the rules shown in Figure 2. Also, we can see in Figure 3b) that incomplete SRL structures cannot be generated due to the absence of a proper conversion rule. 1 The translation is Xinjiang’s Yili holds propaganda drive and the Pinyin transliteration is Xinjiang daguimo kaizhan mianduimian xuanjiang huodong 110 Figure 3: Example of a derivations of sentence We can see from the example in Figure 3a), that the SRL-aware SCFG rules fit perfectly in the SCFG framework. Therefore no modification need to be made on a decoder, such as MosesChart decoder,for instance (Hoang and Koehn, 2008). The main problem is how to extract the SRL-aware SCFG rules from the corpus and estimate the feature values so that it works together with the conventional Hiero rules. In the next two sections we will present the rule extraction algorithm and two alternative methods for comparison. 4 Rule Extraction Algorithm The Hiero rule extraction algorithm uses the following steps: 1. Extract the initial phrases with the commonly used alignment template heuristics. To reduce the number of phrases extracted, an additional restriction is applied that the boundary words must be aligned on both sides. Also"
W11-1012,D10-1014,0,0.0723261,"Missing"
W11-1012,P07-2045,0,0.00411831,"Missing"
W11-1012,W09-0424,0,0.0270084,"Missing"
W11-1012,C10-1081,0,0.204799,"labels are only on the constituents of predicate and arguments. There is no analysis conducted inside the augments. That is different from syntactic parsing or dependency parsing, which both provide a complete tree from the sentence to every individual word. As we can see in Figure 1, words such as “Second” and “and” are not covered. Inside the NPs such as “a flood prevention system”, SRL will not provide more information. Therefore it is hard to build a self-contained formalization based only on SRL labels. Most work on SRL labels is built upon or assisted by other formalisms. For instance, Liu and Gildea (2010) integrated SRL label into a treeto-string translation system. Wu and Fung (2009) used SRL labels for reordering the n-best output of phrase-based translation systems. Similarly, in our work we also adopt the methodology of using SRL information to assist existing formalism. The difference of our method from Wu and Fung is that we embed the SRL information directly into the decode, instead of doing two-pass decoding. Also, our method is different from Liu and Gildea (2010) that we utilize target side SRL information instead of the source side. As we will see in section 3, we define a mapping f"
W11-1012,P10-5002,0,0.0117082,"ax-based Machine Translation methods have achieved comparable performance to Phrase-based systems. Hierarchical Phrase-based Machine Translation, proposed by Chiang (Chiang, 2007), uses a general non-terminal label X but does not use linguistic information from the source or the target language. There have been efforts to include linguistic information into machine translation. Liu et al (2006) experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the Packed Forest data structure to reduce the impact of parsing errors (Liu and Huang, 2010). The string-to-tree (Galley et al, 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as 107 well as other formalisms such as Dependency Trees (Shen et al., 2008). One problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules. SAMT (Zollmann and Venugopal, 2006) and Tree Sequence Alignment (Zhang et al., 2008) are proposed to amend this problem by allowing non-constituent phrases to be extracted. The reported results show that wh"
W11-1012,P06-1077,0,0.0755187,"ctures. We perform machine translation experiments using 9 different Chinese-English test-sets. Our approach achieved an average BLEU score improvement of 0.49 as well as 1.21 point reduction in TER. 1 Introduction Syntax-based Machine Translation methods have achieved comparable performance to Phrase-based systems. Hierarchical Phrase-based Machine Translation, proposed by Chiang (Chiang, 2007), uses a general non-terminal label X but does not use linguistic information from the source or the target language. There have been efforts to include linguistic information into machine translation. Liu et al (2006) experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the Packed Forest data structure to reduce the impact of parsing errors (Liu and Huang, 2010). The string-to-tree (Galley et al, 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as 107 well as other formalisms such as Dependency Trees (Shen et al., 2008). One problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing covera"
W11-1012,J05-1004,0,0.0174057,"nstead of doing two-pass decoding. Also, our method is different from Liu and Gildea (2010) that we utilize target side SRL information instead of the source side. As we will see in section 3, we define a mapping function from the SRL structures that a phrase covers to a non-terminal tag before extracting the SCFG rules. The tags will restrict the derivation of the target side parse tree to accept only SRL structures we have seen in the training corpus. The mapping from SRL structures to non-terminal tags can be defined according to the SRL annotation set. In this paper we adopt the PropBank (Palmer et al., 2005) annotation set of semantic labels, because the annotation set is relatively simple and easy to parse. The small set of argument tags also makes the number of LHS non-terminal tags small, which 109 alleviates the problem of data scarcity. However the methodology of this paper is not limited to PropBank tags. By defining appropriate mapping, it is also possible to use other annotation sets, such as FrameNet (Baker et al., 2002). 3 SRL-aware SCFG Rules The SRL-aware SCFG rules are SCFG rules. They contain at least one non-terminal label with information about the PA structure that is covered by"
W11-1012,N04-1030,0,0.0403517,"e expanded rule table and conversion rules, the decoder does not need to be modified. We incorporate MERT to tune the feature weights. The minimum modifications for the decoder make the proposed method an easy replacement for Hiero rule extractors. 6 Experiments and discussion We performed experiments on Chinese to English translation tasks. The data set we used in the experiments is a subset of the FBIS corpus. We filter the corpus with maximum sentence length be 30. The corpus has 2.5 million words in Chinese side and 3.1 million on English side. We adopted the ASSERT semantic role labeler (Pradhan et al., 2004) to label the English side sentences. The parallel sentences are aligned using MGIZA++ (Gao and Vogel, 2008) and then the proposed rule extraction algorithm was used in extracting the SRL-aware SCFG rules. We used the MosesChart decoder (Hoang and Koehn, 2008) and the Moses toolkit (Koehn et al, 2007) for tuning and decoding. The language model is a trigram language model trained on English GIGAWord corpus (V1V3) using the SRILM toolkit. We used the NIST MT06 test set for tuning, and experimented with an additional 9 test sets, including MT02, 03, 04, 05, 08, and GALE test sets DEV07-dev and D"
W11-1012,P08-1066,0,0.0225096,"l label X but does not use linguistic information from the source or the target language. There have been efforts to include linguistic information into machine translation. Liu et al (2006) experimented with tree-to-string translation models that utilize source side parse trees, and later improved the method by using the Packed Forest data structure to reduce the impact of parsing errors (Liu and Huang, 2010). The string-to-tree (Galley et al, 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as 107 well as other formalisms such as Dependency Trees (Shen et al., 2008). One problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules. SAMT (Zollmann and Venugopal, 2006) and Tree Sequence Alignment (Zhang et al., 2008) are proposed to amend this problem by allowing non-constituent phrases to be extracted. The reported results show that while utilizing linguistic information helps, the coverage is more important (Chiang, 2010). When dealing with formalisms such as semantic role labeling, the coverage problem is also critical. In this paper"
W11-1012,N09-2004,0,0.396409,"labeling, the coverage problem is also critical. In this paper we follow Chiang’s observation and use SRL labels to augment the extraction of SCFG rules. I.e., the formalism provides additional information and more rules instead of restrictions that remove existing rules. This preserves the coverage of rules. Recently there has been increased attention to use semantic information in machine translation. Liu and Gildea (2008; 2010) proposed using Semantic Role Labels (SRL) in their tree-to-string machine translation system and demonstrated improvement over conventional tree-to-string methods. Wu and Fung (2009) developed a framework to reorder the output using information from both the source and the target SRL labels. In this paper, we explore an approach of using the target side SRL information in addition to a Hierarchical Phrase-based Machine Translation framework. The proposed method extracts initial phrases with two different heuristics: The first heuristic is used to extract rules that have a general left-hand-side (LHS) non-terminal tag X, Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 107–115, c ACL HLT 2011, Portland, Oregon, USA,"
W11-1012,P08-1064,0,0.0193668,"e trees, and later improved the method by using the Packed Forest data structure to reduce the impact of parsing errors (Liu and Huang, 2010). The string-to-tree (Galley et al, 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as 107 well as other formalisms such as Dependency Trees (Shen et al., 2008). One problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules. SAMT (Zollmann and Venugopal, 2006) and Tree Sequence Alignment (Zhang et al., 2008) are proposed to amend this problem by allowing non-constituent phrases to be extracted. The reported results show that while utilizing linguistic information helps, the coverage is more important (Chiang, 2010). When dealing with formalisms such as semantic role labeling, the coverage problem is also critical. In this paper we follow Chiang’s observation and use SRL labels to augment the extraction of SCFG rules. I.e., the formalism provides additional information and more rules instead of restrictions that remove existing rules. This preserves the coverage of rules. Recently there has been i"
W11-1012,W06-3119,0,0.0495985,"-to-string translation models that utilize source side parse trees, and later improved the method by using the Packed Forest data structure to reduce the impact of parsing errors (Liu and Huang, 2010). The string-to-tree (Galley et al, 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as 107 well as other formalisms such as Dependency Trees (Shen et al., 2008). One problem that arises by using full syntactic labels is that they require an exact match of the constituents in extracted phrases, so it faces the risk of losing coverage of the rules. SAMT (Zollmann and Venugopal, 2006) and Tree Sequence Alignment (Zhang et al., 2008) are proposed to amend this problem by allowing non-constituent phrases to be extracted. The reported results show that while utilizing linguistic information helps, the coverage is more important (Chiang, 2010). When dealing with formalisms such as semantic role labeling, the coverage problem is also critical. In this paper we follow Chiang’s observation and use SRL labels to augment the extraction of SCFG rules. I.e., the formalism provides additional information and more rules instead of restrictions that remove existing rules. This preserves"
W11-1012,W08-0308,0,\N,Missing
W11-1209,J93-2003,0,0.027867,"s: parallel and not parallel. p(c|S, T ) ∈ [0, 1] is the probability where a value p(c = 1|S, T ) close to 1.0 indicates that S and T are translations of each other. fi (c, S, T ) are feature functions that are coindexed with respect to the class variable c. The parameters λi are the weights for the feature functions obtained during training. Z(S, T ) is the normalization factor. In the feature vector for phrase pair (S, T ), each feature appears twice, once for each class c ∈ {0, 1}. The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al., 1993) alignments for source and target pairs. However, in our experiments, the features are computed primarily on IBM Model-1 probabilities (i.e. lexicon). We do not explicitly compute IBM Model-1 alignments. To compute coverage features, we identify alignment points for which IBM Model-1 probability is above a threshold. We produce two sets of features based on IBM Model-1 probabilities obtained by training in both directions. All the features have been normalized with respect to the source phrase length L or the target phrase length K. We use the following 11 features: 1. Lexical probability (2):"
W11-1209,W04-3208,0,0.342879,"Non-Viterbi alignment approach outperforms the other two approaches on F1 measure. 1 Comparable corpora provide a possible solution to this data sparseness problem. Comparable documents are not strictly parallel, but contain rough translations of each other, with overlapping information. A good example for comparable documents is the newswire text produced by multilingual news organizations such as AFP or Reuters. The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004). The Web is by far the largest source of comparable data. Resnik and Smith (2003) exploit the similarities in URL structure, document structure and other clues for mining the Web for parallel documents. Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al., 2010). Introduction Statistical Machine Translation (SMT), like many natural language processing tasks, relies primarily on parallel corpora. The translation performance of SMT systems directly depends on the quantity and the quality of the available parallel data. However, such corpora are onl"
W11-1209,2007.tmi-papers.12,0,0.728302,"al fragments, including word translation pairs, named entities, and long phrase pairs. The ability to identify these pairs would create a valuable resource for SMT, especially for low-resource languages. The first attempt to detect sub-sentential fragments from comparable sentences is (Munteanu and Marcu, 2006). Quirk et al. (2007) later extended this work by proposing two generative models for comparable sentences and showed improvements when applied to cross-domain test data. In both these approaches the extracted fragment data was used as additional training data to train alignment models. Kumano et al. (2007) have proposed a phrasal alignment approach for comparable corpora using the joint probability SMT model. While this approach is appealing for low-resource scenarios as it does not require any seed parallel corpus, the high computational cost is a deterrent in its applicability 62 to large corpora. In this paper we explore several phrase alignment approaches to detect parallel phrase pairs embedded in comparable sentence pairs. We assume that comparable sentence pairs have already been detected. Our intention is to use the extracted phrases directly in the translation process, along with other"
W11-1209,J05-4003,0,0.922067,"anguages, including English, Arabic, Chinese and some European languages. Much Comparable corpora may contain parallel data in different levels of granularity. This includes: parallel documents, parallel sentence pairs, or parallel sub-sentential fragments. To simplify the process and reduce the computational overhead, the parallel sentence extraction is typically divided into two tasks. First, a document level alignment is identified between comparable documents, and second, the parallel sentences are detected within the identified document pairs. Cross-lingual information retrieval methods (Munteanu and Marcu, 2005) and 61 Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 61–68, 49th Annual Meeting of the Association for Computational Linguistics, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics Figure 1: Sample comparable sentences that contain parallel phrases other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. Zhao and Vogel (2002) have extended parallel sentence alignment algorithms to identify parallel sentence pairs within comparable news corpora. Tillmann and Xu (2009) introduced a system th"
W11-1209,P06-1011,0,0.539039,"oduced a system that performs both tasks in a single run without any document level pre-filtering. Such a system is useful when document level boundaries are not available in the comparable corpus. Even if two comparable documents have few or no parallel sentence pairs, there could still be parallel sub-sentential fragments, including word translation pairs, named entities, and long phrase pairs. The ability to identify these pairs would create a valuable resource for SMT, especially for low-resource languages. The first attempt to detect sub-sentential fragments from comparable sentences is (Munteanu and Marcu, 2006). Quirk et al. (2007) later extended this work by proposing two generative models for comparable sentences and showed improvements when applied to cross-domain test data. In both these approaches the extracted fragment data was used as additional training data to train alignment models. Kumano et al. (2007) have proposed a phrasal alignment approach for comparable corpora using the joint probability SMT model. While this approach is appealing for low-resource scenarios as it does not require any seed parallel corpus, the high computational cost is a deterrent in its applicability 62 to large c"
W11-1209,2007.mtsummit-papers.50,0,0.779333,"ms both tasks in a single run without any document level pre-filtering. Such a system is useful when document level boundaries are not available in the comparable corpus. Even if two comparable documents have few or no parallel sentence pairs, there could still be parallel sub-sentential fragments, including word translation pairs, named entities, and long phrase pairs. The ability to identify these pairs would create a valuable resource for SMT, especially for low-resource languages. The first attempt to detect sub-sentential fragments from comparable sentences is (Munteanu and Marcu, 2006). Quirk et al. (2007) later extended this work by proposing two generative models for comparable sentences and showed improvements when applied to cross-domain test data. In both these approaches the extracted fragment data was used as additional training data to train alignment models. Kumano et al. (2007) have proposed a phrasal alignment approach for comparable corpora using the joint probability SMT model. While this approach is appealing for low-resource scenarios as it does not require any seed parallel corpus, the high computational cost is a deterrent in its applicability 62 to large corpora. In this paper"
W11-1209,J03-3002,0,0.133556,"e. 1 Comparable corpora provide a possible solution to this data sparseness problem. Comparable documents are not strictly parallel, but contain rough translations of each other, with overlapping information. A good example for comparable documents is the newswire text produced by multilingual news organizations such as AFP or Reuters. The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004). The Web is by far the largest source of comparable data. Resnik and Smith (2003) exploit the similarities in URL structure, document structure and other clues for mining the Web for parallel documents. Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al., 2010). Introduction Statistical Machine Translation (SMT), like many natural language processing tasks, relies primarily on parallel corpora. The translation performance of SMT systems directly depends on the quantity and the quality of the available parallel data. However, such corpora are only available in large quantities for a handful of languages, including English, Ara"
W11-1209,N10-1063,0,0.0731541,"arable documents is the newswire text produced by multilingual news organizations such as AFP or Reuters. The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004). The Web is by far the largest source of comparable data. Resnik and Smith (2003) exploit the similarities in URL structure, document structure and other clues for mining the Web for parallel documents. Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al., 2010). Introduction Statistical Machine Translation (SMT), like many natural language processing tasks, relies primarily on parallel corpora. The translation performance of SMT systems directly depends on the quantity and the quality of the available parallel data. However, such corpora are only available in large quantities for a handful of languages, including English, Arabic, Chinese and some European languages. Much Comparable corpora may contain parallel data in different levels of granularity. This includes: parallel documents, parallel sentence pairs, or parallel sub-sentential fragments. To"
W11-1209,N09-2024,0,0.116088,"retrieval methods (Munteanu and Marcu, 2005) and 61 Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 61–68, 49th Annual Meeting of the Association for Computational Linguistics, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics Figure 1: Sample comparable sentences that contain parallel phrases other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. Zhao and Vogel (2002) have extended parallel sentence alignment algorithms to identify parallel sentence pairs within comparable news corpora. Tillmann and Xu (2009) introduced a system that performs both tasks in a single run without any document level pre-filtering. Such a system is useful when document level boundaries are not available in the comparable corpus. Even if two comparable documents have few or no parallel sentence pairs, there could still be parallel sub-sentential fragments, including word translation pairs, named entities, and long phrase pairs. The ability to identify these pairs would create a valuable resource for SMT, especially for low-resource languages. The first attempt to detect sub-sentential fragments from comparable sentences"
W11-1209,2005.mtsummit-papers.33,1,0.744492,"e used parallel phrases pairs extracted from a manually word-aligned corpus. In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005): pairing all source phrases with all target phrases, but filter out the parallel pairs and those that have high length difference or a low lexical overlap, and then randomly select a subset of phrase pairs as the negative training set. The model parameters are estimated using the GIS algorithm. 2.3 Non-Viterbi (PESA) Alignment A phrase alignment algorithm called “PESA” that does not rely on the Viterbi path is described in (Vogel, 2005). PESA identifies the boundaries of the target phrase by aligning words inside the source phrase with words inside the target phrase, and similarly for the words outside the boundaries of the phrase pair. It does not attempt to generate phrase alignments for the full sentence. Rather, it identifies the best target phrase that matches a given source phrase. PESA requires a statistical word-to-word lexicon. A seed parallel corpus is required to automatically build this lexicon. This algorithm seems particularly well suited in extracting phrase pairs from comparable sentence pairs, as it is desig"
W11-1209,P98-1069,0,\N,Missing
W11-1209,C98-1066,0,\N,Missing
W11-1209,P99-1067,0,\N,Missing
W11-1209,P07-2045,0,\N,Missing
W11-1209,P03-1010,0,\N,Missing
W11-1210,J93-2003,0,0.0188065,"Missing"
W11-1210,D09-1009,0,0.0237797,"m multiple learning tasks. There has been very less work in the area of multitask active learning. (Reichart et al., 2008) proposes an extension of the single-sided active elicitation task to a multi-task scenario, where data elicitation is performed for two or more independent tasks at the same time. (Settles et al., 2008) propose elicitation of annotations for image segmentation under a multi-instance learning framework. Active learning with multiple annotations also has similarities to the recent body of work in learning from instance feedback and feature feedback (Melville et al., 2005). (Druck et al., 2009) propose active learning extensions to the gradient approach of learning from feature and instance feedback. However, in the comparable corpora problem although the second annotation is geared towards learning better features by enhancing the coverage of the lexicon, the annotation itself is not on the features but for extracting training data that is then used to train the lexicon. 3 Supervised Comparable Sentence Classification In this section we discuss our supervised training setup and the classification algorithm. Our classifier tries to identify comparable sentences from among a large po"
W11-1210,P10-1074,0,0.0678486,"Missing"
W11-1210,P98-1069,0,0.0228526,"e discuss the supervised training setup for our classifier. In Section 4 we discuss the application of active learning to the classification task. Section 5 discusses the case of active learning with two different annotations and proposes an approach for combining them. Section 6 presents experimental results and the effectiveness of the active learning strategies. We conclude with further discussion and future work. 2 Related Work There has been a lot of interest in using comparable corpora for MT, primarily on extracting parallel sentence pairs from comparable sources (Zhao and Vogel, 2002; Fung and Yee, 1998). Some work has gone beyond this focussing on extracting subsentential fragments from noisier comparable data (Munteanu and Marcu, 2006; Quirk et al., 2007). The research conducted in this paper has two primary contributions and so we will discuss the related work as relevant to each of them. Our first contribution in this paper is the application of active learning for acquiring comparable 70 data in the low-resource scenario, especially relevant when working with low-resource languages. There is some earlier work highlighting the need for techniques to deal with low-resource scenarios.(Munte"
W11-1210,J05-4003,0,0.285922,"gual information retrieval techniques. Once we have identified a subset of documents that are potentially parallel, the second challenge is to identify comparable sentence pairs. This is an interesting challenge as the availability of completely parallel sentences on the internet is quite low in most language-pairs, but one can observe very few comparable sentences among comparable documents for a given language-pair. Our work tries to address this problem by posing the identification of comparable sentences from comparable data as a supervised classification problem. Unlike earlier research (Munteanu and Marcu, 2005) where the authors try to identify parallel sentences among a pool of comparable documents, we try to first identify comparable sentences in a pool with dominantly non-parallel sentences. We then build a supervised classifier that learns from user annotations for comparable corpora identification. Training such a classifier requires reliably annotated data that may be unavailable for low-resource language pairs. Involving a human expert to perform such annotations is expensive for low-resource languages and so we propose active learning as a suitable technique to reduce the labeling effort. Th"
W11-1210,P06-1011,0,0.0179755,"sification task. Section 5 discusses the case of active learning with two different annotations and proposes an approach for combining them. Section 6 presents experimental results and the effectiveness of the active learning strategies. We conclude with further discussion and future work. 2 Related Work There has been a lot of interest in using comparable corpora for MT, primarily on extracting parallel sentence pairs from comparable sources (Zhao and Vogel, 2002; Fung and Yee, 1998). Some work has gone beyond this focussing on extracting subsentential fragments from noisier comparable data (Munteanu and Marcu, 2006; Quirk et al., 2007). The research conducted in this paper has two primary contributions and so we will discuss the related work as relevant to each of them. Our first contribution in this paper is the application of active learning for acquiring comparable 70 data in the low-resource scenario, especially relevant when working with low-resource languages. There is some earlier work highlighting the need for techniques to deal with low-resource scenarios.(Munteanu and Marcu, 2005) propose bootstrapping using an existing classifier for collecting new data. However, this approach works when ther"
W11-1210,J03-1002,0,0.00350371,"t one other issue that needs to be solved in order for our classification based approach to work for truly low-resource language pairs. As we will describe later in the paper, our comparable sentence classifier relies on the availability of an ini69 Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 69–77, 49th Annual Meeting of the Association for Computational Linguistics, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics tial seed lexicon that can either be provided by a human or can be statistically trained from parallel corpora (Och and Ney, 2003). Experiments show that a broad coverage lexicon provides us with better coverage for effective identification of comparable corpora. However, availability of such a resource can not be expected in very low-resource language pairs, or even if present may not be of good quality. This opens an interesting research question - Can we also elicit such information effectively at low costs? We propose active learning strategies for identifying the most informative comparable sentence pairs which a human can then extract parallel segments from. While the first form of supervision provides us with clas"
W11-1210,2007.mtsummit-papers.50,0,0.0154183,"discusses the case of active learning with two different annotations and proposes an approach for combining them. Section 6 presents experimental results and the effectiveness of the active learning strategies. We conclude with further discussion and future work. 2 Related Work There has been a lot of interest in using comparable corpora for MT, primarily on extracting parallel sentence pairs from comparable sources (Zhao and Vogel, 2002; Fung and Yee, 1998). Some work has gone beyond this focussing on extracting subsentential fragments from noisier comparable data (Munteanu and Marcu, 2006; Quirk et al., 2007). The research conducted in this paper has two primary contributions and so we will discuss the related work as relevant to each of them. Our first contribution in this paper is the application of active learning for acquiring comparable 70 data in the low-resource scenario, especially relevant when working with low-resource languages. There is some earlier work highlighting the need for techniques to deal with low-resource scenarios.(Munteanu and Marcu, 2005) propose bootstrapping using an existing classifier for collecting new data. However, this approach works when there is a classifier of"
W11-1210,P08-1098,0,0.0288953,"ontribution of the paper is to extend the traditional active learning setup that is suitable for eliciting a single annotation. We highlight the needs of the comparable corpora scenario where we have two kinds of annotations - class label assignment and parallel segment extraction and propose strategies in active learning that involve multiple annotations. A relevant setup is multitask learning (Caruana, 1997) which is increasingly becoming popular in natural language processing for learning from multiple learning tasks. There has been very less work in the area of multitask active learning. (Reichart et al., 2008) proposes an extension of the single-sided active elicitation task to a multi-task scenario, where data elicitation is performed for two or more independent tasks at the same time. (Settles et al., 2008) propose elicitation of annotations for image segmentation under a multi-instance learning framework. Active learning with multiple annotations also has similarities to the recent body of work in learning from instance feedback and feature feedback (Melville et al., 2005). (Druck et al., 2009) propose active learning extensions to the gradient approach of learning from feature and instance feed"
W11-1210,J03-3002,0,0.0441443,"the annotations independently. 1 Introduction The state-of-the-art Machine Translation (MT) systems are statistical, requiring large amounts of parallel corpora. Such corpora needs to be carefully created by language experts or speakers, which makes building MT systems feasible only for those language pairs with sufficient public interest or financial support. With the increasing rate of social media creation and the quick growth of web media in languages other than English makes it relevant for language research community to explore the feasibility of Internet as a source for parallel data. (Resnik and Smith, 2003) show that parallel corpora for a variety of languages can be harvested on the Internet. It is to be observed that a major portion of the multilingual web documents are created independent of one another and so are only mildly parallel at the document level. There are multiple challenges in building comparable corpora for consumption by the MT systems. The first challenge is to identify the parallelism between documents of different languages which has been reliably done using cross lingual information retrieval techniques. Once we have identified a subset of documents that are potentially par"
W11-1210,C98-1066,0,\N,Missing
W11-2124,W10-1704,0,0.0698574,"Missing"
W11-2124,N03-2002,0,0.0604885,"been derived from the work of Casacuberta and Vidal (2004), which used finite state transducers for statistical machine translation. In this approach, units of source and target words are used as basic translation units. Then the translation model is implemented as an n-gram model over the tuples. As it is also done in phrase-based translations, the different translations are scored by a log-linear combination of the translation model and additional models. Crego and Yvon (2010) extended the approach to be able to handle different word factors. They used factored language models introduced by Bilmes and Kirchhoff (2003) to integrate different word factors into the translation process. In contrast, we use a log-linear combination of language models on different factors in our approach. A first approach of integrating the idea presented in the n-gram approach into phrase-based machine translation was described in Matusov et al. (2006). In contrast to our work, they used the bilingual units as defined in the original approach and they did not use additional word factors. Hasan et al. (2008) used lexicalized triplets to introduce bilingual context into the translation process. These triplets include source words"
W11-2124,D07-1007,0,0.048499,"the translation process. These triplets include source words from outside the phrase and form and additional probability p(f |e, e0 ) that modifies the conventional word probability of f given e depending on trigger words e0 in the sentence enabling a context-based translation of ambiguous phrases. Other approaches address this problem by integrating word sense disambiguation engines into a phrase-based SMT system. In Chan and Ng (2007) a classifier exploits information such as local col199 locations, parts-of-speech or surrounding words to determine the lexical choice of target words, while Carpuat and Wu (2007) use rich context features based on position, syntax and local collocations to dynamically adapt the lexicons for each sentence and facilitate the choice of longer phrases. In this work we present a method to extend the locally limited context of phrase pairs and n-grams by using bilingual language models. We keep the phrase-based approach as the main SMT framework and introduce an n-gram language model trained in a similar way as the one used in the finite state transducer approach as an additional feature in the loglinear model. 3 Motivation To motivate the introduction of the bilingual lang"
W11-2124,P07-1005,0,0.0236585,"lingual units as defined in the original approach and they did not use additional word factors. Hasan et al. (2008) used lexicalized triplets to introduce bilingual context into the translation process. These triplets include source words from outside the phrase and form and additional probability p(f |e, e0 ) that modifies the conventional word probability of f given e depending on trigger words e0 in the sentence enabling a context-based translation of ambiguous phrases. Other approaches address this problem by integrating word sense disambiguation engines into a phrase-based SMT system. In Chan and Ng (2007) a classifier exploits information such as local col199 locations, parts-of-speech or surrounding words to determine the lexical choice of target words, while Carpuat and Wu (2007) use rich context features based on position, syntax and local collocations to dynamically adapt the lexicons for each sentence and facilitate the choice of longer phrases. In this work we present a method to extend the locally limited context of phrase pairs and n-grams by using bilingual language models. We keep the phrase-based approach as the main SMT framework and introduce an n-gram language model trained in a"
W11-2124,C10-1040,1,0.800514,"rate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data as well as on additional monolingual News data. The translation model as well as the language model was adapted towards the target domain in a log-linear way. The Arabic-to-English system was trained using GALE Arabic data, which contains 6.1M sentences. The word alignment is generated using EMDC, which is a combination of a discriminative approach and the IBM Models as described in Gao et al. (2010). The phrase table is generated using Chaski as described in Gao and Vogel (2010). The language model data we trained on the GIGAWord V3 data plus BBN English data. After splitting the corpus according to sources, individual models were trained. Then the individual models were interpolated to minimize the perplexity on the MT03/MT04 data. For both tasks the reordering was performed as a preprocessing step using POS information from the TreeTagger (Schmid, 1994) for German and using the Amira Tagger (Diab, 2009) for Arabic. For Arabic the approach described in Rottmann and Vogel (2007) was used"
W11-2124,D08-1039,0,0.0110035,"xtended the approach to be able to handle different word factors. They used factored language models introduced by Bilmes and Kirchhoff (2003) to integrate different word factors into the translation process. In contrast, we use a log-linear combination of language models on different factors in our approach. A first approach of integrating the idea presented in the n-gram approach into phrase-based machine translation was described in Matusov et al. (2006). In contrast to our work, they used the bilingual units as defined in the original approach and they did not use additional word factors. Hasan et al. (2008) used lexicalized triplets to introduce bilingual context into the translation process. These triplets include source words from outside the phrase and form and additional probability p(f |e, e0 ) that modifies the conventional word probability of f given e depending on trigger words e0 in the sentence enabling a context-based translation of ambiguous phrases. Other approaches address this problem by integrating word sense disambiguation engines into a phrase-based SMT system. In Chan and Ng (2007) a classifier exploits information such as local col199 locations, parts-of-speech or surrounding"
W11-2124,P07-2045,0,0.0121597,"model on the English-to-German, German-to-English and French-to-English systems with which we participated in the WMT 2011. 5.1 System Description The German-to-English translation system was trained on the European Parliament corpus, News Commentary corpus and small amounts of additional Web data. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data as well as on additional monolingual News data. The translation model as well as the language model was adapted towards the target domain in a log-linear way. The Arabic-to-English system was trained using GALE Arabic data, which contains 6.1M sentences. The word alignment is generated using EMDC, which is a combination of a discriminative approach and the IBM Models as described in Gao et al. (2010). The phrase table is generated using Chaski as described in Gao and Vogel (2010). The language model data we trained on the"
W11-2124,J06-4004,0,0.431121,"Missing"
W11-2124,W08-0303,1,0.31921,"k. On the other hand, we evaluated the approach on the Arabic-to-English direction on News and Web data. Additionally, we present the impact of the bilingual language model on the English-to-German, German-to-English and French-to-English systems with which we participated in the WMT 2011. 5.1 System Description The German-to-English translation system was trained on the European Parliament corpus, News Commentary corpus and small amounts of additional Web data. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data as well as on additional monolingual News data. The translation model as well as the language model was adapted towards the target domain in a log-linear way. The Arabic-to-English system was trained using GALE Arabic data, which contains 6.1M sentences. The word alignment is generated using EMDC, which is a combination of a discriminative approach and the IBM Mod"
W11-2124,W09-0413,1,0.910746,"ined on the GIGAWord V3 data plus BBN English data. After splitting the corpus according to sources, individual models were trained. Then the individual models were interpolated to minimize the perplexity on the MT03/MT04 data. For both tasks the reordering was performed as a preprocessing step using POS information from the TreeTagger (Schmid, 1994) for German and using the Amira Tagger (Diab, 2009) for Arabic. For Arabic the approach described in Rottmann and Vogel (2007) was used covering short-range reorderings. For the German-to-English translation task the extended approach described in Niehues et al. (2009) was used to cover also the long-range reorderings typical when translating between German and English. For both directions an in-house phrase-based decoder (Vogel, 2003) was used to generate the translation hypotheses and the optimization was performed using MER training. The performance on the testsets were measured in case-insensitive BLEU and TER scores. 5.2 German to English We evaluated the approach on two different test sets from the News Commentary domain. The first consists of 2000 sentences with one reference. It will be referred to as Test 1. The second test set consists of 1000 sen"
W11-2124,2007.tmi-papers.21,1,0.445573,"as described in Gao et al. (2010). The phrase table is generated using Chaski as described in Gao and Vogel (2010). The language model data we trained on the GIGAWord V3 data plus BBN English data. After splitting the corpus according to sources, individual models were trained. Then the individual models were interpolated to minimize the perplexity on the MT03/MT04 data. For both tasks the reordering was performed as a preprocessing step using POS information from the TreeTagger (Schmid, 1994) for German and using the Amira Tagger (Diab, 2009) for Arabic. For Arabic the approach described in Rottmann and Vogel (2007) was used covering short-range reorderings. For the German-to-English translation task the extended approach described in Niehues et al. (2009) was used to cover also the long-range reorderings typical when translating between German and English. For both directions an in-house phrase-based decoder (Vogel, 2003) was used to generate the translation hypotheses and the optimization was performed using MER training. The performance on the testsets were measured in case-insensitive BLEU and TER scores. 5.2 German to English We evaluated the approach on two different test sets from the News Comment"
W11-2124,W10-1719,1,\N,Missing
W11-2124,J04-2004,0,\N,Missing
W11-2124,N03-1017,0,\N,Missing
W11-2124,W11-2145,1,\N,Missing
W11-2146,N10-1064,0,0.028016,"Missing"
W11-2146,P00-1037,0,0.182323,"Missing"
W11-2146,J93-2003,0,0.0157096,"nlm.nih.gov 390 and c1 = parallel . A value closer to one for P r(c1 |S, T ) indicates that (S, T ) are parallel. The features are defined primarily based on translation lexicon probabilities. Rather than computing word alignment between the two sentences, we use lexical probabilities to determine alignment points as follows: a source word s is aligned to a target word t if p(s|t) &gt; 0.5. Target word alignment is computed similarly. We defined a feature set which includes: length ratio and length difference between source and target sentences, lexical probability scores similar to IBM model 1 (Brown et al., 1993), number of aligned/unaligned words and the length of the longest aligned word sequence. Lexical probability score, and alignment features generate two sets of features based on translation lexica obtained by training in both directions. Features are normalized with respect to the sentence length. 5.2 Training and Testing the Classifier To train the model we need training examples that belong to each of the two classes: parallel and nonparallel. Initially we used a subset of the available parallel data as training examples for the classifier. This data was primarily sourced from medical conver"
W11-2146,P11-2051,1,0.91658,"ds in S2. Table 4 presents a comparison of translation performance of the baseline, S1 and S2 for the SMS test sets. Unfortunately, none of systems with spelling normalization outperformed the system trained on the original data. Restricting the spelling correction only to infrequent words (S2) performed better for the devtest sets, but not for the dev set, although all the test sets come from the same domain. 4 Corpus Expansion using Semantic Role Labeling To address the problem of limited resources, we tried to expand the training corpus by applying the corpus expansion method described in (Gao and Vogel, 2011). First, we parsed and labeled the semantic roles of the English side of the corpus, using the ASSERT labeler (Pradhan et al., 2004). Next, using the word alignment models of the parallel corpus, we extracted Semantic Role Label (SRL) substitution rules. SRL rules consist of source and target phrases that cover whole constituents of semantic roles, the verb frames they belong to, and the role labels of 389 the constituents. The source and target phrases must comply with the restrictions detailed in (Gao and Vogel, 2011). Third, for each sentence, we replaced one of embedded SRL substitution ru"
W11-2146,C90-2036,0,0.0582583,"Missing"
W11-2146,2010.eamt-1.37,0,0.0768969,"the text inherently contained several different spelling variants. These messages were translated into English by a group of volunteers during the disaster response. Another challenge with building a Haitian CreoleEnglish translation system is the lack of parallel data. As Haitian Creole is a less commonly spoken language, the available resources are limited. Other than the manually translated SMS messages, the available Haitian Creole-English parallel data is about 2 million tokens, which is considerably smaller than the parallel data available for the Standard Translation Task of the WMT11. Lewis (2010) details the effort quickly put forth by the Microsoft Translator team in building a Haitian Creole-English translation system from scratch, as part of the relief effort in Haiti. We took a similar approach to this shared task: rapidly building a translation system to a new language pair utilizing available resources. Within a short span (of about one week), we built a baseline translation system, identified the problems with the system, and exploited several approaches to rectify them and improve its overall performance. We addressed the issues above (namely: noise in the data and sparsity of"
W11-2146,2010.amta-workshop.1,0,0.0351288,"Missing"
W11-2146,J05-4003,0,0.352065,"Although some of the medical articles seemed to be direct translations of each other, converting the original pdf formats into text did not produce sentence aligned parallel articles. Rather, it produced sentence fragments (sometimes in different orders) due to the structural differences in the article pair. Hence a parallel sentence detection technique was necessary to process the data. Because the SMS messages are related to the disaster relief effort, which may include many words in the medical domain, we believe the newly extracted data may help improve translation performance. Following Munteanu and Marcu (2005), we used a Maximum Entropy classifier to identify comparable sentence. To avoid the problem of having different sentence orderings in the article pair, we take every source-target sentence pair in the two articles, and apply the classifier to detect if they are parallel. The classifier approach is appealing to a lowresource language such as Haitian Creole, because the features for the classifier can be generated with minimal translation resources (i.e. a translation lexicon). 5.1 Maximum Entropy Classifier The classifier probability can be defined as: P  n exp λ f (c , S, T ) j ij i j=1 P r"
W11-2146,J03-1002,0,0.00449179,"24,338 1.81M / 1.67M Parallel-OOD +Wikipedia +SMS dev (cl) devtest (cl) devtest (r) 23.84 23.89 32.28 22.28 22.42 33.49 17.32 17.37 29.95 Table 2: Translation results in BLEU for different corpora Table 1: Haitian Creole (HT) and English (EN) parallel data provide by WMT11 We preprocessed the data by separating the punctuations, and converting both sides into lower case. SMS data was further processed to normalize quotations and other punctuation marks, and to remove all markups. To build a baseline translation system we followed the recommended steps: generate word align1 ments using GIZA++ (Och and Ney, 2003) and phrase extraction using Moses (Koehn et al., 2007). We built a 4-gram language model with the SRI LM toolkit (Stolcke, 2002) using English side of the training corpus. Model parameters for the language model, phrase table, and lexicalized reordering model were optimized via minimum error-rate (MER) training (Och, 2003). The SMS test sets were provided in two formats: raw (r) and cleaned (cl), where the latter had been manually cleaned. We used the SMS dev clean to optimize the decoder parameters and the SMS devtest clean and SMS devtest raw as held-out evaluation sets. Each set contains 9"
W11-2146,P03-1021,0,0.0086037,"d converting both sides into lower case. SMS data was further processed to normalize quotations and other punctuation marks, and to remove all markups. To build a baseline translation system we followed the recommended steps: generate word align1 ments using GIZA++ (Och and Ney, 2003) and phrase extraction using Moses (Koehn et al., 2007). We built a 4-gram language model with the SRI LM toolkit (Stolcke, 2002) using English side of the training corpus. Model parameters for the language model, phrase table, and lexicalized reordering model were optimized via minimum error-rate (MER) training (Och, 2003). The SMS test sets were provided in two formats: raw (r) and cleaned (cl), where the latter had been manually cleaned. We used the SMS dev clean to optimize the decoder parameters and the SMS devtest clean and SMS devtest raw as held-out evaluation sets. Each set contains 900 sentences. A separate SMS test, with 1274 sentences, was used as the unseen test set in the final evaluation. For each experiment we report the case-insensitive BLEU (Papineni et al., 2002) score. Using the available training data we built several baseline systems: The first system (Parallel-OOD), uses all the out-of-dom"
W11-2146,P02-1040,0,0.0816128,"s. Model parameters for the language model, phrase table, and lexicalized reordering model were optimized via minimum error-rate (MER) training (Och, 2003). The SMS test sets were provided in two formats: raw (r) and cleaned (cl), where the latter had been manually cleaned. We used the SMS dev clean to optimize the decoder parameters and the SMS devtest clean and SMS devtest raw as held-out evaluation sets. Each set contains 900 sentences. A separate SMS test, with 1274 sentences, was used as the unseen test set in the final evaluation. For each experiment we report the case-insensitive BLEU (Papineni et al., 2002) score. Using the available training data we built several baseline systems: The first system (Parallel-OOD), uses all the out-of-domain parallel data except the Wikipedia sentences. The second system, in addition, includes Wikipedia data. The third system uses all available parallel training data (including both the out-of-domain data as well as in-domain SMS data). We used the third system as the baseline for later experiments. www.speech.cs.cmu.edu/haitian/ 387 Translation results for different test sets using the three systems are presented in Table 2. No significant difference in BLEU was"
W11-2146,N04-1030,0,0.0411674,"none of systems with spelling normalization outperformed the system trained on the original data. Restricting the spelling correction only to infrequent words (S2) performed better for the devtest sets, but not for the dev set, although all the test sets come from the same domain. 4 Corpus Expansion using Semantic Role Labeling To address the problem of limited resources, we tried to expand the training corpus by applying the corpus expansion method described in (Gao and Vogel, 2011). First, we parsed and labeled the semantic roles of the English side of the corpus, using the ASSERT labeler (Pradhan et al., 2004). Next, using the word alignment models of the parallel corpus, we extracted Semantic Role Label (SRL) substitution rules. SRL rules consist of source and target phrases that cover whole constituents of semantic roles, the verb frames they belong to, and the role labels of 389 the constituents. The source and target phrases must comply with the restrictions detailed in (Gao and Vogel, 2011). Third, for each sentence, we replaced one of embedded SRL substitution rules with equivalent rules that have the same verb frame and the same role label. The original method includes an additional but cruc"
W11-2146,P02-1019,0,0.078117,"Missing"
W11-2146,P07-2045,0,\N,Missing
W11-2164,W11-1210,1,0.751051,"cal Turk and Crowdflower, one can now easily tap the crowd to generate new data. The major challenge will be identifying if speakers of the target language(s) are available on the desired platform, and if not, if they could be motivated to particpate.10 Likewise, infrastructure and resources will be needed to evaluate the quality of the resulting translations (Zaidan and CallisonBurch, 2011). – Active Crowd Translation – This method combines active learning with crowdsourcing for annotation of parallel data in comparable resources, and can be used to increase the amount of data that is found (Ambati et al., 2011). Active learning might be applicable to other crowdsourcing tasks as well, such as being used in crowdsourcing for translating content or repairing translated content. Additionally, writers of Krey`ol use a large number of abbreviated forms for common expressions, a kind of shorthand. For example, av`en can be used to represent av`ek nou, mandem can be used for mande mwen, etc. Overall, the number of alternations and multi-way ambiguities also increases the level of noise and data sparsity. 9 So, even with a morphologically reduced language like Krey`ol, one has issues with data sparsity beyo"
W11-2164,W09-1804,0,0.0162292,"effects, and might also counter the large number of forms in morphologically rich languages (e.g., (Yeniterzi and Oflazer, 2010; Genzel, 2010), and many others). • Strategies to systematically deal with complex morphology – this is one on-going area of research that could still net large returns, since, even with some relatively high-data languages, such as Finnish, data is made sparser due to the multiplication of possible forms. There is too long a literature to really do justice here, but some recent work includes discrimitative lexicons (Jeong et al., 2010), sub-word alignment strategies (Bodrumlu et al., 2009), learning the morphological variants in a language (Oflazer and El-kahlout, 2007), using off-the-shelf morphological tools, e.g., Morfessor 11 , etc. • Use syntax or linguistic knowledge in the translation task – By reducing the hypothesis space for possible alignments, syntax-based 11 http://www.cis.hut.fi/projects/morpho/ approaches can do better in lower-data situations and can handle source-target discontinuities better than straight phrase-based systems (e.g., (Quirk and Menezes, 2006; Li et al., 2010)). 7 The MT Crisis Cookbook Given the relatively narrow domain context of Crisis MT—gen"
W11-2164,W97-0409,0,0.820593,"03; Oard and Och, 2003), in which participants were given a month to collect 504 data and build language technologies for previously unknown languages, including Machine Translation systems, there was a surprising amount of data for Krey`ol at the start of the Haitian crisis, and it became available relatively quickly. Partly, this is due to the growth of the Web, which has proven to be a surpisingly diverse multi-lingual resource. But it also stems crucially from work that had been done in the past on Krey`ol, specifically, the work that was done in the DIPLOMAT and NESPOLE! projects at CMU (Frederking et al., 1997). It was possible to assemble a reasonable sample of data for the language in very short order (i.e., days). Further, since the language itself is fairly reduced morphologically, it is an easier target for SMT. In contrast, if one were to sample a language at random from the set of the 7,000 languages spoken on the earth, one is more likely to find a language that is morphologically richer (e.g., fusional, aggutinating, polysynthetic). Morphological richness compounds the data sparsity problem, reducing the quality of the resulting SMT engines. In other words, a combination of a simple morphol"
W11-2164,C10-1043,0,0.011125,"target (or vice versa) – A corollary to data sparsity is faulty word alignment, where low frequency words fail to get good alignments because there is not enough data to reinforce fairly weak hypotheses, or where source-target distortion is high. Both problems disfavor what alignments do exist. If the source and target are reordered so that one side more closely matches the other, or one side is “enriched” to be more like the other, one can reduce distortion related effects, and might also counter the large number of forms in morphologically rich languages (e.g., (Yeniterzi and Oflazer, 2010; Genzel, 2010), and many others). • Strategies to systematically deal with complex morphology – this is one on-going area of research that could still net large returns, since, even with some relatively high-data languages, such as Finnish, data is made sparser due to the multiplication of possible forms. There is too long a literature to really do justice here, but some recent work includes discrimitative lexicons (Jeong et al., 2010), sub-word alignment strategies (Bodrumlu et al., 2009), learning the morphological variants in a language (Oflazer and El-kahlout, 2007), using off-the-shelf morphological to"
W11-2164,W11-1209,1,0.758743,"r of the world’s 10 Based on the results of an informal survey, there may be speakers of a hundred or more languages on Mechanical Turk. See http://www.junglelightspeed.com/amt language/ for a list of the languages that may be available on Turk. languages. Tapping non-traditional sources of data can help increase the supply of ever valuable training data for a language: – Mining comparable sources of data – mining comparable data for parallel data has a long history, including mining comparable sources for named entities (Udupa et al., 2009; Irvine et al., 2010; Hewavitharana and Vogel, 2008; Hewavitharana and Vogel, 2011), mining Wikipedia for parallel content, including sentences (Smith et al., 2010), and many more too numerous to list. There is always room for improvement and hybridization in this space, as well as tapping additional sources of data, such as the volumes of noisy comparable data on the Web. – Monolingual – More recent work has focused on mining monolingual sources of data, treating MT as a decipherment problem (Ravi and Knight, 2011), rather than a source-target mapping problem. – Dictionary bootstraps and backoffs – Despite the absence of context, dictionaries can be useful, especially for r"
W11-2164,2010.amta-papers.12,0,0.0322993,"difficult to impossible to come by for a large number of the world’s 10 Based on the results of an informal survey, there may be speakers of a hundred or more languages on Mechanical Turk. See http://www.junglelightspeed.com/amt language/ for a list of the languages that may be available on Turk. languages. Tapping non-traditional sources of data can help increase the supply of ever valuable training data for a language: – Mining comparable sources of data – mining comparable data for parallel data has a long history, including mining comparable sources for named entities (Udupa et al., 2009; Irvine et al., 2010; Hewavitharana and Vogel, 2008; Hewavitharana and Vogel, 2011), mining Wikipedia for parallel content, including sentences (Smith et al., 2010), and many more too numerous to list. There is always room for improvement and hybridization in this space, as well as tapping additional sources of data, such as the volumes of noisy comparable data on the Web. – Monolingual – More recent work has focused on mining monolingual sources of data, treating MT as a decipherment problem (Ravi and Knight, 2011), rather than a source-target mapping problem. – Dictionary bootstraps and backoffs – Despite the a"
W11-2164,2010.amta-papers.33,0,0.0207672,"e like the other, one can reduce distortion related effects, and might also counter the large number of forms in morphologically rich languages (e.g., (Yeniterzi and Oflazer, 2010; Genzel, 2010), and many others). • Strategies to systematically deal with complex morphology – this is one on-going area of research that could still net large returns, since, even with some relatively high-data languages, such as Finnish, data is made sparser due to the multiplication of possible forms. There is too long a literature to really do justice here, but some recent work includes discrimitative lexicons (Jeong et al., 2010), sub-word alignment strategies (Bodrumlu et al., 2009), learning the morphological variants in a language (Oflazer and El-kahlout, 2007), using off-the-shelf morphological tools, e.g., Morfessor 11 , etc. • Use syntax or linguistic knowledge in the translation task – By reducing the hypothesis space for possible alignments, syntax-based 11 http://www.cis.hut.fi/projects/morpho/ approaches can do better in lower-data situations and can handle source-target discontinuities better than straight phrase-based systems (e.g., (Quirk and Menezes, 2006; Li et al., 2010)). 7 The MT Crisis Cookbook Give"
W11-2164,W10-1718,0,0.0371579,"des discrimitative lexicons (Jeong et al., 2010), sub-word alignment strategies (Bodrumlu et al., 2009), learning the morphological variants in a language (Oflazer and El-kahlout, 2007), using off-the-shelf morphological tools, e.g., Morfessor 11 , etc. • Use syntax or linguistic knowledge in the translation task – By reducing the hypothesis space for possible alignments, syntax-based 11 http://www.cis.hut.fi/projects/morpho/ approaches can do better in lower-data situations and can handle source-target discontinuities better than straight phrase-based systems (e.g., (Quirk and Menezes, 2006; Li et al., 2010)). 7 The MT Crisis Cookbook Given the relatively narrow domain context of Crisis MT—generally the needed vocabulary and data should be centered on relief work, medical interactions, and communicating with the affected populations—it may be possible to approach Crisis MT as we would MT for any domain (e.g., news, government, etc.). With enough data relevant to a particular domain or sub-domain (e.g., earthquake, tsunami, nuclear disaster, flooding, etc.), it would be possible to build the relevant translation memories (TMs) and train highly domain-specific MT engines to produce translations of"
W11-2164,2010.amta-workshop.1,1,0.766314,"okbook available. Section 9 wraps up the paper. 2 Mission 4636 In Haiti, crowdsourced translation enabled communications between the Krey`ol-speaking Haitian population and English-speaking emergency responders. A small group of international aid workers established a phone-number,‘4636’1 , that people were able to send text messages to for free within Haiti. The actual translations were made by about 2000 Krey`ol2 and French speaking volunteers collaborating on an online microtasking platform that they used to translate, categorize, identify missing people and geolocate information on a map (Munro, 2010).3 After a month, this work was gradually transferred to paid workers in Mirebalais, Haiti. These messages, about 80,000 in total, were used as part of the shared task for the 2011 Workshop on Machine Translation. About 3,000 of the messages had the categories and coordinates refined by a third workforce working with the Ushahidi platform out of Boston.4 They published this information on an online crisis map and worked directly with the main emergency responder, the American Military, to identify actionable information. 1 See the “Mission 4636” website at http://www.mission4636.org for more i"
W11-2164,W11-0309,1,0.850354,"Missing"
W11-2164,2003.mtsummit-papers.37,0,0.236286,"ally available MT engine existed for Krey`ol. In less than five days, the Microsoft Translator site was supporting the language. Given that it can take weeks to months to develop an MT engine for a new language, it would not seem possible that an engine could be developed so quickly, especially for a lowresource, minority language. The reasons this was possible are varied, and are in some ways unique to Krey`ol. Haitian Krey`ol, as it turns out, has proven to be an exceptional case for a surprise language. Unlike the languages in Surprise Language Exercises of nearly a decade ago (Oard, 2003; Oard and Och, 2003), in which participants were given a month to collect 504 data and build language technologies for previously unknown languages, including Machine Translation systems, there was a surprising amount of data for Krey`ol at the start of the Haitian crisis, and it became available relatively quickly. Partly, this is due to the growth of the Web, which has proven to be a surpisingly diverse multi-lingual resource. But it also stems crucially from work that had been done in the past on Krey`ol, specifically, the work that was done in the DIPLOMAT and NESPOLE! projects at CMU (Frederking et al., 1997"
W11-2164,W07-0704,0,0.0150379,"y rich languages (e.g., (Yeniterzi and Oflazer, 2010; Genzel, 2010), and many others). • Strategies to systematically deal with complex morphology – this is one on-going area of research that could still net large returns, since, even with some relatively high-data languages, such as Finnish, data is made sparser due to the multiplication of possible forms. There is too long a literature to really do justice here, but some recent work includes discrimitative lexicons (Jeong et al., 2010), sub-word alignment strategies (Bodrumlu et al., 2009), learning the morphological variants in a language (Oflazer and El-kahlout, 2007), using off-the-shelf morphological tools, e.g., Morfessor 11 , etc. • Use syntax or linguistic knowledge in the translation task – By reducing the hypothesis space for possible alignments, syntax-based 11 http://www.cis.hut.fi/projects/morpho/ approaches can do better in lower-data situations and can handle source-target discontinuities better than straight phrase-based systems (e.g., (Quirk and Menezes, 2006; Li et al., 2010)). 7 The MT Crisis Cookbook Given the relatively narrow domain context of Crisis MT—generally the needed vocabulary and data should be centered on relief work, medical i"
W11-2164,2006.amta-panels.3,0,0.072993,"ut some recent work includes discrimitative lexicons (Jeong et al., 2010), sub-word alignment strategies (Bodrumlu et al., 2009), learning the morphological variants in a language (Oflazer and El-kahlout, 2007), using off-the-shelf morphological tools, e.g., Morfessor 11 , etc. • Use syntax or linguistic knowledge in the translation task – By reducing the hypothesis space for possible alignments, syntax-based 11 http://www.cis.hut.fi/projects/morpho/ approaches can do better in lower-data situations and can handle source-target discontinuities better than straight phrase-based systems (e.g., (Quirk and Menezes, 2006; Li et al., 2010)). 7 The MT Crisis Cookbook Given the relatively narrow domain context of Crisis MT—generally the needed vocabulary and data should be centered on relief work, medical interactions, and communicating with the affected populations—it may be possible to approach Crisis MT as we would MT for any domain (e.g., news, government, etc.). With enough data relevant to a particular domain or sub-domain (e.g., earthquake, tsunami, nuclear disaster, flooding, etc.), it would be possible to build the relevant translation memories (TMs) and train highly domain-specific MT engines to produc"
W11-2164,P11-1002,0,0.0271778,"data has a long history, including mining comparable sources for named entities (Udupa et al., 2009; Irvine et al., 2010; Hewavitharana and Vogel, 2008; Hewavitharana and Vogel, 2011), mining Wikipedia for parallel content, including sentences (Smith et al., 2010), and many more too numerous to list. There is always room for improvement and hybridization in this space, as well as tapping additional sources of data, such as the volumes of noisy comparable data on the Web. – Monolingual – More recent work has focused on mining monolingual sources of data, treating MT as a decipherment problem (Ravi and Knight, 2011), rather than a source-target mapping problem. – Dictionary bootstraps and backoffs – Despite the absence of context, dictionaries can be useful, especially for resolving out-of-vocabulary items (OOVs). Many bilingual dictionaries also contain example sentences, which can be harvested and used in training. – Field data from linguists – Given that linguists have variously studied a large percentage of the world’s languages, tapping the supply of data that they have accumulated could prove quite fruitful. Some recent work tapping annotated bitexts (at this time, for over 1,200 languages) produce"
W11-2164,N10-1063,0,0.0447339,"Missing"
W11-2164,E09-1091,0,0.0190999,"a. Parallel data is difficult to impossible to come by for a large number of the world’s 10 Based on the results of an informal survey, there may be speakers of a hundred or more languages on Mechanical Turk. See http://www.junglelightspeed.com/amt language/ for a list of the languages that may be available on Turk. languages. Tapping non-traditional sources of data can help increase the supply of ever valuable training data for a language: – Mining comparable sources of data – mining comparable data for parallel data has a long history, including mining comparable sources for named entities (Udupa et al., 2009; Irvine et al., 2010; Hewavitharana and Vogel, 2008; Hewavitharana and Vogel, 2011), mining Wikipedia for parallel content, including sentences (Smith et al., 2010), and many more too numerous to list. There is always room for improvement and hybridization in this space, as well as tapping additional sources of data, such as the volumes of noisy comparable data on the Web. – Monolingual – More recent work has focused on mining monolingual sources of data, treating MT as a decipherment problem (Ravi and Knight, 2011), rather than a source-target mapping problem. – Dictionary bootstraps and bac"
W11-2164,P10-1047,0,0.00780412,"he source look more like the target (or vice versa) – A corollary to data sparsity is faulty word alignment, where low frequency words fail to get good alignments because there is not enough data to reinforce fairly weak hypotheses, or where source-target distortion is high. Both problems disfavor what alignments do exist. If the source and target are reordered so that one side more closely matches the other, or one side is “enriched” to be more like the other, one can reduce distortion related effects, and might also counter the large number of forms in morphologically rich languages (e.g., (Yeniterzi and Oflazer, 2010; Genzel, 2010), and many others). • Strategies to systematically deal with complex morphology – this is one on-going area of research that could still net large returns, since, even with some relatively high-data languages, such as Finnish, data is made sparser due to the multiplication of possible forms. There is too long a literature to really do justice here, but some recent work includes discrimitative lexicons (Jeong et al., 2010), sub-word alignment strategies (Bodrumlu et al., 2009), learning the morphological variants in a language (Oflazer and El-kahlout, 2007), using off-the-shelf m"
W11-2164,P11-1122,0,0.0649221,"Missing"
W11-2164,monson-etal-2008-linguistic,0,\N,Missing
W11-2164,2010.eamt-1.37,1,\N,Missing
W11-2164,W02-1039,0,\N,Missing
W12-3136,J93-2003,0,0.0307065,"nted with. In Section 3, we discuss our primary and secondary submissions for the two language pairs. Finally, in Section 4, we provide a short summary. 2.1 Initial Configuration Our baseline system can be summarized as follows: • Training: News Commentary + Europarl training bi-texts; • Tuning: news2010; • Testing: news2011; • Tokenization: splitting words containing a dash, e.g., first-order becomes first @-@ order; • Maximum sentence length: 100 tokens; • Truecasing: convert sentence-initial words to their most frequent case in the training dataset; • Word alignments: directed IBM model 4 (Brown et al., 1993) alignments in both directions, then grow-diag-final-and heuristics; • Maximum phrase length: 7 tokens; 1 The WMT12 organizers invited systems translating between English and four other European languages, in both directions: French, Spanish, German, and Czech. However, we only participated in Spanish→English and German→English. • Phrase table scores: forward & reverse phrase translation probabilities, forward & reverse lexical translation probabilities, phrase penalty; 298 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 298–303, c Montr´eal, Canada, June 7-8, 2012. 2"
W12-3136,N03-1017,0,0.0392872,"g Research Institute for the WMT12 Shared Translation Task. We used a phrase-based statistical machine translation model with several non-standard settings, most notably tuning data selection and phrase table combination. The evaluation results show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English. 1 System Description Introduction The team of the Qatar Computing Research Institute (QCRI) participated in the Shared Translation Task of WMT12 for two language pairs:1 SpanishEnglish and German-English. We used the state-ofthe-art phrase-based model (Koehn et al., 2003) for statistical machine translation (SMT) with several non-standard settings, e.g., data selection and phrase table combination. The evaluation results show that we rank second in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) for Spanish-English, and in the top tier for German-English. In Section 2, we describe the parameters of our baseline system and the non-standard settings we experimented with. In Section 3, we discuss our primary and secondary submissions for the two language pairs. Finally, in Section 4, we provide a short summary. 2.1 Initial Configuration Our baseline sy"
W12-3136,P07-2045,0,0.0110187,"2008): when a phrase pair appeared in both tables, they only kept the entry from the first table, while we keep the entries from both tables. 299 30.94 31.36 Table 2: Phrase table merging. 2.3 Language Models We built the language models (LM) for our systems using a probabilistic 5-gram model with KneserNey (KN) smoothing. We experimented with LMs trained on different training datasets. We used the SRILM toolkit (Stolcke, 2002) for training the language models, and the KenLM toolkit (Heafield and Lavie, 2010) for binarizing the resulting ARPA models for faster loading with the Moses decoder (Koehn et al., 2007). 2.3.1 Using WMT12 Corpora Only We trained 5-gram LMs on datasets provided by the task organizers. The results are presented in Table 3. The first line reports the baseline BLEU scores using a language model trained on the target side of the News Commentary + Europarl training bi-texts. The second line shows the results when using an interpolation (minimizing the perplexity on the news2010 tuning dataset) of different language models, trained on the following corpora: • the monolingual News Commentary corpus plus the English sides of all training News Commentary v.7 bi-texts (for French-Engli"
W12-3136,N04-1022,0,0.0436485,"t. This means that our selected source-side sentences tended to be shorter than in the baseline. Moreover, the standard deviation of the sentence lengths was smaller for our samples as well, which means that there were fewer long sentences; this is good since long sentences can take very long to translate. As a result, we observed sizable speedup in parameter tuning when running MERT on our selected tuning datasets. Decoding and Hypothesis Reranking We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmann and Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). The results are shown in Table 7. We can see that both yield improvements in BLEU, even if small. 2.6 Baseline (es:#2,de:#3) +MP 29.83 29.98 21.72 22.03 Baseline (es:#4,de:#5) +MBR 30.16 30.31 22.30 22.48 Table 7: Decoding parameters. Experiments with monotone at punctuation (MP) reordering, and minimum Bayes risk (MBR) decoding. The results for the actual news2012 testset are shown in Table 8: the system combination results are our primary submission. We can see that system combination yielded 0.4 BLEU points of improvement for Spanish-English and 0.2-0.3 BLEU points for German-English. 3 T"
W12-3136,W08-0320,1,0.751604,"g MERT.3 Table 2 shows that this improves by +0.42 BLEU points. 2 In theory, we should also re-normalize the conditional probabilities (forward/reverse phrase translation probability, and forward/reverse lexicalized phrase translation probability) since they may not sum to one anymore. In practice, this is not that important since the log-linear phrase-based SMT model does not require that the phrase table features be probabilities (e.g., F1 , F2 , F3 , and the phrase penalty are not probabilities); moreover, we have extra features whose impact is bigger. 3 This is similar but different from (Nakov, 2008): when a phrase pair appeared in both tables, they only kept the entry from the first table, while we keep the entries from both tables. 299 30.94 31.36 Table 2: Phrase table merging. 2.3 Language Models We built the language models (LM) for our systems using a probabilistic 5-gram model with KneserNey (KN) smoothing. We experimented with LMs trained on different training datasets. We used the SRILM toolkit (Stolcke, 2002) for training the language models, and the KenLM toolkit (Heafield and Lavie, 2010) for binarizing the resulting ARPA models for faster loading with the Moses decoder (Koehn"
W12-3136,P02-1040,0,0.0922053,"le combination. The evaluation results show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English. 1 System Description Introduction The team of the Qatar Computing Research Institute (QCRI) participated in the Shared Translation Task of WMT12 for two language pairs:1 SpanishEnglish and German-English. We used the state-ofthe-art phrase-based model (Koehn et al., 2003) for statistical machine translation (SMT) with several non-standard settings, e.g., data selection and phrase table combination. The evaluation results show that we rank second in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) for Spanish-English, and in the top tier for German-English. In Section 2, we describe the parameters of our baseline system and the non-standard settings we experimented with. In Section 3, we discuss our primary and secondary submissions for the two language pairs. Finally, in Section 4, we provide a short summary. 2.1 Initial Configuration Our baseline system can be summarized as follows: • Training: News Commentary + Europarl training bi-texts; • Tuning: news2010; • Testing: news2011; • Tokenization: splitting words containing a dash, e.g., first-order become"
W12-3136,2006.amta-papers.25,0,0.0155937,"esults show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English. 1 System Description Introduction The team of the Qatar Computing Research Institute (QCRI) participated in the Shared Translation Task of WMT12 for two language pairs:1 SpanishEnglish and German-English. We used the state-ofthe-art phrase-based model (Koehn et al., 2003) for statistical machine translation (SMT) with several non-standard settings, e.g., data selection and phrase table combination. The evaluation results show that we rank second in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) for Spanish-English, and in the top tier for German-English. In Section 2, we describe the parameters of our baseline system and the non-standard settings we experimented with. In Section 3, we discuss our primary and secondary submissions for the two language pairs. Finally, in Section 4, we provide a short summary. 2.1 Initial Configuration Our baseline system can be summarized as follows: • Training: News Commentary + Europarl training bi-texts; • Tuning: news2010; • Testing: news2011; • Tokenization: splitting words containing a dash, e.g., first-order becomes first @-@ order; • Maximum s"
W12-3136,J03-1005,0,0.0353527,"smaller than in our baseline, the news2011 development dataset. This means that our selected source-side sentences tended to be shorter than in the baseline. Moreover, the standard deviation of the sentence lengths was smaller for our samples as well, which means that there were fewer long sentences; this is good since long sentences can take very long to translate. As a result, we observed sizable speedup in parameter tuning when running MERT on our selected tuning datasets. Decoding and Hypothesis Reranking We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmann and Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). The results are shown in Table 7. We can see that both yield improvements in BLEU, even if small. 2.6 Baseline (es:#2,de:#3) +MP 29.83 29.98 21.72 22.03 Baseline (es:#4,de:#5) +MBR 30.16 30.31 22.30 22.48 Table 7: Decoding parameters. Experiments with monotone at punctuation (MP) reordering, and minimum Bayes risk (MBR) decoding. The results for the actual news2012 testset are shown in Table 8: the system combination results are our primary submission. We can see that system combination yielded 0.4 BLEU points of improvement for Sp"
W13-2246,C04-1046,0,0.131552,"fficulty of a source sentence is often closely related to the sentence length, as longer sentences tend to have a more complex structure. Also a skewed ratio between the length of the source sentence and its translation can be an indicator for a bad translation. We use plain sentence length features, namely the source sentence length, the translation hypothesis length and their ratio as introduced in Quirk (2004). En train 1,714,385 26.84 14.54 0 - 12.29 12.29 - 41.38 41.38 - 100 0.9624 0.9315 0.9559 0.9817 Table 1: Sentence Length Statistics for the English-Spanish Parallel Corpus Similar to Blatz et al. (2004) we use the n-best list as an information source. We calculate the average hypothesis length in the n-best list for one source sentence. Then we compare the current hypothesis to that and calculate both the diversion from that average as well as their ratio. We also calculate the source-target ratio to this average hypothesis length. 3.6 Source Language Word and Bi-gram Frequency Features The length of words is often related to whether they are content words and how frequently they are used in the language. Therefore we use the maximum and average word length as features. Similar to Blatz et a"
W13-2246,W12-3102,0,0.0663066,"Missing"
W13-2246,de-marneffe-etal-2006-generating,0,0.195032,"Missing"
W13-2246,2005.eamt-1.15,0,0.240735,"egie Mellon University Pittsburgh, USA silja@cs.cmu.edu Stephan Vogel Qatar Computing Research Institute Doha, Qatar svogel@qf.org.qa Abstract al. (2004)), some operate on the word level (e.g. Ueffing and Ney (2007), Sanchis et al. (2007), and Bach et al. (2011)), whereas Soricut and Echihabi (2010) use the document level. Various classifiers and regression models have been used in QE in the past. Gandrabur and Foster (2003) compare single layer to Multi Layer Perceptron (MLP), Quirk et al. (2004) report that Linear Regression (LR) produced the best results in a comparison of LR, MLP and SVM, Gamon et al. (2005) use SVM, Soricut and Echihabi (2010) find the M5P tree works best among a number of regression models, while Bach et al. (2011) define the problem as a word sequence labeling task and use MIRA. The QE shared task was added to the WMT evaluation campaign in 2012 (Callison-Burch et al., 2012), providing standard training and test data for system development. In this paper we present our entry to the WMT’13 shared task: Quality Estimation (QE) for machine translation (MT). We participated in the 1.1, 1.2 and 1.3 sub-tasks with our QE system trained on features from diverse information sources li"
W13-2246,W03-0413,0,0.0375283,"These include bilingual training corpora, language models, 1000-best lists, models from giza and moses training and various other statistics and models depending on task and language pair. 3 3.1 3.4 From giza training we use IBM-4 statistical word lexica in both directions. We use six probability based features as described in Hildebrand and Vogel (2008): Normalized probability, maximum word probability and word deletion count from each language direction. To judge the translation difficulty of each word in the source sentence we collect the number of lexicon entries for each word similar to Gandrabur and Foster (2003). The intuition is, that a word with many translation alternatives in the word-toword lexicon is difficult to translate while a word with only a few translation choices is easy to translate. In fact it is not quite this straight forward. There are words in the lexicon, which have many lexicon entries, but the probability for them is not very equally distributed. One entry has a very high probability while all others have a very low one - not much ambiguity there. Other words on the other hand have several senses in one language and therefore are translated frequently into two or three differen"
W13-2246,W11-2123,0,0.0153656,"ther or not the word is OOV. If a word has no lexicon entry with a probability over the threshold we exclude the word from the lexicon for this purpose and count it as an OOV. As sentence level features we use the sum of the word level features normalized by the sentence length as well as the total OOV count for the sentence, which results in five features. Features Language Models To calculate language model (LM) features, we train traditional n-gram language models with ngram lengths of four and five using the SRILM toolkit (Stolcke, 2002). We calculate our features using the KenLM toolkit (Heafield, 2011). We normalize all our features with the target sentence length to get an average word feature score, which is comparable for translation hypotheses of different length. In addition to the LM probability we record the average n-gram length found in the language model for the sentence, the total number of LM OOVs and OOVs per word, as well as the maximum and the minimum word probability of the sentence, six features total. We use language models trained on source language data and target language data to measure source sentence difficulty as well as translation fluency. 3.2 Distortion Model The"
W13-2246,P10-1063,0,0.0349837,"urgh, USA silja@cs.cmu.edu Stephan Vogel Qatar Computing Research Institute Doha, Qatar svogel@qf.org.qa Abstract al. (2004)), some operate on the word level (e.g. Ueffing and Ney (2007), Sanchis et al. (2007), and Bach et al. (2011)), whereas Soricut and Echihabi (2010) use the document level. Various classifiers and regression models have been used in QE in the past. Gandrabur and Foster (2003) compare single layer to Multi Layer Perceptron (MLP), Quirk et al. (2004) report that Linear Regression (LR) produced the best results in a comparison of LR, MLP and SVM, Gamon et al. (2005) use SVM, Soricut and Echihabi (2010) find the M5P tree works best among a number of regression models, while Bach et al. (2011) define the problem as a word sequence labeling task and use MIRA. The QE shared task was added to the WMT evaluation campaign in 2012 (Callison-Burch et al., 2012), providing standard training and test data for system development. In this paper we present our entry to the WMT’13 shared task: Quality Estimation (QE) for machine translation (MT). We participated in the 1.1, 1.2 and 1.3 sub-tasks with our QE system trained on features from diverse information sources like MT decoder features, n-best lists,"
W13-2246,W10-1745,1,0.887673,"t: 0.3 to 0.01 improved the performance considerably. We report Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) for all results. Source-Target Word Alignment Features A forced alignment algorithm utilizes the trained alignment models from the MT systems GIZA (Och and Ney, 2003) training to align each source sentence to each translation hypothesis. We use the score given by the word alignment models, the number of unaligned words and the number of NULL aligned words, all normalized by the sentence length, as three separate features. We calculate those for both language directions. Hildebrand and Vogel (2010) successfully applied these features in n-best list re-ranking. 3.10 5 Experiment Results For Tasks 1.1 and 1.3 we used the 1000-best output provided. As first step we removed duplicate entries in these n-best list. This brought the size down to an average of 152.9 hypotheses per source sentence for the Task 1.1 training data, 172.7 on the WMT12 tests set and 204.3 hypotheses per source sentence on the WMT13 blind test data. The training data for task 1.3 has on average 129.0 hypothesis per source sentence, the WMT13 blind test data 129.8. In addition to our own features described above we ext"
W13-2246,P03-1054,0,0.0134309,"re in each gap. We go recursively up the tree, always including all sub-trees for each head word. If there was a violation in one of the sub-trees it might be resolved by adding in its siblings, but if the violation persists, it is counted again. Source Parse Features The intuition is that a sentence is harder to translate, if its structure is more complicated. A simple indicator for a more complex sentence structure is the presence of subclauses and also the length of any clauses and subclauses. To obtain the clause structure, we parse the source language sentence using the Stanford Parser2 (Klein and Manning, 2003). Features are: The number of clauses and subclauses, the average clause length, and the number of sentence fragments found. If the parse does not contain a clause tag, it is treated as one clause which is a fragment. 3.9 4 For all experiments we used the Weka3 data mining toolkit described in Hall et. al. (2009) to compare four different classifiers: Linear Regression (LR), M5P tree (M5Ptree), Multi Layer Perceptron (MLP) and Support Vector Machine for Regression (SVM). Each of these has been identified as effective in previous publications. All but one of the Weka default settings proved rel"
W13-2246,J07-1003,0,0.0478928,"Missing"
W13-2246,J03-1002,0,0.00608715,"different classifiers: Linear Regression (LR), M5P tree (M5Ptree), Multi Layer Perceptron (MLP) and Support Vector Machine for Regression (SVM). Each of these has been identified as effective in previous publications. All but one of the Weka default settings proved reliable, changing the learning rate for the MLP from default: 0.3 to 0.01 improved the performance considerably. We report Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) for all results. Source-Target Word Alignment Features A forced alignment algorithm utilizes the trained alignment models from the MT systems GIZA (Och and Ney, 2003) training to align each source sentence to each translation hypothesis. We use the score given by the word alignment models, the number of unaligned words and the number of NULL aligned words, all normalized by the sentence length, as three separate features. We calculate those for both language directions. Hildebrand and Vogel (2010) successfully applied these features in n-best list re-ranking. 3.10 5 Experiment Results For Tasks 1.1 and 1.3 we used the 1000-best output provided. As first step we removed duplicate entries in these n-best list. This brought the size down to an average of 152."
W13-2246,P03-1021,0,0.0306125,"not available. We rounded the predicted ranks to integer. Since the training data contains many ties we did not employ a strategy to resolve ties. As a contrastive approach we ran the hypothesis selection system described in Hildebrand and Vogel (2010) using the BLEU MT metric as ranking criteria. For this system it would have been very beneficial to have access to the n-best lists for the different system’s translations. The BLEU score for the translation listed as the first system for each source sentence would be 30.34 on the entire training data. We ran n-best list re-ranking using MERT (Och, 2003) for two feature sets: The full feature set, 100 features in total and a slim feature set with 59 features. For the slim feature set we removed all features that are solely based on The official result for our system for task 1.1 on the WMT13 blind data is MAE 13.84, RMSE 17.46 for the no-WMT12-base+SVM system and MAE 15.25 RMSE 18.97 for the full+MLP system. Surprising here is the fact that our full system clearly outperforms the 17-feature baseline on the WMT12 test set, but is behind it on the WMT13 blind test set. (Baseline bb17 SVM: MAE 14.81, 377 set was the right criterium for selecting"
W13-2246,quirk-2004-training,0,0.032951,"Features number of sentences average length standard deviation class short class medium class long s/t ratio overall s/t ratio for short s/t ratio for medium s/t ratio for long The translation difficulty of a source sentence is often closely related to the sentence length, as longer sentences tend to have a more complex structure. Also a skewed ratio between the length of the source sentence and its translation can be an indicator for a bad translation. We use plain sentence length features, namely the source sentence length, the translation hypothesis length and their ratio as introduced in Quirk (2004). En train 1,714,385 26.84 14.54 0 - 12.29 12.29 - 41.38 41.38 - 100 0.9624 0.9315 0.9559 0.9817 Table 1: Sentence Length Statistics for the English-Spanish Parallel Corpus Similar to Blatz et al. (2004) we use the n-best list as an information source. We calculate the average hypothesis length in the n-best list for one source sentence. Then we compare the current hypothesis to that and calculate both the diversion from that average as well as their ratio. We also calculate the source-target ratio to this average hypothesis length. 3.6 Source Language Word and Bi-gram Frequency Features The l"
W14-3628,bouamor-etal-2014-multidialectal,0,0.0565805,"a for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an unsupervised segmentation tool, Morphessor, and its MAP model (Creutz and Lagus, 2007), using different variations of the collected Qatari data. We optimize the single hyp"
W14-3628,J93-2003,0,0.0394221,"Missing"
W14-3628,2012.eamt-1.60,0,0.0204419,"n help to better adapt resources for dialects and MSA for SMT. This section describes our experimental setup. to Qº, and we reduce character elongations to be just two characters long. In order to maintain consistency among different resources, we re  move supplementary diacritics, e.g., Y®« ‘knots’ Datasets: We divided the QCA corpus into 1k sentences each for development and testing, and we used the remaining 12k for training. We adapted parallel corpora for Egyptian, Levantine and MSA to English to be used for Qatari Arabic to English SMT. For MSA, we used parallel corpora of TED talks (Cettolo et al., 2012) and the AMARA corpus (Abdelali et al., 2014), which consists of educational videos. Since the QCA corpus is in the speech domain, we believe that an MSA corpus of spoken domain would be more helpful than a text domain such as News. For Egyptian and Levantine, we used the parallel corpus provided by Zbib et al. (2012). There is no Gulf–English parallel data available in the literature. The data that we found was a very small collection of subdialects of Gulf Arabic; we did not use it for MT experiments. However, we used the Qatari part of the AVIA corpus to train Morfessor.  /Euqad/ is normal"
W14-3628,E06-1047,0,0.0771248,"so explain our experimental setup and we present the results (Section 5). We then discuss translating in the reverse direction, i.e., into Qatari Arabic (Section 6). Finally, we point to possible directions for future work and we conclude the paper (Section 7). Building morphological segmenters for the Arabic dialects: Researchers have already focused efforts on crafting and extending existing MSA tools to DA by mainly using a set of rules (Habash et al., 2012). Habash and Rambow (2006) presented MAGEAD, a knowledge-based morphological analyzer and generator for Egyptian and Levantine Arabic. Chiang et al. (2006) developed a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Meanwhile, the MSA consonant /Z/ is realized as /D/ in EGY. For example, the MSA ‘luck’ is maintained pronunciation /HaZ/ of ¡k in QA and transformed to /HaD/ in EGY. This change is consistent in all words within each dialect. However, such phonological variations between dialects have the potential to add ambiguity to dialectal Arabic. The MSA consonant h. /j/ can be used to distinguish between different dialects, particularly Gulf subdialects. h. /j/ is pronounced as ø /y/ i"
W14-3628,P11-1105,0,0.0332246,"Missing"
W14-3628,elmahdy-etal-2014-development,0,0.0277134,"More detailed description follows below. Lexical variations are among the most obvious differences between Arabic dialects. For exam ple, the MSA word @ XAÓ ‘what’ /mA*A/ would be 3.4 QCA Table 1: Statistics about the collected parallel corpora (in thousands). AVIAO shows the statistics about the AVIA corpus excluding Qatari data. /taEal˜am/ becomes ÕÎªK @ /AitEalim/ in EGY, while the MSA form is preserved in QA. 3.3 Corpus Bilingual corpora: – The QCA speech corpus, comprises 14.7k sentences that are phonetically transcribed from TV broadcasts in Qatari Arabic and translated to English; see (Elmahdy et al., 2014) for more detail. The corpus was designed for speech recognition and we faced several normalization-related issues that we had to resolve before it could be used for machine translation and language modeling. One example is the usage of five Persian characters to represent some sounds in Arabic words. Moreover, the English side had some grammatical and spelling errors. We normalized the Arabic side and corrected the English side of the corpus as described in Section 4.2. The corpus can be found at http://sprosig.isle. illinois.edu/corpora/1. – The AVIA corpus1 is designed as a reference source"
W14-3628,P12-1016,0,0.0171831,"/V/ to ¬ H /P/ to H. /b/, and P and h /J/ to h. /j/. /f/, For the English texts, the orthographic variations were already normalized. However, the English side of the QCA corpus had some spelling and grammatical errors, which we corrected manually. On the grammatical side, we only corrected a subset of the data, which we used for tuning and testing our SMT system (see Section 5). 4.3 Morphological Decomposition There is no general Arabic morphological segmenter that works for all variations of Arabic. The most commonly used segmenters for Arabic were designed for MSA (Habash et al., 2009; Green and DeNero, 2012). Due to the lexical and morphological differences between dialects and MSA, these MSA-based morphological tools do not work well for dialects. 5 Experimental Setup Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation. 6 This is an extension of the basic Morfessor method and is based on a Maximum a Posteriori model. This issue relates to the QCA corpus. 211 We built separate directed word alignments for source-to-target and target-to-source using IBM model 4 (Brown et a"
W14-3628,abdelali-etal-2014-amara,1,0.75838,"s and MSA for SMT. This section describes our experimental setup. to Qº, and we reduce character elongations to be just two characters long. In order to maintain consistency among different resources, we re  move supplementary diacritics, e.g., Y®« ‘knots’ Datasets: We divided the QCA corpus into 1k sentences each for development and testing, and we used the remaining 12k for training. We adapted parallel corpora for Egyptian, Levantine and MSA to English to be used for Qatari Arabic to English SMT. For MSA, we used parallel corpora of TED talks (Cettolo et al., 2012) and the AMARA corpus (Abdelali et al., 2014), which consists of educational videos. Since the QCA corpus is in the speech domain, we believe that an MSA corpus of spoken domain would be more helpful than a text domain such as News. For Egyptian and Levantine, we used the parallel corpus provided by Zbib et al. (2012). There is no Gulf–English parallel data available in the literature. The data that we found was a very small collection of subdialects of Gulf Arabic; we did not use it for MT experiments. However, we used the Qatari part of the AVIA corpus to train Morfessor.  /Euqad/ is normalized to Y®«, and we map Persian letters to th"
W14-3628,P06-1086,0,0.176067,"uses morphological segmentation to combine resources for other Arabic dialects in a QA-EN SMT system effectively (Section 4.3). We also explain our experimental setup and we present the results (Section 5). We then discuss translating in the reverse direction, i.e., into Qatari Arabic (Section 6). Finally, we point to possible directions for future work and we conclude the paper (Section 7). Building morphological segmenters for the Arabic dialects: Researchers have already focused efforts on crafting and extending existing MSA tools to DA by mainly using a set of rules (Habash et al., 2012). Habash and Rambow (2006) presented MAGEAD, a knowledge-based morphological analyzer and generator for Egyptian and Levantine Arabic. Chiang et al. (2006) developed a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Meanwhile, the MSA consonant /Z/ is realized as /D/ in EGY. For example, the MSA ‘luck’ is maintained pronunciation /HaZ/ of ¡k in QA and transformed to /HaD/ in EGY. This change is consistent in all words within each dialect. However, such phonological variations between dialects have the potential to add ambiguity to dialectal Arabic. The MSA conso"
W14-3628,al-sabbagh-girju-2010-mining,0,0.0365102,"me additional monolingual data for Qatari Arabic. Qatari Arabic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such a"
W14-3628,W14-3601,0,0.0778929,"ages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an unsupervised segmentation tool, Morphessor, and its MAP model (Creutz and Lagus, 2007), using different variations of the collected Qatari data. We optimize the single hyperparameter of the MAP model by maximizing the translation quality of the QA-EN SMT system in terms of BLEU. Our experimental results demo"
W14-3628,D09-1141,1,0.822284,"ith the QCA bitext for Qatari Arabic to English machine translation. We explored three segmentation options for the Arabic side of the data: (i) no segmentation, (ii) ATB segmentation, and (iii) unsupervised segmentation using Morfessor. The QCA corpus is of much smaller size compared to other Arabic variants, say MSA. It is possible that in the training of the machine translation models, the large corpus dominates the QCA corpus. In order to avoid that, we balanced the two corpora by replicating the smaller corpus X number of times in order to make it approximately equal to the large corpus (Nakov and Ng, 2009).8 The complete procedure is described below. In a nutshell, for building a machine translation system using the MSA plus Qatari corpus, we first balanced the Qatari corpus to make it approximately equal to MSA and concatenated them. For training Morfessor, the Qatari Arabic data consisted of QCA, Novels and AVIAQA , while for SMT, it consisted of QCA only. In both cases, we balanced it to be approximately equal to MSA. We then trained Morfessor on the balanced (QCA, Novels, AVIAQA ) plus MSA data and we segmented the Arabic side of the balanced QCA plus MSA training data for machine translati"
W14-3628,C12-1121,1,0.888804,"Missing"
W14-3628,W11-2123,0,0.0741014,"Missing"
W14-3628,P02-1040,0,0.0966622,"Missing"
W14-3628,D11-1125,0,0.0296435,"Missing"
W14-3628,W14-3627,0,0.108906,"ialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data. We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. The rest of the paper is organized as follows: Fi"
W14-3628,N09-1024,0,0.0276176,"s the first collection of monolingual corpora for Gulf Arabic subdialects. It can be helpful for, e.g., language modeling when translating into Arabic, for learning the similarities and differences between Gulf subdialects, etc. Table 2 shows some statistics about the data after punctuation tokenization. In this work, we used an unsupervised morphological segmenter, Morfessor-categories MAP6 , an unsupervised model with a single hyperparameter (Creutz and Lagus, 2007). We chose Morfessor because of its superior performance on Arabic compared to other unsupervised models (Siivola et al., 2007; Poon et al., 2009). The model has a single hyperparameter, the perplexity threshold parameter B, which controls the granularity of segmentation. The recommended value ranges from 1 to 400 where 1 means maximum fine-grained segmentation, and 400 restricts it to the least segmented output. We set the threshold empirically to 70, as shown in Section 5.1. 4.2 5 Corpus Tokens Types AE BH Novel KW OM QA SA Forum QA 573 43 244 22 178 27 412 43 614 71 69 15 372 27 Table 2: Statistics about the collected monolingual corpora (in thousands of words). Orthographic Normalization The inconsistency in the orthographic spellin"
W14-3628,2006.amta-papers.21,0,0.0275349,"oped a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Meanwhile, the MSA consonant /Z/ is realized as /D/ in EGY. For example, the MSA ‘luck’ is maintained pronunciation /HaZ/ of ¡k in QA and transformed to /HaD/ in EGY. This change is consistent in all words within each dialect. However, such phonological variations between dialects have the potential to add ambiguity to dialectal Arabic. The MSA consonant h. /j/ can be used to distinguish between different dialects, particularly Gulf subdialects. h. /j/ is pronounced as ø /y/ in KW, Riesa and Yarowsky (2006) trained a supervised trie-based model using a small lexicon of dialectal affixes. In our work, we eliminate the need for linguistic knowledge by training an unsupervised model using available resources. The unsupervised mode of learning allowed us to develop a multi-dialectal morphological segmenter. 3 Arabic Dialects In this section, we highlight some of the linguistic differences between Arabic dialects and MSA, with a focus on the Qatari dialect. 3.1 BH, QA, AE,  /q/ in OM, much like in EGY, h. /j/ in SA, much like in LEV. For example, the MSA word Yj.Ó ‘mosque’ /masjid/ is  pronounced"
W14-3628,N03-1017,0,0.00752633,"these MSA-based morphological tools do not work well for dialects. 5 Experimental Setup Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation. 6 This is an extension of the basic Morfessor method and is based on a Maximum a Posteriori model. This issue relates to the QCA corpus. 211 We built separate directed word alignments for source-to-target and target-to-source using IBM model 4 (Brown et al., 1993), and we symmetrized them using the grow-diag-final-and heuristics (Koehn et al., 2003). We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011). Finally, we built a log-linear model using the above features. We tuned the model weights by optimizing BLEU (Papineni et al., 2002) on the tuning set, using PRO (Hopkins and May, 2011) with sentencelevel BLEU+1 optimization (Nakov et al., 2012). In testing, we used"
W14-3628,P13-2001,1,0.937556,"ic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data. We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. The rest of the paper is o"
W14-3628,N04-1022,0,0.0213042,"Missing"
W14-3628,salama-etal-2014-youdacc,0,0.1266,"ic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an unsupervised segmentat"
W14-3628,maamouri-etal-2006-developing,0,0.115294,"l, we also collected some additional monolingual data for Qatari Arabic. Qatari Arabic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dia"
W14-3628,W11-2602,0,0.0453054,"which we train on the Arabic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data. We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. The r"
W14-3628,2010.amta-papers.5,0,0.355818,"ation model, which we train on the Arabic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data. We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap betwe"
W14-3628,P11-2007,0,0.145365,"ta for Qatari Arabic. Qatari Arabic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an"
W14-3628,N12-1006,0,0.715837,"the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an unsupervised segmentation tool, Morphessor"
W14-3628,N06-2051,1,0.790504,"word forms of a root word may not be always possible. Considering the different variants of Arabic, the problem is exacerabated as dialects could use different choices of affixes for the same function. For example, the MSA . ªÊK /yalEabuwn/, meaning ‘they are playword àñJ ing’, could be found as . ªÊK /ylEbuwn/ in Gulf, àñJ @ñJ.ªÊK Ñ« /Eam yilEabuA/ in Levantine, and as @ñJ.ªÊJ K. /biylEabwA/ in Egyptian Arabic. as Introduction One possible solution is to use a morphological segmenter that segments words into simpler units such as stems and affixes, which might be covered in the training set (Zollmann et al., 2006; Tsai et al., 2010). When applied to dialects, this may reduce the lexical gap between dialects and MSA by matching the common stems. Unfortunately, there are no standard morphological segmentation tools for dialects. Due to the difference in morphology, tools designed for MSA do not work well for dialects. Developing rule-based segmenters for each dialect might appear to be the ideal solution, but, as the orthography of dialects is not standardized, crafting linguistic rules for them is very hard. The Arabic language has many varieties, where the Modern Standard Arabic (MSA) coexists with va"
W14-3628,P07-2045,0,\N,Missing
W14-3628,W12-2301,0,\N,Missing
W14-3628,2013.iwslt-evaluation.8,1,\N,Missing
W15-3059,W11-2101,0,0.0448672,"Missing"
W15-3059,W07-0718,0,0.744642,"ls with only target language information. 1 Introduction Each year thousands of human judgments are used to evaluate the quality of Machine Translation (MT) systems to determine which algorithms and techniques are to be considered the new state-ofthe-art. In a typical scenario human judges evaluate a system’s output (or hypothesis) by comparing it to a source sentence and/or to a reference translation. Then, they score the hypothesis according to a set of defined criteria such as fluency and adequacy (White et al., 1994); or rank a set of hypotheses in order of preference (Vilar et al., 2007; Callison-Burch et al., 2007). Evaluating MT output can be a challenging task for a number of reasons: it is tedious and therefore evaluators can lose interest quickly; it is complex, especially if the guidelines are not well defined; and evaluators can have difficulty distinguishing between different aspects of the translations (Callison-Burch et al., 2007). • Given different scenarios, what source of information do evaluators use to evaluate a translation? Do they use the source text, the target text, or both? Does the availability of specific information changes the consistency of the evaluation? • Are there difference"
W15-3059,W12-3102,0,0.110664,"Missing"
W15-3059,P14-1065,1,0.885127,"Missing"
W15-3059,2013.mtsummit-wptp.5,0,0.0518021,"Missing"
W15-3059,2006.amta-papers.25,0,0.201332,"Missing"
W15-3059,stymne-etal-2012-eye,0,0.215144,"information do evaluators use to evaluate a translation? Do they use the source text, the target text, or both? Does the availability of specific information changes the consistency of the evaluation? • Are there differences of behavior between bilinguals (i.e. evaluators fluent in both source and target languages) and monolinguals (i.e. evaluators fluent only in the target language)? Which group is more consistent? 457 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 457–466, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. Stymne et al. (2012) applied eye-tracking to machine translation error analysis. They found that longer gaze time and and a higher number of fixations correlate with high number of errors in the MT output. Doherty and O’Brien (2014) used eyetracking to evaluate the quality of raw machine translation output in terms of its usability by an end user. They concluded that eye-tracking correlates well with the other measures which they used for their study. In this work, we use eye-tracking to observe which sources of information evaluators use while performing an MT evaluation task and how this impacts the task comple"
W15-3059,2003.mtsummit-papers.51,0,0.22915,"Missing"
W15-3059,W07-0713,0,0.0259127,"er to use monolinguals with only target language information. 1 Introduction Each year thousands of human judgments are used to evaluate the quality of Machine Translation (MT) systems to determine which algorithms and techniques are to be considered the new state-ofthe-art. In a typical scenario human judges evaluate a system’s output (or hypothesis) by comparing it to a source sentence and/or to a reference translation. Then, they score the hypothesis according to a set of defined criteria such as fluency and adequacy (White et al., 1994); or rank a set of hypotheses in order of preference (Vilar et al., 2007; Callison-Burch et al., 2007). Evaluating MT output can be a challenging task for a number of reasons: it is tedious and therefore evaluators can lose interest quickly; it is complex, especially if the guidelines are not well defined; and evaluators can have difficulty distinguishing between different aspects of the translations (Callison-Burch et al., 2007). • Given different scenarios, what source of information do evaluators use to evaluate a translation? Do they use the source text, the target text, or both? Does the availability of specific information changes the consistency of the eval"
W15-3059,H93-1040,0,0.574996,"Missing"
W15-3059,1994.amta-1.25,0,0.487622,"Missing"
W15-3059,1993.mtsummit-1.24,0,\N,Missing
W15-3059,W14-3352,1,\N,Missing
W15-3059,W12-4906,0,\N,Missing
zhang-etal-2004-interpreting,P02-1040,0,\N,Missing
