2004.jeptalnrecital-long.36,P01-1029,0,0.0669136,"Missing"
2004.jeptalnrecital-long.36,P94-1036,0,0.0944794,"Missing"
2004.jeptalnrecital-long.36,J01-1004,0,0.0520535,"Missing"
2005.jeptalnrecital-court.19,2004.jeptalnrecital-long.36,1,0.662612,"Missing"
2005.jeptalnrecital-long.11,clement-etal-2004-morphology,1,0.881976,"Missing"
2005.jeptalnrecital-long.11,2005.jeptalnrecital-long.1,1,0.702588,"Missing"
2006.jeptalnrecital-long.26,2005.jeptalnrecital-long.1,1,0.744321,"Missing"
2006.jeptalnrecital-long.26,P04-1057,0,0.132069,"Missing"
2006.jeptalnrecital-poster.25,C96-2103,0,0.062027,"Missing"
2006.jeptalnrecital-poster.25,E93-1045,0,0.121524,"Missing"
2007.jeptalnrecital-long.21,clement-etal-2004-morphology,1,0.892425,"Missing"
2007.jeptalnrecital-long.21,2005.jeptalnrecital-long.13,1,0.84026,"Missing"
2007.jeptalnrecital-long.21,sagot-etal-2006-lefff,1,0.919001,"Missing"
2007.jeptalnrecital-long.21,P06-1042,1,0.888994,"Missing"
2008.jeptalnrecital-long.18,steinberger-etal-2006-jrc,0,0.0288159,"Missing"
2008.jeptalnrecital-long.18,W97-0213,0,0.0620634,"Missing"
2008.jeptalnrecital-long.18,E03-1026,0,0.0235623,"Missing"
2009.jeptalnrecital-court.20,sagot-etal-2006-lefff,1,0.884228,"Missing"
2009.jeptalnrecital-court.20,2005.jeptalnrecital-long.1,0,0.0762713,"Missing"
2009.jeptalnrecital-long.23,P98-1014,0,0.0256784,"Missing"
2009.jeptalnrecital-long.23,C08-1080,1,0.68042,"Missing"
2009.jeptalnrecital-long.23,P06-1042,1,0.904283,"Missing"
2009.jeptalnrecital-long.23,2005.jeptalnrecital-long.1,1,0.828483,"Missing"
2009.jeptalnrecital-long.23,P04-1057,0,0.0785878,"Missing"
2009.jeptalnrecital-long.23,zhang-kordoni-2006-automated,0,0.0270662,"Missing"
2010.jeptalnrecital-court.15,J97-3003,0,0.462262,"Missing"
2010.jeptalnrecital-court.15,C92-1063,0,0.323032,"Missing"
2010.jeptalnrecital-court.15,N01-1024,0,0.0425629,"Missing"
2010.jeptalnrecital-court.15,I05-3005,0,0.07338,"Missing"
2010.jeptalnrecital-court.15,W08-1306,0,0.0338936,"Missing"
2010.jeptalnrecital-court.23,W09-3302,0,0.0201013,"Missing"
2010.jeptalnrecital-court.23,C96-1079,0,0.0992014,"Missing"
2010.jeptalnrecital-court.23,W03-0419,0,0.228812,"Missing"
2010.jeptalnrecital-court.5,W09-1008,0,0.0398913,"Missing"
2010.jeptalnrecital-court.5,Y09-1013,1,0.869933,"Missing"
2010.jeptalnrecital-long.3,J96-1002,0,0.0330093,"Missing"
2010.jeptalnrecital-long.3,chrupala-etal-2008-learning,0,0.052522,"Missing"
2010.jeptalnrecital-long.3,Y09-1013,1,0.876777,"Missing"
2010.jeptalnrecital-long.3,A00-2013,0,0.15949,"Missing"
2010.jeptalnrecital-long.3,W02-2018,0,0.0872838,"Missing"
2010.jeptalnrecital-long.3,2004.jeptalnrecital-long.10,0,0.0956811,"Missing"
2010.jeptalnrecital-long.3,W96-0213,0,0.640931,"Missing"
2010.jeptalnrecital-long.3,sagot-2010-lefff,1,0.824661,"Missing"
2010.jeptalnrecital-long.3,W00-1308,0,0.516959,"Missing"
2010.jeptalnrecital-long.40,Y09-1013,1,0.873968,"Missing"
2010.jeptalnrecital-long.40,francopoulo-etal-2006-lexical,0,0.0174629,"phénomène de sandhi a lieu qui induit l’insertion d’un e. B ENOÎT S AGOT, G ÉRALDINE WALTHER 6 Construction du lexique PerLex Nous avons exploité différentes sources d’informations lexicales librement disponibles, avec une importance respective qui varie d’une partie du discours à l’autre : – le corpus BijanKhan (BijanKhan, 2004; Amiri et al., 2007), un corpus annoté automatiquement en parties du discours ; – la Wikipedia Persane8 , – un lexique nominal du Persan en cours de construction par Mehdi Ghassemi à l’Université Paris-Est (Ghassemi, c.p.), – la grammaire de référence de Lazard et al. (2006), – l’introspection par des linguistes locuteurs natifs du Persan. Les entrées lexicales ont été créées en trois étapes : 1. construction d’entrées lexicales à partir des ressources ci-dessus ; 2. nettoyage des entrées obtenues ; 3. ajout manuel d’entrées manquantes à partir des formes trouvées dans le corpus BijanKhan mais non couvertes par les entrées déjà construites. Construction du lexique de base Dans un premier temps, nous avons développé et vérifié manuellement une liste de lemmes verbaux (à l’infinitif) à partir de conjugueurs en ligne librement disponibles sur internet. Nous avons as"
2010.jeptalnrecital-long.40,W04-1607,0,0.32023,"Missing"
2010.jeptalnrecital-long.40,C08-1080,1,0.900792,"Missing"
2010.jeptalnrecital-long.40,2009.mtsummit-caasl.8,0,0.267577,"Missing"
2010.jeptalnrecital-long.40,sagot-2010-lefff,1,0.884837,"Missing"
2010.jeptalnrecital-long.40,sagot-etal-2006-lefff,1,0.872582,"Missing"
2010.jeptalnrecital-long.40,sagot-walther-2010-morphological,1,0.71926,"Missing"
2010.jeptalnrecital-long.40,shamsfard-fadaee-2008-hybrid,0,0.350698,"Missing"
2011.jeptalnrecital-court.12,E09-1016,0,0.0563414,"Missing"
2011.jeptalnrecital-court.12,W04-2104,0,0.0345395,"Missing"
2011.jeptalnrecital-court.12,sagot-2010-lefff,1,0.877338,"Missing"
2011.jeptalnrecital-court.12,2002.jeptalnrecital-long.22,0,0.0518052,"Missing"
2011.jeptalnrecital-court.13,Y09-1013,1,0.886654,"Missing"
2011.jeptalnrecital-court.13,erjavec-2010-multext,0,0.0610235,"Missing"
2011.jeptalnrecital-court.13,W04-1607,0,0.0724758,"Missing"
2011.jeptalnrecital-court.13,P04-1031,0,0.0281981,"Missing"
2011.jeptalnrecital-court.13,2009.mtsummit-caasl.8,0,0.0407086,"Missing"
2011.jeptalnrecital-court.13,sagot-2010-lefff,1,0.882243,"Missing"
2011.jeptalnrecital-court.13,2010.jeptalnrecital-long.40,1,0.717571,"Missing"
2011.jeptalnrecital-court.13,shamsfard-fadaee-2008-hybrid,0,0.0240934,"Missing"
2011.jeptalnrecital-court.4,W09-3302,0,0.0289141,"Missing"
2011.jeptalnrecital-court.4,W98-1118,0,0.0597957,"Missing"
2011.jeptalnrecital-court.4,W03-0430,0,0.0612071,"Missing"
2011.jeptalnrecital-long.12,W09-3302,0,0.0214641,"Missing"
2011.jeptalnrecital-long.12,W10-1806,0,0.0208006,"Missing"
2011.jeptalnrecital-long.12,P08-1092,0,0.0245517,"Missing"
2011.jeptalnrecital-long.12,W10-0701,0,0.0271508,"Missing"
2011.jeptalnrecital-long.12,W10-0712,0,0.0271393,"Missing"
2011.jeptalnrecital-long.12,mcgraw-etal-2010-collecting,0,0.0673832,"de mise en place des garde-fous est non nul (Callison-Burch & Dredze, 2010). De même, le coût de validation (Kaisser & Lowe, 2008) ou de développement (Xu & Klakow, 2010) post-MTurk permettant de compenser la mauvaise qualité des résultats (voir section 3.4) n’est généralement pas précisément évalué. Or, ces coûts supplémentaires ne sont jamais pris en compte dans le calcul final. De plus, certaines tâches peuvent se révéler plus coûteuses que prévues. Ainsi, si l’on ne trouve pas de Turkers pour faire la tâche, on peut être obligé d’augmenter la rémunération, comme Novotney & Callison-Burch (2010), qui, partant d’un coût très bas (5 dollars de l’heure transcrite), ont été obligés de le multiplier par 7 (37 dollars de l’heure) pour transcrire du coréen, par manque de Turkers qualifiés. 3.4 MTurk permet de produire une qualité équivalente ? 3.4.1 Limitations liées à la non expertise Les Turkers étant des non experts, le Requester (fournisseur de tâches) doit découper les tâches complexes en tâches plus simples (HIT, Human Intelligence Task), afin de les rendre réalisables. Ce faisant, le chercheur est amené à faire des choix qui peuvent biaiser les résultats. Un exemple de ce type de bia"
2011.jeptalnrecital-long.12,U08-1016,0,0.0295311,"Missing"
2011.jeptalnrecital-long.12,N10-1024,0,0.0332135,"Missing"
2011.jeptalnrecital-long.12,pak-paroubek-2010-twitter,0,0.0286786,"Missing"
2011.jeptalnrecital-long.12,P05-1044,0,0.0189206,"Missing"
2011.jeptalnrecital-long.12,D08-1027,0,0.0220232,"Missing"
2011.jeptalnrecital-long.12,W07-1523,0,0.0188492,"Missing"
2011.jeptalnrecital-long.12,P10-1070,0,0.019888,"Missing"
2011.jeptalnrecital-long.12,W07-2203,0,0.0512438,"Missing"
2011.jeptalnrecital-long.12,xu-klakow-2010-paragraph,0,0.0241202,"Missing"
2011.jeptalnrecital-long.12,P95-1026,0,0.332177,"Missing"
2011.jeptalnrecital-long.12,W10-0728,0,0.0291831,"Missing"
2011.jeptalnrecital-long.23,Y96-1018,0,0.228191,"Missing"
2011.jeptalnrecital-long.23,C96-1009,0,0.14725,"Missing"
2011.jeptalnrecital-long.23,C96-2184,0,0.162304,"Missing"
2011.jeptalnrecital-long.23,P06-2056,0,0.511551,"Missing"
2011.jeptalnrecital-long.23,2009.jeptalnrecital-court.37,0,0.0553031,"Missing"
2011.jeptalnrecital-long.23,J96-3004,0,0.214246,"Missing"
2011.jeptalnrecital-long.23,I05-1009,0,0.531509,"Missing"
2011.jeptalnrecital-long.23,O03-4001,0,0.043207,"Missing"
2011.jeptalnrecital-long.23,2010.jeptalnrecital-recital.3,0,0.0379787,"Missing"
2011.jeptalnrecital-long.23,W10-4130,0,0.0224674,"Missing"
2011.jeptalnrecital-long.23,O03-4002,0,0.0975252,"Missing"
2011.jeptalnrecital-long.23,D10-1081,0,0.0486558,"Missing"
2016.jeptalnrecital-poster.16,A00-1031,0,0.453418,"Missing"
2016.jeptalnrecital-poster.16,W09-3821,0,0.034006,"Missing"
2016.jeptalnrecital-poster.16,chrupala-etal-2008-learning,0,0.05985,"Missing"
2016.jeptalnrecital-poster.16,W11-0809,0,0.0388297,"Missing"
2016.jeptalnrecital-poster.16,Y09-1013,1,0.864318,"Missing"
2016.jeptalnrecital-poster.16,2010.jeptalnrecital-long.3,1,0.786273,"Missing"
2016.jeptalnrecital-poster.16,A00-2013,0,0.250473,"Missing"
2016.jeptalnrecital-poster.16,W99-0615,0,0.248522,"Missing"
2016.jeptalnrecital-poster.16,P95-1037,0,0.0325203,"Missing"
2016.jeptalnrecital-poster.16,J94-2001,0,0.477961,"Missing"
2016.jeptalnrecital-poster.16,D13-1032,0,0.0336231,"Missing"
2016.jeptalnrecital-poster.16,N15-1055,0,0.0218581,"Missing"
2016.jeptalnrecital-poster.16,C04-1082,0,0.0353087,"Missing"
2016.jeptalnrecital-poster.16,petrov-etal-2012-universal,0,0.101936,"Missing"
2016.jeptalnrecital-poster.16,W96-0213,0,0.660387,"Missing"
2016.jeptalnrecital-poster.16,sagot-2010-lefff,1,0.876222,"Missing"
2016.jeptalnrecital-poster.16,W13-4917,0,0.0356619,"Missing"
2016.jeptalnrecital-poster.16,W00-1308,0,0.338577,"Missing"
2019.jeptalnrecital-court.12,sagot-2010-lefff,1,0.674733,"Missing"
2019.jeptalnrecital-court.12,sagot-2014-delex,1,0.86041,"Missing"
2019.jeptalnrecital-court.12,stein-2014-parsing,0,0.0440333,"Missing"
2020.acl-main.107,N16-3003,0,0.0631558,"Missing"
2020.acl-main.107,W19-4603,0,0.0224334,"s. The annotation phases are (i) Morphology/tokenization, (ii) Translation, (iii) Preannotation Syntax, (iv) Correction, (v) Final Syntax. P.M stands for person.month 9 Related Work Research on Arabic dialects is quite extensive. Space is lacking to describe it exhaustively. In relation to our work regarding North-African dialect, we refer to the work of (Samih, 2017) who along his PhD covered an large range of topics regarding the dialect spoken specifically in Morocco and generally regarding language identification (Samih et al., 2016) in code-switching scenario for various Arabic dialects (Attia et al., 2019). Unlike NArabizi dialects, the resource situation for Arabic dialects in canonical written form can hardly be qualified as scarce given the amount of resources produced by the Linguistic Data Consortium regarding these languages, see (Diab et al., 2013) for details on those corpora. These data have been extensively covered in various NLP aspects by the former members of the Columbia Arabic NLP team, among which Mona Diab, Nizar Habash, and Owen Rambow, in their respective subsequent lines of works. Many small to medium scale linguistics resources, such as morphological lexicons or bilingual d"
2020.acl-main.107,bouamor-etal-2014-multidialectal,0,0.0267007,"rces produced by the Linguistic Data Consortium regarding these languages, see (Diab et al., 2013) for details on those corpora. These data have been extensively covered in various NLP aspects by the former members of the Columbia Arabic NLP team, among which Mona Diab, Nizar Habash, and Owen Rambow, in their respective subsequent lines of works. Many small to medium scale linguistics resources, such as morphological lexicons or bilingual dictionaries have been produced (Shoufan and Alameri, 2015). Recently, in addition to the release of a small-range parallel corpus for some Arabic dialects (Bouamor et al., 2014), a larger corpus collection was released, covering 25 city dialects in the travel domain (Bouamor et al., 2018). Regarding the specific NLP modeling challenges of processing Arabic-based languages, as part of the morphologically-rich languages, recent advances in joint models have been addressed by Zalmout and Habash (2019) that recently efficiently adapted a neural architecture to perform joint word segmentation, lemmatization, morphological analysis and POS tagging on an Arabic dialect. Recent works on cross-language learning using the whole massively multilingual pre-trained language model"
2020.acl-main.107,L18-1535,0,0.0311994,"Missing"
2020.acl-main.107,K17-3026,1,0.885235,"Missing"
2020.acl-main.107,P12-3005,0,0.108063,"Missing"
2020.acl-main.107,W19-6905,0,0.0281918,"Nivre et al., 2018), namely gender, number, tense and verbal mood. Note that instead of adding lemmas, we included French glosses for two reasons: firstly for practical reasons, as they helped manual corrections done by non-native speakers of NArabizi, and secondly because of the non-formalized nature of this language, which makes lemmatization very hard, almost akin to etymological research as in the case of garjouma/the throat which can either originate from French gorge or be of Amazigh root. Code-Switching identification Unlike other works in user-generated content for minority languages (Lynn and Scannell, 2019), we do not distinguish between inter- and intra-sentential code-switching and consider word-level codemixing as lexical borrowing. We annotate code-switching at the word level with information about the source language, regardless of the canonical-ness of spelling. Syntactic Annotations Here again we follow the Universal Dependencies 2.2 annotation scheme (Nivre et al., 2018). When facing sequences of French words with regular French syntax, we followed the UD French guidelines; otherwise, we followed the UD Arabic guidelines, following the Prague Arabic Dependency UD Treebank. Translation La"
2020.acl-main.107,W16-3905,1,0.895219,"Missing"
2020.acl-main.107,L18-1008,0,0.039715,"Missing"
2020.acl-main.107,2010.amta-workshop.1,0,0.0191298,"Introduction Until the rise of fully unsupervised techniques that would free our field from its addiction to annotated data, the question of building useful data sets for under-resourced languages at a reasonable cost is still crucial. Whether the lack of labeled data originates from being a minority language status, its almost oral-only nature or simply its programmed political disappearance, geopolitical events are a factor highlighting a language deficiency in terms of natural language processing resources that can have an important societal impact. Events such as the Haïti crisis in 2010 (Munro, 2010) and the current Algerian revolts (Nossiter, 2019)1 are massively reflected on social media, yet often in languages or dialects that are poorly re1 https://www.nytimes.com/2019/03/01/world/ africa/algeria-protests-bouteflika.html sourced, namely Haitian Creole and Algerian dialectal Arabic in these cases. No readily available parsing and machine translations systems are available for such languages. Taking as an example the Arabic dialects spoken in North-Africa, mostly from Morocco to Tunisia, sometimes called Maghribi, sometimes Darija, these idioms notoriously contain various degrees of cod"
2020.acl-main.107,P02-1040,0,0.106052,"Missing"
2020.acl-main.107,W15-3208,0,0.181696,"d/ africa/algeria-protests-bouteflika.html sourced, namely Haitian Creole and Algerian dialectal Arabic in these cases. No readily available parsing and machine translations systems are available for such languages. Taking as an example the Arabic dialects spoken in North-Africa, mostly from Morocco to Tunisia, sometimes called Maghribi, sometimes Darija, these idioms notoriously contain various degrees of code-switching with languages of former colonial powers such as French, Spanish, and, to a much lesser extent, Italian, depending on the area of usage (Habash, 2010; Cotterell et al., 2014; Saadane and Habash, 2015). They share Modern Standard Arabic (MSA) as their matrix language (Myers-Scotton, 1993), and of course present a rich morphology. In conjunction with the resource scarcity issue, the codeswitching variability displayed by these languages challenges most standard NLP pipelines, if not all. What makes these dialects especially interesting is their widespread use in user-generated content found on social media platforms, where they are generally written using a romanized version of the Arabic script, called Arabizi, which is neither standardized nor formalized. The absence of standardization for"
2020.acl-main.107,W16-5806,0,0.0131401,"4 7 3 5th p.m Costs (ke) 6 6 15 7 6 28 45 21 21 87 Table 8: Treebanking costs. The annotation phases are (i) Morphology/tokenization, (ii) Translation, (iii) Preannotation Syntax, (iv) Correction, (v) Final Syntax. P.M stands for person.month 9 Related Work Research on Arabic dialects is quite extensive. Space is lacking to describe it exhaustively. In relation to our work regarding North-African dialect, we refer to the work of (Samih, 2017) who along his PhD covered an large range of topics regarding the dialect spoken specifically in Morocco and generally regarding language identification (Samih et al., 2016) in code-switching scenario for various Arabic dialects (Attia et al., 2019). Unlike NArabizi dialects, the resource situation for Arabic dialects in canonical written form can hardly be qualified as scarce given the amount of resources produced by the Linguistic Data Consortium regarding these languages, see (Diab et al., 2013) for details on those corpora. These data have been extensively covered in various NLP aspects by the former members of the Columbia Arabic NLP team, among which Mona Diab, Nizar Habash, and Owen Rambow, in their respective subsequent lines of works. Many small to mediu"
2020.acl-main.107,C12-1149,1,0.637917,"iven language identification models to extract NArabizi samples among the whole collection of the Common-Crawl-based OSCAR corpora (Ortiz Suárez et al., 2019) as well as 2 millions sentences of additional crawled webdata, resulting in 50k NArabizi sentences of high quality, to date the largest corpus of this language. This makes this collection a valuable test bed for low-resource NLP research. 3.2 Annotation Layers Our NArabizi treebank contains 5 annotations layers: (i) tokenization, (ii) morphology, (iii) codeswitching identification, (iv) syntax and (v) translation. Tokenization Following Seddah et al. (2012) and their work on the French Social Media Bank, we decided to apply a light tokenization process where we manually tokenized only the obvious cases of wrongly detached punctuations and “missing whitespaces” (i.e. cases where two words are 4 https://github.com/ryancotterell/arabic_ dialect_annotation contracted into one token).5 Morphological Analysis This layer consists of two sets of part-of-speech tags, one following the Universal POS tagset (Petrov et al., 2011) and the other the FTB-cc tagset extended to deal with user-generated content (Seddah et al., 2012). In cases of word contractions"
2020.acl-main.107,W15-3205,0,0.0284269,"resource situation for Arabic dialects in canonical written form can hardly be qualified as scarce given the amount of resources produced by the Linguistic Data Consortium regarding these languages, see (Diab et al., 2013) for details on those corpora. These data have been extensively covered in various NLP aspects by the former members of the Columbia Arabic NLP team, among which Mona Diab, Nizar Habash, and Owen Rambow, in their respective subsequent lines of works. Many small to medium scale linguistics resources, such as morphological lexicons or bilingual dictionaries have been produced (Shoufan and Alameri, 2015). Recently, in addition to the release of a small-range parallel corpus for some Arabic dialects (Bouamor et al., 2014), a larger corpus collection was released, covering 25 city dialects in the travel domain (Bouamor et al., 2018). Regarding the specific NLP modeling challenges of processing Arabic-based languages, as part of the morphologically-rich languages, recent advances in joint models have been addressed by Zalmout and Habash (2019) that recently efficiently adapted a neural architecture to perform joint word segmentation, lemmatization, morphological analysis and POS tagging on an Ar"
2020.acl-main.107,K17-3009,0,0.0225523,"ization systems have better performances (Belinkov and Glass, 2015) but are either not maintained with the proper python packages, or come with a fee. 10 Theh, U+062B. Dev OOV 78.74 88.10 72.61 32.28 55.85 70.04 40.94 all Test OOV 80.37 87.17 73.87 32.75 57.42 69.12 43.50 Table 5: POS tagging results. 6.2 Early Parsing experiments As stated earlier in this paper, NArabizi contains a high-level of code-switching with French and is closely related to MSA. We described in Section 5 how we built a mixed treebank based on the 11 Note that we performed a set of baseline experiments with UDPipe 2.0 (Straka and Straková, 2017) as well on a previous version of this data set. It reached only 73.7 of UPOS on the test set. 12 https://github.com/VowpalWabbit/vowpal_ wabbit/wiki 13 Without this endogenous lexicon extraction step, the tagger performed slightly worse, although the difference is small. 1144 French GSD UD treebank and our Arabizi version of the Prague Arabic Dependency Treebank. We trained the UDPipe parser (Straka and Straková, 2017) on various treebanks obtained by combining different proportions of the French GSD and our PADT-based pseudo-Arabizi treebank. We ran these parsers with already annotated gold"
2020.acl-main.107,W10-1401,1,0.7322,"ances exhibit so much variations that they are not mutually understandable across geographically distant regions. As space is missing for an exhaustive description of Arabic language variations, we refer the reader to Habash (2010), Samih (2017) and especially to Saadane and Habash (2015) for a thorough account of Algerian dialectal Arabic, which is the focus of this work. In short, the key properties of North-African dialectal Arabic are: • It is a Semitic language, non codified, mostly spoken; • It has a rich-inflexion system, which qualifies this dialect as a morphologically-rich language (Tsarfaty et al., 2010), even though Saadane and Habash (2015) write that many properties present in Classical Arabic are absent from this dialect (e.g. it has simplified nominal and verbal case systems); • It displays a high degree of variability at all levels: spelling and transliteration conventions, phonology, morphology, lexicon; • It exhibits a high degree of code-switching; due to historical reasons and cultural influence of French in the media circles, the Algerian dialect, as well as Tunisian and Morocco, is known for its heavy use of French words. 2 http://almanach-treebanks.fr/NArabizi Gloss Attested form"
2020.acl-main.156,C18-1139,0,0.0272196,"and its surrounding context, so that a single word can have multiple representations, each one depending on how it is used. In practice, most fixed embeddings are used as feature-based models. The most notable examples are word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Mikolov et al., 2018). All of them are extensively used in a variety of applications nowadays. On the other hand, contextualized word representations and language models have been developed using both featurebased architectures, the most notable examples being ELMo and Flair (Peters et al., 2018; Akbik et al., 2018), and transformer based architectures, that are commonly used in a fine-tune setting, as is the case of GPT-1, GPT-2 (Radford et al., 2018, 2019), BERT and its derivatives (Devlin et al., 2018; Liu et al., 2019; Lan et al., 2019) and more recently T5 (Raffel et al., 2019). All of them have repeatedly improved the state-of-the art in many downstream NLP tasks over the last year. In general, the main advantage of using language models is that they are mostly built in an unsupervised manner and they can be trained with raw, unannotated plain text. Their main drawback is that enormous quantities o"
2020.acl-main.156,W13-3520,0,0.188146,"main drawback is that enormous quantities of data seem to be required to properly train them especially in the case of contextualized models, for which larger corpora are thought to be needed to properly address polysemy and cover the wide range of uses that commonly exist within languages. For gathering data in a wide range of languages, 1703 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1703–1714 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Wikipedia is a commonly used option. It has been used to train fixed embeddings (Al-Rfou et al., 2013; Bojanowski et al., 2017) and more recently the multilingual BERT (Devlin et al., 2018), hereafter mBERT. However, for some languages, Wikipedia might not be large enough to train good quality contextualized word embeddings. Moreover, Wikipedia data all belong to the same specific genre and style. To address this problem, one can resort to crawled text from the internet; the largest and most widespread dataset of crawled text being Common Crawl.1 Such an approach generally solves the quantity and genre/style coverage problems but might introduce noise in the data, an issue which has earned th"
2020.acl-main.156,P18-1246,0,0.0456448,"Missing"
2020.acl-main.156,L18-1550,0,0.301063,"ng Common Crawl also leads to data management challenges as the corpus is distributed in the form of a large set of plain text each containing a large quantity of unclassified multilingual documents from different websites. In this paper we study the trade-off between quantity and quality of data for training contextualized representations. To this end, we use the OSCAR corpus (Ortiz Suárez et al., 2019), a freely available2 multilingual dataset obtained by performing language classification, filtering and cleaning of the whole Common Crawl corpus.3 OSCAR was created following the approach of Grave et al. (2018) but proposing a simple improvement on their filtering method. We then train OSCARbased and Wikipedia-based ELMo contextualized word embeddings (Peters et al., 2018) for 5 languages: Bulgarian, Catalan, Danish, Finnish and Indonesian. We evaluate the models by attaching them to the to UDPipe 2.0 architecture (Straka, 2018; Straka et al., 2019) for dependency parsing and part-of-speech (POS) tagging. We show that the models using the OSCAR-based ELMo embeddings consistently outperform the Wikipediabased ones, suggesting that big high-coverage noisy corpora might be better than small high-qualit"
2020.acl-main.156,P82-1020,0,0.523599,"Missing"
2020.acl-main.156,Q17-1010,0,0.801985,"enormous quantities of data seem to be required to properly train them especially in the case of contextualized models, for which larger corpora are thought to be needed to properly address polysemy and cover the wide range of uses that commonly exist within languages. For gathering data in a wide range of languages, 1703 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1703–1714 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Wikipedia is a commonly used option. It has been used to train fixed embeddings (Al-Rfou et al., 2013; Bojanowski et al., 2017) and more recently the multilingual BERT (Devlin et al., 2018), hereafter mBERT. However, for some languages, Wikipedia might not be large enough to train good quality contextualized word embeddings. Moreover, Wikipedia data all belong to the same specific genre and style. To address this problem, one can resort to crawled text from the internet; the largest and most widespread dataset of crawled text being Common Crawl.1 Such an approach generally solves the quantity and genre/style coverage problems but might introduce noise in the data, an issue which has earned the corpus some criticism, m"
2020.acl-main.156,E17-2068,0,0.10044,"Missing"
2020.acl-main.156,K18-2005,0,0.0774715,"antly increased the performance of the embeddings especially for mid- to low-resource languages. Regarding contextualized models, the most notable non-English contribution has been that of the mBERT (Devlin et al., 2018), which is distributed as (i) a single multilingual model for 100 different languages trained on Wikipedia data, and as (ii) a single multilingual model for both Simplified and Traditional Chinese. Four monolingual fully trained ELMo models have been distributed for Japanese, Portuguese, German and Basque5 ; 44 monolingual ELMo models6 where also released by the HIT-SCIR team (Che et al., 2018) during the CoNLL 2018 Shared Task (Zeman et al., 2018), but their training sets where capped at 20 million words. A German BERT (Chan et al., 2019) as well as a French BERT model (called CamemBERT) (Martin et al., 2019) have also been released. In general no particular effort in creating a set of highquality monolingual contextualized representations has been shown yet, or at least not on a scale that 5 https://allennlp.org/elmo https://github.com/HIT-SCIR/ ELMoForManyLangs 1704 6 is comparable with what was done for fixed word embeddings. For dependency parsing and POS tagging the most notab"
2020.acl-main.156,2021.ccl-1.108,0,0.102417,"Missing"
2020.acl-main.156,2020.acl-main.645,1,0.850749,"Missing"
2020.acl-main.156,K17-3002,0,0.019878,"ffort in creating a set of highquality monolingual contextualized representations has been shown yet, or at least not on a scale that 5 https://allennlp.org/elmo https://github.com/HIT-SCIR/ ELMoForManyLangs 1704 6 is comparable with what was done for fixed word embeddings. For dependency parsing and POS tagging the most notable non-English specific contribution is that of the CoNLL 2018 Shared Task (Zeman et al., 2018), where the 1st place (LAS Ranking) was awarded to the HIT-SCIR team (Che et al., 2018) who used Dozat and Manning (2017)’s Deep Biaffine parser and its extension described in (Dozat et al., 2017), coupled with deep contextualized ELMo embeddings (Peters et al., 2018) (capping the training set at 20 million words). The 1st place in universal POS tagging was awarded to Smith et al. (2018) who used two separate instances of Bohnet et al. (2018)’s tagger. More recent developments in POS tagging and parsing include those of Straka et al. (2019) which couples another CoNLL 2018 shared task participant, UDPipe 2.0 (Straka, 2018), with mBERT greatly improving the scores of the original model, and UDify (Kondratyuk and Straka, 2019), which adds an extra attention layer on top of mBERT plus a D"
2020.acl-main.156,L18-1008,0,0.0299702,"k-specific parameters, and instead copy the weights from a pre-trained Embeddings or language models can be divided into fixed, meaning that they generate a single representation for each word in the vocabulary; and contextualized, meaning that a representation is generated based on both the word and its surrounding context, so that a single word can have multiple representations, each one depending on how it is used. In practice, most fixed embeddings are used as feature-based models. The most notable examples are word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Mikolov et al., 2018). All of them are extensively used in a variety of applications nowadays. On the other hand, contextualized word representations and language models have been developed using both featurebased architectures, the most notable examples being ELMo and Flair (Peters et al., 2018; Akbik et al., 2018), and transformer based architectures, that are commonly used in a fine-tune setting, as is the case of GPT-1, GPT-2 (Radford et al., 2018, 2019), BERT and its derivatives (Devlin et al., 2018; Liu et al., 2019; Lan et al., 2019) and more recently T5 (Raffel et al., 2019). All of them have repeatedly im"
2020.acl-main.156,D14-1162,0,0.083274,"which introduce a minimal number of task-specific parameters, and instead copy the weights from a pre-trained Embeddings or language models can be divided into fixed, meaning that they generate a single representation for each word in the vocabulary; and contextualized, meaning that a representation is generated based on both the word and its surrounding context, so that a single word can have multiple representations, each one depending on how it is used. In practice, most fixed embeddings are used as feature-based models. The most notable examples are word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Mikolov et al., 2018). All of them are extensively used in a variety of applications nowadays. On the other hand, contextualized word representations and language models have been developed using both featurebased architectures, the most notable examples being ELMo and Flair (Peters et al., 2018; Akbik et al., 2018), and transformer based architectures, that are commonly used in a fine-tune setting, as is the case of GPT-1, GPT-2 (Radford et al., 2018, 2019), BERT and its derivatives (Devlin et al., 2018; Liu et al., 2019; Lan et al., 2019) and more recently T5 (Raffel et al., 2"
2020.acl-main.156,N18-1202,0,0.498403,"sed on both the word and its surrounding context, so that a single word can have multiple representations, each one depending on how it is used. In practice, most fixed embeddings are used as feature-based models. The most notable examples are word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and fastText (Mikolov et al., 2018). All of them are extensively used in a variety of applications nowadays. On the other hand, contextualized word representations and language models have been developed using both featurebased architectures, the most notable examples being ELMo and Flair (Peters et al., 2018; Akbik et al., 2018), and transformer based architectures, that are commonly used in a fine-tune setting, as is the case of GPT-1, GPT-2 (Radford et al., 2018, 2019), BERT and its derivatives (Devlin et al., 2018; Liu et al., 2019; Lan et al., 2019) and more recently T5 (Raffel et al., 2019). All of them have repeatedly improved the state-of-the art in many downstream NLP tasks over the last year. In general, the main advantage of using language models is that they are mostly built in an unsupervised manner and they can be trained with raw, unannotated plain text. Their main drawback is that"
2020.acl-main.156,petrov-etal-2012-universal,0,0.0544696,"Ms, with Softmax classifiers on top, which process its output and generate UPOS, XPOS, UFeats and Lemmas. The lemma classifier also takes the character-level word embeddings as input. • The parser-specific bidirectional LSTM layer, whose output is then passed to a bi-affine attention layer (Dozat and Manning, 2017) producing labeled dependency trees. 4.3 Treebanks To train the selected parser and tagger (cf. Section 4.2) and evaluate the pre-trained language models in our 5 languages, we run our experiments using the Universal Dependencies (UD)14 paradigm and its corresponding UD POS tag set (Petrov et al., 2012). We use all the treebanks available for our five languages in the UD treebank collection version 2.2 (Nivre et al., 2018), which was used for the CoNLL 2018 shared task, thus we perform our evaluation tasks in 6 different treebanks (see Table 4 for treebank size information). After the CoNLL 2018 Shared Task, the UDPipe 2.0 authors added the option to concatenate contextualized representations to the embedding 12 https://github.com/allenai/bilm-tf/ issues/135 13 https://github.com/CoNLL-UD-2018/ UDPipe-Future • Bulgarian BTB: Created at the Institute of Information and Communication Technolog"
2020.acl-main.156,K18-2011,0,0.0254792,"Missing"
2020.acl-main.156,P10-1013,0,0.0096547,"n articles in 301 different languages. Because articles are curated by language and written in an Language Size Bulgarian Catalan Danish Finnish Indonesian 609M 1.1G 338M 669M 488M #Ktokens #Kwords #Ksentences 64,190 211,627 60,644 89,580 80,809 54,748 179,108 52,538 76,035 68,955 3,685 8,293 3,226 6,847 4,298 Table 1: Size of Wikipedia corpora, measured in bytes, thousands of tokens, words and sentences. open collaboration model, its text tends to be of very high-quality in comparison to other free online resources. This is why Wikipedia has been extensively used in various NLP applications (Wu and Weld, 2010; Mihalcea, 2007; Al-Rfou et al., 2013; Bojanowski et al., 2017). We downloaded the XML Wikipedia dumps7 and extracted the plaintext from them using the wikiextractor.py script8 from Giuseppe Attardi. We present the number of words and tokens available for each of our 5 languages in Table 1. We decided against deduplicating the Wikipedia data as the corpora are already quite small. We tokenize the 5 corpora using UDPipe (Straka and Straková, 2017). 3.2 OSCAR Common Crawl is a non-profit organization that produces and maintains an open, freely available repository of crawled data from the web."
2020.acl-main.156,K18-2001,0,0.0265257,"Missing"
2020.acl-main.156,K17-3009,0,0.029716,"t tends to be of very high-quality in comparison to other free online resources. This is why Wikipedia has been extensively used in various NLP applications (Wu and Weld, 2010; Mihalcea, 2007; Al-Rfou et al., 2013; Bojanowski et al., 2017). We downloaded the XML Wikipedia dumps7 and extracted the plaintext from them using the wikiextractor.py script8 from Giuseppe Attardi. We present the number of words and tokens available for each of our 5 languages in Table 1. We decided against deduplicating the Wikipedia data as the corpora are already quite small. We tokenize the 5 corpora using UDPipe (Straka and Straková, 2017). 3.2 OSCAR Common Crawl is a non-profit organization that produces and maintains an open, freely available repository of crawled data from the web. Common Crawl’s complete archive consists of petabytes of monthly snapshots collected since 2011. Common Crawl snapshots are not classified by language, and contain a certain level of noise (e.g. one-word “sentences” such as “OK” and “Cancel” are unsurprisingly very frequent). This is what motivated the creation of the freely available multilingual OSCAR corpus (Ortiz Suárez et al., 2019), extracted from the November 2018 snapshot, which amounts to"
2020.acl-main.156,P19-1355,0,0.0476506,"that all the UDPipe 2.0 + ELMoOSCAR(1) perform better than the UDPipe 2.0 + ELMoWikipedia(1) models across all metrics. Thus we believe that talking in terms of training steps as opposed to training epochs might be a more transparent way of comparing two pretrained models. 5.3 Computational cost and carbon footprint Considering the discussion above, we believe an interesting follow-up to our experiments would be training the ELMo models for more of the languages included in the OSCAR corpus. However training ELMo is computationally costly, and one way to estimate this cost, as pointed out by Strubell et al. (2019), is by using the training times of each model to compute both power consumption and CO2 emissions. In our set-up we used two different machines, each one having 4 NVIDIA GeForce GTX 1080 Ti graphic cards and 128GB of RAM, the difference between the machines being that one uses a single Intel Xeon Gold 5118 processor, while the other uses two Intel Xeon E5-2630 v4 processors. One GeForce GTX 1080 Ti card is rated at around Language Hours Days KWh·PUE CO2 e OSCAR-Based ELMos Bulgarian 1183 515.00 Catalan 1118 199.98 Danish 1183 200.89 Finnish 1118 591.25 Indonesian 1183 694.26 Power 21.45 8.33"
2020.acl-main.156,taule-etal-2008-ancora,0,0.0184647,"different treebanks (see Table 4 for treebank size information). After the CoNLL 2018 Shared Task, the UDPipe 2.0 authors added the option to concatenate contextualized representations to the embedding 12 https://github.com/allenai/bilm-tf/ issues/135 13 https://github.com/CoNLL-UD-2018/ UDPipe-Future • Bulgarian BTB: Created at the Institute of Information and Communication Technologies, Bulgarian Academy of Sciences, it consists of legal documents, news articles and fiction pieces. 14 1707 https://universaldependencies.org • Catalan-AnCora: Built on top of the SpanishCatalan AnCora corpus (Taulé et al., 2008), it contains mainly news articles. • Danish-DDT: Converted from the Danish Dependency Treebank (Buch-Kromann, 2003). It includes news articles, fiction and non fiction texts and oral transcriptions. • Finnish-FTB: Consists of manually annotated grammatical examples from VISK15 (The Web Version of the Large Grammar of Finnish). • Finnish-TDT: Based on the Turku Dependency Treebank (TDT). Contains texts from Wikipedia, Wikinews, news articles, blog entries, magazine articles, grammar examples, Europarl speeches, legal texts and fiction. Treebank Model UPOS UAS LAS Bulgarian BTB UDify UDPipe 2.0"
2020.acl-main.424,I17-1030,1,0.922576,"Missing"
2020.acl-main.424,D19-3009,1,0.86211,"ms added, deleted and kept by the simplification system. It does so by comparing the output of the simplification model to multiple references and the original sentence, using both precision and recall. BLEU has shown positive correlation with human judgements of grammaticality and meaning preserˇ vation (Stajner et al., 2014; Wubben et al., 2012; Xu et al., 2016), while SARI has high correlation with judgements of simplicity gain (Xu et al., 2016). In our experiments, we used the implementations of these metrics available in the EASSE package for automatic sentence simplification evaluation (Alva-Manchego et al., 2019).5 We computed all the scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible, 5 https://github.com/feralvam/easse System Outputs. We used publicly-available simplifications produced by automatic SS systems: PBSMT-R (Wubben et al., 2012), which is a phrase-based MT model; Hybrid (Narayan and Gardent, 2014), which uses phrase-based MT coupled with semantic analysis; SBSMT-SARI (Xu et al., 2016), which relies on syn"
2020.acl-main.424,2020.cl-1.4,1,0.846425,"experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed. 1 Introduction Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and ∗ Equal Contribution Branco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e."
2020.acl-main.424,W19-5301,0,0.0593597,"Missing"
2020.acl-main.424,D18-1289,0,0.0383172,"Missing"
2020.acl-main.424,C96-2183,0,0.822657,"T, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed. 1 Introduction Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and ∗ Equal Contribution Branco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; Alu´ısio et al., 2008; Bott and Saggion, 2011). However, models for automatic SS are evaluated on datasets whose simplifications are not representative of this va"
2020.acl-main.424,P11-2117,0,0.0921157,"ese studies, it can be argued that the scope of rewriting transformations involved in the simplification process goes beyond only replacing words with simpler synonyms. In fact, human perception of complexity is most affected by syntactic features related to sentence structure (Brunato et al., 2018). Therefore, since human editors make several changes to both the lexical content and syntactic structure of sentences when simplifying them, we should expect that models for automatic sentence simplification can also make such changes. Evaluation Data for SS Most datasets for SS (Zhu et al., 2010; Coster and Kauchak, 2011; Hwang et al., 2015) consist of automatic sentence alignments between related articles in English Wikipedia (EW) and Simple English Wikipedia (SEW). In SEW, contributors are asked to write texts using simpler language, such as by shortening sentences or by using words from Basic English (Ogden, 1930). However, Yasseri et al. (2012) found that the syntactic complexity of sentences in SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with only a few words replaced or"
2020.acl-main.424,W14-1215,0,0.239238,"ivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed. 1 Introduction Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and ∗ Equal Contribution Branco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; Alu´ısio et al., 2008; Bott and Saggion, 2011). However, models for automat"
2020.acl-main.424,W13-2305,0,0.114853,"to submit their level of agreement (0: Strongly disagree, 100: Strongly agree) with the following statements: 1. The Simplified sentence adequately expresses the meaning of the Original, perhaps omitting the least important information. 2. The Simplified sentence is fluent, there are no grammatical errors. 3. The Simplified sentence is easier to understand than the Original sentence. Using continuous scales when crowdsourcing human evaluations is common practice in Machine Translation (Bojar et al., 2018; Barrault et al., 2019), since it results in higher levels of interannotator consistency (Graham et al., 2013). The six sentence pairs for the Rating QT consisted of: • Three submissions to the Annotation QT, manually selected so that one contains splitting, one has a medium level of compression, and one contains grammatical and spelling mistakes. These allowed to check that the particular characteristics of each sentence pair affect the corresponding evaluation criteria. • One sentence pair from WikiLarge where the Original and the Simplification had no relation to each other. This served to check the attention level of the worker. All submitted ratings were manually reviewed to validate the quality"
2020.acl-main.424,N15-1022,0,0.0552503,"ued that the scope of rewriting transformations involved in the simplification process goes beyond only replacing words with simpler synonyms. In fact, human perception of complexity is most affected by syntactic features related to sentence structure (Brunato et al., 2018). Therefore, since human editors make several changes to both the lexical content and syntactic structure of sentences when simplifying them, we should expect that models for automatic sentence simplification can also make such changes. Evaluation Data for SS Most datasets for SS (Zhu et al., 2010; Coster and Kauchak, 2011; Hwang et al., 2015) consist of automatic sentence alignments between related articles in English Wikipedia (EW) and Simple English Wikipedia (SEW). In SEW, contributors are asked to write texts using simpler language, such as by shortening sentences or by using words from Basic English (Ogden, 1930). However, Yasseri et al. (2012) found that the syntactic complexity of sentences in SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with only a few words replaced or dropped and the rest"
2020.acl-main.424,W02-0109,0,0.0656209,"table) have been shown to be best indicators of word complexity (Paetzold and Specia, 2016). The ratio is then the value of this score on the simplification divided by that of the original sentence. In order to quantify the rewriting transformations, we computed several low-level features for all simplification instances using the tseval package (Martin et al., 2018): • Number of sentence splits: Corresponds to the difference between the number of sentences in the simplification and the number of sentences in the original sentence. In tseval, the number of sentences is calculated using NLTK (Loper and Bird, 2002). • Compression level: Number of characters in the simplification divided by the number of characters in the original sentence. • Dependency tree depth ratio: We compute the ratio of the depth of the dependency parse tree of the simplification relative to that of the original sentence. When a simplification is composed by more than one sentence, we choose the maximum depth of all dependency trees. Parsing is performed using spaCy.4 This feature serves as a proxy to measure improvements in structural simplicity. • Replace-only Levenshtein distance: Computed as the normalised character-level Lev"
2020.acl-main.424,W18-7005,1,0.877343,"Missing"
2020.acl-main.424,P14-1041,0,0.0247229,"mentations of these metrics available in the EASSE package for automatic sentence simplification evaluation (Alva-Manchego et al., 2019).5 We computed all the scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible, 5 https://github.com/feralvam/easse System Outputs. We used publicly-available simplifications produced by automatic SS systems: PBSMT-R (Wubben et al., 2012), which is a phrase-based MT model; Hybrid (Narayan and Gardent, 2014), which uses phrase-based MT coupled with semantic analysis; SBSMT-SARI (Xu et al., 2016), which relies on syntax-based MT; NTSSARI (Nisioi et al., 2017), a neural sequence-tosequence model with a standard encoder-decoder architecture; and ACCESS (Martin et al., 2020), an encoder-decoder architecture conditioned on explicit attributes of sentence simplification. Collection of Human Ratings. We randomly chose 100 original sentences from ASSET and, for each of them, we sampled one system simplification. The automatic simplifications were selected so that the distribution of simplification transf"
2020.acl-main.424,P17-2014,0,0.10479,"scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible, 5 https://github.com/feralvam/easse System Outputs. We used publicly-available simplifications produced by automatic SS systems: PBSMT-R (Wubben et al., 2012), which is a phrase-based MT model; Hybrid (Narayan and Gardent, 2014), which uses phrase-based MT coupled with semantic analysis; SBSMT-SARI (Xu et al., 2016), which relies on syntax-based MT; NTSSARI (Nisioi et al., 2017), a neural sequence-tosequence model with a standard encoder-decoder architecture; and ACCESS (Martin et al., 2020), an encoder-decoder architecture conditioned on explicit attributes of sentence simplification. Collection of Human Ratings. We randomly chose 100 original sentences from ASSET and, for each of them, we sampled one system simplification. The automatic simplifications were selected so that the distribution of simplification transformations (e.g. sentence splitting, compression, paraphrases) would match that from human simplifications in ASSET. That was done so that we could obtain"
2020.acl-main.424,C16-1069,1,0.880195,"racteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed. 1 Introduction Sentence Simplification (SS) consists in modifying the content and structure of a sentence to make it easier to understand, while retaining its main idea and most of its original meaning (Alva-Manchego et al., 2020). Simplified texts can benefit non-native speakers (Paetzold, 2016), people suffering from aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) or autism (Evans et al., 2014). They also help language processing tasks, such as parsing (Chandrasekar et al., 1996), summarisation (Silveira and ∗ Equal Contribution Branco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the se"
2020.acl-main.424,P02-1040,0,0.116616,"of the Association for Computational Linguistics, pages 4668–4679 c July 5 - 10, 2020. 2020 Association for Computational Linguistics original sentence). Simplifications in ASSET were collected via crowdsourcing (§ 3), and encompass a variety of rewriting transformations (§ 4), which make them simpler than those in TurkCorpus and HSplit (§ 5), thus providing an additional suitable benchmark for comparing and evaluating automatic SS models. In addition, we study the applicability of standard metrics for evaluating SS using simplifications in ASSET as references (§ 6). We analyse whether BLEU (Papineni et al., 2002) or SARI (Xu et al., 2016) scores correlate with human judgements of fluency, adequacy and simplicity, and find that neither of the metrics shows a strong correlation with simplicity ratings. This motivates the need for developing better metrics for assessing SS when multiple rewriting transformations are performed. We make the following contributions: • A high quality large dataset for tuning and evaluation of SS models containing simplifications produced by applying multiple rewriting transformations.1 • An analysis of the characteristics of the dataset that turn it into a new suitable bench"
2020.acl-main.424,Q16-1005,0,0.027315,"mplification), we crowdsourced 15 human ratings on fluency (i.e. grammaticality), adequacy (i.e. meaning preservation) and simplicity, using the same worker selection criteria and HIT design of the Qualification Test as in § 5.1. 6.2 Inter-Annotator Agreement We followed the process suggested in (Graham et al., 2013). First, we normalised the scores of each rater by their individual mean and standard deviation, which helps eliminate individual judge preferences. Then, the normalised continuous scores were converted to five interval categories using equally spaced bins. After that, we followed Pavlick and Tetreault (2016) and computed quadratic weighted Cohen’s κ (Cohen, 1968) simulating two raters: for each sentence, we chose one worker’s rating as the category for annotator A, and selected the rounded average scores for the remaining workers as the category for annotator B. We then computed κ for this pair over the whole dataset. We repeated the process 1,000 times to compute the mean and variance of κ. The resulting values are: 0.687 ± 0.028 for Fluency, 0.686 ± 0.030 for Meaning and 0.628 ± 0.032 for Simplicity. All values point to a moderate level 4675 Metric References BLEU ASSET TurkCorpus ASSET TurkCor"
2020.acl-main.424,W14-1210,0,0.0497303,"litting the sentences. This prevents evaluating a model’s ability to perform a more diverse set of rewriting transformations when simplifying sentences. HSplit (Sulem et al., 2018a), on the other hand, provides simplifications involving only splitting for sentences in the test set of TurkCorpus. We build on TurkCorpus and HSplit by collecting a dataset that provides several manuallyproduced simplifications involving multiple types of rewriting transformations. 2.3 Crowdsourcing Manual Simplifications A few projects have been carried out to collect manual simplifications through crowdsourcing. Pellow and Eskenazi (2014a) built a corpus of everyday documents (e.g. driving test preparation materials), and analysed the feasibly of crowdsourcing their sentence-level simplifications. Of all the quality control measures taken, the most successful was providing a training session to workers, since it allowed to block spammers and those without the skills to perform the task. Additionally, they proposed to use workers’ self-reported confidence scores to flag submissions that could be discarded or reviewed. Later on, Pellow and Eskenazi (2014b) presented a preliminary study on producing simplifications through a col"
2020.acl-main.424,L18-1685,1,0.858597,"uld be discarded or reviewed. Later on, Pellow and Eskenazi (2014b) presented a preliminary study on producing simplifications through a collaborative process. Groups of four workers were assigned one sentence to simplify, and they had to discuss and agree on the process to perform it. Unfortunately, the data collected in these studies is no longer publicly available. Simplifications in TurkCorpus were also collected through crowdsourcing. Regarding the methodology followed, Xu et al. (2016) only report removing bad workers after manual check of their first several submissions. More recently, Scarton et al. (2018) used volunteers to collect simplifications for SimPA, a dataset with sentences from the Public Administration domain. One particular characteristic of the methodology followed is that lexical and syntactic simplifications were performed independently. 3 Creating ASSET We extended TurkCorpus (Xu et al., 2016) by using the same original sentences, but crowdsourced manual simplifications that encompass a richer set of rewriting transformations. Since TurkCorpus was adopted as the standard dataset for evaluating SS models, several system outputs on this data are already publicly available (Zhang"
2020.acl-main.424,D18-1081,0,0.370088,"Missing"
2020.acl-main.424,N18-1063,0,0.0727483,"Missing"
2020.acl-main.424,L18-1615,0,0.026282,"SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with only a few words replaced or dropped and the rest of the sentences left unchanged. More diverse simplifications are available in the Newsela corpus (Xu et al., 2015), a dataset of 1,130 news articles that were each manually simplified 4669 to up to 5 levels of simplicity. The parallel articles can be automatically aligned at the sentence level to train and test simplification models (Alvaˇ Manchego et al., 2017; Stajner et al., 2018). However, the Newsela corpus can only be accessed after signing a restrictive license that prevents publicly sharing train/test splits of the dataset, which impedes reproducibility. Evaluating models on automatically-aligned sentences is problematic. Even more so if only one (potentially noisy) reference simplification for each original sentence is available. With this concern in mind, Xu et al. (2016) collected the TurkCorpus, a dataset with 2,359 original sentences from EW, each with 8 manual reference simplifications. The dataset is divided into two subsets: 2,000 sentences for validation"
2020.acl-main.424,C10-1152,0,0.326813,"nces). From all these studies, it can be argued that the scope of rewriting transformations involved in the simplification process goes beyond only replacing words with simpler synonyms. In fact, human perception of complexity is most affected by syntactic features related to sentence structure (Brunato et al., 2018). Therefore, since human editors make several changes to both the lexical content and syntactic structure of sentences when simplifying them, we should expect that models for automatic sentence simplification can also make such changes. Evaluation Data for SS Most datasets for SS (Zhu et al., 2010; Coster and Kauchak, 2011; Hwang et al., 2015) consist of automatic sentence alignments between related articles in English Wikipedia (EW) and Simple English Wikipedia (SEW). In SEW, contributors are asked to write texts using simpler language, such as by shortening sentences or by using words from Basic English (Ogden, 1930). However, Yasseri et al. (2012) found that the syntactic complexity of sentences in SEW is almost the same as in EW. In addition, Xu et al. (2015) determined that automaticallyaligned simple sentences are sometimes just as complex as their original counterparts, with onl"
2020.acl-main.424,W14-1201,0,0.016775,"utputs: BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016). BLEU is a precision-oriented metric that relies on the number of n-grams in the output that match n-grams in the references, independently of position. SARI measures improvement in the simplicity of a sentence based on the n-grams added, deleted and kept by the simplification system. It does so by comparing the output of the simplification model to multiple references and the original sentence, using both precision and recall. BLEU has shown positive correlation with human judgements of grammaticality and meaning preserˇ vation (Stajner et al., 2014; Wubben et al., 2012; Xu et al., 2016), while SARI has high correlation with judgements of simplicity gain (Xu et al., 2016). In our experiments, we used the implementations of these metrics available in the EASSE package for automatic sentence simplification evaluation (Alva-Manchego et al., 2019).5 We computed all the scores at sentence-level as in the experiment by Xu et al. (2016), where they compared sentencelevel correlations of FKGL, BLEU and SARI with human ratings. We used a smoothed sentence-level version of BLEU so that comparison is possible, 5 https://github.com/feralvam/easse Sy"
2020.acl-main.424,P12-1107,0,0.313438,"Missing"
2020.acl-main.424,Q15-1021,0,0.263892,"syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; Alu´ısio et al., 2008; Bott and Saggion, 2011). However, models for automatic SS are evaluated on datasets whose simplifications are not representative of this variety of transformations. For instance, TurkCorpus (Xu et al., 2016), a standard dataset for assessment in SS, contains simplifications produced mostly by lexical paraphrasing, while reference simplifications in HSplit (Sulem et al., 2018a) focus on splitting sentences. The Newsela corpus (Xu et al., 2015) contains simplifications produced by professionals applying multiple rewriting transformations, but sentence alignments are automatically computed and thus imperfect, and its data can only be accessed after signing a restrictive publicsharing licence and cannot be redistributed, hampering reproducibility. These limitations in evaluation data prevent studying models’ capabilities to perform a broad range of simplification transformations. Even though most SS models are trained on simplification instances displaying several text transformations (e.g. WikiLarge (Zhang and Lapata, 2017)), we curr"
2020.acl-main.424,Q16-1029,0,0.180298,"ranco, 2012), and machine translation (Hasler et al., 2017). In order simplify a sentence, several rewriting transformations can be performed: replacing complex words/phrases with simpler synonyms (i.e. lexical paraphrasing), changing the syntactic structure of the sentence (e.g. splitting), or removing superfluous information that make the sentence more complicated (Petersen, 2007; Alu´ısio et al., 2008; Bott and Saggion, 2011). However, models for automatic SS are evaluated on datasets whose simplifications are not representative of this variety of transformations. For instance, TurkCorpus (Xu et al., 2016), a standard dataset for assessment in SS, contains simplifications produced mostly by lexical paraphrasing, while reference simplifications in HSplit (Sulem et al., 2018a) focus on splitting sentences. The Newsela corpus (Xu et al., 2015) contains simplifications produced by professionals applying multiple rewriting transformations, but sentence alignments are automatically computed and thus imperfect, and its data can only be accessed after signing a restrictive publicsharing licence and cannot be redistributed, hampering reproducibility. These limitations in evaluation data prevent studying"
2020.acl-main.424,D17-1062,0,0.658579,"Newsela corpus (Xu et al., 2015) contains simplifications produced by professionals applying multiple rewriting transformations, but sentence alignments are automatically computed and thus imperfect, and its data can only be accessed after signing a restrictive publicsharing licence and cannot be redistributed, hampering reproducibility. These limitations in evaluation data prevent studying models’ capabilities to perform a broad range of simplification transformations. Even though most SS models are trained on simplification instances displaying several text transformations (e.g. WikiLarge (Zhang and Lapata, 2017)), we currently do not measure their performance in more abstractive scenarios, i.e. cases with substantial modifications to the original sentences. In this paper we introduce ASSET (Abstractive Sentence Simplification Evaluation and Tuning), a new dataset for tuning and evaluation of automatic SS models. ASSET consists of 23,590 human simplifications associated with the 2,359 original sentences from TurkCorpus (10 simplifications per 4668 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4668–4679 c July 5 - 10, 2020. 2020 Association for Computati"
2020.acl-main.424,D18-1355,0,0.229799,"o collect simplifications for SimPA, a dataset with sentences from the Public Administration domain. One particular characteristic of the methodology followed is that lexical and syntactic simplifications were performed independently. 3 Creating ASSET We extended TurkCorpus (Xu et al., 2016) by using the same original sentences, but crowdsourced manual simplifications that encompass a richer set of rewriting transformations. Since TurkCorpus was adopted as the standard dataset for evaluating SS models, several system outputs on this data are already publicly available (Zhang and Lapata, 2017; Zhao et al., 2018; Martin et al., 2020). Therefore, we can now assess the capabilities of these and other systems in scenarios with varying simplification expectations: lexical paraphrasing with TurkCorpus, sentence splitting with HSplit, and multiple transformations with ASSET. 3.1 Data Collection Protocol Manual simplifications were collected using Amazon Mechanical Turk (AMT). AMT allows us to publish HITs (Human Intelligence Tasks), which workers can choose to work on, submit an answer, and collect a reward if the work is approved. This was also the platform used for TurkCorpus. Worker Requirements. Partic"
2020.acl-main.424,P06-4018,0,\N,Missing
2020.acl-main.424,S16-1085,1,\N,Missing
2020.acl-main.424,W18-6401,0,\N,Missing
2020.acl-main.645,C18-1139,0,0.188832,"e of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks. 1 Introduction Pretrained word representations have a long history in Natural Language Processing (NLP), from noncontextual (Brown et al., 1992; Ando and Zhang, 2005; Mikolov et al., 2013; Pennington et al., 2014) to contextual word embeddings (Peters et al., 2018; Akbik et al., 2018). Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide ∗ Equal contribution. Order determined alphabetically. range of tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al., 2019). These transf"
2020.acl-main.645,bawden-etal-2014-correcting,0,0.0112223,"ncy parsing consists in predicting the labeled syntactic tree in order to capture the syntactic relations between words. For both of these tasks we run our experiments using the Universal Dependencies (UD)3 framework and its corresponding UD POS tag set (Petrov et al., 2012) and UD treebank collection (Nivre et al., 2018), which was used for the CoNLL 2018 shared task (Seker et al., 2018). We perform our evaluations on the four freely available French UD treebanks in UD v2.2: GSD (McDonald et al., 2013), Sequoia4 (Candito and Seddah, 2012; Candito et al., 2014), Spoken (Lacheret et al., 2014; Bawden et al., 2014)5 , and ParTUT (Sanguinetti and Bosco, 2015). A brief overview of the size and content of each treebank can be found in Table 1. Treebank #Tokens #Sentences GSD 389,363 16,342 68,615 3,099 Spoken 34,972 ParTUT 27,658 ···················· FTB 350,930 2,786 1,020 27,658 ···················· Sequoia ···················· ···················· sion of the Multi-Genre NLI (MultiNLI) corpus (Williams et al., 2018) to 15 languages by translating the validation and test sets manually into each of those languages. The English training set is machine translated for all languages other than English. The da"
2020.acl-main.645,J92-4003,0,0.338164,"Missing"
2020.acl-main.645,W09-3821,0,0.0439594,"l model based on mBERT, UDify is trained simultaneously on 124 different UD treebanks, creating a single POS tagging and dependency parsing model that works across 75 different languages. We report the scores from Kondratyuk (2019) paper. Table 1: Statistics on the treebanks used in POS tagging, dependency parsing, and NER (FTB). We also evaluate our model in NER, which is a sequence labeling task predicting which words refer to real-world objects, such as people, locations, artifacts and organisations. We use the French Treebank6 (FTB) (Abeillé et al., 2003) in its 2008 version introduced by Candito and Crabbé (2009) and with NER annotations by Sagot et al. (2012). The FTB contains more than 11 thousand entity mentions distributed among 7 different entity types. A brief overview of the FTB can also be found in Table 1. Finally, we evaluate our model on NLI, using the French part of the XNLI dataset (Conneau et al., 2018). NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the exten3 https://universaldependencies.org https://deep-sequoia.inria.fr 5 Speech transcript uncased that includes annotated disfluencies without punctua"
2020.acl-main.645,2020.acl-main.747,0,0.0712291,"Missing"
2020.acl-main.645,D18-1269,0,0.021825,"g, and NER (FTB). We also evaluate our model in NER, which is a sequence labeling task predicting which words refer to real-world objects, such as people, locations, artifacts and organisations. We use the French Treebank6 (FTB) (Abeillé et al., 2003) in its 2008 version introduced by Candito and Crabbé (2009) and with NER annotations by Sagot et al. (2012). The FTB contains more than 11 thousand entity mentions distributed among 7 different entity types. A brief overview of the FTB can also be found in Table 1. Finally, we evaluate our model on NLI, using the French part of the XNLI dataset (Conneau et al., 2018). NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the exten3 https://universaldependencies.org https://deep-sequoia.inria.fr 5 Speech transcript uncased that includes annotated disfluencies without punctuation 6 This dataset has only been stored and used on Inria’s servers after signing the research-only agreement. 4 • UDPipe Future (Straka, 2018): An LSTMbased model ranked 3rd in dependency parsing and 6th in POS tagging at the CoNLL 2018 shared task (Seker et al., 2018). We report the scores from Kondratyuk"
2020.acl-main.645,2020.findings-emnlp.292,0,0.0196656,"B model (81.88 vs. 81.55). This might be due to the random seed used for pretraining, as each model is pretrained only once. 7210 language understanding tasks. However, even with a BASE architecture and 4GB of training data, the validation loss is still decreasing beyond 100k steps (and 400 epochs). This suggests that we are still under-fitting the 4GB pretraining dataset, training longer might increase downstream performance. 7 Discussion Since the pre-publication of this work (Martin et al., 2019), many monolingual language models have appeared, e.g. (Le et al., 2019; Virtanen et al., 2019; Delobelle et al., 2020), for as much as 30 languages (Nozza et al., 2020). In almost all tested configurations they displayed better results than multilingual language models such as mBERT (Pires et al., 2019). Interestingly, Le et al. (2019) showed that using their FlauBert, a RoBERTa-based language model for French, which was trained on less but more edited data, in conjunction to CamemBERT in an ensemble system could improve the performance of a parsing model and establish a new state-of-the-art in constituency parsing of French, highlighting thus the complementarity of both models.18 As it was the case for Engli"
2020.acl-main.645,N19-1423,0,0.496677,"ennington et al., 2014) to contextual word embeddings (Peters et al., 2018; Akbik et al., 2018). Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide ∗ Equal contribution. Order determined alphabetically. range of tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al., 2019). These transfer learning methods exhibit clear advantages over more traditional task-specific approaches. In particular, they can be trained in an unsupervized manner, thereby taking advantage of the information contained in large amounts of raw text. Yet they come with implementation challenges, namely the amount of data and computational resources needed for pretraining, which can reach hundreds of gigabytes of text and require hundreds of GPUs (Yang et al., 2019; Liu et al., 2019). This has limited the availability of these stat"
2020.acl-main.645,E17-2068,0,0.0593732,"training data, architecture, training objective and optimisation setup we use for CamemBERT. 4.1 Training data Pretrained language models benefits from being trained on large datasets (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019). We therefore use the French part of the OSCAR corpus (Ortiz Suárez et al., 2019), a pre-filtered and pre-classified version of Common Crawl.7 OSCAR is a set of monolingual corpora extracted from Common Crawl snapshots. It follows the same approach as (Grave et al., 2018) by using a language classification model based on the fastText linear classifier (Grave et al., 2017; Joulin et al., 2016) pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 languages. No other filtering is done. We use a non-shuffled version of the French data, which amounts to 138GB of raw text and 32.7B tokens after subword tokenization. 4.2 Pre-processing We segment the input text data into subword units using SentencePiece (Kudo and Richardson, 2018). SentencePiece is an extension of Byte-Pair encoding (BPE) (Sennrich et al., 2016) and WordPiece (Kudo, 2018) that does not require pre-tokenization (at the word or token level), thus removing the need for language-specific to"
2020.acl-main.645,P19-1356,1,0.885465,"Missing"
2020.acl-main.645,D14-1162,0,0.0850598,"entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks. 1 Introduction Pretrained word representations have a long history in Natural Language Processing (NLP), from noncontextual (Brown et al., 1992; Ando and Zhang, 2005; Mikolov et al., 2013; Pennington et al., 2014) to contextual word embeddings (Peters et al., 2018; Akbik et al., 2018). Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide ∗ Equal contribution. Order determined alphabetically. range of tasks (Devlin et al., 2019; Ra"
2020.acl-main.645,N18-1202,0,0.351202,". We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks. 1 Introduction Pretrained word representations have a long history in Natural Language Processing (NLP), from noncontextual (Brown et al., 1992; Ando and Zhang, 2005; Mikolov et al., 2013; Pennington et al., 2014) to contextual word embeddings (Peters et al., 2018; Akbik et al., 2018). Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide ∗ Equal contribution. Order determined alphabetically. range of tasks (Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019; Raffel et al."
2020.acl-main.645,petrov-etal-2012-universal,0,0.0660713,"Natural Language Inference (NLI). We also present the baselines that we will use for comparison. 1 Released at: https://camembert-model.fr under the MIT open-source license. 7204 2 https://allennlp.org/elmo Tasks POS tagging is a low-level syntactic task, which consists in assigning to each word its corresponding grammatical category. Dependency parsing consists in predicting the labeled syntactic tree in order to capture the syntactic relations between words. For both of these tasks we run our experiments using the Universal Dependencies (UD)3 framework and its corresponding UD POS tag set (Petrov et al., 2012) and UD treebank collection (Nivre et al., 2018), which was used for the CoNLL 2018 shared task (Seker et al., 2018). We perform our evaluations on the four freely available French UD treebanks in UD v2.2: GSD (McDonald et al., 2013), Sequoia4 (Candito and Seddah, 2012; Candito et al., 2014), Spoken (Lacheret et al., 2014; Bawden et al., 2014)5 , and ParTUT (Sanguinetti and Bosco, 2015). A brief overview of the size and content of each treebank can be found in Table 1. Treebank #Tokens #Sentences GSD 389,363 16,342 68,615 3,099 Spoken 34,972 ParTUT 27,658 ···················· FTB 350,930 2,786"
2020.acl-main.645,P19-1493,0,0.0210785,"itecture and 4GB of training data, the validation loss is still decreasing beyond 100k steps (and 400 epochs). This suggests that we are still under-fitting the 4GB pretraining dataset, training longer might increase downstream performance. 7 Discussion Since the pre-publication of this work (Martin et al., 2019), many monolingual language models have appeared, e.g. (Le et al., 2019; Virtanen et al., 2019; Delobelle et al., 2020), for as much as 30 languages (Nozza et al., 2020). In almost all tested configurations they displayed better results than multilingual language models such as mBERT (Pires et al., 2019). Interestingly, Le et al. (2019) showed that using their FlauBert, a RoBERTa-based language model for French, which was trained on less but more edited data, in conjunction to CamemBERT in an ensemble system could improve the performance of a parsing model and establish a new state-of-the-art in constituency parsing of French, highlighting thus the complementarity of both models.18 As it was the case for English when BERT was first released, the availability of similar scale language models for French enabled interesting applications, such as large scale anonymization of legal texts, where Ca"
2020.acl-main.645,P16-1162,0,0.0173054,"l snapshots. It follows the same approach as (Grave et al., 2018) by using a language classification model based on the fastText linear classifier (Grave et al., 2017; Joulin et al., 2016) pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 languages. No other filtering is done. We use a non-shuffled version of the French data, which amounts to 138GB of raw text and 32.7B tokens after subword tokenization. 4.2 Pre-processing We segment the input text data into subword units using SentencePiece (Kudo and Richardson, 2018). SentencePiece is an extension of Byte-Pair encoding (BPE) (Sennrich et al., 2016) and WordPiece (Kudo, 2018) that does not require pre-tokenization (at the word or token level), thus removing the need for language-specific tokenisers. We use a vocabulary size of 32k subword tokens. These subwords are learned on 107 sentences sampled randomly from the pretraining dataset. We do not use subword regularisation (i.e. sampling from multiple possible segmentations) for the sake of simplicity. 7 https://commoncrawl.org/about/ 4.3 Language Modeling Transformer Similar to RoBERTa and BERT, CamemBERT is a multi-layer bidirectional Transformer (Vaswani et al., 2017). Given the widesp"
2020.acl-main.645,K18-2020,0,0.0902039,"ent entity types. A brief overview of the FTB can also be found in Table 1. Finally, we evaluate our model on NLI, using the French part of the XNLI dataset (Conneau et al., 2018). NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the exten3 https://universaldependencies.org https://deep-sequoia.inria.fr 5 Speech transcript uncased that includes annotated disfluencies without punctuation 6 This dataset has only been stored and used on Inria’s servers after signing the research-only agreement. 4 • UDPipe Future (Straka, 2018): An LSTMbased model ranked 3rd in dependency parsing and 6th in POS tagging at the CoNLL 2018 shared task (Seker et al., 2018). We report the scores from Kondratyuk (2019) paper. • UDPipe Future + mBERT + Flair (Straka et al., 2019): The original UDPipe Future implementation using mBERT and Flair as feature-based contextualized word embeddings. We report the scores from Straka et al. (2019) paper. In French, no extensive work has been done on NER due to the limited availability of annotated corpora. Thus we compare our model with the only recent available baselines set by Dupont (2017), who t"
2020.acl-main.645,P81-1022,0,0.325042,"Missing"
2020.acl-main.645,P19-1527,0,0.185418,"ining set is machine translated for all languages other than English. The dataset is composed of 122k train, 2490 development and 5010 test examples for each language. As usual, NLI performance is evaluated using accuracy. Baselines In dependency parsing and POStagging we compare our model with: • mBERT: The multilingual cased version of BERT (see Section 2.1). We fine-tune mBERT on each of the treebanks with an additional layer for POS-tagging and dependency parsing, in the same conditions as our CamemBERT model. • XLMMLM-TLM : A multilingual pretrained language model from Lample and Conneau (2019), which showed better performance than mBERT on NLI. We use the version available in the Hugging’s Face transformer library (Wolf et al., 2019); like mBERT, we fine-tune it in the same conditions as our model. Genres Blogs, News Reviews, Wiki Medical, News Non-fiction, Wiki Spoken Legal, News, Wikis News • UDify (Kondratyuk, 2019): A multitask and multilingual model based on mBERT, UDify is trained simultaneously on 124 different UD treebanks, creating a single POS tagging and dependency parsing model that works across 75 different languages. We report the scores from Kondratyuk (2019) paper."
2020.acl-main.645,P19-1452,0,0.028162,"Missing"
2020.acl-main.645,N18-1101,0,0.0873351,"Missing"
2020.acl-main.645,D19-1077,0,0.0177636,"Straka et al. (2019) paper. In French, no extensive work has been done on NER due to the limited availability of annotated corpora. Thus we compare our model with the only recent available baselines set by Dupont (2017), who trained both CRF (Lafferty et al., 2001) and 7205 BiLSTM-CRF (Lample et al., 2016) architectures on the FTB and enhanced them using heuristics and pretrained word embeddings. Additionally, as for POS and dependency parsing, we compare our model to a fine-tuned version of mBERT for the NER task. For XNLI, we provide the scores of mBERT which has been reported for French by Wu and Dredze (2019). We report scores from XLMMLM-TLM (described above), the best model from Lample and Conneau (2019). We also report the results of XLM-R (Conneau et al., 2019). 4 CamemBERT: a French Language Model In this section, we describe the pretraining data, architecture, training objective and optimisation setup we use for CamemBERT. 4.1 Training data Pretrained language models benefits from being trained on large datasets (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2019). We therefore use the French part of the OSCAR corpus (Ortiz Suárez et al., 2019), a pre-filtered and pre-classified vers"
2020.cmlc-1.3,W09-3821,0,0.0209598,"dependency parsing. Finally, it is also relevant to compare our results with CamemBERT on the selected tasks, because compared to UDify it is the work that pushed the furthest the performance in fine-tuning end-to-end a BERT-based model. Genre News Wiki. Blogs Pop. Wiki. Med. EuroParl Oral transcip. Oral Wiki. Legal Table 5: Sizes of the 4 treebanks used in the evaluations of POS-tagging and dependency parsing. 4.3.2. Named Entity Recognition Treebanks test data-set The benchmark data set from the French Treebank (FTB) (Abeillé et al., 2003) was selected in its 2008 version, as introduced by Candito and Crabbé (2009) and complemented with NER annotations by Sagot et al. (2012)10 . The tree-bank, shows a large proportion of the entity mentions that are multi-word entities. We therefore report the three metrics that are commonly used to evaluate models: precision, recall, and F1 score. consists in predicting which words refer to real-world objects, such as people, locations, artifacts and organizations, it directly probes the quality and specificity of semantic representations issued by the more or less balanced corpora under comparison. 4.3.1. POS-tagging and dependency parsing Experiments were run using t"
2020.cmlc-1.3,candito-etal-2014-deep,1,0.785322,"te-of-the-art language model for French, CamemBERT (Martin et al., 2019), based on the RoBERTa architecture pre-trained on the French sub-corpus of the newly available multilingual corpus OSCAR (Ortiz Suárez et al., 2019). Treebanks test data-set We perform our work on the four freely available French UD treebanks in UD v2.2: GSD, Sequoia, Spoken, and ParTUT, presented in Table 5. GSD treebank (McDonald et al., 2013) is the second-largest tree-bank available for French after the FTB (described in subsection 4.3.2.), it contains data from blogs, news, reviews, and Wikipedia. Sequoia tree-bank (Candito et al., 2014) comprises more than 3000 sentences, from the French Europarl, the regional newspaper L’Est Républicain, the French Wikipedia and documents from the European Medicines Agency. Spoken was automatically converted from the Rhapsodie tree-bank (Lacheret et al., 2014) with manual corrections. It consists of 57 sound samples of spoken French with phonetic transcription aligned with sound (word boundaries, syllables, and phonemes), syntactic and prosodic annotations. Finally, ParTUT is a conversion of a multilingual parallel treebank developed at the University of Turin, and consisting of a variety o"
2020.cmlc-1.3,N16-1030,0,0.0868252,"Missing"
2020.cmlc-1.3,P13-2017,0,0.021082,"o trained both CRF and BiLSTM-CRF architectures on the FTB and enhanced them using heuristics and pre-trained word-embeddings. And additional term of comparison was identified in a recently released state-of-the-art language model for French, CamemBERT (Martin et al., 2019), based on the RoBERTa architecture pre-trained on the French sub-corpus of the newly available multilingual corpus OSCAR (Ortiz Suárez et al., 2019). Treebanks test data-set We perform our work on the four freely available French UD treebanks in UD v2.2: GSD, Sequoia, Spoken, and ParTUT, presented in Table 5. GSD treebank (McDonald et al., 2013) is the second-largest tree-bank available for French after the FTB (described in subsection 4.3.2.), it contains data from blogs, news, reviews, and Wikipedia. Sequoia tree-bank (Candito et al., 2014) comprises more than 3000 sentences, from the French Europarl, the regional newspaper L’Est Républicain, the French Wikipedia and documents from the European Medicines Agency. Spoken was automatically converted from the Rhapsodie tree-bank (Lacheret et al., 2014) with manual corrections. It consists of 57 sound samples of spoken French with phonetic transcription aligned with sound (word boundari"
2020.cmlc-1.3,W18-4904,0,0.0227094,"e a significant predictor of ELMo’s accuracy on Spoken test data-set and for NER tasks. Other perspective uses of CaBeRnet involve it use as a corpus offering a reference point for lexical frequency measures, like association measures. Its comparability with English COCA further grants the cross-linguistic validity of measures like Point-wise Mutual Information or DICE’s Coefficient. The representativeness probed through our experimental approach are key aspects that allow such measures to be tested against psycho-linguistic and neurolinguistic data as shown in previous neuro-imaging studies (Fabre et al., 2018). The results obtained for the parsing tasks on ParTUT open a new perspective for the development of the French Balanced Reference Corpus, involving the enhancement of the terminological coverage of CaBeRnet. A sixth sub-part could be included to cover technical domains like legal and medical ones, and thereby enlarge the specialized lexical coverage of CaBeRnet. Further developments of this resource would involve an extension to cover user-generated content, ranging from well written blogs, tweets to more variable written productions like newspaper’s comment or We acknowledge Benoit Crabbé fo"
2020.cmlc-1.3,E17-2068,0,0.0312967,"set of comparable corpora with a similar balance and representativeness. C HILDREN B OOK T EST - FR number of different lemmas total number of forms mean number of forms per lemma Number of lemmas having more than one form : Percentage of lemmas with multiple forms OSCAR gathers a set of monolingual text extracted from Common Crawl - in plain text WET format - where all HTML tags are removed and all text encodings are converted to UTF-8. It follows a similar approach to (Grave et al., 2018) by using a language classification model based on the fastText linear classifier (Joulin et al., 2016; Grave et al., 2017) pre-trained on Wikipedia, Tatoeba and SETimes, supporting 176 different languages. After language classification, a deduplication step is performed without introducing a specialized filtering scheme: paragraphs containing 100 or more UTF-8 encoded characters are kept. This makes OSCAR an example of unfiltered data that is nearly as noisy as to the original Crawled data. WORDS 25 139 95 058 3.78 14 128 56.20 Table 2: Lexical statistics of French CBT, performed as described in §3. 3. 3.1.2. FrWIKI This corpus collects a selection of pages from Wikipediafr from a dump executed in April 2019, whe"
2020.cmlc-1.3,L18-1550,0,0.0194647,"plain narrative text. The effort of keeping proportion, genre, domain, and time as equal as possible yields a multilingual set of comparable corpora with a similar balance and representativeness. C HILDREN B OOK T EST - FR number of different lemmas total number of forms mean number of forms per lemma Number of lemmas having more than one form : Percentage of lemmas with multiple forms OSCAR gathers a set of monolingual text extracted from Common Crawl - in plain text WET format - where all HTML tags are removed and all text encodings are converted to UTF-8. It follows a similar approach to (Grave et al., 2018) by using a language classification model based on the fastText linear classifier (Joulin et al., 2016; Grave et al., 2017) pre-trained on Wikipedia, Tatoeba and SETimes, supporting 176 different languages. After language classification, a deduplication step is performed without introducing a specialized filtering scheme: paragraphs containing 100 or more UTF-8 encoded characters are kept. This makes OSCAR an example of unfiltered data that is nearly as noisy as to the original Crawled data. WORDS 25 139 95 058 3.78 14 128 56.20 Table 2: Lexical statistics of French CBT, performed as described"
2020.cmlc-1.3,2005.mtsummit-papers.11,0,0.0651172,"Missing"
2020.cmlc-1.3,D19-1279,0,0.0884182,"ce, CaBeRnet’s balanced oral language use shows to pay off in POS-tagging. These results are extremely surprising especially given the fact that State-of-the-art For POS-tagging and Parsing we select as a baseline UDPipe Future (2.0), without any additional contextualized embeddings (Straka, 2018). This model was ranked 3rd in dependency parsing and 6th in POS-tagging during the CoNLL 2018 shared task (Seker et al., 2018). Notably, UDPipe Future provides us a strong baseline that does not make use of any pre-trained contextual embedding. We report on Table 6 the published results on UDify by (Kondratyuk, 2019), a multitask and multilingual model based on mBERT that is near state-of-the-art on all UD lan10 The NER-annotated FTB contains approximately than 12k sentences, and more than 350k tokens were extracted from articles of Le Monde newspaper (1989 - 1995). As a whole, it encompasses 11,636 entity mentions distributed among 7 different types : 2025 mentions of “Person”, 3761 of “Location”, 2382 of “Organisation”, 3357 of “Company”, 67 of “Product”, 15 of “POI” (Point of Interest) and 29 of “Fictional Character”. 19 GSD M ODEL S EQUOIA S POKEN PARTUT UPOS UAS LAS UPOS UAS LAS UPOS UAS LAS UPOS UAS"
2020.cmlc-1.3,N18-1202,0,0.187501,"valuation scores on different evaluation sets and tasks. Keywords: Balanced French Corpus, Language Models, French, BERT, ELMo, Tagging, Parsing, NER 1. Introduction uation of the quality of the word-embeddings obtained by pre-training and fine-tuning on different corpora, that are made here publicly available. Based on the underlying assumption that a linguistically representative corpus would possibly generate better word-embeddings. We provide an evaluation-based investigation of how a balanced crossgenre corpus can yield improvements in the performance of neural language models like ELMo (Peters et al., 2018) on various downstream tasks. The two corpora, CaBeRnet and CBT-fr, and the ELMos will be distributed freely under Creative Commons License. The question of quality versus size of training corpora is increasingly gaining attention and interest in the context of the latest developments in neural language models’ performance. The longstanding issue of corpora &quot;representativeness&quot; is here addressed, in order to grasp to what extent a linguistically balanced cross-genre language sample is sufficient for a language model to gain in accuracy for contextualized word-embeddings on different NLP tasks."
2020.cmlc-1.3,F12-2050,1,0.747324,"sults with CamemBERT on the selected tasks, because compared to UDify it is the work that pushed the furthest the performance in fine-tuning end-to-end a BERT-based model. Genre News Wiki. Blogs Pop. Wiki. Med. EuroParl Oral transcip. Oral Wiki. Legal Table 5: Sizes of the 4 treebanks used in the evaluations of POS-tagging and dependency parsing. 4.3.2. Named Entity Recognition Treebanks test data-set The benchmark data set from the French Treebank (FTB) (Abeillé et al., 2003) was selected in its 2008 version, as introduced by Candito and Crabbé (2009) and complemented with NER annotations by Sagot et al. (2012)10 . The tree-bank, shows a large proportion of the entity mentions that are multi-word entities. We therefore report the three metrics that are commonly used to evaluate models: precision, recall, and F1 score. consists in predicting which words refer to real-world objects, such as people, locations, artifacts and organizations, it directly probes the quality and specificity of semantic representations issued by the more or less balanced corpora under comparison. 4.3.1. POS-tagging and dependency parsing Experiments were run using the Universal Dependencies (UD) paradigm and its corresponding"
2020.cmlc-1.3,K18-2021,0,0.0185014,", only an even smaller amount of data comes from purely oral transcripts comparable the ones in the Spoken tree-bank, namely 67,444 words from Rhapsodie corpus, and 575,894 words form ORFEO. Hence, CaBeRnet’s balanced oral language use shows to pay off in POS-tagging. These results are extremely surprising especially given the fact that State-of-the-art For POS-tagging and Parsing we select as a baseline UDPipe Future (2.0), without any additional contextualized embeddings (Straka, 2018). This model was ranked 3rd in dependency parsing and 6th in POS-tagging during the CoNLL 2018 shared task (Seker et al., 2018). Notably, UDPipe Future provides us a strong baseline that does not make use of any pre-trained contextual embedding. We report on Table 6 the published results on UDify by (Kondratyuk, 2019), a multitask and multilingual model based on mBERT that is near state-of-the-art on all UD lan10 The NER-annotated FTB contains approximately than 12k sentences, and more than 350k tokens were extracted from articles of Le Monde newspaper (1989 - 1995). As a whole, it encompasses 11,636 entity mentions distributed among 7 different types : 2025 mentions of “Person”, 3761 of “Location”, 2382 of “Organisat"
2020.cmlc-1.3,P19-1527,0,0.0151385,"more or less balanced corpora under comparison. 4.3.1. POS-tagging and dependency parsing Experiments were run using the Universal Dependencies (UD) paradigm and its corresponding UD POS-tag set (Petrov et al., 2011) and UD treebank collection version 2.2 (Nivre et al., 2018), which was used for the CoNLL 2018 shared task. Different terms of comparisons were considered on the two downstream tasks of part-of-speech (POS) tagging and dependency parsing. NER State-of-the-art English has received the most attention in NER in the past, with some recent developments in German, Dutch and Spanish by Straková et al. (2019). In French, no extensive work has been done due to the limited availability of NER corpora. We compare our model with the stable baselines settled by (Dupont, 2018), who trained both CRF and BiLSTM-CRF architectures on the FTB and enhanced them using heuristics and pre-trained word-embeddings. And additional term of comparison was identified in a recently released state-of-the-art language model for French, CamemBERT (Martin et al., 2019), based on the RoBERTa architecture pre-trained on the French sub-corpus of the newly available multilingual corpus OSCAR (Ortiz Suárez et al., 2019). Treeba"
2020.jeptalnrecital-taln.5,2020.findings-emnlp.292,0,0.0264053,"Missing"
2020.jeptalnrecital-taln.5,N19-1423,0,0.0614451,"Missing"
2020.jeptalnrecital-taln.5,L18-1550,0,0.0310337,"Missing"
2020.jeptalnrecital-taln.5,E17-2068,0,0.0612565,"Missing"
2020.jeptalnrecital-taln.5,P18-1031,0,0.0436608,"Missing"
2020.jeptalnrecital-taln.5,P19-1356,1,0.883155,"Missing"
2020.jeptalnrecital-taln.5,D14-1162,0,0.0850524,"Missing"
2020.jeptalnrecital-taln.5,N18-1202,0,0.122933,"Missing"
2020.jeptalnrecital-taln.5,P19-1493,0,0.0268051,"Missing"
2020.jeptalnrecital-taln.5,F12-2050,1,0.854707,"Missing"
2020.jeptalnrecital-taln.5,P19-1527,0,0.0505485,"Missing"
2020.jeptalnrecital-taln.5,P19-1452,0,0.0305722,"Missing"
2020.jeptalnrecital-taln.5,N18-1101,0,0.0357062,"Missing"
2020.lrec-1.392,P19-1302,0,0.0972515,"computational historical linguistics tasks (List et al., 2018; Carling et al., 2018). However, over the last few years, diachronic resource use has found its way to more general applications, notably for tasks involving low resource languages, from machine translation using diachronic language relations (Nguyen and Chiang, 2017) or cognates and loan words sets (Grönroos et al., 2018) to bilingual lexicons generation for low resource languages (Nasution et al., 2017). Yet only a small number of multilingual etymological lexicons are available (de Melo, 2014; Sagot, 2017; Pantaleo et al., 2017; Batsuren et al., 2019), most extracted from the etymological information found in the Wiktionary.1 There is still room for improvement regarding the quality, richness, lexical or language coverage and etymological granularity (differentiation between inheritance, borrowing, cognacy) of such resources. The work described in this paper has two parallel objectives. We investigate the methodological challenges underlying the development and use of an etymological database based on the Wiktionary. At the same time, we describe how we addressed these challenges in the case of our own etymological database, EtymDB. After"
2020.lrec-1.392,W19-5304,0,0.0135885,"nd Middle Low German (against 47,000 for EtymDB 1.0); it also contains 72,000 relations including either one of those languages (for detail, see Table 1, against 57,000 for EtymDB 1.0). As such, EtymDB 2.0 constitutes a valuable historical linguistics lexical resource in itself, as it includes lexical entries for past words, as well as their relations to previous and future lexemes. 7.3. Low Resource Languages Translation Indo-Aryan Family In the last couple of years, machine translation (MT) researchers have tried to improve the MT of low-resource language pairs using a number of techniques. Bawden et al. (2019) tried to improve MT from English to Gujarati by using Hindi as a pivot language. Grönroos et al. (2018) have used cognates and borrowings to improve MT systems for low-resource languages, by bootstrapping them with the help of etymologically related high resource language translations. EtymDB 2.0 contains 2,316 lexical entries in Gujarati, 7,748 in Hindi and 225 direct cognacy relations between those two languages. It also contains 10,826 lexemes in Sanskrit, as well as 500 etymological relations between Gujarati and Sanskrit and about 3,000 between Hindi and Sanskrit, which could permit the"
2020.lrec-1.392,Q18-1041,0,0.0151025,".2. Documenting the Resource 6.2.1. General Documentation Language databases are generally described in a research paper that indicates where the data comes from, how it was extracted and parsed, and how errors were managed. To ensure reproducibility, it is also important to provide the original data, the scripts which were used to generate the data, and a documentation for future users, which details how to use the base, as well as summarises information type, encoding, structure, and ideally sources. All three items are provided along our database on a git repository.7 6.2.2. Data Statement Bender and Friedman (2018) proposed the introduction of data statements as “a design solution and professional practice for natural language processing technologists.” In concrete terms, the data statement of a dataset is a description of all elements needed to understand the context of its creation and edition, among which the curation rationale, language variety description, and different sub-items depending on the type of data (annotator demographic for annotated data, speaker demographic, speech acquisition situation and recording quality for audio data, text characteristics (such as type and topic) for textual res"
2020.lrec-1.392,de-melo-2014-etymological,0,0.364187,"Missing"
2020.lrec-1.392,ehrmann-etal-2014-representing,0,0.0297902,"eyer and Gurevych (2012) contains a (now slightly dated) full description of the Wiktionary, as well as a discussion of its update mechanism, coverage and quality, in comparison to expert resources. Sérasset (2015) considers it to be an interesting starting point to build linguistic resources from. The Wiktionary has been used in general NLP tasks, such as semantic relatedness assessment (Zesch et al., 2008), cognate clustering (using translation pairs, see Wu and Yarowsky (2018)). It has also been used in linguistic resources creation, such as encyclopedic dictionary and ontology generation (Ehrmann et al., 2014), wordnet induction (de Melo, 2014), or etymological tree representation (Pantaleo et al., 2017). More recently, Hartmann (2019) 3208 argued that “Especially regarding reconstructed language data, Wiktionary has the decisive advantage that the reconstructions follow certain guidelines [...] unlike data collected from various different traditional dictionaries”. This rationale was behind the choice made in Sagot (2017) to use the Wiktionary as the starting point for EtymDB. 3.3. Identifying Bias No matter the source chosen, it will be biased, either due to the data gathering process or to exter"
2020.lrec-1.392,W18-6410,0,0.128179,"on Most available electronic lexical resources are synchronic (formalising a language as it is or was at a specific, although sometimes broad, period of time). Until recently, the few diachronic resources available were mostly used for computational historical linguistics tasks (List et al., 2018; Carling et al., 2018). However, over the last few years, diachronic resource use has found its way to more general applications, notably for tasks involving low resource languages, from machine translation using diachronic language relations (Nguyen and Chiang, 2017) or cognates and loan words sets (Grönroos et al., 2018) to bilingual lexicons generation for low resource languages (Nasution et al., 2017). Yet only a small number of multilingual etymological lexicons are available (de Melo, 2014; Sagot, 2017; Pantaleo et al., 2017; Batsuren et al., 2019), most extracted from the etymological information found in the Wiktionary.1 There is still room for improvement regarding the quality, richness, lexical or language coverage and etymological granularity (differentiation between inheritance, borrowing, cognacy) of such resources. The work described in this paper has two parallel objectives. We investigate the me"
2020.lrec-1.392,W19-4713,0,0.0126994,"hanism, coverage and quality, in comparison to expert resources. Sérasset (2015) considers it to be an interesting starting point to build linguistic resources from. The Wiktionary has been used in general NLP tasks, such as semantic relatedness assessment (Zesch et al., 2008), cognate clustering (using translation pairs, see Wu and Yarowsky (2018)). It has also been used in linguistic resources creation, such as encyclopedic dictionary and ontology generation (Ehrmann et al., 2014), wordnet induction (de Melo, 2014), or etymological tree representation (Pantaleo et al., 2017). More recently, Hartmann (2019) 3208 argued that “Especially regarding reconstructed language data, Wiktionary has the decisive advantage that the reconstructions follow certain guidelines [...] unlike data collected from various different traditional dictionaries”. This rationale was behind the choice made in Sagot (2017) to use the Wiktionary as the starting point for EtymDB. 3.3. Identifying Bias No matter the source chosen, it will be biased, either due to the data gathering process or to external factors. In the case of EtymDB, for example, using the Wiktionary biased the language distribution of our dataset towards En"
2020.lrec-1.392,I17-2050,0,0.0124548,"t, Language Resource Life-cycle, Methodology 1. Introduction Most available electronic lexical resources are synchronic (formalising a language as it is or was at a specific, although sometimes broad, period of time). Until recently, the few diachronic resources available were mostly used for computational historical linguistics tasks (List et al., 2018; Carling et al., 2018). However, over the last few years, diachronic resource use has found its way to more general applications, notably for tasks involving low resource languages, from machine translation using diachronic language relations (Nguyen and Chiang, 2017) or cognates and loan words sets (Grönroos et al., 2018) to bilingual lexicons generation for low resource languages (Nasution et al., 2017). Yet only a small number of multilingual etymological lexicons are available (de Melo, 2014; Sagot, 2017; Pantaleo et al., 2017; Batsuren et al., 2019), most extracted from the etymological information found in the Wiktionary.1 There is still room for improvement regarding the quality, richness, lexical or language coverage and etymological granularity (differentiation between inheritance, borrowing, cognacy) of such resources. The work described in this"
2020.lrec-1.392,L18-1538,0,0.0627943,"nary, which contain large scale structured information, most of the time sourced from already existing and published etymological works. Meyer and Gurevych (2012) contains a (now slightly dated) full description of the Wiktionary, as well as a discussion of its update mechanism, coverage and quality, in comparison to expert resources. Sérasset (2015) considers it to be an interesting starting point to build linguistic resources from. The Wiktionary has been used in general NLP tasks, such as semantic relatedness assessment (Zesch et al., 2008), cognate clustering (using translation pairs, see Wu and Yarowsky (2018)). It has also been used in linguistic resources creation, such as encyclopedic dictionary and ontology generation (Ehrmann et al., 2014), wordnet induction (de Melo, 2014), or etymological tree representation (Pantaleo et al., 2017). More recently, Hartmann (2019) 3208 argued that “Especially regarding reconstructed language data, Wiktionary has the decisive advantage that the reconstructions follow certain guidelines [...] unlike data collected from various different traditional dictionaries”. This rationale was behind the choice made in Sagot (2017) to use the Wiktionary as the starting poi"
2020.lrec-1.393,chrupala-etal-2008-learning,0,0.0426036,"Missing"
2020.lrec-1.393,N18-2031,0,0.0159538,"et al., 2013) with inconclusive results. In our methodology we need to distinguish inflectional forms (f ) with lexemes (L), the former are obtained from raw text while the latter are extracted from OFrLex lexicon. To obtain a lexeme embedding (e(L)), we apply the FastText model trained on the raw text corpus made of inflectional forms (ft(f )). We then make the average of the form embeddings from the lexeme, weighted by the occurrences of each form with the same the PoS tag (p) as the lexeme. The weighted average has recently been demonstrated to be a good approach to obtain meta-embeddings (Coates and Bollegala, 2018). Here we apply this logic while taking into account occurrences per PoS tag. This is formalised in Equation 1. P f ∈F (L) ft(f ) occ(f ) e(L) = P (1) f ∈F (L) occ(f, p) We use the set of lexeme embeddings obtained using equation 1 as an input for clustering. We cluster this lexeme embedding space using Spectral Clustering (Ng et al., 2002). As for the hyper-parameters we set a Gaussian kernel, a gamma of 0.7 and discretisation to assign clusters. Moreover, we do not use eigenvalue decomposition strategy and set the number of targeted clusters as 20, according to the number of PoS tags (n clus"
2020.lrec-1.393,N19-1423,0,0.0098963,"for candidates We start by using the raw texts as input to train a FastText model (Joulin et al., 2017) using the Gensim implementation14 . FastText was selected for various reasons. First, we need to take into account morphological information about the words with their inflections. Formal similarity could be 14 https://radimrehurek.com/gensim/ used externally but the bag of n-grams used in this model is already dedicated to this. Second, we do possess relatively small data (see Table 6) in comparison to other languages with a lot of resources. Thus, we cannot use latest models such as Bert (Devlin et al., 2019) or ELMo (Peters et al., 2018) which require a large amount of data. In fact, we tried both architectures of Word2Vec (Mikolov et al., 2013) with inconclusive results. In our methodology we need to distinguish inflectional forms (f ) with lexemes (L), the former are obtained from raw text while the latter are extracted from OFrLex lexicon. To obtain a lexeme embedding (e(L)), we apply the FastText model trained on the raw text corpus made of inflectional forms (ft(f )). We then make the average of the form embeddings from the lexeme, weighted by the occurrences of each form with the same the P"
2020.lrec-1.393,A00-2013,0,0.327585,"Missing"
2020.lrec-1.393,E17-2068,0,0.00985913,"to validate or refute. To automatically obtain pseudo-synonyms we need to consider the words in context given their morphosyntactic category. This is why we use the BFM corpus in its two available versions: 170 raw texts and 42 CONLL files with verified part-of-speech (PoS) tags. The new annotated BFM corpus which is the current biggest annotated corpus for Old French. Table 6 shows information about the PoS tagged version. Tokens Vocab EN POS tags FR POS tags (CATTEX) 3,640,013 158,620 20 65 Table 6: Data used for candidates We start by using the raw texts as input to train a FastText model (Joulin et al., 2017) using the Gensim implementation14 . FastText was selected for various reasons. First, we need to take into account morphological information about the words with their inflections. Formal similarity could be 14 https://radimrehurek.com/gensim/ used externally but the bag of n-grams used in this model is already dedicated to this. Second, we do possess relatively small data (see Table 6) in comparison to other languages with a lot of resources. Thus, we cannot use latest models such as Bert (Devlin et al., 2019) or ELMo (Peters et al., 2018) which require a large amount of data. In fact, we tr"
2020.lrec-1.393,W99-0615,0,0.28572,"reebank by applying a well-known and tested grammar or parser on the corpus. Recently, incremental parsebanking showed good results for enriching morphological lexicons with high coverage (Rosén et al., 2016). Valency retrieval through deverbative nouns was also tackled (Fuˇcíková et al., 2016) but requires a task oriented gold dataset. Another recent enrichment strategy consists into using word embeddings to obtain clusters of words in order to enrich a lexicon (Siklósi, 2016). Morphological lexicons have been used for several tasks. From constraints derived from lexicon at PoS tagging time (Kim et al., 1999; Hajiˇc, 2000) to additional lexicon-based features combined with standard ones during the training process (Chrupała et al., 2008; Goldberg et al., 2009; Denis and Sagot, 2012). To improve these lexicon usages for different tasks such as multilingual PoS tagging supported by a lexicon (Sagot, 2016), we need to create a computational morphological lexicon for Old French: the OFrLex lexicon. 3. 3.1. Lexicon Creation Heterogeneous Language Resources may vary. However, the part-of-speech tags (PoS) are already converted to their CATTEX7 (Guillot et al., 2010) equivalent with additional gender an"
2020.lrec-1.393,P13-2017,0,0.0299477,"ais Médiéval (BFM - Medieval French Base) (Guillot et al., 2017)1 with more than 4 million words and the Nouveau Corpus d’Amsterdam (NCA - New Amsterdam Corpus) (Stein and al., 2008)2 with more than 3 million words. The main treebanks for Old French are the Syntactic Reference Corpus of Medieval French (SRCMF) (Stein and Prévost, 2013) and the Old French subpart from the Modéliser le changement : les voies du français (MCVF) corpus (Martineau, 2008). However, they do not share the same syntactic and POS tag sets, and only SRCMF is on open access with part of it in Universal Dependencies (UD) (McDonald et al., 2013) format3 . In the available resources different kinds of text are gathered. Some vary in style (prose, verse), literary genre (religious, historical, didactical, etc.), or even in time span (from 10th century to 13th century). Nevertheless, there is no available morphological lexicon,4 and a fortiori no 1 http://bfm.ens-lyon.fr https://sites.google.com/site/ achimstein/research/resources/nca 3 https://github.com/ UniversalDependencies/UD_Old_French-SRCMF/ 4 A morphological lexicon is a collection of entries of the form 2 syntactic lexicon5 for Old French. Most of the existing lexicons and dict"
2020.lrec-1.393,nicolas-etal-2012-unsupervised,0,0.0298521,"(NLP) tasks. PoS tagging has been applied on SRCMF using TreeTagger (Schmid, 1999; Stein, 2014) and Conditional Random Fields (Lafferty et al., 2001; Guibon et al., 2014; Guibon et al., 2015) as a preparation for Old French dependency parsing using Mate (Bohnet, 2010). On the other hand, lexicon enrichment is a part of the lexicon creation process and has been the subject of several research work, particularly for morphological lexicons. Nicolas et al. (2010) developed an unsupervised morphological rule acquisition tool which was combined with the Alexina framework (Walther and Nicolas, 2011; Nicolas et al., 2012) to enrich morphological lexicons. Another approach used to enrich or create a lexicon is derived from parsebanking (Rosén and de Smedt, 2007) which consists of creating a new treebank by applying a well-known and tested grammar or parser on the corpus. Recently, incremental parsebanking showed good results for enriching morphological lexicons with high coverage (Rosén et al., 2016). Valency retrieval through deverbative nouns was also tackled (Fuˇcíková et al., 2016) but requires a task oriented gold dataset. Another recent enrichment strategy consists into using word embeddings to obtain clu"
2020.lrec-1.393,N18-1202,0,0.00985562,"ng the raw texts as input to train a FastText model (Joulin et al., 2017) using the Gensim implementation14 . FastText was selected for various reasons. First, we need to take into account morphological information about the words with their inflections. Formal similarity could be 14 https://radimrehurek.com/gensim/ used externally but the bag of n-grams used in this model is already dedicated to this. Second, we do possess relatively small data (see Table 6) in comparison to other languages with a lot of resources. Thus, we cannot use latest models such as Bert (Devlin et al., 2019) or ELMo (Peters et al., 2018) which require a large amount of data. In fact, we tried both architectures of Word2Vec (Mikolov et al., 2013) with inconclusive results. In our methodology we need to distinguish inflectional forms (f ) with lexemes (L), the former are obtained from raw text while the latter are extracted from OFrLex lexicon. To obtain a lexeme embedding (e(L)), we apply the FastText model trained on the raw text corpus made of inflectional forms (ft(f )). We then make the average of the form embeddings from the lexeme, weighted by the occurrences of each form with the same the PoS tag (p) as the lexeme. The"
2020.lrec-1.393,W19-7816,0,0.0124717,"to enrich it, taking into account the fact that it is not a living language. For the moment, syntactic information is mostly limited to verbs; why we plan on extending it to adjectives and nouns in the near future. As shown in Section 5., the user interface is currently used for the lexicon validation phase supported by multiple enrichment propositions, such as those described in Section 4.2.. Even if our preliminary results focused on part-of-speech tagging, we plan to also use parsebanking as a way to improve the lexicon. To do so, a meta grammar for Old French parsing is under development (Regnault et al., 2019) and already uses OFrLex to improve parsing quality and to incrementally fix possible noise or silence present in the lexicon. OFrLex is available for everyone and future validation will yield new versions once the validation phase is done. 3223 Acknowledgements This work was partly funded by the French national ANR grant PROFITEROLE (ANR-16-CE38-0010) headed by Sophie Prévost, as well as by the second author’s chair in the PRAIRIE institute,18 funded by the French national agency ANR as part of the “Investissements d’avenir” programme under the reference ANR-19-P3IA-0001. Bibliographical Refe"
2020.lrec-1.393,W07-2422,0,0.0121769,"Missing"
2020.lrec-1.393,sagot-2010-lefff,1,0.734215,"e rules. This resource is useful because it links entries with other dictionaries such as TL and Godefroy. Inflected forms are also available for each entry. 11 https://fr.wikisource.org/wiki/Lexique_ de_l’ancien_francais 12 http://stella.atilf.fr/scripts/fantomes. exe 13 French national center of textual and lexical resources: https://www.cnrtl.fr/ Merging information Syntactic Information Addition. We complete this morphological lexicon for Old French with syntactic information. To do so, we follow the Alexina conventions already used for the contemporary French morphological lexicon Lefff (Sagot, 2010). From Lefff we obtain different types of syntactic information such as redistribution and valency. To retrieve them we make the hypothesis that verbs syntactically similar between Old French and Contemporary French can share information if and only if the former do not possesses any in the lexicon. To be more precise, we use different types of information from Lefff with a hierarchical priority presented in Algorithm 1. In this process, valency is retrieved from multiple sources (Godefroy, TL and DECT) looking for textual markers such as ""I"" (intransitiv in TL), ""trans."" (transitiv) or ""refl."
2020.lrec-1.393,sagot-2014-delex,1,0.77699,"DMF, a dictionary for Middle French, and the fact that lemmas are not represented by all their inflected forms, makes some entries and silence irrelevant for our purpose of obtaining a morphological lexicon for Old French. Wiktionary Wiktionary8 is a free dictionary which contains 6,500 entries for Old French corresponding to a lexeme and containing formalised descriptions for the inflection classes. The lexeme mengier9 (i.e. to eat) comes with alternative forms such as mangier, along with the etymology, and english gloss, and inflection information. We use the extraction process described in Sagot (2014): converting Wiktionary (wiki format) into a structured XML file before using it to extract morphological entries. A morphological entry consists of a citation form, an inflection class identifier, and the list of stems or irregular forms if relevant. Finally, we manually developed a morphological grammar describing the most important inflection classes present in Wiktionary. This morphological grammar use the Alexinaparsli format (Sagot and Walther, 2013). For instance, for verbs we use a model containing 8 stems and 2 exponent levels: an intermediate level for some consonant palatalisation a"
2020.lrec-1.393,stein-2014-parsing,0,0.0172125,"onal grammar that defined how to generate inflected forms given the citation form and an inflection class label (intensional inflectional lexicon). 5 A syntactic lexicon associates each entry (generally at the lexeme level) with syntactic information, including valency information, control/raising/attribution information, and other types of information describing the syntactic behaviour of the entry. 3217 2. Related Work Recent work used the previously mentioned textual databases for Natural Language Processing (NLP) tasks. PoS tagging has been applied on SRCMF using TreeTagger (Schmid, 1999; Stein, 2014) and Conditional Random Fields (Lafferty et al., 2001; Guibon et al., 2014; Guibon et al., 2015) as a preparation for Old French dependency parsing using Mate (Bohnet, 2010). On the other hand, lexicon enrichment is a part of the lexicon creation process and has been the subject of several research work, particularly for morphological lexicons. Nicolas et al. (2010) developed an unsupervised morphological rule acquisition tool which was combined with the Alexina framework (Walther and Nicolas, 2011; Nicolas et al., 2012) to enrich morphological lexicons. Another approach used to enrich or crea"
2020.lrec-1.393,K17-3026,1,0.778678,"=VERB,@pl.3.subj.prs.std afinera1 v 100 pred=""afiner___60674__1<Suj:cln|sn,Obj:(cla|sn)&gt;"",@pers,cat=v,upos=VERB,@sg.3.ind.fut.std Table 8: OFrLex intensional and extensional examples . 6. Preliminary Usage This lexicon can be used for multiple purposes such as diachronic studies, dependency parsing for Old French or PoS tagging. We evaluated OFrLex impact on PoS tagging using the Universal Dependencies (Nivre and al., 2019) version of SRCMF treebank’s training set. In order to do so we trained three models using alVWTagger17 initially developed for the CONLL-2017 shared task (Villemonte de La Clergerie et al., 2017). Like MElt (Denis and Sagot, 2012), this PoS tagger can use an external lexicon to infer complementary information from the train set or the test set. Thus, we only use OFrLex to extract the inflected forms with their associated PoS tag as the external lexicon for one model, and no external resource for the second model. Model alVWTagger alVWTagger + OFrLex v1 alVWTagger + OFrLex v1.2 Accuracy Unknown words Accuracy 93.80 94.80 95.08 81.60 85.70 87.10 Table 9: PoS tagging accuracy scores on SRCMF-UD using alVWTagger combined with the initial OFrLex (v1) and the one currently under enrichment"
2020.lrec-1.569,C18-1139,0,0.0420735,"cular with the CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and Ontonotes v5 (Pradhan et al., 2012; Pradhan et al., 2013) corpora. In recent years, NER was traditionally tackled using Conditional Random Fields (CRF) (Lafferty et al., 2001) which are quite suited for NER; CRFs were later used as decoding layers for Bi-LSTM architectures (Huang et al., 2015; Lample et al., 2016) showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualized word embeddings which yet again brought major improvements to the task (Peters et al., 2018; Akbik et al., 2018). Finally, large pre-trained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures (Devlin et al., 2019; Baevski et al., 2019). For French, rule-based system have been developed until relatively recently, due to the lack of proper training data (Sekine and Nobata, 2004; Rosset et al., 2005; Stern and Sagot, 2010; Nouvel et al., 2011). The limited availability of a few annotated corpora (cf. Section 1.) made it possible to apply statistical machine learning techniques (Bechet and Charton, 2010; Dupont and Tellier, 2"
2020.lrec-1.569,D19-1539,0,0.0122762,"om Fields (CRF) (Lafferty et al., 2001) which are quite suited for NER; CRFs were later used as decoding layers for Bi-LSTM architectures (Huang et al., 2015; Lample et al., 2016) showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualized word embeddings which yet again brought major improvements to the task (Peters et al., 2018; Akbik et al., 2018). Finally, large pre-trained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures (Devlin et al., 2019; Baevski et al., 2019). For French, rule-based system have been developed until relatively recently, due to the lack of proper training data (Sekine and Nobata, 2004; Rosset et al., 2005; Stern and Sagot, 2010; Nouvel et al., 2011). The limited availability of a few annotated corpora (cf. Section 1.) made it possible to apply statistical machine learning techniques (Bechet and Charton, 2010; Dupont and Tellier, 2014; Dupont, 2017) as well as hybrid techniques combining handcrafted grammars and machine learning (Béchet et al., 2011). To the best of our knowledge, the best results previously published on FTB NER are"
2020.lrec-1.569,Q17-1010,0,0.0748939,"n the FTB, the named entity annotations could only be provided to people having signed the FTB license, which prevented them from being made freely downloadable online. The goal of this paper is to establish a new state of the art for French NER by (i) providing a new, easy-to-use UDaligned version of the named entity annotation layer in the FTB and (ii) using this corpus as a training and evaluation dataset for carrying out NER experiments using state-ofthe-art architectures, thereby improving over the previous state of the art in French NER. In particular, by using both FastText embeddings (Bojanowski et al., 2017) and one of the versions of the CamemBERT French neural contextual language model (Martin et al., 2019) within an LSTM-CRF architecture, we can reach an F1-score of 90.25, a 6.5point improvement over the previously state-of-the-art system SEM (Dupont, 2017). 2. A named entity annotation layer for the UD version of the French TreeBank In this section, we describe the process whereby we realigned the named entity FTB annotations by Sagot et al. (2012) with the UD version of the FTB. This makes it possible to share these annotations in the form of a set of additional columns that can easily be pa"
2020.lrec-1.569,N19-1423,0,0.0326386,"Missing"
2020.lrec-1.569,doddington-etal-2004-automatic,0,0.0864256,"gst others). Importantly, it has long been recognised that the type of named entities can be defined in two ways, which underlies the notion of metonymy: the intrinsic type (France is always a location) and the contextual type (in la France a signé un traité ‘France signed a treaty’, France denotes an organization). NER has been an important task in natural language processing for quite some time. It was already the focus of the MUC conferences and associated shared tasks (Marsh and Perzanowski, 1998), and later that of the CoNLL 2003 and ACE shared tasks (Tjong Kim Sang and De Meulder, 2003; Doddington et al., 2004). Traditionally, as for instance was the case for the MUC shared tasks, only person names, location names, organization names, and sometimes “other proper names” are considered. However, the notion of named entity mention is sometimes extended to cover any text span that does not follow the general grammar of the language at hand, but a type- and often culturespecific grammar, thereby including entities ranging from product and brand names to dates and from URLs to monetary amounts and other types of numbers. As for many other tasks, NER was first addressed using rule-based approaches, followe"
2020.lrec-1.569,W11-0411,0,0.023484,"er tasks, NER was first addressed using rule-based approaches, followed by statistical and now neural machine learning techniques (see Section 3.1. for a brief discussion on NER approaches). Of course, evaluating NER systems as well as training machine-learningbased NER systems, statistical or neural, require namedentity-annotated corpora. Unfortunately, most named entity annotated French corpora are oral transcripts, and they are not always freely available. The ESTER and ESTER2 corpora (60 plus 150 hours of NER-annotated broadcast transcripts) (Galliano et al., 2009), as well as the Quaero (Grouin et al., 2011) corpus are based on oral transcripts (radio broadcasts). Interestingly, the Quaero corpus relies on an original, very rich and structured definition of the notion of named entity (Rosset et al., 2011). It contains both the intrinsic and the contextual types of each mention, whereas the ESTER and ESTER2 corpora only provide the contextual type. Sagot et al. (2012) describe the addition to the French Treebank (FTB) (Abeillé et al., 2003) in its FTB-UC version (Candito and Crabbé, 2009) of a new, freely available annotation layer providing named entity information in terms of span and type (NER)"
2020.lrec-1.569,N16-1030,0,0.00755553,"y statistical and now neural machine learning techniques. In addition, many systems use a lexicon of named entity mentions, usually called a “gazetteer” in this context. Most of the advances in NER have been achieved on English, in particular with the CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and Ontonotes v5 (Pradhan et al., 2012; Pradhan et al., 2013) corpora. In recent years, NER was traditionally tackled using Conditional Random Fields (CRF) (Lafferty et al., 2001) which are quite suited for NER; CRFs were later used as decoding layers for Bi-LSTM architectures (Huang et al., 2015; Lample et al., 2016) showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualized word embeddings which yet again brought major improvements to the task (Peters et al., 2018; Akbik et al., 2018). Finally, large pre-trained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures (Devlin et al., 2019; Baevski et al., 2019). For French, rule-based system have been developed until relatively recently, due to the lack of proper training data (Sekine and Nobata, 2004; Rosset et al."
2020.lrec-1.569,P10-1052,0,0.011033,"res were averaged over a 10-fold cross validation. To see why this is important for FTB-NE, see section 3.2.4.. In this section, we will compare our strong baseline with a series of neural models. We will use the two current stateof-the-art neural architectures for NER, namely seq2seq and LSTM-CRFs models. We will use various pre-trained embeddings in said architectures: fastText, CamemBERT (a French BERT-like model) and FrELMo (a French ELMo model) embeddings. 3.2.1. SEM SEM (Dupont, 2017) is a tool that relies on linear-chain CRFs (Lafferty et al., 2001) to perform tagging. SEM uses Wapiti (Lavergne et al., 2010) v1.5.0 as linear-chain CRFs implementation. SEM uses the following features for NER: • token, prefix/suffix from 1 to 5 and a Boolean isDigit features in a [-2, 2] window; • previous/next common noun in sentence; • 10 gazetteers (including NE lists and trigger words for NEs) applied with some priority rules in a [-2, 2] window; 9 https://github.com/kermitt2/grobid-ner# corpus-lemonde-ftb-french 4633 M ODEL P RECISION R ECALL F1-S CORE SEM (CRF) 87.18 80.48 83.70 LSTM-seq2seq + FastText + FastText + FrELMo + FastText + CamemBERTOSCAR-BASE-WWM + FastText + CamemBERTOSCAR-BASE-WWM + FrELMo + Fas"
2020.lrec-1.569,M98-1002,0,0.414906,"wing for instance to distinguish between the telecommunication company Orange and the town Orange in southern France (amongst others). Importantly, it has long been recognised that the type of named entities can be defined in two ways, which underlies the notion of metonymy: the intrinsic type (France is always a location) and the contextual type (in la France a signé un traité ‘France signed a treaty’, France denotes an organization). NER has been an important task in natural language processing for quite some time. It was already the focus of the MUC conferences and associated shared tasks (Marsh and Perzanowski, 1998), and later that of the CoNLL 2003 and ACE shared tasks (Tjong Kim Sang and De Meulder, 2003; Doddington et al., 2004). Traditionally, as for instance was the case for the MUC shared tasks, only person names, location names, organization names, and sometimes “other proper names” are considered. However, the notion of named entity mention is sometimes extended to cover any text span that does not follow the general grammar of the language at hand, but a type- and often culturespecific grammar, thereby including entities ranging from product and brand names to dates and from URLs to monetary amo"
2020.lrec-1.569,N18-1202,0,0.0129468,"on English, in particular with the CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and Ontonotes v5 (Pradhan et al., 2012; Pradhan et al., 2013) corpora. In recent years, NER was traditionally tackled using Conditional Random Fields (CRF) (Lafferty et al., 2001) which are quite suited for NER; CRFs were later used as decoding layers for Bi-LSTM architectures (Huang et al., 2015; Lample et al., 2016) showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualized word embeddings which yet again brought major improvements to the task (Peters et al., 2018; Akbik et al., 2018). Finally, large pre-trained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures (Devlin et al., 2019; Baevski et al., 2019). For French, rule-based system have been developed until relatively recently, due to the lack of proper training data (Sekine and Nobata, 2004; Rosset et al., 2005; Stern and Sagot, 2010; Nouvel et al., 2011). The limited availability of a few annotated corpora (cf. Section 1.) made it possible to apply statistical machine learning techniques (Bechet and Charton, 2010;"
2020.lrec-1.569,W12-4501,0,0.0334694,"unhandled ""&"", for instance) or slightly altered text (for example, a missing comma). Errors in FTB-UD were probably some XML artifacts. 3. 3.1. Benchmarking NER Models Brief state of the art of NER As mentioned above, NER was first addressed using rulebased approaches, followed by statistical and now neural machine learning techniques. In addition, many systems use a lexicon of named entity mentions, usually called a “gazetteer” in this context. Most of the advances in NER have been achieved on English, in particular with the CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and Ontonotes v5 (Pradhan et al., 2012; Pradhan et al., 2013) corpora. In recent years, NER was traditionally tackled using Conditional Random Fields (CRF) (Lafferty et al., 2001) which are quite suited for NER; CRFs were later used as decoding layers for Bi-LSTM architectures (Huang et al., 2015; Lample et al., 2016) showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualized word embeddings which yet again brought major improvements to the task (Peters et al., 2018; Akbik et al., 2018). Finally, large pre-trained architectures settled the current state of the art sho"
2020.lrec-1.569,W13-3516,0,0.0300601,"xperiments We used SEM (Dupont, 2017) as our strong baseline because, to the best of our knowledge, it was the previous state-of-the-art for named entity recognition on the FTBNE corpus. Other French NER systems are available, such as the one given by SpaCy. However, it was trained on another corpus called WikiNER, making the results noncomparable. We can also cite the system of (Stern et al., 2012). This system was trained on another newswire (AFP) using the same annotation guidelines, so the results given in this article are not directly comparable. This model was trained on FTB-NE in Stern (2013) (table C.7, page 303), but the article is written in French. The model yielded an F1-score of 0.7564, which makes it a weaker baseline than SEM. We can cite yet another NER system, namely grobidner.9 It was trained on the FTB-NE and yields an F1-score of 0.8739. Two things are to be taken into consideration: the tagset was slightly modified and scores were averaged over a 10-fold cross validation. To see why this is important for FTB-NE, see section 3.2.4.. In this section, we will compare our strong baseline with a series of neural models. We will use the two current stateof-the-art neural a"
2020.lrec-1.569,sagot-stern-2012-aleda,1,0.657457,"ly, the Quaero corpus relies on an original, very rich and structured definition of the notion of named entity (Rosset et al., 2011). It contains both the intrinsic and the contextual types of each mention, whereas the ESTER and ESTER2 corpora only provide the contextual type. Sagot et al. (2012) describe the addition to the French Treebank (FTB) (Abeillé et al., 2003) in its FTB-UC version (Candito and Crabbé, 2009) of a new, freely available annotation layer providing named entity information in terms of span and type (NER) as well as reference (NE linking), using the Wikipedia-based Aleda (Sagot and Stern, 2012) as a reference entity database. This was the first freely available French corpus annotated with referential named entity information and the first freely available such corpus for the written journalistic genre. However, this annotation is provided in the form of an XML-annotated text with sentence boundaries but no tokenization. This corpus will be referred to as FTB-NE in the rest of the article. Since the publication of that named entity FTB annotation layer, the field has evolved in many ways. Firstly, most treebanks are now available as part of the Universal Dependencies (UD)1 treebank"
2020.lrec-1.569,sekine-nobata-2004-definition,0,0.114687,"ang et al., 2015; Lample et al., 2016) showing considerable improvements over CRFs alone. These Bi-LSTM-CRF architectures were later enhanced with contextualized word embeddings which yet again brought major improvements to the task (Peters et al., 2018; Akbik et al., 2018). Finally, large pre-trained architectures settled the current state of the art showing a small yet important improvement over previous NER-specific architectures (Devlin et al., 2019; Baevski et al., 2019). For French, rule-based system have been developed until relatively recently, due to the lack of proper training data (Sekine and Nobata, 2004; Rosset et al., 2005; Stern and Sagot, 2010; Nouvel et al., 2011). The limited availability of a few annotated corpora (cf. Section 1.) made it possible to apply statistical machine learning techniques (Bechet and Charton, 2010; Dupont and Tellier, 2014; Dupont, 2017) as well as hybrid techniques combining handcrafted grammars and machine learning (Béchet et al., 2011). To the best of our knowledge, the best results previously published on FTB NER are those obtained by Dupont (2017), who trained both CRF and BiLSTM-CRF architectures and improved them using heuristics and pre-trained word embe"
2020.lrec-1.569,W12-0508,1,0.797554,"task included a German corpus (Tjong Kim Sang and De Meulder, 2003). The recent efforts by Straková et al. (2019) settled the state of the art for Spanish and Dutch, while Akbik et al. (2018) did so for German. 3.2. Experiments We used SEM (Dupont, 2017) as our strong baseline because, to the best of our knowledge, it was the previous state-of-the-art for named entity recognition on the FTBNE corpus. Other French NER systems are available, such as the one given by SpaCy. However, it was trained on another corpus called WikiNER, making the results noncomparable. We can also cite the system of (Stern et al., 2012). This system was trained on another newswire (AFP) using the same annotation guidelines, so the results given in this article are not directly comparable. This model was trained on FTB-NE in Stern (2013) (table C.7, page 303), but the article is written in French. The model yielded an F1-score of 0.7564, which makes it a weaker baseline than SEM. We can cite yet another NER system, namely grobidner.9 It was trained on the FTB-NE and yields an F1-score of 0.8739. Two things are to be taken into consideration: the tagset was slightly modified and scores were averaged over a 10-fold cross valida"
2020.lrec-1.569,P19-1527,0,0.0477841,"ing handcrafted grammars and machine learning (Béchet et al., 2011). To the best of our knowledge, the best results previously published on FTB NER are those obtained by Dupont (2017), who trained both CRF and BiLSTM-CRF architectures and improved them using heuristics and pre-trained word embeddings. We use this system as our strong baseline. Leaving aside French and English, the CoNLL 2002 shared task included NER corpora for Spanish and Dutch corpora (Tjong Kim Sang, 2002) while the CoNLL 2003 shared task included a German corpus (Tjong Kim Sang and De Meulder, 2003). The recent efforts by Straková et al. (2019) settled the state of the art for Spanish and Dutch, while Akbik et al. (2018) did so for German. 3.2. Experiments We used SEM (Dupont, 2017) as our strong baseline because, to the best of our knowledge, it was the previous state-of-the-art for named entity recognition on the FTBNE corpus. Other French NER systems are available, such as the one given by SpaCy. However, it was trained on another corpus called WikiNER, making the results noncomparable. We can also cite the system of (Stern et al., 2012). This system was trained on another newswire (AFP) using the same annotation guidelines, so t"
2020.lrec-1.569,W03-0419,0,0.416548,"Missing"
2020.lrec-1.569,W02-2024,0,0.222248,"Missing"
2020.lrec-1.577,D19-3009,1,0.911461,"Missing"
2020.lrec-1.577,W11-1601,0,0.0801056,"ication Text simplification has gained increasing interest through the years and has benefited from advances in Natural Language Processing and notably Machine Translation. In recent years, SS was largely treated as a monolingual variant of machine translation (MT), where simplification operations are learned from complex-simple sentence pairs automatically extracted from English Wikipedia and Simple English Wikipedia (Zhu et al., 2010; Wubben et al., 2012). Phrase-based and Syntax-based MT was successfully used for SS (Zhu et al., 2010) and further tailored to the task using deletion models (Coster and Kauchak, 2011) and candidate reranking (Wubben et al., 2012). The candidate reranking method by Wubben et al. (2012) favors simplifications that are most dissimilar to the source using Levenshtein distance. The authors argue that dissimilarity is a key factor of simplification. Lately, SS has mostly been tackled using Seq2Seq MT models (Sutskever et al., 2014). Seq2Seq models were either used as-is (Nisioi et al., 2017) or combined with reinforcement learning thanks to a specific simplification reward (Zhang and Lapata, 2017), augmented with an external simplification database as a dynamic memory (Zhao et a"
2020.lrec-1.577,W14-1215,0,0.06777,"ion benchmarks. Our model, which we call ACCESS (as shorthand for AudienCe-CEntric Sentence Simplification), establishes the state of the art at 41.87 SARI on the WikiLarge test set, a +1.42 improvement over the best previously reported score. Keywords: Text Simplification, Sequence-to-Sequence models, ACCESS 1. Introduction In Natural Language Processing, the Text Simplification task aims at making a text easier to read and understand. Text simplification can be beneficial for people with cognitive disabilities such as aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) and autism (Evans et al., 2014) but also for second language learners (Xia et al., 2016) and people with low literacy (Watanabe et al., 2009). The type of simplification needed for each of these audiences is different. Some aphasic patients struggle to read sentences with a high cognitive load such as long sentences with intricate syntactic structures, whereas second language learners might not understand texts with rare or specific vocabulary. Yet, research in text simplification has been mostly focused on developing models that generate a single generic simplification for a given source text with no possibility to adapt o"
2020.lrec-1.577,K18-1040,0,0.138127,"ic memory (Zhao et al., 2018) or trained with multi-tasking on entailment and paraphrase generation (Guo et al., 2018). 4689 This work builds upon Seq2Seq as well. We prepend additional inputs to the source sentences at train time, in the form of plain text special tokens. Our approach does not require any external data or modified training objective. 2.2. Controllable Text Generation Conditional training with Seq2Seq models was applied to multiple natural language processing tasks such as summarization (Kikuchi et al., 2016; Fan et al., 2017), dialog (See et al., 2019), sentence compression (Fevry and Phang, 2018; Mallinson et al., 2018) or poetry generation (Ghazvininejad et al., 2017). Most approaches for controllable text generation are either decoding-based or learning-based. Decoding-based methods Decoding-based methods use a standard Seq2Seq training setup but modify the system during decoding to control a given attribute. For instance, the length of summaries was controlled by preventing the decoder from generating the End-Of-Sentence token before reaching the desired length or by only selecting hypotheses of a given length during the beam search (Kikuchi et al., 2016). Weighted decoding (i.e."
2020.lrec-1.577,W17-4912,0,0.0414037,"Seq model on the considered attribute at train time, and can then be used to control the output at inference time. Kikuchi et al. (2016) explored learning-based methods to control the length of summaries, e.g. by feeding a target length vector to the neural network. They concluded that learning-based methods worked better than decoding-based methods and allowed finer control on the length without degrading performances. Length control was likewise used in sentence compression by feeding the network a length countdown scalar (Fevry and Phang, 2018) or a length vector (Mallinson et al., 2018). (Ficler and Goldberg, 2017) concatenate a context vector to the hidden state of each time step of their recurrent neural network decoder. This context vector represents the controlled stylistic attributes of the text, where an embedding is learnt for each attribute value. (Hu et al., 2017) achieved controlled text generation by disentangling the latent space representations of a variational auto-encoder between the text representation and its controlled attributes such as sentiment and tense. They impose the latent space structure during training by using additional discriminators. Our work uses a simpler approach: we c"
2020.lrec-1.577,P17-4008,0,0.0154976,"ment and paraphrase generation (Guo et al., 2018). 4689 This work builds upon Seq2Seq as well. We prepend additional inputs to the source sentences at train time, in the form of plain text special tokens. Our approach does not require any external data or modified training objective. 2.2. Controllable Text Generation Conditional training with Seq2Seq models was applied to multiple natural language processing tasks such as summarization (Kikuchi et al., 2016; Fan et al., 2017), dialog (See et al., 2019), sentence compression (Fevry and Phang, 2018; Mallinson et al., 2018) or poetry generation (Ghazvininejad et al., 2017). Most approaches for controllable text generation are either decoding-based or learning-based. Decoding-based methods Decoding-based methods use a standard Seq2Seq training setup but modify the system during decoding to control a given attribute. For instance, the length of summaries was controlled by preventing the decoder from generating the End-Of-Sentence token before reaching the desired length or by only selecting hypotheses of a given length during the beam search (Kikuchi et al., 2016). Weighted decoding (i.e. assigning weights to specific words during decoding) was also used with dia"
2020.lrec-1.577,C18-1039,0,0.203724,"Wubben et al. (2012) favors simplifications that are most dissimilar to the source using Levenshtein distance. The authors argue that dissimilarity is a key factor of simplification. Lately, SS has mostly been tackled using Seq2Seq MT models (Sutskever et al., 2014). Seq2Seq models were either used as-is (Nisioi et al., 2017) or combined with reinforcement learning thanks to a specific simplification reward (Zhang and Lapata, 2017), augmented with an external simplification database as a dynamic memory (Zhao et al., 2018) or trained with multi-tasking on entailment and paraphrase generation (Guo et al., 2018). 4689 This work builds upon Seq2Seq as well. We prepend additional inputs to the source sentences at train time, in the form of plain text special tokens. Our approach does not require any external data or modified training objective. 2.2. Controllable Text Generation Conditional training with Seq2Seq models was applied to multiple natural language processing tasks such as summarization (Kikuchi et al., 2016; Fan et al., 2017), dialog (See et al., 2019), sentence compression (Fevry and Phang, 2018; Mallinson et al., 2018) or poetry generation (Ghazvininejad et al., 2017). Most approaches for"
2020.lrec-1.577,P13-1151,0,0.0596419,"d preprocess using SentencePiece (Kudo and Richardson, 2018) with 10k vocabulary size to handle rare and unknown words. For generation we use beam search with a beam size of 8. 3 Training and evaluation datasets Our models are trained and evaluated on the WikiLarge dataset (Zhang and Lapata, 2017) which contains 296,402/2,000/359 samples (train/validation/test). WikiLarge is a set of automatically aligned complex-simple sentence pairs from English Wikipedia (EW) and Simple English Wikipedia (SEW). It is compiled from previous extractions of EW-SEW (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). Its validation and test sets are taken from Turkcorpus (Xu 2 We did not investigate predicting ratios on a per sentence basis as done by Scarton and Specia (2018), and leave this for future work. End-users can nonetheless choose the target ratios as they see fit, for each source sentence. 3 Code and pretrained models are released with an open-source license at https://github.com/facebookresearch/access. 4691 et al., 2016), where each complex sentence has 8 human simplifications created by Amazon Mechanical Turk workers. Human annotators were instructed to only paraphrase the source sentences"
2020.lrec-1.577,D16-1140,0,0.543181,"sed on developing models that generate a single generic simplification for a given source text with no possibility to adapt outputs for the needs of various target populations. In this paper, we propose a controllable simplification model that provides explicit ways for users to manipulate and update simplified outputs as they see fit. This work only considers the task of Sentence Simplification (SS) where the input of the model is a single source sentence and the output can be composed of one sentence or split into multiple. Our work builds upon previous work on controllable text generation (Kikuchi et al., 2016; Fan et al., 2017; Scarton and Specia, 2018; Nishihara et al., 2019) where a Sequence-toSequence (Seq2Seq) model is modified to control attributes of the output text. We tailor this mechanism to the task of SS by considering relevant attributes of the output sentence such as the output length, the amount of paraphrasing, lexical complexity, and syntactic complexity. To this end, we condition the model at train time, by feeding parameter tokens representing these attributes along with the source sentence as additional inputs. Our contributions are the following: (1) We adapt a parametrization"
2020.lrec-1.577,D18-1267,0,0.115508,"2018) or trained with multi-tasking on entailment and paraphrase generation (Guo et al., 2018). 4689 This work builds upon Seq2Seq as well. We prepend additional inputs to the source sentences at train time, in the form of plain text special tokens. Our approach does not require any external data or modified training objective. 2.2. Controllable Text Generation Conditional training with Seq2Seq models was applied to multiple natural language processing tasks such as summarization (Kikuchi et al., 2016; Fan et al., 2017), dialog (See et al., 2019), sentence compression (Fevry and Phang, 2018; Mallinson et al., 2018) or poetry generation (Ghazvininejad et al., 2017). Most approaches for controllable text generation are either decoding-based or learning-based. Decoding-based methods Decoding-based methods use a standard Seq2Seq training setup but modify the system during decoding to control a given attribute. For instance, the length of summaries was controlled by preventing the decoder from generating the End-Of-Sentence token before reaching the desired length or by only selecting hypotheses of a given length during the beam search (Kikuchi et al., 2016). Weighted decoding (i.e. assigning weights to spec"
2020.lrec-1.577,P14-1041,0,0.0947046,"Missing"
2020.lrec-1.577,P19-2036,0,0.424028,"tion for a given source text with no possibility to adapt outputs for the needs of various target populations. In this paper, we propose a controllable simplification model that provides explicit ways for users to manipulate and update simplified outputs as they see fit. This work only considers the task of Sentence Simplification (SS) where the input of the model is a single source sentence and the output can be composed of one sentence or split into multiple. Our work builds upon previous work on controllable text generation (Kikuchi et al., 2016; Fan et al., 2017; Scarton and Specia, 2018; Nishihara et al., 2019) where a Sequence-toSequence (Seq2Seq) model is modified to control attributes of the output text. We tailor this mechanism to the task of SS by considering relevant attributes of the output sentence such as the output length, the amount of paraphrasing, lexical complexity, and syntactic complexity. To this end, we condition the model at train time, by feeding parameter tokens representing these attributes along with the source sentence as additional inputs. Our contributions are the following: (1) We adapt a parametrization mechanism to the specific task of Sentence Simplification by conditio"
2020.lrec-1.577,P17-2014,0,0.232541,"Missing"
2020.lrec-1.577,N19-4009,0,0.0706498,"Missing"
2020.lrec-1.577,P02-1040,0,0.107131,"SS. Plain text special tokens were used to encode attributes such as the target school grade-level • Paraphrasing: Paraphrasing is an important aspect for good text simplification systems (Wubben et al., 2012), especially because it allows the user from choosing if he prefers very safe simplifications (i.e. close to the source) or to try and simplify the input more at the cost of more mistakes when using imperfect systems. The amount of paraphrasing was also shown to correlate with human jugdment of meaning preservation and simplicity sometimes even more than traditional metrics such as BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016). • Lexical and Syntactic complexity: (Shardlow, 2014) identified lexical simplification and syntactic simplification as core components of SS systems, which often decomposes there approach into these two subcomponents. Audiences also have different simplification needs along these two attributes. In order to understand a text correctly, second language learner will require a text with less complicated words. On the other hand, some specific types of aphasia will make people struggle more with complex syntactic structures, intricated clauses, and long sentence, thus"
2020.lrec-1.577,P16-2024,0,0.0509818,"ng, combined with a lexical simplification model. Pointer+Ent+Par (Guo et al., 2018) Seq2Seq model based on the pointer-copy mechanism and trained via multi-task learning on the Entailment and Paraphrase Generation tasks. NTS+SARI (Nisioi et al., 2017) Standard Seq2Seq model. The second beam search hypothesis is selected during decoding; the hypothesis number is an hyper-parameter fine-tuned with SARI. NSELSTM-S (Vu et al., 2018) Seq2Seq with a memory-augmented Neural Semantic Encoder, tuned with SARI. DMASS+DCSS (Zhao et al., 2018) Seq2Seq integrating the simple PPDB simplification database (Pavlick and Callison-Burch, 2016) as a dynamic memory. The database is also used to modify the loss and re-weight word probabilities to favor simpler words. We select the model with the best SARI on the validation set and report its score on the test set. This model uses three 4692 Figure 1: Density distribution of the compression ratios between the source sentence and the target sentence. The automatically aligned pairs from WikiLarge train set are spread (red) while human simplifications from the validation and test set (green) are gathered together with a mean ratio of 0.93 (i.e. nearly no compression). parameter tokens ou"
2020.lrec-1.577,P15-2070,0,0.0568582,"Missing"
2020.lrec-1.577,P18-2113,0,0.787515,"single generic simplification for a given source text with no possibility to adapt outputs for the needs of various target populations. In this paper, we propose a controllable simplification model that provides explicit ways for users to manipulate and update simplified outputs as they see fit. This work only considers the task of Sentence Simplification (SS) where the input of the model is a single source sentence and the output can be composed of one sentence or split into multiple. Our work builds upon previous work on controllable text generation (Kikuchi et al., 2016; Fan et al., 2017; Scarton and Specia, 2018; Nishihara et al., 2019) where a Sequence-toSequence (Seq2Seq) model is modified to control attributes of the output text. We tailor this mechanism to the task of SS by considering relevant attributes of the output sentence such as the output length, the amount of paraphrasing, lexical complexity, and syntactic complexity. To this end, we condition the model at train time, by feeding parameter tokens representing these attributes along with the source sentence as additional inputs. Our contributions are the following: (1) We adapt a parametrization mechanism to the specific task of Sentence S"
2020.lrec-1.577,N19-1170,0,0.0475922,"Missing"
2020.lrec-1.577,N16-1005,0,0.0785006,"r each attribute value. (Hu et al., 2017) achieved controlled text generation by disentangling the latent space representations of a variational auto-encoder between the text representation and its controlled attributes such as sentiment and tense. They impose the latent space structure during training by using additional discriminators. Our work uses a simpler approach: we condition the generation process by concatenating plain text special tokens to the source text. This method only modifies the source data and not the training procedure. Such mechanism was used to control politeness in MT (Sennrich et al., 2016), to control summaries in terms of length, of news source style, or to make the summary more focused on a given named entity (Fan et al., 2017). Scarton and Specia (2018) and Nishihara et al. (2019) similarly showed that adding special tokens at the beginning of sentences can improve the performance of Seq2Seq models for SS. Plain text special tokens were used to encode attributes such as the target school grade-level • Paraphrasing: Paraphrasing is an important aspect for good text simplification systems (Wubben et al., 2012), especially because it allows the user from choosing if he prefers"
2020.lrec-1.577,D18-1081,0,0.198008,"Missing"
2020.lrec-1.577,N18-2013,0,0.341706,"Missing"
2020.lrec-1.577,D11-1038,0,0.147036,"the NLTK NIST tokenizer and preprocess using SentencePiece (Kudo and Richardson, 2018) with 10k vocabulary size to handle rare and unknown words. For generation we use beam search with a beam size of 8. 3 Training and evaluation datasets Our models are trained and evaluated on the WikiLarge dataset (Zhang and Lapata, 2017) which contains 296,402/2,000/359 samples (train/validation/test). WikiLarge is a set of automatically aligned complex-simple sentence pairs from English Wikipedia (EW) and Simple English Wikipedia (SEW). It is compiled from previous extractions of EW-SEW (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). Its validation and test sets are taken from Turkcorpus (Xu 2 We did not investigate predicting ratios on a per sentence basis as done by Scarton and Specia (2018), and leave this for future work. End-users can nonetheless choose the target ratios as they see fit, for each source sentence. 3 Code and pretrained models are released with an open-source license at https://github.com/facebookresearch/access. 4691 et al., 2016), where each complex sentence has 8 human simplifications created by Amazon Mechanical Turk workers. Human annotators were instructed to only paraphrase the"
2020.lrec-1.577,P12-1107,0,0.656872,"Missing"
2020.lrec-1.577,W16-0502,0,0.0231397,"nd for AudienCe-CEntric Sentence Simplification), establishes the state of the art at 41.87 SARI on the WikiLarge test set, a +1.42 improvement over the best previously reported score. Keywords: Text Simplification, Sequence-to-Sequence models, ACCESS 1. Introduction In Natural Language Processing, the Text Simplification task aims at making a text easier to read and understand. Text simplification can be beneficial for people with cognitive disabilities such as aphasia (Carroll et al., 1998), dyslexia (Rello et al., 2013) and autism (Evans et al., 2014) but also for second language learners (Xia et al., 2016) and people with low literacy (Watanabe et al., 2009). The type of simplification needed for each of these audiences is different. Some aphasic patients struggle to read sentences with a high cognitive load such as long sentences with intricate syntactic structures, whereas second language learners might not understand texts with rare or specific vocabulary. Yet, research in text simplification has been mostly focused on developing models that generate a single generic simplification for a given source text with no possibility to adapt outputs for the needs of various target populations. In th"
2020.lrec-1.577,Q16-1029,0,0.106759,"these attributes along with the source sentence as additional inputs. Our contributions are the following: (1) We adapt a parametrization mechanism to the specific task of Sentence Simplification by conditioning on relevant attributes; (2) We show through a detailed analysis that our model can indeed control the considered attributes, making the simplifications potentially able to fit the needs of various end audiences; (3) With careful calibration, our controllable parametrization improves the performance of out-of-thebox Seq2Seq models leading to a new state-of-the-art score of 41.87 SARI (Xu et al., 2016) on the WikiLarge benchmark (Zhang and Lapata, 2017), a +1.42 gain over previous scores, without requiring any external resource or modified training objective. 2. 2.1. Related Work Sentence Simplification Text simplification has gained increasing interest through the years and has benefited from advances in Natural Language Processing and notably Machine Translation. In recent years, SS was largely treated as a monolingual variant of machine translation (MT), where simplification operations are learned from complex-simple sentence pairs automatically extracted from English Wikipedia and Simpl"
2020.lrec-1.577,D17-1062,0,0.48392,"nce as additional inputs. Our contributions are the following: (1) We adapt a parametrization mechanism to the specific task of Sentence Simplification by conditioning on relevant attributes; (2) We show through a detailed analysis that our model can indeed control the considered attributes, making the simplifications potentially able to fit the needs of various end audiences; (3) With careful calibration, our controllable parametrization improves the performance of out-of-thebox Seq2Seq models leading to a new state-of-the-art score of 41.87 SARI (Xu et al., 2016) on the WikiLarge benchmark (Zhang and Lapata, 2017), a +1.42 gain over previous scores, without requiring any external resource or modified training objective. 2. 2.1. Related Work Sentence Simplification Text simplification has gained increasing interest through the years and has benefited from advances in Natural Language Processing and notably Machine Translation. In recent years, SS was largely treated as a monolingual variant of machine translation (MT), where simplification operations are learned from complex-simple sentence pairs automatically extracted from English Wikipedia and Simple English Wikipedia (Zhu et al., 2010; Wubben et al."
2020.lrec-1.577,D18-1355,0,0.646117,"ak, 2011) and candidate reranking (Wubben et al., 2012). The candidate reranking method by Wubben et al. (2012) favors simplifications that are most dissimilar to the source using Levenshtein distance. The authors argue that dissimilarity is a key factor of simplification. Lately, SS has mostly been tackled using Seq2Seq MT models (Sutskever et al., 2014). Seq2Seq models were either used as-is (Nisioi et al., 2017) or combined with reinforcement learning thanks to a specific simplification reward (Zhang and Lapata, 2017), augmented with an external simplification database as a dynamic memory (Zhao et al., 2018) or trained with multi-tasking on entailment and paraphrase generation (Guo et al., 2018). 4689 This work builds upon Seq2Seq as well. We prepend additional inputs to the source sentences at train time, in the form of plain text special tokens. Our approach does not require any external data or modified training objective. 2.2. Controllable Text Generation Conditional training with Seq2Seq models was applied to multiple natural language processing tasks such as summarization (Kikuchi et al., 2016; Fan et al., 2017), dialog (See et al., 2019), sentence compression (Fevry and Phang, 2018; Mallin"
2020.lrec-1.577,C10-1152,0,0.572576,"chmark (Zhang and Lapata, 2017), a +1.42 gain over previous scores, without requiring any external resource or modified training objective. 2. 2.1. Related Work Sentence Simplification Text simplification has gained increasing interest through the years and has benefited from advances in Natural Language Processing and notably Machine Translation. In recent years, SS was largely treated as a monolingual variant of machine translation (MT), where simplification operations are learned from complex-simple sentence pairs automatically extracted from English Wikipedia and Simple English Wikipedia (Zhu et al., 2010; Wubben et al., 2012). Phrase-based and Syntax-based MT was successfully used for SS (Zhu et al., 2010) and further tailored to the task using deletion models (Coster and Kauchak, 2011) and candidate reranking (Wubben et al., 2012). The candidate reranking method by Wubben et al. (2012) favors simplifications that are most dissimilar to the source using Levenshtein distance. The authors argue that dissimilarity is a key factor of simplification. Lately, SS has mostly been tackled using Seq2Seq MT models (Sutskever et al., 2014). Seq2Seq models were either used as-is (Nisioi et al., 2017) or c"
2020.lrec-1.577,Q15-1021,0,0.245796,"Missing"
2020.lt4hala-1.12,P07-2045,0,0.0123059,"Missing"
2020.lt4hala-1.12,J19-1004,0,0.0142786,"stor, two words are said to be cognates if they are an evolution of the same word from said ancestor, called their proto-form.2,3 Therefore, the phonological differences between two cognates, which can be modelled as a sequence of sound correspondences, capture some of the differences between the phonetic evolution of the languages. Most methods for sound correspondences identification start by aligning sequences of characters or phones, to which they then apply statistical models, clustering methods, or both (Mann and Yarowsky, 2001; Inkpen et al., 2005; List et al., 2017; List et al., 2018; List, 2019) with the notable exception of Mulloni (2007), who uses Support Vector Machines. However, this task presents a number of similarities with machine translation (MT), as they 2. 2.1. Data Artificial Data Creation In order to compare how both model types perform on the task of sound correspondence learning in an ideal setup, we create an artificial lexicon, composed of a proto-language and its reflect in two artificially defined daughter languages. Using artificial data for such a proof of concept offers several advantages: we can investigate the minimum number 1 For example, the sequence [ka] in"
2020.lt4hala-1.12,D15-1166,0,0.177851,"ely Latin, Italian and Spanish, to see if those performances generalise well. We show that both model types manage to learn sound changes despite data scarcity, although the best performing model type depends on several parameters such as the size of the training data, the ambiguity, and the prediction direction. Keywords: Cognate prediction, Proto-form prediction, Statistical models, Neural models 1. Introduction both involve modelling sequence-to-sequence cross-lingual correspondences,4 yet state-of-the-art neural network techniques used in MT (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015) have only been used once for sound correspondence prediction, with disappointing results (Dekker, 2018). Our goal in this paper is to study under which conditions either a neural network or a statistical model performs best to learn sound changes between languages, given the usually limited available training data.5 We first compare the performances of these two types of models in an ideal setting. To do that, we generate an artificial phonetised trilingual lexicon between a proto-language and two daughter languages, use it to train each model with varying hyperparameters and compare the resu"
2020.lt4hala-1.12,N01-1020,0,0.244471,"are usually identified by studying cognates: given two languages with a common ancestor, two words are said to be cognates if they are an evolution of the same word from said ancestor, called their proto-form.2,3 Therefore, the phonological differences between two cognates, which can be modelled as a sequence of sound correspondences, capture some of the differences between the phonetic evolution of the languages. Most methods for sound correspondences identification start by aligning sequences of characters or phones, to which they then apply statistical models, clustering methods, or both (Mann and Yarowsky, 2001; Inkpen et al., 2005; List et al., 2017; List et al., 2018; List, 2019) with the notable exception of Mulloni (2007), who uses Support Vector Machines. However, this task presents a number of similarities with machine translation (MT), as they 2. 2.1. Data Artificial Data Creation In order to compare how both model types perform on the task of sound correspondence learning in an ideal setup, we create an artificial lexicon, composed of a proto-language and its reflect in two artificially defined daughter languages. Using artificial data for such a proof of concept offers several advantages: w"
2020.lt4hala-1.12,P07-3005,0,0.0547781,"they are an evolution of the same word from said ancestor, called their proto-form.2,3 Therefore, the phonological differences between two cognates, which can be modelled as a sequence of sound correspondences, capture some of the differences between the phonetic evolution of the languages. Most methods for sound correspondences identification start by aligning sequences of characters or phones, to which they then apply statistical models, clustering methods, or both (Mann and Yarowsky, 2001; Inkpen et al., 2005; List et al., 2017; List et al., 2018; List, 2019) with the notable exception of Mulloni (2007), who uses Support Vector Machines. However, this task presents a number of similarities with machine translation (MT), as they 2. 2.1. Data Artificial Data Creation In order to compare how both model types perform on the task of sound correspondence learning in an ideal setup, we create an artificial lexicon, composed of a proto-language and its reflect in two artificially defined daughter languages. Using artificial data for such a proof of concept offers several advantages: we can investigate the minimum number 1 For example, the sequence [ka] in Vulgar Latin changed into [Ùa] in Old French"
2020.lt4hala-1.12,J03-1002,0,0.0574738,"Missing"
2020.lt4hala-1.12,W18-6319,0,0.0123228,"lexicon. MEDeA learns a single model for all possible language pairs, on 50 epochs. We train it with hidden dimensions of 12, 25, 37, and 50, training set sizes of 500, 1000, 1500, 11 Code available at https://github.com/clefourrier/MEDeA For example, for a bilingual Spanish-Italian lexicon, the model will learn on Spanish to itself, Italian to itself, Spanish to Italian and vice versa. 13 The embedding size was chosen in preliminary experiments, and was the best choice between 2, 5 and 10. This seems adequate relative to the total vocabulary size, of less than 100 items 14 We use SacreBLEU, Post (2018)’s implementation 12 15 Results obtained with a data size 500 skew the average considerably, being several standard deviations apart from the others, for reasons discussed in Section 4.2., and were thus removed. 81 (a) 1-best (b) 2-best (c) 3-best Figure 3: BLEU scores for the n-best prediction, for all experiments. MOSES is trained on pairs of language combinations separately. We provide it with the same data splits, with the exception of monolingual data, removed from its training set. The triplets of manually corrected data is treated as several sets of pairs, for the same reasons. Along th"
2020.lt4hala-1.12,2020.lrec-1.392,1,0.65492,"Missing"
2020.lt4hala-1.12,W11-2123,0,0.0290533,"Missing"
2021.eacl-main.189,2020.acl-main.692,0,0.0213185,"rops when the evaluation is done on a distinct language (e.g. -5.82 when evaluated on French). The trends are similar for all the domains and tasks we tested on. We conclude that the pretrained parameters at the lower layers are consistently more critical for cross-language transfer than for same-language transfer, and cannot be explained by the possibly different domain of the evaluated datasets. 3 Although other factors might play a part in out-ofdistribution, we suspect that domains plays a crucial part in transfer. Moreover, it was shown that BERT encodes out-ofthe-box domain information (Aharoni and Goldberg, 2020) 2216 Domain Analyses EN - EN EN - NEWS Cross-Language EN - FR ∆0-1 90.40 77.91 75.77 45.90 -1.41 -0.91 -2.14 -1.97 RANDOM-INIT of layers ∆2-3 ∆4-5 ∆6-7 ∆8-9 Parsing -2.33 -1.57 -1.43 -0.60 -1.38 -1.85 -0.83 -0.23 -2.42 -2.54 -1.42 -0.71 -2.75 -2.10 -1.04 -0.39 83.25 71.29 -5.82 -7.86 -2.69 -4.33 -2.42 -4.64 -0.44 -0.92 0.25 -0.11 0.94 0.33 96.83 93.09 89.67 68.93 -1.35 -0.58 -1.07 -2.38 -0.98 -0.65 -1.21 -1.07 -0.70 -0.28 -0.41 -0.14 POS -0.40 -0.04 -0.10 0.54 -0.28 -0.06 0.03 -0.04 -0.24 0.12 0.21 0.63 93.43 91.13 -3.59 -5.10 -0.88 -0.93 -1.31 -1.16 -0.56 -0.74 0.46 0.15 0.25 -0.07 -0.15 -0."
2021.eacl-main.189,2020.acl-tutorials.1,0,0.0184177,"raining. Focusing on syntax, Chi et al. (2020) recently showed that the multilingual version of BERT (mBERT) (Devlin et al., 2019), encodes linguistic properties in shared multilingual sub-spaces. Recently, Gonen et al. (2020) suggest that mBERT learns a language encoding component and an abstract cross-lingual component. In this work, we are interested in understanding the mechanism that leads mBERT to perform zero-shot cross-lingual transfer. More specifically, we ask what parts of the model and what mechanisms support cross-lingual transfer? By combining behavioral and structural analyses (Belinkov et al., 2020), we show that mBERT operates as the stacking of two modules: (1) A multilingual encoder, located in the lower part of the model, critical for cross-lingual transfer, is in charge of 2214 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2214–2231 April 19 - 23, 2021. ©2021 Association for Computational Linguistics aligning multilingual representations; and (2) a task-specific, language-agnostic predictor which has little importance for cross-lingual transfer and is dedicated to performing the downstream task. This mechanism that"
2021.eacl-main.189,D15-1075,0,0.044925,"ne-tuned model on NER with and w/o RANDOM-INIT between English (source) and Russian (target). The higher the score the greater the similarity. less of the language and task, and hints on an alignment that occurs in the lower part of the model. Interestingly, the same trend is also observed in the pretrained model, suggesting that the fine-tuning step preserves the multilingual alignment. These results do not match the findings of Singh et al. (2019), who found no language alignment across layers, although they inspected Natural Language Inference, a more “high-level task” (Dagan et al., 2005; Bowman et al., 2015). We leave the inspection of this mismatch to future work. 4.3 Better Alignment Leads to Better Cross-Lingual Transfer In the previous section we showed that fine-tuned models align the representations between parallel sentences, across languages. Moreover, we demonstrated that the lower part of the model is critical for cross-language transfer but hardly impacts the same-language performance. In this section, we show that the alignment measured plays a critical role in cross-lingual transfer. As seen in Figure 2 in the case of English to Russian (and in Figures 6-8 in the Appendix for other l"
2021.eacl-main.189,2020.acl-main.493,0,0.0195689,"ion of monolingual corpora across multiple languages, (ii) fine-tuning the model on a specific task in the source language, and (iii) using the finetuned model on a target language. The success of this approach is remarkable, and in contrast to the standard cross-lingual pipeline, the model sees neither aligned data nor task-specific annotated data in the target language at any training stage. The source of such a successful transfer is still largely unexplained. Pires et al. (2019) hypothesize that these models learn shared multilingual representations during pretraining. Focusing on syntax, Chi et al. (2020) recently showed that the multilingual version of BERT (mBERT) (Devlin et al., 2019), encodes linguistic properties in shared multilingual sub-spaces. Recently, Gonen et al. (2020) suggest that mBERT learns a language encoding component and an abstract cross-lingual component. In this work, we are interested in understanding the mechanism that leads mBERT to perform zero-shot cross-lingual transfer. More specifically, we ask what parts of the model and what mechanisms support cross-lingual transfer? By combining behavioral and structural analyses (Belinkov et al., 2020), we show that mBERT ope"
2021.eacl-main.189,P18-1198,0,0.0250893,"elates significantly with the cross-lang gap for all three tasks, both on the fine-tuned and pretrained models. The spearman correlation for the fine-tuned models are 0.76, 0.75 and 0.47 for parsing, POS and NER, respectively.5 In summary, our results show that the cross-lingual alignment is highly correlated with the cross-lingual transfer. 5 Discussion Understanding the behavior of pretrained language models is currently a fundamental challenge in NLP. A popular approach consists of probing the intermediate representations with external classifiers (Alain and Bengio, 2017; Adi et al., 2017; Conneau et al., 2018) to measure if a specific layer captures a given property. Using this technique, Tenney et al. (2019) showed that BERT encodes linguistic properties in the same order as the “classical NLP pipeline”. However, probing techniques only indirectly explain the behavior of a model and do not explain the relationship between the information captured in the representations and its effect on the task (Elazar et al., 2020). Moreover, recent works have questioned the usage of probing as an interpretation tool (Hewitt and Liang, 2019; Ravichander et al., 2020). This motivates our approach to combine a str"
2021.eacl-main.189,D19-6106,0,0.0254583,"andom-Init 4-7 Random-Init 8-11 Random-Init 0-3 2 4 6 *52.03 *34.66 8 Hidden Layer Index 10 12 Figure 2: Cross-Lingual similarity (CKA) of the representations of a fine-tuned model on NER with and w/o RANDOM-INIT between English (source) and Russian (target). The higher the score the greater the similarity. less of the language and task, and hints on an alignment that occurs in the lower part of the model. Interestingly, the same trend is also observed in the pretrained model, suggesting that the fine-tuning step preserves the multilingual alignment. These results do not match the findings of Singh et al. (2019), who found no language alignment across layers, although they inspected Natural Language Inference, a more “high-level task” (Dagan et al., 2005; Bowman et al., 2015). We leave the inspection of this mismatch to future work. 4.3 Better Alignment Leads to Better Cross-Lingual Transfer In the previous section we showed that fine-tuned models align the representations between parallel sentences, across languages. Moreover, we demonstrated that the lower part of the model is critical for cross-language transfer but hardly impacts the same-language performance. In this section, we show that the al"
2021.eacl-main.189,P11-2120,0,0.0231124,"lly diverse languages and multiple domains to support our hypothesis. 1 Introduction Zero-shot Cross-Lingual transfer aims at building models for a target language by reusing knowledge acquired from a source language. Historically, it has been tackled with a two-step standard crosslingual pipeline (Ruder et al., 2019): (1) Building a shared multilingual representation of text, typically by aligning textual representations across languages. This step can be done using feature extraction (Aone and McKee, 1993; Schultz and Waibel, 2001) as with the delexicalized approach (Zeman and Resnik, 2008; Søgaard, 2011) or using word embedding techniques (Mikolov et al., 2013; Smith et al., 2017) by projecting monolingual embeddings onto a shared multilingual embedding space, this step requiring explicit supervision signal in the target language in the form of features or parallel data. (2) Training a task-specific model using supervision on a source language on top of the shared representation. Recently, the rise of multilingual language models entailed a paradigm shift in this field. Multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019) have been shown to perform efficient"
2021.eacl-main.189,D19-1077,0,0.0323671,"ojecting monolingual embeddings onto a shared multilingual embedding space, this step requiring explicit supervision signal in the target language in the form of features or parallel data. (2) Training a task-specific model using supervision on a source language on top of the shared representation. Recently, the rise of multilingual language models entailed a paradigm shift in this field. Multilingual pretrained language models (Devlin et al., 2019; Conneau and Lample, 2019) have been shown to perform efficient zero-shot cross-lingual transfer for many tasks and languages (Pires et al., 2019; Wu and Dredze, 2019). Such transfer relies on three-steps: (i) pretraining a mask-language model (e.g. Devlin et al. (2019)) on the concatenation of monolingual corpora across multiple languages, (ii) fine-tuning the model on a specific task in the source language, and (iii) using the finetuned model on a target language. The success of this approach is remarkable, and in contrast to the standard cross-lingual pipeline, the model sees neither aligned data nor task-specific annotated data in the target language at any training stage. The source of such a successful transfer is still largely unexplained. Pires et a"
2021.findings-acl.75,I13-1112,0,0.106659,"Inkpen, 2006, 2009; Hauer and Kondrak, 2011; Dinu and Ciobanu, 2014) and neural networks only (Ciobanu and Dinu, 2014; Rama, 2016; Kumar et al., 2017; Soisalon-Soininen and Granroth-Wilding, 2019). Automatic cognate prediction is less studied despite its interesting applications, such as predicting plausible new cognates to help field linguists (Bodt et al., 2018) and inducing translation lexicons (Mann and Yarowsky, 2001). In the last few years, it has been approached as an MT task, as it can be seen as modelling sequence-to-sequence correspondences. Using neural networks has been promising (Beinborn et al., 2013; Wu and Yarowsky, 2018; Dekker, 2018; Hämäläinen and Rueter, 2019; Fourrier and Sagot, 2020a), although in most works the hyper-parameters of the neural models were not optimised. Moreover, the differences between MT and cognate prediction have not been studied. In this paper, we choose to study the application of MT approaches to the cognate prediction task. Our aim is to investigate whether the task can benefit from techniques commonly seen to improve standard low-resource MT. We first highlight the specific characteristics of cognate prediction, and (to our knowledge) provide the first det"
2021.findings-acl.75,W09-0432,0,0.0536348,"ral techniques are commonly used in lowresource MT to mitigate the lack of parallel data: monolingual pretraining, backtranslation and using data from additional languages. Monolingual pretraining (unsupervised) has, as in other NLP tasks, been highly beneficial to MT (Song et al., 2019; Conneau and Lample, 2019; Devlin et al., 2019; Liu et al., 2020). Before training on a translation task, model parameters are first pretrained using a language modelling objective, which enables the exploitation of monolingual data, more freely available than bilingual data. Backtranslation originated in SMT (Bertoldi and Federico, 2009; Bojar and Tamchyna, 2011), and has been standard in NMT for several years (Sennrich et al., 2016; Edunov et al., 2018). Its goal is to artificially create larger quantities of parallel data from monolingual datasets, which are often more readily available. Target-side monolingual data is provided to a bilingual model trained in the opposite direction (target-to-source), which produces synthetic source-side data. The data is then filtered to keep the highest quality sentences. The newly generated dataset, made of synthetic source-side data parallel to real target-side data is then combined wi"
2021.findings-acl.75,N09-1008,0,0.0407697,"ata from extra languages. However, using extra monolingual data via backtranslation or pretraining is not always as beneficial as it is in standard MT settings.2 2 Related Work 2.1 Cognate Prediction Cognate prediction is the task that aims to produce from words in a source language plausible cognates in a target language (according to the aforementioned definition of cognates). It is a lexical task that models regular, word-internal sound changes that transform words over time. It has been approached with phylogenetic trees combined with stochastic sound change models (Bouchard et al., 2007; Bouchard-Côté et al., 2009; Bouchard-Côté et al., 2013), purely statistical methods (Bodt et al., 2018), neural networks (Mulloni, 2007), language models (Hauer et al., 2019) and character-level MT techniques (Beinborn et al., 2013; Wu and Yarowsky, 2018; Dekker, 2018; Hämäläinen and Rueter, 2019; Fourrier and Sagot, 2020a; Meloni et al., 2021), because of its similarity to a translation task (modelling sequence-to-sequence crosslingual correspondences between words). 2.2 Low-resource MT Since data is scarce, we postulate that cognate prediction could benefit from low-resource MT settings techniques and architectural c"
2021.findings-acl.75,P14-2017,0,0.0202342,"of historical linguistics. Over the last three decades, automatic cognate identification has benefited from advances in computational techniques, first using dictionary-based methods (Dinu and Ciobanu, 2014) and purely statistical methods (Mitkov et al., 2007; McCoy and Frank, 2018), then statistical methods combined with clustering algorithms (Hall and Klein, 2010, 2011; List et al., 2017; St Arnaud et al., 2017), statistical methods combined with neural classifiers (Inkpen et al., 2005; Frunza and Inkpen, 2006, 2009; Hauer and Kondrak, 2011; Dinu and Ciobanu, 2014) and neural networks only (Ciobanu and Dinu, 2014; Rama, 2016; Kumar et al., 2017; Soisalon-Soininen and Granroth-Wilding, 2019). Automatic cognate prediction is less studied despite its interesting applications, such as predicting plausible new cognates to help field linguists (Bodt et al., 2018) and inducing translation lexicons (Mann and Yarowsky, 2001). In the last few years, it has been approached as an MT task, as it can be seen as modelling sequence-to-sequence correspondences. Using neural networks has been promising (Beinborn et al., 2013; Wu and Yarowsky, 2018; Dekker, 2018; Hämäläinen and Rueter, 2019; Fourrier and Sagot, 2020a),"
2021.findings-acl.75,P10-1105,0,0.0400699,"en and Rueter, 2019). 847 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 847–861 August 1–6, 2021. ©2021 Association for Computational Linguistics likely cognates in related languages) are two of the fundamental tasks of historical linguistics. Over the last three decades, automatic cognate identification has benefited from advances in computational techniques, first using dictionary-based methods (Dinu and Ciobanu, 2014) and purely statistical methods (Mitkov et al., 2007; McCoy and Frank, 2018), then statistical methods combined with clustering algorithms (Hall and Klein, 2010, 2011; List et al., 2017; St Arnaud et al., 2017), statistical methods combined with neural classifiers (Inkpen et al., 2005; Frunza and Inkpen, 2006, 2009; Hauer and Kondrak, 2011; Dinu and Ciobanu, 2014) and neural networks only (Ciobanu and Dinu, 2014; Rama, 2016; Kumar et al., 2017; Soisalon-Soininen and Granroth-Wilding, 2019). Automatic cognate prediction is less studied despite its interesting applications, such as predicting plausible new cognates to help field linguists (Bodt et al., 2018) and inducing translation lexicons (Mann and Yarowsky, 2001). In the last few years, it has been"
2021.findings-acl.75,D11-1032,0,0.0707092,"Missing"
2021.findings-acl.75,W19-6006,0,0.128881,"changes are usually identified by looking at the attested (or hypothesised) phonetic form Figure 1: Related words in Italian and Spanish, both the outcome of Latin bonus ‘good’. Plain arrows represent inheritance, dotted arrows derivation. “?” indicates that the word is not present in our database. Cognate identification (finding cognate pairs in a multilingual word set) and prediction (producing 1 Cognates have for example been defined as words sharing spelling and meaning, regardless of their etymology (Frunza and Inkpen, 2006, 2009), or words etymologically related no matter the relation (Hämäläinen and Rueter, 2019). 847 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 847–861 August 1–6, 2021. ©2021 Association for Computational Linguistics likely cognates in related languages) are two of the fundamental tasks of historical linguistics. Over the last three decades, automatic cognate identification has benefited from advances in computational techniques, first using dictionary-based methods (Dinu and Ciobanu, 2014) and purely statistical methods (Mitkov et al., 2007; McCoy and Frank, 2018), then statistical methods combined with clustering algorithms (Hall and Klein, 2010"
2021.findings-acl.75,hanoka-sagot-2014-open,1,0.87215,"Missing"
2021.findings-acl.75,W19-4202,0,0.0203892,"ings.2 2 Related Work 2.1 Cognate Prediction Cognate prediction is the task that aims to produce from words in a source language plausible cognates in a target language (according to the aforementioned definition of cognates). It is a lexical task that models regular, word-internal sound changes that transform words over time. It has been approached with phylogenetic trees combined with stochastic sound change models (Bouchard et al., 2007; Bouchard-Côté et al., 2009; Bouchard-Côté et al., 2013), purely statistical methods (Bodt et al., 2018), neural networks (Mulloni, 2007), language models (Hauer et al., 2019) and character-level MT techniques (Beinborn et al., 2013; Wu and Yarowsky, 2018; Dekker, 2018; Hämäläinen and Rueter, 2019; Fourrier and Sagot, 2020a; Meloni et al., 2021), because of its similarity to a translation task (modelling sequence-to-sequence crosslingual correspondences between words). 2.2 Low-resource MT Since data is scarce, we postulate that cognate prediction could benefit from low-resource MT settings techniques and architectural choices. 2.2.1 Architecture Comparison Several papers comparing SMT with NMT (recurrent neural networks (RNNs) with attention) in low-resource settin"
2021.findings-acl.75,I11-1097,0,0.0323282,"ics likely cognates in related languages) are two of the fundamental tasks of historical linguistics. Over the last three decades, automatic cognate identification has benefited from advances in computational techniques, first using dictionary-based methods (Dinu and Ciobanu, 2014) and purely statistical methods (Mitkov et al., 2007; McCoy and Frank, 2018), then statistical methods combined with clustering algorithms (Hall and Klein, 2010, 2011; List et al., 2017; St Arnaud et al., 2017), statistical methods combined with neural classifiers (Inkpen et al., 2005; Frunza and Inkpen, 2006, 2009; Hauer and Kondrak, 2011; Dinu and Ciobanu, 2014) and neural networks only (Ciobanu and Dinu, 2014; Rama, 2016; Kumar et al., 2017; Soisalon-Soininen and Granroth-Wilding, 2019). Automatic cognate prediction is less studied despite its interesting applications, such as predicting plausible new cognates to help field linguists (Bodt et al., 2018) and inducing translation lexicons (Mann and Yarowsky, 2001). In the last few years, it has been approached as an MT task, as it can be seen as modelling sequence-to-sequence correspondences. Using neural networks has been promising (Beinborn et al., 2013; Wu and Yarowsky, 201"
2021.findings-acl.75,W11-2123,0,0.035417,"{10, 30, 65, 100} {8, 12, 16, 20, 24} × {18, 36, 54, 72} 1, 2, 4 1, 2, 3, 4 None, Bahdanau, Luong (dot, concat, general) 2) Embed. dim. × Hidden dim. 3) Number of layers 4) Number of heads 4) Attention type Table 2: Parameter exploration experiments for NMT models. In bold, the initial parameters at each step. MT Architectures 4.2.1 SMT We train a separate SMT model for each language direction using the M OSES toolkit (Koehn et al., 2007). Our bilingual training data is aligned with GIZA++ (Och and Ney, 2003). The target data for the pair is used to train a 3-gram language model using KenLM (Heafield, 2011). We tune our models using MERT based on BLEU on the dev set. Parameters Table 2 contains the successive parameter exploration steps: at the end of a step, we automatically selected (according to average dev BLEU) the step-best value, used as input parameter for the next parameter exploration step.7 The final best parameters are given in Appendix A.1. Smaller learning rates (0.005 and 0.001) are better, while there is no observable pattern to the best batch sizes or numbers of layers. Interestingly, however, for the RNNs, the best results are obtained with the highest hidden dimension irrespec"
2021.findings-acl.75,P07-2045,0,0.0114144,"using the Adam optimiser (Kingma and Ba, 2015), the cross-entropy loss, and dev BLEU as selection criterion. Values studied 1) Learning rate × Batch size {0.01, 0.05, 0.001} × {10, 30, 65, 100} {8, 12, 16, 20, 24} × {18, 36, 54, 72} 1, 2, 4 1, 2, 3, 4 None, Bahdanau, Luong (dot, concat, general) 2) Embed. dim. × Hidden dim. 3) Number of layers 4) Number of heads 4) Attention type Table 2: Parameter exploration experiments for NMT models. In bold, the initial parameters at each step. MT Architectures 4.2.1 SMT We train a separate SMT model for each language direction using the M OSES toolkit (Koehn et al., 2007). Our bilingual training data is aligned with GIZA++ (Och and Ney, 2003). The target data for the pair is used to train a 3-gram language model using KenLM (Heafield, 2011). We tune our models using MERT based on BLEU on the dev set. Parameters Table 2 contains the successive parameter exploration steps: at the end of a step, we automatically selected (according to average dev BLEU) the step-best value, used as input parameter for the next parameter exploration step.7 The final best parameters are given in Appendix A.1. Smaller learning rates (0.005 and 0.001) are better, while there is no obs"
2021.findings-acl.75,2020.tacl-1.47,0,0.0151274,"actually be outperformed by NMT when architectures and hyper-parameters are carefully chosen, but only above a certain quantity of data. 2 Both our code and data are freely available at http: //github.com/clefourrier/CopperMT. 848 2.2.2 Leveraging Extra Data Several techniques are commonly used in lowresource MT to mitigate the lack of parallel data: monolingual pretraining, backtranslation and using data from additional languages. Monolingual pretraining (unsupervised) has, as in other NLP tasks, been highly beneficial to MT (Song et al., 2019; Conneau and Lample, 2019; Devlin et al., 2019; Liu et al., 2020). Before training on a translation task, model parameters are first pretrained using a language modelling objective, which enables the exploitation of monolingual data, more freely available than bilingual data. Backtranslation originated in SMT (Bertoldi and Federico, 2009; Bojar and Tamchyna, 2011), and has been standard in NMT for several years (Sennrich et al., 2016; Edunov et al., 2018). Its goal is to artificially create larger quantities of parallel data from monolingual datasets, which are often more readily available. Target-side monolingual data is provided to a bilingual model train"
2021.findings-acl.75,D15-1166,0,0.0230396,"tilingual translation graph, YaMTG (Hanoka and Sagot, 2014), by keeping all unique words for each language of interest. To remove noise, words containing non-alphabetic characters were discarded (punctuation marks, parentheses, etc.). The final datasets (cleaned and phonetised) contain between 18,639 and 99,949 unique words (the LA set is more than 4 times smaller than the others). 4.2 across seeds. Our initial parameters were selected from preliminary experiments (in bold in Table 2). 4.2.2 NMT We compare two encoder-decoder NMT models: the RNN (bi-GRU) with attention (Bahdanau et al., 2015; Luong et al., 2015) and the Transformer (Vaswani et al., 2017). We use the multilingual Transformer implementation of fairseq (Ott et al., 2019), and extend the library with an implementation of the multilingual RNN with attention (following the many-to-many setting from (Firat et al., 2016a) but with separate attention mechanisms for each decoder).5 Each model is composed of one encoder per input language, and one decoder (and its own attention) per output language.6 We train each model for 20 epochs (which is systematically after convergence), using the Adam optimiser (Kingma and Ba, 2015), the cross-entropy l"
2021.findings-acl.75,N01-1020,0,0.273144,"ds combined with clustering algorithms (Hall and Klein, 2010, 2011; List et al., 2017; St Arnaud et al., 2017), statistical methods combined with neural classifiers (Inkpen et al., 2005; Frunza and Inkpen, 2006, 2009; Hauer and Kondrak, 2011; Dinu and Ciobanu, 2014) and neural networks only (Ciobanu and Dinu, 2014; Rama, 2016; Kumar et al., 2017; Soisalon-Soininen and Granroth-Wilding, 2019). Automatic cognate prediction is less studied despite its interesting applications, such as predicting plausible new cognates to help field linguists (Bodt et al., 2018) and inducing translation lexicons (Mann and Yarowsky, 2001). In the last few years, it has been approached as an MT task, as it can be seen as modelling sequence-to-sequence correspondences. Using neural networks has been promising (Beinborn et al., 2013; Wu and Yarowsky, 2018; Dekker, 2018; Hämäläinen and Rueter, 2019; Fourrier and Sagot, 2020a), although in most works the hyper-parameters of the neural models were not optimised. Moreover, the differences between MT and cognate prediction have not been studied. In this paper, we choose to study the application of MT approaches to the cognate prediction task. Our aim is to investigate whether the task"
2021.findings-acl.75,N19-4009,0,0.0208557,"ve noise, words containing non-alphabetic characters were discarded (punctuation marks, parentheses, etc.). The final datasets (cleaned and phonetised) contain between 18,639 and 99,949 unique words (the LA set is more than 4 times smaller than the others). 4.2 across seeds. Our initial parameters were selected from preliminary experiments (in bold in Table 2). 4.2.2 NMT We compare two encoder-decoder NMT models: the RNN (bi-GRU) with attention (Bahdanau et al., 2015; Luong et al., 2015) and the Transformer (Vaswani et al., 2017). We use the multilingual Transformer implementation of fairseq (Ott et al., 2019), and extend the library with an implementation of the multilingual RNN with attention (following the many-to-many setting from (Firat et al., 2016a) but with separate attention mechanisms for each decoder).5 Each model is composed of one encoder per input language, and one decoder (and its own attention) per output language.6 We train each model for 20 epochs (which is systematically after convergence), using the Adam optimiser (Kingma and Ba, 2015), the cross-entropy loss, and dev BLEU as selection criterion. Values studied 1) Learning rate × Batch size {0.01, 0.05, 0.001} × {10, 30, 65, 100"
2021.findings-acl.75,P02-1040,0,0.111304,"ed, and choosing the models performing best 5 These implementations are used in all setups, bilingual (using one language as source and one as target) as well as multilingual. 6 In a multilingual setup, encoders, decoders and attention mechanisms can either be shared between languages or be language-specific. In preliminary experiments, using independent items proved to be the most effective. We also observe that a coherent phonetic embedding space is learned during training (described in Appendix A.2). 851 4.3 Evaluation For our task, we use the most commonly used MT evaluation metric, BLEU (Papineni et al., 2002), using the sacreBLEU implementation (Post, 2018). It is based on the proportion of 1- to 4-grams in the prediction that match the reference. In standard MT, BLEU can under-score the many valid translations that do not match the reference. For cognate prediction, however, we expect a single correct prediction in most cases (there are a few exceptions such as variants due to gender distinctions specific to the target language). This makes BLEU better suited to the cognate prediction task than it is to standard MT.8 7 When looking at multilingual models, we chose the model performing best on mos"
2021.findings-acl.75,W18-6319,0,0.0117823,"tations are used in all setups, bilingual (using one language as source and one as target) as well as multilingual. 6 In a multilingual setup, encoders, decoders and attention mechanisms can either be shared between languages or be language-specific. In preliminary experiments, using independent items proved to be the most effective. We also observe that a coherent phonetic embedding space is learned during training (described in Appendix A.2). 851 4.3 Evaluation For our task, we use the most commonly used MT evaluation metric, BLEU (Papineni et al., 2002), using the sacreBLEU implementation (Post, 2018). It is based on the proportion of 1- to 4-grams in the prediction that match the reference. In standard MT, BLEU can under-score the many valid translations that do not match the reference. For cognate prediction, however, we expect a single correct prediction in most cases (there are a few exceptions such as variants due to gender distinctions specific to the target language). This makes BLEU better suited to the cognate prediction task than it is to standard MT.8 7 When looking at multilingual models, we chose the model performing best on most languages, as measured by comparing the sum of"
2021.findings-acl.75,C16-1097,0,0.0165032,"s. Over the last three decades, automatic cognate identification has benefited from advances in computational techniques, first using dictionary-based methods (Dinu and Ciobanu, 2014) and purely statistical methods (Mitkov et al., 2007; McCoy and Frank, 2018), then statistical methods combined with clustering algorithms (Hall and Klein, 2010, 2011; List et al., 2017; St Arnaud et al., 2017), statistical methods combined with neural classifiers (Inkpen et al., 2005; Frunza and Inkpen, 2006, 2009; Hauer and Kondrak, 2011; Dinu and Ciobanu, 2014) and neural networks only (Ciobanu and Dinu, 2014; Rama, 2016; Kumar et al., 2017; Soisalon-Soininen and Granroth-Wilding, 2019). Automatic cognate prediction is less studied despite its interesting applications, such as predicting plausible new cognates to help field linguists (Bodt et al., 2018) and inducing translation lexicons (Mann and Yarowsky, 2001). In the last few years, it has been approached as an MT task, as it can be seen as modelling sequence-to-sequence correspondences. Using neural networks has been promising (Beinborn et al., 2013; Wu and Yarowsky, 2018; Dekker, 2018; Hämäläinen and Rueter, 2019; Fourrier and Sagot, 2020a), although in"
2021.findings-acl.75,W18-0311,0,0.019614,"Inkpen, 2006, 2009), or words etymologically related no matter the relation (Hämäläinen and Rueter, 2019). 847 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 847–861 August 1–6, 2021. ©2021 Association for Computational Linguistics likely cognates in related languages) are two of the fundamental tasks of historical linguistics. Over the last three decades, automatic cognate identification has benefited from advances in computational techniques, first using dictionary-based methods (Dinu and Ciobanu, 2014) and purely statistical methods (Mitkov et al., 2007; McCoy and Frank, 2018), then statistical methods combined with clustering algorithms (Hall and Klein, 2010, 2011; List et al., 2017; St Arnaud et al., 2017), statistical methods combined with neural classifiers (Inkpen et al., 2005; Frunza and Inkpen, 2006, 2009; Hauer and Kondrak, 2011; Dinu and Ciobanu, 2014) and neural networks only (Ciobanu and Dinu, 2014; Rama, 2016; Kumar et al., 2017; Soisalon-Soininen and Granroth-Wilding, 2019). Automatic cognate prediction is less studied despite its interesting applications, such as predicting plausible new cognates to help field linguists (Bodt et al., 2018) and inducin"
2021.findings-acl.75,P16-1009,0,0.0291466,"etraining, backtranslation and using data from additional languages. Monolingual pretraining (unsupervised) has, as in other NLP tasks, been highly beneficial to MT (Song et al., 2019; Conneau and Lample, 2019; Devlin et al., 2019; Liu et al., 2020). Before training on a translation task, model parameters are first pretrained using a language modelling objective, which enables the exploitation of monolingual data, more freely available than bilingual data. Backtranslation originated in SMT (Bertoldi and Federico, 2009; Bojar and Tamchyna, 2011), and has been standard in NMT for several years (Sennrich et al., 2016; Edunov et al., 2018). Its goal is to artificially create larger quantities of parallel data from monolingual datasets, which are often more readily available. Target-side monolingual data is provided to a bilingual model trained in the opposite direction (target-to-source), which produces synthetic source-side data. The data is then filtered to keep the highest quality sentences. The newly generated dataset, made of synthetic source-side data parallel to real target-side data is then combined with the original bilingual set to train a new model. Training multilingual NMT models has been show"
2021.findings-acl.75,P19-1021,0,0.0622095,"gual correspondences between words). 2.2 Low-resource MT Since data is scarce, we postulate that cognate prediction could benefit from low-resource MT settings techniques and architectural choices. 2.2.1 Architecture Comparison Several papers comparing SMT with NMT (recurrent neural networks (RNNs) with attention) in low-resource settings conclude that SMT performs better, being more accurate and less prone to overfitting (Skadin, a and Pinnis, 2017; Dowling et al., 2018; Singh and Hujon, 2020). However, as Dowling et al. (2018) themselves note, they did not optimise hyper-parameters for NMT. Sennrich and Zhang (2019) analysed and reproduced previous comparisons, to conclude that SMT can actually be outperformed by NMT when architectures and hyper-parameters are carefully chosen, but only above a certain quantity of data. 2 Both our code and data are freely available at http: //github.com/clefourrier/CopperMT. 848 2.2.2 Leveraging Extra Data Several techniques are commonly used in lowresource MT to mitigate the lack of parallel data: monolingual pretraining, backtranslation and using data from additional languages. Monolingual pretraining (unsupervised) has, as in other NLP tasks, been highly beneficial to"
2021.findings-acl.75,P07-3005,0,0.0779002,"cial as it is in standard MT settings.2 2 Related Work 2.1 Cognate Prediction Cognate prediction is the task that aims to produce from words in a source language plausible cognates in a target language (according to the aforementioned definition of cognates). It is a lexical task that models regular, word-internal sound changes that transform words over time. It has been approached with phylogenetic trees combined with stochastic sound change models (Bouchard et al., 2007; Bouchard-Côté et al., 2009; Bouchard-Côté et al., 2013), purely statistical methods (Bodt et al., 2018), neural networks (Mulloni, 2007), language models (Hauer et al., 2019) and character-level MT techniques (Beinborn et al., 2013; Wu and Yarowsky, 2018; Dekker, 2018; Hämäläinen and Rueter, 2019; Fourrier and Sagot, 2020a; Meloni et al., 2021), because of its similarity to a translation task (modelling sequence-to-sequence crosslingual correspondences between words). 2.2 Low-resource MT Since data is scarce, we postulate that cognate prediction could benefit from low-resource MT settings techniques and architectural choices. 2.2.1 Architecture Comparison Several papers comparing SMT with NMT (recurrent neural networks (RNNs)"
2021.findings-acl.75,J03-1002,0,0.0409572,"and dev BLEU as selection criterion. Values studied 1) Learning rate × Batch size {0.01, 0.05, 0.001} × {10, 30, 65, 100} {8, 12, 16, 20, 24} × {18, 36, 54, 72} 1, 2, 4 1, 2, 3, 4 None, Bahdanau, Luong (dot, concat, general) 2) Embed. dim. × Hidden dim. 3) Number of layers 4) Number of heads 4) Attention type Table 2: Parameter exploration experiments for NMT models. In bold, the initial parameters at each step. MT Architectures 4.2.1 SMT We train a separate SMT model for each language direction using the M OSES toolkit (Koehn et al., 2007). Our bilingual training data is aligned with GIZA++ (Och and Ney, 2003). The target data for the pair is used to train a 3-gram language model using KenLM (Heafield, 2011). We tune our models using MERT based on BLEU on the dev set. Parameters Table 2 contains the successive parameter exploration steps: at the end of a step, we automatically selected (according to average dev BLEU) the step-best value, used as input parameter for the next parameter exploration step.7 The final best parameters are given in Appendix A.1. Smaller learning rates (0.005 and 0.001) are better, while there is no observable pattern to the best batch sizes or numbers of layers. Interestin"
2021.findings-acl.75,I17-1038,0,0.0279563,"Missing"
2021.findings-acl.75,R19-1129,0,0.011114,"automatic cognate identification has benefited from advances in computational techniques, first using dictionary-based methods (Dinu and Ciobanu, 2014) and purely statistical methods (Mitkov et al., 2007; McCoy and Frank, 2018), then statistical methods combined with clustering algorithms (Hall and Klein, 2010, 2011; List et al., 2017; St Arnaud et al., 2017), statistical methods combined with neural classifiers (Inkpen et al., 2005; Frunza and Inkpen, 2006, 2009; Hauer and Kondrak, 2011; Dinu and Ciobanu, 2014) and neural networks only (Ciobanu and Dinu, 2014; Rama, 2016; Kumar et al., 2017; Soisalon-Soininen and Granroth-Wilding, 2019). Automatic cognate prediction is less studied despite its interesting applications, such as predicting plausible new cognates to help field linguists (Bodt et al., 2018) and inducing translation lexicons (Mann and Yarowsky, 2001). In the last few years, it has been approached as an MT task, as it can be seen as modelling sequence-to-sequence correspondences. Using neural networks has been promising (Beinborn et al., 2013; Wu and Yarowsky, 2018; Dekker, 2018; Hämäläinen and Rueter, 2019; Fourrier and Sagot, 2020a), although in most works the hyper-parameters of the neural models were not optim"
2021.findings-acl.75,D17-1267,0,0.0130144,"ion for Computational Linguistics: ACL-IJCNLP 2021, pages 847–861 August 1–6, 2021. ©2021 Association for Computational Linguistics likely cognates in related languages) are two of the fundamental tasks of historical linguistics. Over the last three decades, automatic cognate identification has benefited from advances in computational techniques, first using dictionary-based methods (Dinu and Ciobanu, 2014) and purely statistical methods (Mitkov et al., 2007; McCoy and Frank, 2018), then statistical methods combined with clustering algorithms (Hall and Klein, 2010, 2011; List et al., 2017; St Arnaud et al., 2017), statistical methods combined with neural classifiers (Inkpen et al., 2005; Frunza and Inkpen, 2006, 2009; Hauer and Kondrak, 2011; Dinu and Ciobanu, 2014) and neural networks only (Ciobanu and Dinu, 2014; Rama, 2016; Kumar et al., 2017; Soisalon-Soininen and Granroth-Wilding, 2019). Automatic cognate prediction is less studied despite its interesting applications, such as predicting plausible new cognates to help field linguists (Bodt et al., 2018) and inducing translation lexicons (Mann and Yarowsky, 2001). In the last few years, it has been approached as an MT task, as it can be seen as mo"
2021.findings-acl.75,L18-1538,0,0.0669628,"er and Kondrak, 2011; Dinu and Ciobanu, 2014) and neural networks only (Ciobanu and Dinu, 2014; Rama, 2016; Kumar et al., 2017; Soisalon-Soininen and Granroth-Wilding, 2019). Automatic cognate prediction is less studied despite its interesting applications, such as predicting plausible new cognates to help field linguists (Bodt et al., 2018) and inducing translation lexicons (Mann and Yarowsky, 2001). In the last few years, it has been approached as an MT task, as it can be seen as modelling sequence-to-sequence correspondences. Using neural networks has been promising (Beinborn et al., 2013; Wu and Yarowsky, 2018; Dekker, 2018; Hämäläinen and Rueter, 2019; Fourrier and Sagot, 2020a), although in most works the hyper-parameters of the neural models were not optimised. Moreover, the differences between MT and cognate prediction have not been studied. In this paper, we choose to study the application of MT approaches to the cognate prediction task. Our aim is to investigate whether the task can benefit from techniques commonly seen to improve standard low-resource MT. We first highlight the specific characteristics of cognate prediction, and (to our knowledge) provide the first detailed analysis of the e"
2021.naacl-main.38,2020.findings-emnlp.223,0,0.0412259,"ni, Uyghur and Meadow Mari. For instance, transliterating Arabic to the Latin script leads to a drop in performance of 1.5, 4.1 and 6.9 points for POS tagging, parsing and NER respectively.6 Our findings are generally in line with previous work. Transliteration to English specifically (Lin et al., 2016; Durrani et al., 2014) and named entity transliteration (Kundu et al., 2018; Grundkiewicz and Heafield, 2018) has been proven useful for cross-lingual transfer in tasks like NER, entity linking (Rijhwani et al., 2019), morphological inflection (Murikinati et al., 2020), and Machine Translation (Amrhein and Sennrich, 2020). The transliteration approach provides a viable path for rendering large pretrained models like mBERT useful for all languages of the world. Indeed, as reported in Table 4, transliterating both Uyghur and Sorani leads to matching or outper6 Details and complete results on these controlled experiments can be found in Appendix E. Model Arabic Russian Japanese Original Script → Latin Script POS LAS NER 96.4 → 94.9 98.1 → 96.0 97.4 → 95.7 82.9 → 78.8 88.4 → 84.5 88.5 → 86.9 87.8 → 80.9 88.1 → 86.0 61.5 → 55.6 Table 5: mBERT TASK -T UNED on high resource languages for POS tagging, parsing and NER."
2021.naacl-main.38,2020.osact-1.2,0,0.0231597,"cting the abilities of pretrained multilingual language models 1 Introduction to be used for low-resource languages. We dub those categories Easy, Intermediate and Hard. Language models are now a new standard to build state-of-the-art Natural Language ProcessHard languages include both stable and endaning (NLP) systems. In the past year, monolingual gered languages, but they predominantly are lanlanguage models have been released for more than guages of communities that are majorly under20 languages including Arabic, French, German, served by modern NLP. Hence, we direct our attenand Italian (Antoun et al., 2020; Martin et al., 2020; tion to these Hard languages. For those languages, de Vries et al., 2019; Cañete et al., 2020; Kuratov we show that the script they are written in can be and Arkhipov, 2019; Schweter, 2020, inter alia). a critical element in the transfer abilities of preAdditionally, large-scale multilingual models cov- trained multilingual language models. Translitering more than 100 languages are now available erating them leads to large gains in performance (XLM-R by Conneau et al. (2020) and mBERT outperforming non-contextual strong baselines. To by Devlin et al. (2019)). Still, most"
2021.naacl-main.38,E14-4029,0,0.0224963,"trained languages such as Arabic, Russian or Japanese, mBERT is not able to compete with the performance reached when using the script seen during pretraining. Transliterating the Arabic script and the Cyrillic script to Latin does not automatically improve mBERT performance as it does for Sorani, Uyghur and Meadow Mari. For instance, transliterating Arabic to the Latin script leads to a drop in performance of 1.5, 4.1 and 6.9 points for POS tagging, parsing and NER respectively.6 Our findings are generally in line with previous work. Transliteration to English specifically (Lin et al., 2016; Durrani et al., 2014) and named entity transliteration (Kundu et al., 2018; Grundkiewicz and Heafield, 2018) has been proven useful for cross-lingual transfer in tasks like NER, entity linking (Rijhwani et al., 2019), morphological inflection (Murikinati et al., 2020), and Machine Translation (Amrhein and Sennrich, 2020). The transliteration approach provides a viable path for rendering large pretrained models like mBERT useful for all languages of the world. Indeed, as reported in Table 4, transliterating both Uyghur and Sorani leads to matching or outper6 Details and complete results on these controlled experime"
2021.naacl-main.38,W18-2413,0,0.0172977,"compete with the performance reached when using the script seen during pretraining. Transliterating the Arabic script and the Cyrillic script to Latin does not automatically improve mBERT performance as it does for Sorani, Uyghur and Meadow Mari. For instance, transliterating Arabic to the Latin script leads to a drop in performance of 1.5, 4.1 and 6.9 points for POS tagging, parsing and NER respectively.6 Our findings are generally in line with previous work. Transliteration to English specifically (Lin et al., 2016; Durrani et al., 2014) and named entity transliteration (Kundu et al., 2018; Grundkiewicz and Heafield, 2018) has been proven useful for cross-lingual transfer in tasks like NER, entity linking (Rijhwani et al., 2019), morphological inflection (Murikinati et al., 2020), and Machine Translation (Amrhein and Sennrich, 2020). The transliteration approach provides a viable path for rendering large pretrained models like mBERT useful for all languages of the world. Indeed, as reported in Table 4, transliterating both Uyghur and Sorani leads to matching or outper6 Details and complete results on these controlled experiments can be found in Appendix E. Model Arabic Russian Japanese Original Script → Latin S"
2021.naacl-main.38,W19-7803,0,0.0131002,"tion about their scripts, language families, and amount of available raw data can be found in the Appendix in Table 12. 3.1 Raw Data To perform pretraining and fine-tuning on monolingual data, we use the deduplicated datasets from the OSCAR project (Ortiz Suárez et al., 2019). OSCAR is a corpus extracted from a Common Crawl Web snapshot.2 It provides a significant amount of data for all the unseen languages we work with, except for Buryat, Meadow Mari, Erzya and Livvi for which we use Wikipedia dumps and for Narabizi, Naija and Faroese, for which we use data collected by Seddah et al. (2020), Caron et al. (2019) and Biemann et al. (2007) respectively. 3.2 Non-contextual Baselines For parsing and POS tagging, we use the UDPipe future system (Straka, 2018) as our baseline. This model is a LSTM-based (Hochreiter and Schmidhuber, 1997) recurrent architecture trained with pretrained static word embedding (Mikolov et al., 2013) (hence our non-contextual characterization) along with character-level embeddings. This system was ranked in the very first positions for parsing and tagging in the CoNLL shared task 2018 (Zeman and Hajiˇc, 2018). For NER we use the LSTM-CRF model with character and word level embed"
2021.naacl-main.38,D19-1433,0,0.0229135,"z Suárez et al. (2020) showed that pretraining ELMo models (Peters et al., 2018) on less than 1GB of text leads to state-of-the-art performance while Martin et al. (2020) showed that pretraining a BERT model on as few as 4GB of diverse enough data results in state-of-the-art performance. Micheli et al. (2020) further demonstrated that decent performance was achievable with only 100MB of raw text data. Adapting large-scale models for low-resource languages Multilingual language models can be used directly on unseen languages, or they can also be adapted using unsupervised methods. For example, Han and Eisenstein (2019) successfully used unsupervised model adaptation of the English BERT model to Early Modern English for sequence labeling. Instead of fine-tuning the whole model, Pfeiffer et al. (2020) recently showed that adapter layers (Houlsby et al., 2019) can be injected into multilingual language models to provide parameter efficient task and language transfer. Still, as of today, the availability of monolingual or multilingual language models is limited to approximately 120 languages, leaving many languages without access to valuable NLP technology, although some are spoken by millions of people, includ"
2021.naacl-main.38,2020.findings-emnlp.118,0,0.325304,"Missing"
2021.naacl-main.38,2020.acl-main.747,0,0.526302,"Bambara (spoken guage models on a large amount of raw data by around 5 million people in Mali and neighborhas become a new norm to reach state-of-theing countries) are not covered by any available art performance in NLP. Still, it remains unclear language models at the time of writing. how this approach should be applied for unseen languages that are not covered by any available Even if training multilingual models that cover large-scale multilingual language model and more languages and language varieties is tempting, for which only a small amount of raw data is the curse of multilinguality (Conneau et al., 2020) generally available. In this work, by comparmakes it an impractical solution, as it would require ing multilingual and monolingual models, we to train ever larger models. Furthermore, as shown show that such models behave in multiple ways by Wu and Dredze (2020), large-scale multilingual on unseen languages. Some languages greatly language models are sub-optimal for languages that benefit from transfer learning and behave simiare under-sampled during pretraining. larly to closely related high resource languages whereas others apparently do not. Focusing In this paper, we analyze task and lang"
2021.naacl-main.38,2020.acl-main.560,0,0.0441456,"lable at https://github.com/benjami n-mlr/mbert-unseen-languages.git diate and the Easy languages. 448 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 448–462 June 6–11, 2021. ©2021 Association for Computational Linguistics • We show that Hard languages can be better addressed by transliterating them into a betterhandled script (typically Latin), providing a promising direction towards making multilingual language models useful for a new set of unseen languages. 2 Background and Motivation As Joshi et al. (2020) vividly illustrate, there is a large divergence in the coverage of languages by NLP technologies. The majority of the 6500+ of the world’s languages are not studied by the NLP community, since most have few or no annotated datasets, making systems’ development challenging. The development of such models is a matter of high importance for the inclusion of communities, the preservation of endangered languages and more generally to support the rise of tailored NLP ecosystems for such languages (Schmidt and Wiegand, 2017; Stecklow, 2018; Seddah et al., 2020). In that regard, the advent of the Uni"
2021.naacl-main.38,P17-1178,0,0.309625,"ges by NLP technologies. The majority of the 6500+ of the world’s languages are not studied by the NLP community, since most have few or no annotated datasets, making systems’ development challenging. The development of such models is a matter of high importance for the inclusion of communities, the preservation of endangered languages and more generally to support the rise of tailored NLP ecosystems for such languages (Schmidt and Wiegand, 2017; Stecklow, 2018; Seddah et al., 2020). In that regard, the advent of the Universal Dependencies project (Nivre et al., 2016) and the WikiAnn dataset (Pan et al., 2017) have greatly increased the number of covered languages by providing annotated datasets for more than 90 languages for dependency parsing and 282 languages for NER. Regarding modeling approaches, the emergence of multilingual representation models, first with static word embeddings (Ammar et al., 2016) and then with language model-based contextual representations (Devlin et al., 2019; Conneau et al., 2020) enabled transfer from high to low-resource languages, leading to significant improvements in downstream task performance (Rahimi et al., 2019; Kondratyuk and Straka, 2019). Furthermore, in t"
2021.naacl-main.38,N18-1202,0,0.0649653,"it comes to low-resource languages, one direction is to simply train contextualized embedding models on whatever data is available. Another option is to adapt/fine-tune a multilingual pretrained model to the language of interest. We briefly discuss these two options. of pretraining data seems to correlate with downstream task performance (e.g. compare BERT and RoBERTa (Liu et al., 2020)), several attempts have shown that training a model from scratch can be efficient even if the amount of data in that language is limited. Indeed, Ortiz Suárez et al. (2020) showed that pretraining ELMo models (Peters et al., 2018) on less than 1GB of text leads to state-of-the-art performance while Martin et al. (2020) showed that pretraining a BERT model on as few as 4GB of diverse enough data results in state-of-the-art performance. Micheli et al. (2020) further demonstrated that decent performance was achievable with only 100MB of raw text data. Adapting large-scale models for low-resource languages Multilingual language models can be used directly on unseen languages, or they can also be adapted using unsupervised methods. For example, Han and Eisenstein (2019) successfully used unsupervised model adaptation of the"
2021.naacl-main.38,2020.emnlp-main.617,0,0.107921,"Missing"
2021.wnut-1.47,2020.acl-main.740,0,0.0523348,"Missing"
2021.wnut-1.47,P18-1031,0,0.013994,"f NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pretrained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability settings. 1 Introduction Current state-of-the-art monolingual and multilingual language models require large amounts of data to be trained, showing limited performance on low-resource languages (Howard and Ruder, 2018; Devlin et al., 2019). They lead to state-of-the-art results on most NLP tasks (Devlin et al., 2018; Raffel et al., 2020). In order to achieve high performance, these models rely on transfer learning architectures: the language models need to be trained on First version submitted on August 27th, 2021. Final on October 1st, 2021. large amounts of data (pre-training) to be able to transfer the acquired knowledge to a downstream task via fine-tuning on a relatively small number of examples, resulting in a significant performance improvement with respect to previous approaches. This dependency on"
2021.wnut-1.47,P19-1356,1,0.880254,"Missing"
2021.wnut-1.47,2020.acl-main.560,0,0.0122073,"corpus (Pires et al., 2019); it is still a challenge for unseen languages, especially low-resource ones. However, Muller et al. (2020) achieved promising results by performing unsupervised fine-tuning on small amounts of NArabizi data. We follow their approach by comparing the performance of our pipeline in two setups: M ODEL +MLM+TASK and M ODEL +TASK. We describe these setups in more details in section 6. Low-resource languages, by definition, face a lack of textual resources – annotated or not –, which makes it difficult for the NLP community to develop models and systems adapted to them (Joshi et al., 2020). The majority of the almost-7000 languages worldwide actually fall into the “lowresource” category. This makes the development of systems for low-resource languages necessary to widen the accessibility of NLP technology. For deep learning approaches, which depend on the availability of large data sets, the solution to the low-resource problem comes from the idea of transfer learning. Early instances of cross-lingual 3.2 Tokenization & Character-based models transfer learning rely on non-contextualised word embeddings (Ammar et al., 2016). More recently, Standard language models rely on a subw"
2021.wnut-1.47,P18-1007,0,0.111319,"tokenizais still limited compared to other languages since tion struggle to represent rare words (Schick and they are naturally under-sampled during the train- Schütze, 2020). Many research projects have foing process (Wu and Dredze, 2020). cused on improving subword tokenization. For exTo improve performance on a specific low- ample, Wang et al. (2021) suggested a multi-view resource languages, there are two possibilities. Ei- subword regularization based on the sampling of ther to attempt to train a language model on it from multiple segmentations of the input text, based on 425 the work of Kudo (2018b). Other parallel efforts bid on character-based models. For example, El Boukkouri et al. (2020) proposed a possible solution to get a better tokenization system more resilient to orthographic variations and noise in the data set by using a character-level model, inspired by a previous wordlevel open-vocabulary system (Peters et al., 2018a). This new model gets better results than vanilla BERT on multiple tasks from the medical domain. Furthermore, the authors claim that it is more robust to noise and misspellings. In the same vein, Ma et al. (2020a) combined character-aware and subword-based"
2021.wnut-1.47,lacheret-etal-2014-rhapsodie,0,0.0199201,"Missing"
2021.wnut-1.47,2021.ccl-1.108,0,0.0614088,"Missing"
2021.wnut-1.47,2020.coling-main.4,0,0.197654,"tions of the input text, based on 425 the work of Kudo (2018b). Other parallel efforts bid on character-based models. For example, El Boukkouri et al. (2020) proposed a possible solution to get a better tokenization system more resilient to orthographic variations and noise in the data set by using a character-level model, inspired by a previous wordlevel open-vocabulary system (Peters et al., 2018a). This new model gets better results than vanilla BERT on multiple tasks from the medical domain. Furthermore, the authors claim that it is more robust to noise and misspellings. In the same vein, Ma et al. (2020a) combined character-aware and subword-based information to improve robustness to spelling errors. This initiated a new wave of tokenizer-free models based on characters or bytes (Tay et al., 2021; Xue et al., 2021; Clark et al., 2021). The question of knowing if character-based language models can handle high language variability since they are supposed to be resilient to noise and spelling variations is crucial when dealing with non-normalized dialects and non-canonical forms of language as found on many user-generated content platforms. This is why we focus on this work on the analysis of"
2021.wnut-1.47,P13-2017,0,0.0170191,"Missing"
2021.wnut-1.47,2020.emnlp-main.632,0,0.0871156,"Missing"
2021.wnut-1.47,2021.naacl-main.38,1,0.841079,"Missing"
2021.wnut-1.47,2021.eacl-main.189,1,0.844585,"models rely on transfer learning architectures: the language models need to be trained on First version submitted on August 27th, 2021. Final on October 1st, 2021. large amounts of data (pre-training) to be able to transfer the acquired knowledge to a downstream task via fine-tuning on a relatively small number of examples, resulting in a significant performance improvement with respect to previous approaches. This dependency on large data sets for pre-training is a severe issue for low-resource languages, despite the emergence of large and successful multilingual pre-trained language models (Muller et al., 2021b). This is especially the case for languages with unusual morphological and structural features, which struggle to take advantage from similarities with high-resource, well represented languages such as Romance and Germanic languages. In this work, we focus on one of such highly challenging languages, namely North-African dialectal Arabic. Its Latin transcription (Arabizi) displays a high level of linguistic variability1 , on top of scarce and noisy resource availability, making it a particularly challenging language for most NLP systems relying on pre-trained multilingual models (Muller et a"
2021.wnut-1.47,2020.lrec-1.497,0,0.0328786,"Missing"
2021.wnut-1.47,N18-1202,0,0.365215,"ple, Wang et al. (2021) suggested a multi-view resource languages, there are two possibilities. Ei- subword regularization based on the sampling of ther to attempt to train a language model on it from multiple segmentations of the input text, based on 425 the work of Kudo (2018b). Other parallel efforts bid on character-based models. For example, El Boukkouri et al. (2020) proposed a possible solution to get a better tokenization system more resilient to orthographic variations and noise in the data set by using a character-level model, inspired by a previous wordlevel open-vocabulary system (Peters et al., 2018a). This new model gets better results than vanilla BERT on multiple tasks from the medical domain. Furthermore, the authors claim that it is more robust to noise and misspellings. In the same vein, Ma et al. (2020a) combined character-aware and subword-based information to improve robustness to spelling errors. This initiated a new wave of tokenizer-free models based on characters or bytes (Tay et al., 2021; Xue et al., 2021; Clark et al., 2021). The question of knowing if character-based language models can handle high language variability since they are supposed to be resilient to noise and"
2021.wnut-1.47,2020.emnlp-main.617,0,0.0412432,"Missing"
2021.wnut-1.47,P19-1493,0,0.0231583,"ingual language model on the lowresource language corpus, also called cross-lingual transfer learning (Muller et al., 2021a). The first option can sometimes lead to decent performance, provided that the training corpus is diverse enough (Martin et al., 2020). When following the fine-tuning approach, unsupervised methods can be implemented to facilitate the transfer of knowledge (Pfeiffer et al., 2020). The most widely used unsupervised fine-tuning task is masked language modeling (MLM). This system has proven its efficiency between languages that have already been seen in the training corpus (Pires et al., 2019); it is still a challenge for unseen languages, especially low-resource ones. However, Muller et al. (2020) achieved promising results by performing unsupervised fine-tuning on small amounts of NArabizi data. We follow their approach by comparing the performance of our pipeline in two setups: M ODEL +MLM+TASK and M ODEL +TASK. We describe these setups in more details in section 6. Low-resource languages, by definition, face a lack of textual resources – annotated or not –, which makes it difficult for the NLP community to develop models and systems adapted to them (Joshi et al., 2020). The maj"
2021.wnut-1.47,P19-1561,0,0.0271624,"s of data has always been queson downstream tasks and languages (Gururangan tioned. While splitting texts into subwords based et al., 2020). Therefore, this technique is crucial for on their frequencies works well for English, modmultilingual applications, as most of the world’s els using this kind of tokenization struggle with languages lack large amount of labeled data (Con- noise, whether it is naturally present in the data neau et al., 2019; Eisenschlos et al., 2019; Joshi (Sun et al., 2020) or artificially generated to chalet al., 2020). However the performance of multi- lenge the model (Pruthi et al., 2019). Moreover, lingual language model on low-resource languages language models that use subword-based tokenizais still limited compared to other languages since tion struggle to represent rare words (Schick and they are naturally under-sampled during the train- Schütze, 2020). Many research projects have foing process (Wu and Dredze, 2020). cused on improving subword tokenization. For exTo improve performance on a specific low- ample, Wang et al. (2021) suggested a multi-view resource languages, there are two possibilities. Ei- subword regularization based on the sampling of ther to attempt to t"
2021.wnut-1.47,W19-6101,1,0.897095,"Missing"
2021.wnut-1.47,2021.wnut-1.23,1,0.827828,"Missing"
2021.wnut-1.47,2020.acl-main.107,1,0.934396,"m cross-lingual transfer during pre-training. However, such model is still pre-trained on sentences written in a single language and was not trained to handle the presence 1 Language variability, or language variation, is a term coming from socio-linguistics where, as stated by Nordquist (2019), it refers to regional, social or contextual differences in the ways that a particular language is used. These variations in user-generated content can be characterized through their prevalent idiosyncraisies when compared to canonical texts (Seddah et al., 2012a; Sanguinetti et al., 2020). 2 Following Seddah et al. (2020), we refer to the Arabizi version of North-African Arabic dialects as NArabizi. 423 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 423–436 November 11, 2021. ©2021 Association for Computational Linguistics of multiple languages in the same sentence (codeswitching), a frequent phenomenon in NArabizi. However both monolingual and multilingual model approaches bear the risk of being limited by a subword tokenization-based vocabulary when facing out-of-domain training data language, especially in high-variability noisy scenarios (El Boukkouri"
2021.wnut-1.47,C12-1149,1,0.416138,"g is used there is no significant performance improvement from cross-lingual transfer during pre-training. However, such model is still pre-trained on sentences written in a single language and was not trained to handle the presence 1 Language variability, or language variation, is a term coming from socio-linguistics where, as stated by Nordquist (2019), it refers to regional, social or contextual differences in the ways that a particular language is used. These variations in user-generated content can be characterized through their prevalent idiosyncraisies when compared to canonical texts (Seddah et al., 2012a; Sanguinetti et al., 2020). 2 Following Seddah et al. (2020), we refer to the Arabizi version of North-African Arabic dialects as NArabizi. 423 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 423–436 November 11, 2021. ©2021 Association for Computational Linguistics of multiple languages in the same sentence (codeswitching), a frequent phenomenon in NArabizi. However both monolingual and multilingual model approaches bear the risk of being limited by a subword tokenization-based vocabulary when facing out-of-domain training data language"
2021.wnut-1.47,D19-1102,0,0.012512,"al., 2020) has spread far and wide in NLP, 2018a). This allows the model to handle any word enabling high-performance zero-shot cross-lingual unseen in the training data, working in an “opentransfer for numerous tasks and languages. The vocabulary setting,” where words are represented main idea is to exploit a large amount of unlabeled by a combination of subwords from a pre-defined data to pre-train a model using a self-supervised list. On top of alleviating the issue of out-oftask, such as masked language modeling (Lam- vocabulary words, this approach allows the model ple and Conneau, 2019; Vania et al., 2019). This to handle sequences written in a language unseen pre-trained model is then fine-tuned on a much during training, as long as it uses the same script. smaller annotated data set and used for another lan- Therefore, subword tokenization is a crucial feaguage, domain or task. Strategic knowledge shar- ture of state-of-the-art models in NLP. But its suiting has been shown to improve the performance ability for all types of data has always been queson downstream tasks and languages (Gururangan tioned. While splitting texts into subwords based et al., 2020). Therefore, this technique is crucia"
2021.wnut-1.47,2021.naacl-main.40,0,0.0281491,"hlos et al., 2019; Joshi (Sun et al., 2020) or artificially generated to chalet al., 2020). However the performance of multi- lenge the model (Pruthi et al., 2019). Moreover, lingual language model on low-resource languages language models that use subword-based tokenizais still limited compared to other languages since tion struggle to represent rare words (Schick and they are naturally under-sampled during the train- Schütze, 2020). Many research projects have foing process (Wu and Dredze, 2020). cused on improving subword tokenization. For exTo improve performance on a specific low- ample, Wang et al. (2021) suggested a multi-view resource languages, there are two possibilities. Ei- subword regularization based on the sampling of ther to attempt to train a language model on it from multiple segmentations of the input text, based on 425 the work of Kudo (2018b). Other parallel efforts bid on character-based models. For example, El Boukkouri et al. (2020) proposed a possible solution to get a better tokenization system more resilient to orthographic variations and noise in the data set by using a character-level model, inspired by a previous wordlevel open-vocabulary system (Peters et al., 2018a)."
2021.wnut-1.47,2020.repl4nlp-1.16,0,0.0166793,"lack large amount of labeled data (Con- noise, whether it is naturally present in the data neau et al., 2019; Eisenschlos et al., 2019; Joshi (Sun et al., 2020) or artificially generated to chalet al., 2020). However the performance of multi- lenge the model (Pruthi et al., 2019). Moreover, lingual language model on low-resource languages language models that use subword-based tokenizais still limited compared to other languages since tion struggle to represent rare words (Schick and they are naturally under-sampled during the train- Schütze, 2020). Many research projects have foing process (Wu and Dredze, 2020). cused on improving subword tokenization. For exTo improve performance on a specific low- ample, Wang et al. (2021) suggested a multi-view resource languages, there are two possibilities. Ei- subword regularization based on the sampling of ther to attempt to train a language model on it from multiple segmentations of the input text, based on 425 the work of Kudo (2018b). Other parallel efforts bid on character-based models. For example, El Boukkouri et al. (2020) proposed a possible solution to get a better tokenization system more resilient to orthographic variations and noise in the data se"
apidianaki-sagot-2012-applying,apidianaki-2008-translation,1,\N,Missing
apidianaki-sagot-2012-applying,steinberger-etal-2006-jrc,0,\N,Missing
apidianaki-sagot-2012-applying,E09-1010,1,\N,Missing
apidianaki-sagot-2012-applying,W02-0808,0,\N,Missing
apidianaki-sagot-2012-applying,P10-1023,0,\N,Missing
apidianaki-sagot-2012-applying,P02-1033,0,\N,Missing
apidianaki-sagot-2012-applying,P06-2111,0,\N,Missing
apidianaki-sagot-2012-applying,J03-1002,0,\N,Missing
apidianaki-sagot-2012-applying,2005.mtsummit-papers.11,0,\N,Missing
apidianaki-sagot-2012-applying,2010.jeptalnrecital-court.19,0,\N,Missing
apidianaki-sagot-2012-applying,2010.iwslt-papers.2,1,\N,Missing
baranes-sagot-2014-language,sagot-2010-lefff,1,\N,Missing
baranes-sagot-2014-language,W05-0616,0,\N,Missing
baranes-sagot-2014-language,C00-1071,0,\N,Missing
baranes-sagot-2014-language,J01-2001,0,\N,Missing
baranes-sagot-2014-language,W07-1315,0,\N,Missing
baranes-sagot-2014-language,R09-1049,1,\N,Missing
baranes-sagot-2014-language,gala-etal-2010-tool,0,\N,Missing
C08-1080,2005.jeptalnrecital-long.1,1,0.856072,"Missing"
C08-1080,P04-1057,0,0.346299,"Missing"
C08-1080,zhang-kordoni-2006-automated,0,0.0761505,"ole system has been explained with details, we will expose the similarities and differences of our methods with previously publicated ones. 8 Related works Since efﬁcient and linguistically relevant lexical and grammatical formalisms have been developed, the acquisition/extension/correction of linguistic ressources has been an active research ﬁeld, especially during the last decade. The idea to infer lexical data from the grammatical context ﬁrst appeared in 1990 (Erbach, 1990). The combination with error mining/detection technique, such as van Noord (2004), begun in 2006 (van de Cruys, 2006; Yi and Kordoni, 2006). Except in our previous work (2007), nobody has combined it with the technique described in Sagot and Villemonte de La Clergerie (2006). The idea of preﬁltering the sentences (Sec. 3.2) to improve the error mining performance has never been applied so far. The wildcards generation started to be reﬁned with Barg and Walther (1998). Since then, the wildcards are partially underspeciﬁed and restrained to open class POS. In Yi and Kordoni (2006), the authors use an elegant technique based on an entropy classiﬁer to select the most adequate wildcards. The way to rank the corrections is usually bas"
C08-1080,P98-1014,0,0.170741,"ctive research ﬁeld, especially during the last decade. The idea to infer lexical data from the grammatical context ﬁrst appeared in 1990 (Erbach, 1990). The combination with error mining/detection technique, such as van Noord (2004), begun in 2006 (van de Cruys, 2006; Yi and Kordoni, 2006). Except in our previous work (2007), nobody has combined it with the technique described in Sagot and Villemonte de La Clergerie (2006). The idea of preﬁltering the sentences (Sec. 3.2) to improve the error mining performance has never been applied so far. The wildcards generation started to be reﬁned with Barg and Walther (1998). Since then, the wildcards are partially underspeciﬁed and restrained to open class POS. In Yi and Kordoni (2006), the authors use an elegant technique based on an entropy classiﬁer to select the most adequate wildcards. The way to rank the corrections is usually based on a trained tool (van de Cruys, 2006; Yi and Kordoni, 2006), such as an entropy classiﬁer. Surprisingly, the evaluation of hypotheses on various sentences for a same suspicious form in order to discriminate the irrelevant ones has never been considered so far. Finally, all the previous works were achieved with HPSG parsers and"
C08-1080,E03-1041,0,0.23142,"arsing originally non-parsable sentences with wildcards As explained in Sect. 2, in order to generate lexical corrections, we ﬁrst need to get as close as possible to the set of parses that the grammar would have allowed with an error-free lexicon. We achieve this goal by replacing in the associated sentences every suspicious forms with special underspeciﬁed forms called wildcards. The simplest way would be to use totally underspeciﬁed wildcards. Indeed, this would have the beneﬁt to cover all kinds of conﬂicts and thus, it would notably increase the parsing coverage. However, as observed by (Fouvry, 2003), it introduces an unnecessary ambiguity which usually leads to a severe overgeneration of parses or to no parses at all because of time or memory shortage. In a metaphorical way, we said that we wanted the grammar to tell us what lexical information it would have accepted for the suspicious form. Well, by introducing a totally underspeciﬁed wildcard, either the grammar has so many things to say that it is hard to know what to listen to, or it has so many things to think about that it stutters and does not say anything at all. Therefore, we reﬁned the wildcard by introduc635 ing some data. For"
C08-1080,C98-1014,0,\N,Missing
C08-1080,P06-1042,1,\N,Missing
C08-1080,sagot-etal-2006-lefff,1,\N,Missing
C12-1149,W09-3821,1,0.874376,"cDonald, 2012). Needless to say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for examples, techniques built on richer syntactic models, lexical data sparseness reduction or rich feature set. See (Tsarfaty and Sima’an, 2008; Versley and Rehbein, 2009; Candito and Crabbé, 2009; Green and Manning, 2010) to name but a few. The questions are thus to know: (1) to what extend MRL user generated content is parsable? and (2) more importantly, what is needed to fill that performance gap? To answer question 1, we introduce the first release of the French Social Media Treebank, a representative gold standard treebank for French user-generated data. This treebank consists in around 1,700 sentences extracted from various types of French Web 2.0 user generated content (Facebook, Twitter, video games and medical board). This treebank was developed independently from the Google W"
C12-1149,W09-1008,1,0.895823,"Missing"
C12-1149,W11-2905,1,0.787828,"64.14 69.21 63.09 68.63 64.89 79.70 55.90 64.13 - 58.71 65.48 - 57.27 64.80 83.81 64.34 72.69 96.44 T EST LR SET LP F1 Pos acc. OOVs 70.10 70.59 71.68 71.44 70.88 71.02 79.14 75.70 15.42 19.88 31.50 24.70 54.67 71.29 58.16 73.45 56.36 72.35 64.40 78.88 32.84 24.47 38.25 23.40 5.2 55.26 60.98 66.69 - 59.23 61.79 68.50 - 57.18 61.38 67.58 84.10 54.64 70.68 74.43 96.97 50.40 29.52 22.81 4.89 Table 7: Baseline parsing results split by sub corpora and noisiness level word clustering within a PCFG-LA framework. Indeed, we have successfully applied these techniques for French out-of-domain parsing (Candito et al., 2011), as well as for parsing noisy English web data (Seddah et al., 2012). On the longer term we intend to apply our normalization and correction module before parsing. The parser will then be provided with corrected tokens, closely matching our regular training data, instead of unedited ones. This will compensate the lack of user generated content large unlabeled corpora, still lacking for French. 7 Conclusion As mentioned earlier, the French Social Media Bank shares with the Google web bank a common will to extend the traditional treebank domain towards user generated content. Although of a smal"
C12-1149,C10-2013,1,0.848791,"validation and correction by two annotators followed by an adjudication step. • Functional annotation followed by manual validation and correction by two annotators followed by and adjudication step. 5.1 Pre-annotation strategies for the tokenization and POS layers As mentioned above, we used two different strategies for tokenization and POS pre-annotation, depending on the noisiness score. For less noisy corpora (those with a noisiness score below 1), we used a slightly extended version of the tokenization and sentence splitting tools from our standard FTB-based parsing architecture, Bonsai (Candito et al., 2010). This is because we want to have a tokenization that is as close as possible from the principles underlying the FTB’s tokenization. Next, we used the POS-tagger MORFETTE (Chrupała et al., 2008) as a pre-annotator. For corpora with a high noisiness score, we used a specifically developed pre-annotation process. This is because in such corpora, spelling errors are even more frequent, but also because 15 In the Google Web Treebank, the counterpart of our tag Y is the tag GW. 2450 the original tokens rarely match sound linguistic units, as can be seen on the example in Table 4 taken from the DOCT"
C12-1149,W10-1409,1,0.828577,"ed over time and posters. The next step will involve collecting large unlabeled corpora to perform experiments with selftraining techniques (McClosky and Charniak, 2008; Foster et al., 2011b) and unsupervised 16 Simple linear regressions lead to the following results: without the normalization and correction wrapper, the slope is -4.8 and the correlation coefficient is 0.77; with the wrapper, the slope is -7.2 with a correlation coefficient as high as 0.88 (coefficients of determination are thus respectively 0.59 and 0.77). 17 For convenience, we provide also baseline results on the FTB, see (Candito and Seddah, 2010). 2453 DEV DOCTISSIMO high noisiness other JEUXVIDEOS. COM T WIT TER high noisiness other FACEBOOK high noisiness other all FTB SET LR LP F1 Pos acc. OOVs 37.22 69.68 66.56 41.20 70.19 66.46 39.11 69.94 66.51 51.72 77.96 74.56 40.47 15.56 20.46 62.07 68.06 64.14 69.21 63.09 68.63 64.89 79.70 55.90 64.13 - 58.71 65.48 - 57.27 64.80 83.81 64.34 72.69 96.44 T EST LR SET LP F1 Pos acc. OOVs 70.10 70.59 71.68 71.44 70.88 71.02 79.14 75.70 15.42 19.88 31.50 24.70 54.67 71.29 58.16 73.45 56.36 72.35 64.40 78.88 32.84 24.47 38.25 23.40 5.2 55.26 60.98 66.69 - 59.23 61.79 68.50 - 57.18 61.38 67.58 84.1"
C12-1149,F12-2024,1,0.873511,"iness score ‘Forplay have disappeared for at least 6 months, that is there is almost none.’ Parseval F-measure metric between two functionally annotated set of parses. Agreements range between 93.4 for FACEBOOK data and 97.44 for JEUXVIDEOS.COM (Table 5) and are on the same range than the DCU’s Twitter corpus agreement score (Foster et al., 2011a). Similarly to that corpus, the disagreements involve fragments, interjections and the syntactic status to assign to meta-tokens elements. We note that our agreement scores are higher than those reported in other out-of-domain initiatives for French (Candito and Seddah, 2012). This small annotation error rate comes from the fact that the same team annotated both treebanks and was thus highly trained for that task. Maybe more importantly, social media sentences tend to be shorter than their edited counterparts so once POS tagging errors are solved, the annotation task is made relatively easier. DCU ’ S DOCTISSIMO T WIT TER T WIT TERB ANK 95.05 95.40 95.8 JEUXVIDEOS. COM FACEBOOK - 97.44 93.40 - Table 5: Inter Annotator agreement 6 Preliminary experiments Experimental Protocol In the following experiments, we used the FTB -UC as training data set, in its classical s"
C12-1149,chrupala-etal-2008-learning,0,0.0250729,"Missing"
C12-1149,Y09-1013,1,0.861344,"evelopment corpus (all subcorpora but for the noisy Facebook subcorpus) n-gram sequences involving unknown tokens or occurring at an unexpectedly high frequency; then we manually selected the relevant ones and provided them manually with a corresponding “correction”. The number of “corrected tokens” obtained by applying these rules might be different from the number of original tokens. In such cases, we use 1-to-n or n-to-1 mappings. For example, the rule ni a pa → n’ y a pas explicitely states that ni is an amalgam for n’ and y, whereas pas is the correction of pa. 4. We use the MElt tagger (Denis and Sagot, 2009), trained on the FTB -UC and the Lefff lexicon (Sagot, 2010), for POS-tagging the sequence of corrected “tokens”. 5. We apply a set of 15 generic and almost language-independent manually crafted rewriting rules, originally developed for English data (see below), that aim at assigning the correct POS to tokens that belong to categories not found in MElt’s training corpus, i.e., the FTB; for example, all URLs and e-mail addresses are post-tagged as proper nouns whatever the tag provided by MElt; likewise, all smileys get the POS for interjections. 6. We assign POS tags to the original tokens bas"
C12-1149,P11-1118,0,0.0227658,"aptation. Yet, this is far from being the case as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training data (imperative usage, direct discourse, slang, etc.). This suboptimal parsing behavior on web data was in turn confirmed in follow-up works on Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL shared task, organized by Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Needless to say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for"
C12-1149,N10-1060,0,0.537119,"is highlights the high difficulty of automatically processing such noisy data in a MRL. KEYWORDS: Treebanking, User Generated Content, Parsing, Social Media. Proceedings of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441 1 Introduction Complaining about the lack of robustness of statistical parsers whenever they are applied on out-of-domain text has almost became an overused cliché over the last few years. It remains true that such parsers only perform well on texts that are comparable to their training corpus, especially in terms of genre. As noted by Foster (2010) and Foster et al. (2011b), most studies on out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006a,b), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). The common point between these corpora is that they are edited texts. This means that their underlying syntax, spelling, tokenization and typography remain standard, even if they slightly depart from the newspaper genre. Therefore, standard NLP tools can be used on such corpor"
C12-1149,I11-1100,0,0.0885155,"Missing"
C12-1149,W07-2204,1,0.926577,"Missing"
C12-1149,W01-0521,0,0.0471571,". Proceedings of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441 1 Introduction Complaining about the lack of robustness of statistical parsers whenever they are applied on out-of-domain text has almost became an overused cliché over the last few years. It remains true that such parsers only perform well on texts that are comparable to their training corpus, especially in terms of genre. As noted by Foster (2010) and Foster et al. (2011b), most studies on out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006a,b), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). The common point between these corpora is that they are edited texts. This means that their underlying syntax, spelling, tokenization and typography remain standard, even if they slightly depart from the newspaper genre. Therefore, standard NLP tools can be used on such corpora. Now, new forms of electronic communication have emerged in the last few years,namely social media and Web 2.0 communication media, either synchronous (m"
C12-1149,P11-2008,0,0.148522,"Missing"
C12-1149,C10-1045,0,0.0162317,"o say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for examples, techniques built on richer syntactic models, lexical data sparseness reduction or rich feature set. See (Tsarfaty and Sima’an, 2008; Versley and Rehbein, 2009; Candito and Crabbé, 2009; Green and Manning, 2010) to name but a few. The questions are thus to know: (1) to what extend MRL user generated content is parsable? and (2) more importantly, what is needed to fill that performance gap? To answer question 1, we introduce the first release of the French Social Media Treebank, a representative gold standard treebank for French user-generated data. This treebank consists in around 1,700 sentences extracted from various types of French Web 2.0 user generated content (Facebook, Twitter, video games and medical board). This treebank was developed independently from the Google Web Treebank (Bies et al.,"
C12-1149,P08-2026,0,0.0646701,"Missing"
C12-1149,N06-1020,0,0.0516947,"of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441 1 Introduction Complaining about the lack of robustness of statistical parsers whenever they are applied on out-of-domain text has almost became an overused cliché over the last few years. It remains true that such parsers only perform well on texts that are comparable to their training corpus, especially in terms of genre. As noted by Foster (2010) and Foster et al. (2011b), most studies on out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006a,b), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). The common point between these corpora is that they are edited texts. This means that their underlying syntax, spelling, tokenization and typography remain standard, even if they slightly depart from the newspaper genre. Therefore, standard NLP tools can be used on such corpora. Now, new forms of electronic communication have emerged in the last few years,namely social media and Web 2.0 communication media, either synchronous (micro-blogging) or async"
C12-1149,P06-1043,0,0.0320051,"of COLING 2012: Technical Papers, pages 2441–2458, COLING 2012, Mumbai, December 2012. 2441 1 Introduction Complaining about the lack of robustness of statistical parsers whenever they are applied on out-of-domain text has almost became an overused cliché over the last few years. It remains true that such parsers only perform well on texts that are comparable to their training corpus, especially in terms of genre. As noted by Foster (2010) and Foster et al. (2011b), most studies on out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006a,b), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). The common point between these corpora is that they are edited texts. This means that their underlying syntax, spelling, tokenization and typography remain standard, even if they slightly depart from the newspaper genre. Therefore, standard NLP tools can be used on such corpora. Now, new forms of electronic communication have emerged in the last few years,namely social media and Web 2.0 communication media, either synchronous (micro-blogging) or async"
C12-1149,P06-1055,0,0.162253,"Missing"
C12-1149,N07-1051,0,0.0137649,"provided by MElt for the corrected token is assigned to the corresponding original token. This architecture is now available as part of the MElt distribution. It was also applied on English web data in the context of the SANCL shared task on parsing web data (Petrov and McDonald, 2012), with state-of-the-art results (Seddah et al., 2012). 5.2 Annotation strategy for constituency and functional annotation Parse pre-annotation was achieved using a state-of-the-art statistical parser trained on the FTB UC, provided with the manually validated tagging. The parser we used was the Berkeley parser (Petrov and Klein, 2007) adapted to French (Crabbé and Candito, 2008). Note that when the validated pos tags were discarded by the parser, in case of too many unknown word-pos pairs, those were reinserted. To assess the quality of annotation, we calculated the inter annotator agreement using the 2451 Original tokens Gold corrected “tokens” Automatically corrected and POS-tagged “tokens” sa fé o moin 6 mois qe les preliminaires sont sauté c a dire qil yen a presk pa ça fait au_moins 6 mois que les préliminaires sont sautés c’est-à-dire qu’ il y en a presque pas ça/PRO fait/V au/P+D moins/ADV 6/DET mois/NC que/PROREL l"
C12-1149,sagot-2010-lefff,1,0.839342,") n-gram sequences involving unknown tokens or occurring at an unexpectedly high frequency; then we manually selected the relevant ones and provided them manually with a corresponding “correction”. The number of “corrected tokens” obtained by applying these rules might be different from the number of original tokens. In such cases, we use 1-to-n or n-to-1 mappings. For example, the rule ni a pa → n’ y a pas explicitely states that ni is an amalgam for n’ and y, whereas pas is the correction of pa. 4. We use the MElt tagger (Denis and Sagot, 2009), trained on the FTB -UC and the Lefff lexicon (Sagot, 2010), for POS-tagging the sequence of corrected “tokens”. 5. We apply a set of 15 generic and almost language-independent manually crafted rewriting rules, originally developed for English data (see below), that aim at assigning the correct POS to tokens that belong to categories not found in MElt’s training corpus, i.e., the FTB; for example, all URLs and e-mail addresses are post-tagged as proper nouns whatever the tag provided by MElt; likewise, all smileys get the POS for interjections. 6. We assign POS tags to the original tokens based on the mappings between corrected POStagged tokens and or"
C12-1149,W10-1401,1,0.834989,"Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL shared task, organized by Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Needless to say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for examples, techniques built on richer syntactic models, lexical data sparseness reduction or rich feature set. See (Tsarfaty and Sima’an, 2008; Versley and Rehbein, 2009; Candito and Crabbé, 2009; Green and Manning, 2010) to name but a few. The questions are thus to know: (1) to what extend MRL user generated content is parsable? and (2) more importantly, what is needed to fill that performance gap? To answer question 1, we introduce the first release of the French Social Media Treebank, a representative gold standard treeban"
C12-1149,C08-1112,0,0.0270883,"Missing"
C12-1149,W09-3820,0,0.0221679,"of Web texts (Petrov and McDonald, 2012). Needless to say, such observations are likely to be even more true on web data written in morphologically rich languages (MRLS). These languages are already known to be arguably harder to parse than English for a variety of reasons (e.g., small treebank size, rich inflexion, free word order, etc.) exposed in details in (Tsarfaty et al., 2010). However, a lot of progress has been made in parsing MRLS using, for examples, techniques built on richer syntactic models, lexical data sparseness reduction or rich feature set. See (Tsarfaty and Sima’an, 2008; Versley and Rehbein, 2009; Candito and Crabbé, 2009; Green and Manning, 2010) to name but a few. The questions are thus to know: (1) to what extend MRL user generated content is parsable? and (2) more importantly, what is needed to fill that performance gap? To answer question 1, we introduce the first release of the French Social Media Treebank, a representative gold standard treebank for French user-generated data. This treebank consists in around 1,700 sentences extracted from various types of French Web 2.0 user generated content (Facebook, Twitter, video games and medical board). This treebank was developed indep"
C12-1149,I05-1006,0,\N,Missing
C12-1149,N03-1031,0,\N,Missing
C12-1149,N10-1004,0,\N,Missing
candito-etal-2014-developing,mouton-etal-2010-framenet,1,\N,Missing
candito-etal-2014-developing,burchardt-etal-2006-salto,0,\N,Missing
candito-etal-2014-developing,burchardt-pennacchiotti-2008-fate,0,\N,Missing
candito-etal-2014-developing,sagot-etal-2010-lexicon,1,\N,Missing
candito-etal-2014-developing,N10-1138,0,\N,Missing
candito-etal-2014-developing,burchardt-etal-2006-salsa,0,\N,Missing
candito-etal-2014-developing,P98-1013,0,\N,Missing
candito-etal-2014-developing,C98-1013,0,\N,Missing
candito-etal-2014-developing,W13-4917,1,\N,Missing
candito-etal-2014-developing,P11-2023,1,\N,Missing
candito-etal-2014-developing,J02-3001,0,\N,Missing
candito-etal-2014-developing,J05-1004,0,\N,Missing
candito-etal-2014-developing,heppin-gronostaj-2012-rocky,0,\N,Missing
candito-etal-2014-developing,I11-1132,1,\N,Missing
candito-etal-2014-developing,abeille-barrier-2004-enriching,0,\N,Missing
clement-etal-2004-morphology,C94-1097,0,\N,Missing
clement-etal-2004-morphology,A97-1052,0,\N,Missing
clement-etal-2004-morphology,C00-1032,0,\N,Missing
D19-5539,I13-1041,0,0.0212033,"lin et al., 2018) and Machine Translation (Lample and Conneau, 2019). Moreover, it has recently been shown to capture a rich set of syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019), without the added complexity of more complex syntax-based language models. However, it remains unclear and, to the best of our knowledge, unexplored, how well can BERT be used in handling non-canonical text such as User-Generated Content (UGC), especially in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions), and non standard syntactic constructions and spelling errors. This type of non-canonical text, which we characterize as noisy, negatively impacts NLP models performances on many tasks as shown in (van der Goot et al., 2017; van der Goot and van Noord, 2018; Moon et al., 2018; Michel and Neubig, 2018) on respectively Part-of297 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 297–306 c Hon"
D19-5539,W15-4319,0,0.535767,"Missing"
D19-5539,W15-4312,0,0.0422461,"Missing"
D19-5539,W15-4318,0,0.0327642,"Missing"
D19-5539,N19-1423,0,0.0848551,"Missing"
D19-5539,N19-1419,0,0.0313704,"y of its transformerbased architecture, these three aspects respectively enable BERT to elegantly cope with out-ofvocabulary words and to include contextual information at the token and at the sentence levels, while fully taking advantage of a training corpus containing billions of words. Without listing all of them, BERT successfully improved the state-of-the-art for a number of tasks such as Name-Entity Recognition, Question Answering (Devlin et al., 2018) and Machine Translation (Lample and Conneau, 2019). Moreover, it has recently been shown to capture a rich set of syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019), without the added complexity of more complex syntax-based language models. However, it remains unclear and, to the best of our knowledge, unexplored, how well can BERT be used in handling non-canonical text such as User-Generated Content (UGC), especially in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions),"
D19-5539,P19-1356,1,0.835492,"architecture, these three aspects respectively enable BERT to elegantly cope with out-ofvocabulary words and to include contextual information at the token and at the sentence levels, while fully taking advantage of a training corpus containing billions of words. Without listing all of them, BERT successfully improved the state-of-the-art for a number of tasks such as Name-Entity Recognition, Question Answering (Devlin et al., 2018) and Machine Translation (Lample and Conneau, 2019). Moreover, it has recently been shown to capture a rich set of syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019), without the added complexity of more complex syntax-based language models. However, it remains unclear and, to the best of our knowledge, unexplored, how well can BERT be used in handling non-canonical text such as User-Generated Content (UGC), especially in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions), and non standard synta"
D19-5539,C12-1097,0,0.0676726,"Missing"
D19-5539,D18-1050,0,0.0264044,"ly in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions), and non standard syntactic constructions and spelling errors. This type of non-canonical text, which we characterize as noisy, negatively impacts NLP models performances on many tasks as shown in (van der Goot et al., 2017; van der Goot and van Noord, 2018; Moon et al., 2018; Michel and Neubig, 2018) on respectively Part-of297 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 297–306 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics Speech Tagging, Syntactic Parsing, Name-Entity Recognition and Machine Translation. In this context and as impactful as BERT was shown to be, its ability to handle noisy inputs is still an open question2 . Indeed, as highlighted above, it was trained on highly edited texts, as expected from Wikipedia and BookCorpus sources, which differ from UGC at many levels of linguistic descriptions, a"
D19-5539,N13-1037,0,0.41747,"ion Answering (Devlin et al., 2018) and Machine Translation (Lample and Conneau, 2019). Moreover, it has recently been shown to capture a rich set of syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019), without the added complexity of more complex syntax-based language models. However, it remains unclear and, to the best of our knowledge, unexplored, how well can BERT be used in handling non-canonical text such as User-Generated Content (UGC), especially in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions), and non standard syntactic constructions and spelling errors. This type of non-canonical text, which we characterize as noisy, negatively impacts NLP models performances on many tasks as shown in (van der Goot et al., 2017; van der Goot and van Noord, 2018; Moon et al., 2018; Michel and Neubig, 2018) on respectively Part-of297 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Te"
D19-5539,N10-1060,0,0.0328086,"h as Name-Entity Recognition, Question Answering (Devlin et al., 2018) and Machine Translation (Lample and Conneau, 2019). Moreover, it has recently been shown to capture a rich set of syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019), without the added complexity of more complex syntax-based language models. However, it remains unclear and, to the best of our knowledge, unexplored, how well can BERT be used in handling non-canonical text such as User-Generated Content (UGC), especially in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions), and non standard syntactic constructions and spelling errors. This type of non-canonical text, which we characterize as noisy, negatively impacts NLP models performances on many tasks as shown in (van der Goot et al., 2017; van der Goot and van Noord, 2018; Moon et al., 2018; Michel and Neubig, 2018) on respectively Part-of297 Proceedings of the 2019 EMNLP Workshop W-NUT: The"
D19-5539,D18-1542,0,0.134757,"Missing"
D19-5539,W17-4404,0,0.127361,"Missing"
D19-5539,N18-1078,0,0.0550727,"Missing"
D19-5539,N18-1202,0,0.150892,"Missing"
D19-5539,C12-1149,1,0.821731,"ty Recognition, Question Answering (Devlin et al., 2018) and Machine Translation (Lample and Conneau, 2019). Moreover, it has recently been shown to capture a rich set of syntactic information (Hewitt and Manning, 2019; Jawahar et al., 2019), without the added complexity of more complex syntax-based language models. However, it remains unclear and, to the best of our knowledge, unexplored, how well can BERT be used in handling non-canonical text such as User-Generated Content (UGC), especially in a low resource scenario. This question is the focus of this paper. As described in (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013; Baldwin et al., 2013), UGC is often characterized by the extensive use of abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs or at mentions), and non standard syntactic constructions and spelling errors. This type of non-canonical text, which we characterize as noisy, negatively impacts NLP models performances on many tasks as shown in (van der Goot et al., 2017; van der Goot and van Noord, 2018; Moon et al., 2018; Michel and Neubig, 2018) on respectively Part-of297 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy"
D19-5539,P11-1038,0,0.574297,"words. In this purpose, we make three contributions: • We design a WordPiece tokenizer that enforces alignment between canonical and noisy tokens. • We enhance the BERT architecture so that the model is able to add extra tokens or remove them when normalisation requires it. • We fine-tune the overall architecture with a novel noise-specific strategy. In a few words, our paper is the first attempt to successfully design a domain transfer model based on BERT in a low resource setting. 2 Related Work There is an extensive literature on normalizing text from UGC. The first systematic attempt was Han and Baldwin (2011). They released 549 tweets with their normalized word-aligned counterparts and the first result for a normalization system on tweets. Their model was a Support-Vector-Machine for detecting noisy words. Then a lookup and ngram based system would pick the best candidate among the closest ones in terms of edit and phonetic distances. Following this work, the literature explored different modelling framework to tackle the task, whether it is Statistical Machine Translation (Li and Liu, 2012), purely unsupervised approach (Yang and Eisenstein, 2013), or syllables level model (Xu et al., 2015). In 2"
D19-5539,W15-4311,0,0.528984,"is work, the literature explored different modelling framework to tackle the task, whether it is Statistical Machine Translation (Li and Liu, 2012), purely unsupervised approach (Yang and Eisenstein, 2013), or syllables level model (Xu et al., 2015). In 2015, on the occasion of the Workshop on Noisy User-Generated Text, a shared task on lexical normalization of English tweets was organized (Baldwin et al., 2015) for which a collection of annotated tweets for training and evaluation was released. We will refer it as the lexnorm15 dataset. A wide range of approaches competed. The best approach (Supranovich and Patsepnia, 2015) used a UGC feature-based CRF model for detection and normalization. In 2016, the MoNoise model (van der Goot and van Noord, 2017) significantly improved the Stateof-the-art with a feature-based Random Forest. The model ranks candidates provided by modules such as a spelling checker (aspell), a n-gram based language model and word embeddings trained on millions of tweets. In summary, two aspects of the past literature on UGC normalization are striking. First, all the past work is based on UGC-specific resources such as lexicons or large UGC corpora. Second, most successful models are modular i"
D19-5539,P15-1089,0,0.156879,"Han and Baldwin (2011). They released 549 tweets with their normalized word-aligned counterparts and the first result for a normalization system on tweets. Their model was a Support-Vector-Machine for detecting noisy words. Then a lookup and ngram based system would pick the best candidate among the closest ones in terms of edit and phonetic distances. Following this work, the literature explored different modelling framework to tackle the task, whether it is Statistical Machine Translation (Li and Liu, 2012), purely unsupervised approach (Yang and Eisenstein, 2013), or syllables level model (Xu et al., 2015). In 2015, on the occasion of the Workshop on Noisy User-Generated Text, a shared task on lexical normalization of English tweets was organized (Baldwin et al., 2015) for which a collection of annotated tweets for training and evaluation was released. We will refer it as the lexnorm15 dataset. A wide range of approaches competed. The best approach (Supranovich and Patsepnia, 2015) used a UGC feature-based CRF model for detection and normalization. In 2016, the MoNoise model (van der Goot and van Noord, 2017) significantly improved the Stateof-the-art with a feature-based Random Forest. The mod"
D19-5539,D13-1007,0,0.0270193,"izing text from UGC. The first systematic attempt was Han and Baldwin (2011). They released 549 tweets with their normalized word-aligned counterparts and the first result for a normalization system on tweets. Their model was a Support-Vector-Machine for detecting noisy words. Then a lookup and ngram based system would pick the best candidate among the closest ones in terms of edit and phonetic distances. Following this work, the literature explored different modelling framework to tackle the task, whether it is Statistical Machine Translation (Li and Liu, 2012), purely unsupervised approach (Yang and Eisenstein, 2013), or syllables level model (Xu et al., 2015). In 2015, on the occasion of the Workshop on Noisy User-Generated Text, a shared task on lexical normalization of English tweets was organized (Baldwin et al., 2015) for which a collection of annotated tweets for training and evaluation was released. We will refer it as the lexnorm15 dataset. A wide range of approaches competed. The best approach (Supranovich and Patsepnia, 2015) used a UGC feature-based CRF model for detection and normalization. In 2016, the MoNoise model (van der Goot and van Noord, 2017) significantly improved the Stateof-the-art"
F12-2008,2004.jeptalnrecital-recital.6,1,0.748575,"Missing"
F12-2008,Y09-1013,1,0.893627,"Missing"
F12-2008,W10-1807,1,0.883522,"Missing"
F12-2008,J93-2004,0,0.04017,"Missing"
F12-2008,W04-2104,0,0.0666194,"Missing"
F12-2008,sagot-2010-lefff,1,0.897512,"Missing"
F12-2050,E06-1002,0,0.150646,"Missing"
F12-2050,D07-1074,0,0.124842,"Missing"
F12-2050,doddington-etal-2004-automatic,0,0.0667126,"Missing"
F12-2050,P05-1045,0,0.00822381,"Missing"
F12-2050,W09-3025,0,0.0605079,"Missing"
F12-2050,W11-0411,0,0.0228665,"Missing"
F12-2050,M98-1002,0,0.0941064,"Missing"
F12-2050,sagot-stern-2012-aleda,1,0.877052,"Missing"
F12-2050,W03-0419,0,0.115359,"Missing"
F12-2050,sekine-nobata-2004-definition,0,0.094042,"Missing"
F13-1030,2010.jeptalnrecital-court.15,1,0.8074,"Missing"
F13-1030,chrupala-etal-2008-learning,0,0.0242676,"Missing"
F13-1030,J01-2001,0,0.0690429,"Missing"
F13-1030,P11-1038,0,0.0721505,"Missing"
F13-1030,P98-1120,0,0.111829,"Missing"
F13-1030,maurel-2008-prolexbase,0,0.0487287,"Missing"
F13-1030,J97-3003,0,0.25796,"Missing"
F13-1030,W02-0604,0,0.100415,"Missing"
F13-1030,W04-2104,0,0.0924678,"Missing"
F13-1030,sagot-2010-lefff,1,0.886244,"Missing"
F13-1030,sagot-stern-2012-aleda,1,0.744501,"Missing"
F13-1030,2002.jeptalnrecital-long.22,0,0.0629684,"Missing"
F14-1013,P10-1079,0,0.0502405,"Missing"
F14-1013,N09-3006,0,0.0432847,"Missing"
F14-1013,P00-1037,0,0.248111,"Missing"
F14-1013,2007.jeptalnrecital-long.11,0,0.105792,"Missing"
F14-1013,P11-1038,0,0.0496308,"Missing"
F14-1013,C90-2036,0,0.760928,"Missing"
F14-1013,2008.jeptalnrecital-long.13,0,0.129114,"Missing"
F14-1013,P98-1120,0,0.0539483,"Missing"
F14-1013,C00-1071,0,0.166009,"Missing"
F14-1013,P06-1129,0,0.0470996,"Missing"
F14-1013,max-wisniewski-2010-mining,0,0.0267017,"Missing"
F14-1013,J96-1003,0,0.355489,"Missing"
F14-1013,P11-1094,0,0.0567437,"Missing"
F14-1013,sagot-2010-lefff,1,0.866161,"Missing"
F14-1013,C12-1149,1,0.849247,"Missing"
F14-1013,W05-0616,0,0.026122,"Missing"
F14-1013,P02-1019,0,0.113211,"Missing"
F14-2007,P97-1003,0,0.0782988,"Missing"
F14-2007,F12-2042,1,0.684804,"Missing"
F14-2007,N13-1024,1,0.887446,"Missing"
F14-2007,P02-1035,0,0.0448938,"Missing"
F14-2007,sagot-2010-lefff,1,0.882802,"Missing"
F14-2007,W13-5706,0,0.0344323,"Missing"
F14-2009,P00-1037,0,0.271076,"Missing"
F14-2009,M98-1028,0,0.213554,"Missing"
F14-2009,W04-3238,0,0.0860192,"Missing"
F14-2009,C90-2036,0,0.408851,"Missing"
F14-2009,W04-2216,0,0.0601839,"Missing"
F14-2009,W03-0419,0,0.0345301,"Missing"
gabor-etal-2012-boosting,vilnat-etal-2010-passage,1,\N,Missing
gabor-etal-2012-boosting,S07-1002,0,\N,Missing
gabor-etal-2012-boosting,C92-2082,0,\N,Missing
gabor-etal-2012-boosting,C08-1084,0,\N,Missing
gabor-etal-2012-boosting,J02-3004,0,\N,Missing
gabor-etal-2012-boosting,2010.jeptalnrecital-court.19,0,\N,Missing
hanoka-sagot-2012-wordnet,W02-0808,0,\N,Missing
hanoka-sagot-2012-wordnet,P10-1023,0,\N,Missing
hanoka-sagot-2012-wordnet,P09-1030,0,\N,Missing
hanoka-sagot-2012-wordnet,P91-1017,0,\N,Missing
hanoka-sagot-2014-open,serasset-2012-dbnary,0,\N,Missing
hanoka-sagot-2014-open,W98-0709,0,\N,Missing
hanoka-sagot-2014-open,P10-1023,0,\N,Missing
hanoka-sagot-2014-open,P09-1030,0,\N,Missing
hanoka-sagot-2014-open,P13-2048,0,\N,Missing
hanoka-sagot-2014-open,C10-3010,0,\N,Missing
K17-3026,L16-1262,0,\N,Missing
K17-3026,W14-6111,1,\N,Missing
K18-2023,N15-1184,0,0.0693335,"Missing"
K18-2023,L16-1262,0,0.0200927,"Missing"
K18-2023,N18-1202,0,0.227629,"eate a high performing graph parser, we implement a large BiLSTM-LM network (independent of the ELMoLex parser) which is highly regularized to prevent data overfitting and able to learn useful features. Our BiLSTM-LM consumes both the word and tag embedding as input, which can be formally written as: LM (word) = vi xLM i LM (word) vi = LM (U P oS) ⊕ vi LM (f air) vi ⊕ , (3) LM (char) vi . LM (f air) LM (char) In equation 3, the notations vi , vi LM (U P oS) GP (f air) and vi are the counterparts of vi , GP (char) GP (U P oS) vi and vi respectively. Note that ELMo, as proposed in Peters et al. (2018), builds only on character embeddings, automatically inferring the PoS information in the lower layers of the LSTM network. Since we have less training data to work with, we feed the PoS information explicitly which helps in easening the optimization process of our BiLSTM-LM network. Given a sequence of n LM words, xLM 1 , . . . , xn , BiLSTM-LM learns by maximizing the log likelihood of forward LSTM and backward LSTM directions, which can be defined as: n ∑ − → − → (log Pr(xLM |xLM , . . . , xLM i 1 i−1 ; Θx ; Θ LST M , Θ s )) i=1 ← − ← − LM |xLM (4) + (log Pr(xLM i i+1 , . . . , xn ; Θx ; Θ"
K18-2023,C16-1030,0,0.0323741,"is problem, ELMoLex relies on four signals from the proposed embedding layer: fi_pud sv_pud cs_pud Table 1: Treebanks to source the training data for Delexicalized Parsing of a given target treebank. GP (lex) (vi ) can be computed as follows:9 GP (lex) vi = m ∑ GP (mf ) slex mfj vmfj . (7) j=1 In equation 7, slex mfj corresponds to the softmaxnormalized weight which is a learnable parameter for each available morphological feature (in this case, it is mfj ). The general idea to perform a weighted sum to extract relevant features has been previously studied in the context of sequence labeling (Rei et al., 2016) for integrating word and character level features. Combining the distributional knowledge of words along with the semantic lexicons has been extensively studied for estimating high quality word vectors, also referred to as ‘retrofitting’ in literature (Faruqui et al., 2015). 2.4 Delexicalized Parsing We perform delexicalized “language family” parsing for treebanks with less than 50 or no train sentences (as shown in Table 1). The delexicalized version of ELMoLex throws away word-level information such as vGP (word) and vGP (char) and works with the rest. The source treebanks are concatenated"
K18-2023,L18-1292,1,0.815553,"They exploit the relevant sub-parts of a word such as suffixes or prefixes to generate word representations. They can generalize to unknown words if these unknown words follow such generalizations. Otherwise, they fail to add any improvement (Sagot and Martínez Alonso, 2017) and we may need to look for other sources to complement the information provided by characterlevel embeddings. We term this problem as linguistic naivety. ELMoLex taps into the large inventory of morphological features (gender, number, case, tense, mood, person, etc.) provided by external resources, namely the UDLexicons (Sagot, 2018) lexicon collection, which cover words with an irregular morphology as well as words not present in the training data. Essentially, these lexicons consist of ⟨word, UPoS, morphological features⟩ triplets, which we query using ⟨word, UPoS⟩ pair resulting in one or more hits. When we attempt to integrate the information from these hits, we face the challenge of disambiguation as not all the morphological features returned by the query are relevant to the focal ⟨word, UPoS⟩ pair. ELMoLex relies on attention mechanism (Bahdanau et al., 2014) to select the relevant morphological features, thereby h"
K18-2023,W17-6304,1,0.857446,"Missing"
L18-1292,W06-2920,0,0.0811831,"ly treebanks. This type of resource has recently seen the emergence of a de facto trans-lingual standard and the publication of an increasing number of treebanks for numerous languages following a universal set of guidelines, encoded in the CoNLL-U format and gathered under the name Universal Dependencies (hereafter UD).1 This treebank collection (Nivre et al., 2016; Nivre et al., 2017) follows several previous initiatives, such as the proposal of a universal part-of-speech tagset (Petrov et al., 2012) and the multilingual datasets released in the context of several shared tasks and projects (Buchholz and Marsi, 2006; Nivre et al., 2007; Zeman et al., 2012; Seddah et al., 2013). The UD initiative has therefore allowed a simpler, unified access to treebank resources, giving a new impetus to research in topics such as multilingual and cross-lingual tokenisation, part-of-speech tagging, dependency parsing and quantitative linguistics. It is therefore important for morphological lexicons, another major source of linguistic information for such tasks, to also be available for many lan1 http://www.universaldependencies.org guages following a universal set of guidelines. The obvious choice would be to make use o"
L18-1292,erjavec-2010-multext,0,0.0105883,"f lexical entries that typically associate a wordform with a part-of-speech (or morphosyntactic category), morphological features (such as gender, tense, etc.) and a lemma. Beyond direct lexicon lookup, used in virtually all types of natural language processing applications and computational linguistic studies, morphological lexicons have been shown to significantly improve tasks such as part-of-speech tagging and parsing. There is currently no universally accepted way to encode morphological lexical information. Past multilingual projects such as MULTEXT/MULTEXT-East (Ide and V´eronis, 1994; Erjavec, 2010) have resulted in the publication of morphological lexicons for a number of languages based on the same set of categories and morphosyntactic features, but they are still limited in scope. Yet another type of language resource embeds morphological lexical information, namely treebanks. This type of resource has recently seen the emergence of a de facto trans-lingual standard and the publication of an increasing number of treebanks for numerous languages following a universal set of guidelines, encoded in the CoNLL-U format and gathered under the name Universal Dependencies (hereafter UD).1 Thi"
L18-1292,C94-1097,0,0.666356,"Missing"
L18-1292,R09-1049,1,0.771655,"e decreases. 11 The weight of an operation tranforming a source character cs (or the empty string in the case of an insertion) into a target character ctq (or the empty string in the case of a deletion) is defined as 1 − transfocc(cs ,ct ) , occ(cs ) 5. We use the manually corrected file to automatically convert the source lexicon into a CoNLL-UL lexicon. We applied this strategy on 18 freely available lexicons (see Table 2): • 8 lexicons developed in the Alexina framework (Sagot, 2010), covering French (Sagot, 2010, Lefff ), Polish (Sagot, 2007, PolLex), Slovak (Sagot, 2005, SkLex), Spanish (Molinero et al., 2009, Leffe), Galician (Leffga), Persian (Sagot and Walther, 2010, PerLex) German (Sagot, 2014, DeLex) and English (EnLex); • 10 other lexicons covering Italian (Zanchetta and Baroni, 2005, Morph-it!), Swedish (Borin et al., 2008, where transfocc(cs , ct ) is the number of times cs was transformed into ct in the previous iteration, and occ(cs ) is the total number of cs ’s in the source lexicon. 12 I.e. the treebank whose identifier is the language code itself (e.g. fr rather than fr-sequoia). 1863 From To Form or Token Lemma UPOS CPOS UFEAT Misc 0 1 encodent encoder VERB Mood=Ind|Number=Plur|Pers"
L18-1292,L18-1608,1,0.88269,"Missing"
L18-1292,W14-4607,0,0.0679566,"Missing"
L18-1292,L16-1262,0,0.117903,"Missing"
L18-1292,oliver-tadic-2004-enlarging,0,0.161545,"Missing"
L18-1292,petrov-etal-2012-universal,0,0.0306717,"are still limited in scope. Yet another type of language resource embeds morphological lexical information, namely treebanks. This type of resource has recently seen the emergence of a de facto trans-lingual standard and the publication of an increasing number of treebanks for numerous languages following a universal set of guidelines, encoded in the CoNLL-U format and gathered under the name Universal Dependencies (hereafter UD).1 This treebank collection (Nivre et al., 2016; Nivre et al., 2017) follows several previous initiatives, such as the proposal of a universal part-of-speech tagset (Petrov et al., 2012) and the multilingual datasets released in the context of several shared tasks and projects (Buchholz and Marsi, 2006; Nivre et al., 2007; Zeman et al., 2012; Seddah et al., 2013). The UD initiative has therefore allowed a simpler, unified access to treebank resources, giving a new impetus to research in topics such as multilingual and cross-lingual tokenisation, part-of-speech tagging, dependency parsing and quantitative linguistics. It is therefore important for morphological lexicons, another major source of linguistic information for such tasks, to also be available for many lan1 http://ww"
L18-1292,P16-2067,0,0.029175,"based on parsing results on the development sets. In particular, we chose to use either this new tagger or the UDPipe baseline provided by the organisers. Although our new tagger had higher accuracies than UDPipe on all datasets but 4 (Villemonte de La Clergerie et al., 2017, Table 1), we ended up using our tagger on only half of the testing datasets. This allowed us to be ranked 3/33 for UPOS tagging.15,16 In another recent experiment (Sagot and Mart´ınez Alonso, 2017), we have shown that these lexicons also improve tagging results when a state-of-the-art neural architecture, namely that of Plank et al. (2016), is extended to take into account external lexical information, even when word encodings and character-based encodings are used. 15 More recent, unofficial results using improved parsers have resulted in our tagger being selected more often rather than UDPipe, thus further improving our overall UPOS tagging results. 16 Our UFEAT scores in this shared task are not meaningful, because we explicitly decided to only predict a subset of all morphological features. 1864 4. lang. Apertium/Giellatekno-based resources type #simple #complex #distinct entries entries wforms ar bg ca cs da de el en es et"
L18-1292,W99-0511,0,0.0158148,"S UFEAT Misc 0 1 encodent encoder VERB Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin 0-2 0 1 1 2 auxquels a` lesquels a` lequel ADP PRON Gender=Masc|Number=Plur Table 1: Two entries resulting from the conversion of the Lefff into the CoNLL-UL format (for space reasons, the CPOS column is displayed as if it were empty). saldo), Ancient Greek (Heslin, 2007, Diogenes Ancient Greek lexicon), Latin (Heslin, 2007, Diogenes Latin lexicon), Croatian (Oliver and Tadi´c, 2004, hml), Irish (Mˇechura, 2014, INMDB), Norwegian (Bokm˚al) (The Language Council of Norway, 2011, OrdBankBM), Portuguese (Ranchhod et al., 1999, LabellexPT) and Slovenian (Krek et al., 2008, SloLeks). Some of the above-listed Alexina lexicons include information about multi-word tokens. For instance, the French token auxquels ‘to whichPL ’ is described in Lefff as the contraction of the two wordforms a` ‘to’ and lesquels ‘whichPL ’. Moreover, the part-of-speech of the wordforms involved is sometimes specified. We automatically extended our converted lexicons with CoNLL-UL entries for these multi-word tokens by combining existing entries for the underlying wordforms. Whenever part-of-speech information is provided for a wordfrom, we l"
L18-1292,W17-6304,1,0.904584,"Missing"
L18-1292,sagot-walther-2010-morphological,1,0.774716,"urce character cs (or the empty string in the case of an insertion) into a target character ctq (or the empty string in the case of a deletion) is defined as 1 − transfocc(cs ,ct ) , occ(cs ) 5. We use the manually corrected file to automatically convert the source lexicon into a CoNLL-UL lexicon. We applied this strategy on 18 freely available lexicons (see Table 2): • 8 lexicons developed in the Alexina framework (Sagot, 2010), covering French (Sagot, 2010, Lefff ), Polish (Sagot, 2007, PolLex), Slovak (Sagot, 2005, SkLex), Spanish (Molinero et al., 2009, Leffe), Galician (Leffga), Persian (Sagot and Walther, 2010, PerLex) German (Sagot, 2014, DeLex) and English (EnLex); • 10 other lexicons covering Italian (Zanchetta and Baroni, 2005, Morph-it!), Swedish (Borin et al., 2008, where transfocc(cs , ct ) is the number of times cs was transformed into ct in the previous iteration, and occ(cs ) is the total number of cs ’s in the source lexicon. 12 I.e. the treebank whose identifier is the language code itself (e.g. fr rather than fr-sequoia). 1863 From To Form or Token Lemma UPOS CPOS UFEAT Misc 0 1 encodent encoder VERB Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin 0-2 0 1 1 2 auxquels a` lesquels"
L18-1292,sagot-2010-lefff,1,0.876867,"re, with high-accuracy results (Villemonte de La Clergerie et al., 2017). In some cases, information from Apertium or Giellatekno converted into the UD format was complemented with information extracted from the training sections of the UD treebanks (v2.0). We also developed a simple, unsupervised yet original algorithm for transferring morphological lexical information from a resourced language to a closely-related one not covered by Apertium and Giellatekno. • We converted into the UD format a variety of freely available lexicons, among which the lexicons developed in the Alexina framework (Sagot, 2010).4 • We used the UD treebanks themselves (v2.0) in order to complete our lexicons with two types of information: (i) multi-word tokens and (ii) entries for categories (UPOS) not covered by the lexical sources mentioned above. Our contribution thereby lies in a collection of 53 UDcompatible lexicons covering 38 distinct languages and the methods used to create this collection.5 They are encoded 2 https://svn.code.sf.net/p/apertium/svn/languages https://victorio.uit.no/langtech/trunk/langs 4 These resources were not allowed in the CoNLL 2017 UD parsing shared task and were not used in this conte"
L18-1292,sagot-2014-delex,1,0.765117,"the case of an insertion) into a target character ctq (or the empty string in the case of a deletion) is defined as 1 − transfocc(cs ,ct ) , occ(cs ) 5. We use the manually corrected file to automatically convert the source lexicon into a CoNLL-UL lexicon. We applied this strategy on 18 freely available lexicons (see Table 2): • 8 lexicons developed in the Alexina framework (Sagot, 2010), covering French (Sagot, 2010, Lefff ), Polish (Sagot, 2007, PolLex), Slovak (Sagot, 2005, SkLex), Spanish (Molinero et al., 2009, Leffe), Galician (Leffga), Persian (Sagot and Walther, 2010, PerLex) German (Sagot, 2014, DeLex) and English (EnLex); • 10 other lexicons covering Italian (Zanchetta and Baroni, 2005, Morph-it!), Swedish (Borin et al., 2008, where transfocc(cs , ct ) is the number of times cs was transformed into ct in the previous iteration, and occ(cs ) is the total number of cs ’s in the source lexicon. 12 I.e. the treebank whose identifier is the language code itself (e.g. fr rather than fr-sequoia). 1863 From To Form or Token Lemma UPOS CPOS UFEAT Misc 0 1 encodent encoder VERB Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin 0-2 0 1 1 2 auxquels a` lesquels a` lequel ADP PRON Gender=Ma"
L18-1292,W13-4917,0,0.081606,"Missing"
L18-1292,L16-1680,0,0.0509806,"Missing"
L18-1292,K17-3026,1,0.890174,"Missing"
L18-1292,zeman-etal-2012-hamledt,0,0.0626522,"Missing"
L18-1292,K17-3001,0,0.0494802,"Missing"
L18-1608,W06-2920,0,0.370592,"Missing"
L18-1608,W02-1503,0,0.157572,"tactic parsing, in both pipeline and joint settings, and presenting new opportunities in the development of UD resources for low-resource languages. Keywords: Morphology, Universal Dependencies, Morphological Analysis, Morphological Ambiguity 1. Introduction The development of the universal dependencies (UD) framework and its treebank collection (Nivre et al., 2016; Nivre et al., 2017) follows many shared tasks and multilingual evaluation campaigns in which the linguistic representation schemes across different languages vary (Buchholz and Marsi, 2006; Nivre et al., 2007; Seddah et al., 2013; Butt et al., 2002; Zeman et al., 2012). The UD treebanks collection, in contrast, obeys a single set of annotation guidelines, and respects the discrepancies between surface input tokens and the output nodes in the syntax trees (a.k.a., the two-level representation principle.)1 The UD initiative has paved the way to the development of cross-lingual models for word segmentation, part-of-speech tagging and dependency parsing (Straka and Strakov´a, 2017), as well as cross-linguistic typological investigations (Futrell et al., 2015). Recently, the CoNLL 2017 Shared Task on Multilingual UD Parsing (Zeman et al., 20"
L18-1608,coltekin-2010-freely,1,0.858208,"Missing"
L18-1608,C16-1033,1,0.861239,"4), which is built on top of the databases of SAMA (Maamouri et al., 2010) to output 5 morphology that adheres to the UD Arabic treebank (Taji et al., 2017).6 The Arabic UD treebank, as with other Arabic treebanks, uses the Penn Arabic treebank tokenization scheme (Maamouri et al., 2004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of More and Tsarfaty (2016), based on the BGU Lexicon (Itai and Wintner, 2008), adapted to the UD Hebrew treebank.7 We only modified the HEBLEX SPMRL lattices format to follow the proposed CoNLL-UL format, as the HEBLEX annotations have already been adapted to the treebank counterpart (More and Tsarfaty, 2017). For Turkish, we developed a new morphological analyzer based on TRmorph (C¸o¨ ltekin, 2010).8 The analyzer follows the segmentation and morphological analysis scheme of the UD Turkish treebank v2.0 (Sulubacak et al., 2016) and Turkish-PUD treebank (Zeman et al., 2017). These treebanks have employed a different se"
L18-1608,K17-3027,1,0.780254,"004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of More and Tsarfaty (2016), based on the BGU Lexicon (Itai and Wintner, 2008), adapted to the UD Hebrew treebank.7 We only modified the HEBLEX SPMRL lattices format to follow the proposed CoNLL-UL format, as the HEBLEX annotations have already been adapted to the treebank counterpart (More and Tsarfaty, 2017). For Turkish, we developed a new morphological analyzer based on TRmorph (C¸o¨ ltekin, 2010).8 The analyzer follows the segmentation and morphological analysis scheme of the UD Turkish treebank v2.0 (Sulubacak et al., 2016) and Turkish-PUD treebank (Zeman et al., 2017). These treebanks have employed a different segmentation approach compared to the METU-Sabancı Turkish Treebank (Oflazer et al., 2003). In addition, form and lemma representations, POS tags and morphological tag sets have changed. The existing morphological analyzers are not compatible with this new representation. Thus we intro"
L18-1608,L16-1262,1,0.908557,"Missing"
L18-1608,pasha-etal-2014-madamira,1,0.772852,"w, and Turkish. For these morphological analyzers, their pre-existing morphological analyses adhere to schemes that differ from those employed in the respective UD treebanks. These discrepancies are due to differences between the morphological theories adopted by the UD treebanks developers and those employed by the developers of the morphological analyzers. Therefore, the adapted resources we provide are non-trivial to obtain, and required careful alignment of the morphosyntactic analyses with their UD treebank counterparts. For Arabic, we adapted the morphological analyzer used in MADAMIRA (Pasha et al., 2014), which is built on top of the databases of SAMA (Maamouri et al., 2010) to output 5 morphology that adheres to the UD Arabic treebank (Taji et al., 2017).6 The Arabic UD treebank, as with other Arabic treebanks, uses the Penn Arabic treebank tokenization scheme (Maamouri et al., 2004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of Mo"
L18-1608,L18-1292,1,0.841663,"L format. 3.2. Converted Morphological Lexicons As a complement to the CoNLL-UL-compatible analyzers described above, we have created a set of 53 CoNLL-ULcompatible morphological lexicons covering 38 languages, based on existing freely available resources.9 The source lexicons, the conversion processes and the resulting inventory of freely available CoNLL-UL lexicons are described https://conllul.github.io 3850 6 https://camel.abudhabi.nyu.edu/calima-star/ https://github.com/habeanf/yap 8 https://github.com/coltekin/TRmorph/tree/trmorph2 9 http://pauillac.inria.fr/˜sagot/udlexicons.html 7 in (Sagot, 2018).10 Here we only provide in Table 3 two examples converted from the Lefff , the Alexina lexicon for French. The first one illustrates the 1-to-1 case, with an entry converted from the following original entry: encodent encoder v P3p, which includes the wordform (i.e. the [source and tree] token) encodent ‘encode3pl.pres.ind ’, its lemma, its Lefff POS and its Lefff morphosyntactic tag. The other example illustrates the 1-to-m case with the source token auxquels, which is analyzable as reflecting the sequence of two tree tokens a` lesquels ‘to which’. 4. Related Work and Perspective Our work ov"
L18-1608,W13-4917,1,0.923267,"Missing"
L18-1608,Q15-1026,1,0.906774,"Missing"
L18-1608,K17-3009,0,0.0605054,"Missing"
L18-1608,C16-1325,1,0.883776,"Missing"
L18-1608,W17-1320,1,0.835643,"ctive UD treebanks. These discrepancies are due to differences between the morphological theories adopted by the UD treebanks developers and those employed by the developers of the morphological analyzers. Therefore, the adapted resources we provide are non-trivial to obtain, and required careful alignment of the morphosyntactic analyses with their UD treebank counterparts. For Arabic, we adapted the morphological analyzer used in MADAMIRA (Pasha et al., 2014), which is built on top of the databases of SAMA (Maamouri et al., 2010) to output 5 morphology that adheres to the UD Arabic treebank (Taji et al., 2017).6 The Arabic UD treebank, as with other Arabic treebanks, uses the Penn Arabic treebank tokenization scheme (Maamouri et al., 2004) which segments all proclitics and enclitics except for the definite article. It is worth noting that the format we propose here is independent of the specifics of this tokenization scheme and it can be used with a number of other schemes (Habash, 2010). For Hebrew, we used the HEBLEX morphological analyzer of More and Tsarfaty (2016), based on the BGU Lexicon (Itai and Wintner, 2008), adapted to the UD Hebrew treebank.7 We only modified the HEBLEX SPMRL lattices"
L18-1608,P12-2002,1,0.826771,"y. As a result of the latter, we can maintain a two-way compatibility promise: every morphological disambiguation in a UD v2 treebank can be represented as a CoNLLUL lattice, and every possible path in a CoNLL-UL lattice can serve as the syntactic words of a UD-annotated tree. Thus, we ease the burden on morphological and syntax parser research and development, such that they are relieved of adapting lexical resources (or their analyses) to UD-compliant morphology. The representation scheme for lattices used by the SPMRL shared task datasets (Seddah et al., 2013) and which were introduced by (Tsarfaty et al., 2012; Tsarfaty, 2013),12 allowed for annotating morphological ambiguity of these same languages. Seeker and C ¸ etino˘glu (2015) extended the SPMRL representation to accommodate marking the gold and optionally a predicted morphological analysis. Our proposal extends the latter with two additions: (i) we use the UD convention of specifying a surface token spanning multiple tree tokens; and (ii) we allow the specification of multiple anchors relating lattice arcs to tree tokens, for possibly grounding more than one syntactic tree (i.e., a forest) in the morphological lattice. Since we wanted to main"
L18-1608,P13-2103,1,0.786276,"atter, we can maintain a two-way compatibility promise: every morphological disambiguation in a UD v2 treebank can be represented as a CoNLLUL lattice, and every possible path in a CoNLL-UL lattice can serve as the syntactic words of a UD-annotated tree. Thus, we ease the burden on morphological and syntax parser research and development, such that they are relieved of adapting lexical resources (or their analyses) to UD-compliant morphology. The representation scheme for lattices used by the SPMRL shared task datasets (Seddah et al., 2013) and which were introduced by (Tsarfaty et al., 2012; Tsarfaty, 2013),12 allowed for annotating morphological ambiguity of these same languages. Seeker and C ¸ etino˘glu (2015) extended the SPMRL representation to accommodate marking the gold and optionally a predicted morphological analysis. Our proposal extends the latter with two additions: (i) we use the UD convention of specifying a surface token spanning multiple tree tokens; and (ii) we allow the specification of multiple anchors relating lattice arcs to tree tokens, for possibly grounding more than one syntactic tree (i.e., a forest) in the morphological lattice. Since we wanted to maintain compatibilit"
L18-1608,zeman-etal-2012-hamledt,0,0.319438,"Missing"
L18-1608,K17-3001,1,0.904845,"Missing"
L18-1718,P13-2107,0,0.0454926,"Missing"
L18-1718,W13-4916,0,0.0300012,"Missing"
L18-1718,W09-3821,1,0.740978,"Missing"
L18-1718,F12-2024,1,0.899058,"Missing"
L18-1718,candito-etal-2010-statistical,1,0.865662,"Missing"
L18-1718,C12-1052,0,0.052222,"Missing"
L18-1718,C12-1059,0,0.0416459,"Missing"
L18-1718,J93-2004,0,0.0629965,"Missing"
L18-1718,P09-2010,0,0.0726827,"Missing"
L18-1718,L16-1375,1,0.89553,"Missing"
L18-1718,W13-4917,1,0.856651,"Missing"
L18-1718,W14-6111,1,0.908556,"Missing"
L18-1718,W13-4906,1,0.857223,"Missing"
L18-1718,E17-1034,0,0.0185843,"an iterative fashion, or as new relevant conversion needs are identified. A full manual evaluation of a converted treebank could represent an effort comparable to full re-annotation of a large part of the data. Indeed, few of the UD-conversion papers provide accuracy scores of the conversion on a manually annotated testbench. For instance, The Danish conversion of Johannsen et al. (2015), uses a small set of hand-annotated sentences that reflect specific phenomena and hard cases that is used as held-out section during the iterative development of conversion rules. The Hungarian conversion of Vincze et al. (2017) uses a hand-corrected gold standard of 1,800 sentences. When comparing the quality of the conversion with the gold standard, they consider the accuracy (87.81 UAS and 75.99 LAS) not sufficient to release the resulting treebank. We draw inspiration on their method to develop a handcorrected sample to evaluate the quality of our conversion.One of the authors of the article, an expert in dependency annotation very familiar with the UD formalism, reviewed 100 sentences from the test section and 100 sentences from the dev section manually, correcting edges and labels that were either not properly"
L18-1718,K17-3001,1,0.897083,"Missing"
N09-2047,J99-2004,1,0.793606,"A returns n-best parses for arbitrary n; parse trees are associated with probabilities. A packed forest can also be returned. • MICA is freely available2 , easy to install under Linux, and easy to use. (Input is one sentence per line with no special tokenization required.) There is an enormous amount of related work, and we can mention only the most salient, given space constraints. Our parser is very similar to the work of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency"
N09-2047,W07-2213,1,0.853249,"d by a dependency extractor that relies on the TIG structure of the CFG. The Earley-like parser relies on Earley’s algorithm (Earley, 1970). However, several optimizations have been applied, including guiding techniques (Boullier, 2003), extensive static (offline) computations over the grammar, and efficient data structures. Moreover, Earley’s algorithm has been extended so as to handle input DAGs (and not only sequences of forms). A particular effort has been made to handle huge grammars (over 1 million symbol occurrences in the grammar), thanks to advanced dynamic lexicalization techniques (Boullier and Sagot, 2007). The resulting efficiency is satisfying: with standard ambiguous NLP grammars, huge shared parse forest (over 1010 trees) are often generated in a few dozens of milliseconds. Within MICA, the first module that is applied on top of the shared parse forest is S YNTAX’s n-best module. This module adapts and implements the algorithm of (Huang and Chiang, 2005) for efficient n-best trees extraction from a shared parse forest. In practice, and within the current version of MICA, this module is usually used with n = 1, which identifies the optimal tree w.r.t. the probabilistic model embedded in the"
N09-2047,W03-3005,1,0.86506,"et of the symbols are specialized. 4 Parser S YNTAX (Boullier and Deschamp, 1988) is a system used to generate lexical and syntactic analyzers (parsers) (both deterministic and non-deterministic) for all kind of context-free grammars (CFGs) as well as some classes of contextual grammars. It has been under development at INRIA for several decades. S YNTAX handles most classes of deterministic (unambiguous) grammars (LR, LALR, RLR) as well as general context-free grammars. The non-deterministic features include, among others, an Earley-like parser generator used for natural language processing (Boullier, 2003). Like most S YNTAX Earley-like parsers, the architecture of MICA’s PCFG-based parser is the following: • The Earley-like parser proper computes a shared parse forest that represents in a factorized (polynomial) way all possible parse trees according to the underlying (non-probabilistic) CFG that represents the TIG; • Filtering and/or decoration modules are applied on the shared parse forest; in MICA’s case, an nbest module is applied, followed by a dependency extractor that relies on the TIG structure of the CFG. The Earley-like parser relies on Earley’s algorithm (Earley, 1970). However, sev"
N09-2047,P04-1014,0,0.0349589,"are associated with probabilities. A packed forest can also be returned. • MICA is freely available2 , easy to install under Linux, and easy to use. (Input is one sentence per line with no special tokenization required.) There is an enormous amount of related work, and we can mention only the most salient, given space constraints. Our parser is very similar to the work of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency structures which are slightly different from MIC"
N09-2047,P81-1022,0,0.788449,"processing (Boullier, 2003). Like most S YNTAX Earley-like parsers, the architecture of MICA’s PCFG-based parser is the following: • The Earley-like parser proper computes a shared parse forest that represents in a factorized (polynomial) way all possible parse trees according to the underlying (non-probabilistic) CFG that represents the TIG; • Filtering and/or decoration modules are applied on the shared parse forest; in MICA’s case, an nbest module is applied, followed by a dependency extractor that relies on the TIG structure of the CFG. The Earley-like parser relies on Earley’s algorithm (Earley, 1970). However, several optimizations have been applied, including guiding techniques (Boullier, 2003), extensive static (offline) computations over the grammar, and efficient data structures. Moreover, Earley’s algorithm has been extended so as to handle input DAGs (and not only sequences of forms). A particular effort has been made to handle huge grammars (over 1 million symbol occurrences in the grammar), thanks to advanced dynamic lexicalization techniques (Boullier and Sagot, 2007). The resulting efficiency is satisfying: with standard ambiguous NLP grammars, huge shared parse forest (over 101"
N09-2047,W05-1506,0,0.0313205,"een extended so as to handle input DAGs (and not only sequences of forms). A particular effort has been made to handle huge grammars (over 1 million symbol occurrences in the grammar), thanks to advanced dynamic lexicalization techniques (Boullier and Sagot, 2007). The resulting efficiency is satisfying: with standard ambiguous NLP grammars, huge shared parse forest (over 1010 trees) are often generated in a few dozens of milliseconds. Within MICA, the first module that is applied on top of the shared parse forest is S YNTAX’s n-best module. This module adapts and implements the algorithm of (Huang and Chiang, 2005) for efficient n-best trees extraction from a shared parse forest. In practice, and within the current version of MICA, this module is usually used with n = 1, which identifies the optimal tree w.r.t. the probabilistic model embedded in the original PCFG; other values can also be used. Once the n-best trees have been extracted, the dependency extractor module transforms each of these trees into a dependency tree, by exploiting the fact that the CFG used for parsing has been built from a TIG. 5 Evaluation We compare MICA to the MALT parser. Both parsers are trained on sections 02-21 of our depe"
N09-2047,P04-1042,0,0.0251422,"nd we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency structures which are slightly different from MICA’s, so that direct comparison is difficult. For comparison purposes, we therefore use the MALT parser generator (Nivre et al., 2004), which allows us to train a dependency parser on our own dependency structures. MALT has been among the top performers in the CoNLL dependency parsing competitions. 2 Supertags and Supertagging Supertags are elementary trees of a lexicalized tree grammar such as a Tree-Adjoining Grammar (TAG) (Joshi, 1987)"
N09-2047,C94-1079,0,0.0973378,"rk of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse. All of these systems generate dependency structures which are slightly different from MICA’s, so that direct comparison is difficult. For comparison purposes, we therefore use the MALT parser generator (Nivre et al., 2004), which allows us to train a dependency parser on our own dependency structures. MALT has been among the top performers in the CoNLL dependency parsing competitions. 2 Supertags and Supertagging Supertags are elementary tree"
N09-2047,W04-2407,0,0.102365,"Missing"
N09-2047,J94-1004,0,0.118342,"es anchored by Xl from the symbol Xl∗ . No adjunction, the first adjunction, and the second adjunction are modeled explicitly in the grammar and the associated probabilistic model, while the third and all subsequent adjunctions are modeled together. This conversion method is basically the same as that presented in (Schabes and Waters, 1995), except that our PCFG models multiple adjunctions at the same node by positions (a concern Schabes and Waters (1995) do not share, of course). Our PCFG construction differs from that of Hwa (2001) in that she does not allow multiple adjunction at one node (Schabes and Shieber, 1994) (which we do since we are interested in the derivation structure as a representation of linguistic dependency). For more information about the positional model of adjunction and a discussion of an alternate model, the “bigram model”, see (Nasr and Rambow, 2006). Tree tdi from Section 2 gives rise to the following rule (where tdi and tCO are terminal symbols and the rest are nonterminals): S → S∗l NP VP∗l Vl∗ tdi Vr∗ NP PP∗l P∗l tCO P∗r NP PP∗r VP∗r S∗r The probabilities of the PCFG rules are estimated using maximum likelihood. The probabilistic model refers only to supertag names, not to word"
N09-2047,H05-1102,0,0.0299565,"which derives the syntactic structure from the n-best chosen supertags. Only the supertagger uses lexical information, the parser only sees the supertag hypotheses. • MICA returns n-best parses for arbitrary n; parse trees are associated with probabilities. A packed forest can also be returned. • MICA is freely available2 , easy to install under Linux, and easy to use. (Input is one sentence per line with no special tokenization required.) There is an enormous amount of related work, and we can mention only the most salient, given space constraints. Our parser is very similar to the work of (Shen and Joshi, 2005). They do not employ a supertagging step, and we do not restrict our trees to spinal projections. Other parsers using supertagging include the LDA of Bangalore and Joshi (1999), the CCG-based parser of Clark and Curran (2004), and the constraint-based approach of Wang 2 http://www1.ccls.columbia.edu/˜rambow/mica.html Proceedings of NAACL HLT 2009: Short Papers, pages 185–188, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics and Harper (2004). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a dec"
N09-2047,W04-0307,0,0.115845,"Missing"
N13-1024,P05-1038,0,0.0275125,"a major issue in the design of syntactic formalisms in the eighties and nineties. Unification grammars, such as Lexical Functional Grammars (Bresnan, 1982), Generalized Phrase Structure Grammars (Gazdar et al., 1985) and Head-driven Phrase Structure Grammars (Pollard and Sag, 1994), made SF part of the grammar. Tree Adjoining Grammars (Joshi et al., 1975) proposed to extend the domain of locality of Context Free Grammars partly in order to be able to represent SF in a generative grammar. More recently, (Collins, 1997) proposed a way to introduce SF in a probabilistic context free grammar and (Arun and Keller, 2005) used the same technique for French. (Carroll et al., 1998), used subcategorization probabilities for ranking trees generated by unification-based phrasal grammar and (Zeman, 2002) showed that using frame frequency in a dependency parser can lead to a significant improvement of the performance of the parser. The main novelties of the work presented here is (1) the way a new parse is built by combining subparses that appear in the n best parse list and (2) the use of three very different resources that list the possible SF for verbs. 240 The organization of the paper is the following: in sectio"
N13-1024,P91-1027,0,0.756275,"Missing"
N13-1024,2009.jeptalnrecital-long.4,0,0.376303,"Missing"
N13-1024,W98-1114,0,0.490937,"ighties and nineties. Unification grammars, such as Lexical Functional Grammars (Bresnan, 1982), Generalized Phrase Structure Grammars (Gazdar et al., 1985) and Head-driven Phrase Structure Grammars (Pollard and Sag, 1994), made SF part of the grammar. Tree Adjoining Grammars (Joshi et al., 1975) proposed to extend the domain of locality of Context Free Grammars partly in order to be able to represent SF in a generative grammar. More recently, (Collins, 1997) proposed a way to introduce SF in a probabilistic context free grammar and (Arun and Keller, 2005) used the same technique for French. (Carroll et al., 1998), used subcategorization probabilities for ranking trees generated by unification-based phrasal grammar and (Zeman, 2002) showed that using frame frequency in a dependency parser can lead to a significant improvement of the performance of the parser. The main novelties of the work presented here is (1) the way a new parse is built by combining subparses that appear in the n best parse list and (2) the use of three very different resources that list the possible SF for verbs. 240 The organization of the paper is the following: in section 2, we will briefly describe the parsing model that we wil"
N13-1024,P05-1022,0,0.64006,"the nature of the task itself. The first one is the availability of treebanks such as the Penn Treebank (Marcus et al., 1993) or the French Treebank (Abeill´e et al., 2003), which have been used in the parsing community to train stochastic parsers, such as (Collins, 1997; Petrov and Klein, 2008). Such work remained rooted in the classical language theoretic tradition of parsing, generally based on variants of generative context free grammars. The second change occurred with the use of discriminative machine learning techniques, first to rerank the output of a stochastic parser (Collins, 2000; Charniak and Johnson, 2005) and then in the parser itself (Ratnaparkhi, 1999; Nivre et al., 2007; McDonald et al., 2005a). Such parsers clearly depart from classical parsers in the sense that they do not rely anymore on a generative grammar: given a sentence S, all possible parses for S 1 are considered as possible parses of S. A parse tree is seen as a set of lexico-syntactic features which are associated to weights. The score of a parse is computed as the sum of the weights of its features. This new generation of parsers allows to reach high accuracy but possess their own limitations. We will focus in this paper on on"
N13-1024,P97-1003,0,0.538724,"an n best list in order to build a new parse. Taking into account SF in a parser has been a major issue in the design of syntactic formalisms in the eighties and nineties. Unification grammars, such as Lexical Functional Grammars (Bresnan, 1982), Generalized Phrase Structure Grammars (Gazdar et al., 1985) and Head-driven Phrase Structure Grammars (Pollard and Sag, 1994), made SF part of the grammar. Tree Adjoining Grammars (Joshi et al., 1975) proposed to extend the domain of locality of Context Free Grammars partly in order to be able to represent SF in a generative grammar. More recently, (Collins, 1997) proposed a way to introduce SF in a probabilistic context free grammar and (Arun and Keller, 2005) used the same technique for French. (Carroll et al., 1998), used subcategorization probabilities for ranking trees generated by unification-based phrasal grammar and (Zeman, 2002) showed that using frame frequency in a dependency parser can lead to a significant improvement of the performance of the parser. The main novelties of the work presented here is (1) the way a new parse is built by combining subparses that appear in the n best parse list and (2) the use of three very different resources"
N13-1024,2010.jeptalnrecital-long.3,1,0.734837,"atically computed Subcat Frames The extraction procedure described above has been used to extract ASF from an automatically parsed corpus. The corpus is actually a collection of three corpora of slightly different genres. The first one is a collection of news reports of the French press agency Agence France Presse, the second is a collection of newspaper articles from a local French newspaper : l’Est R´epublicain. The third one is a collection of articles from the French Wikipedia. The size of the different corpora are detailed in table 4. The corpus was first POS tagged with the MELT tagger (Denis and Sagot, 2010), lemmatized with the MACAON tool suite (Nasr et al., 2011) and parsed in order to get the best parse for every sentence. Then the ASF have been extracted. The number of verbs, number of SF and average number of SF per verb are represented in table 3, in column A0 (A stands for Automatic). As one can see, the number of verbs and SF are unrealistic. This is due to the fact that the data that we extract SF from is noisy: it consists of automatically produced syntactic trees which contain errors (recall CORPUS AFP EST REP WIKI TOTAL Sent. nb. 2 041 146 2 998 261 1 592 035 5 198 642 Tokens nb. 59"
N13-1024,J93-2004,0,0.0441011,"Missing"
N13-1024,P05-1012,0,0.419757,"ebank (Marcus et al., 1993) or the French Treebank (Abeill´e et al., 2003), which have been used in the parsing community to train stochastic parsers, such as (Collins, 1997; Petrov and Klein, 2008). Such work remained rooted in the classical language theoretic tradition of parsing, generally based on variants of generative context free grammars. The second change occurred with the use of discriminative machine learning techniques, first to rerank the output of a stochastic parser (Collins, 2000; Charniak and Johnson, 2005) and then in the parser itself (Ratnaparkhi, 1999; Nivre et al., 2007; McDonald et al., 2005a). Such parsers clearly depart from classical parsers in the sense that they do not rely anymore on a generative grammar: given a sentence S, all possible parses for S 1 are considered as possible parses of S. A parse tree is seen as a set of lexico-syntactic features which are associated to weights. The score of a parse is computed as the sum of the weights of its features. This new generation of parsers allows to reach high accuracy but possess their own limitations. We will focus in this paper on one kind of weakness of such parser which is their inability to properly take into account sub"
N13-1024,H05-1066,0,0.0332134,"Missing"
N13-1024,messiant-etal-2008-lexschem,0,0.138044,"Missing"
N13-1024,W11-2917,1,0.936423,"troduced this quantity in order to measure more accurately the impact of the methods described in this paper on the selection of a SF for the verbs of a sentence. SAS LAS UAS TEST 80.84 88.88 90.71 DEV 79.88 88.53 90.37 Table 2: Subcategorization Frame Accuracy, Labeled and Unlabeled Accuracy Score on TEST and DEV. We have chosen a second order graph parser in this work for two reasons. The first is that it is the parsing model that obtained the best results on the French Treebank. The second is that it allows us to impose structural constraints in the solution of the parser, as described in (Mirroshandel and Nasr, 2011), a feature that will reveal itself precious when enforcing SF in the parser output. 3 The Resources Three resources have been used in this work in order to correct SF errors. The first one has been extracted from a treebank, the second has been extracted from an automatically parsed corpus that is several order of magnitude bigger than the treebank. The third one has been extracted from an existing lexico-syntactic resource. The three resources are respectively described in sections 3.2, 3.3 and 3.4. Before describing the resources, we describe in details, in section 3.1 our definition of SF."
N13-1024,P11-4015,1,0.843682,"Missing"
N13-1024,sagot-2010-lefff,1,0.929954,"omputed Subcat Frames The extraction procedure described above has been used to extract ASF from an automatically parsed corpus. The corpus is actually a collection of three corpora of slightly different genres. The first one is a collection of news reports of the French press agency Agence France Presse, the second is a collection of newspaper articles from a local French newspaper : l’Est R´epublicain. The third one is a collection of articles from the French Wikipedia. The size of the different corpora are detailed in table 4. The corpus was first POS tagged with the MELT tagger (Denis and Sagot, 2010), lemmatized with the MACAON tool suite (Nasr et al., 2011) and parsed in order to get the best parse for every sentence. Then the ASF have been extracted. The number of verbs, number of SF and average number of SF per verb are represented in table 3, in column A0 (A stands for Automatic). As one can see, the number of verbs and SF are unrealistic. This is due to the fact that the data that we extract SF from is noisy: it consists of automatically produced syntactic trees which contain errors (recall CORPUS AFP EST REP WIKI TOTAL Sent. nb. 2 041 146 2 998 261 1 592 035 5 198 642 Tokens nb. 59"
N13-1024,C02-1118,0,0.715298,"mars (Gazdar et al., 1985) and Head-driven Phrase Structure Grammars (Pollard and Sag, 1994), made SF part of the grammar. Tree Adjoining Grammars (Joshi et al., 1975) proposed to extend the domain of locality of Context Free Grammars partly in order to be able to represent SF in a generative grammar. More recently, (Collins, 1997) proposed a way to introduce SF in a probabilistic context free grammar and (Arun and Keller, 2005) used the same technique for French. (Carroll et al., 1998), used subcategorization probabilities for ranking trees generated by unification-based phrasal grammar and (Zeman, 2002) showed that using frame frequency in a dependency parser can lead to a significant improvement of the performance of the parser. The main novelties of the work presented here is (1) the way a new parse is built by combining subparses that appear in the n best parse list and (2) the use of three very different resources that list the possible SF for verbs. 240 The organization of the paper is the following: in section 2, we will briefly describe the parsing model that we will be using for this work and give accuracy results on a French corpus. Section 3 will describe three different resources"
N13-1024,H91-1067,0,\N,Missing
N13-1024,P93-1032,0,\N,Missing
P06-1042,P04-1057,0,0.220969,"Missing"
P06-1042,J96-1002,0,0.00747552,"Missing"
P06-1042,W05-1501,1,\N,Missing
P10-1054,P92-1012,1,0.694915,"Giorgio Satta Department of Information Engineering University of Padua, Italy satta@dei.unipd.it adjoining languages, LCFRSs with f = 2 can also generate languages in which pair of strings derived from different nonterminals appear in socalled crossing configurations. It has recently been observed that, in this way, LCFRSs with f = 2 can model the vast majority of data in discontinuous phrase structure treebanks and non-projective dependency treebanks (Maier and Lichte, 2009; Kuhlmann and Satta, 2009). Under a theoretical perspective, the parsing problem for LCFRSs with f = 2 is NP-complete (Satta, 1992), and in known parsing algorithms the running time is exponentially affected by the rank r of the grammar. Nonetheless, in natural language parsing applications, it is possible to achieve efficient, polynomial parsing if we succeed in reducing the rank r (number of nonterminals in the right-hand side) of individual LCFRSs’ productions (Kuhlmann and Satta, 2009). This process is called production factorization. Production factorization is very similar to the reduction of a context-free grammar production into Chomsky normal form. However, in the LCFRS case some productions might not be reducibl"
P10-1054,P87-1015,0,0.922266,"Missing"
P10-1054,N10-1118,0,0.267133,"Missing"
P10-1054,P09-1111,1,0.748599,"Missing"
P10-1054,N09-1061,1,0.734354,"Missing"
P10-1054,E09-1055,1,\N,Missing
P10-1054,P92-1018,0,\N,Missing
P12-2075,I05-3017,0,0.651911,"bstract 2 State of the Art In this paper, we present an unsupervized segmentation system tested on Mandarin Chinese. Following Harris&apos;s Hypothesis in Kempe (1999) and Tanaka-Ishii&apos;s (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbidecoding. This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on diﬀerent corpora available from the Segmentation bake-oﬀ II (Emerson, 2005) and deﬁne a more precise topline for the task using cross-trained supervized system available oﬀ-the-shelf (Zhang and Clark, 2010; Zhao and Kit, 2008; Huang and Zhao, 2007) 1 Introduction The Chinese script has no explicit “word” boundaries. Therefore, tokenization itself, although the very ﬁrst step of many text processing systems, is a challenging task. Supervized segmentation systems exist but rely on manually segmented corpora, which are often speciﬁc to a genre or a domain and use many diﬀerent segmentation guidelines. In order to deal with a larger variety of genres and domains, or to t"
P12-2075,J04-1004,0,0.110787,"ed segmentation is still an important issue. After a short review of the corresponding literature in Section 2, we discuss the challenging issue of evaluating unsupervized word segmentation systems in Section 3. Section 4 and Section 5 present the core of our system. Finally, in Section 6, we detail and discuss our results. Unsupervized word segmentation systems tend to make use of three diﬀerent types of information: the cohesion of the resulting units (e.g., Mutual Information, as in (Sproat and Shih, 1990)), the degree of separation between the resulting units (e.g., Accessor Variety, see (Feng et al., 2004)) and the probability of a segmentation given a string (Goldwater et al., 2006; Mochihashi et al., 2009). A recently published work by Wang et al. (2011) introduce ESA: “Evaluation, Selection, Adjustment.” This method combines cohesion and separation measures in a “goodness” metric that is maximized during an iterative process. This work is the current state-of-the-art in unsupervized segmentation of Mandarin Chinese data. The main drawbacks of ESA are the need to iterate the process on the corpus around 10 times to reach good performance levels and the need to set a parameter that balances th"
P12-2075,P06-1085,0,0.0286763,"orresponding literature in Section 2, we discuss the challenging issue of evaluating unsupervized word segmentation systems in Section 3. Section 4 and Section 5 present the core of our system. Finally, in Section 6, we detail and discuss our results. Unsupervized word segmentation systems tend to make use of three diﬀerent types of information: the cohesion of the resulting units (e.g., Mutual Information, as in (Sproat and Shih, 1990)), the degree of separation between the resulting units (e.g., Accessor Variety, see (Feng et al., 2004)) and the probability of a segmentation given a string (Goldwater et al., 2006; Mochihashi et al., 2009). A recently published work by Wang et al. (2011) introduce ESA: “Evaluation, Selection, Adjustment.” This method combines cohesion and separation measures in a “goodness” metric that is maximized during an iterative process. This work is the current state-of-the-art in unsupervized segmentation of Mandarin Chinese data. The main drawbacks of ESA are the need to iterate the process on the corpus around 10 times to reach good performance levels and the need to set a parameter that balances the impact of the cohesion measure w.r.t. the separation measure. Empirically, a"
P12-2075,P06-2056,0,0.723824,"s that is directly inspired by a linguistic hypothesis formulated by Harris (1955). In Tanaka-Ishii (2005) (following Kempe (1999)) who use Branching Entropy (BE), this hypothesis goes as follows: if sequences produced by human language were random, we would expect the Branching Entropy of a sequence (estimated from the n-grams in a corpus) to decrease as we increase the length of the sequence. Therefore the variation of the branching entropy (VBE) should be negative. When we observe that it is not the case, Harris hypothesizes that we are at a linguistic boundary. Following this hypothesis, (Jin and Tanaka-Ishii, 2006) propose a system that segments when BE is rising or when it reach a certain maximum. The main drawback of Jin and Tanaka-Ishii (2006) model is that segmentation decisions are taken very locally1 and do not depend on neighboring cuts. Moreover, this system also also relies on parameters, namely the threshold on the VBE above which the system decides to segment (in their system, this is when VBE≥ 0). In theory, we could expect a decreasing BE and look for a less decreasing value (or on the contrary, rising at least to some extent). A threshold of 0 can be seen as a default value. Finally, Jin a"
P12-2075,W99-0702,0,0.681443,"nt may not be easily available in all situations. However, if we only consider their experiments using settings similar to ours, their results consistently lie around an f-score of 0.80. An older approach, introduced by Jin and TanakaIshii (2006), solely relies on a separation measure 383 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 383–387, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics that is directly inspired by a linguistic hypothesis formulated by Harris (1955). In Tanaka-Ishii (2005) (following Kempe (1999)) who use Branching Entropy (BE), this hypothesis goes as follows: if sequences produced by human language were random, we would expect the Branching Entropy of a sequence (estimated from the n-grams in a corpus) to decrease as we increase the length of the sequence. Therefore the variation of the branching entropy (VBE) should be negative. When we observe that it is not the case, Harris hypothesizes that we are at a linguistic boundary. Following this hypothesis, (Jin and Tanaka-Ishii, 2006) propose a system that segments when BE is rising or when it reach a certain maximum. The main drawback"
P12-2075,2011.jeptalnrecital-long.23,1,0.787529,"n measure and get high segmentation scores. When maximized over a sentence, this measure captures at least in part what can be modeled by a cohesion measure without the need for ﬁne-tuning the balance between the two. The evolution of the results w.r.t. word length is consistent with the supervized cross-evaluation results of the various segmentation guidelines as performed in Section 3. Due to space constraints, we cannot detail here a qualitative analysis of the results. We can simply mention that the errors we observed are consistent with previous systems based on Harris&apos;s hypothesis (see (Magistry and Sagot, 2011) and Jin (2007) for a longer discussion). Many errors are related to dates and Chinese numbers. This could and should be dealt with during preprocessing. Other errors often involve frequent grammatical morphemes or productive aﬃxes. These errors are often interesting for linguists and could be studied as such and/or corrected in a post-processing stage that would introduce linguistic knowledge. Indeed, unlike content words, grammatical morphemes belongs to closed classes, 386 System ESA worst ESA best nVBE VBE &gt; 0 ESA worst ESA best nVBE AS CITYU Setting 3 0.729 0.795 0.782 0.816 0.758 0.775 S"
P12-2075,P09-1012,0,0.20025,"Missing"
P12-2075,I05-1009,0,0.0258564,". Therefore, this proper exponent may not be easily available in all situations. However, if we only consider their experiments using settings similar to ours, their results consistently lie around an f-score of 0.80. An older approach, introduced by Jin and TanakaIshii (2006), solely relies on a separation measure 383 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 383–387, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics that is directly inspired by a linguistic hypothesis formulated by Harris (1955). In Tanaka-Ishii (2005) (following Kempe (1999)) who use Branching Entropy (BE), this hypothesis goes as follows: if sequences produced by human language were random, we would expect the Branching Entropy of a sequence (estimated from the n-grams in a corpus) to decrease as we increase the length of the sequence. Therefore the variation of the branching entropy (VBE) should be negative. When we observe that it is not the case, Harris hypothesizes that we are at a linguistic boundary. Following this hypothesis, (Jin and Tanaka-Ishii, 2006) propose a system that segments when BE is rising or when it reach a certain ma"
P12-2075,J11-3001,0,0.608206,"magistry@inria.fr Benoît Sagot Alpage, INRIA & Univ. Paris 7, 175 rue du Chevaleret, 75013 Paris, France benoit.sagot@inria.fr Abstract 2 State of the Art In this paper, we present an unsupervized segmentation system tested on Mandarin Chinese. Following Harris&apos;s Hypothesis in Kempe (1999) and Tanaka-Ishii&apos;s (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbidecoding. This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on diﬀerent corpora available from the Segmentation bake-oﬀ II (Emerson, 2005) and deﬁne a more precise topline for the task using cross-trained supervized system available oﬀ-the-shelf (Zhang and Clark, 2010; Zhao and Kit, 2008; Huang and Zhao, 2007) 1 Introduction The Chinese script has no explicit “word” boundaries. Therefore, tokenization itself, although the very ﬁrst step of many text processing systems, is a challenging task. Supervized segmentation systems exist but rely on manually segmented corpora, which are often speciﬁc to a genre or a"
P12-2075,D10-1082,0,0.0732508,"ing Harris&apos;s Hypothesis in Kempe (1999) and Tanaka-Ishii&apos;s (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbidecoding. This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on diﬀerent corpora available from the Segmentation bake-oﬀ II (Emerson, 2005) and deﬁne a more precise topline for the task using cross-trained supervized system available oﬀ-the-shelf (Zhang and Clark, 2010; Zhao and Kit, 2008; Huang and Zhao, 2007) 1 Introduction The Chinese script has no explicit “word” boundaries. Therefore, tokenization itself, although the very ﬁrst step of many text processing systems, is a challenging task. Supervized segmentation systems exist but rely on manually segmented corpora, which are often speciﬁc to a genre or a domain and use many diﬀerent segmentation guidelines. In order to deal with a larger variety of genres and domains, or to tackle more theoretic questions about linguistic units, unsupervized segmentation is still an important issue. After a short review"
P12-2075,I08-1002,0,0.736393,"in Kempe (1999) and Tanaka-Ishii&apos;s (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbidecoding. This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on diﬀerent corpora available from the Segmentation bake-oﬀ II (Emerson, 2005) and deﬁne a more precise topline for the task using cross-trained supervized system available oﬀ-the-shelf (Zhang and Clark, 2010; Zhao and Kit, 2008; Huang and Zhao, 2007) 1 Introduction The Chinese script has no explicit “word” boundaries. Therefore, tokenization itself, although the very ﬁrst step of many text processing systems, is a challenging task. Supervized segmentation systems exist but rely on manually segmented corpora, which are often speciﬁc to a genre or a domain and use many diﬀerent segmentation guidelines. In order to deal with a larger variety of genres and domains, or to tackle more theoretic questions about linguistic units, unsupervized segmentation is still an important issue. After a short review of the correspondin"
P19-1356,D15-1075,0,0.0312175,"x tree (e.g. ‘LR’ denotes the right child of left child of root). The authors assume that, for a given role scheme, if a TPDN can be trained well to approximate the representation learned by a neural model, then that role scheme likely specifies the compositionality implicitly learned by the model. For each BERT layer, we work with five different role schemes. Each word’s role is computed based on its left-to-right index, its right-to-left index, an ordered pair containing its left-to-right and Following McCoy et al. (2019), we train our TPDN model on the premise sentences in the SNLI corpus (Bowman et al., 2015). We initialize the filler embeddings of the TPDN with the pre-trained word embeddings from BERT’s input layer, freeze it, learn a linear projection on top of it and use a Mean Squared Error (MSE) loss function. Other trainable parameters include the role embeddings and a linear projection on top of tensor product sum to match the embedding size of BERT. Table 4 displays the MSE between representation from pretrained BERT and representation from TPDN trained to approximate BERT. We discover that BERT implicitly implements a treebased scheme, as a TPDN model following that scheme best approxima"
P19-1356,L18-1269,0,0.0653377,"e grouped into three categories. Surface tasks probe for sentence length (SentLen) and for the presence of words in the sentence (WC). Syntactic tasks test for sensitivity to word order (BShift), the depth of the syntactic tree (TreeDepth) and the sequence of toplevel constituents in the syntax tree (TopConst). Semantic tasks check for the tense (Tense), the subject (resp. direct object) number in the main clause (SubjNum, resp. ObjNum), the sensitivity to random replacement of a noun/verb (SOMO) and the random swapping of coordinated clausal conjuncts (CoordInv). We use the SentEval toolkit (Conneau and Kiela, 2018) along with the recommended hyperparameter space to search for the best probing classifier. As random encoders can surprisingly encode a lot of lexical and structural information (Zhang and Bowman, 2018), we also evaluate the untrained version of BERT, obtained by setting all model weights to a random number. Table 2 shows that BERT embeds a rich hierarchy of linguistic signals: surface information at the bottom, syntactic information in the middle, semantic information at the top. BERT has also surpassed the previously published results for two tasks: BShift and CoordInv. We find that the unt"
P19-1356,P18-1198,0,0.0981764,"Missing"
P19-1356,N19-1423,0,0.211957,"Missing"
P19-1356,P03-1054,0,0.0944559,"0.0081 0.0099 0.0201 0.0233 0.0251 0.0226 0.0179 0.0237 0.0179 0.0203 0.0221 0.0201 0.0155 0.0214 0.0284 0.0337 0.0355 0.0311 0.0249 0.0338 0.0428 0.0486 0.0507 0.0453 0.0363 0.0486 0.0362 0.0411 0.0422 0.0391 0.0319 0.0415 0.0305 0.0339 0.0348 0.0334 0.0278 0.0340 Table 4: Mean squared error between TPDN and BERT representation for a given layer and role scheme on SNLI test instances. Each number corresponds to the average across five random initializations. 27/02/2019 depparse_layer_1.svg right-to-left indices, its position in a syntactic tree (formatted version of the Stanford PCFG Parser (Klein and Manning, 2003) with no unary nodes and no labels) and an index common to all the words in the sentence (bag-of-words), which ignores its position. Additionally, we also define a role scheme based on random binary trees. The keys to the cabinet are on the table The keys to the cabinet are on the table Figure 2: Dependency parse tree induced from attention head #11 in layer #2 using gold root (‘are’) as starting node for maximum spanning tree algorithm. ﬁle:///Users/ganeshj/Downloads/todelete/depparse_layer_1.svg 1/1 Results in Table 3 show that the middle layers perform well in most cases, which supports the"
P19-1356,Q16-1037,0,0.0751865,"asks: BShift and CoordInv. We find that the untrained version of BERT corresponding to the higher layers outperforms the trained version in the task of predicting sentence length (SentLen). This could indicate that untrained models contain sufficient information to predict a basic surface feature such as sentence length, whereas training the model results in the model storing more complex information, at the expense of its ability to predict such basic surface features. 5 Subject-Verb Agreement Subject-verb agreement is a proxy task to probe whether a neural model encodes syntactic structure (Linzen et al., 2016). The task of predicting the verb number becomes harder when there are more nouns with opposite number (attractors) intervening between the subject and the verb. Goldberg (2019) has shown that BERT learns syntactic phenomenon surprisingly well using various stimuli for subject-verb agreement. We extend his work by performing the test on each layer of BERT and controlling for the number of attractors. In our study, we use the stimuli created by Linzen et al. (2016) and the SentEval toolkit (Conneau and Kiela, 2018) to build the binary classifier with the recommended hyperparameter space, using"
P19-1356,N19-1112,0,0.214179,"a contemporaneous work that introduces a novel edge probing task to investigate how contextual word representations encode sentence structure across a range of syntactic, semantic, local and long-range phenomena. They conclude that contextual word representations trained on language modeling and machine translation encode syntactic phenomena strongly, but offer comparably small improvements on semantic tasks over a non-contextual baseline. Their result using BERT model on capturing linguistic hierarchy confirms our probing task results although using a set of relatively simple probing tasks. Liu et al. (2019) is another contemporaneous work that studies the features of language captured/missed by contextualized vectors, transferability across different layers of the model and the impact of pretraining on the linguistic knowledge and transferability. They find that (i) contextualized word embeddings do not capture finegrained linguistic knowledge, (ii) higher layers of RNN to be task-specific (with no such pattern for a transformer) and (iii) pretraining on a closely related task yields better performance than language model pretraining. Hewitt and Manning (2019) is a very recent work which showed"
P19-1356,D14-1162,0,0.101356,"nd “are”) are captured accurately. Surprisingly, the predicate-argument structure seems to be partly modeled as shown by the chain of dependencies between “key”,“cabinet” and “table”. 3654 7 Related Work Peters et al. (2018) studies how the choice of neural architecture such as CNNs, Transformers and RNNs used for language model pretraining affects the downstream task accuracy and the qualitative properties of the contextualized word representations that are learned. They conclude that all architectures learn high quality representations that outperform standard word embeddings such as GloVe (Pennington et al., 2014) for challenging NLP tasks. They also show that these architectures hierarchically structure linguistic information, such that morphological, (local) syntactic and (longer range) semantic information tend to be represented in, respectively, the word embedding layer, lower contextual layers and upper layers. In our work, we observe that such hierarchy exists as well for BERT models that are not trained using the standard language modelling objective. Goldberg (2019) shows that the BERT model captures syntactic information well for subject-verb agreement. We build on this work by performing the"
P19-1356,D18-1179,0,0.151216,"e linguistic structure implicitly learned by BERT’s representations. We use the PyTorch implementation of BERT, which hosts the models trained by (Devlin et al., 2018). All our experiments are based on the bert-base-uncased variant,2 which consists of 12 layers, each having a hidden size of 768 and 12 attention heads (110M parameters). In all our experiments, we seek the activation of the first input token (‘[CLS]’) (which summarizes the information from the actual tokens using a self-attention mechanism) at every layer to compute BERT representation, unless otherwise stated. 3 Phrasal Syntax Peters et al. (2018) have shown that the representations underlying LSTM-based language models(Hochreiter and Schmidhuber, 1997) can capture phrase-level (or span-level) information.3 It remains unclear if this holds true for models not trained with a traditional language modeling objective, such as BERT. Even if it does, would the information be present in multiple layers of the model? To investigate this question we extract span representations from each layer of BERT. 2 We obtained similar results in preliminary experiments with the bert-large-uncased variant. 3 Peters et al. (2018) experimented with ELMo-styl"
P19-1356,N19-1419,0,0.0377872,"g a set of relatively simple probing tasks. Liu et al. (2019) is another contemporaneous work that studies the features of language captured/missed by contextualized vectors, transferability across different layers of the model and the impact of pretraining on the linguistic knowledge and transferability. They find that (i) contextualized word embeddings do not capture finegrained linguistic knowledge, (ii) higher layers of RNN to be task-specific (with no such pattern for a transformer) and (iii) pretraining on a closely related task yields better performance than language model pretraining. Hewitt and Manning (2019) is a very recent work which showed that we can recover parse trees from the linear transformation of contextual word representation consistently, better than with non-contextual baselines. They focused mainly on syntactic structure while our work additionally experimented with linear structures (leftto-right, right-to-left) to show that the compositionality modelling underlying BERT mimics traditional syntactic analysis. The recent burst of papers around these questions illustrates the importance of interpreting contextualized word embedding models and our work complements the growing literat"
P19-1356,W18-5431,0,0.0410636,"p of tensor product sum to match the embedding size of BERT. Table 4 displays the MSE between representation from pretrained BERT and representation from TPDN trained to approximate BERT. We discover that BERT implicitly implements a treebased scheme, as a TPDN model following that scheme best approximates BERT’s representation at most layers. This result is remarkable, as BERT encodes classical, tree-like structures despite relying purely on attention mechanisms. Motivated by this study, we perform a case study on dependency trees induced from self attention weight following the work done by Raganato and Tiedemann (2018). Figure 2 displays the dependencies inferred from an example sentence by obtaining self attention weights for every word pairs from attention head #11 in layer #2, fixing the gold root as the starting node and invoking the Chu-Liu-Edmonds algorithm (Chu and Liu, 1967). We observe that determiner-noun dependencies (“the keys”, “the cabinet” and “the table”) and subject-verb dependency (“keys” and “are”) are captured accurately. Surprisingly, the predicate-argument structure seems to be partly modeled as shown by the chain of dependencies between “key”,“cabinet” and “table”. 3654 7 Related Work"
P19-1356,W00-0726,0,0.313791,"ch layer of BERT. 2 We obtained similar results in preliminary experiments with the bert-large-uncased variant. 3 Peters et al. (2018) experimented with ELMo-style CNN and Transformer but did not report this finding for these models. Following Peters et al. (2018), for a token sequence si , . . . , sj , we compute the span representation s(si ,sj ),l at layer l by concatenating the first (hsi ,l ) and last hidden vector (hsj ,l ), along with their element-wise product and difference. We randomly pick 3000 labeled chunks and 500 spans not labeled as chunks from the CoNLL 2000 chunking dataset (Sang and Buchholz, 2000). As shown in Figure 1, we visualize the span representations obtained from multiple layers using tSNE (Maaten and Hinton, 2008), a non-linear dimensionality reduction algorithm for visualizing high-dimensional data. We observe that BERT mostly captures phrase-level information in the lower layers and that this information gets gradually diluted in higher layers. The span representations from the lower layers map chunks (e.g. ‘to demonstrate’) that project their underlying category (e.g. VP) together. We further quantify this claim by performing a k-means clustering on span representations wit"
P19-1356,W18-5446,0,0.0898822,"Missing"
P19-1356,W18-5448,0,0.0414088,"h of the syntactic tree (TreeDepth) and the sequence of toplevel constituents in the syntax tree (TopConst). Semantic tasks check for the tense (Tense), the subject (resp. direct object) number in the main clause (SubjNum, resp. ObjNum), the sensitivity to random replacement of a noun/verb (SOMO) and the random swapping of coordinated clausal conjuncts (CoordInv). We use the SentEval toolkit (Conneau and Kiela, 2018) along with the recommended hyperparameter space to search for the best probing classifier. As random encoders can surprisingly encode a lot of lexical and structural information (Zhang and Bowman, 2018), we also evaluate the untrained version of BERT, obtained by setting all model weights to a random number. Table 2 shows that BERT embeds a rich hierarchy of linguistic signals: surface information at the bottom, syntactic information in the middle, semantic information at the top. BERT has also surpassed the previously published results for two tasks: BShift and CoordInv. We find that the untrained version of BERT corresponding to the higher layers outperforms the trained version in the task of predicting sentence length (SentLen). This could indicate that untrained models contain sufficient"
R09-1049,francopoulo-etal-2006-lexical,0,0.0522378,"tion: • a morphological class, which defines the patterns that build all inflected forms of the lemma [12]; 3 The Alexina framework A detailed lexical description of all words (or as many as possible) belonging to a language is needed in order to perform high-level NLP tasks such as deep parsing. This information is usually compiled into a lexicon, which could be defined as a list of lexical forms associated with morphological and syntactic information. Alexina is a framework that represents lexical information in a complete, efficient and readable way [11, 1], and is compatible with the LMF4 [2] standard. The flexibility and completeness of the Alexina format allow a straightforward integration with deep grammatical formalisms (LFG, LTAG) which require detailed syntactic data for all forms, and allow to model lexical information for diverse languages. It is indeed the lexical framework of the Lefff, a large-coverage morphological and syntactic lexicon for French, but also that of other lexical resources for languages such as Polish, Slovak, and soon English. The Alexina model is based on a two-level representation, detailed below, that separates the description of a lexicon from its"
R09-1049,sagot-etal-2006-lefff,1,0.811961,", but are often absent or incomplete for other languages, even major ones. For example, some lexical resources exist for Spanish, but none of them combines satisfactorily the following properties: • coverage: all words, including rare ones, in all categories should be included; • quality: manually and automatically developed resources contain various errors; • richness: applications such as (deep) parsing require at least morphological and syntactic information, including subcategorization frames. The Leffe1 is a wide-coverage morphological and syntactic lexicon based on the Alexina framework [13, 15, 1]. This lexicon follows the linguistic criteria applied on the French lexicon Lefff2 taking advantage of the linguistic proximity between Spanish and French as Romance languages. The main contributions of this piece of research are the following: • we present a morphological and syntactic wide coverage lexicon for Spanish; • we describe an enhanced available lexical framework, 1 L´exico de formas flexionadas del espa˜nol - Lexicon of Spanish inflected forms 2 Lexique des formes fl´echies du franc¸ais - Lexicon of French inflected forms The work described here is one of the starting points of th"
R09-1049,C94-1097,0,0.399405,"Missing"
R09-1049,W09-4619,1,0.352106,"a resource was worth using or not for this task, we payed more attention to quality or richness than coverage. After all, combining several resources shall lead to a good coverage that will generally be wider than 266 grammar for Spanish [7] grounded in the theoretical framework of Head-driven Phrase Structure Grammar (HPSG). It includes a lexicon describing syntactic information for Spanish in a well organized hierarchy of syntactic classes. However, it is not easily readable, and specific to the HPSG formalism. In order to merge these resources, we followed a process described in details in [9]. We briefly remind it here. As mentioned in Section 2, Multext and USC lexicons only include morphological information, whereas the SRG Lexicon and ADESSE include syntactic information. Therefore, we proceeded in the following way: 1. we built a morphological baseline lexicon by converting the Multext lexicon into the Alexina format and added some Alexina-specific entries (prefixes, suffixes, named entities, punctuation signs); 2. we converted the USC Lexicon into the Alexina format and merged it with the baseline lexicon extracted from Multext, so as to obtain the morphological base of the L"
R09-1049,C08-1080,1,0.8358,"tant coverage in terms of morphological information but a more restricted one in terms of syntactic information. Indeed, for morphological entries10 for which no syntactic information could be found, we added default syntactic features corresponding to the most common ones among entries with the same PoS. For example, all verbal lemmas that were not covered by ADESSE or SRG received the following subcategorization frame: <Suj:sn|cln,Obj:(sn|cla)&gt; (transitive verb with optional direct object). However, the application of semiautomatic techniques to extend and correct a lexicon, as described in [10], should help us fixing this aspect. 5 Tagger-based identification missing entries of The next step after obtaining a first version of the Leffe was to continue upgrading it by adding missing entries. Usually, this task is manually performed and thus, is a time-costly process. We now present a simple but effective semi-automatic technique which greatly eases the process by identifying possible missing entries. We distinguish two types of missing entries: 1. totally non referenced forms, 2. missing homonyms of forms referenced in the lexicon, i.e., forms non associated to a different Part-of-Sp"
R09-1058,francopoulo-etal-2006-lexical,0,0.0611908,"ility, we mean the capacity of the resource to be integrated in NLP tools or applied to a particular language • Foreseeing the exhaustive list of those uses is simply impossible. Therefore, it is essential for the formalisms to be compared to various kind of languages and practical tools in order to adapt and extend them. • The formalisms need to be regularly maintained so as to guarantee their extension to uncovered phenomena. In order to develop our morphological and lexical resources, we chose to use the Alexina framework [7, 8, 2]. This framework, which is compatible with the LMF standard [3] represents morphological and syntactic information in a complete, efficient and readable fashion. The Alexina model is based on a two-level representation distinguishing the description of a lexicon from its use. The intensional level, used for an efficient description, factorizes the lexical information by associating each lemma with a morphological class and deep syntactic information (a deep subcategorization frame, a list of possible restructurations, and other syntactic features such as information on control, attributes, mood of sentencial complements, etc.). The extensional level, used"
R09-1058,W09-4619,1,0.826122,"ological and syntactic lexicons Two wide coverage lexicons for Spanish and Galician have already been produced following the Alexina format. Both 15 At this point, the process discards all entries that do not have their lemma as one of their inflected forms. lexicons are currently being upgraded using the techniques described in section 5.2 and will be available under LGPLLR licenses soon. The Spanish lexicon Leffe 16 has overtaken other well known Spanish lexicons in terms of coverage despite being in beta version. It has been obtained by merging several existing Spanish linguistic resources [4]. Nowadays, the Leffe beta contains more than 165,000 unique (lemma,PoS) pairs, corresponding to approx. 1,590,000 inflected entries that associate a form with morpho-syntactic information (approx. 680,000 unique (form,PoS) pairs). The Leffga17 has been created after the Galician lexicon developed in the CORGA18 project. The Leffga is still in alpha version (April 2009), and less developed than the Leffe. It contains more than 52,000 unique (lemma,PoS) pairs (approx. 515,000 unique (form,PoS) pairs). The complete lexicon includes more than 742,000 inflected entries with little syntactic inform"
R09-1058,C08-1080,1,0.841415,"ious practical experiments (see sect. 5.3.2 and [2]), have shown that existing resource usually share common points. Adapting a large part of the available existing resources is often a reasonable objective. Using existing resources Existing resources are generally valuable sources of data when building new resources or extending others. Ignoring the great efforts invested in order to build existing resources does not seem reasonable or productive. Such an approach 320 We now describe a generic approach which has been abstracted from practical research results described essentially in [9] and [5]. In order to efficiently produce new formalized knowledge, a source of data is needed to detect and acquire the missing knowledge. Since this source should be available in sufficient quantity for any language, we have discarded annotated data13 which is only available in limited quantities for a small number of languages, and opted for plain digital text which is daily produced for most languages. In order to extend and correct a resource from plain text, we apply the following two-step generic approach: • identify as accurately as possible which part of the text is not covered by a resource,"
R09-1058,sagot-etal-2006-lefff,1,0.860569,"inguistic Resources LGPL-compatible, http://www.cecill.info/. By usability, we mean the capacity of the resource to be integrated in NLP tools or applied to a particular language • Foreseeing the exhaustive list of those uses is simply impossible. Therefore, it is essential for the formalisms to be compared to various kind of languages and practical tools in order to adapt and extend them. • The formalisms need to be regularly maintained so as to guarantee their extension to uncovered phenomena. In order to develop our morphological and lexical resources, we chose to use the Alexina framework [7, 8, 2]. This framework, which is compatible with the LMF standard [3] represents morphological and syntactic information in a complete, efficient and readable fashion. The Alexina model is based on a two-level representation distinguishing the description of a lexicon from its use. The intensional level, used for an efficient description, factorizes the lexical information by associating each lemma with a morphological class and deep syntactic information (a deep subcategorization frame, a list of possible restructurations, and other syntactic features such as information on control, attributes, moo"
R09-1058,P04-1057,0,0.0730998,"under such a set of classes. 5.2.2 Syntactic lexical information improvement To achieve this task, we apply the technique described in [5] where the tool observed is a syntactic parser, the unexpected behaviors are parsing failures, and the resources A and B are a morpho-syntactic lexicon and a grammar. In order to correct and extend a lexicon, the authors firstly detect lexical forms suspected to be responsible for some parse failures thanks to two techniques. A statistical computation which emphasizes “suspicious” lexical forms present more frequently than the rest in non-parsable sentence [10]. Lexical forms are even more “suspicious” if present in non-parsable along with forms “cleared” by their presence in parsable ones [9]. A tagger-based approach which highlights absent entries by relying on the tagger’s ability to guess a tag for unknown words and forcing the tagger to use it on forms that are in fact known. If the tag answered represents data absent in the lexicon, the form is suspected. Once the suspicious forms have been identified, the authors rely on the grammar to generate lexical corrections for the identified forms. To achieve this task, they study the expectations of"
R09-1058,W05-1522,1,0.884862,"Missing"
R09-1058,P06-1042,1,\N,Missing
sagot-2010-lefff,sagot-walther-2010-morphological,1,\N,Missing
sagot-2010-lefff,clement-etal-2004-morphology,1,\N,Missing
sagot-2010-lefff,W07-2201,0,\N,Missing
sagot-2010-lefff,W05-1501,1,\N,Missing
sagot-2010-lefff,Y09-1013,1,\N,Missing
sagot-2010-lefff,P06-1042,1,\N,Missing
sagot-2010-lefff,R09-1049,1,\N,Missing
sagot-2010-lefff,sagot-etal-2006-lefff,1,\N,Missing
sagot-2010-lefff,francopoulo-etal-2006-lexical,0,\N,Missing
sagot-2010-lefff,paroubek-etal-2006-data,0,\N,Missing
sagot-2014-delex,adolphs-2008-acquiring,0,\N,Missing
sagot-2014-delex,R09-1049,1,\N,Missing
sagot-boullier-2006-deep,W03-3005,1,\N,Missing
sagot-etal-2006-lefff,clement-etal-2004-morphology,1,\N,Missing
sagot-etal-2006-lefff,W05-1522,1,\N,Missing
sagot-etal-2006-lefff,W05-1501,1,\N,Missing
sagot-etal-2006-lefff,P04-1057,0,\N,Missing
sagot-etal-2010-lexicon,sagot-2010-lefff,1,\N,Missing
sagot-etal-2010-lexicon,W05-1522,0,\N,Missing
sagot-fiser-2012-cleaning,widdows-ferraro-2008-semantic,0,\N,Missing
sagot-fiser-2012-cleaning,P10-1023,0,\N,Missing
sagot-fiser-2012-cleaning,Y09-1013,1,\N,Missing
sagot-fiser-2012-cleaning,bond-etal-2008-boot,0,\N,Missing
sagot-fiser-2012-cleaning,S10-1002,0,\N,Missing
sagot-stern-2012-aleda,C10-1032,0,\N,Missing
sagot-stern-2012-aleda,E06-1002,0,\N,Missing
sagot-stern-2012-aleda,W09-3302,0,\N,Missing
sagot-stern-2012-aleda,C08-1034,0,\N,Missing
sagot-stern-2012-aleda,charton-torres-moreno-2010-nlgbase,0,\N,Missing
sagot-stern-2012-aleda,D07-1074,0,\N,Missing
sagot-stern-2012-aleda,N10-1072,0,\N,Missing
sagot-walther-2010-morphological,shamsfard-fadaee-2008-hybrid,0,\N,Missing
sagot-walther-2010-morphological,sagot-2010-lefff,1,\N,Missing
sagot-walther-2010-morphological,W04-1607,0,\N,Missing
sagot-walther-2010-morphological,francopoulo-etal-2006-lexical,0,\N,Missing
scherrer-sagot-2014-language,W13-5306,1,\N,Missing
scherrer-sagot-2014-language,C04-1137,0,\N,Missing
scherrer-sagot-2014-language,H01-1035,0,\N,Missing
scherrer-sagot-2014-language,N01-1020,0,\N,Missing
scherrer-sagot-2014-language,D11-1119,0,\N,Missing
scherrer-sagot-2014-language,P99-1067,0,\N,Missing
scherrer-sagot-2014-language,W07-0705,0,\N,Missing
scherrer-sagot-2014-language,W02-0902,0,\N,Missing
scherrer-sagot-2014-language,P07-2045,0,\N,Missing
scherrer-sagot-2014-language,2009.eamt-1.3,0,\N,Missing
scherrer-sagot-2014-language,J03-1002,0,\N,Missing
scherrer-sagot-2014-language,R11-1018,0,\N,Missing
scherrer-sagot-2014-language,I13-1112,0,\N,Missing
scherrer-sagot-2014-language,feldman-etal-2006-cross,0,\N,Missing
tolone-etal-2012-evaluating,candito-etal-2010-statistical,0,\N,Missing
tolone-etal-2012-evaluating,sagot-2010-lefff,1,\N,Missing
tolone-etal-2012-evaluating,W05-1522,1,\N,Missing
tolone-etal-2012-evaluating,P06-1042,1,\N,Missing
tolone-etal-2012-evaluating,D07-1096,0,\N,Missing
tolone-etal-2012-evaluating,paroubek-etal-2006-data,0,\N,Missing
W05-1501,W97-1514,0,0.0232743,"ever, the descriptive expressivity of resulting analyses is far below what is needed to represent, e.g., phrases or long-distance dependencies in a way that is consistent with serious linguistic definitions of these concepts. For this reason, we designed a parser that is compatible with a linguistic theory, namely LFG, as well as robust and efficient despite the high variability of language production. Developing a new parser for LFG (LexicalFunctional Grammars, see, e.g., (Kaplan, 1989)) is not in itself very original. Several LFG parsers already exist, including those of (Andrews, 1990) or (Briffault et al., 1997). However, the most famous LFG system is undoubtedly the Xerox Linguistics Environment (XLE) project which is the successor of the Grammars Writer’s Workbench (Kaplan and Maxwell, 1994; Riezler et al., 2002; Kaplan et al., 2004). XLE is a large project which concentrates a lot of linguistic and computational technology, relies on a similar point of view on the balance between shallow and deep parsing, and has been successfully used to parse large unrestricted corpora. Nevertheless, these parsers do not always use in the most extensive way all existing algorithmic techniques of computation shar"
W05-1501,N04-1013,0,0.353288,"his reason, we designed a parser that is compatible with a linguistic theory, namely LFG, as well as robust and efficient despite the high variability of language production. Developing a new parser for LFG (LexicalFunctional Grammars, see, e.g., (Kaplan, 1989)) is not in itself very original. Several LFG parsers already exist, including those of (Andrews, 1990) or (Briffault et al., 1997). However, the most famous LFG system is undoubtedly the Xerox Linguistics Environment (XLE) project which is the successor of the Grammars Writer’s Workbench (Kaplan and Maxwell, 1994; Riezler et al., 2002; Kaplan et al., 2004). XLE is a large project which concentrates a lot of linguistic and computational technology, relies on a similar point of view on the balance between shallow and deep parsing, and has been successfully used to parse large unrestricted corpora. Nevertheless, these parsers do not always use in the most extensive way all existing algorithmic techniques of computation sharing and compact information representation that make it possible to write an efficient LFG parser, despite the fact that the LFG formalism, as many other formalisms relying on unification, is NP-hard. Of course our purpose is no"
W05-1501,1993.iwpt-1.12,0,0.048575,"n β if in the suffix β = β1 Xβ2 there exists a derived phrase from the symbol X which starts with a terminal symbol r and if there exists a node k in the DAG, k ≥ j, with an out-transition on r. If it is the case and if this possible recovery is selected, we put the item [A → αtβ1 .Xβ2 , i] in table T [k]. This will ensure 11 Let us recall here that the Earley algorithm, like the GLR algorithm, has the valid prefix property. This is still true when the input is a DAG. 12 The combination of these two recovery techniques leads to a more general algorithm than the skipping of the GLR* algorithm (Lavie and Tomita, 1993). Indeed, we can not only skip terminals, but in fact replace any invalid prefix by a valid prefix (of a right sentential form) with an increased span. In other words, both terminals and non-terminals may be skipped, inserted or changed, following the heuristics described later on. However, in (Lavie and Tomita, 1993), considering only the skipping of terminal symbols was fully justified since their aim was to parse spontaneous speech, full of noise and irrelevances that surround the meaningful words of the utterance. S2 spunct S ... NP V N v prep VP pn essaye de ε PVP Jean Figure 1: Simplifie"
W05-1501,J93-4001,0,0.0471588,"ss all phrasal constraints by a CF parser which produces a shared forest2 of polynomial size in polynomial time. Second, this shared forest is used, as a whole, to decide which functional constraints to process. For ambiguous CF backbones, this two pass computation is more efficient than interleaving phrasal and functional constraints.3 Another advantage of this two pass vision is that the CF parser may be easily replaced by another one. It may also be replaced by a more powerful parser.4 We choose to evaluate functional constraints directly on the shared forest since it has been proven (See (Maxwell and Kaplan, 1993)), as one can easily expect, that techniques which evaluate functional constraints on an enumeration of the resulting phrase-structure trees are a computational disaster. This article explores the computation of f-structures directly (without unfolding) on shared forests. We will see how, in some cases, our parser allows to deal with potential combinatorial explosion. Moreover, at all levels, error recovering mechanisms turn our system into a robust parser. Our parser, called S X L FG, has been evaluated with two large-coverage grammars for French, on corpora of various genres. In the last sec"
W05-1501,P02-1035,0,0.441904,"these concepts. For this reason, we designed a parser that is compatible with a linguistic theory, namely LFG, as well as robust and efficient despite the high variability of language production. Developing a new parser for LFG (LexicalFunctional Grammars, see, e.g., (Kaplan, 1989)) is not in itself very original. Several LFG parsers already exist, including those of (Andrews, 1990) or (Briffault et al., 1997). However, the most famous LFG system is undoubtedly the Xerox Linguistics Environment (XLE) project which is the successor of the Grammars Writer’s Workbench (Kaplan and Maxwell, 1994; Riezler et al., 2002; Kaplan et al., 2004). XLE is a large project which concentrates a lot of linguistic and computational technology, relies on a similar point of view on the balance between shallow and deep parsing, and has been successfully used to parse large unrestricted corpora. Nevertheless, these parsers do not always use in the most extensive way all existing algorithmic techniques of computation sharing and compact information representation that make it possible to write an efficient LFG parser, despite the fact that the LFG formalism, as many other formalisms relying on unification, is NP-hard. Of co"
W05-1501,W03-3005,1,\N,Missing
W06-1522,W98-0106,0,\N,Missing
W06-1522,E93-1045,0,\N,Missing
W06-1522,C96-2103,0,\N,Missing
W07-2213,P89-1018,0,0.0855301,"e both a configuration as an element of Q×T ∗ and derive a binary relation between 5 For example, in the previous complete derivation d, let the right-hand side α be the (vocabulary) string X1 · · · Xk · · · Xp in which each symbol Xk derives the ter∗ minal string xk ∈ T ∗ (we have Xk ⇒ xk and w2 = G x1 · · · xk · · · xp ), then the instantiated production A[i0 ..ip ] → X1 [i0 ..i1 ] · · · Xk [ik−1 ..ik ] · · · Xp [ip−1 ..ip ] with i0 = |w1 |+ 1, i1 = i0 + |x1 |, . . . , ik = ik−1 + |xk |. . . and ip = i0 + |w2 | is an element of PGw . 6 The popular notion of shared forests mainly comes from (Billot and Lang, 1989). 96 configurations, noted ⊢ by (q, tx) ⊢ (q ′ , x), iff A A (q, t, q ′ ) ∈ δ. If w′ w′′ ∈ T ∗ , we call derivation any sequence of the form (q ′ , w′ w′′ ) ⊢ · · · ⊢ (q ′′ , w′′ ). A A If w ∈ T ∗ , the initial configuration is noted c0 and is the pair (q0 , w). A final configuration is noted cf and has the form (qf , ε) with qf ∈ F . A complete derivation is a derivation which starts with c0 and ends in a final configuration cf . In that case we have ∗ c0 ⊢ cf . A The language L(A) defined (generated, recognized) by the FSA A is the set of all terminal strings w for which there exists a compl"
W07-2213,W05-1501,1,0.853649,"easures presented in this section have been taken on a 1.7GHz AMD Athlon PC with 1.5 Gb of RAM running Linux. All parsers are written in C and have been compiled with gcc 2.96 with the O2 optimization flag. 4.1 Grammars and corpus We have performed experiments with two large grammars described below. The first one is an auto101 matically generated CFG, the other one is the CFG equivalent of a TIG automatically extracted from a factorized TAG. The first grammar, named GT >N , is a variant of the CFG backbone of a large-coverage LFG grammar for French used in the French LFG parser described in (Boullier and Sagot, 2005). In this variant, the set T of terminal symbols is the whole set of French inflected forms present in the Lefff , a largecoverage syntactic lexicon for French (Sagot et al., 2006). This leads to as many as 407,863 different terminal symbols and 520,711 lexicalized productions (hence, the average number of categories — which are here non-terminal symbols — for an inflected form is 1.27). Moreover, this CFG entails a non-neglectible amount of syntactic constraints (including over-generating sub-categorization frame checking), which implies as many as |Pu |= 19, 028 non-lexicalized productions."
W07-2213,W03-3005,1,0.685636,"s check can be performed on (Gc , w) in worst-case time O(|Gc |×|Σ|3 ) (recall that |Σ |≤ n). This time reduces to O(|Gc |× |Σ|2 ) if the input sentence is not a DAG but a string. 13 This is equivalent to assume the existence in the grammar of a super-production whose right-hand side has the form $S$. This statement does not hold any more if we exclude from P the productions that have been previously erased during the current a-filter. In that case, an empty set indicates that the production Z → αU β can be erased. Z→αU β σ ∗ 100 14 c 3.5 Dynamic Set Automaton Filtering Strategy: d-filter In (Boullier, 2003) the author has presented a method that takes a CFG G and computes a FSA that defines a regular superset of L(G). However his method would produce intractable gigantic FSAs. Thus he uses his method to dynamically compute the FSA at parse time on a given source text. Based on experimental results, he shows that his method called dynamic set automaton (DSA) is tractable. He uses it to guide an Earley parser (See (Earley, 1970)) and shows improvements over the non guided version. The DSA method can directly be used as a filtering strategy since the states of the underlying FSA are in fact sets of"
W07-2213,P81-1022,0,0.817343,"a-filter. In that case, an empty set indicates that the production Z → αU β can be erased. Z→αU β σ ∗ 100 14 c 3.5 Dynamic Set Automaton Filtering Strategy: d-filter In (Boullier, 2003) the author has presented a method that takes a CFG G and computes a FSA that defines a regular superset of L(G). However his method would produce intractable gigantic FSAs. Thus he uses his method to dynamically compute the FSA at parse time on a given source text. Based on experimental results, he shows that his method called dynamic set automaton (DSA) is tractable. He uses it to guide an Earley parser (See (Earley, 1970)) and shows improvements over the non guided version. The DSA method can directly be used as a filtering strategy since the states of the underlying FSA are in fact sets of items. For a CFG G = (N, T, P, S), an item (or dotted production) is an element of {[A → α.β] |A → αβ ∈ P }. A complete item has the form [A → γ.], it indicates that the production A → γ has been, in some sense, recognized. Thus, the complete items of the DSA states gives the set of productions selected by the DSA. This selection can be further refined if we also use the mirror DSA which processes the source text from right"
W07-2213,2000.iwpt-1.18,0,0.0305198,"p βp ⇒ ∗ α1 · · · αp Xp βp ⇒ α1 · · · αp Xp , ∗ with βp ⇒ ε. If for each couple (a′ , b) in which b a′ has the previous definition and is a terminal symbol that can terminate (the terminal strings generated by) Xp , there is no transition on b that can follow a transition on a′ in the DAG w, the production Xp−1 → αp Xp βp can be erased if it is not valid in another context. In order to formalize these notions we define several binary relations together with their (reflexive) transitive closure. Within a CFG G = (N, T, P, S), we first define left-corner noted x. Left-corner (Nederhof, 1993; 99 Moore, 2000), hereafter LC, is a well-known relation since many parsing strategies are based upon it. We say that X is in the LC of A and we write A x X ∗ iff (A, X) ∈ {(B, Y ) |B → αY β ∈ P ∧ α ⇒ ε}. We can write A G x A→αXβ X to enforce how the couple (A, X) may be produced. For its dual relation, right-corner, noted y, we say that X is in the right corner of A and we write X y A ∗ iff (X, A) ∈ {(Y, B) |B → αY β ∈ P ∧ β ⇒ G ε}. We can write X y A→αXβ A to enforce how the couple (X, A) may be produced. We also define the first (resp. last) relation noted →t (resp. ←t ) by →t = {(X, t) |X ∈ V ∧ t ∈ ∗ T"
W07-2213,A00-2036,0,0.142193,"echniques that the key to practically improve these processes is to reduce their search space. This is also the case in parsing and in particular in CF parsing. 104 Many parsers process their inputs from left to right but we can find in the literature other parsing strategies. In particular, in NLP, (van Noord, 1997) and (Satta and Stock, 1994) propose bidirectional algorithms. These parsers have the reputation to have a better efficiency than their left-to-right counterpart. This reputation is not only based upon experimental results (van Noord, 1997) but also upon mathematical arguments in (Nederhof and Satta, 2000). This is specially true when the productions of the CFG strongly depend on lexical information. In that case the parsing search space is reduced because the constraints associated to lexical elements are evaluated as early as possible. We can note that our filtering strategies try to reach the same purpose by a totally different mean: we reduce the parsing search space by eliminating as many productions as possible, including possibly non-lexicalized productions whose irrelevance to parse the current input can not be directly deduced from that input. We can also remark that our results are no"
W07-2213,E93-1036,0,0.073119,"Xi ⇒ ··· Xp−1 →αp Xp βp ⇒ ∗ α1 · · · αp Xp βp ⇒ α1 · · · αp Xp , ∗ with βp ⇒ ε. If for each couple (a′ , b) in which b a′ has the previous definition and is a terminal symbol that can terminate (the terminal strings generated by) Xp , there is no transition on b that can follow a transition on a′ in the DAG w, the production Xp−1 → αp Xp βp can be erased if it is not valid in another context. In order to formalize these notions we define several binary relations together with their (reflexive) transitive closure. Within a CFG G = (N, T, P, S), we first define left-corner noted x. Left-corner (Nederhof, 1993; 99 Moore, 2000), hereafter LC, is a well-known relation since many parsing strategies are based upon it. We say that X is in the LC of A and we write A x X ∗ iff (A, X) ∈ {(B, Y ) |B → αY β ∈ P ∧ α ⇒ ε}. We can write A G x A→αXβ X to enforce how the couple (A, X) may be produced. For its dual relation, right-corner, noted y, we say that X is in the right corner of A and we write X y A ∗ iff (X, A) ∈ {(Y, B) |B → αY β ∈ P ∧ β ⇒ G ε}. We can write X y A→αXβ A to enforce how the couple (X, A) may be produced. We also define the first (resp. last) relation noted →t (resp. ←t ) by →t = {(X, t)"
W07-2213,J95-4002,0,0.0599378,"million symbol occurrences and several hundred thousands rules. Traditional parsers are not usually prepared to handle them, either because these grammars are simply too big (the parser’s internal structures blow up) or the time spent to analyze a sentence becomes prohibitive. This paper will concentrate on context-free grammars (CFG) and their associated parsers. However, virtually all Tree Adjoining Grammars (TAG, see e.g., (Schabes et al., 1988)) used in NLP applications can (almost) be seen as lexicalized Tree Insertion Grammars (TIG), which can be converted into strongly equivalent CFGs (Schabes and Waters, 1995). Hence, the parsing techniques and tools described here can be applied to most TAGs used for NLP, with, in the worst case, a light over-generation which can be easily and efficiently eliminated in a complementary pass. This is indeed what we have achieved with a TAG automatically extracted from (Villemonte de La Clergerie, 2005)’s large-coverage factorized French TAG, as we will see in Section 4. Even (some kinds of) non CFGs may benefit from the ideas described in this paper. The reason why the run-time of context-free (CF) parsers for large CFGs is damaged relies on a theoretical result. A"
W07-2213,C88-2121,0,0.425771,"Missing"
W07-2213,J97-3004,0,0.0965258,"Missing"
W07-2213,W05-1522,0,0.169817,"their associated parsers. However, virtually all Tree Adjoining Grammars (TAG, see e.g., (Schabes et al., 1988)) used in NLP applications can (almost) be seen as lexicalized Tree Insertion Grammars (TIG), which can be converted into strongly equivalent CFGs (Schabes and Waters, 1995). Hence, the parsing techniques and tools described here can be applied to most TAGs used for NLP, with, in the worst case, a light over-generation which can be easily and efficiently eliminated in a complementary pass. This is indeed what we have achieved with a TAG automatically extracted from (Villemonte de La Clergerie, 2005)’s large-coverage factorized French TAG, as we will see in Section 4. Even (some kinds of) non CFGs may benefit from the ideas described in this paper. The reason why the run-time of context-free (CF) parsers for large CFGs is damaged relies on a theoretical result. A well-known result is that CF parsers may reach a worst-case running time of O(|G|× n3 ) where |G |is the size of the CFG and n is the length of the source text.1 In typical NLP applications which mainly work at the sentence level, the length of a sentence does not often go beyond a value of say 100, while its average length is ar"
W07-2213,sagot-etal-2006-lefff,1,\N,Missing
W09-3818,W05-1501,1,0.861787,"the number of trees in the forest, such as LFG f-structures construction or some advanced reranking techniques. The experiments detailed in the last part of this paper show that the overgeneration factor of pruned sub-forest is more or less constant (see 6): after pruning the forest so as to keep the n best trees, the resulting forest contains approximately 103 n trees. At least for some post-parsing processes, this overhead is highly problematic. For example, although LFG parsing can be achieved by computing LFG f-structures on top of a c-structure parse forest with a reasonable efficiency (Boullier and Sagot, 2005), it is clear that a 103 factor drastically affects the overall speed of the LFG parser. Therefore, simply pruning the forest is not an adequate solution. However, it will prove useful for comparison purposes. The new direction that we explore in this paper is the production of shared forests that contain exactly the n most likely trees, avoiding both the explicit construction of n different trees and the over-generation of pruning techniques. This can be seen as a transduction which is applied on a forest and produces another forest. The transduction applies some local transformations on the"
W09-3818,P81-1022,0,0.635558,"st and produce shared forests that contain exactly the n most likely trees of the initial forest. Such forests are suitable for subsequent processing, such as (some types of) reranking or LFG fstructure computation, that can be performed ontop of a shared forest, but that may have a high (e.g., exponential) complexity w.r.t. the number of trees contained in the forest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank. 1 Introduction The output of a CFG parser based on dynamic programming, such as an Earley parser (Earley, 1970), is a compact representation of all syntactic parses of the parsed sentence, called a shared parse forest (Lang, 1974; Lang, 1994). It can represent an exponential number of parses (with respect to the length of the sentence) in a cubic size structure. This forest can be used for further processing, as reranking (Huang, 2008) or machine translation (Mi et al., 2008). When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only the n most likely trees of the forest. Standard state-of-the-art algorithms that extract the n best"
W09-3818,W05-1506,0,0.594224,"ompact representation of all syntactic parses of the parsed sentence, called a shared parse forest (Lang, 1974; Lang, 1994). It can represent an exponential number of parses (with respect to the length of the sentence) in a cubic size structure. This forest can be used for further processing, as reranking (Huang, 2008) or machine translation (Mi et al., 2008). When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only the n most likely trees of the forest. Standard state-of-the-art algorithms that extract the n best parses (Huang and Chiang, 2005) produce a collection of trees, losing the factorization that has been achieved by the parser, and reproduce some identical sub-trees in several parses. This situation is not satisfactory since postparsing processes, such as reranking algorithms or attribute computation, cannot take advantage 117 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 117–128, c Paris, October 2009. 2009 Association for Computational Linguistics (resp. upper bound), and can be extracted by the operator lb() (resp. ub()). An instantiated production (or instantiated rule) is a cont"
W09-3818,P08-1067,0,0.0708541,"e number of trees contained in the forest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank. 1 Introduction The output of a CFG parser based on dynamic programming, such as an Earley parser (Earley, 1970), is a compact representation of all syntactic parses of the parsed sentence, called a shared parse forest (Lang, 1974; Lang, 1994). It can represent an exponential number of parses (with respect to the length of the sentence) in a cubic size structure. This forest can be used for further processing, as reranking (Huang, 2008) or machine translation (Mi et al., 2008). When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only the n most likely trees of the forest. Standard state-of-the-art algorithms that extract the n best parses (Huang and Chiang, 2005) produce a collection of trees, losing the factorization that has been achieved by the parser, and reproduce some identical sub-trees in several parses. This situation is not satisfactory since postparsing processes, such as reranking algorithms or attribute computation, cannot take advantage 117"
W09-3818,W01-1812,0,0.069726,"Missing"
W09-3818,J93-2004,0,0.031559,"ectangles algorithm to this example, we can now give its final result, in which the axiom’s (unnecessary) decorations have been removed: {1,1} 1,2 S1..3 → A1..2 B2..3 {2,2} 1,1 S1..3 → A1..2 B2..3 → A21..2 A21..2 → a1..2 → B12..3 B12..3 → b2..3 → B22..3 B22..3 → b2..3 Compared to the forest built by the ranksets algorithm, this forest has one less production and one less non-terminal symbol. It has only one more production than the over-generating pruned forest. 4 Experiments on the Penn Treebank The methods described in section 3 have been tested on a PCFG G extracted from the Penn Treebank (Marcus et al., 1993). G has been extracted naively: the trees have been decomposed into binary context free rules, and the probability of every rule has been estimated by its relative frequency (number of occurrences of the rule divided by the number of occurrences of its left hand side). Rules occurring less than 3 times and rules with probabilities lower than 3 × 10−4 have been eliminated. The grammar produced contains 932 non terminals and 3, 439 rules.7 The parsing has been realized using the S YN TAX system which implements, and optimizes, the Earley algorithm (Boullier, 2003). The evaluation has been conduc"
W09-3818,P08-1023,0,0.0155304,"rest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank. 1 Introduction The output of a CFG parser based on dynamic programming, such as an Earley parser (Earley, 1970), is a compact representation of all syntactic parses of the parsed sentence, called a shared parse forest (Lang, 1974; Lang, 1994). It can represent an exponential number of parses (with respect to the length of the sentence) in a cubic size structure. This forest can be used for further processing, as reranking (Huang, 2008) or machine translation (Mi et al., 2008). When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only the n most likely trees of the forest. Standard state-of-the-art algorithms that extract the n best parses (Huang and Chiang, 2005) produce a collection of trees, losing the factorization that has been achieved by the parser, and reproduce some identical sub-trees in several parses. This situation is not satisfactory since postparsing processes, such as reranking algorithms or attribute computation, cannot take advantage 117 Proceedings of the 11th International Co"
W09-3818,W03-3005,1,\N,Missing
W09-3841,P87-1015,0,0.530208,"Missing"
W09-3841,W01-1807,0,0.110451,"h will be abbreviated as PRCG.1 PRCGs are very attractive since they are more powerful than the Linear Context-Free Rewriting Systems (LCFRSs) by (Vijay-Shanker et al., 1987). In fact LCFRSs are equivalent to simple PRCGs which are a subclass of PRCGs. Many Mildly ContextSensitive (MCS) formalisms, including Tree Adjoining Grammars (TAGs) and various kinds of Multi-Component TAGs, have already been 1 Negative RCGs do not add formal power since both versions exactly cover the class PTIME of languages recognizable in deterministic polynomial time (see (Boullier, 2004) for an indirect proof and (Bertsch and Nederhof, 2001) for a direct proof). translated into their simple PRCG counterpart in order to get an efficient parser for free (see for example (Barth´elemy et al., 2001)). However, in many Natural Language Processing applications, the most suitable input for a parser is not a sequence of words (forms, terminal symbols), but a more complex representation, usually defined as a Direct Acyclic Graph (DAG), which correspond to finite regular languages, for taking into account various kinds of ambiguities. Such ambiguities may come, among others, from the output of speech recognition systems, from lexical ambigu"
W09-3841,P01-1007,1,\N,Missing
W09-4619,C94-1097,0,0.469233,"Missing"
W09-4619,C08-1080,1,\N,Missing
W09-4619,sagot-etal-2006-lefff,1,\N,Missing
W10-1807,J96-2004,0,0.0312586,"nn Treebank annotations as reference and calculated a simple precision as compared to this reference. Figure 1 gives an overview of the obtained results (note that the scale is not regular). However, this is not sufficient to evaluate the quality of the annotation as, actually, the reference annotation is not perfect (see below). We therefore evaluated the reliability of the annotation, calculating the inter-annotator agreement between Annotator1 and Annotator2 on the 100-sentence series they both annotated. We calculated this agreement on some of the subcorpora using π, aka Carletta’s Kappa (Carletta, 1996)4 . The results of this are shown in table 2. 4.2 Impact of the Pre-annotation Accuracy on Annotation Time Before discussing the results of Experiment 2, annotation time measurements during Experiment 3 confirm that using a good quality pre-annotation (say, MEltALL en ) strongly reduces the annotation time as compared with fully manual annotation. For example, Annotator1 needed an average time of approximately 7.5 minutes to annotate 10 sentences without pre-annotation (Experiment 3), whereas Experiment 2 shows that it goes down to approximately 2.5 minutes when using MEltALL en pre-annotation"
W10-1807,Y09-1013,1,0.574324,"Missing"
W10-1807,2009.jeptalnrecital-long.29,1,0.701093,"s are a bit disappointing as they could not find a direct improvement of annotation time using pre-annotation. The authors reckon this might be at least partly due to “an interaction between time savings from pre-annotation and time savings due to a training effect.” For the same reason, they had to exclude some of the annotation results for quality evaluation in order to show that, in line with (Marcus et al., 1993), quality pre-annotation helps increasing annotation quality. They also found that noisy and low quality pre-annotation does not overall corrupt human judgment. On the other hand, Fort et al. (2009) claim that pre-annotation introduces a bias in named entity annotation, due to the preference given by anno3 Experimental Setup The idea underlying our experiments is the following. We split the Penn Treebank corpus (Marcus et al., 1993) in a usual manner, namely we use Sections 2 to 21 to train various instances of a POS tagger, and Section 23 to perform the actual experiments. In order to measure the impact of the POS tagger’s quality, we trained it on subcorpora of increasing sizes, and pre-annotated Section 23 with these various POS taggers. Then, we manually annotated parts of Section 23"
W10-1807,J93-2004,0,0.0756342,"ion time and inter-annotator agreement. This article is organized as follows. In Section 2, we mention some related work, while Section 3 describes the experimental setup, followed by a discussion on the obtained results (Section 4) and a conclusion. This article details a series of carefully designed experiments aiming at evaluating the influence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a specific attention drawn to biases. For this purpose, we manually annotated parts of the Penn Treebank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations. These experiments confirm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should be taken into account. They finally demonstrate that even a not so accurate tagger can help improving annotation speed. 1 2 2.1 Related Work Pre-annotation for POS Tagging Very few manual annotation projects give details about the campaign itself. One major exception is the Penn Treebank project (Marcus et al., 1993), th"
W10-1807,W96-0213,0,0.479735,"Missing"
W10-1807,W09-3003,0,0.172305,"n 4) and a conclusion. This article details a series of carefully designed experiments aiming at evaluating the influence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a specific attention drawn to biases. For this purpose, we manually annotated parts of the Penn Treebank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations. These experiments confirm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should be taken into account. They finally demonstrate that even a not so accurate tagger can help improving annotation speed. 1 2 2.1 Related Work Pre-annotation for POS Tagging Very few manual annotation projects give details about the campaign itself. One major exception is the Penn Treebank project (Marcus et al., 1993), that provided detailed information about the manual annotation methodology, evaluation and cost. Marcus et al. (1993) thus showed that manual tagging took twice as long as correcting pre-tagged text and resulted in twice the inter-"
W10-1807,E09-1087,0,0.0102857,"Missing"
W10-1807,W09-3002,0,\N,Missing
W10-1807,J08-4004,0,\N,Missing
W10-4413,E03-1030,0,0.0806901,"Missing"
W10-4413,2005.jeptalnrecital-court.13,0,0.631025,"e Laurence Danlos Alpage & Univ. Paris 7 Paris, France djame.seddah@paris-sorbonne.fr benoit.sagot@inria.fr laurence.danlos@linguist.jussieu.fr Abstract In this paper1 we present an extension of MCTAGs with Local Shared Derivation (Seddah, 2008) which can handle non local elliptic coordinations. Based on a model for control verbs that makes use of so-called ghost trees, we show how this extension leads to an analysis of argument cluster coordinations that provides an adequate derivation graph. This is made possible by an original interpretation of the MCTAG derivation tree mixing the views of Kallmeyer (2005) and Weir (1988). 1 Introduction Elliptic coordinate structures are a challenge for most constituent-based syntactic theories. To model such complex phenomena, many works have argued in favor of factorized syntactic structures (Maxwell and Manning, 1996), while others have argued for distributive structures that include a certain amount of non-lexically realized elements (Beavers and Sag, 2004). Of course, the boundary between those two approaches is not sharp since one can decide to first build a factorized syntactic analysis and then construct a more distributive structure (e.g., logical or"
W10-4413,W10-4412,0,0.249106,"element. (1) Jean aimei Marie et Paul εi Virginie John lovesi Mary and Paul εi Virginia Calling this second lexically unrealized tree a ghost tree, the missing anchor can be retrieved simply because the tree it anchors is in the same MC-Set as its ghost tree. In other words, the label of the MCSet includes the anchor of its fully lexicalized tree. The application of this model to (1) is shown in Figure 1. Note that this account only requires the expressivity of Tree-Local MCTAGs and that unlike other approaches for gapping in the LTAG framework (Sarkar and Joshi, 1996; Seddah and Sagot, 2006; Lichte and Kallmeyer, 2010), this proposal for gapping does not require any special device or modification of the formalism itself. In order to model derivations that involve the elision of one syntactic verbal argument as in right node raising cases (RNR) or right subject elision coordinations, the formalism is extended with oriented links, called local shared derivation (local SD), between mandatory derivation site nodes: whenever a derivation is not realized on a given node and assuming that a local SD has been defined between this node and one possible antecedent, a derivation between those nodes is inserted in the"
W10-4413,C94-2151,0,0.0386764,"showed that, assuming the use of regular operators to handle n-ary coordinations, a broad range of coordinate structures could be processed using a TreeLocal MCTAG-based formalism named Tree Local MCTAG with Local Shared Derivations. Nevertheless, being tied to the domain of locality of a tree set, the very nature of this mechanism forbids the sharing of derivations between different tree sets, thus preventing it from analyzing non-local elliptic coordinations. In this paper, we introduce an extension of this model that can handle non-local elliptic coordination — close to unbounded ellipsis (Milward, 1994) —, which can be found in structures involving 2 See (Abeillé, 2006; Mouret, 2006) for discussions about this assumption. 101 Djamé Seddah, Benoit Sagot, Laurence Danlos α-aimer    a)               α-et ✟ N0↓ S ✟❍ ✟✟ ❍ ❍ V [aimer] b) ❍ N1↓          ✟ S↓ Sc ✟❍ ✟✟ ❍❍ N0↓ V N1↓        ε  S ✟❍ ❍ ✟ ❍ et Sc ↓ α-X N α-et ✟❍ ✟✟ ❍❍ ✟✟ α-aimer(a) ✟❍ ✟ ❍ ✟ ❍ α-Jean α-Marie ❍❍ α-aimer(b) ✟❍ ✟ ❍ ✟ ❍ α-Paul α-Virginie X={Jean|Marie|Paul|Virginie} Figure 1: Sketch of an analysis for “Jean aime Marie et Paul Virignie” The root label of α-aimer(b) is subscripted in order to av"
W10-4413,W06-1522,1,0.639988,"is anchored by an empty element. (1) Jean aimei Marie et Paul εi Virginie John lovesi Mary and Paul εi Virginia Calling this second lexically unrealized tree a ghost tree, the missing anchor can be retrieved simply because the tree it anchors is in the same MC-Set as its ghost tree. In other words, the label of the MCSet includes the anchor of its fully lexicalized tree. The application of this model to (1) is shown in Figure 1. Note that this account only requires the expressivity of Tree-Local MCTAGs and that unlike other approaches for gapping in the LTAG framework (Sarkar and Joshi, 1996; Seddah and Sagot, 2006; Lichte and Kallmeyer, 2010), this proposal for gapping does not require any special device or modification of the formalism itself. In order to model derivations that involve the elision of one syntactic verbal argument as in right node raising cases (RNR) or right subject elision coordinations, the formalism is extended with oriented links, called local shared derivation (local SD), between mandatory derivation site nodes: whenever a derivation is not realized on a given node and assuming that a local SD has been defined between this node and one possible antecedent, a derivation between th"
W10-4413,W08-2311,1,0.829275,"ill a challenge for theories based on strict atomic category coordination. In the broader context of ellipsis resolution, Dalrymple et al. (1991) propose to consider elided elements as free logical variables resolved using Higher Order Unification as the solving operation. Inspired by this approach and assuming that non-constituent coordination can be analyzed with ellipsis (Beavers and Sag, 2004),2 we consider elliptic coordination as involving parallel structures where all non lexically realized syntactic elements must be represented in a derivation structure. This path was also followed by Seddah (2008) who proposed to use the ability of Multi Component TAGs (MCTAGs) (Weir, 1988) to model such a parallelism by including conjunct trees in a same tree set. This simple proposal allows for a straightforward analysis of gapping constructions. The coverage of this account is then extended by introducing links called local shared derivations which, by allowing derivations to be shared across trees of a same set, permit to handle various elliptic coordinate structures in an efficient way. This work showed that, assuming the use of regular operators to handle n-ary coordinations, a broad range of coo"
W10-4413,W07-0402,0,0.0495269,"Missing"
W10-4413,C90-3045,0,0.712309,"Missing"
W10-4413,W05-1522,0,0.0307018,"her end of the link. This mechanism can be generalized to MCSets with more than one local shared derivation. This skteches the proof that the set of languages generated by MCTAG-LSDs is the same as that generated by MCTAGs. Therefore, MCTAG-LSDs and MCTAGs have the same weak generative capacity. Moreover, these considerations still hold while restricting GM CT AG to be TL-MCTAG. Therefore, TL-MCTAG-LSDs and TL-MCTAGs have the same weak generative power. In order to cope with very large grammar size, the use of regular operators to factorize out TAG trees has been proposed by (Villemonte de La Clergerie, 2005), and has lead to a drastic reduction of the number of trees in the grammar. The resulting formalism is called factorized TAGs and was adapted by Seddah (2008) to the MCTAG-LSD framework in order to handle n-ary coordinations. The idea is to factorize MCTAG-LSD sets that have the same underlying MCTAG set (i.e. they are identical if links are ignored). Indeed, all such MC sets can be merged into one unique tree set associated with the union of all corresponding link sets. However, as with factorized TAGs, we need to add to the resulting tree set a list of constraints, R, on the construction of"
W12-0508,E06-1002,0,0.0382143,"ection to improve the result precision but do not provide referential information, which can be useful in IE applications. EL achieves the association of NER results with uniquely identified entities, by relying on an entity repository, available to the extraction system and defined beforehand in order to serve as a target for mention linking. Knowledge about entities is gathered in a dedicated knowledge base (KB) to evaluate each entity’s similarity to a given context. After the task of EL was initiated with Wikipedia-based works on entity disambiguation, in particular by Cucerzan (2007) and Bunescu and Pasca (2006), numerous systems have been developed, encouraged by the TAC 2009 KB population task (McNamee and Dang, 2009). Most often in EL, Wikipedia serves both as an entity repository (the set of articles referring to entities) and as a KB about entities (derived from Wikipedia infoboxes and articles which contain text, metadata such as categories and hyperlinks). Zhang et al. (2010) show how Wikipedia, by providing a large annotated corpus of linked ambiguous entity mentions, pertains efficiently to the EL task. Evaluated EL systems at TAC report a top accuracy rate of 0.80 on English data (McNamee e"
W12-0508,D07-1074,0,0.073248,"n can follow the detection to improve the result precision but do not provide referential information, which can be useful in IE applications. EL achieves the association of NER results with uniquely identified entities, by relying on an entity repository, available to the extraction system and defined beforehand in order to serve as a target for mention linking. Knowledge about entities is gathered in a dedicated knowledge base (KB) to evaluate each entity’s similarity to a given context. After the task of EL was initiated with Wikipedia-based works on entity disambiguation, in particular by Cucerzan (2007) and Bunescu and Pasca (2006), numerous systems have been developed, encouraged by the TAC 2009 KB population task (McNamee and Dang, 2009). Most often in EL, Wikipedia serves both as an entity repository (the set of articles referring to entities) and as a KB about entities (derived from Wikipedia infoboxes and articles which contain text, metadata such as categories and hyperlinks). Zhang et al. (2010) show how Wikipedia, by providing a large annotated corpus of linked ambiguous entity mentions, pertains efficiently to the EL task. Evaluated EL systems at TAC report a top accuracy rate of 0."
W12-0508,doddington-etal-2004-automatic,0,0.145699,"a set of possible mentions, preserving a number of ambiguous readings. The linking process must thereafter evaluate which readings are the most probable, based on the most likely entity matches inferred from a similarity measure with the context. NER has been widely addressed by symbolic, statistical as well as hybrid approaches. Its major part in information extraction (IE) and other NLP applications has been stated and encouraged by several editions of evaluation campaigns such as MUC (Marsh and Perzanowski, 1998), the CoNLL-2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) or ACE (Doddington et al., 2004), where NER systems show near-human performances for the English language. Our system aims at benefitting from both symbolic and statistical NER techniques, which have proven efficient 52 Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 52–60, c Avignon, France, April 23 2012. 2012 Association for Computational Linguistics but not necessarily over the same type of data and with different precision/recall tradeoff. NER considers the surface form of entities; some type disambiguation and name normalization can follow the"
W12-0508,C10-1032,0,0.0808692,"Missing"
W12-0508,M98-1002,0,0.304329,"Missing"
W12-0508,mcnamee-etal-2010-evaluation,0,0.0120695,"ca (2006), numerous systems have been developed, encouraged by the TAC 2009 KB population task (McNamee and Dang, 2009). Most often in EL, Wikipedia serves both as an entity repository (the set of articles referring to entities) and as a KB about entities (derived from Wikipedia infoboxes and articles which contain text, metadata such as categories and hyperlinks). Zhang et al. (2010) show how Wikipedia, by providing a large annotated corpus of linked ambiguous entity mentions, pertains efficiently to the EL task. Evaluated EL systems at TAC report a top accuracy rate of 0.80 on English data (McNamee et al., 2010). Entities that are unknown to the reference database, called out-of-base entities, are also considered by EL, when a given mention refers to an entity absent from the available Wikipedia articles. This is addressed by various methods, such as setting a threshold of minimal similarity for an entity selection (Bunescu and Pasca, 2006), or training a separate binary classifier to judge whether the returned top candidate is the actual denotation (Zheng et al., 2010). Our approach of this issue is closely related to the method of Dredze et al. in (2010), where the out-of-base entity is considered"
W12-0508,sagot-stern-2012-aleda,1,0.842661,"methodology applied are presented in section 4. Section 5 illustrates this methodology with a number of experiments and evaluation results. 2 Entity Resources Our system relies on two large-scale resources which are very different in nature: • the entity database Aleda, automatically extracted from the French Wikipedia and Geonames; • a knowledge base extracted from a large corpus of AFP news wires, with distributional and contextual information about automatically detected entites. 2.1 Aleda The Aleda entity repository2 is the result of an extraction process from freely available resources (Sagot and Stern, 2012). We used the French Aleda databased, extracted the French Wikipedia3 and Geonames4 . In its current development, it provides a generic and wide coverage entity resource accessible via a database. Each entity in Aleda is associated with a range of attributes, either referential (e.g., the type of the entity among Person, Location, Organization and Company, the population for a location or the gender of a person, etc.) 2 Aleda is part of the Alexina project and freely available at https://gforge.inria.fr/projects/alexina/. 3 4 53 www.fr.wikipedia.org www.geonames.org or formal, like the entity’"
W12-0508,sagot-2010-lefff,1,0.912742,"set of entities forms the majority of occurrences. Our particular context can thus justify the need for a domain specific KB. As opposed to Wikipedia where entities are identifiable by hyperlinks, AFP corpora provide no such indications. Wikipedia is in fact a corpus where entity mentions are clearly and uniquely linked, whereas this is what we aim at achieving over AFP’s raw textual data. The acquisition of domain specific knowledge about entities from AFP corpora must circumvent this lack of indications. In this perspective we use an implementation of a naive linker described in (Stern and Sagot, 2010). For the main part, this system is based on heuristics favoring popular entities in cases of ambiguities. An evaluation of this system showed good accuracy of entity linking (0.90) over the subset of correctly detected entity mentions:5 on the evaluation data, the resulting NER reached a precision of 0.86 and a recall of 0.80. Therefore we rely on the good accuracy of this system to identify entities in our corpus, bearing in mind that it will however include cases of false detections, while knowledge will not be available on missed entities. It can be observed that by doing so, we aim at per"
W12-0508,2010.jeptalnrecital-court.23,1,0.778606,"ce a small set of entities forms the majority of occurrences. Our particular context can thus justify the need for a domain specific KB. As opposed to Wikipedia where entities are identifiable by hyperlinks, AFP corpora provide no such indications. Wikipedia is in fact a corpus where entity mentions are clearly and uniquely linked, whereas this is what we aim at achieving over AFP’s raw textual data. The acquisition of domain specific knowledge about entities from AFP corpora must circumvent this lack of indications. In this perspective we use an implementation of a naive linker described in (Stern and Sagot, 2010). For the main part, this system is based on heuristics favoring popular entities in cases of ambiguities. An evaluation of this system showed good accuracy of entity linking (0.90) over the subset of correctly detected entity mentions:5 on the evaluation data, the resulting NER reached a precision of 0.86 and a recall of 0.80. Therefore we rely on the good accuracy of this system to identify entities in our corpus, bearing in mind that it will however include cases of false detections, while knowledge will not be available on missed entities. It can be observed that by doing so, we aim at per"
W12-0508,W03-0419,0,0.422026,"Missing"
W12-0508,C10-1145,0,0.0227266,"in a dedicated knowledge base (KB) to evaluate each entity’s similarity to a given context. After the task of EL was initiated with Wikipedia-based works on entity disambiguation, in particular by Cucerzan (2007) and Bunescu and Pasca (2006), numerous systems have been developed, encouraged by the TAC 2009 KB population task (McNamee and Dang, 2009). Most often in EL, Wikipedia serves both as an entity repository (the set of articles referring to entities) and as a KB about entities (derived from Wikipedia infoboxes and articles which contain text, metadata such as categories and hyperlinks). Zhang et al. (2010) show how Wikipedia, by providing a large annotated corpus of linked ambiguous entity mentions, pertains efficiently to the EL task. Evaluated EL systems at TAC report a top accuracy rate of 0.80 on English data (McNamee et al., 2010). Entities that are unknown to the reference database, called out-of-base entities, are also considered by EL, when a given mention refers to an entity absent from the available Wikipedia articles. This is addressed by various methods, such as setting a threshold of minimal similarity for an entity selection (Bunescu and Pasca, 2006), or training a separate binary"
W12-0508,N10-1072,0,0.0182522,"ity mentions, pertains efficiently to the EL task. Evaluated EL systems at TAC report a top accuracy rate of 0.80 on English data (McNamee et al., 2010). Entities that are unknown to the reference database, called out-of-base entities, are also considered by EL, when a given mention refers to an entity absent from the available Wikipedia articles. This is addressed by various methods, such as setting a threshold of minimal similarity for an entity selection (Bunescu and Pasca, 2006), or training a separate binary classifier to judge whether the returned top candidate is the actual denotation (Zheng et al., 2010). Our approach of this issue is closely related to the method of Dredze et al. in (2010), where the out-of-base entity is considered as another entry to rank. Our task differs from EL configurations outlined previously, in that its target is entity extraction from raw news wires from the news agency Agence France Presse (AFP), and not only linking relying on gold NER annotations: the input of the linking system is the result of an automatic NER step, which will produce errors of various kinds. In particular, spans erroneously detected as NEs will have to be discarded by our EL system. This cas"
W12-3007,W09-3302,0,0.0427638,"Missing"
W12-3007,E06-1002,0,0.00814671,"rlie the construction and enrichment of such a KB. 1 These entity types are the usual focus of Information Extraction systems and are defined among others by the ACE entity recognition task (Doddington et al., 2004). Benoˆıt Sagot INRIA-Alpage, Paris, France benoit.sagot@inria.fr This specific need is met by the KB population task, now well defined within the annual TAC dedicated track (Ji et al., 2011). KB population indeed relies on the ability to link entity mentions in textual data to a KB entry (entity linking subtask, henceforth EL),2 which follows pioneer work in entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). In a similar way to systems described in Dredze et al. (2010) and Ji & Grishman (2011), we conduct EL over AFP news wires in order to obtain relevant entities meant to populate the target KB, adapting these techniques to French data. This linking process is based on Web data extraction for both coverage and the purpose of Linked Data integration, which has become a widely explored trend in news management and publishing projects, such as the ones conducted by the BBC (Kobilarov et al., 2009) or the New York Times (NYT). Compared to other KB population settings, this knowledg"
W12-3007,charton-torres-moreno-2010-nlgbase,0,0.0536482,"tions for entities involved in current 36 events. The creation of a large-scale and unified entity resource is achieved by defining a database schema dedicated to entity representation, and by aligning both Wikipedia’s and GeoNames’ model with it. The schema considers Person, Organization and GPE as types of entries. The building of Aleda therefore relies on the identification of Wikipedia’s and GeoNames’ entries to which one of these types can be assigned. Wikipedia Exploiting Wikipedia as a large-scale entity resource has been the focus of numerous efforts, such as (Balasuriya et al., 2009; Charton and Torres-Moreno, 2010). Each Wikipedia article, referring to an entity, concept or notion, is referenced under a number of categories, often fine-grained and specific. A hierarchical or ontological organization of these categories can be inferred (Syed et al., 2008) but a given article is not anchored in a generic conceptual class such as an entity type in a straightforward fashion. The present model alignment thus consists in a mapping from Wikipedia categories to one of the target entity types. Each article mapped to one of these types leads to adding a corresponding entity in Aleda. The selection and typing proc"
W12-3007,D07-1074,0,0.0329637,"enrichment of such a KB. 1 These entity types are the usual focus of Information Extraction systems and are defined among others by the ACE entity recognition task (Doddington et al., 2004). Benoˆıt Sagot INRIA-Alpage, Paris, France benoit.sagot@inria.fr This specific need is met by the KB population task, now well defined within the annual TAC dedicated track (Ji et al., 2011). KB population indeed relies on the ability to link entity mentions in textual data to a KB entry (entity linking subtask, henceforth EL),2 which follows pioneer work in entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). In a similar way to systems described in Dredze et al. (2010) and Ji & Grishman (2011), we conduct EL over AFP news wires in order to obtain relevant entities meant to populate the target KB, adapting these techniques to French data. This linking process is based on Web data extraction for both coverage and the purpose of Linked Data integration, which has become a widely explored trend in news management and publishing projects, such as the ones conducted by the BBC (Kobilarov et al., 2009) or the New York Times (NYT). Compared to other KB population settings, this knowledge acquisition pro"
W12-3007,doddington-etal-2004-automatic,0,0.0299561,"ly entities, relevant for the news production and usable as metadata for content enrichment. This objective sets off the need for a dedicated knowledge base (KB) relying on a light-weight ontology of entities mappable to the Linked Data framework. Identification of entities such as persons, organizations and geopolitical entities (GPE S)1 in unstructured textual data, news wires in French in our case, underlie the construction and enrichment of such a KB. 1 These entity types are the usual focus of Information Extraction systems and are defined among others by the ACE entity recognition task (Doddington et al., 2004). Benoˆıt Sagot INRIA-Alpage, Paris, France benoit.sagot@inria.fr This specific need is met by the KB population task, now well defined within the annual TAC dedicated track (Ji et al., 2011). KB population indeed relies on the ability to link entity mentions in textual data to a KB entry (entity linking subtask, henceforth EL),2 which follows pioneer work in entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). In a similar way to systems described in Dredze et al. (2010) and Ji & Grishman (2011), we conduct EL over AFP news wires in order to obtain relevant entities meant to popul"
W12-3007,P11-1115,0,0.139599,"xtraction systems and are defined among others by the ACE entity recognition task (Doddington et al., 2004). Benoˆıt Sagot INRIA-Alpage, Paris, France benoit.sagot@inria.fr This specific need is met by the KB population task, now well defined within the annual TAC dedicated track (Ji et al., 2011). KB population indeed relies on the ability to link entity mentions in textual data to a KB entry (entity linking subtask, henceforth EL),2 which follows pioneer work in entity disambiguation (Bunescu and Pasca, 2006; Cucerzan, 2007). In a similar way to systems described in Dredze et al. (2010) and Ji & Grishman (2011), we conduct EL over AFP news wires in order to obtain relevant entities meant to populate the target KB, adapting these techniques to French data. This linking process is based on Web data extraction for both coverage and the purpose of Linked Data integration, which has become a widely explored trend in news management and publishing projects, such as the ones conducted by the BBC (Kobilarov et al., 2009) or the New York Times (NYT). Compared to other KB population settings, this knowledge acquisition process is done throughout a sequence of resources and extraction steps rather than in a cy"
W12-3007,C10-1032,0,\N,Missing
W12-3007,sagot-stern-2012-aleda,1,\N,Missing
W12-3408,W10-1408,1,0.916496,"Missing"
W12-3408,W10-1409,1,0.884663,"ols of the treebank by replacing word tokens by lemmas. 3.1 Experimental Setup In this section we describe the parsing formalism and POS tagging settings used in our experiments. PCFG-LAs To test our hypothesis, we use the grammatical formalism of Probabilistic ContextFree Grammars with Latent Annotations (PCFGLAs) (Matsuzaki et al., 2005; Petrov et al., 2006). These grammars depart from the standard PCFGs by automatically refining grammatical symbols during the training phase, using unsupervised techniques. They have been applied successfully to a wide range of languages, among which French (Candito and Seddah, 2010), German (Petrov and Klein, 2008), Chinese and Italian (Lavelli and Corazza, 2009). 56 For our experiments, we used the LORG PCFGLA parser implementing the CKY algorithm. This software also implements the techniques from Attia et al. (2010) for handling out-of-vocabulary words, where interesting suffixes for part-of-speech tagging are collected on the training set, ranked according to their information gain with regards to the partof-speech tagging task. Hence, all the experiments are presented in two settings. In the first one, called generic, unknown words are replaced with a dummy token UNK"
W12-3408,chrupala-etal-2008-learning,0,0.220092,"Missing"
W12-3408,J05-1003,0,0.0105145,"nce (around 2.3 points better, see Table 8). Table 7: Lemmmatization Experiments In all cases reduced2 is below the other tagsets wrt. to Parseval F1 although tagging accuracy is better. We can conclude that it is too poor from an informational point of view. 4 Discussion There is relatively few works actively pursued on statistical constituency parsing for Spanish. The initial work of Cowan and Collins (2005) consisted in a thorough study of the impact of various morphological features on a lexicalized parsing model (the Collins Model 1) and on the performance gain brought by the reranker of Collins and Koo (2005) used in conjunction with the feature set developed for English. Direct comparison is difficult as they used a different test set (approximately, the concatenation of our development and test sets). They report an F-score of 85.1 on sentences of length less than 40.5 However, we are directly comparable with Chrupała (2008)6 who adapted the Collins Model 2 to Spanish. As he was focusing on wide coverage LFG grammar induction, he enriched the non terminal annotation scheme with functional paths rather than trying to obtain the optimal tagset with respect to pure parsing performance. Nevertheless"
W12-3408,H05-1100,0,0.200659,"ce techniques. We rely on accurate data-driven lemmatization and partof-speech tagging to reduce data sparseness and ease the burden on the parser. We try to see how we can improve parsing structure predictions solely by modifying the terminals and/or the preterminals of the trees. We keep the rest of the tagset as is. In order to validate our method, we perform experiments on the Cast3LB constituent treebank for Spanish (Castillan). This corpus is quite small, around 3,500 trees, and Spanish is known to have a rich verbal morphology, making the tag set quite complex and difficult to predict. Cowan and Collins (2005) and Chrupała (2008) already showed interesting results on this corpus that will provide us with a comparison for this work, especially on the lexical aspects as they used lexicalized frameworks while we choose PCFG-LAs. This paper is structured as follows. In Section 2 we describe the Cast3LB corpus in details. In Section 3 we present our experimental setup and results which we discuss and compare in Section 4. Finally, Section 5 concludes the presentation. 2 Data Set The Castillan 3LB treebank (Civit and Martì, 2004) contains 3,509 constituent trees with functional annotations. It is divided"
W12-3408,Y09-1013,1,0.835017,"strategy dubbed simple lexicon in the Berkeley parser. Rare words – words occurring less than 3 times in the training set – are replaced by a special token, which depends on the OOV handling method (generic or IG), before collecting counts. POS tagging We performed parsing experiments with three different settings regarding POS information provided as an input to the parser: (i) with no POS information, which constitutes our baseline; (ii) with gold POS information, which can be considered as a topline for a given parser setting; (iii) with POS information predicted using the MElt POS-tagger (Denis and Sagot, 2009), using three different tagsets that we describe below. MElt is a state-of-the-art sequence labeller that is trained on both an annotated corpus and an external lexicon. The standard version of MElt relies on Maximum-Entropy Markov models (MEMMs). However, in this work, we have used a multiclass perceptron instead, as it allows for much faster training with very small performance drops (see Table 2). For training purposes, we used the training section of the Cast3LB (76,931 tokens) and the Leffe lexicon (Molinero et al., 2009), which contains almost 800,000 distinct (form, category) pairs.3 We"
W12-3408,P05-1010,0,0.0162841,"iments We conducted experiments on the Cast3LB development set in order to test various treebank modifications, that can be divided in two categories: (i) modification of the preterminal symbols of the treebank by using simplified POS tagsets; (ii) modification of the terminal symbols of the treebank by replacing word tokens by lemmas. 3.1 Experimental Setup In this section we describe the parsing formalism and POS tagging settings used in our experiments. PCFG-LAs To test our hypothesis, we use the grammatical formalism of Probabilistic ContextFree Grammars with Latent Annotations (PCFGLAs) (Matsuzaki et al., 2005; Petrov et al., 2006). These grammars depart from the standard PCFGs by automatically refining grammatical symbols during the training phase, using unsupervised techniques. They have been applied successfully to a wide range of languages, among which French (Candito and Seddah, 2010), German (Petrov and Klein, 2008), Chinese and Italian (Lavelli and Corazza, 2009). 56 For our experiments, we used the LORG PCFGLA parser implementing the CKY algorithm. This software also implements the techniques from Attia et al. (2010) for handling out-of-vocabulary words, where interesting suffixes for part-"
W12-3408,R09-1049,1,0.830632,"ng; (iii) with POS information predicted using the MElt POS-tagger (Denis and Sagot, 2009), using three different tagsets that we describe below. MElt is a state-of-the-art sequence labeller that is trained on both an annotated corpus and an external lexicon. The standard version of MElt relies on Maximum-Entropy Markov models (MEMMs). However, in this work, we have used a multiclass perceptron instead, as it allows for much faster training with very small performance drops (see Table 2). For training purposes, we used the training section of the Cast3LB (76,931 tokens) and the Leffe lexicon (Molinero et al., 2009), which contains almost 800,000 distinct (form, category) pairs.3 We performed experiments using three different 1 Names generic and IG originally come from Attia et al. (2010). 2 We tried to perform 4 and 5 rounds but 3 rounds proved to be optimal on this corpus. 3 Note that MElt does not use information from the exterTAGSET baseline reduced2 Nb. of tags 106 42 Multiclass Perceptron Overall Acc. 96.34 97.42 Unk. words Acc. 91.17 93.35 Maximum-Entropy Markov model (MEMM) Overall Acc. 96.46 97.42 Unk. words Acc. 91.57 93.76 reduced3 57 97.25 92.30 97.25 92.87 Table 2: MElt POS tagging accuracy"
W12-3408,W08-1005,0,0.0710606,"rd tokens by lemmas. 3.1 Experimental Setup In this section we describe the parsing formalism and POS tagging settings used in our experiments. PCFG-LAs To test our hypothesis, we use the grammatical formalism of Probabilistic ContextFree Grammars with Latent Annotations (PCFGLAs) (Matsuzaki et al., 2005; Petrov et al., 2006). These grammars depart from the standard PCFGs by automatically refining grammatical symbols during the training phase, using unsupervised techniques. They have been applied successfully to a wide range of languages, among which French (Candito and Seddah, 2010), German (Petrov and Klein, 2008), Chinese and Italian (Lavelli and Corazza, 2009). 56 For our experiments, we used the LORG PCFGLA parser implementing the CKY algorithm. This software also implements the techniques from Attia et al. (2010) for handling out-of-vocabulary words, where interesting suffixes for part-of-speech tagging are collected on the training set, ranked according to their information gain with regards to the partof-speech tagging task. Hence, all the experiments are presented in two settings. In the first one, called generic, unknown words are replaced with a dummy token UNK, while in the second one, dubbed"
W12-3408,P06-1055,0,0.111335,"riments on the Cast3LB development set in order to test various treebank modifications, that can be divided in two categories: (i) modification of the preterminal symbols of the treebank by using simplified POS tagsets; (ii) modification of the terminal symbols of the treebank by replacing word tokens by lemmas. 3.1 Experimental Setup In this section we describe the parsing formalism and POS tagging settings used in our experiments. PCFG-LAs To test our hypothesis, we use the grammatical formalism of Probabilistic ContextFree Grammars with Latent Annotations (PCFGLAs) (Matsuzaki et al., 2005; Petrov et al., 2006). These grammars depart from the standard PCFGs by automatically refining grammatical symbols during the training phase, using unsupervised techniques. They have been applied successfully to a wide range of languages, among which French (Candito and Seddah, 2010), German (Petrov and Klein, 2008), Chinese and Italian (Lavelli and Corazza, 2009). 56 For our experiments, we used the LORG PCFGLA parser implementing the CKY algorithm. This software also implements the techniques from Attia et al. (2010) for handling out-of-vocabulary words, where interesting suffixes for part-of-speech tagging are"
W12-3408,P81-1022,0,0.762867,"Missing"
W12-3408,N07-1051,0,\N,Missing
W12-5107,P85-1037,0,0.348154,"ach with its own definition which is examined separately. Hence, the hierarchy actually detects académie-6 → école-1 → établissement After a successful connection attempt, the pairs of unique senses immediately connected to each other (like académie-6 → école-1) are recorded and a frequentation counter associated with the sense pair is incremented. The result of the process allows us to tell which sense of école is expressed in the definition of académie that we considered. 2 Several studies proposed automatic or semi-automatic methods to develop lexical hierarchies from dictionary data, e.g. [2, 10]. 82 2 Resources 2.1 TLFi The Trésor de la Langue française informatisé (TLFi) [11] is the digital version of the Trésor de la Langue française, a large reference dictionary for French. The two main reasons why we have chosen the TLFi is that it is available in electronic form for research purpose and that most of its definitions belong to so-called definitions by genus and differentiæ allowing us to extract genus (or hypernym of the defined unit). The TLFi has also a wide coverage with around 270,000 definitions. This study is restricted to nouns, for which the TLFi provide 100,493 definition"
W12-5107,hanoka-sagot-2012-wordnet,1,0.806801,"om the Princeton WordNet (PWN) and various other resources [12]. Monosemous literals in the PWN 2.0 were translated using a bilingual French-English lexicon built from various multilingual resources. Polysemous PWN literals were handled by an alignment approach based on a multilingual parallel corpus. The synsets obtained from both approaches were then merged. The resulting resource, WOLF, preserves the hierarchy and structure of PWN 2.0 and contains the definitions and usage examples provided in PWN for each synset. Although new approaches are currently being used for increasing its coverage [5], WOLF is rather sparse, as information was not found for all PWN synsets by these automatic methods. Indeed, one of the difficulties in completing WOLF is to disambiguate the words contained in its synsets as to 4 http://alpage.inria.fr/~sagot/wolf.html 85 allow a correct translation, since the level of polysemy is high. In this work, we used the version 0.2.0 of the WOLF, in which 46,449 out of the 115,424 PWN 2.0 synsets are filled with at least one French literal. WOLF 0.2.0 contains 50,968 unique literals which take part in 86,235 (literal, synset) pairs, i.e., lexical entries (to be comp"
W12-5107,P11-4015,1,0.82167,"e Definiens project, TLFi definitions of nouns were POS-tagged and processed to determine the genus of a given definition, that is, the noun or noun phrase that corresponds to the hypernym of the defined noun [1]. The Definiens heuristic relies on lexicosyntactic patterns that recognise nouns or noun phrases as possible genus candidates. More precisely, around fifty rules have been manually elaborated to identify geni in the TLFi definitions. Represented as finite-state transducers, the rules have been run on definitions previously labeled with part of speech tags by the NLP tool suite MACAON [8]. The rule presented in figure 1 identifies nominal definitions that begin with a common noun (nc for nom commun in French), followed by a preposition and then another noun (left hand side of the rule). The right hand side of the rule proposes two possible geni for this kind definition: the first noun or a more specific phrasal genus constituted by the three elements (noun, preposition, noun) detected in the left hand side of the rule. This rule matches for example the definition of JODHPURS presented below since it begins with a noun (pantalon) followed by a preposition (de) followed by a nou"
W12-5107,E09-1068,0,0.0233717,"with a set of word sense pairs that TLFi puts in direct hypernymy relation. We can then use these pairs to populate WOLF: if two words w and W are deemed to have definitions d and D in direct hypernymy according to TLFi, and belong to synsets s and S in WOLF, these synsets also being in the hypernymy relation, then we can safely identify d to s and D to S. To disambiguate the hyponyms of an (h, H) pair, we explore the graph by hypernymic ascent: we consider the different senses h1 , . . . , hn that TLFi provides for h, and attempt to connect each of them to any of the senses of H. Inspired by [9], we propose a connection scheme whereby we jump from one word to a word of its definition, iteratively, until we reach the target H. In our implementation of the hypernymic ascent scheme, we select the genus of the 86 action action attaque assaut abordage abordage Figure 3: TLFi ambiguous structure (left) WOLF structure (right) definition of a word (that can also be considered as its hypernym) to carry on the next iteration step, taking advantage of the preprocessing performed in the Definiens project [1]. This process is illustrated in figure 3. In the left hand part of the figure, we have r"
W13-4402,I05-3017,0,0.315069,"of DL algorithm using this scoring method are presented in ﬁgure 4. As we shall see, this system can be further improved. We shall therefore refer to it as the base system. 5.1 Characters Tokens Types Table 1: Size of the diﬀerent corpora Figure 1: DL minimization 5 Words Tokens Types Rw = #correct words in the results #words in the gold corpus and the word precision Pw = Evaluation of the base system Reference corpora #correct words in the result , #words in the result which leads to the following: The evaluation presented here uses the corpora from the 2005 Chinese Word Segmentation Bakeoﬀ (Emerson, 2005). These corpora are available from the bakeoﬀ website and many previous works use them for evaluation, results are therefore easily comparable. This dataset also has the advantage of providing corpora that are segmented manually following four diﬀerent guidelines. Given the lack of consensus on the deﬁnition of the minimal segmentation unit, it is interesting to evaluate unsupervised systems against multiple guidelines and data sources: since an unsupervised system is not trained to mimic a speciﬁc guideline, its output may be closer to one or another. The dataset includes data from the Peking"
W13-4402,P11-2095,0,0.0178687,"2012): we rely on the notion of autonomy introduced by the latter and use it both for computing an initial segmentation and for guiding the MDL in a way inspired by the former. #wi log #wi N As shown for example by Zhikov et al. (2010), it is possible to decompose this formula to allow fast update of the DL value when we change the segmentation and avoid the total computation at each step of the minimization. MDL is often used in unsupervised segmentation systems, where it mostly plays one of the two following roles: (i) it can help selecting an optimal parameter value in an unsupervised way (Hewlett and Cohen, 2011), and (ii) it can drive the search for a more compact solution in the set of all possible segmentations. When an unsupervised segmentation model relies on parameters, one needs a way to assign them adequate values. In a fully unsupervised setup, we cannot make use of a manually segmented corpus to compute these values. Hewlett and Cohen (2011) address this issue by choosing the set of parameters that yields the segmentation associated with the smallest DL. They show that the output corresponding to the smallest DL almost always corresponds to the best segmentation in terms of word-based f-scor"
W13-4402,P06-2056,0,0.0192116,"iety of what may come next suddenly increase. From h→ (x0..n ) and h→ (x0..n−1 ) on the one hand, and from h← (x0..n ) and h← (x1..n ) on the other hand, we can deﬁned the Variation of Branching Entropy (VBE) in both directions: δh→ (x0..n ) = h→ (x0..n ) − h→ (x0..n−1 ) δh← (x0..n ) = h← (x0..n ) − h← (x1..n ). 2.3 Previous work on VBE-based segmentation Several unsupervised segmentation algorithms and systems in the literature are based on BE or VBE. Cohen et al. (2002) use BE as an indicator in their Voting Experts system. They point the need for normalisation but use BE directly, not VBE. Jin and Tanaka-Ishii (2006) propose a system for unsupervised Chinese word segmentation based on the VBE and evaluate it against a manually segmented corpus in Mandarin Chinese. Zhikov et al. (2010) use BE to get an initial segmentation. They put a boundary at each position that exceeds a threshold. This threshold is determined by an unsupervised procedure based on MDL. They reﬁne this initial segmentation using two diﬀerent procedures, also based on BE, which aim at minimizing the Description Length (see next section). Wang et al. (2011) propose ESA (Evaluation, Selection, and Adjustment), a more complex system combini"
W13-4402,W99-0702,0,0.0762589,"meter used to balance the two measures that can be diﬃcult to set without training data. In Magistry and Sagot (2012), we use a normalized VBE to deﬁne a measure of the autonomy of a string (word candidate). The autonomy of a word candidate x is deﬁned as a(x) = ˜ ← (x) + δh ˜ → (x) where δh(x) ˜ δh denotes VBE normalized in order to reduce the bias related to the variation of word lengths. This autonomy function is then used in a segmentation algorithm that maximize the autonomy of all the words in a sentence. The segmentation choosen for a given sentence s 2.2 Variation of Branching Entropy Kempe (1999) adapted the method proposed by Harris to corpus linguistics and did the switch from variation of AV to variation of BE (hereafter VBE) which is a better estimation of uncertainty. Branching Entropy (Right and Left) can be deﬁned as follows: given an n-gram x0..n = x0..1 x1..2 . . . xn−1..n with a left context χ→ , its Right Branching Entropy (RBE) h→ (x0..n ) writes as h→ (x0..n ) = H(χ→ |x0..n ) ∑ =− P (x |x0..n ) log P (x |x0..n ). x∈χ→ 3 is then chosen among all possible segmentations w ∈ Seg(s) as being arg max ∑ W ∈Seg(s) w ∈W i The content of the lexicon can be further encoded as a sequ"
W13-4402,P12-2075,1,0.711265,"hold. This threshold is determined by an unsupervised procedure based on MDL. They reﬁne this initial segmentation using two diﬀerent procedures, also based on BE, which aim at minimizing the Description Length (see next section). Wang et al. (2011) propose ESA (Evaluation, Selection, and Adjustment), a more complex system combining two measures of cohesion and noncohesion iteratively. The Branching Entropy is also at the root of their calculations. They achieve best published results but rely on a parameter used to balance the two measures that can be diﬃcult to set without training data. In Magistry and Sagot (2012), we use a normalized VBE to deﬁne a measure of the autonomy of a string (word candidate). The autonomy of a word candidate x is deﬁned as a(x) = ˜ ← (x) + δh ˜ → (x) where δh(x) ˜ δh denotes VBE normalized in order to reduce the bias related to the variation of word lengths. This autonomy function is then used in a segmentation algorithm that maximize the autonomy of all the words in a sentence. The segmentation choosen for a given sentence s 2.2 Variation of Branching Entropy Kempe (1999) adapted the method proposed by Harris to corpus linguistics and did the switch from variation of AV to v"
W13-4402,J11-3001,0,0.311962,"stem. They point the need for normalisation but use BE directly, not VBE. Jin and Tanaka-Ishii (2006) propose a system for unsupervised Chinese word segmentation based on the VBE and evaluate it against a manually segmented corpus in Mandarin Chinese. Zhikov et al. (2010) use BE to get an initial segmentation. They put a boundary at each position that exceeds a threshold. This threshold is determined by an unsupervised procedure based on MDL. They reﬁne this initial segmentation using two diﬀerent procedures, also based on BE, which aim at minimizing the Description Length (see next section). Wang et al. (2011) propose ESA (Evaluation, Selection, and Adjustment), a more complex system combining two measures of cohesion and noncohesion iteratively. The Branching Entropy is also at the root of their calculations. They achieve best published results but rely on a parameter used to balance the two measures that can be diﬃcult to set without training data. In Magistry and Sagot (2012), we use a normalized VBE to deﬁne a measure of the autonomy of a string (word candidate). The autonomy of a word candidate x is deﬁned as a(x) = ˜ ← (x) + δh ˜ → (x) where δh(x) ˜ δh denotes VBE normalized in order to reduc"
W13-4402,D10-1081,0,0.108285,"ion of Branching Entropy (VBE) in both directions: δh→ (x0..n ) = h→ (x0..n ) − h→ (x0..n−1 ) δh← (x0..n ) = h← (x0..n ) − h← (x1..n ). 2.3 Previous work on VBE-based segmentation Several unsupervised segmentation algorithms and systems in the literature are based on BE or VBE. Cohen et al. (2002) use BE as an indicator in their Voting Experts system. They point the need for normalisation but use BE directly, not VBE. Jin and Tanaka-Ishii (2006) propose a system for unsupervised Chinese word segmentation based on the VBE and evaluate it against a manually segmented corpus in Mandarin Chinese. Zhikov et al. (2010) use BE to get an initial segmentation. They put a boundary at each position that exceeds a threshold. This threshold is determined by an unsupervised procedure based on MDL. They reﬁne this initial segmentation using two diﬀerent procedures, also based on BE, which aim at minimizing the Description Length (see next section). Wang et al. (2011) propose ESA (Evaluation, Selection, and Adjustment), a more complex system combining two measures of cohesion and noncohesion iteratively. The Branching Entropy is also at the root of their calculations. They achieve best published results but rely on a"
W13-5306,N03-1017,0,0.00421059,"rm better than language-independent methods with an accurately chosen threshold. Cognate extraction by formal similarity (4.1.1) Training of the C-SMT model (4.1.2) Application of the C-SMT model, frequency and confidence filtering (4.1.3) Inferring word pairs with combined contextual and formal similarity (4.2.1) 2.2 Bilingual lexicon induction: hwRL , wNRL i pairs The principle underlying statistical machine translation (SMT) consists in learning alignments between pairs of words co-occurring in a parallel corpus. In phrase-based SMT, words may be grouped together to form so-called phrases (Koehn et al., 2003). Recently, a variant of this model has been proposed: character-based SMT, or henceforth C-SMT (Vilar et al., 2007; Tiedemann, 2009). In this paradigm, instead of aligning words (or word phrases) in a corpus consisting of sentences, one aligns characters (or segments of characters) in a corpus consisting of words. Of course, character alignments are well defined only for cognate pairs. Thus, it has been applied to translation between closely related languages (Vilar et al., 2007; Tiedemann, 2009) and to transliteration (Tiedemann and Nabende, 2009). Whereas in the existing C-SMT literature tr"
W13-5306,C04-1137,0,0.805659,"are regular. In closely related languages, cognates account for a large part of the lexicon. Mann and Yarowsky (2001) aim to detect cognate pairs in order to induce a translation lexicon. They evaluate different measures of phonetic or graphemic distance on this task. In particular, they distinguish static measures (independent of the language pair) from adaptive measures (adapted to the language pair by machine learning). Unsurprisingly, the authors observe better performances with the adaptive measures. However, they require a bilingual training corpus which we do not have at our disposal. Kondrak and Dorr (2004) present a large number of language-independent distance measures in order to predict whether two drug names are confusable or not. Among the graphemic measures (they also propose measures operating on phonetic transcriptions), the BI-SIM algorithm (see Section 4.1.1) yields the best results. Inkpen et al. (2005) apply these measures to the task of cognate identification in related languages (English– 2.3 Context similarity Exploiting context similarity is a promising approach for the induction of translation pairs from comparable corpora, whether the languages are closely related or not. The"
W13-5306,feldman-etal-2006-cross,0,0.466697,"706 431 884 456 197 515 193 3 451 532 2 252 337 Spanish Portuguese Transfer of morphosyntactic annotations Table 1: Wikipedia corpora The most straightforward idea for annotating a text from a non-resourced language consists in using a word-aligned parallel corpus, annotating the resourced side of it, and transferring the annotations to the aligned words in the other language. Yarowsky et al. (2001) successfully apply this approach to POS tagging, noun phrase chunking, named entity classification and even morphological analysis induction. Another approach to this problem has been proposed by Feldman et al. (2006). They train a tagger on the resourced language and apply it to the non-resourced language, after some modifications to the tagging model. Such a tagger is bound to have a high OOV rate, and Feldman et al. (2006) propose two strategies to reduce it. First, they use a basic morphological analyzer for the nonresourced language to predict potential tags. Second, they extract a list of cognate pairs in order to transfer tags from one language to the other. While this approach looks promising, we chose to avoid the manual creation of a morphological analyzer, thus keeping our approach fully automat"
W13-5306,N01-1020,0,0.20443,"ech tags (5.1) Tagging of non-tagged words by suffix analogy (5.2) Creation of morphological lexicon: hwNRL ,ti pairs Figure 1: Flowchart of the proposed approach. 2.1 Character-based statistical machine translation Cognate detection Hauer and Kondrak (2011) define cognates as words of different languages that share a common linguistic origin. Two words form a cognate pair if they are (1) phonetically or graphemically similar, (2) semantically similar, and (3) if the phonetic or graphemic similarities are regular. In closely related languages, cognates account for a large part of the lexicon. Mann and Yarowsky (2001) aim to detect cognate pairs in order to induce a translation lexicon. They evaluate different measures of phonetic or graphemic distance on this task. In particular, they distinguish static measures (independent of the language pair) from adaptive measures (adapted to the language pair by machine learning). Unsurprisingly, the authors observe better performances with the adaptive measures. However, they require a bilingual training corpus which we do not have at our disposal. Kondrak and Dorr (2004) present a large number of language-independent distance measures in order to predict whether t"
W13-5306,R11-1018,0,0.438587,"Missing"
W13-5306,J03-1002,0,0.00480808,"we consider the NRL as the source language and the RL as the target language. In particular, this allows us to match different wNRL with the same wRL and thus to take into account orthographic variation in the NRL. Such variation is less expected in the RL, which is assumed to have standardized spelling. Moreover, the classic SMT architecture puts the resource-intensive language model on the target language side, which is an additional argument in favour of the chosen translation direction. 4.1.2 Training of the C-SMT model Our C-SMT model relies on the standard pipeline consisting of GIZA++ (Och and Ney, 2003) for character alignment, IRSTLM (Federico et al., 2008) for language modelling, and Moses (Koehn et al., 2007) for phrase extraction and decoding. These tools may be configured in various ways; we have tested a large set of parameter configurations in preliminary experiments, but due to space restrictions, we just mention the parameter settings that we finally retained. • We add special symbols to the beginning and the end of each word. • We train a character 10-gram language model on the target language words. We removed words appearing less than 10 times in the corpus; each word is repeated"
W13-5306,P99-1067,0,0.467575,"of language-independent distance measures in order to predict whether two drug names are confusable or not. Among the graphemic measures (they also propose measures operating on phonetic transcriptions), the BI-SIM algorithm (see Section 4.1.1) yields the best results. Inkpen et al. (2005) apply these measures to the task of cognate identification in related languages (English– 2.3 Context similarity Exploiting context similarity is a promising approach for the induction of translation pairs from comparable corpora, whether the languages are closely related or not. The main idea (Fung, 1998; Rapp, 1999) is to extract word n-grams (or alternatively, bags of words) from both languages and induce word pairs that co-occur in the neighbourhood (context) of already known word pairs. For example, a French word appearing in the context of the word école is likely to be translated by an English word appearing in the context of the word school. This method requires a seed word lexicon (e.g., containing the pair hécole, schooli), as well as large corpora in both languages in order to build sufficiently large similarity vectors. Fišer and Ljubeši´c (2011) adapt this method to closely related languages:"
W13-5306,P01-1058,0,0.0377259,"data and etymological distance, making them a good testing ground for our methods. Moreover, we use subsets of varying size of Catalan–Spanish to assess the impact of the data size (see Table 1). We evaluate all five language pairs on the lexicon induction task on the basis of the dictionaries made available through the Apertium project (Forcada et al., 2011) (see Table 2). The Spanish tag dictionary is extracted from the AnCora-ES corpus (Taulé et al., 2008).1 It contains 42 part-of-speech tags and covers 40 148 words. The Portuguese tag dictionary is extracted from the CETEMPúblico corpus (Santos and Rocha, 2001).2 It contains 117 part-of-speech tags (of which 48 are combinations of two tags) and covers 107 235 words. Data Our approach relies on three types of data: 1. A raw text of the NRL. From this text we extract word lists for cognate induction, frequency information by word-type as well as morphosyntactic contexts. 2. A raw text of the RL, from which we extract the same information. 3. A tag dictionary which associates RL words with their part-of-speech tags. We extract this dictionary from an annotated RL corpus; note however that tag dictionaries may be obtained from other sources, in which ca"
W13-5306,taule-etal-2008-ancora,0,0.0349568,"ician–Spanish and Galician– Portuguese, using raw text extracted from the respective Wikipedias. These language pairs vary widely in terms of available raw data and etymological distance, making them a good testing ground for our methods. Moreover, we use subsets of varying size of Catalan–Spanish to assess the impact of the data size (see Table 1). We evaluate all five language pairs on the lexicon induction task on the basis of the dictionaries made available through the Apertium project (Forcada et al., 2011) (see Table 2). The Spanish tag dictionary is extracted from the AnCora-ES corpus (Taulé et al., 2008).1 It contains 42 part-of-speech tags and covers 40 148 words. The Portuguese tag dictionary is extracted from the CETEMPúblico corpus (Santos and Rocha, 2001).2 It contains 117 part-of-speech tags (of which 48 are combinations of two tags) and covers 107 235 words. Data Our approach relies on three types of data: 1. A raw text of the NRL. From this text we extract word lists for cognate induction, frequency information by word-type as well as morphosyntactic contexts. 2. A raw text of the RL, from which we extract the same information. 3. A tag dictionary which associates RL words with their"
W13-5306,I11-1097,0,0.0235239,"tion (Tiedemann and Nabende, 2009). Whereas in the existing C-SMT literature training data is extracted from parallel corpora, we propose to create a (noisy) training corpus from monolingual corpora using cognate detection. Inferring high-frequency word pairs with contextual similarity (4.2.2) Addition of formally identical word pairs (4.3) Transfer of part-of-speech tags (5.1) Tagging of non-tagged words by suffix analogy (5.2) Creation of morphological lexicon: hwNRL ,ti pairs Figure 1: Flowchart of the proposed approach. 2.1 Character-based statistical machine translation Cognate detection Hauer and Kondrak (2011) define cognates as words of different languages that share a common linguistic origin. Two words form a cognate pair if they are (1) phonetically or graphemically similar, (2) semantically similar, and (3) if the phonetic or graphemic similarities are regular. In closely related languages, cognates account for a large part of the lexicon. Mann and Yarowsky (2001) aim to detect cognate pairs in order to induce a translation lexicon. They evaluate different measures of phonetic or graphemic distance on this task. In particular, they distinguish static measures (independent of the language pair)"
W13-5306,2009.eamt-1.3,0,0.398913,"g of the C-SMT model (4.1.2) Application of the C-SMT model, frequency and confidence filtering (4.1.3) Inferring word pairs with combined contextual and formal similarity (4.2.1) 2.2 Bilingual lexicon induction: hwRL , wNRL i pairs The principle underlying statistical machine translation (SMT) consists in learning alignments between pairs of words co-occurring in a parallel corpus. In phrase-based SMT, words may be grouped together to form so-called phrases (Koehn et al., 2003). Recently, a variant of this model has been proposed: character-based SMT, or henceforth C-SMT (Vilar et al., 2007; Tiedemann, 2009). In this paradigm, instead of aligning words (or word phrases) in a corpus consisting of sentences, one aligns characters (or segments of characters) in a corpus consisting of words. Of course, character alignments are well defined only for cognate pairs. Thus, it has been applied to translation between closely related languages (Vilar et al., 2007; Tiedemann, 2009) and to transliteration (Tiedemann and Nabende, 2009). Whereas in the existing C-SMT literature training data is extracted from parallel corpora, we propose to create a (noisy) training corpus from monolingual corpora using cognate"
W13-5306,W02-0902,0,0.754796,"gible and demand for translation is low. In this paper, we present a generic approach for the transfer of part-of-speech (POS) annotations from a resourced language (RL) towards an etymologically closely related non-resourced language (NRL), without using any bilingual (i.e., parallel) data. We rely on two hypotheses. First, on the lexical level, the two languages share a lot of cognates, i.e., word pairs that are formally similar and that are translations of each other. Second, on the structural level, we admit that the word order of both languages is similar, and that the set 2 Related work Koehn and Knight (2002) propose various methods for inferring translation lexicons using only monolingual data. They consider several clues, including the identity or formal similarity of words (i.e., borrowings and cognates), similarity of the contexts of occurrence, and similarity of the frequency of words. They evaluate their method on English–German noun pairs. Our work is partly inspired by this paper, but uses different combinations of clues as well as updated methods and algorithms, and extends the task to POS tagging. We shall now describe in more detail the three major types of clues used in the literature."
W13-5306,W07-0705,0,0.289818,"rity (4.1.1) Training of the C-SMT model (4.1.2) Application of the C-SMT model, frequency and confidence filtering (4.1.3) Inferring word pairs with combined contextual and formal similarity (4.2.1) 2.2 Bilingual lexicon induction: hwRL , wNRL i pairs The principle underlying statistical machine translation (SMT) consists in learning alignments between pairs of words co-occurring in a parallel corpus. In phrase-based SMT, words may be grouped together to form so-called phrases (Koehn et al., 2003). Recently, a variant of this model has been proposed: character-based SMT, or henceforth C-SMT (Vilar et al., 2007; Tiedemann, 2009). In this paradigm, instead of aligning words (or word phrases) in a corpus consisting of sentences, one aligns characters (or segments of characters) in a corpus consisting of words. Of course, character alignments are well defined only for cognate pairs. Thus, it has been applied to translation between closely related languages (Vilar et al., 2007; Tiedemann, 2009) and to transliteration (Tiedemann and Nabende, 2009). Whereas in the existing C-SMT literature training data is extracted from parallel corpora, we propose to create a (noisy) training corpus from monolingual cor"
W13-5306,D11-1119,0,0.0997769,"Missing"
W13-5306,H01-1035,0,0.183135,"22 876 44 502 487 945 2 699 006 7 939 544 5 478 092 3 600 117 32 240 505 200 011 499 978 999 948 9 999 857 49 999 543 139 160 258 215 809 201 417 674 848 23 230 41 908 62 772 267 786 882 842 1 712 078 23 381 287 12 611 706 431 884 456 197 515 193 3 451 532 2 252 337 Spanish Portuguese Transfer of morphosyntactic annotations Table 1: Wikipedia corpora The most straightforward idea for annotating a text from a non-resourced language consists in using a word-aligned parallel corpus, annotating the resourced side of it, and transferring the annotations to the aligned words in the other language. Yarowsky et al. (2001) successfully apply this approach to POS tagging, noun phrase chunking, named entity classification and even morphological analysis induction. Another approach to this problem has been proposed by Feldman et al. (2006). They train a tagger on the resourced language and apply it to the non-resourced language, after some modifications to the tagging model. Such a tagger is bound to have a high OOV rate, and Feldman et al. (2006) propose two strategies to reduce it. First, they use a basic morphological analyzer for the nonresourced language to predict potential tags. Second, they extract a list"
W13-5306,P07-2045,0,\N,Missing
W14-0608,C90-2036,0,0.648111,"ANCE kata.gabor@inria.fr Benoˆıt Sagot INRIA & Universit´e Paris 7 Domaine de Voluceau - BP 105 78153 Le Chesnay Cedex FRANCE benoit.sagot@inria.fr Abstract errors and generate correction candidates. The technology is based on a symbolic linguistic preprocessing, followed by a statistical module which adpots the noisy channel model (Shannon, 1948). Symbolic methods for error correction allow to target specific phenomena with a high precision, but they typically strongly rely on presumptions about the nature of errors encountered. This drawback can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-exist"
W14-0608,H05-1109,0,0.0757593,"Missing"
W14-0608,P00-1037,0,0.0900857,"Benoˆıt Sagot INRIA & Universit´e Paris 7 Domaine de Voluceau - BP 105 78153 Le Chesnay Cedex FRANCE benoit.sagot@inria.fr Abstract errors and generate correction candidates. The technology is based on a symbolic linguistic preprocessing, followed by a statistical module which adpots the noisy channel model (Shannon, 1948). Symbolic methods for error correction allow to target specific phenomena with a high precision, but they typically strongly rely on presumptions about the nature of errors encountered. This drawback can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionari"
W14-0608,M98-1028,0,0.0463225,"trix calculated from our training corpus in which OCR output is aligned with its manually corrected, noiseless equivalent. The post-correction process is summarized in 1. The integration of a symbolic module for NE recognition and the use of part of speech and named entity tags constitute a novel aspect in our method. Moreover, linguistic preprocessing allows us to challenge tokenisation decisions prior 1 57 https://gforge.inria.fr/projects/lingwb/ since they often include punctuation marks, usually considered as separators. As compared to the consensual use of the term (Maynard et al., 2001; Chinchor, 1998; Sang and Meulder, 2003), our definition covers a wider range of entities, e.g., numerals, currency units, dimensions.2 The correct annotation of these entities has a double relevance for our project: given entity category, based on the presence of category-specific markers (lexical units, acronyms etc.)4 . However, chemical formulae were evaluated directly on sentences extracted from the archives of the European Patent Office; no filtering was needed due to the density of formulae in these documents. Legal IDs were evaluated on a legal corpus from the Publications Office of the European Unio"
W14-0608,W04-3238,0,0.0244357,"ork best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web (Cucerzan and Brill, 2004; Strohmaier et al., 2003) or from the corpus (Reynaert, 2004). The work reported in this paper aims at performance optimization in the digitization of documents pertaining to the cultural heritage domain. A hybrid method is proposed, combining statistical classification algorithms and linguistic knowledge to automatize post-OCR error detection and correction. The current paper deals with the integration of linguistic modules and their impact on error detection. 1 Introduction Providing wider access to national cultural heritage by massive digitization confronts the actors of the field with a"
W14-0608,2010.jeptalnrecital-long.3,1,0.790459,"ing phase (similarly to Kolak (2005)) ; this constitutes a significant feature as OCR errors often boil down to a fusion or split of tokens. System Architecture Our OCR error detection and correction system uses a hybrid methodology with a symbolic module for linguistic preprocessing, a POS tagger, followed by statistical decoding and correction modules. The SxPipe toolchain (Sagot and Boullier, 2008) is used for shallow processing tasks (tokenisation, sentence segmentation, named entity recognition). The NE-tagged text is input to POS tagging with MElt-h, a hybrid version of the MElt tagger (Denis and Sagot, 2010; Denis and Sagot, 2012). MELT-h can take both NE tagged texts and raw text as input. The decoding phase is based on the noisy channel model (Shannon, 1948) adapted to spell checking (Kernighan et al., 1990). In a noisy channel model, given an input string s, we want to find the word w which maximizes P (w|s). Using Bayes theorem, this can be written as: argmax(w)P (s|w) ∗ P (w) The corpus we use comes from the archives of the French National Library and contains 1 500 documents (50 000 000 tokens). This corpus is available both as a ”reference corpus”, i.e., in a manually corrected, clean ver"
W14-0608,W04-2216,0,0.0140529,"ch are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically create lexicons either by exploiting the Web (Cucerzan and Brill, 2004; Strohmaier et al., 2003) or from the corpus (Reynaert, 2004). The work reported in this paper aims at performance optimization in the digitization of documents pertaining to the cultural heritage domain. A hybrid method is proposed, combining statistical classification algorithms and linguistic knowledge to automatize post-OCR error detection and correction. The current paper deals with the integration of linguistic modules and their impact on error detection. 1 Introduction Providing wider access to national cultural heritage by massive digitization confronts the actors of the field with a set of new challenges. State of the art optical character reco"
W14-0608,sagot-2010-lefff,1,0.811179,"(similarly to Kolak (2005)) ; this constitutes a significant feature as OCR errors often boil down to a fusion or split of tokens. System Architecture Our OCR error detection and correction system uses a hybrid methodology with a symbolic module for linguistic preprocessing, a POS tagger, followed by statistical decoding and correction modules. The SxPipe toolchain (Sagot and Boullier, 2008) is used for shallow processing tasks (tokenisation, sentence segmentation, named entity recognition). The NE-tagged text is input to POS tagging with MElt-h, a hybrid version of the MElt tagger (Denis and Sagot, 2010; Denis and Sagot, 2012). MELT-h can take both NE tagged texts and raw text as input. The decoding phase is based on the noisy channel model (Shannon, 1948) adapted to spell checking (Kernighan et al., 1990). In a noisy channel model, given an input string s, we want to find the word w which maximizes P (w|s). Using Bayes theorem, this can be written as: argmax(w)P (s|w) ∗ P (w) The corpus we use comes from the archives of the French National Library and contains 1 500 documents (50 000 000 tokens). This corpus is available both as a ”reference corpus”, i.e., in a manually corrected, clean ver"
W14-0608,W03-0419,0,0.276064,"Missing"
W14-0608,P96-1010,0,0.14452,"fusion of digitized books and journals through emerging technologies such as e-books. Our paper deals with the automatic post-processing of digitized documents with the aim of reducing the OCR error rate by using contextual information and linguistic processing, by and large absent from current OCR engines. In the current stage of the project, we are focusing on French texts from the archives of the French National Library (Biblioth`eque Nationale de France) covering the period from 1646 to 1990. As to linguistically enhanced models, POS tagging was succesfully applied to spelling correction (Golding and Schabes, 1996; Schaback, 2007). However, to our knowledge, very little work has been done to exploit linguistic analysis for post-OCR correction (Francom and Hulden, 2013). We propose to apply a shallow processing module to detect certain types of named entities (NEs), and a POS tagger trained specifically to deal with NE-tagged input. Our studies aim to demonstrate that linguistic preprocessing can efficiently contribute to reduce the error rate by 1) detecting false corrections proposed by the We adopted a hybrid approach, making use of both statistical classification techniques and linguistically motiva"
W14-0608,W96-0108,0,0.453789,"P 105 78153 Le Chesnay Cedex FRANCE benoit.sagot@inria.fr Abstract errors and generate correction candidates. The technology is based on a symbolic linguistic preprocessing, followed by a statistical module which adpots the noisy channel model (Shannon, 1948). Symbolic methods for error correction allow to target specific phenomena with a high precision, but they typically strongly rely on presumptions about the nature of errors encountered. This drawback can be overcome by using the noisy channel model (Kernighan et al., 1990; Brill and Moore, 2000; Kolak and Resnik, 2002; Mays et al., 1991; Tong and Evans, 1996). However, error models in such systems work best if they are created from manually corrected training data, which are not always available. Other alternatives to OCR error correction include (weighted) FSTs (Beaufort and Mancas-Thillou, 2007), voting systems using the output of different OCR engines (Klein and Kope, 2002), textual alignment combined with dictionary lookup (Lund and Ringger, 2009), or heuristic correction methods (Alex et al., 2012). While correction systems rely less and less on pre-existing external dictionaries, a shift can be observed towards methods that dinamically creat"
W16-3905,W09-3821,0,0.0643458,"Missing"
W16-3905,C10-2013,0,0.0199812,"had to make bananas the main i i 20 predicate, and treat freeze as a subordinate clause. Regardless of the difficulty of the domain, it appears that a UD dependency analysis lends itself to dependency annotation in an easier way: Since non-leaf relations appear between lexical words, this representation is more robust to missing determiners, prepositions and punctuations, even phrases. Also, if we used other dependency formalisms that for instance place prepositions as heads of nouns, it would be more difficult to annotate as it is the case using the current French Treebank dependency scheme (Candito et al., 2010). Nevertheless, dependency analyses conflate functional and structural information (Silveira and Manning, 2015), and some of the structural information can be lost in cases such as the Example C, discussed above. Annotating dependencies lends itself well to noisy user-generated data. In a strict lexicalist analysis such as UD, where there are no tokens for unobserved words (e.g. dropped subjects, missing main verbs), we must build a structure from the existing words, and not from idealised sentence representations. We finally observe that for UGC, shorter sentences are harder to annotate. Inde"
W16-3905,N13-1037,0,0.0627008,"f the main causes behind the expectable performance drops in tagging and parsing. We therefore conduct a series of automatic and manual inspections to better understand the linguistic phenomena behind UGC linguistic variability. We explore the relation between predicting performance and annotation difficulty, which is seldom explicitly addressed (Plank et al., 2015). 4 A Threefold Categorisation for UGC Idiosyncrasies Even though user-generated content does not constitute a uniform genre, many works have characterised its idiosyncrasies (Foster, 2010; Gimpel et al., 2011; Seddah et al., 2012; Eisenstein, 2013), which can be characterised on three axes, defined by the intentionality or communication needs of the word variants: 1. Encoding simplification: This axis covers ergographic phenomena, i.e.,phenomena aiming at reducing the writing effort, perceived as first glance as genuine misspell errors, and transverse phe4 In fact, in the case of L EAGUE OF L EGENDS in-game data, the normalisation step adds a significant amount of noise. A solution to this problem and more generally to the limitations of deterministic rule-based normalisation lies in the development of non-supervised or semi-supervised"
W16-3905,P11-1118,0,0.0222664,"n. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training data (imperative usage, direct discourse, slang, etc.). This suboptimal parsing behavior on web data was in turn confirmed in follow-up works on Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL shared task, organised by Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Foster (2010) and Foster et al. (2011b) noted that simple lexical and tokenisation convention adaptation to the Wall-Street Journal text genre could increase the parsing performance by a large margin. In addition, Seddah et al. (2012) showed that a certain amount of normalisation brought a large improvement in POS tagger performance of French social media texts. These normalisation steps mostly apply at the lex"
W16-3905,W07-2204,1,0.86197,"Missing"
W16-3905,I11-1100,0,0.0964601,"Missing"
W16-3905,N10-1060,0,0.65778,"a, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training data (imperative usage, direct discourse, slang, etc.). This suboptimal parsing behavior on web data was in turn confirmed in follow-up works on Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL s"
W16-3905,W01-0521,0,0.0192116,"UGC, and (iii) the first corpus obtained from M INECRAFT and L EAGUE OF L EGENDS gaming logs. All corpora and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typo"
W16-3905,P11-2008,0,0.306203,"Missing"
W16-3905,C16-1286,0,0.0226449,"shorter sentences are harder to annotate. Indeed, sentences closer to the lower threshold of 4 tokens we have determined, seem to present more ellipsis, while longer sentences in our data have structures closer to more canonical syntax. 5.7 The Unspoken Costs of Treebank Annotation As we all know, creating annotated data is a rewarding task, extremely useful for evaluation as well as for building feature-rich supervised models. Yet, it is time consuming and as generally said, relatively costly (Schneider, 2015) even though crowd-sourcing solutions through games with a purpose start to emerge (Guillaume et al., 2016). The dataset we presented in that paper are part of process that was initiated 5 years ago when we were confronted to the lack of syntactically-annotated out-of-domain dataset for French. The purely syntactic annotation phase for the LoL and Minecraft data is still ongoing and we expect it to be finished in the first few months of 2017. It is important to consider that such a task was made possible because of the experience we gained along the years and because we relied on a highly trained team of annotators. This training was the most important point in term of costs and had to be extended"
W16-3905,D15-1157,0,0.166833,"Missing"
W16-3905,I05-1006,0,0.0412939,"gaming logs. All corpora and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training"
W16-3905,P08-2026,0,0.0138217,"and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences, bad segmentation, etc.) or syntactic structures absent from training data (imperative usage, direct"
W16-3905,P06-1043,0,0.0310279,"the first corpus obtained from M INECRAFT and L EAGUE OF L EGENDS gaming logs. All corpora and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences,"
W16-3905,N06-1020,0,0.0421604,"the first corpus obtained from M INECRAFT and L EAGUE OF L EGENDS gaming logs. All corpora and annotations are freely available. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 13 Proceedings of the 2nd Workshop on Noisy User-generated Text, pages 13–23, Osaka, Japan, December 11 2016. 2 Related Work Before the global availability of social-media feeds, studies on the difficulties of out-of-domain statistical parsing have been focusing mainly on slightly different newspaper texts (Gildea, 2001; McClosky et al., 2006b; McClosky et al., 2006a), biomedical data (Lease and Charniak, 2005; McClosky and Charniak, 2008) or balanced corpora mixing different genres (Foster et al., 2007). For such data, which is as edited as standard data sources, the problem is “simply” a matter of domain adaptation. It is far from being the case for UGC data, as shown by Foster (2010). Indeed, in her seminal work on parsing web data, different issues preventing reasonably good parsing performance were highlighted; most of them were tied to lexical differences (coming from either genuine unknown words, typographical divergences,"
W16-3905,C14-1166,0,0.075608,"without normalisation. The tagger (Denis and Sagot, 2012) was trained on the canonical training section of the French Treebank (Abeillé et al., 2003) instance, F TB - UC, from (Candito and Crabbé, 2009). We used an extended version of the rewriting rules used to pre-annotate the French Social Media Bank (Seddah et al., 2012). They work jointly with the tagger to provide internal cleaned versions of a token, or a sequence of, which are tagged separately. Resulting POS tags are finally merged to the original token(s). (e.g.wanna → want/VB to/TO → wanna/VB+TO). UGC tagging (Seddah et al., 2012; Nooralahzadeh et al., 2014): normalisation helps to cover some of the most frequent lexical variations and hence improves substantially the tagging accuracy. However in the case of in-game L EAGUE OF L EGENDS chat session, the normalisation is detrimental to the overall tagging performance as well as for unseen words. One obvious hypothesis is simply that the rules are applied deterministically and assign wrong PoSs while letting the pure tagging model work alone provide reasonable assumption on what would be the correct label for an out-of-domain word. Let us add that the MElt tagger makes a heavy use of features extra"
W16-3905,P14-2083,0,0.0176469,"Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Foster (2010) and Foster et al. (2011b) noted that simple lexical and tokenisation convention adaptation to the Wall-Street Journal text genre could increase the parsing performance by a large margin. In addition, Seddah et al. (2012) showed that a certain amount of normalisation brought a large improvement in POS tagger performance of French social media texts. These normalisation steps mostly apply at the lexical level, at the very definition of what constitutes a minimal unit. Plank et al. (2014) attempt to quantify how much of the domain-specific variation of POS labeling is a result of different interpretations, and how much is arguably just noise. Regarding the study of French UGC, our starting point is the part-of-speech and phrase-structure annotation guidelines by Seddah et al. (2012). However, we conduct our syntactic analysis in terms of dependency structures. 3 Data Collection and Part-of-Speech Annotation Our dataset contains three different sources of user-generated content. Two of them are logs of multiplayer video-game chat sessions, M INECRAFT and L EAGUE OF L EGENDS1 ,"
W16-3905,W15-1617,1,0.836342,"lf-training or other semi-supervised learning techniques. Therefore, the purpose of the present work is not only to potentially provide a new dataset to be used as additional training data for domain adaptation. Rather, we provide a close inspection of the main causes behind the expectable performance drops in tagging and parsing. We therefore conduct a series of automatic and manual inspections to better understand the linguistic phenomena behind UGC linguistic variability. We explore the relation between predicting performance and annotation difficulty, which is seldom explicitly addressed (Plank et al., 2015). 4 A Threefold Categorisation for UGC Idiosyncrasies Even though user-generated content does not constitute a uniform genre, many works have characterised its idiosyncrasies (Foster, 2010; Gimpel et al., 2011; Seddah et al., 2012; Eisenstein, 2013), which can be characterised on three axes, defined by the intentionality or communication needs of the word variants: 1. Encoding simplification: This axis covers ergographic phenomena, i.e.,phenomena aiming at reducing the writing effort, perceived as first glance as genuine misspell errors, and transverse phe4 In fact, in the case of L EAGUE OF L"
W16-3905,W15-1618,0,0.0197729,"e existing words, and not from idealised sentence representations. We finally observe that for UGC, shorter sentences are harder to annotate. Indeed, sentences closer to the lower threshold of 4 tokens we have determined, seem to present more ellipsis, while longer sentences in our data have structures closer to more canonical syntax. 5.7 The Unspoken Costs of Treebank Annotation As we all know, creating annotated data is a rewarding task, extremely useful for evaluation as well as for building feature-rich supervised models. Yet, it is time consuming and as generally said, relatively costly (Schneider, 2015) even though crowd-sourcing solutions through games with a purpose start to emerge (Guillaume et al., 2016). The dataset we presented in that paper are part of process that was initiated 5 years ago when we were confronted to the lack of syntactically-annotated out-of-domain dataset for French. The purely syntactic annotation phase for the LoL and Minecraft data is still ongoing and we expect it to be finished in the first few months of 2017. It is important to consider that such a task was made possible because of the experience we gained along the years and because we relied on a highly trai"
W16-3905,L16-1375,1,0.910238,"g point is the part-of-speech and phrase-structure annotation guidelines by Seddah et al. (2012). However, we conduct our syntactic analysis in terms of dependency structures. 3 Data Collection and Part-of-Speech Annotation Our dataset contains three different sources of user-generated content. Two of them are logs of multiplayer video-game chat sessions, M INECRAFT and L EAGUE OF L EGENDS1 , the last one is made of instant cooking-related web questions from M ARMITON2 , a widely popular French recipe website. This set of questions was collected during the building of the French QuestionBank (Seddah and Candito, 2016) but was not described nor analysed because of its syntactic peculiarity and was thus considered by the authors as a clear outlier. We chose to include this sample in our study because it offers a sharp contrast with video games chat logs in term of domain variation while retaining a live nature: users asks questions related to their immediate needs and expect a quick answer. The L EAGUE OF L EGENDS data set was collected by Lamy (2015) in early 2015 and consists of two types of recorded user interactions: a first part is the record of discussions occurring during an on-going game session whil"
W16-3905,C12-1149,1,0.559586,"urse, slang, etc.). This suboptimal parsing behavior on web data was in turn confirmed in follow-up works on Twitter and IRC chat (Foster et al., 2011a; Gimpel et al., 2010; Elsner and Charniak, 2011). They were again confirmed during the SANCL shared task, organised by Google, aimed at assessing the performances of parsers on various genres of Web texts (Petrov and McDonald, 2012). Foster (2010) and Foster et al. (2011b) noted that simple lexical and tokenisation convention adaptation to the Wall-Street Journal text genre could increase the parsing performance by a large margin. In addition, Seddah et al. (2012) showed that a certain amount of normalisation brought a large improvement in POS tagger performance of French social media texts. These normalisation steps mostly apply at the lexical level, at the very definition of what constitutes a minimal unit. Plank et al. (2014) attempt to quantify how much of the domain-specific variation of POS labeling is a result of different interpretations, and how much is arguably just noise. Regarding the study of French UGC, our starting point is the part-of-speech and phrase-structure annotation guidelines by Seddah et al. (2012). However, we conduct our synt"
W16-3905,W15-2134,0,0.0203327,"e difficulty of the domain, it appears that a UD dependency analysis lends itself to dependency annotation in an easier way: Since non-leaf relations appear between lexical words, this representation is more robust to missing determiners, prepositions and punctuations, even phrases. Also, if we used other dependency formalisms that for instance place prepositions as heads of nouns, it would be more difficult to annotate as it is the case using the current French Treebank dependency scheme (Candito et al., 2010). Nevertheless, dependency analyses conflate functional and structural information (Silveira and Manning, 2015), and some of the structural information can be lost in cases such as the Example C, discussed above. Annotating dependencies lends itself well to noisy user-generated data. In a strict lexicalist analysis such as UD, where there are no tokens for unobserved words (e.g. dropped subjects, missing main verbs), we must build a structure from the existing words, and not from idealised sentence representations. We finally observe that for UGC, shorter sentences are harder to annotate. Indeed, sentences closer to the lower threshold of 4 tokens we have determined, seem to present more ellipsis, whil"
W16-3905,L16-1262,0,\N,Missing
W17-0805,P13-1162,0,0.0779491,"Missing"
W17-0805,1999.tmi-1.13,0,0.312672,"ing, is ellipsis. In the case of an ellipsis, the deleted segment can be reconstructed given a discourse antecedent in the same document, be it observed or idealized (Asher et al., 2001; Merchant, 2016). In the case of omission, a reference and a target version of a statement are involved, the deleted segment in one version having an antecedent in the other version of the statement, in another document, as a result of editorial choices. Our task is similar to the problem of omission detection in translations, but the bilingual setting allows for word-alignment-based approaches (Melamed, 1996; Russell, 1999), which we cannot use in our setup. Omission detection is also related to hedge detection, which can be achieved using specific lexical triggers such as vagueness markers (Szarvas et al., 2012; Vincze, 2013). 3 something substantial, such as time, place, cause, people involved or important event information.” The OMp scheme aims to represent a naive user intuition of the relevance of a difference between statements, akin to the intuition of the users mentioned in Section 1, whereas OMe aims at capturing our intuition that relevant omissions relate to missing key news elements describable in te"
W17-0805,J12-2004,0,0.0429233,"Missing"
W17-0805,P05-1045,0,0.0150831,"appear in r, we generate the following feature sets: 1. Dice (Fa ): Dice coefficient between r and t. 2. Length (Fb ): The length of r, the length of t, and their difference. 3. BoW (Fc ): A bag of words (BoW) of M . 4. DWR (Fd ): A dense word representation is word-vector representation of M built from the average word vector for all words in M . We use the representations from GloVe (Pennington et al., 2014). 5. Stop proportion (Fe ): The proportion of stop words and punctuation in M . 6. Entities (Ff ): The number of entities in M predicted by the 4-class Stanford Named Entity Recognizer (Finkel et al., 2005). Table 3 shows the classification results. We use all exhaustive combinations of these feature sets to train a discriminative classifier, namely a logistic regression classifier, to obtain a best feature combination. We consider a feature combination to be the best when it outperforms the others in both accuracy and F1 for the Omission label. We compare all systems against the most frequent label (MFL) baseline. We evaluate each feature twice, namely using five-cold cross validation (CV-5 OMp ), and in a split scenario where we test on the 100 examples of OE after training with the remaining"
W17-0805,I13-1044,0,0.0146224,"he case of omission, a reference and a target version of a statement are involved, the deleted segment in one version having an antecedent in the other version of the statement, in another document, as a result of editorial choices. Our task is similar to the problem of omission detection in translations, but the bilingual setting allows for word-alignment-based approaches (Melamed, 1996; Russell, 1999), which we cannot use in our setup. Omission detection is also related to hedge detection, which can be achieved using specific lexical triggers such as vagueness markers (Szarvas et al., 2012; Vincze, 2013). 3 something substantial, such as time, place, cause, people involved or important event information.” The OMp scheme aims to represent a naive user intuition of the relevance of a difference between statements, akin to the intuition of the users mentioned in Section 1, whereas OMe aims at capturing our intuition that relevant omissions relate to missing key news elements describable in terms of the 5-W questions (Parton et al., 2009; Das et al., 2012). We ran AMT task twice, once for each scheme. For each scheme, we assigned 5 turkers per instance, and we required that the annotators be Cate"
W17-0805,N13-1132,0,0.0771197,"Missing"
W17-0805,P13-2011,0,0.0332052,"Missing"
W17-0805,C96-2129,0,0.0872199,"different setting, is ellipsis. In the case of an ellipsis, the deleted segment can be reconstructed given a discourse antecedent in the same document, be it observed or idealized (Asher et al., 2001; Merchant, 2016). In the case of omission, a reference and a target version of a statement are involved, the deleted segment in one version having an antecedent in the other version of the statement, in another document, as a result of editorial choices. Our task is similar to the problem of omission detection in translations, but the bilingual setting allows for word-alignment-based approaches (Melamed, 1996; Russell, 1999), which we cannot use in our setup. Omission detection is also related to hedge detection, which can be achieved using specific lexical triggers such as vagueness markers (Szarvas et al., 2012; Vincze, 2013). 3 something substantial, such as time, place, cause, people involved or important event information.” The OMp scheme aims to represent a naive user intuition of the relevance of a difference between statements, akin to the intuition of the users mentioned in Section 1, whereas OMe aims at capturing our intuition that relevant omissions relate to missing key news elements d"
W17-0805,P09-1048,0,0.0494486,"Missing"
W17-0805,Q14-1025,0,0.0211382,"used two different annotation schemes, namely OMp , where the option to mark an omission is “Text B leaves out some substantial information”, and OMe , where it is “Text B leaves out Figure 1: Annotation scheme for OMp Annotation results The first column in Table 2 shows the agreement of the annotation tasks in terms of Krippendorff’s α coefficient. A score of e.g. 0.52 is not a very high value, but is well within what can be expected on crowdsourced semantic annotations. Note, however, the chance correction that the calculation of α applies to a skewed binary distribution is very aggressive (Passonneau and Carpenter, 2014). The conservativeness of the chance-corrected coefficient can be assessed if we compare the raw agreement between experts (0.86) with the α of 0.67. OMe causes agreement to descend slightly, and damages the agreement of Same, while Omission remains largely constant. Moreover, disagreement is not evenly distributed across annotated instances, i.e. some instances show perfect agreement, while other instances have maximal disagreement. We also measured the median annotation time per instance for all three methods; OMe is almost twice as slow as OMp (42s vs. 22s), while 3 The full distribution of"
W17-0805,D14-1162,0,0.0835688,"ined by simple proxy linguistic properties like word overlap or length of the statements and/or lexical properties. Features: For a reference statement r, a target statement t and a set M of the words that only appear in r, we generate the following feature sets: 1. Dice (Fa ): Dice coefficient between r and t. 2. Length (Fb ): The length of r, the length of t, and their difference. 3. BoW (Fc ): A bag of words (BoW) of M . 4. DWR (Fd ): A dense word representation is word-vector representation of M built from the average word vector for all words in M . We use the representations from GloVe (Pennington et al., 2014). 5. Stop proportion (Fe ): The proportion of stop words and punctuation in M . 6. Entities (Ff ): The number of entities in M predicted by the 4-class Stanford Named Entity Recognizer (Finkel et al., 2005). Table 3 shows the classification results. We use all exhaustive combinations of these feature sets to train a discriminative classifier, namely a logistic regression classifier, to obtain a best feature combination. We consider a feature combination to be the best when it outperforms the others in both accuracy and F1 for the Omission label. We compare all systems against the most frequent"
W17-2212,W10-1807,1,0.798059,"yet mainly manually and at enormous costs. Relying solely on manual annotators is too costly an option. Minimising resource development costs is crucial. Manual language resource development for language documentation and language acquisition projects should be sped up, as soon as technically possible, by employing NLP tools such as spelling correction/normalisation tools and part-of-speech (POS) taggers. For instance, POS taggers used as pre-annotators have been shown to increase both annotation speed and annotation quality, even when trained on limited amounts of data (Marcus et al., 1993; Fort and Sagot, 2010). Yet language acquisition and language documentation data presents specific challenges for automatic linguistic annotation. Firstly, such data usually consists of transcriptions of spontaneous speech. Secondly, previously undescribed languages are often not written and lack established orthographies, resulting in noisy transcriptions. Thirdly, acquisition data consists of recordings of child-parent interactions. The recorded target children’s language production can differ dramatically from adult language, adding another layer of linguistic variation. Finally, as new data is usually still bei"
W17-2212,J93-2004,0,0.0607177,"ct (McWhinney, 2000), yet mainly manually and at enormous costs. Relying solely on manual annotators is too costly an option. Minimising resource development costs is crucial. Manual language resource development for language documentation and language acquisition projects should be sped up, as soon as technically possible, by employing NLP tools such as spelling correction/normalisation tools and part-of-speech (POS) taggers. For instance, POS taggers used as pre-annotators have been shown to increase both annotation speed and annotation quality, even when trained on limited amounts of data (Marcus et al., 1993; Fort and Sagot, 2010). Yet language acquisition and language documentation data presents specific challenges for automatic linguistic annotation. Firstly, such data usually consists of transcriptions of spontaneous speech. Secondly, previously undescribed languages are often not written and lack established orthographies, resulting in noisy transcriptions. Thirdly, acquisition data consists of recordings of child-parent interactions. The recorded target children’s language production can differ dramatically from adult language, adding another layer of linguistic variation. Finally, as new da"
W17-6304,W13-3520,0,0.0410365,"over the UD1.3 corpora. “MA” stands for morphological-analyser-based lexicon. Lexicons based on Apertium and Giellatekno data are in their coarse version unless full is indicated. Other lexicons have been adapted from available resources.1 We also provide the type-token ratio of the corpus (TTR) and whether there were available Polyglot embeddings (PG) to initialize w. ~ in which labels are the concatenation of the Universal PoS and Universal Features. Pre-computed embeddings Whenever available and following Plank et al. (2016), we performed experiments using Polyglot pre-computed embeddings (Al-Rfou et al., 2013). Languages for which Polyglot embeddings are available are indicated in Table 1. We also took advantage of other existing lexicons. For space reasons, we are not able to describe here the language-specific transformations we applied to some of these lexicons. See Table 1 and its caption for more information. We determine the best performing lexicon for each language based on tagging accuracy on the development set. In the remainder of this paper, all information about the lexicons (Table 1) and accuracy results are restricted to these best performing lexicons. We trained our tagger with and w"
W17-6304,erjavec-2010-multext,0,0.0545621,"e-art bi-LSTM PoS tagger bilty, a freely available6 and “significantly refactored version of the code originally used” by Plank et al. (2016). We use its standard configuration, with one bi-LSTM layer, characterbased embeddings size of 100, word embedding size of 64 (same as Polyglot embeddings), no multitask learning,7 and 20 iterations for training. We extended bilty for enabling integration of lexical morphosyntactic information, in the way described in the previous section. 5 Bouma et al., 2000; Oliver and Tadi´c, 2004; Heslin, 2007; Borin et al., 2008; Molinero et al., 2009; Sagot, 2010; Erjavec, 2010; Sagot and Walther, 2010; Mˇechura, 2014; Sagot, 2014. 6 https://github.com/bplank/bilstm-aux 7 Plank et al.’s (2016) secondary task—predicting the frequency class of each word—results in better OOV scores but virtually identical overall scores when averaged over all tested languages/corpora. 8 Note that we discarded alternative UD 1.3 corpora (e.g. nl lassysmall vs. nl), as well as corpora for languages for which we had neither a lexicon nor Polyglot embeddings (Old Church Slavonic, Hungarian, Gothic, Tamil). 28 w(P ~ ) +~ c + ~l lang a lexicon are those with smallest datasets. ∆ w.r.t.w(P ~"
W17-6304,D15-1041,0,0.0274164,"information. First, morphosyntactic lexicons provide a large inventory of (word, PoS) pairs. Such lexical information can be used in the form of constraints at tagging time (Kim et al., 1999; Hajiˇc, 2000) or during the training process as additional features combined with standard features extracted from the training corpus (Chrupała et al., 2008; Goldberg et al., 2009; Denis and Sagot, 2012). Second, lexical information encoded in vector representations, known as word embeddings, have emerged more recently (Bengio et al., 2003; Collobert and Weston, 2008; Chrupała, 2013; Ling et al., 2015; Ballesteros et al., 2015; M¨uller and Sch¨utze, 2015). Such representations, often extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono- or bi-directional, word-level or characterlevel long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016). Character-level embeddings are of particular interest for PoS tagging as they generate vector representations that result from the internal characterleve"
W17-6304,E09-1038,0,0.0301809,"ce {benoit.sagot,hector.martinez-alonso}@inria.fr Abstract curacy levels, improving over the state of the art in a number of settings (Plank et al., 2016). As a complement to annotated training corpora, external lexicons can be a valuable source of information. First, morphosyntactic lexicons provide a large inventory of (word, PoS) pairs. Such lexical information can be used in the form of constraints at tagging time (Kim et al., 1999; Hajiˇc, 2000) or during the training process as additional features combined with standard features extracted from the training corpus (Chrupała et al., 2008; Goldberg et al., 2009; Denis and Sagot, 2012). Second, lexical information encoded in vector representations, known as word embeddings, have emerged more recently (Bengio et al., 2003; Collobert and Weston, 2008; Chrupała, 2013; Ling et al., 2015; Ballesteros et al., 2015; M¨uller and Sch¨utze, 2015). Such representations, often extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono- or bi-directional, word-level or characterlevel long short-term memory networks (LSTMs) (Ho"
W17-6304,A00-2013,0,0.296612,"Missing"
W17-6304,W99-0615,0,0.0414295,"Missing"
W17-6304,D15-1176,0,0.102226,"Missing"
W17-6304,A00-1031,0,0.670113,"Missing"
W17-6304,P95-1037,0,0.0868754,"to associate each “word” with a morphosyntactic tag, whose granularity can range from a simple morphosyntactic category, or part-of-speech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, person, etc.). The use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers. A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Constant and Tellier, 2012). Recently, neural approaches have reached very competitive ac25 Proceedings of the 15th International Conference on Parsing Technologies, pages 25–31, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics vantages of using character-level embeddings and external lexical information is an interesting idea to follow. However, the inclusion of morphosyntactic information from lexicons into neural PoS tagging architectur"
W17-6304,chrupala-etal-2008-learning,0,0.0677445,"Missing"
W17-6304,J94-2001,0,0.377092,"ng is now a classic task in natural language processing. Its aim is to associate each “word” with a morphosyntactic tag, whose granularity can range from a simple morphosyntactic category, or part-of-speech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, person, etc.). The use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers. A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Constant and Tellier, 2012). Recently, neural approaches have reached very competitive ac25 Proceedings of the 15th International Conference on Parsing Technologies, pages 25–31, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics vantages of using character-level embeddings and external lexical information is an interesting idea to follow. However, the inclusion of morphosyn"
W17-6304,R09-1049,1,0.847833,"We use as a baseline the state-of-the-art bi-LSTM PoS tagger bilty, a freely available6 and “significantly refactored version of the code originally used” by Plank et al. (2016). We use its standard configuration, with one bi-LSTM layer, characterbased embeddings size of 100, word embedding size of 64 (same as Polyglot embeddings), no multitask learning,7 and 20 iterations for training. We extended bilty for enabling integration of lexical morphosyntactic information, in the way described in the previous section. 5 Bouma et al., 2000; Oliver and Tadi´c, 2004; Heslin, 2007; Borin et al., 2008; Molinero et al., 2009; Sagot, 2010; Erjavec, 2010; Sagot and Walther, 2010; Mˇechura, 2014; Sagot, 2014. 6 https://github.com/bplank/bilstm-aux 7 Plank et al.’s (2016) secondary task—predicting the frequency class of each word—results in better OOV scores but virtually identical overall scores when averaged over all tested languages/corpora. 8 Note that we discarded alternative UD 1.3 corpora (e.g. nl lassysmall vs. nl), as well as corpora for languages for which we had neither a lexicon nor Polyglot embeddings (Old Church Slavonic, Hungarian, Gothic, Tamil). 28 w(P ~ ) +~ c + ~l lang a lexicon are those with smal"
W17-6304,constant-tellier-2012-evaluating,0,0.0203217,"ech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, person, etc.). The use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers. A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Constant and Tellier, 2012). Recently, neural approaches have reached very competitive ac25 Proceedings of the 15th International Conference on Parsing Technologies, pages 25–31, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics vantages of using character-level embeddings and external lexical information is an interesting idea to follow. However, the inclusion of morphosyntactic information from lexicons into neural PoS tagging architecture, as a replacement or complement to character-based or pre-computed word embeddings, remains to be investigated. In this paper, we describe how suc"
W17-6304,N15-1055,0,0.040058,"Missing"
W17-6304,W96-0213,0,0.749519,"ose granularity can range from a simple morphosyntactic category, or part-of-speech (hereafter PoS), to finer categories enriched with morphological features (gender, number, case, tense, mood, person, etc.). The use of machine learning algorithms trained on manually annotated corpora has long become the standard way to develop PoS taggers. A large variety of algorithms have been used, such as (in approximative chronological order) bigram and trigram hidden Markov models (Merialdo, 1994; Brants, 1996, 2000), decision trees (Schmid, 1994; Magerman, 1995), maximum entropy Markov models (MEMMs) (Ratnaparkhi, 1996) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; Constant and Tellier, 2012). Recently, neural approaches have reached very competitive ac25 Proceedings of the 15th International Conference on Parsing Technologies, pages 25–31, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics vantages of using character-level embeddings and external lexical information is an interesting idea to follow. However, the inclusion of morphosyntactic information from lexicons into neural PoS tagging architecture, as a replacement or complement to character-based or pre"
W17-6304,W14-4607,0,0.614965,"Missing"
W17-6304,sagot-2010-lefff,1,0.904736,"e state-of-the-art bi-LSTM PoS tagger bilty, a freely available6 and “significantly refactored version of the code originally used” by Plank et al. (2016). We use its standard configuration, with one bi-LSTM layer, characterbased embeddings size of 100, word embedding size of 64 (same as Polyglot embeddings), no multitask learning,7 and 20 iterations for training. We extended bilty for enabling integration of lexical morphosyntactic information, in the way described in the previous section. 5 Bouma et al., 2000; Oliver and Tadi´c, 2004; Heslin, 2007; Borin et al., 2008; Molinero et al., 2009; Sagot, 2010; Erjavec, 2010; Sagot and Walther, 2010; Mˇechura, 2014; Sagot, 2014. 6 https://github.com/bplank/bilstm-aux 7 Plank et al.’s (2016) secondary task—predicting the frequency class of each word—results in better OOV scores but virtually identical overall scores when averaged over all tested languages/corpora. 8 Note that we discarded alternative UD 1.3 corpora (e.g. nl lassysmall vs. nl), as well as corpora for languages for which we had neither a lexicon nor Polyglot embeddings (Old Church Slavonic, Hungarian, Gothic, Tamil). 28 w(P ~ ) +~ c + ~l lang a lexicon are those with smallest datasets"
W17-6304,sagot-2014-delex,1,0.845311,"“significantly refactored version of the code originally used” by Plank et al. (2016). We use its standard configuration, with one bi-LSTM layer, characterbased embeddings size of 100, word embedding size of 64 (same as Polyglot embeddings), no multitask learning,7 and 20 iterations for training. We extended bilty for enabling integration of lexical morphosyntactic information, in the way described in the previous section. 5 Bouma et al., 2000; Oliver and Tadi´c, 2004; Heslin, 2007; Borin et al., 2008; Molinero et al., 2009; Sagot, 2010; Erjavec, 2010; Sagot and Walther, 2010; Mˇechura, 2014; Sagot, 2014. 6 https://github.com/bplank/bilstm-aux 7 Plank et al.’s (2016) secondary task—predicting the frequency class of each word—results in better OOV scores but virtually identical overall scores when averaged over all tested languages/corpora. 8 Note that we discarded alternative UD 1.3 corpora (e.g. nl lassysmall vs. nl), as well as corpora for languages for which we had neither a lexicon nor Polyglot embeddings (Old Church Slavonic, Hungarian, Gothic, Tamil). 28 w(P ~ ) +~ c + ~l lang a lexicon are those with smallest datasets. ∆ w.r.t.w(P ~ ) +~ c OOTC OOTC in Lex. OOTC OOTC in Lex. ar bg ca c"
W17-6304,sagot-walther-2010-morphological,1,0.897189,"oS tagger bilty, a freely available6 and “significantly refactored version of the code originally used” by Plank et al. (2016). We use its standard configuration, with one bi-LSTM layer, characterbased embeddings size of 100, word embedding size of 64 (same as Polyglot embeddings), no multitask learning,7 and 20 iterations for training. We extended bilty for enabling integration of lexical morphosyntactic information, in the way described in the previous section. 5 Bouma et al., 2000; Oliver and Tadi´c, 2004; Heslin, 2007; Borin et al., 2008; Molinero et al., 2009; Sagot, 2010; Erjavec, 2010; Sagot and Walther, 2010; Mˇechura, 2014; Sagot, 2014. 6 https://github.com/bplank/bilstm-aux 7 Plank et al.’s (2016) secondary task—predicting the frequency class of each word—results in better OOV scores but virtually identical overall scores when averaged over all tested languages/corpora. 8 Note that we discarded alternative UD 1.3 corpora (e.g. nl lassysmall vs. nl), as well as corpora for languages for which we had neither a lexicon nor Polyglot embeddings (Old Church Slavonic, Hungarian, Gothic, Tamil). 28 w(P ~ ) +~ c + ~l lang a lexicon are those with smallest datasets. ∆ w.r.t.w(P ~ ) +~ c OOTC OOTC in Lex."
W17-6304,oliver-tadic-2004-enlarging,0,0.60392,"Missing"
W17-6304,P16-2067,0,0.379799,"sentations, known as word embeddings, have emerged more recently (Bengio et al., 2003; Collobert and Weston, 2008; Chrupała, 2013; Ling et al., 2015; Ballesteros et al., 2015; M¨uller and Sch¨utze, 2015). Such representations, often extracted from large amounts of raw text, have proved very useful for numerous tasks including PoS tagging, in particular when used in recurrent neural networks (RNNs) and more specifically in mono- or bi-directional, word-level or characterlevel long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997; Ling et al., 2015; Ballesteros et al., 2015; Plank et al., 2016). Character-level embeddings are of particular interest for PoS tagging as they generate vector representations that result from the internal characterlevel make-up of each word. It can generalise over relevant sub-parts such as prefixes or suffixes, thus directly addressing the problem of unknown words. However, unknown words do not always follow such generalisations. In such cases, character-level models cannot bring any advantage. This is a difference with external lexicons, which provides information about any word it contains, yet without any quantitative distinction between relevant and"
W18-7005,S15-2017,0,0.0685004,"Missing"
W18-7005,L18-1008,0,0.0395658,"s... • Metrics based on the baseline Q U E ST features (17 features) (Specia et al., 2013), such as statistics on the number of words, word lengths, language model probability and ngram frequency. Features In our experiments, we compared about 60 elementary metrics, which can be organised as follows: • Metrics based on other features: frequency table position, concreteness as extracted from Brysbaert et al.’s 2014 list, language model probability of words using a convolutional sequence to sequence model from (Gehring et al., 2017), comparison methods using pretrained fastText word embeddings (Mikolov et al., 2018) or Skip-thought sentence embeddings (Kiros et al., 2015). • MT metrics – BLEU, ROUGE, METEOR, TERp – Variants of BLEU: BLEU 1gram, BLEU 3gram, BLEU 2gram, BLEU 4gram and seven smoothing methods5 from NLTK (Bird and Loper, 2004). – Intermediate components of TERp inˇ spired by (Stajner et al., 2016a): e.g. number of insertions, deletions, shifts... TABLE 2 lists 30 of the elementary metrics that we compared, which are those that we found to correlate the most with human judgments on one or more of the three dimensions (grammaticality, meaning preservation, simplicity). 3.3 4 http://qats2016.gi"
W18-7005,P04-3031,0,0.120063,"compared about 60 elementary metrics, which can be organised as follows: • Metrics based on other features: frequency table position, concreteness as extracted from Brysbaert et al.’s 2014 list, language model probability of words using a convolutional sequence to sequence model from (Gehring et al., 2017), comparison methods using pretrained fastText word embeddings (Mikolov et al., 2018) or Skip-thought sentence embeddings (Kiros et al., 2015). • MT metrics – BLEU, ROUGE, METEOR, TERp – Variants of BLEU: BLEU 1gram, BLEU 3gram, BLEU 2gram, BLEU 4gram and seven smoothing methods5 from NLTK (Bird and Loper, 2004). – Intermediate components of TERp inˇ spired by (Stajner et al., 2016a): e.g. number of insertions, deletions, shifts... TABLE 2 lists 30 of the elementary metrics that we compared, which are those that we found to correlate the most with human judgments on one or more of the three dimensions (grammaticality, meaning preservation, simplicity). 3.3 4 http://qats2016.github.io/shared.html https://www.nltk.org/api/nltk. translate.html#nltk.translate.bleu_ score.SmoothingFunction 5 Experimental setup Evaluation of elementary metrics We rank all features by comparing their behaviour with human 32"
W18-7005,P14-1041,0,0.472366,"Missing"
W18-7005,W14-1206,0,0.0550454,"Missing"
W18-7005,D17-1064,0,0.0138618,"aluate the structural simplicity of a TS system output given the corresponding source sentence. SAMSA is maximized when the simplified text is a sequence of short and simple sentences, each accounting for one semantic event in the original sentence. It relies on an in-depth analysis of the source sentence and the corresponding output, based on a semantic parser and a word aligner. A drawback of this approach is that good quality semantic parsers are only available for a handful of languages. The intuition that sentence splitting is an important sub-task for producing simplified text motivated Narayan et al. (2017) to organize the Split and Rephrase shared task, which was dedicated to this problem. 2.3 Figure 1: Label repartition on the QATS Shared task this end, we use the dataset provided in the QATS shared task. We first compare the behaviour of elementary metrics, which range from commonly used metrics such as BLEU to basic metrics based on a single low-level feature such as sentence length. We then compare the effect of aggregating these elementary metrics into more complex ones and compare our results with the state of the art, based on the QATS shared task data and results. Other metrics One can"
W18-7005,W15-1604,0,0.170014,"Missing"
W18-7005,P02-1040,0,0.106586,"city in this context. Previous works often rely on the intuition of human annotators to evaluate the level of simplicity of a TS system output. 29 Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA), pages 29–38, c Tilburg, The Netherlands, November 8 2018. 2018 Association for Computational Linguistics evaluation methods and traditional quality estimation features. We then present the QATS shared task and the associated dataset, which we use for our experiments. Finally we compare all methods in a reference-less setting and analyze the results. evaluation metrics such as BLEU (Papineni et al., 2002). However, MT evaluation metrics rely on the existence of parallel corpora of source sentences and manually produced reference translations, which are available on a large scale for many language pairs (Tiedemann, 2012). TS datasets are less numerous and smaller. Moreover, they are often automatically extracted from comparable corpora rather than strictly parallel corpora, which results in noisier reference data. For example, the PWKP dataset (Zhu et al., 2010) consists of 100,000 sentences from the English Wikipedia automatically aligned with sentences from the Simple English Wikipedia based"
W18-7005,C96-2183,0,0.67445,"er version of a source text that is both easier to read and to understand, thus improving the accessibility of text for people suffering from a range of disabilities such as aphasia (Carroll et al., 1998) or dyslexia (Rello et al., 2013), as well as for second language learners (Xia et al., 2016) and people with low literacy (Watanabe et al., 2009). This topic has been researched for a variety of languages such as English (Zhu et al., 2010; Wubben 1 Note that text simplification has also been used as a preprocessing step for other natural language processing tasks such as machine translation (Chandrasekar et al., 1996) and semantic role labelling (Vickrey and Koller, 2008). 2 There is no unique way to define the notion of simplicity in this context. Previous works often rely on the intuition of human annotators to evaluate the level of simplicity of a TS system output. 29 Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA), pages 29–38, c Tilburg, The Netherlands, November 8 2018. 2018 Association for Computational Linguistics evaluation methods and traditional quality estimation features. We then present the QATS shared task and the associated dataset, which we use for our experiments. Final"
W18-7005,W15-5710,0,0.0361497,"cs One can also estimate the quality of a TS system output based on simple features extracted from it. For instance, the Q U E ST framework for quality estimation in MT gives a number of useful baseline features for evaluating an output sentence (Specia et al., 2013). These features range from simple statistics, such as the number of words in the sentence, to more sophisticated features, such as the probability of the sentence according to a language model. Several teams who participated in the QATS shared task used metrics ˇ based on this framework, namely SMH (Stajner et al., 2016a), UoLGP (Rios and Sharoff, 2015) and UoW (B´echara et al., 2015). Readability metrics such as Flesch-Kincaid Grade Level (FKGL) and Flesch Reading Ease (FRE) (Kincaid et al., 1975) have been extensively used for evaluating simplicity. These two metrics, which were shown experimentally to give good results, are linear combinations of the number of words per sentence and the number of syllables per word, using carefully adjusted weights. 3 3.1 The QATS shared task ˇ The data from the QATS shared task (Stajner et al., 2016b) consists of a collection of 631 pairs of english sentences composed of a source sentence extracted from"
W18-7005,P08-1040,0,0.100971,"and to understand, thus improving the accessibility of text for people suffering from a range of disabilities such as aphasia (Carroll et al., 1998) or dyslexia (Rello et al., 2013), as well as for second language learners (Xia et al., 2016) and people with low literacy (Watanabe et al., 2009). This topic has been researched for a variety of languages such as English (Zhu et al., 2010; Wubben 1 Note that text simplification has also been used as a preprocessing step for other natural language processing tasks such as machine translation (Chandrasekar et al., 1996) and semantic role labelling (Vickrey and Koller, 2008). 2 There is no unique way to define the notion of simplicity in this context. Previous works often rely on the intuition of human annotators to evaluate the level of simplicity of a TS system output. 29 Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA), pages 29–38, c Tilburg, The Netherlands, November 8 2018. 2018 Association for Computational Linguistics evaluation methods and traditional quality estimation features. We then present the QATS shared task and the associated dataset, which we use for our experiments. Finally we compare all methods in a reference-less setting a"
W18-7005,P12-1107,0,0.140763,"Missing"
W18-7005,P13-4014,0,0.0857038,"of elementary metrics, which range from commonly used metrics such as BLEU to basic metrics based on a single low-level feature such as sentence length. We then compare the effect of aggregating these elementary metrics into more complex ones and compare our results with the state of the art, based on the QATS shared task data and results. Other metrics One can also estimate the quality of a TS system output based on simple features extracted from it. For instance, the Q U E ST framework for quality estimation in MT gives a number of useful baseline features for evaluating an output sentence (Specia et al., 2013). These features range from simple statistics, such as the number of words in the sentence, to more sophisticated features, such as the probability of the sentence according to a language model. Several teams who participated in the QATS shared task used metrics ˇ based on this framework, namely SMH (Stajner et al., 2016a), UoLGP (Rios and Sharoff, 2015) and UoW (B´echara et al., 2015). Readability metrics such as Flesch-Kincaid Grade Level (FKGL) and Flesch Reading Ease (FRE) (Kincaid et al., 1975) have been extensively used for evaluating simplicity. These two metrics, which were shown exper"
W18-7005,W16-0502,0,0.0198883,"nt of (sentencelevel) MT. The standard approach to automatic TS evaluation is therefore to view the task as a translation problem and to use machine translation (MT) Introduction Text simplification (hereafter TS) has received increasing interest by the scientific community in recent years. It aims at producing a simpler version of a source text that is both easier to read and to understand, thus improving the accessibility of text for people suffering from a range of disabilities such as aphasia (Carroll et al., 1998) or dyslexia (Rello et al., 2013), as well as for second language learners (Xia et al., 2016) and people with low literacy (Watanabe et al., 2009). This topic has been researched for a variety of languages such as English (Zhu et al., 2010; Wubben 1 Note that text simplification has also been used as a preprocessing step for other natural language processing tasks such as machine translation (Chandrasekar et al., 1996) and semantic role labelling (Vickrey and Koller, 2008). 2 There is no unique way to define the notion of simplicity in this context. Previous works often rely on the intuition of human annotators to evaluate the level of simplicity of a TS system output. 29 Proceedings"
W18-7005,P15-2135,0,0.0481227,"Missing"
W18-7005,Q15-1021,0,0.0797566,"of parallel corpora of source sentences and manually produced reference translations, which are available on a large scale for many language pairs (Tiedemann, 2012). TS datasets are less numerous and smaller. Moreover, they are often automatically extracted from comparable corpora rather than strictly parallel corpora, which results in noisier reference data. For example, the PWKP dataset (Zhu et al., 2010) consists of 100,000 sentences from the English Wikipedia automatically aligned with sentences from the Simple English Wikipedia based on term-based similarity metrics. It has been shown by Xu et al. (2015) that many of PWKP’s “simplified” sentences are in fact not simpler or even not related to their corresponding source sentence. Even if better quality corpora such as Newsela do exist (Xu et al., 2015), they are costly to create, often of limited size, and not necessarily open-access. 2 2.1 Existing evaluation methods Using MT metrics to compare the output and a reference TS can be considered as a monolingual translation task. As a result, MT metrics such as BLEU (Papineni et al., 2002), which compare the output of an MT system to a reference translation, have been extensively used for TS (Nar"
W18-7005,W14-1201,0,0.222835,"Missing"
W18-7005,Q16-1029,0,0.303592,"in fact not simpler or even not related to their corresponding source sentence. Even if better quality corpora such as Newsela do exist (Xu et al., 2015), they are costly to create, often of limited size, and not necessarily open-access. 2 2.1 Existing evaluation methods Using MT metrics to compare the output and a reference TS can be considered as a monolingual translation task. As a result, MT metrics such as BLEU (Papineni et al., 2002), which compare the output of an MT system to a reference translation, have been extensively used for TS (Narayan and Garˇ dent, 2014; Stajner et al., 2015; Xu et al., 2016). Other successful MT metrics include TER (Snover et al., 2009), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005), but they have not gained much traction in the TS literature. These metrics rely on good quality references, something which is often not available in TS, as ˇ discussed by Xu et al. (2015). Moreover, Stajner et al. (2015) and Sulem et al. (2018a) showed that using BLEU to compare the system output with a reference is not a good way to perform TS evaluation, even when good quality references are available. This is especially true when the TS system produces more than one sen"
W18-7005,C10-1152,0,0.468655,"e translation (MT) Introduction Text simplification (hereafter TS) has received increasing interest by the scientific community in recent years. It aims at producing a simpler version of a source text that is both easier to read and to understand, thus improving the accessibility of text for people suffering from a range of disabilities such as aphasia (Carroll et al., 1998) or dyslexia (Rello et al., 2013), as well as for second language learners (Xia et al., 2016) and people with low literacy (Watanabe et al., 2009). This topic has been researched for a variety of languages such as English (Zhu et al., 2010; Wubben 1 Note that text simplification has also been used as a preprocessing step for other natural language processing tasks such as machine translation (Chandrasekar et al., 1996) and semantic role labelling (Vickrey and Koller, 2008). 2 There is no unique way to define the notion of simplicity in this context. Previous works often rely on the intuition of human annotators to evaluate the level of simplicity of a TS system output. 29 Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA), pages 29–38, c Tilburg, The Netherlands, November 8 2018. 2018 Association for Computation"
W18-7005,D18-1081,0,0.362221,"translation task. As a result, MT metrics such as BLEU (Papineni et al., 2002), which compare the output of an MT system to a reference translation, have been extensively used for TS (Narayan and Garˇ dent, 2014; Stajner et al., 2015; Xu et al., 2016). Other successful MT metrics include TER (Snover et al., 2009), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005), but they have not gained much traction in the TS literature. These metrics rely on good quality references, something which is often not available in TS, as ˇ discussed by Xu et al. (2015). Moreover, Stajner et al. (2015) and Sulem et al. (2018a) showed that using BLEU to compare the system output with a reference is not a good way to perform TS evaluation, even when good quality references are available. This is especially true when the TS system produces more than one sentence for a single source sentence. This creates a challenge for the use of referencebased MT metrics for TS evaluation. However, TS has the advantage of being a monolingual translation-like task, the source being in the same language as the output. This allows for new, nonconventional ways to use MT evaluation metrics, namely by using them to compare the output o"
W18-7005,N18-1063,0,0.115581,"Missing"
W18-7005,tiedemann-2012-parallel,0,0.0105104,"8, c Tilburg, The Netherlands, November 8 2018. 2018 Association for Computational Linguistics evaluation methods and traditional quality estimation features. We then present the QATS shared task and the associated dataset, which we use for our experiments. Finally we compare all methods in a reference-less setting and analyze the results. evaluation metrics such as BLEU (Papineni et al., 2002). However, MT evaluation metrics rely on the existence of parallel corpora of source sentences and manually produced reference translations, which are available on a large scale for many language pairs (Tiedemann, 2012). TS datasets are less numerous and smaller. Moreover, they are often automatically extracted from comparable corpora rather than strictly parallel corpora, which results in noisier reference data. For example, the PWKP dataset (Zhu et al., 2010) consists of 100,000 sentences from the English Wikipedia automatically aligned with sentences from the Simple English Wikipedia based on term-based similarity metrics. It has been shown by Xu et al. (2015) that many of PWKP’s “simplified” sentences are in fact not simpler or even not related to their corresponding source sentence. Even if better quali"
Y09-1013,J96-1002,0,0.0085507,"Missing"
Y09-1013,W09-1008,0,0.0620165,"Missing"
Y09-1013,A00-2013,0,0.159583,"Missing"
Y09-1013,W02-2018,0,0.00784523,"model, the choice of the parameters is subject to constraints that force the model expectations of the features to be equal to their empirical expectations over the training data (Berger et al., 4 5 (Ratnaparkhi, 1996) and (Toutanova and Manning, 2000) report accuracy scores of 96.43 and 96.86 on section 23-24 of the Penn Treebank, respectively. Arguably better suited for sequential problems, Conditional Random Fields (CRF) (Lafferty et al., 2001) are considerably slower to train. 112 1996). In our experiments, the parameters were estimated using the Limited Memory Variable Metric Algorithm (Malouf, 2002) implemented in the Megam package.6 The feature templates we used for designing our French tagging model is a superset of the features used by (Ratnaparkhi, 1996) and (Toutanova and Manning, 2000) for English (these were largely language independent). These features fall into two main categories. A first set of features try to capture the lexical form of the word being tagged: these include the actual word string for the current word wi , prefixes and suffixes (of character length 4 and less), as well as binary features testing whether wi contains special characters like numbers, hyphens, and"
Y09-1013,J93-2004,0,0.0321969,"s sizes, summed up in Table 7. For each resulting tagger, we measured the overall accuracy and the accuracy on unknown words. Table 7: Varying training corpus and lexicon sizes: experimental setups Lexicon size (lemmas) Corpus size (sentences) 5.2 0, 500, 1, 000, 2, 000, 5, 000, 10, 000, 20, 000, 50, 000, 110, 000 50, 100, 200, 500, 1, 000, 2, 000, 5, 000, 9, 881 Results and discussion Before comparing the respective relevance of lexicon and corpus manual development for optimizing the tagger’s performance, we need to be able to quantitatively compare their development costs, i.e., times. In (Marcus et al., 1993), the authors report a POS annotation speed that “exceeds 3, 000 words per hour” during the development of the Penn TreeBank. This speed is reached after a 1 month period (with 15 annotation hours per week, i.e., approximately 60 hours) during which the POS tagger used for pre-annotation was still improving. The authors also report on a manual tagging experiment (without automatic pre-annotation); they observed an annotation speed that is around 1, 300 words per hour. Therefore, it is probably safe to assume that, on average, the creation of a manually validated training corpus starts at a spe"
Y09-1013,W96-0213,0,0.974064,"ags into the same tagset used in the training corpus, hence building a large-coverage morphosyntactic lexicon containing 507, 362 distinct entries of the form (form, tag, lemma), corresponding to 502, 223 distinct entries of the form (form, tag). If grouping all verbal tags into a single “category” while considering all tags as “categories”, these entries correspond to 117, 397 (lemma, category) pairs (the relevance of these pairs will appear in Section 5). 3 Baseline MaxEnt tagger This section presents our baseline MaxEnt-based French POS tagger, MElt0fr . This tagger is largely inspired by (Ratnaparkhi, 1996) and (Toutanova and Manning, 2000), both in terms of the model and the features being used. To date, MaxEnt conditional sequence taggers are still among the best performing taggers developed for English.4 An important appeal of MaxEnt models is that they allow for the combination of very diverse, potentially overlapping features without assuming independence between the predictors. These models have also the advantage of being very fast to train.5 3.1 Description of the task Given a tagset T and a string of words w1n , we define the task of tagging as the process of assigning the maximum likel"
Y09-1013,sagot-etal-2006-lefff,1,0.41862,"determine the best trade-off between annotating data and constructing dictionaries. This paper addresses these questions through various tagging experiments carried out on French, based on our new tagging system called MElt (Maximum-Entropy Lexicon-enriched Tagger). An obvious motivation for working on this language is the availability of a training corpus Copyright 2009 by Pascal Denis and Benoît Sagot 23rd Pacific Asia Conference on Language, Information and Computation, pages 110–119 110 (namely, the French Treebank (Abeillé et al., 2003)) and a large-scale lexical resource (namely, Lefff (Sagot et al., 2006)). Additional motivation comes from the fact that there has been comparatively little work in probabilistic POS tagging in this language. An important side contribution of our paper is the development of a state-of-the-art, freely distributed POS tagger for French.1 Specifically, we here adopt a maximum entropy (MaxEnt) sequential classification approach. MaxEnt models remain among the best performing tagging systems for English and they are particularly easy to build and fast to train. This paper is organized as follows. Section 2 describes the datasets and the lexical resources that were use"
Y09-1013,W00-1308,0,0.902494,"used in the training corpus, hence building a large-coverage morphosyntactic lexicon containing 507, 362 distinct entries of the form (form, tag, lemma), corresponding to 502, 223 distinct entries of the form (form, tag). If grouping all verbal tags into a single “category” while considering all tags as “categories”, these entries correspond to 117, 397 (lemma, category) pairs (the relevance of these pairs will appear in Section 5). 3 Baseline MaxEnt tagger This section presents our baseline MaxEnt-based French POS tagger, MElt0fr . This tagger is largely inspired by (Ratnaparkhi, 1996) and (Toutanova and Manning, 2000), both in terms of the model and the features being used. To date, MaxEnt conditional sequence taggers are still among the best performing taggers developed for English.4 An important appeal of MaxEnt models is that they allow for the combination of very diverse, potentially overlapping features without assuming independence between the predictors. These models have also the advantage of being very fast to train.5 3.1 Description of the task Given a tagset T and a string of words w1n , we define the task of tagging as the process of assigning the maximum likelihood tag sequence tˆn1 ∈ T n to w"
