2016.gwc-1.31,broda-etal-2012-kpwr,1,0.869422,"Missing"
2016.gwc-1.31,W14-0146,1,0.887455,"Missing"
2016.gwc-1.31,piasecki-etal-2012-recognition,1,0.831991,"Missing"
2016.gwc-1.42,W97-0802,0,0.610878,"Missing"
2016.gwc-1.42,kedzia-piasecki-2014-ruled,1,0.856536,"Missing"
2016.gwc-1.42,R13-1058,1,0.886126,"Missing"
2016.gwc-1.42,W14-0142,1,0.872788,"Missing"
2016.gwc-1.42,R15-1056,1,0.885123,"Missing"
2016.gwc-1.42,mititelu-2012-adding,0,0.0251846,"Missing"
2016.gwc-1.42,R13-1073,1,0.864579,"Missing"
2016.gwc-1.42,C12-2101,1,0.90159,"Missing"
2016.gwc-1.42,R15-1092,1,0.894692,"Missing"
C12-2101,bentivogli-etal-2000-coping,0,0.333981,"lexical unit SŁOWA KLUCZOWE: wordnet, wordnet dwujęzyczny, rzutowanie wordnetów, synset, jednostka leksykalna + Work financed by the EU, the European Innovative Economy Programme Project POIG.01.01.02-14-013/09 Proceedings of COLING 2012: Posters, pages 1039–1048, COLING 2012, Mumbai, December 2012. 1039 1 Introduction We present a strategy and the preliminary results of the mapping of Polish WordNet [plWordNet] onto Princeton WordNet [PWN] (Fellbaum 1998). There have been many attempts to build such mappings for wordnets, including EuroWordNet [EWN] (Vossen 1998, Vossen 2002), MultiWordNet (Bentivogli, et al. 2000; Bentivogli & Pianta 2000), AsianWordNet (Robkop et al. 2010) and IndoWordNet (Sinha, et al. 2006, Bhattacharyya 2010). Those projects usually took advantage of EWN’s transfer-and-merge method, which largely consisted in the translation of most of PWN’s structure and content into the target language. In contrast with this, plWordNet’s design and construction are independent of EWN or PWN, though inevitably substantially influenced by both. A unique corpus-based method was employed (Maziarz et al. 2012, Piasecki et al. 2009). Synsets in plWordNet are merely groups of similarly interconnected l"
C12-2101,P00-1064,0,0.122466,"ediate hypernyms/hyponyms and meronyms and holonyms, if there are any. These are strefa and świat (zone, world). In case of doubts or difficulties with determining the synset sense, the editor considers the direct and indirect hypernyms (or other relations). Once the sense of the analysed synset has been established (‘area located beyond the borders of a given country’), the editor can move to the 1040 next stage: seek the equivalent target synset in PWN. First, automatic prompts are checked if they are present. We re-implemented an automated mapping algorithm described in (Daudé et al. 2003, Daudé et al. 2000). If there is no prompt, the editor’s language intuitions help select among target-language LUs one or two candidates which share the sense of the source-language synset (‘foreign country’). These candidate LUs are located in PWN and their synsets are analysed with respect to their sense and position in the wordnet structure (hypernym state). Special attention must be paid to their immediate hypernym(s) and hyponyms (or other relations if there are any), since these are going to be juxtaposed with the equivalent relations of the target synset. The editor must check if there already exist, or a"
C12-2101,W97-0802,0,0.574431,"Missing"
C14-1005,N13-1019,0,0.0614749,"cations. Topical segmentation is a lightweight form of such structural analysis: given a sequence of sentences or paragraphs, split it into a sequence of topical segments, each characterized by a certain degree of topical unity. This is particularly useful for texts with little structure imposed by the author, such as speech transcripts, meeting notes or literature. The past decade has witnessed significant progress in the area of text segmentation. Most of the topical segmenters (Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008; Kazantseva and Szpakowicz, 2011; Misra et al., 2011; Du et al., 2013) can only produce single-level segmentation, a worthy endeavour in and of itself. Yet, to view the structure of a document linearly, as a sequence of segments, is in certain discord with most theories of discourse structure, where it is more customary to consider documents as trees (Mann and Thompson, 1988; Marcu, 2000; Hernault et al., 2010; Feng and Hirst, 2012) or graphs (Wolf and Gibson, 2006). Regardless of the theory, we hypothesize that it may be useful to have an idea about fluctuations of topic in documents beyond the coarsest level. It is the contribution of this work that we develop"
C14-1005,D08-1035,0,0.837288,"information retrieval (Ponte and Croft, 1998) are some of the examples of such applications. Topical segmentation is a lightweight form of such structural analysis: given a sequence of sentences or paragraphs, split it into a sequence of topical segments, each characterized by a certain degree of topical unity. This is particularly useful for texts with little structure imposed by the author, such as speech transcripts, meeting notes or literature. The past decade has witnessed significant progress in the area of text segmentation. Most of the topical segmenters (Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008; Kazantseva and Szpakowicz, 2011; Misra et al., 2011; Du et al., 2013) can only produce single-level segmentation, a worthy endeavour in and of itself. Yet, to view the structure of a document linearly, as a sequence of segments, is in certain discord with most theories of discourse structure, where it is more customary to consider documents as trees (Mann and Thompson, 1988; Marcu, 2000; Hernault et al., 2010; Feng and Hirst, 2012) or graphs (Wolf and Gibson, 2006). Regardless of the theory, we hypothesize that it may be useful to have an idea about fluctuations of topic in documents beyond"
C14-1005,N09-1040,0,0.594111,"ons.org/licenses/by/4.0/ 37 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 37–47, Dublin, Ireland, August 23-29 2014. The objective function maximized by the segmenter is net similarity – the sum of similarities between all segment centres and their children for all levels of the tree. This function is similar to the objective function of the well-known k-means algorithm, except that here it is computed hierarchically. It is not easy to evaluate HAPS. We are not aware of comparable hierarchical segmenters other than that in (Eisenstein, 2009) which, unfortunately, is no longer publicly available. Therefore we compared the trees built by HAPS to the results of running iteratively two state-of-the-art flat segmenters. The results are compared on two datasets. A set of Wikipedia articles was automatically compiled by Carroll (2010). The other set, created to evaluate HAPS, consists of nine chapters from the novel Moonstone by Wilkie Collins. Each chapter was annotated for hierarchical structure by 3-6 people. The evaluation is based on two metrics, windowDiff (Pevzner and Hearst, 2002) and evalHDS (Carroll, 2010). Both metrics are le"
C14-1005,P12-1007,0,0.0329437,"rature. The past decade has witnessed significant progress in the area of text segmentation. Most of the topical segmenters (Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008; Kazantseva and Szpakowicz, 2011; Misra et al., 2011; Du et al., 2013) can only produce single-level segmentation, a worthy endeavour in and of itself. Yet, to view the structure of a document linearly, as a sequence of segments, is in certain discord with most theories of discourse structure, where it is more customary to consider documents as trees (Mann and Thompson, 1988; Marcu, 2000; Hernault et al., 2010; Feng and Hirst, 2012) or graphs (Wolf and Gibson, 2006). Regardless of the theory, we hypothesize that it may be useful to have an idea about fluctuations of topic in documents beyond the coarsest level. It is the contribution of this work that we develop such a hierarchical segmenter, implement it and do our best to evaluate it. The segmenter described here is HAPS – Hierarchical Affinity Propagation for Segmentation. It is closely based on a graphical model for hierarchical clustering called Hierarchical Affinity Propagation (Givoni et al., 2011). It is a similarity-based segmenter. It takes as input a matrix of"
C14-1005,N12-1016,0,0.0156671,", were paid $50 dollars each. Procedure. The instructions asked the annotator to read the chapter and split it into top-level segments according to where there is a perceptible shift of topic. She had to provide a one-sentence description of what the segment is about. The procedure had to be repeated for each segment all the way down to the level of individual paragraphs. Effectively, the annotators were building a detailed hierarchical outline for each chapter. Metrics. Two different metrics helped estimate the quality of our hierarchical dataset: windowDiff (Pevzner and Hearst, 2002) and S (Fournier and Inkpen, 2012). windowDiff is computed by sliding a window across the input sequence and checking, for each window position, whether the number of reference breaks is the same as the number of breaks in the hypothetical segmentation. The number of erroneous windows is then normalized by the total number of windows. In Equation 1, N is the length of the input sequence and k is the size of the sliding window. windowDif f = N −k X 1 (|ref − hyp |= 6 0) N −k (1) i=1 windowDiff is designed to compare sequences of segments, not trees. That is why we compute it for each level between each pair of annotators who wo"
C14-1005,N09-1041,0,0.0209244,"graph which best describes its contents. We evaluate the segmenter on a subset of a novel manually segmented by several annotators, and on a dataset of Wikipedia articles. The results suggest that hierarchical segmentations produced by HAPS are better than those obtained by iteratively running several one-level segmenters. An additional advantage of HAPS is that it does not require the “gold standard” number of segments in advance. 1 Introduction When an NLP application works with a document, it may benefit from knowing something about this document’s high-level structure. Text summarization (Haghighi and Vanderwende, 2009), question answering (Oh et al., 2007) and information retrieval (Ponte and Croft, 1998) are some of the examples of such applications. Topical segmentation is a lightweight form of such structural analysis: given a sequence of sentences or paragraphs, split it into a sequence of topical segments, each characterized by a certain degree of topical unity. This is particularly useful for texts with little structure imposed by the author, such as speech transcripts, meeting notes or literature. The past decade has witnessed significant progress in the area of text segmentation. Most of the topical"
C14-1005,J97-1003,0,0.821679,"rizer. A Java implementation of HAPS and the corpus of hierarchical segmentations for nine chapters of Moonstone are publicly available. We consider these to be the main contributions of this research. 2 Related work Most work on topical text segmentation has been done for single-level segmentation. Contemporary approaches usually rely on the idea that topic shifts can be identified by finding shifts in the vocabulary (Youmans, 1991). We can distinguish between local and global models for topical text segmentation. Local algorithms have a limited view of the document. For example, TextTiling (Hearst, 1997) operates by sliding a window through the input sequence and computing similarity between adjacent units. By identifying “valleys” in similarities, TextTiling identifies topic shifts. More recently, Marathe (2010) used lexical chains and Blei and Moreno (2001) used Hidden Markov Models. Such methods are usually very fast, but can be thrown off by small digressions in the text. Among global algorithms, we can distinguish generative probabilistic models and similarity-based models. Eisenstein and Barzilay (2008) model a document as a sequence of segments generated by latent topic variables. Misr"
C14-1005,D11-1026,1,0.867933,"d Croft, 1998) are some of the examples of such applications. Topical segmentation is a lightweight form of such structural analysis: given a sequence of sentences or paragraphs, split it into a sequence of topical segments, each characterized by a certain degree of topical unity. This is particularly useful for texts with little structure imposed by the author, such as speech transcripts, meeting notes or literature. The past decade has witnessed significant progress in the area of text segmentation. Most of the topical segmenters (Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008; Kazantseva and Szpakowicz, 2011; Misra et al., 2011; Du et al., 2013) can only produce single-level segmentation, a worthy endeavour in and of itself. Yet, to view the structure of a document linearly, as a sequence of segments, is in certain discord with most theories of discourse structure, where it is more customary to consider documents as trees (Mann and Thompson, 1988; Marcu, 2000; Hernault et al., 2010; Feng and Hirst, 2012) or graphs (Wolf and Gibson, 2006). Regardless of the theory, we hypothesize that it may be useful to have an idea about fluctuations of topic in documents beyond the coarsest level. It is the con"
C14-1005,N12-1022,1,0.878293,"ch level. For each segment it also outputs a segment centre, a unit of text which best captures the contents of the segment. 3 Creating a corpus of hierarchical segmentations Before embarking on the task of building a hierarchical segmenter, we wanted to study how people perform such a task. We also needed a benchmark corpus which could be used to evaluate the quality of segmentations produced by HAPS. To this end, we annotated nine chapters of the novel Moonstone for hierarchical structure. We settled on these data because it is a subset of a publicly available dataset for flat segmentation (Kazantseva and Szpakowicz, 2012). In our study, each chapter was annotated by 3-6 people (4.8 on average). The annotators, undergraduate students of English, were paid $50 dollars each. Procedure. The instructions asked the annotator to read the chapter and split it into top-level segments according to where there is a perceptible shift of topic. She had to provide a one-sentence description of what the segment is about. The procedure had to be repeated for each segment all the way down to the level of individual paragraphs. Effectively, the annotators were building a detailed hierarchical outline for each chapter. Metrics."
C14-1005,W04-1013,0,0.0150813,"tion produces better results then iteratively running flat segmenters. Compared to the baseline segmenters, HAPS has an important practical advantage. It does not require the number of segments as an input; this requirement is customary for most flat segmenters. We also made a rough attempt to evaluate the quality of the segment centres identified by HAPS. Using 20 chapters from several novels of Jane Austen, we compared the centres identified for each chapter against summaries produces by a recent automatic summarizer CohSum (Smith et al., 2012). The basis of comparison was the ROUGE metric (Lin, 2004). While far from conclusive, the results suggest that segment centres identified by HAPS are rather comparable with the summaries produced by an automatic summarizer. A Java implementation of HAPS and the corpus of hierarchical segmentations for nine chapters of Moonstone are publicly available. We consider these to be the main contributions of this research. 2 Related work Most work on topical text segmentation has been done for single-level segmentation. Contemporary approaches usually rely on the idea that topic shifts can be identified by finding shifts in the vocabulary (Youmans, 1991). W"
C14-1005,P06-1004,0,0.697908,"swering (Oh et al., 2007) and information retrieval (Ponte and Croft, 1998) are some of the examples of such applications. Topical segmentation is a lightweight form of such structural analysis: given a sequence of sentences or paragraphs, split it into a sequence of topical segments, each characterized by a certain degree of topical unity. This is particularly useful for texts with little structure imposed by the author, such as speech transcripts, meeting notes or literature. The past decade has witnessed significant progress in the area of text segmentation. Most of the topical segmenters (Malioutov and Barzilay, 2006; Eisenstein and Barzilay, 2008; Kazantseva and Szpakowicz, 2011; Misra et al., 2011; Du et al., 2013) can only produce single-level segmentation, a worthy endeavour in and of itself. Yet, to view the structure of a document linearly, as a sequence of segments, is in certain discord with most theories of discourse structure, where it is more customary to consider documents as trees (Mann and Thompson, 1988; Marcu, 2000; Hernault et al., 2010; Feng and Hirst, 2012) or graphs (Wolf and Gibson, 2006). Regardless of the theory, we hypothesize that it may be useful to have an idea about fluctuation"
C14-1005,J02-1002,0,0.695374,"f comparable hierarchical segmenters other than that in (Eisenstein, 2009) which, unfortunately, is no longer publicly available. Therefore we compared the trees built by HAPS to the results of running iteratively two state-of-the-art flat segmenters. The results are compared on two datasets. A set of Wikipedia articles was automatically compiled by Carroll (2010). The other set, created to evaluate HAPS, consists of nine chapters from the novel Moonstone by Wilkie Collins. Each chapter was annotated for hierarchical structure by 3-6 people. The evaluation is based on two metrics, windowDiff (Pevzner and Hearst, 2002) and evalHDS (Carroll, 2010). Both metrics are less then ideal. They do not give a complete picture of the quality of topical segmentations, but the preliminary results suggest that running a global model for hierarchical segmentation produces better results then iteratively running flat segmenters. Compared to the baseline segmenters, HAPS has an important practical advantage. It does not require the number of segments as an input; this requirement is customary for most flat segmenters. We also made a rough attempt to evaluate the quality of the segment centres identified by HAPS. Using 20 ch"
C14-1005,C12-2113,0,0.0198296,"sults suggest that running a global model for hierarchical segmentation produces better results then iteratively running flat segmenters. Compared to the baseline segmenters, HAPS has an important practical advantage. It does not require the number of segments as an input; this requirement is customary for most flat segmenters. We also made a rough attempt to evaluate the quality of the segment centres identified by HAPS. Using 20 chapters from several novels of Jane Austen, we compared the centres identified for each chapter against summaries produces by a recent automatic summarizer CohSum (Smith et al., 2012). The basis of comparison was the ROUGE metric (Lin, 2004). While far from conclusive, the results suggest that segment centres identified by HAPS are rather comparable with the summaries produced by an automatic summarizer. A Java implementation of HAPS and the corpus of hierarchical segmentations for nine chapters of Moonstone are publicly available. We consider these to be the main contributions of this research. 2 Related work Most work on topical text segmentation has been done for single-level segmentation. Contemporary approaches usually rely on the idea that topic shifts can be identif"
C14-1005,W04-3252,0,\N,Missing
C14-1005,N10-1143,0,\N,Missing
C14-1046,W01-0514,0,0.0626636,"ly cohesive segment. One version of TextTiling (Hearst, 1997) uses lexical chains manually constructed using Roget’s Thesaurus. Okumura and Honda (1994) apply automatically created lexical chains to segment a small set of documents in Japanese. More recently, Marathe (2010) tried to build lexical chains using distributional semantics and apply the method to text segmentation. Other proposals to move beyond word repetition in topical segmentation include the use of bigram overlap in (Reynar, 1999), information about collocations in (Jobbins and Evett, 1998), LSA (Landauer and Dumais, 1997) in (Choi et al., 2001; Olney and Cai, 2005) and WordNet in (Scaiano et al., 2010). It should be noted that much of the recent work on topical segmentation revolves around generative models. For example Blei and Moreno (2001) use HMM,while Eisenstein and Barzilay (2008), Misra et al. (2011) and Du et al. (2013) use higher-order models. We do not review this work in detail here because it centers on algorithms for text segmentation and not on the information supplied to those algorithms, which is the focus of this research. Fundamentally, the text is modelled as a sequence of tokens generated by latent topic variabl"
C14-1046,N13-1019,0,0.0116689,"exical chains using distributional semantics and apply the method to text segmentation. Other proposals to move beyond word repetition in topical segmentation include the use of bigram overlap in (Reynar, 1999), information about collocations in (Jobbins and Evett, 1998), LSA (Landauer and Dumais, 1997) in (Choi et al., 2001; Olney and Cai, 2005) and WordNet in (Scaiano et al., 2010). It should be noted that much of the recent work on topical segmentation revolves around generative models. For example Blei and Moreno (2001) use HMM,while Eisenstein and Barzilay (2008), Misra et al. (2011) and Du et al. (2013) use higher-order models. We do not review this work in detail here because it centers on algorithms for text segmentation and not on the information supplied to those algorithms, which is the focus of this research. Fundamentally, the text is modelled as a sequence of tokens generated by latent topic variables. Although probabilistic segmenters can be extended to use additional information (e.g., Eisenstein and Barzilay (2008) augment their segmenter with information about discourse markers), it is not trivial to change these models to include information such as synonymy, co-reference and so"
C14-1046,D08-1035,0,0.0286806,"Japanese. More recently, Marathe (2010) tried to build lexical chains using distributional semantics and apply the method to text segmentation. Other proposals to move beyond word repetition in topical segmentation include the use of bigram overlap in (Reynar, 1999), information about collocations in (Jobbins and Evett, 1998), LSA (Landauer and Dumais, 1997) in (Choi et al., 2001; Olney and Cai, 2005) and WordNet in (Scaiano et al., 2010). It should be noted that much of the recent work on topical segmentation revolves around generative models. For example Blei and Moreno (2001) use HMM,while Eisenstein and Barzilay (2008), Misra et al. (2011) and Du et al. (2013) use higher-order models. We do not review this work in detail here because it centers on algorithms for text segmentation and not on the information supplied to those algorithms, which is the focus of this research. Fundamentally, the text is modelled as a sequence of tokens generated by latent topic variables. Although probabilistic segmenters can be extended to use additional information (e.g., Eisenstein and Barzilay (2008) augment their segmenter with information about discourse markers), it is not trivial to change these models to include informa"
C14-1046,P94-1002,0,0.460828,"is based on the idea that changes of topic are usually accompanied by vocabulary changes. Introduced by Youmans (1991), it has since formed the backbone 477 of research on topical segmentation. We now briefly review recent work on text segmentation. Since the focus of this research is on what information is useful for text segmentation, this review emphasizes representations rather than algorithms. Perhaps the simplest way of estimating topical similarity between sentences is to measure cosine similarity between corresponding feature vectors. It has been used extensively in text segmentation. Hearst (1994; 1997) describes TextTiling, an algorithm which identifies topical shifts by sliding a window through the document and measures cosine similarity between adjacent windows. The drops in similarity signal shifts of topic. More recently, Malioutov and Barzilay (2006) as well as Kazantseva and Szpakowicz (2011) use graph cuts and factor graph clustering for text segmentation. Both systems rely on cosine similarity between bag-of-word vectors as an underlying representation. While cosine similarity between vectors is easy to compute, it is hardly a reliable metric of topical similarity. Several re"
C14-1046,J97-1003,0,0.370476,"stering for text segmentation. Both systems rely on cosine similarity between bag-of-word vectors as an underlying representation. While cosine similarity between vectors is easy to compute, it is hardly a reliable metric of topical similarity. Several researchers have used lexical chains – first introduced by Halliday and Hasan (1976) – to improve the performance of topical segmenters.1 The intuition behind using lexical chains for text segmentation is that the beginning and end of a chain tend to correspond to the beginning and end of a topically cohesive segment. One version of TextTiling (Hearst, 1997) uses lexical chains manually constructed using Roget’s Thesaurus. Okumura and Honda (1994) apply automatically created lexical chains to segment a small set of documents in Japanese. More recently, Marathe (2010) tried to build lexical chains using distributional semantics and apply the method to text segmentation. Other proposals to move beyond word repetition in topical segmentation include the use of bigram overlap in (Reynar, 1999), information about collocations in (Jobbins and Evett, 1998), LSA (Landauer and Dumais, 1997) in (Choi et al., 2001; Olney and Cai, 2005) and WordNet in (Scaia"
C14-1046,P98-1100,0,0.104621,"chain tend to correspond to the beginning and end of a topically cohesive segment. One version of TextTiling (Hearst, 1997) uses lexical chains manually constructed using Roget’s Thesaurus. Okumura and Honda (1994) apply automatically created lexical chains to segment a small set of documents in Japanese. More recently, Marathe (2010) tried to build lexical chains using distributional semantics and apply the method to text segmentation. Other proposals to move beyond word repetition in topical segmentation include the use of bigram overlap in (Reynar, 1999), information about collocations in (Jobbins and Evett, 1998), LSA (Landauer and Dumais, 1997) in (Choi et al., 2001; Olney and Cai, 2005) and WordNet in (Scaiano et al., 2010). It should be noted that much of the recent work on topical segmentation revolves around generative models. For example Blei and Moreno (2001) use HMM,while Eisenstein and Barzilay (2008), Misra et al. (2011) and Du et al. (2013) use higher-order models. We do not review this work in detail here because it centers on algorithms for text segmentation and not on the information supplied to those algorithms, which is the focus of this research. Fundamentally, the text is modelled as"
C14-1046,D11-1026,1,0.794682,"tactic parser, but a lighter form of processing might do, perhaps even if it captured personal pronouns. Using this information, we augment and correct a matrix of lexical similarities between sentences, a structure frequently used as an input to a topical segmenter. The results of using coreferential similarity are evaluated on a dataset of manually segmented chapters from a novel (Kazantseva and Szpakowicz, 2012) and on transcripts of lectures in Artificial Intelligence (Malioutov and Barzilay, 2006). We try the new similarity matrix on two publicly available similaritybased segmenters APS (Kazantseva and Szpakowicz, 2011) and MinCutSeg (Malioutov and Barzilay, 2006). The results suggest that the new matrix never hurts, and in several case improves, the performance of the segmenter, especially for the novel. We also check whether this metric would still be useful if instead of the traditionally used lexical similarity we used a similarity metric which took synonymy into account. In this case, the margin of improvement is lower, but still the coreferential similarity metric never hurts the performance and often improves it. Section 2 of the paper gives an overview of related work. Section 3 describes our similar"
C14-1046,N12-1022,1,0.926545,"y. “And I’m sent to fetch her in. All the hard work falls on my shoulders in this house. Let me alone, Mr. Betteredge!” The person here mentioned as Rosanna was our second housemaid. “Where is she?” I inquired. [. . . ] “At the sands, of course!” says Nancy, with a toss of her head. “She had another of her fainting fits this morning, and she asked to go out and get a breath of fresh air. I have no patience with her!” “Go back to your dinner, my girl,” I said. “I have patience with her, and I’ll fetch her in.” Figure 1 shows a snippet of a dialogue from the publicly available Moonstone corpus (Kazantseva and Szpakowicz, 2012). The two speakers discuss a specific person, Rosanna, yet her name is mentioned explicitly only twice. In the remainder of the dialogue the author uses pronouns to refer to this person, whose identity is evident from the context. Running an automatic segmenter on such a document would likely be challenging since focal concepts – characters – are often referred to by pronouns or definite noun phrases (NPs) instead of explicit repetition. The focal entity Rosanna is introduced once and then it is referred to by nominal and pronominal anaphora, not by explicit repetition. Simplifying things some"
C14-1046,P06-1004,0,0.465394,"quality of text segmentation. We extract NPs and classify them by informativeness. This is achieved with the help of a syntactic parser, but a lighter form of processing might do, perhaps even if it captured personal pronouns. Using this information, we augment and correct a matrix of lexical similarities between sentences, a structure frequently used as an input to a topical segmenter. The results of using coreferential similarity are evaluated on a dataset of manually segmented chapters from a novel (Kazantseva and Szpakowicz, 2012) and on transcripts of lectures in Artificial Intelligence (Malioutov and Barzilay, 2006). We try the new similarity matrix on two publicly available similaritybased segmenters APS (Kazantseva and Szpakowicz, 2011) and MinCutSeg (Malioutov and Barzilay, 2006). The results suggest that the new matrix never hurts, and in several case improves, the performance of the segmenter, especially for the novel. We also check whether this metric would still be useful if instead of the traditionally used lexical similarity we used a similarity metric which took synonymy into account. In this case, the margin of improvement is lower, but still the coreferential similarity metric never hurts the"
C14-1046,C94-2121,0,0.33763,"g-of-word vectors as an underlying representation. While cosine similarity between vectors is easy to compute, it is hardly a reliable metric of topical similarity. Several researchers have used lexical chains – first introduced by Halliday and Hasan (1976) – to improve the performance of topical segmenters.1 The intuition behind using lexical chains for text segmentation is that the beginning and end of a chain tend to correspond to the beginning and end of a topically cohesive segment. One version of TextTiling (Hearst, 1997) uses lexical chains manually constructed using Roget’s Thesaurus. Okumura and Honda (1994) apply automatically created lexical chains to segment a small set of documents in Japanese. More recently, Marathe (2010) tried to build lexical chains using distributional semantics and apply the method to text segmentation. Other proposals to move beyond word repetition in topical segmentation include the use of bigram overlap in (Reynar, 1999), information about collocations in (Jobbins and Evett, 1998), LSA (Landauer and Dumais, 1997) in (Choi et al., 2001; Olney and Cai, 2005) and WordNet in (Scaiano et al., 2010). It should be noted that much of the recent work on topical segmentation r"
C14-1046,H05-1122,0,0.0283256,". One version of TextTiling (Hearst, 1997) uses lexical chains manually constructed using Roget’s Thesaurus. Okumura and Honda (1994) apply automatically created lexical chains to segment a small set of documents in Japanese. More recently, Marathe (2010) tried to build lexical chains using distributional semantics and apply the method to text segmentation. Other proposals to move beyond word repetition in topical segmentation include the use of bigram overlap in (Reynar, 1999), information about collocations in (Jobbins and Evett, 1998), LSA (Landauer and Dumais, 1997) in (Choi et al., 2001; Olney and Cai, 2005) and WordNet in (Scaiano et al., 2010). It should be noted that much of the recent work on topical segmentation revolves around generative models. For example Blei and Moreno (2001) use HMM,while Eisenstein and Barzilay (2008), Misra et al. (2011) and Du et al. (2013) use higher-order models. We do not review this work in detail here because it centers on algorithms for text segmentation and not on the information supplied to those algorithms, which is the focus of this research. Fundamentally, the text is modelled as a sequence of tokens generated by latent topic variables. Although probabili"
C14-1046,A97-1011,0,0.137628,"Missing"
C14-1046,P99-1046,0,\N,Missing
C14-1046,C98-1097,0,\N,Missing
C14-1046,P13-4021,0,\N,Missing
C16-1213,P13-1133,0,0.1167,"al dictionary: Web-based (http://plwordnet.pwr.edu.pl) via an Android application, and via WordnetLoom (Piasecki et al., 2013) (http://ws.clarin-pl.eu/public/WordnetLoom-Viewer.zip) a wordnet editor which offers advanced visual, graph-based browsing. plWordNet has also been included in a very large and popular Polish multilingual dictionary Lingo (http://ling.pl). Access to plWordNet as a dictionary amounts to tens of thousand of visits a month. In addition to monolingual resources, plWordNet is part of multilingual resources, e.g., WordTies (Pedersen et al., 2012), Open Multilingual WordNet (Bond and Foster, 2013) and multimodal resources, e.g., the classification of gestures based on the verb categorisation in plWordNet (Lis and Navarretta, 2014). plWordNet was referred to in the resource for textual entailment (Przepiórkowski, 2015) and utilised for ontology mapping and linking ontology to lexicon (Jastrząb et al., 2016). Assorted applications of plWordNet include language correction, relation extraction (Mykowiecka and Marciniak, 2014), text indexing (Kaleta, 2014), Text Mining (Maciołek and Dobrowolski, 2013), text classification (Wróbel et al., 2016; Mirończuk and Protasiewicz, 2016), Open Domain"
C16-1213,2016.gwc-1.14,0,0.0176512,"formed by a group separate from the plWordNet editors, so it also served as a form of verification of the plWordNet content. The newest release of plWordNet, version 3.0, complements the preceding versions. After version 2.3, the work concentrated on a modified system of relations for adjectives (Maziarz et al., 2012) and on the expansion of the adjective sub-database; the construction of the adverb subnetwork,7 supported by a semi-automated method based on adjective-adverb derivational relations (Maziarz et al., 2016); and a major increase of the number of lexicalised multi-word expressions (Dziob and Wendelberger, 2016). 3 3.1 Comparative analysis The lexical net A wordnet is a lexical net, so it can be evaluated with statistical measures suitable for graphs (Lewis, 2009). We consider graph size, network volume, average graph density, corpus coverage, clustering coefficient, distance measure and connectivity. A wordnet of good quality ought to have a large, dense network, covering contemporary corpora well, and showing traits of “small-worldness”. Network volume and density. Table 2 shows the number of synsets, lemmas and LUs in three manually and independently constructed wordnets: Princeton WordNet, plWord"
C16-1213,W14-0104,0,0.023116,"and Piasecki, 2014), 10 R is a registered trademark. We cannot use it. The symbol WordNet 2265 • NELexion2, a very large lexicon of Polish Proper Names (PNs), ≈1.5 million, manually linked at the level of fine-grained semantic PN classes (Marcińczuk, 2016), • a lexicon of ≈60,000 multiword expressions with syntactic structures described, linked to plWordNet’s LUs by lemmas (Maziarz et al., 2015; Dziob et al., 2016), • a syntactic-semantic lexicon of Polish valency frames (≈15,000 lemmas described) linked to plWordNet at the LU level and semantic restrictions of frame arguments (Kotsyba, 2014; Hajnicz, 2014). The system is a very large network, linking knowledge elements to lexical meaning and descriptions of local syntactic-semantic structures. Given the mapping to WordNet, the system can be an anchor to a global Linked Data network,11 a powerful cloud of heterogenous data webs. Manually crafted lexicalsemantic resources could serve as a skeleton for the cloud, notably with plWordNet’s comprehensive coverage. Lexical item descriptions therein would be the means of anchoring webs to text clouds. plWordNet has become an important reference for research on the development of wordnets; (Fišer and Sa"
C16-1213,kedzia-piasecki-2014-ruled,1,0.832533,"s where a link from the Polish side would have been inaccurate. The result, enWordNet 1.0,10 is also part of this release, which ought to encourage comparative studies and cross-lingual research. 4 Applications of plWordNet Language resources are developed for applications: the higher the uptake, the better the perceived quality. plWordNet is a pivotal element of a system of language and knowledge resources; plWordNet’s wide coverage helps a lot. The system has several layers, with plWordNet in the middle: • top- and medium-level ontology SUMO with plWordNet semi-automatically mapped onto it (Kędzia and Piasecki, 2014), 10 R is a registered trademark. We cannot use it. The symbol WordNet 2265 • NELexion2, a very large lexicon of Polish Proper Names (PNs), ≈1.5 million, manually linked at the level of fine-grained semantic PN classes (Marcińczuk, 2016), • a lexicon of ≈60,000 multiword expressions with syntactic structures described, linked to plWordNet’s LUs by lemmas (Maziarz et al., 2015; Dziob et al., 2016), • a syntactic-semantic lexicon of Polish valency frames (≈15,000 lemmas described) linked to plWordNet at the LU level and semantic restrictions of frame arguments (Kotsyba, 2014; Hajnicz, 2014). The"
C16-1213,R15-1056,1,0.740008,"ge and knowledge resources; plWordNet’s wide coverage helps a lot. The system has several layers, with plWordNet in the middle: • top- and medium-level ontology SUMO with plWordNet semi-automatically mapped onto it (Kędzia and Piasecki, 2014), 10 R is a registered trademark. We cannot use it. The symbol WordNet 2265 • NELexion2, a very large lexicon of Polish Proper Names (PNs), ≈1.5 million, manually linked at the level of fine-grained semantic PN classes (Marcińczuk, 2016), • a lexicon of ≈60,000 multiword expressions with syntactic structures described, linked to plWordNet’s LUs by lemmas (Maziarz et al., 2015; Dziob et al., 2016), • a syntactic-semantic lexicon of Polish valency frames (≈15,000 lemmas described) linked to plWordNet at the LU level and semantic restrictions of frame arguments (Kotsyba, 2014; Hajnicz, 2014). The system is a very large network, linking knowledge elements to lexical meaning and descriptions of local syntactic-semantic structures. Given the mapping to WordNet, the system can be an anchor to a global Linked Data network,11 a powerful cloud of heterogenous data webs. Manually crafted lexicalsemantic resources could serve as a skeleton for the cloud, notably with plWordNe"
C16-1213,2016.gwc-1.31,1,0.747333,"LUs are the object of linguistic tests or are included in usage examples. The annotation was performed by a group separate from the plWordNet editors, so it also served as a form of verification of the plWordNet content. The newest release of plWordNet, version 3.0, complements the preceding versions. After version 2.3, the work concentrated on a modified system of relations for adjectives (Maziarz et al., 2012) and on the expansion of the adjective sub-database; the construction of the adverb subnetwork,7 supported by a semi-automated method based on adjective-adverb derivational relations (Maziarz et al., 2016); and a major increase of the number of lexicalised multi-word expressions (Dziob and Wendelberger, 2016). 3 3.1 Comparative analysis The lexical net A wordnet is a lexical net, so it can be evaluated with statistical measures suitable for graphs (Lewis, 2009). We consider graph size, network volume, average graph density, corpus coverage, clustering coefficient, distance measure and connectivity. A wordnet of good quality ought to have a large, dense network, covering contemporary corpora well, and showing traits of “small-worldness”. Network volume and density. Table 2 shows the number of sy"
C16-1213,P13-3014,0,0.0147475,"urces, e.g., the classification of gestures based on the verb categorisation in plWordNet (Lis and Navarretta, 2014). plWordNet was referred to in the resource for textual entailment (Przepiórkowski, 2015) and utilised for ontology mapping and linking ontology to lexicon (Jastrząb et al., 2016). Assorted applications of plWordNet include language correction, relation extraction (Mykowiecka and Marciniak, 2014), text indexing (Kaleta, 2014), Text Mining (Maciołek and Dobrowolski, 2013), text classification (Wróbel et al., 2016; Mirończuk and Protasiewicz, 2016), Open Domain Question Answering (Przybyła, 2013), and use as a quasi-ontology in document structure recognition (Kamola et al., 2015). Registered users of plWordNet declare its applications. Here is a selection of such declaration: education (at different levels) including Polish language teaching, building dictionaries, extraction of synonyms and semantically related words, detection of loanwords, cross-linguistic study on phonesthemes, classification of metaphorical expressions, corpus studies, grammar development, comparative and contrastive studies, language recognition, parsing disambiguation, semantic analysis of text, document simila"
C16-1213,C12-2101,1,0.820266,"larger than for plWordNet 3.0) and shortest path lengths. The conglomerate has small-world behaviour more than its separate parts. It seems that linking independently built resources creates a new quality. 3.2 Comparison by mapping As noted, plWordNet has been developed independently from WordNet, without any transfer of structures between the two resources, thus avoiding any bias towards WordNet. Even so, the alignment of plWordNet and WordNet was needed for a variety of (bilingual and multilingual) applications and research tasks. We have designed a strategy of mapping plWordNet to WordNet (Rudnicka et al., 2012). The key element 2263 I-relation I-Synonymy I-Hyponymy I-Hypernymy I-Meronymy I-Holonymy I-Partial synonymy I-Inter-register synonymy I-Cross-categorial synonymy Total Noun 36,367 74,394 4,121 6,982 3,471 4,339 1,672 131,346 Adjective 4,077 29,216 167 1,544 54 19,286 54,344 Adverb 448 781 51 4 22 1,306 Total 40,892 104,391 4,339 6,982 3,471 5,887 1,748 19,286 186,996 Table 4: Interlingual relation counts of the strategy was a comparison of the two relation structures in order to find the corresponding nodes of synset graph structures and link them via one of eight interlingual relations (hier"
C16-1213,R15-1092,1,0.614174,"Missing"
C18-1033,P10-1015,0,0.0293545,"ness, overspecialization, occurs when the system makes non-diverse recommendations (Alharthi et al., 2017). Hybrid RSs can combine CF and CB to tackle such issues and enhance the recommendation accuracy. A content-based RS is proposed that analyzes the texts of books to learn users’ reading interests. A survey on book recommender systems (Alharthi et al., 2017) describes only a few RSs that take the actual text of books into account. There is a lot of work that explores literary works, including automatic genre identification (Ardanuy and Sporleder, 2016) and learning the narrative structure (Elson et al., 2010). It is quite surprising, then, that the analysis of the textual content of books to improve their recommendations is still quite limited. A user’s reading preferences might be influenced by many elements specific to books. For example, recommendations given by the readers’ advisory (a library service that suggests books to patrons) rely on multiple appeal factors. The factors are characterization, frame, language and writing style, pacing, special topics, storyline, and tone. To obtain information about books’ appeal factors, librarians can subscribe (at a fee) to reader-advisory databases su"
C18-1033,D14-1181,0,0.0151007,"etwork adopted in (Macke and Hirshman, 2015) showed high accuracy only when the number of classes was small: 10 authors. Our preliminary experiments show that RNN-based author identification models have poor accuracy, so we do not use such models in this work. 3 Methodology This section describes the two components of the system. It first illustrates and explains the author identification system, which was proposed by (Solorio et al., 2017), and our modifications to the system. Next, the recommendation procedure is described. 3.1 Author Identification As shown in Figure 1, drawn similarly to (Kim, 2014), the neural network has the following layers. Embedding layer (also called lookup table). It takes a sequence of words or character bigrams as input, and maps each word or bigram to an embedding of size k. The embedding of the ith word/character is a dense k-dimensional vector xi ∈ Rk . Each book is represented as a matrix, with one word/character embedding per row. A book has sequence length n (n is the number of words/characters) where a sequence x1:n = x1 ⊕ x2 ... ⊕ xn is a concatenation of all words from x1 to xn . Convolution layer. This layer consists of one or more filters (also called"
C18-1033,E14-3011,0,0.0271692,"Missing"
C18-1033,E17-2106,0,0.0607163,"Missing"
C92-3156,J87-1007,0,0.0620008,"Missing"
C92-3156,J83-1005,0,0.178886,"Missing"
C92-3156,J91-2003,0,0.029507,"Missing"
C98-1015,P95-1007,0,0.0769519,"postmodifying prepositional phrases Mk is the preposition. For appositives, Mk is the symbol appos. The (M, H, Mk) triples for examples (9), (I0) and (11) appear in Table 2. Noun Modifier Bracketing Before assigning NMRs, the system must bracket the head noun and the premodifier sequence into modifier-head pairs. Example (2) shows the bracketing for noun phrase ( 1). (1) dynamic high impedance microphone (2) (dynamic ((high impedance) microphone)) The bracketing problem for noun-noun-noun compounds has been investigated by Liberman & Sproat (1992), Pustejovsky et al. (1993), Resnik (1993) and Lauer (1995) among others. Since the NMR analyzer must handle premodifier sequences of any length with both nouns and adjectives, it requires more general techniques. Our semi-automatic bracketer (Barker, 1998) allows for any number of adjective or noun premodifiers. After bracketing, each non-atomic element of a bracketed pair is considered a subphrase of the original phrase. The subphrases for the bracketing in (2) appear in (3), (4) and (5). (9) monitor cable plug (10) large piece of chocolate cake (11) my brother, a friend to all young people To assign an NMR to a triple (M, H, Mk), the system looks f"
C98-1015,J93-2005,0,0.00878919,"item links the premodifier to the head. For postmodifying prepositional phrases Mk is the preposition. For appositives, Mk is the symbol appos. The (M, H, Mk) triples for examples (9), (I0) and (11) appear in Table 2. Noun Modifier Bracketing Before assigning NMRs, the system must bracket the head noun and the premodifier sequence into modifier-head pairs. Example (2) shows the bracketing for noun phrase ( 1). (1) dynamic high impedance microphone (2) (dynamic ((high impedance) microphone)) The bracketing problem for noun-noun-noun compounds has been investigated by Liberman & Sproat (1992), Pustejovsky et al. (1993), Resnik (1993) and Lauer (1995) among others. Since the NMR analyzer must handle premodifier sequences of any length with both nouns and adjectives, it requires more general techniques. Our semi-automatic bracketer (Barker, 1998) allows for any number of adjective or noun premodifiers. After bracketing, each non-atomic element of a bracketed pair is considered a subphrase of the original phrase. The subphrases for the bracketing in (2) appear in (3), (4) and (5). (9) monitor cable plug (10) large piece of chocolate cake (11) my brother, a friend to all young people To assign an NMR to a tripl"
D11-1026,A00-2004,0,0.0827403,"uring lexical similarity between them. Drops in similarity correspond to topic shifts. Other examples include text 285 segmentation using Hidden Markov Models (Blei and Moreno, 2001) or Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001). Locally informed methods are often very efficient because of lean memory and CPU time requirements. Due to a limited view of the document, however, they can easily be thrown off by short inconsequential digressions in narration. Globally informed methods consider “the big picture” when determining the most likely location of segment boundaries. Choi (2000) applies divisive clustering to segmentation. Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. They cast segmentation as a graphcutting problem. The document is represented as a graph: nodes are sentences and edges are weighted using a measure of lexical similarity. The graph is cut in a way which maximizes the net edge weight within each segment and minimizes the net weight of severed edges. Such Minimum Cut segmentation resembles APS the most among others mentioned in this paper. The main difference between t"
D11-1026,D08-1035,0,0.790433,"in error-correcting decoding, image processing and data compression. Theoretically, such algorithms are not guaranteed to converge or to maximize the objective function. Yet in practice they often achieve competitive results. APS works on an already pre-compiled similaritiy matrix, so it offers flexibility in the choice of similarity metrics. The desired number of segments can be set by adjusting preferences. We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisenstein and Barzilay, 2008) and identifying chapter breaks in novels. We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). The comparison is based on the WindowDiff metric (Pevzner and Hearst, 2002). APS matches or outperforms these very competitive baselines. Section 2 of the paper outlines relevant research on topical text segmentation. Section 3 briefly covers the framework of factor graphs and outlines the original Affinity Propagation algorithm for clustering. Section 4 contains the derivation of the new update m"
D11-1026,N09-1041,0,0.0292708,"Section 2), but for a globallyinformed algorithm the requirements are very reasonable. APS is an instance of loopy-belief propagation (belief propagation on cyclic graphs) which has In complex narratives, it is typical for the topic to shift continually. Some shifts are gradual, others – more abrupt. Topical text segmentation identifies the more noticeable topic shifts. A topical segmenter’s output is a very simple picture of the document’s structure. Segmentation is a useful intermediate step in such applications as subjectivity analysis (Stoyanov and Cardie, 2008), automatic summarization (Haghighi and Vanderwende, 2009), question answering (Oh, Myaeng, and Jang, 2007) and others. 1 That is why improved quality of text segmentation An implementation of APS in Java, and the data sets, can be can benefit other language-processing tasks. downloaded at hwww.site.uottawa.ca/∼ankazanti. 284 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 284–293, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics been used to achieved state-of-the-art performance in error-correcting decoding, image processing and data compression. Theoretically,"
D11-1026,J97-1003,0,0.712901,"ports the results, Section 7 discusses conclusions and future work. 2 Related Work This sections discusses selected text segmentation methods and positions the proposed APS algorithm in that context. Most research on automatic text segmentation revolves around a simple idea: when the topic shifts, so does the vocabulary (Youmans, 1991). We can roughly subdivide existing approaches into two categories: locally informed and globally informed. Locally informed segmenters attempt to identify topic shifts by considering only a small portion of complete document. A classical approach is TextTiling (Hearst, 1997). It consists of sliding two adjacent windows through text and measuring lexical similarity between them. Drops in similarity correspond to topic shifts. Other examples include text 285 segmentation using Hidden Markov Models (Blei and Moreno, 2001) or Conditional Random Fields (Lafferty, McCallum, and Pereira, 2001). Locally informed methods are often very efficient because of lean memory and CPU time requirements. Due to a limited view of the document, however, they can easily be thrown off by short inconsequential digressions in narration. Globally informed methods consider “the big picture"
D11-1026,P06-1004,0,0.73925,"putational Linguistics been used to achieved state-of-the-art performance in error-correcting decoding, image processing and data compression. Theoretically, such algorithms are not guaranteed to converge or to maximize the objective function. Yet in practice they often achieve competitive results. APS works on an already pre-compiled similaritiy matrix, so it offers flexibility in the choice of similarity metrics. The desired number of segments can be set by adjusting preferences. We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisenstein and Barzilay, 2008) and identifying chapter breaks in novels. We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). The comparison is based on the WindowDiff metric (Pevzner and Hearst, 2002). APS matches or outperforms these very competitive baselines. Section 2 of the paper outlines relevant research on topical text segmentation. Section 3 briefly covers the framework of factor graphs and outlines the original Affinity Propagation algo"
D11-1026,J02-1002,0,0.827063,"x, so it offers flexibility in the choice of similarity metrics. The desired number of segments can be set by adjusting preferences. We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisenstein and Barzilay, 2008) and identifying chapter breaks in novels. We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). The comparison is based on the WindowDiff metric (Pevzner and Hearst, 2002). APS matches or outperforms these very competitive baselines. Section 2 of the paper outlines relevant research on topical text segmentation. Section 3 briefly covers the framework of factor graphs and outlines the original Affinity Propagation algorithm for clustering. Section 4 contains the derivation of the new update messages for APSeg. Section 5 describes the experimental setting, Section 6 reports the results, Section 7 discusses conclusions and future work. 2 Related Work This sections discusses selected text segmentation methods and positions the proposed APS algorithm in that context"
D11-1026,C08-1103,0,0.0295739,"ion algorithms such as those based on HMM or CRF (see Section 2), but for a globallyinformed algorithm the requirements are very reasonable. APS is an instance of loopy-belief propagation (belief propagation on cyclic graphs) which has In complex narratives, it is typical for the topic to shift continually. Some shifts are gradual, others – more abrupt. Topical text segmentation identifies the more noticeable topic shifts. A topical segmenter’s output is a very simple picture of the document’s structure. Segmentation is a useful intermediate step in such applications as subjectivity analysis (Stoyanov and Cardie, 2008), automatic summarization (Haghighi and Vanderwende, 2009), question answering (Oh, Myaeng, and Jang, 2007) and others. 1 That is why improved quality of text segmentation An implementation of APS in Java, and the data sets, can be can benefit other language-processing tasks. downloaded at hwww.site.uottawa.ca/∼ankazanti. 284 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 284–293, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics been used to achieved state-of-the-art performance in error-correcting decodi"
hermet-etal-2008-using,W01-1819,0,\N,Missing
I08-1034,P04-3034,0,0.0576114,"Missing"
I08-1034,P07-1102,0,0.119251,"of neuro-linguistic programming investigates how to program our language (among other things) to achieve a goal. In the 1980s, Rodger Bailey developed the Language and Behaviour Profile based on 60 meta-programs. Charvet (1997) presents a simplified approach with 14 metaprograms. This profile proposes that people’s language patterns are indicators of behavioural preferences. In the study of planning dialogues (ChuCarroll and Carberry, 2000), Searle’s theory of speech acts used through the discourse analysis also supports the fact that language carries much of people’s behaviour and emotions. Reitter and Moore (2007) studied repetitions in task-oriented conversations. They demonstrated that a speaker’s shortterm ability to copy the interlocutor’s syntax is autonomous from the success of the task, whereas long-term adaptation varies with such success. We consider a negotiation to be a communication in which the participants want to reach an agreement relative to the splitting/sharing of resources. Language is one of the tools used to reach the goal. We propose that not all messages exchanged throughout a negotiation have the same effect on the negotiation outcome. To test this hypothesis, we take an ever s"
I08-1034,D07-1048,0,0.147094,"model of behaviour (Koeszegi et al, 2007), which is common in faceto-face negotiations (Adair and Brett, 2005). Here is an example of behavioural phases in face-to-face negotiations: Perform Relational Positioning → Identify the Problem → Generate Solutions → Reach Agreement. Unexpected turns and moves – typical of human behaviour – make prediction of the negotiation outcome difficult. In case of electronic negotiation, the absence of the usual negotiation structure further complicates the outcome prediction. This distinguishes e-negotiations from agentcustomer phone conversations studied in (Takeuchi et al, 2007), where an agent follows the call flow pre-defined by his company’s policy. 258 The longer an e-negotiation takes, the more elaborate the structure of the e-negotiation process becomes. Simpler e-negotiation may involve an exchange of well-structured business documents such as pre-defined contract or retail transactions. A more complex process comprises numerous offers and counter-offers and has a high degree of uncertainty because of the possible unpredictability of negotiation moves. The next challenge stems from the limitations imposed by the use of electronic means. This overloads text mes"
I08-1034,H92-1073,0,0.0278682,"ble data of electronic negotiations. Our data come from the Web-based negotiAmong the wealth of data gathered by Inspire, we have focussed on the accompanying text messages, extracted from the transcripts of 2557 negotiations. Each negotiation had two different participants, and one person participated in only one negotiation. The total number of contributors was over 5000; most of them were not native English speakers. The data contain 1, 514, 623 word tokens and 27, 055 types. Compared with benchmark corpora, for example the Brown or the Wall Street Journal corpus (Francis and Kucera, 1997; Paul and Baker, 1992), this collection has a lower type-token ratio and a higher presence of content words among the most frequent words (this is typical of texts on a specific topic), and a high frequency of singular first- and second-person pronouns (this is typical of dialogues). We considered all messages from one negotiation to be a single negotiation text. We concatenated the messages in chronological order, keeping the punctuation and spelling unedited. Each negotiation had a unique label, either positive or negative, and was a training example in one of two classes – success259 Features negotiation-related"
I08-1041,H05-1073,0,0.282263,"ther consists of words extracted from WordNet-Affect. Experiments on the data obtained from blogs show that a combination of corpus-based unigram features with emotion-related features provides superior classification performance. We achieve Fmeasure values that outperform the rulebased baseline method for all emotion classes. 1 Introduction Recognizing emotions conveyed by a text can provide an insight into the author’s intent and sentiment, and can lead to better understanding of the text’s content. Emotion recognition in text has recently attracted increased attention of the NLP community (Alm et al., 2005; Liu et al, 2003; Mihalcea and Liu, 2006); it is also one of the tasks at Semeval-2007 1. Automatic recognition of emotions can be applied in the development of affective interfaces for Computer-Mediated Communication and HumanComputer Interaction. Other areas that can potentially benefit from automatic emotion analysis are personality modeling and profiling (Liu and Maes, 2004), affective interfaces and communication systems (Liu et al, 2003; Neviarouskaya et al., 2007a) consumer feedback analysis, affective tutoring in e-learning systems (Zhang et al., 2006), and textto-speech synthesis (Al"
I08-1041,W04-3253,0,0.066775,"Missing"
I08-1041,W02-1011,0,0.0114994,".390 0.283 0.262 0.099 0.296 0.365 0.867 F-Measure 0.469 0.368 0.379 0.179 0.306 0.506 0.579 Table 2. Performance metrics of the baseline system 5 Approach Based on Machine Learning We study two types of features: corpus-based features and features based on emotion lexicons. 5.1 Corpus-based features The corpus-based features exploit the statistical characteristics of the data on the basis of the ngram distribution. In our experiments, we take unigrams (n=1) as features. Unigram models have been previously shown to give good results in sentiment classification tasks (Kennedy and Inkpen, 2006; Pang et al., 2002): unigram representations can capture a variety of lexical combinations and distributions, including those of emotion words. This is particularly important in the case of blogs, whose language is often characterized by frequent use of new words, acronyms (such as “lol”), onomatopoeic words (“haha”, “grrr”), and slang, most of which can be captured in a unigram representa314 Similarity Score 16 14 12 Happiness Sadness Anger family, home, friends, life, house, loving, partying, bed, pleasure, rest, close, event, lucks, times crying, lost, wounds, bad, pills, falling, messed, spot, unhappy, pass,"
I08-1041,strapparava-valitutti-2004-wordnet,0,0.150553,"discernible emotion. We experiment with two types of features for representing text in emotion classification based on machine learning (ML). Features of the first type are a corpus-based unigram representation of text. Features of the second type comprise words that appear in emotion lexicons. One such lexicon consists of words that we automatically extracted from Roget’s Thesaurus (1852). We chose words for their semantic similarity to a basic set of terms that represent each emotion category. Another lexicon builds on lists of words for each emotion category, extracted from WordNet-Affect (Strapparava and Valitutti, 2004). We compare the classification results for groups of features of these two types. We get good results when the features are combined in a series of ML experiments. 1 Affective Text: Semeval Task at the 4th International Workshop on Semantic Evaluations, 2007, Prague (nlp.cs.swarthmore.edu/semeval/tasks/task14/summary.shtml). 312 2 Related Work Research in emotion recognition has focused on discerning emotions along the dimensions of valence (positive / negative) and arousal (calm / excited), and on recognizing distinct emotion categories. We focus on the latter. Liu et al. (2003) use a real-w"
I08-1041,P04-1035,0,\N,Missing
J10-1003,J08-4004,0,0.00827951,"stellan 1988). 96 (10) Kazantseva and Szpakowicz Summarizing Short Stories Table 6 Inter-annotator agreement on selecting summary-worthy sentences. Statistic π(4) π(3) Group 1 Group 2 Average 0.52 0.50 0.34 0.34 0.43 0.42 Agreementobserved = 1 ic(c − 1) Agreementexpected =  nik (nik − 1) (11) n2k (12) i∈I k∈K 1 (ic)2  k∈K where i is the number of items to be classiﬁed in set I, k is the number of available categories in set K, c is the number of coders, nik is the number of coders who assign item i to category k, and nk is the total number of items assigned to category k by all annotators (Artstein and Poesio 2008, pp. 562–563). Subjects. Six human subjects participated in annotating the test set of stories for the presence of summary-worthy sentences. These people are colleagues and acquaintances of the ﬁrst author. At the time of the experiment none of them was familiar with the design of the system. Four annotators are native speakers of English and the remaining two have a very good command of the language. Materials. The material for the experiment consisted of the 20 stories of the test set. Three annotators created extractive summaries for each story. In addition, there were eight distinct autom"
J10-1003,J05-3002,0,0.0156987,"and Gerv´as 2007), generation of tables-of-contents (Branavan, Deshpande, and Barzilay 2007), summarization of speech (Fuentes et al. 2005), dialogues (Zechner 2002), evaluative text (Carenini, Ng, and Pauls 2006), and biomedical documents (Reeve, Han, and Brooks 2007). In addition, more researchers have been venturing past purely extractive summarization (Krahmer, Marsi, and van Pelt 2008; Nomoto 2007; McDonald 2006). By and large, however, most research in text summarization still revolves around texts characterized by rigid structure. The better explored among such texts are news articles (Barzilay and McKeown 2005), medical documents (Elhadad et al. 2005), legal documents (Moens 2007), and papers in the area of computer science (Teufel and Moens 2002; Mei and Zhai 2008). Although summarizing these genres is a formidable challenge in itself, it excludes a continually increasing number of informal documents available electronically. Such documents, ranging from novels to personal Web pages, offer a wealth of information that merits the attention of the text summarization community. ∗ School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ottawa, Ontario K1N 6N5, Cana"
J10-1003,P07-1069,0,0.0574522,"Missing"
J10-1003,E06-1039,0,0.0178223,"Missing"
J10-1003,P88-1011,0,0.0277095,"elied on a set of manually encoded schemas and chose the most appropriate one for a given story (Cullingford 1978; Dyer 1983; Leake 1989). For example, a system called BORIS (Dyer 1983) processed stories word-by-word to create a very rich semantic representation of them using Memory Organization Packets (MOPs) and Thematic Affect Units (TAUs). These knowledge structures were activated by means of a very detailed lexicon where each lexeme was associated with MOPs and TAUs it could invoke. Systems such as BORIS could not process stories that did not conform to schemas already at their disposal. Charniak and Goldman (1988) and Norvig (1989) attempted to circumvent this problem by learning to recognize more general structures. FAUSTUS (Norvig 1989) recognized six general classes of inferences by ﬁnding patterns of connectivity in a semantic network. It could be adapted to new kinds of documents by extending its knowledge base and not the underlying algorithm or patterns. Research in automatic story comprehension offered a number of important solutions for subsequent developments in artiﬁcial intelligence. No less important, it pointed out a number of challenges. All these systems required a formidable amount of"
J10-1003,P02-1022,0,0.00786825,"e devil—albeit for different reasons—the latter takes place in Moscow around 1930s and the two works are dramatically different. 76 Kazantseva and Szpakowicz Summarizing Short Stories 3. Identifying Important Entities During the ﬁrst stage of summary production the system identiﬁes important entities in stories. Initially, we planned to identify three types of entities: people, locations, and time stamps. During a preliminary exploration of the corpus, we analyzed 14 stories for the presence of surface indicators of characters, locations, and temporal anchors.4 We employed the GATE Gazetteer (Cunningham et al. 2002), and only considered entities it recognized automatically. The experiment revealed that the stories in the corpus contained multiple mentions of characters (on average, 64 mentions per story, excluding pronouns). On the other hand, the 14 stories contained only 22 location markers, mostly street names. Four stories had no identiﬁable location markers. Finally, merely four temporal anchors were identiﬁed in all 14 stories: two absolute (such as year) and two relative (e.g., Christmas). These ﬁndings support the intuitive idea that short stories revolve around their characters, even if the ulti"
J10-1003,P97-1020,0,0.174094,"ned that several researchers have looked into automatically determining various semantic properties of verbs (Siegel 1998b; Merlo et al. 2002). These approaches, however, attempt to determine properties of verbs viewed in isolation and do not deal with particular usages in the context of concrete sentences. That is why we cannot directly re-apply that research in determining the aspectual type of clauses. 8 By parent sentence we mean the sentence from which the clause is taken. 81 Computational Linguistics Volume 36, Number 1 Table 3 Privative featural identiﬁcation of aspectual classes after Dorr and Olsen (1997). Aspectual class State Activity Accomplishment Achievement Telic Dynamic + + + + + Durative Examples + + + know, believe paint, walk destroy notice, win Location-related features. In discourse such as ﬁction, not all tokens that the Gazetteer recognizes as markers of location denote locations. Location-related features help identify mentions of locations in each clause and verify that these mentions indeed denote a place. These features describe whether a clause contains a mention of a location and whether it is embedded in a prepositional phrase. The rationale for these features is that true"
J10-1003,P08-2049,0,0.0255658,"Missing"
J10-1003,J94-4002,0,0.137029,"n of Aspect We rely on aspect to select salient sentences that set out the background of a story. In this paper, the term aspect denotes the same concept as what Huddleston and Pullum (2002, page 118) call the situation type. The term refers to “different ways of viewing the internal temporal consistency of a situation” (Comrie 1976, page 3). Informally, the aspect of a clause suggests the temporal ﬂow of an event or a state and the speaker’s position with respect to it. A general aspectual classiﬁcation based on Huddleston and Pullum (2002) appears in Figure 4, with examples for each type. 5 Lappin and Leass (1994) present a rule-based algorithm for resolving pronominal anaphora. The algorithm suggests the most likely antecedent after taking into account the candidates’ syntactic function, recency, and absence or presence of parallelism and cataphora with the referent. It also enforces agreement between referent–antecedent pairs. 78 Kazantseva and Szpakowicz Summarizing Short Stories Figure 4 Aspectual hierarchy after Huddleston and Pullum (2002). The ﬁrst distinction is between states and events. Events are processes that go on in time and consist of successive phases (Vendler 1967, page 99). For insta"
J10-1003,W04-1013,0,0.0922691,"er than the random baseline but they fall far short of the quality of the manual summaries. During the second evaluative experiment, the machine-made summaries are compared against extracts created by people using sentence co-selection measures (precision, recall, and F-score). By sentence co-selection we mean measuring how many sentences found in manually created extracts are selected for inclusion in automatically produced summaries. The results suggest that our system outperforms all baselines, including state-of-the-art summarizers. The third part of the evaluation uses two ROUGE metrics (Lin 2004) to compare the machine-made and the baseline summaries with the model abstracts. The results suggest that these measures are not well suited for evaluating extractive indicative summaries of short stories. This paper is organized in the following manner. Section 2 gives a brief overview of the body of research in automatic story comprehension. Section 3 describes the process of identifying important entities in short stories. Section 4 introduces the notion of aspect, gives an overview of the system’s design, and discusses the linguistic motivation behind it. Section 5 describes the classiﬁca"
J10-1003,C00-1072,0,0.02133,"Missing"
J10-1003,N03-1020,0,0.0451124,"Missing"
J10-1003,E06-1038,0,0.00651011,"A few innovative research directions have emerged, including headline generation (Soricut and Marcu 2007), summarization of books (Mihalcea and Ceylan 2007), personalized summarization (D´ıaz and Gerv´as 2007), generation of tables-of-contents (Branavan, Deshpande, and Barzilay 2007), summarization of speech (Fuentes et al. 2005), dialogues (Zechner 2002), evaluative text (Carenini, Ng, and Pauls 2006), and biomedical documents (Reeve, Han, and Brooks 2007). In addition, more researchers have been venturing past purely extractive summarization (Krahmer, Marsi, and van Pelt 2008; Nomoto 2007; McDonald 2006). By and large, however, most research in text summarization still revolves around texts characterized by rigid structure. The better explored among such texts are news articles (Barzilay and McKeown 2005), medical documents (Elhadad et al. 2005), legal documents (Moens 2007), and papers in the area of computer science (Teufel and Moens 2002; Mei and Zhai 2008). Although summarizing these genres is a formidable challenge in itself, it excludes a continually increasing number of informal documents available electronically. Such documents, ranging from novels to personal Web pages, offer a wealt"
J10-1003,P08-1093,0,0.0135325,"evaluative text (Carenini, Ng, and Pauls 2006), and biomedical documents (Reeve, Han, and Brooks 2007). In addition, more researchers have been venturing past purely extractive summarization (Krahmer, Marsi, and van Pelt 2008; Nomoto 2007; McDonald 2006). By and large, however, most research in text summarization still revolves around texts characterized by rigid structure. The better explored among such texts are news articles (Barzilay and McKeown 2005), medical documents (Elhadad et al. 2005), legal documents (Moens 2007), and papers in the area of computer science (Teufel and Moens 2002; Mei and Zhai 2008). Although summarizing these genres is a formidable challenge in itself, it excludes a continually increasing number of informal documents available electronically. Such documents, ranging from novels to personal Web pages, offer a wealth of information that merits the attention of the text summarization community. ∗ School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ottawa, Ontario K1N 6N5, Canada. E-mail: ankazant@site.uottawa.ca. ∗∗ School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ottawa, Ontario K1N 6N5"
J10-1003,P02-1027,0,0.010765,"premodiﬁed by a noun phrase. The interest in such mentions is inspired by the fact that these constructions—appositions—often introduce new entities into the discourse (Vieira and Poesio 2000). For the same reasons, the system also establishes whether a character mention is nominal or pronominal (e.g., Jack vs. he), whether it is used in the genitive case (e.g., Jack’s) and, for common nouns, whether the mention is accompanied by an indeﬁnite article. 7 It must be mentioned that several researchers have looked into automatically determining various semantic properties of verbs (Siegel 1998b; Merlo et al. 2002). These approaches, however, attempt to determine properties of verbs viewed in isolation and do not deal with particular usages in the context of concrete sentences. That is why we cannot directly re-apply that research in determining the aspectual type of clauses. 8 By parent sentence we mean the sentence from which the clause is taken. 81 Computational Linguistics Volume 36, Number 1 Table 3 Privative featural identiﬁcation of aspectual classes after Dorr and Olsen (1997). Aspectual class State Activity Accomplishment Achievement Telic Dynamic + + + + + Durative Examples + + + know, believe"
J10-1003,D07-1040,0,0.0679132,"Missing"
J10-1003,N04-1019,0,0.0703944,"Missing"
J10-1003,W98-0702,0,0.155079,"ention that is premodiﬁed by a noun phrase. The interest in such mentions is inspired by the fact that these constructions—appositions—often introduce new entities into the discourse (Vieira and Poesio 2000). For the same reasons, the system also establishes whether a character mention is nominal or pronominal (e.g., Jack vs. he), whether it is used in the genitive case (e.g., Jack’s) and, for common nouns, whether the mention is accompanied by an indeﬁnite article. 7 It must be mentioned that several researchers have looked into automatically determining various semantic properties of verbs (Siegel 1998b; Merlo et al. 2002). These approaches, however, attempt to determine properties of verbs viewed in isolation and do not deal with particular usages in the context of concrete sentences. That is why we cannot directly re-apply that research in determining the aspectual type of clauses. 8 By parent sentence we mean the sentence from which the clause is taken. 81 Computational Linguistics Volume 36, Number 1 Table 3 Privative featural identiﬁcation of aspectual classes after Dorr and Olsen (1997). Aspectual class State Activity Accomplishment Achievement Telic Dynamic + + + + + Durative Example"
J10-1003,A97-1011,0,0.127752,"Missing"
J10-1003,J02-4002,0,0.104367,"alogues (Zechner 2002), evaluative text (Carenini, Ng, and Pauls 2006), and biomedical documents (Reeve, Han, and Brooks 2007). In addition, more researchers have been venturing past purely extractive summarization (Krahmer, Marsi, and van Pelt 2008; Nomoto 2007; McDonald 2006). By and large, however, most research in text summarization still revolves around texts characterized by rigid structure. The better explored among such texts are news articles (Barzilay and McKeown 2005), medical documents (Elhadad et al. 2005), legal documents (Moens 2007), and papers in the area of computer science (Teufel and Moens 2002; Mei and Zhai 2008). Although summarizing these genres is a formidable challenge in itself, it excludes a continually increasing number of informal documents available electronically. Such documents, ranging from novels to personal Web pages, offer a wealth of information that merits the attention of the text summarization community. ∗ School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ottawa, Ontario K1N 6N5, Canada. E-mail: ankazant@site.uottawa.ca. ∗∗ School of Information Technology and Engineering, University of Ottawa, 800 King Edward Ave., Ott"
J10-1003,W03-0508,0,0.0284995,"Missing"
J10-1003,J00-4003,0,0.0251218,"use contains a character mention and what its grammatical function is (subject, object, indirect object, or other). Mentions of characters early in the text tend to contain more salient background information. That is why character-related features reﬂect the position of a parent sentence8 relative to the sentence where the character is introduced. In addition, these features capture the presence of a character mention that is premodiﬁed by a noun phrase. The interest in such mentions is inspired by the fact that these constructions—appositions—often introduce new entities into the discourse (Vieira and Poesio 2000). For the same reasons, the system also establishes whether a character mention is nominal or pronominal (e.g., Jack vs. he), whether it is used in the genitive case (e.g., Jack’s) and, for common nouns, whether the mention is accompanied by an indeﬁnite article. 7 It must be mentioned that several researchers have looked into automatically determining various semantic properties of verbs (Siegel 1998b; Merlo et al. 2002). These approaches, however, attempt to determine properties of verbs viewed in isolation and do not deal with particular usages in the context of concrete sentences. That is"
J10-1003,J02-4003,0,0.0378881,"outcome of this evaluation suggests that the summaries are helpful in achieving the original objective. 1. Introduction In the last decade, automatic text summarization has become a popular research topic with a curiously restricted scope of applications. A few innovative research directions have emerged, including headline generation (Soricut and Marcu 2007), summarization of books (Mihalcea and Ceylan 2007), personalized summarization (D´ıaz and Gerv´as 2007), generation of tables-of-contents (Branavan, Deshpande, and Barzilay 2007), summarization of speech (Fuentes et al. 2005), dialogues (Zechner 2002), evaluative text (Carenini, Ng, and Pauls 2006), and biomedical documents (Reeve, Han, and Brooks 2007). In addition, more researchers have been venturing past purely extractive summarization (Krahmer, Marsi, and van Pelt 2008; Nomoto 2007; McDonald 2006). By and large, however, most research in text summarization still revolves around texts characterized by rigid structure. The better explored among such texts are news articles (Barzilay and McKeown 2005), medical documents (Elhadad et al. 2005), legal documents (Moens 2007), and papers in the area of computer science (Teufel and Moens 2002;"
J10-1003,P03-1048,0,\N,Missing
J10-1003,W01-0100,0,\N,Missing
N12-1022,J08-4004,0,0.175262,"idual chapters, so the only source of variance is the disagreement between annotators as to what is the appropriate level of detail for the task. Figure 1 confirms 1 We hired 30 students. Three did not complete the task. Figure 1: Distribution of segment counts across chapters. Agreementexpected = 1 X 2 nk (ic)2 k∈K (3) where i is the number of items to be classified in set I, k is the number of available categories in set K, c is the number of annotators, nik is the number of annotators who assign item i to category k, nk is the total number of items assigned to category k by all annotators (Artstein and Poesio, 2008, pp. 562-563). Effectively κ measures how much the annotators agree above what can be expected by chance. The value of κ is 0 where there is no agreement above chance and 1 where the annotators agree completely. other researchers’ findings: people find topical shifts at different levels of granularity (Malioutov and Barzilay, 2006; Gruenstein, Niekrasz, and Purver, 2005). We take this investigation further and explore whether there are patterns to this disagreement and how they can be interpreted and leveraged. 4.1 Inter-annotator Agreement In order to make sure that our guidelines are suffic"
N12-1022,D08-1035,0,0.26329,"Missing"
N12-1022,N12-1016,0,0.122082,"ajority opinion). To evaluate the output of topical segmenters is hard. There is disagreement between the annotators about the appropriate level of granularity and about the exact placement of segment boundaries. The task itself is also a little vague. Just as it is the case in automatic text summarization, generation 219 and other advanced NLP tasks, there is no single correct answer and the goal of a good evaluation metric is to reward plausible hypotheses and to penalize improbable ones. It is quite possible that a better metric than the one proposed here can be devised; see, for example, (Fournier and Inkpen, 2012)(Scaiano and Inkpen, 2012). We feel, however, that any reliable metric for evaluating segmentations must – in one manner or another – take into account more than one annotation and the prominence of segment breaks. Acknowledgments We thank William Klement and Chris Fournier for commenting on an early draft of this paper. The first author also thanks Chris Fournier and Martin Scaiano for insightful discussions about the evaluation of topical segmenters. This work is partially funded by the National Sciences and Engineering Research Council of Canada and by the Ontario Graduate Scholarship progr"
N12-1022,P03-1071,0,0.0472202,"t, they also note that people include segment boundaries at different rates. Gruenstein, Niekrasz, and Purver (2005) describe the process of annotating parts of two corpora of meeting transcripts: ICSI (Janin et al., 2003) and ISL (Burger, MacLaren, and Yu, 2002). Two people annotated the texts at two levels: major and minor, corresponding to the more and less important topic shifts. Topical shifts were to be annotated so as to allow an outsider to glance at the transcript and get the gist of what she missed. Not unlike our work, the authors report rather low overall interannotator agreement. Galley et al. (2003) also compiled a layer of annotation for topical shifts for part of the ICSI corpus, using a somewhat different procedure with three annotators. Malioutov and Barzilay (2006) created a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity. In these three projects, manual annotations were compiled into a single gold standard reference for use in evaluating and fine-tuning automatic segmenters. The work described in this paper is different in several ways. To the best of our knowledge, this is the first attempt to annotate"
N12-1022,2005.sigdial-1.13,0,0.0177681,"lassified in set I, k is the number of available categories in set K, c is the number of annotators, nik is the number of annotators who assign item i to category k, nk is the total number of items assigned to category k by all annotators (Artstein and Poesio, 2008, pp. 562-563). Effectively κ measures how much the annotators agree above what can be expected by chance. The value of κ is 0 where there is no agreement above chance and 1 where the annotators agree completely. other researchers’ findings: people find topical shifts at different levels of granularity (Malioutov and Barzilay, 2006; Gruenstein, Niekrasz, and Purver, 2005). We take this investigation further and explore whether there are patterns to this disagreement and how they can be interpreted and leveraged. 4.1 Inter-annotator Agreement In order to make sure that our guidelines are sufficiently clear and the annotators in fact annotate the same phenomenon, it is important to measure interannotator agreement (Artstein and Poesio, 2008). This is particularly important given the fact that the resulting corpus is intended as a benchmark dataset for evaluation of automatic segmenters. When looking at inter-annotator agreement independently of the domain, the"
N12-1022,D11-1026,1,0.893981,"segmenters with and without considering the importance of the individual breaks, and emphasize the more insightful nature of the latter analysis. 1 Introduction Topical segmentation is a useful intermediate step in many high-level NLP applications such as information retrieval, automatic summarization and question answering. It is often necessary to split a long document into topically continuous segments. Segmentation may be particularly beneficial when working with documents without overt structure: speech transcripts (Malioutov and Barzilay, 2006), newswire (Misra et al., 2011) or novels (Kazantseva and Szpakowicz, 2011). The customary approach The evaluation of text segmentation remains an open research problem. It is a tradition to compile a gold-standard segmentation reference using one or more annotations created by humans. If an automatic segmenter agrees with the reference, it is rewarded, otherwise it is penalized (see Section 4 for details). The nature of the task, however, is such that creating and applying a reference segmentation is far from trivial. The identification of topical shifts requires discretization of a continuous concept – how much the topic changes between two adjacent units. That is"
N12-1022,P06-1004,0,0.749096,"etric. We discuss the experimental results of evaluating several topical segmenters with and without considering the importance of the individual breaks, and emphasize the more insightful nature of the latter analysis. 1 Introduction Topical segmentation is a useful intermediate step in many high-level NLP applications such as information retrieval, automatic summarization and question answering. It is often necessary to split a long document into topically continuous segments. Segmentation may be particularly beneficial when working with documents without overt structure: speech transcripts (Malioutov and Barzilay, 2006), newswire (Misra et al., 2011) or novels (Kazantseva and Szpakowicz, 2011). The customary approach The evaluation of text segmentation remains an open research problem. It is a tradition to compile a gold-standard segmentation reference using one or more annotations created by humans. If an automatic segmenter agrees with the reference, it is rewarded, otherwise it is penalized (see Section 4 for details). The nature of the task, however, is such that creating and applying a reference segmentation is far from trivial. The identification of topical shifts requires discretization of a continuou"
N12-1022,J97-1005,0,0.42084,"lip-side of this definition is identifying topic shifts – places where the topic shifts significantly or abruptly. In the context of this paper we allow ourselves to use these two definitions interchangeably, sometimes talking about identifying topic shifts, at other times – about identifying topically continuous segments. While the theoretical correctness of such usage remains questionable, it is sufficient for the purpose of our discussion, and it is in line with the literature on the topic. There is a number of corpora annotated for the presence of topical shifts by one or more annotators. Passonneau and Litman (1997) describe an experiment where seven untrained annotators were asked to find discourse segments in a corpus of transcribed narratives about a movie. While the authors show that the agreement is significant, they also note that people include segment boundaries at different rates. Gruenstein, Niekrasz, and Purver (2005) describe the process of annotating parts of two corpora of meeting transcripts: ICSI (Janin et al., 2003) and ISL (Burger, MacLaren, and Yu, 2002). Two people annotated the texts at two levels: major and minor, corresponding to the more and less important topic shifts. Topical sh"
N12-1022,J02-1002,0,0.717894,"of disagreement. When a program misses or misplaces a prominent topic shift – a segment boundary marked by all annotators – it should be penalized more than if it was mistaken about a boundary marked by one person. Similarly, a false positive in the region where none of the annotators found a change in topic is worse than a boundary inserted in a place where at least one person perceived a topic change. We suggest that it is important to use all available reference segmentations instead of compiling them into a single gold standard. We show how a small modification to the popular windowDiff (Pevzner and Hearst, 2002) metric can allow considering multiple annotations at once. To demonstrate the increased interpretive power of such evaluation we run and evaluate several stateof-the art segmenters on the corpus described in this work. We evaluate their performance first in a conventional manner – by combining all available references into one – and then by using the proposed modification. Comparing the results suggests that the information provided by this method differs from what existing methods provide. Section 2 gives a brief background on text segmentation. Section 3 describes the corpus and how it was"
N12-1022,N12-1038,0,0.0282144,"Missing"
P08-1048,C04-1051,0,0.0449843,"gies (among them WordNet). We have shown that given this style of text representation both versions of Roget’s Thesaurus work comparably to WordNet. All three perform fairly well compared to the baseline Simple method. Once again, the 1987 version is superior to the 1911 version, but the 1911 version still works quite well. We hope to investigate further the representation of sentences and other short texts using Roget’s Thesaurus. These kinds of measurements can help with problems such as identifying relevant sentences for extractive text summarization, or possibly paraphrase identification (Dolan et al., 2004). Another – longer-term – direction of future work could be merging Roget’s Thesaurus with WordNet. We also plan to study methods of automatically updating the 1911 Roget’s Thesaurus with modern words. Some work has been done on adding new terms and relations to WordNet (Snow et al., 2006) and FACTOTUM (O’Hara and Wiebe, 2003). Similar methods could be used for identifying related terms and assigning them to a correct semicolon group or paragraph. Acknowledgments Our research is supported by the Natural Sciences and Engineering Research Council of Canada and the University of Ottawa. We thank"
P08-1048,O97-1002,0,0.0174714,"ely, for example, whether the phrase “change of direction” should be indexed only as a whole, or as all of “change”, “of”, “direction” and “change of direction”. The outcome of this experiment appears in Table 4. There is a clear improvement: breaking phrases up gives superior results on all three data sets, for both versions of Roget’s. In the remaining experiments, we have each word in a phrase indexed. We compare the results for the 1911 and 1987 Roget’s Thesauri with a variety of WordNet-based semantic relatedness measures – see Table 5. We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997), Resnik (Resnik, 1995), Lin (Lin, 1998), W&P (Wu and Palmer, 1994), L&C (Leacock and Chodorow, 1998), H&SO (Hirst and St-Onge, 1998), Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002), and finally Vector and Vector Pair (Patwardhan, 2003). The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen’s Semantic Distance software package (Pedersen et al., 2004). The results suggest that neither version of Roget’s is best for these data sets. In fact, the Vector method is superior on all three sets, and t"
P08-1048,N04-3012,0,0.0751511,"Missing"
P08-1048,P06-1101,0,0.0475729,"Missing"
P08-1048,P94-1019,0,0.0969849,"dexed only as a whole, or as all of “change”, “of”, “direction” and “change of direction”. The outcome of this experiment appears in Table 4. There is a clear improvement: breaking phrases up gives superior results on all three data sets, for both versions of Roget’s. In the remaining experiments, we have each word in a phrase indexed. We compare the results for the 1911 and 1987 Roget’s Thesauri with a variety of WordNet-based semantic relatedness measures – see Table 5. We consider 10 measures, noted in the table as J&C (Jiang and Conrath, 1997), Resnik (Resnik, 1995), Lin (Lin, 1998), W&P (Wu and Palmer, 1994), L&C (Leacock and Chodorow, 1998), H&SO (Hirst and St-Onge, 1998), Path (counts edges between synsets), Lesk (Banerjee and Pedersen, 2002), and finally Vector and Vector Pair (Patwardhan, 2003). The latter two work with large vectors of cooccurring terms from a corpus, so WordNet is only part of the system. We used Pedersen’s Semantic Distance software package (Pedersen et al., 2004). The results suggest that neither version of Roget’s is best for these data sets. In fact, the Vector method is superior on all three sets, and the Lesk algorithm performs very closely to Roget’s 1987. Even on th"
P98-1015,P95-1007,0,0.0451269,"postmodifying prepositional phrases Mk is the preposition. For appositives, Mk is the symbol appos. The (M, H, Mk) triples for examples (9), (10) and (11) appear in Table 2. Noun Modifier Bracketing Before assigning NMRs, the system must bracket the head noun and the premodifier sequence into modifier-head pairs. Example (2) shows the bracketing for noun phrase (1). (1) dynamic high impedance microphone (2) (dynamic ((high impedance) microphone)) The bracketing problem for noun-noun-noun compounds has been investigated by Liberrnan & Sproat (1992), Pustejovsky et al. (1993), Resnik (1993) and Lauer (1995) among others. Since the NMR analyzer must handle premodifier sequences of any length with both nouns and adjectives, it requires more general techniques. Our semi-automatic bracketer (Barker, 1998) allows for any number of adjective or noun premodifiers. After bracketing, each non-atomic element of a bracketed pair is considered a subphrase of the original phrase. The subphrases for the bracketing in (2) appear in (3), (4) and (5). (9) monitor cable plug (10) large piece of chocolate cake (11) my brother, a friend to all young people To assign an NMR to a triple (M, H, Mk), the system looks f"
P98-1015,J93-2005,0,0.00848535,"item links the premodifier to the head. For postmodifying prepositional phrases Mk is the preposition. For appositives, Mk is the symbol appos. The (M, H, Mk) triples for examples (9), (10) and (11) appear in Table 2. Noun Modifier Bracketing Before assigning NMRs, the system must bracket the head noun and the premodifier sequence into modifier-head pairs. Example (2) shows the bracketing for noun phrase (1). (1) dynamic high impedance microphone (2) (dynamic ((high impedance) microphone)) The bracketing problem for noun-noun-noun compounds has been investigated by Liberrnan & Sproat (1992), Pustejovsky et al. (1993), Resnik (1993) and Lauer (1995) among others. Since the NMR analyzer must handle premodifier sequences of any length with both nouns and adjectives, it requires more general techniques. Our semi-automatic bracketer (Barker, 1998) allows for any number of adjective or noun premodifiers. After bracketing, each non-atomic element of a bracketed pair is considered a subphrase of the original phrase. The subphrases for the bracketing in (2) appear in (3), (4) and (5). (9) monitor cable plug (10) large piece of chocolate cake (11) my brother, a friend to all young people To assign an NMR to a tripl"
R13-1058,broda-etal-2012-kpwr,1,0.766015,"Missing"
R13-1058,broda-etal-2012-tools,1,0.859555,"owa´c ‘rule’ – władca ‘ruler’); AGENT (spawacz ‘welder’ – spawa´c ‘weld’); IN STRUMENT (nadajnik ‘transmitter’ – nadawa´c ‘transmit’); DIMINUTIVE (córeczka ‘little daughter’ – córka ‘daughter’).9 2.2 The construction process • whether a given lemma is correct in Polish (e.g., tagger mistakes are weeded out); • how many LUs should be distinguished – whether all existing senses appear in usage examples or WordnetWeaver’s suggestions; • how to describe a given LU by plWordNet relations – what relation types should be used. Wordnet construction is rather like writing a dictionary (Fellbaum, 1998; Broda et al., 2012b). Lexicography distinguishes four phases: data collection, selection, analysis and presentation (Svensén, 2009). In the plWordNet project, language technologies support all four phases. Professional linguists under the supervision of senior coordinators work with WordnetLoom, a Web application. This graph-based wordnet editor allows visual browsing and concurrent editing. Many semiautomatic tools are integrated into WordnetLoom. In the data collection phase, a large corpus is essential (Wynne, 2005). A multi-source corpus with 1.8 billion tokens, the foundation of plWordNet’s systematic grow"
R13-1058,lis-2012-polish,0,0.0619675,"ear perfectly usable in linking other wordnets. The Ihyponymy links are now a clear sign of gaps which can be repaired in the further stages of the development of the networks. Mapping plWordNet to PWN also opens up the possibility of establishing links to other wordnets already linked to PWN. 5 Applications Freely available for any purpose on a licence identical to the PWN licence, plWordNet has already proven its value in at least 16 research applications and in many publication which cite it. The verb portion of plWordNet was used in semantic annotation in a corpus of referential gestures (Lis, 2012) and in a lexicon of semantic valency frames (Hajnicz, 2011; Hajnicz, 2012). In the latter, plWordNet domains were also used in algorithms of verb classification. In (Maciołek, 2010; Maciołek and Dobrowolski, 2013) plWordNet is used to extend a set of features for text mining from Web pages. In (Wróblewska et al., 2013) 20 Glosses for all synsets are a relatively late addition to PWN. We have only recently begun to introduce them into plWordNet. 449 study on phonesthemes.”), Affect Analysis (multilingual systems), humour analysis, development of Polish Link Grammar, and plWordNet as an object"
R13-1058,mititelu-2012-adding,0,0.0341853,"), terminology extraction and clustering (Mykowiecka and Marciniak, 2012), automated extraction of opinion attribute lexicons from product descriptions (Wawer and Gołuchowski, 2012), named entity recognition, word-sense disambiguation, extraction of semantic relations (Gołuchowski and Przepiórkowski, 2012), temporal information (Jarz˛ebowski and Przepiórkowski, 2012) and anaphora resolution. Open Multilingual Wordnet (Bond, 2013) now includes plWordNet. It is referred to in other work on wordnets and semantic lexicons (Pedersen et al., 2009; Lindén and Carlson, 2010; Borin and Forsberg, 2010; Mititelu, 2012; Zafar et al., 2012; Šojat et al., 2012). 6 The resource has attracted about 450 registered individual and institutional users (registration upon download is not mandatory). The plWordNet Web page and Web service have had tens of thousands of visitors (hundreds of thousands of searches). The intended use includes 70 commercial applications, and 50 scientific and educational applications (at all levels: university, high school and primary school). The declared topics of scientific applications include semantic word similarity calculation, multilingual wordsense disambiguation, text classificat"
R13-1058,C12-1119,0,0.0587364,"nguistics and even as an illustration of linguistic notions in education in primary and secondary schools. plWordNet was the basis for building a mapping between a lexicon and an ontology. Miłkowski (2010) included plWordNet in a set of dictionaries in his proofreading tool. There are applications of plWordNet in word-to-word similarity measures utilised in research on ontologies (Lula and Paliwoda-P˛ekosz, 2009) or in calculating text similarity (Siemi´nski, 2012). As a semantic lexicon, plWordNet has been useful in text classification (Maciołek, 2010), terminology extraction and clustering (Mykowiecka and Marciniak, 2012), automated extraction of opinion attribute lexicons from product descriptions (Wawer and Gołuchowski, 2012), named entity recognition, word-sense disambiguation, extraction of semantic relations (Gołuchowski and Przepiórkowski, 2012), temporal information (Jarz˛ebowski and Przepiórkowski, 2012) and anaphora resolution. Open Multilingual Wordnet (Bond, 2013) now includes plWordNet. It is referred to in other work on wordnets and semantic lexicons (Pedersen et al., 2009; Lindén and Carlson, 2010; Borin and Forsberg, 2010; Mititelu, 2012; Zafar et al., 2012; Šojat et al., 2012). 6 The resource h"
R13-1058,C12-2101,1,0.360293,"umber of instances of I-relations in plWordNet 2.0 and in GermaNet 8.0, another partially manually constructed and mapped wordnet.19 I-synonymy, a primary relation in both wordnets has a comparable number of instances. It is the most frequent relation in GermaNet, while in plWordNet it has been overtaken by I-hyponymy. The latter statistic can be explained by profound differences in the struc18 Two LUs mean roughly the same but belong to different stylistic registers. 19 We thank Verena Henrich for providing us with the relevant GermaNet data. A partial mapping of plWordNet onto PWN is ready (Rudnicka et al., 2012). A hierarchically 448 Relation type I-synonymy I-hyponymy I-hypernymy I-meronymy I-holonymy I-near synonymy I-inter-register synonymy plWordNet 2.0 14240 22873 3329 1732 394 923 GermaNet 8.0 15259 1397 760 126 52 3389 522 — Table 8: Inter-lingual relation count (instances) in plWordNet and in GermaNet. ture and content of plWordNet and PWN, discovered during mapping and discussed below. In GermaNet, I-hyponymy has quite few instances. On the other hand, the second largest relation in GermaNet is I-near synonymy. There are lexico-semantic and lexicogrammatical differences between English and P"
R15-1056,J08-4004,0,0.0329758,"Missing"
R15-1056,J08-3001,0,0.0416163,"Missing"
R15-1092,baccianella-etal-2010-sentiwordnet,0,0.535765,"Missing"
R15-1092,de-albornoz-etal-2012-sentisense,0,0.0912499,"Missing"
R15-1092,P14-2063,0,0.0774922,"Missing"
R15-1092,W10-3208,0,0.0837259,"Missing"
R15-1092,C12-1119,0,0.102611,"Missing"
R15-1092,esuli-sebastiani-2006-sentiwordnet,0,0.731205,"nnotation process, and introduces the resulting resource, plWordNetemo. We discuss the selection of the material for the pilot study, show the distribution of annotations across the wordnet, and consider the statistics, including interannotator agreement and the resolution of disagreement. 1 Introduction The Polish wordnet, plWordNet (Piasecki et al., 2009; Maziarz et al., 2013), is very large and comprehensive, with well over 150,000 synsets 1 The term lexical unit will be abbreviated to LU throughout this paper. 2 This annotation is already on a scale several times larger than SentiWordNet (Esuli and Sebastiani, 2006). 721 Proceedings of Recent Advances in Natural Language Processing, pages 721–730, Hissar, Bulgaria, Sep 7–9 2015. 2 Sentiment and Affect Annotations in Wordnets relations. The authors’ visualisation and editing tools, designed to allow relatively easy expansion and adaptation, did not add much to the resource, so every user must enlarge it further to make it really applicable. Several sentiment lexicons are available for English, but hardly any for most other languages. Chen and Skiena (2014) have found 12 publicly available sentiment lexicons for 5 languages; there are none for Polish. Some"
R15-1092,C12-2101,1,0.811373,"y equal across different types of polarity. A possible explanation: it is harder to read the meaning of adjectival LUs from plWordNet, and the annotators were more careful in reading the wordnet structures exactly. 6 on sentiment polarity propagation along the wordnet graph. The development of plWordNet has been independent of PWN, and the amount of sentiment annotation in our pilot project exceeds that in SentiWordNet and WordNet-Affect. It might therefore be interesting to compare our annotation with the automatic annotation in those wordnets, using the manual mapping of plWordNet onto PWN (Rudnicka et al., 2012). Acknowledgments Work financed by the Polish Ministry of Science and Higher Education, a program in support of scientific units involved in the development of a European research infrastructure for the humanities and social sciences in the scope of the consortia CLARIN ERIC and ESS-ERIC, 2015-2016. Heartfelt thanks to our annotators: Ada Zajaczkowska, ˛ Anna Nizi´nska, Kamil Wabnic, Monika Górka, Amelia Kiełbawska and Joanna Wieczorek. Conclusions The resource we have constructed is a first, important step towards sentiment annotation of the whole plWordNet. That is because the achieved size"
R15-1092,strapparava-valitutti-2004-wordnet,0,0.320501,"Missing"
R15-1092,W11-1710,0,0.68128,"Missing"
S07-1003,W04-3205,0,0.0491922,"ierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications. For example, the gene-gene relation scheme of Stephens et al. (2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text. We hav"
S07-1003,C92-2082,0,0.0345238,"a variety of methods (since we work with relations between nominals, the part of speech is always noun). We have used WordNet 3.0 on the Web and sense index tags. We chose the following semantic relations: Cause-Effect, Content-Container, InstrumentAgency, Origin-Entity, Part-Whole, ProductProducer and Theme-Tool. We wrote seven detailed definitions, including restrictions and conventions, plus prototypical positive and near-miss negative examples. For each relation separately, we based data collection on wild-card search patterns that Google allows. We built the patterns manually, following Hearst (1992) and Nakov and Hearst (2006). Instances of the relation Content-Container, for example, come up in response to queries such as “* contains *”, “* holds *”, “the * in the *”. Following the model of the Senseval-3 English Lexical Sample Task, we set out to collect 140 training and at least 70 test examples per relation, so we had a number of different patterns to ensure variety. We also aimed to collect a balanced number of positive and negative examples. The use of heuristic patterns to search for both positive and negative examples 14 should naturally result in negative examples that are near"
S07-1003,J02-3004,0,0.0275234,"oun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications. For example, the gene-gene relation scheme of Stephens et al. (2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text. We have created a benchmark data set to allow the evaluation of different semantic relation classification algorithms. We do not presume to propose a single classification scheme, however allurin"
S07-1003,W04-2609,1,0.951787,"cine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work f"
S07-1003,W01-0511,0,0.263052,"cer relation. The classification occurs in the context of a sentence in a written English text. Algorithms for classifying semantic relations can be applied in information retrieval, information extraction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2"
S07-1003,P02-1032,0,0.0943661,"traction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and"
S07-1003,H05-1047,0,0.0359726,"their results. There were 14 teams who submitted 15 systems. 1 Task Description and Related Work The theme of Task 4 is the classification of semantic relations between simple nominals (nouns or base noun phrases) other than named entities – honey bee, for example, shows an instance of the ProductProducer relation. The classification occurs in the context of a sentence in a written English text. Algorithms for classifying semantic relations can be applied in information retrieval, information extraction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, w"
S07-1003,H91-1061,0,\N,Missing
S10-1006,W09-1401,0,0.0287746,"for each semantic relation. Here, we describe the general guidelines, which delineate the scope of the data to be collected and state general principles relevant to the annotation of all relations.1 Our objective is to annotate instances of semantic relations which are true in the sense of holding in the most plausible truth-conditional interpretation of the sentence. This is in the tradition of the Textual Entailment or Information Validation paradigm (Dagan et al., 2009), and in contrast to “aboutness” annotation such as semantic roles (Carreras and M`arquez, 2004) or the BioNLP 2009 task (Kim et al., 2009) where negated relations are also labelled as positive. Similarly, we exclude instances of semantic relations which hold only in speculative or counterfactural scenarios. In practice, this means disallowing annotations within the scope of modals or negations, e.g., “Smoking may/may not have caused cancer in this case.” We accept as relation arguments only noun phrases with common-noun heads. This distinguishes our task from much work in Information Extraction, which tends to focus on specific classes of named entities and on considerably more finegrained relations than we do. Named entities ar"
S10-1006,W05-0620,0,\N,Missing
S10-1007,P07-1072,0,0.0565426,"ited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose a"
S10-1007,I05-1082,1,0.151111,"a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected dire"
S10-1007,P06-2064,1,0.272527,"(MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edition of SemEval. This pa"
S10-1007,E03-1073,0,0.0115181,"ample, malaria mosquito is a ‘mosquito that carries malaria’. Evaluating the quality of such paraphrases is the theme of Task 9 at SemEval-2010. This paper describes some background, the task definition, the process of data collection and the task results. We also venture a few general conclusions before the participating teams present their systems at the SemEval-2010 workshop. There were 5 teams who submitted 7 systems. 1 Introduction Noun compounds (NCs) are sequences of two or more nouns that act as a single noun,1 e.g., stem cell, stem cell research, stem cell research organization, etc. Lapata and Lascarides (2003) observe that NCs pose syntactic and semantic challenges for three basic reasons: (1) the compounding process is extremely productive in English; (2) the semantic relation between the head and the modifier is implicit; (3) the interpretation can be influenced by contextual and pragmatic factors. Corpus studies have shown that while NCs are very common in English, their frequency distribution follows a Zipfian or power-law distribution and the majority of NCs encountered will be rare types (Tanaka and Baldwin, 2003; Lapata and Lascarides, 2003; Bald´ S´eaghdha, 2008). As a win and Tanaka, 2004;"
S10-1007,C94-2125,0,0.271982,"Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edit"
S10-1007,W04-2609,0,0.051487,"ing, NC semantics plays a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popular"
S10-1007,P08-1052,1,0.415224,"e, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edition of SemEval. This paper gives a bird’s-eye view of the task. Section 2"
S10-1007,W06-3813,1,0.819134,"lex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-bas"
S10-1007,W07-1108,1,0.889705,"Missing"
S10-1007,D08-1027,0,0.00993176,"Missing"
S10-1007,C08-1011,1,\N,Missing
S10-1007,W03-1803,0,\N,Missing
S10-1007,W04-0404,0,\N,Missing
S10-1007,P84-1109,0,\N,Missing
S10-1007,W01-0511,0,\N,Missing
S13-2025,W04-0404,0,0.0532624,"Missing"
S13-2025,C08-1011,1,0.662917,"ages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve rich and specific meanings."
S13-2025,W09-2416,1,0.899683,"Missing"
S13-2025,P07-1072,0,0.203668,"Missing"
S13-2025,P06-2064,0,0.395087,"Missing"
S13-2025,W04-2609,0,0.0835979,"located in Geneva. 138 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE)"
S13-2025,P08-1052,1,0.483189,"uation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve r"
S13-2025,E09-1071,1,0.900906,"Missing"
S13-2025,C08-1082,1,0.890902,"Missing"
S13-2025,W03-1803,0,0.161511,"Missing"
S13-2025,P10-1070,0,0.829879,"Missing"
W04-1005,N03-1020,0,0.043042,"ir of test documents: their phrases, and ultimately, their individual tokens. Phrases were extracted by applying a 987-item stop list developed by the authors (Copeck and Szpakowicz 2003) to the test documents. Each collocation separated by stop words is taken as a phrase1 . Test documents were tokenized by breaking the text on white space and trimming off punctuation external to the token. Instances of each sort of item were recorded in a hash table and written to file. Tokens are an obvious and unambiguous baseline for lexical agreement, one used by such summary evaluation systems as ROUGE (Lin and Hovy, 2003). On the other hand, it is important to explain what we mean by units we call phrases; they should not be confused with syntactically correct constituents such as noun phrases or verb phrases. Our units often are not syntactically well-formed. Adjacent constituents not separated by a stop word are unified, single constituents are divided on any embedded stop word, and those composed entirely of stop words are simply missed. Our phrases, however, are not n-grams. A 10word summary has precisely 9 bigrams but, in this study, only 3.4 phrases on average (Table 2). On the continuum of grammatic ali"
W04-1005,W02-0406,0,0.0735612,"Missing"
W04-1005,W03-0508,0,0.0359886,"Missing"
W06-0702,W01-0100,0,0.740787,"Missing"
W06-0702,N04-1019,0,0.0555115,"Missing"
W06-0702,A97-1011,0,0.0941055,"Missing"
W06-0702,W03-0508,0,0.048543,"Missing"
W06-3805,O01-1003,0,0.0713771,"Missing"
W06-3805,H05-1049,0,0.0746317,"Missing"
W06-3805,P04-3020,0,0.0271221,"nformation that can be extracted from a topic description. In particular, we look at connections between open-class words. A dependency parser, MiniPar (Lin, 1998), builds a dependency relation graph for each sentence. We apply such graphs in two ways. We match a graph that covers the entire topic description against the graph for each sentence in the collection. We also extract all pairs of open-class words from the topic description, and check whether they are connected in the sentence graphs. Both methods let us rank sentences; the top-ranking ones go into a summary Erkan and Radev (2004), Mihalcea (2004), Mihalcea and Tarau (2004) introduced graph methods for summarization, word sense disambiguation and other NLP applications. The summarization graph-based systems implement a form of sentence ranking, based on the idea of prestige or centrality in social networks. In this case the network consists of sentences, and significantly similar sentences are interconnected. Various measures (such as node degree) help find the most central sentences, or to score each sentence. In topic-driven summarization, one or more sentences or questions describe an information need which the summaries must addres"
W06-3805,N04-1019,0,0.0391172,"ents for a given topic, one sentence per line, cleaned of XML tags. We process each file with MiniPar, and post-process the output similarly to the topics. For documents we keep the list of dependency relations but not a separate list of words. This processing also gives one file per topic, each sentence followed by its list of dependency relations. 3.3 Summary Content Units The DUC 2005 summary evaluation included an analysis based on Summary Content Units. SCUs are manually-selected topic-specific summaryworthy phrases which the summarization systems are expected to include in their output (Nenkova and Passonneau, 2004; Copeck and Szpakowicz, 2005). The SCUs for 20 of the test topics became available after the challenge. We use the SCU data to measure the performance of our graph-matching and path-search algorithms: the total number, weight and number of unique SCUs per summary, and the number of negative SCU sentences, explicitly marked as not relevant to the summary. 30 The overall score is S = SN + W eightF actor ∗ SE , where W eightF actor ∈ {0, 1, 2, ..., 15, 20, 50, 100}. Varying the weight factor allows us to find various combinations of node and edge score matches which work best for sentence extrac"
W06-3805,W04-3252,0,\N,Missing
W06-3813,P98-1013,0,0.0863126,"s an old idea. The first chronicled endeavour to connect text elements and organise connections between them goes back to the 5th century B.C. and the work of Panini1 . He was a grammarian who analysed Sanskrit (Misra, 1966). The idea resurfaced forcefully at several points in the more recent history of linguistic research (Tesni`ere, 1959; Gruber, 1965; Fillmore, 1968). Now it has the attention of many researchers in natural language processing, as shown by recent research in semantic parsing and semantic 1 7 th The sources date his work variously between the 5th and century. role labelling (Baker et al., 1998; Kipper et al., 2000; Carreras and Marquez, 2004; Carreras and Marquez, 2005; Atserias et al., 2001; Shi and Mihalcea, 2005). Graph-like structures are a natural way of organising one’s impressions of a text seen from the perspective of connections between its simpler constituents of varying granularity, from sections through paragraphs, sentences, clauses, phrases, words to morphemes. In this work we pursue a well-known and often tacitly assumed line of thinking: connections at the syntactic level reflect connections at the semantic level (in other words, syntax carries meaning). Anecdotal s"
W06-3813,J02-3001,0,0.0131234,"sign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion. Discussion and conclusions appear in Section 6. 2 Related Work Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components: events, entities and modifiers (Clark and Porter, 1997). The system’s interface facilitates the expert’s task of creating and manipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semant"
W06-3813,P05-1072,0,0.0123887,"anipulating structures which represent domain concepts, and assigning them relations from a relation dictionary. In current work on semantic relation analysis, the focus is on semantic roles – relations between verbs and their arguments. Most approaches rely on VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 1998) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions (Carreras and Marquez, 2004; Carreras and Marquez, 2005) and also (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Shi and Mihalcea, 2005). These systems share two ideas which make them different from the approach presented here: they all analyse verb-argument relations, and they all use machine learning or probabilistic approaches (Pradhan et al., 2005) to assign a label to a new instance. Labelling every instance relies on the same previously encoded knowledge (see (Carreras and Marquez, 2004; Carreras and Marquez, 2005) for an overview of the systems in the semantic role labelling competitions from 2004 and 2005). Pradhan 82 et al. (2005) combine the outputs of multiple parsers to extract reliable syn"
W06-3813,W01-0511,0,0.0834081,"nd shows how often the heuristic was used when processing the input text. We show in detail our findings about syntactic levels (how often graph matching helped assign a relation between two clauses, a verb and its arguments, or a noun and its modifier) and about the accuracy of the suggestion. Discussion and conclusions appear in Section 6. 2 Related Work Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts (Baker et al., 1998). In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al., 2002)). Lists of semantic relations are designed to capture salient domain information. In the Rapid Knowledge Formation Project (RKF) a support system was developed for domain experts. It helps them build complex knowledge bases by combining components: events, entities and modifiers (Clark and Porter, 1997). The system’s interface facilitates the expert"
W06-3813,C98-1013,0,\N,Missing
W06-3813,P02-1032,0,\N,Missing
W09-2415,W04-3205,0,0.0639941,"unrelated semantic roles. There is a rudimentary frame hierarchy that defines mappings between roles of individual frames,5 but it is far from complete. The situation is similar in PropBank. PropBank does use a small number of semantic roles, but these are again to be interpreted at the level of individual predicates, with little cross-predicate generalization. In contrast, all of the semantic relation inventories discussed in Section 1 contain fewer than 50 types of semantic relations. More generally, semantic relation inventories attempt to generalize relations across wide groups of verbs (Chklovski and Pantel, 2004) and include relations that are not verbcentered (Nastase and Szpakowicz, 2003; Moldovan et al., 2004). Using the same labels for similar semantic relations facilitates supervised learning. For example, a model trained with examples of sell relations should be able to transfer what it has learned to give relations. This has the potential of adding 5 For example, it relates the B UYER role of the C OM frame (verb sell ) to the R ECIPIENT role of the G IVING frame (verb give). MERCE SELL 97 1. People in Hawaii might be feeling &lt;e1>aftershocks&lt;/e1> from that powerful &lt;e2>earthquake&lt;/e2> for weeks"
W09-2415,P08-1027,0,0.0924452,"tical NLP settings, where any relation can hold between a pair of nominals which occur in a sentence or a discourse. 2.4 Summary While there is a substantial amount of work on relation extraction, the lack of standardization makes it difficult to compare different approaches. It is known from other fields that the availability of standard benchmark data sets can provide a boost to the advancement of a field. As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; Katrenko and Adriaans, 2008; Nakov and Hearst, 2008; ´ S´eaghdha and Copestake, 2008). O Our objective is to build on the achievements of SemEval-2007 Task 4 while addressing its shortcomings. In particular, we consider a larger set of semantic relations (9 instead of 7), we assume a proper multi-class classification setting, we emulate the effect of an “open” relation inventory by means of a tenth class OTHER, and we will release to the research community a data set with a considerably 1 http://www.itl.nist.gov/iad/mig/tests/ ace/ 2 Although it was not designed for a multi-class set-up, som"
W09-2415,J02-3001,0,0.0386032,"This is motivated by modelling considerations. Presumably, the data for OTHER will be very nonhomogeneous. By including it, we force any model of the complete data set to correctly identify the decision boundaries between the individual relations and “everything else”. This encourages good generalization behaviour to larger, noisier data sets commonly seen in real-world applications. 3.1 Semantic Relations versus Semantic Roles There are three main differences between our task (classification of semantic relations between nominals) and the related task of automatic labeling of semantic roles (Gildea and Jurafsky, 2002). The first difference is to do with the linguistic phenomena described. Lexical resources for theories of semantic roles such as FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have been developed to describe the linguistic realization patterns of events and states. Thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns. In contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominals in this task. Also, semantic role descriptions typically relate an event to a set of mu"
W09-2415,S07-1003,1,0.384989,"Missing"
W09-2415,C92-2082,0,0.060649,"tion will take place in two rounds. In the first round, we will do a coarse-grained search for positive examples for each relation. We will collect data from the Web using a semi-automatic, pattern-based search procedure. In order to ensure a wide variety of example sentences, we will use several dozen patterns per relation. We will also ensure that patterns retrieve both positive and negative example sentences; the latter will help populate the OTHER relation with realistic near-miss negative examples of the other relations. The patterns will be manually constructed following the approach of Hearst (1992) and Nakov and Hearst (2008).6 The example collection for each relation R will be passed to two independent annotators. In order to maintain exclusivity of relations, only examples that are negative for all relations but R will be included as positive and only examples that are negative for all nine relations will be included as OTHER. Next, the annotators will compare their decisions and assess inter-annotator agreement. Consensus will be sought; if the annotators cannot agree on an example it will not be included in the data set, but it will be recorded for future analysis. Finally, two othe"
W09-2415,P08-2047,0,0.0143881,"relation can hold between a pair of nominals which occur in a sentence or a discourse. 2.4 Summary While there is a substantial amount of work on relation extraction, the lack of standardization makes it difficult to compare different approaches. It is known from other fields that the availability of standard benchmark data sets can provide a boost to the advancement of a field. As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; Katrenko and Adriaans, 2008; Nakov and Hearst, 2008; ´ S´eaghdha and Copestake, 2008). O Our objective is to build on the achievements of SemEval-2007 Task 4 while addressing its shortcomings. In particular, we consider a larger set of semantic relations (9 instead of 7), we assume a proper multi-class classification setting, we emulate the effect of an “open” relation inventory by means of a tenth class OTHER, and we will release to the research community a data set with a considerably 1 http://www.itl.nist.gov/iad/mig/tests/ ace/ 2 Although it was not designed for a multi-class set-up, some subsequent publications tri"
W09-2415,I05-1082,1,0.187851,"nds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relat"
W09-2415,W04-2609,0,0.0615549,"fy noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context. When one looks at real data, it becomes c"
W09-2415,P08-1052,1,0.611835,"medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is wh"
W09-2415,C08-1082,1,0.339838,"Missing"
W09-2415,J05-1004,0,0.0280101,"boundaries between the individual relations and “everything else”. This encourages good generalization behaviour to larger, noisier data sets commonly seen in real-world applications. 3.1 Semantic Relations versus Semantic Roles There are three main differences between our task (classification of semantic relations between nominals) and the related task of automatic labeling of semantic roles (Gildea and Jurafsky, 2002). The first difference is to do with the linguistic phenomena described. Lexical resources for theories of semantic roles such as FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have been developed to describe the linguistic realization patterns of events and states. Thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns. In contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominals in this task. Also, semantic role descriptions typically relate an event to a set of multiple participants and props, while semantic relations are in practice (although not necessarily) binary. The second major difference is the syntactic context. Theories of semantic roles usually d"
W09-2415,P06-1015,1,0.178213,"m of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context. When one looks at real data, it becomes clear that context does indeed play a role. Consider, for example, the noun compound wood shed : it may refer either to a shed made of wood, or to a shed of any material used to store wood. This ambiguity is likely to be resolved in particular contexts. In fact, most NLP appli"
W09-2415,D07-1075,0,0.0368294,"annotation, we define a nominal as a noun or a base noun phrase. A base noun phrase is a noun and its pre-modifiers (e.g., nouns, adjectives, determiners). We do not include complex noun phrases (e.g., noun phrases with attached prepositional phrases or relative clauses). For example, lawn is a noun, lawn mower is a base noun phrase, and the engine of the lawn mower is a complex noun phrase. We focus on heads that are common nouns. This emphasis distinguishes our task from much work in IE, which focuses on named entities and on considerably more fine-grained relations than we do. For example, Patwardhan and Riloff (2007) identify categories like Terrorist organization as participants in terror-related semantic relations, which consists predominantly of named entities. We feel that named entities are a specific category of nominal expressions best dealt with using techniques which do not apply to common nouns; for example, they do not lend themselves well to semantic generalization. Figure 1 shows two examples of annotated sentences. The XML tags &lt;e1> and &lt;e2> mark the target nominals. Since all nine proper semantic relations in this task are asymmetric, the ordering of the two nominals must be taken into acco"
W09-2415,W01-0511,0,0.0250487,"CL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94–99, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 2 Semantic Relation Classification: Issues 2.1 Defining the Relation Inventory A wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications. Some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have a more general focus. Some schemes are specific to a domain such as biomedical text. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one"
W09-2415,P02-1032,0,0.00907258,"stics 2 Semantic Relation Classification: Issues 2.1 Defining the Relation Inventory A wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications. Some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have a more general focus. Some schemes are specific to a domain such as biomedical text. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and dat"
W09-2415,W09-1401,0,\N,Missing
W09-2415,J02-3004,0,\N,Missing
W09-2415,S10-1006,1,\N,Missing
W09-2415,W04-2412,0,\N,Missing
W09-2416,W04-0404,0,0.485888,"Missing"
W09-2416,C08-1011,1,0.300292,"he problem of synonymy, we do not provide a single correct paraphrase for a given NC but a probability distribution over a range of candidates. For example, highly probable paraphrases for chocolate bar are bar made of chocolate and bar that tastes like chocolate, while bar that eats chocolate is very unlikely. As described in Section 3.3, a set of goldstandard paraphrase distributions can be constructed by collating responses from a large number of human subjects. In this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an NC. Nakov (2008b) and Butnariu and Veale (2008) have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in SemEval-2010 Task 9 will further develop suitable techniques for this problem. Paraphrases of this kind have been shown to be useful in applications such as machine translation (Nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (Kim and Baldwin, 2006; Nakov and Hearst, 2008). Progress in paraphrasing is therefore likely to have follow-on benefits in many areas. 3 Task Description The description of the task we"
W09-2416,P07-1072,0,0.421183,"Missing"
W09-2416,P84-1109,0,0.450496,"araphrasing models, but it is exacerbated by the restricted nature of prepositions. Furthermore, many NCs cannot be paraphrased adequately with prepositions, e.g., woman driver, honey bee. A richer, more flexible paraphrasing model is afforded by the use of verbs. In such a model, a honey 102 bee is a bee that produces honey, a sleeping pill is a pill that induces sleeping and a headache pill is a pill that relieves headaches. In some previous computational work on NC interpretation, manually constructed dictionaries provided typical activities or functions associated with nouns (Finin, 1980; Isabelle, 1984; Johnston and Busa, 1996). It is, however, impractical to build large structured lexicons for broad-coverage systems; these methods can only be applied to specialized domains. On the other hand, we expect that the ready availability of large text corpora should facilitate the automatic mining of rich paraphrase information. The SemEval-2010 task we present here builds on the work of Nakov (Nakov and Hearst, 2006; Nakov, 2007; Nakov, 2008b), where NCs are paraphrased by combinations of verbs and prepositions. Given the problem of synonymy, we do not provide a single correct paraphrase for a gi"
W09-2416,W96-0309,0,0.0699356,"ls, but it is exacerbated by the restricted nature of prepositions. Furthermore, many NCs cannot be paraphrased adequately with prepositions, e.g., woman driver, honey bee. A richer, more flexible paraphrasing model is afforded by the use of verbs. In such a model, a honey 102 bee is a bee that produces honey, a sleeping pill is a pill that induces sleeping and a headache pill is a pill that relieves headaches. In some previous computational work on NC interpretation, manually constructed dictionaries provided typical activities or functions associated with nouns (Finin, 1980; Isabelle, 1984; Johnston and Busa, 1996). It is, however, impractical to build large structured lexicons for broad-coverage systems; these methods can only be applied to specialized domains. On the other hand, we expect that the ready availability of large text corpora should facilitate the automatic mining of rich paraphrase information. The SemEval-2010 task we present here builds on the work of Nakov (Nakov and Hearst, 2006; Nakov, 2007; Nakov, 2008b), where NCs are paraphrased by combinations of verbs and prepositions. Given the problem of synonymy, we do not provide a single correct paraphrase for a given NC but a probability d"
W09-2416,P06-2064,1,0.371927,"rge number of human subjects. In this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an NC. Nakov (2008b) and Butnariu and Veale (2008) have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in SemEval-2010 Task 9 will further develop suitable techniques for this problem. Paraphrases of this kind have been shown to be useful in applications such as machine translation (Nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (Kim and Baldwin, 2006; Nakov and Hearst, 2008). Progress in paraphrasing is therefore likely to have follow-on benefits in many areas. 3 Task Description The description of the task we present below is preliminary. We invite the interested reader to visit the official Website of SemEval-2010 Task 9, where upto-date information will be published; there is also a discussion group and a mailing list.2 3.1 Preliminary Study In a preliminary study, we asked 25-30 human subjects to paraphrase 250 noun-noun compounds using suitable paraphrasing verbs. This is the Levi250 dataset (Levi, 1978); see (Nakov, 2008b) for detai"
W09-2416,E03-1073,0,0.0568415,"Missing"
W09-2416,P08-1052,1,0.883976,"jects. In this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an NC. Nakov (2008b) and Butnariu and Veale (2008) have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in SemEval-2010 Task 9 will further develop suitable techniques for this problem. Paraphrases of this kind have been shown to be useful in applications such as machine translation (Nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (Kim and Baldwin, 2006; Nakov and Hearst, 2008). Progress in paraphrasing is therefore likely to have follow-on benefits in many areas. 3 Task Description The description of the task we present below is preliminary. We invite the interested reader to visit the official Website of SemEval-2010 Task 9, where upto-date information will be published; there is also a discussion group and a mailing list.2 3.1 Preliminary Study In a preliminary study, we asked 25-30 human subjects to paraphrase 250 noun-noun compounds using suitable paraphrasing verbs. This is the Levi250 dataset (Levi, 1978); see (Nakov, 2008b) for details.3 The most popular par"
W09-2416,W07-1108,1,0.886065,"Missing"
W09-2416,W01-0511,0,0.105028,"ONSTITUTE into SOURCE-RESULT, RESULT-SOURCE and COPULA; COPULA is then further subdivided at two additional levels. 101 In computational linguistics, popular inventories of semantic relations have been proposed by Nastase and Szpakowicz (2003) and Girju et al. (2005), among others. The former groups 30 finegrained relations into five coarse-grained supercategories, while the latter is a flat list of 21 relations. Both schemes are intended to be suitable for broad-coverage analysis of text. For specialized applications, however, it is often useful to use domain-specific relations. For example, Rosario and Hearst (2001) propose 18 abstract relations for interpreting NCs in biomedical text, e.g., DEFECT, MATERIAL, PERSON AFFILIATED, ATTRIBUTE OF CLINICAL STUDY. Inventory-based analyses offer significant advantages. Abstract relations such as ‘location’ and ‘possession’ capture valuable generalizations about NC semantics in a parsimonious framework. Unlike paraphrase-based analyses (Section 2.2), they are not tied to specific lexical items, which may themselves be semantically ambiguous. They also lend themselves particularly well to automatic interpretation methods based on multi-class classification. On the"
W09-2416,D08-1027,0,0.0137166,"Missing"
W09-2416,W03-1803,0,0.0693929,"Missing"
W10-0217,S07-1094,0,\N,Missing
W10-0217,S07-1067,0,\N,Missing
W10-0217,S07-1013,0,\N,Missing
W10-0217,S07-1072,0,\N,Missing
W10-0217,J09-3003,0,\N,Missing
W10-0217,I08-1041,1,\N,Missing
W12-3711,H05-1073,0,0.145956,"table in this regard are two classes, anger and disgust, which human annotators often find hard to distinguish (Aman and Szpakowicz, 2007). In order to recognize and analyze affect in written text – seldom explicitly marked for emotions – NLP researchers have come up with a variety of techniques, including the use of machine learning, rule-based methods and the lexical approach (Neviarouskaya, Prendinger, and Ishizuka, 2011). There has been previous work using statistical methods and supervised machine learning applied to corpus-based features, mainly unigrams, combined with lexical features (Alm, Roth, and Sproat, 2005; Aman and Szpakowicz, 2007; Katz, Singleton, and Wicentowski, 2007). The weakness of such methods Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 70–78, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics is that they neglect negation, syntactic relations and semantic dependencies. They also require large (annotated) corpora for meaningful statistics and good performance. Processing may take time, and annotation effort is inevitably high. Rule-based methods (Chaumartin, 2007; Neviarouskaya, Prendinge"
W12-3711,S07-1094,0,0.0240853,"ical features (Alm, Roth, and Sproat, 2005; Aman and Szpakowicz, 2007; Katz, Singleton, and Wicentowski, 2007). The weakness of such methods Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 70–78, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics is that they neglect negation, syntactic relations and semantic dependencies. They also require large (annotated) corpora for meaningful statistics and good performance. Processing may take time, and annotation effort is inevitably high. Rule-based methods (Chaumartin, 2007; Neviarouskaya, Prendinger, and Ishizuka, 2011) require manual creation of rules. That is an expensive process with weak guarantee of consistency and coverage, and likely very task-dependent; the set of rules of rule-based affect analysis task (Neviarouskaya, Prendinger, and Ishizuka, 2011) can differ drastically from what underlies other tasks such as rule-based part-of-speech tagger, discourse parsers, word sense disambiguation and machine translation. The study of emotions in lexical semantics was the theme of a SemEval 2007 task (Strapparava and Mihalcea, 2007), carried out in an unsuperv"
W12-3711,H05-1045,0,0.0919912,"Hoffmann, 2009). The prior-polarity subjectivity lexicon contains over 8000 subjectivity clues collected from a number of sources. To create this lexicon, the authors began with the list of subjectivity clues extracted by Riloff (2003). The list was expanded using a dictionary and a thesaurus, and adding positive and negative word lists from the General Inquirer.1 Words are grouped into strong subjective and weak subjective clues; Table 2 presents the distribution of their polarity. The features used in our experiments were motivated both by the literature (Wilson, Wiebe, and Hoffmann, 2009; Choi et al., 2005) and by the exploration of contextual emotion of words in the annotated data. All of the features are counted based on the emotional word from the lexicon which occurs in the sentence. For ease of description, we group the features into four distinct sets: emotion-word features, part-of-speech features, sentence features and dependency-tree features. Emotion-word features. This set of features are based on the emotion-word itself. Intensifier Lexicon (Neviarouskaya, Prendinger, and Ishizuka, 2010). It is a list of 112 modifiers (adverbs). Two annotators gave coefficients for intensity degree –"
W12-3711,esuli-sebastiani-2006-sentiwordnet,0,0.0431294,"ask (Neviarouskaya, Prendinger, and Ishizuka, 2011) can differ drastically from what underlies other tasks such as rule-based part-of-speech tagger, discourse parsers, word sense disambiguation and machine translation. The study of emotions in lexical semantics was the theme of a SemEval 2007 task (Strapparava and Mihalcea, 2007), carried out in an unsupervised setting (Strapparava and Mihalcea, 2008; Chaumartin, 2007; Kozareva et al., 2007; Katz, Singleton, and Wicentowski, 2007). The participants were encouraged to work with WordNet-Affect (Strapparava and Valitutti, 2004) and SentiWordNet (Esuli and Sebastiani, 2006). Word-level analysis, however, will not suffice when affect is expressed by phrases which require complex phrase- and sentence-level analyses: words are interrelated and they mutually influence their affect-related interpretation. On the other hand, words can have more than one sense, and they can only be disambiguated in context. Consequently, the emotion conveyed by a word in a sentence can differ drastically from the emotion of the word on its own. For example, according to the WordNet-Affect lexicon, the word ”afraid” is listed in the ”fear” category, but in the sentence “I am afraid it i"
W12-3711,S07-1067,0,0.0290197,"which human annotators often find hard to distinguish (Aman and Szpakowicz, 2007). In order to recognize and analyze affect in written text – seldom explicitly marked for emotions – NLP researchers have come up with a variety of techniques, including the use of machine learning, rule-based methods and the lexical approach (Neviarouskaya, Prendinger, and Ishizuka, 2011). There has been previous work using statistical methods and supervised machine learning applied to corpus-based features, mainly unigrams, combined with lexical features (Alm, Roth, and Sproat, 2005; Aman and Szpakowicz, 2007; Katz, Singleton, and Wicentowski, 2007). The weakness of such methods Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 70–78, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics is that they neglect negation, syntactic relations and semantic dependencies. They also require large (annotated) corpora for meaningful statistics and good performance. Processing may take time, and annotation effort is inevitably high. Rule-based methods (Chaumartin, 2007; Neviarouskaya, Prendinger, and Ishizuka, 2011) require manual creation of rules. That is an"
W12-3711,S07-1072,0,0.021395,"creation of rules. That is an expensive process with weak guarantee of consistency and coverage, and likely very task-dependent; the set of rules of rule-based affect analysis task (Neviarouskaya, Prendinger, and Ishizuka, 2011) can differ drastically from what underlies other tasks such as rule-based part-of-speech tagger, discourse parsers, word sense disambiguation and machine translation. The study of emotions in lexical semantics was the theme of a SemEval 2007 task (Strapparava and Mihalcea, 2007), carried out in an unsupervised setting (Strapparava and Mihalcea, 2008; Chaumartin, 2007; Kozareva et al., 2007; Katz, Singleton, and Wicentowski, 2007). The participants were encouraged to work with WordNet-Affect (Strapparava and Valitutti, 2004) and SentiWordNet (Esuli and Sebastiani, 2006). Word-level analysis, however, will not suffice when affect is expressed by phrases which require complex phrase- and sentence-level analyses: words are interrelated and they mutually influence their affect-related interpretation. On the other hand, words can have more than one sense, and they can only be disambiguated in context. Consequently, the emotion conveyed by a word in a sentence can differ drastically f"
W12-3711,de-marneffe-etal-2006-generating,0,0.00637076,"tanford tagger’s output (Toutanova et al., 2003), every word in a sentence gets one of the Penn Treebank tags. • The part-of-speech of the emotional word itself, both according to the emotion lexicon and Stanford tagger. • The POS of neighbouring words in the same sentence. We choose a window of [-2,2], as it is usually suggested by the literature (Choi et al., 2005). Sentence features. For now we only consider the number of words in the sentence. Dependency-tree features. For each emotional word, we create features based on the parse tree and its dependencies produced by the Stanford parser (Marneffe, Maccartney, and Manning, 2006). The dependencies are all binary relations: a grammatical relation holds between a governor (head) and a dependent (modifier). According to Mohammad and Turney (2010),2 adverbs and adjectives are some of the most emotion-inspiring terms. This is not surprising considering that they are used to qualify a noun or a verb; therefore to keep the number of features small, among all the 52 different type of dependencies, we only chose the negation, adverb and adjective modifier dependencies. After parsing the sentence and getting the dependencies, we count the following dependency-tree Boolean feat"
W12-3711,W10-0204,0,0.0234347,"he emotion lexicon and Stanford tagger. • The POS of neighbouring words in the same sentence. We choose a window of [-2,2], as it is usually suggested by the literature (Choi et al., 2005). Sentence features. For now we only consider the number of words in the sentence. Dependency-tree features. For each emotional word, we create features based on the parse tree and its dependencies produced by the Stanford parser (Marneffe, Maccartney, and Manning, 2006). The dependencies are all binary relations: a grammatical relation holds between a governor (head) and a dependent (modifier). According to Mohammad and Turney (2010),2 adverbs and adjectives are some of the most emotion-inspiring terms. This is not surprising considering that they are used to qualify a noun or a verb; therefore to keep the number of features small, among all the 52 different type of dependencies, we only chose the negation, adverb and adjective modifier dependencies. After parsing the sentence and getting the dependencies, we count the following dependency-tree Boolean features for the emotional word. • Whether the word is in a “neg” dependency (negation modifier): true when there is a negation word which modifies the emotional word. • Wh"
W12-3711,W02-1011,0,0.0277354,"iological activity. It is only recently that there has been a growing interest in automatic identification and extraction of sentiment, opinions and emotions in text. Sentiment analysis is the task of identifying positive and negative opinions, emotions and evaluations (Wilson, Wiebe, and Hoffmann, 2005). Most of the current work in sentiment analysis has focused on 70 determining the presence of sentiment in the given text, and on determining its polarity – the positive or negative orientation. The applications of sentiment analysis range from classifying positive and negative movie reviews (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002) to opinion question-answering (Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie, and Wiebe, 2005). The analysis of sentiment must, however, go beyond differentiating positive from negative emotions to give a systematic account of the qualitative differences among individual emotion (Ortony, Collins, and Clore, 1988). In this work, we deal with assigning fine-grained emotion classes to sentences in text. It might seem that these two tasks are strongly tied, but the higher level of classification in emotion recognition task and the presence of certain degrees of similarities betwe"
W12-3711,W03-1014,0,0.20239,"Missing"
W12-3711,H05-1116,0,0.0181966,"xtraction of sentiment, opinions and emotions in text. Sentiment analysis is the task of identifying positive and negative opinions, emotions and evaluations (Wilson, Wiebe, and Hoffmann, 2005). Most of the current work in sentiment analysis has focused on 70 determining the presence of sentiment in the given text, and on determining its polarity – the positive or negative orientation. The applications of sentiment analysis range from classifying positive and negative movie reviews (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002) to opinion question-answering (Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie, and Wiebe, 2005). The analysis of sentiment must, however, go beyond differentiating positive from negative emotions to give a systematic account of the qualitative differences among individual emotion (Ortony, Collins, and Clore, 1988). In this work, we deal with assigning fine-grained emotion classes to sentences in text. It might seem that these two tasks are strongly tied, but the higher level of classification in emotion recognition task and the presence of certain degrees of similarities between some emotion labels make categorization into distinct emotion classes more challenging and difficult. Partic"
W12-3711,S07-1013,0,0.0352476,"rt is inevitably high. Rule-based methods (Chaumartin, 2007; Neviarouskaya, Prendinger, and Ishizuka, 2011) require manual creation of rules. That is an expensive process with weak guarantee of consistency and coverage, and likely very task-dependent; the set of rules of rule-based affect analysis task (Neviarouskaya, Prendinger, and Ishizuka, 2011) can differ drastically from what underlies other tasks such as rule-based part-of-speech tagger, discourse parsers, word sense disambiguation and machine translation. The study of emotions in lexical semantics was the theme of a SemEval 2007 task (Strapparava and Mihalcea, 2007), carried out in an unsupervised setting (Strapparava and Mihalcea, 2008; Chaumartin, 2007; Kozareva et al., 2007; Katz, Singleton, and Wicentowski, 2007). The participants were encouraged to work with WordNet-Affect (Strapparava and Valitutti, 2004) and SentiWordNet (Esuli and Sebastiani, 2006). Word-level analysis, however, will not suffice when affect is expressed by phrases which require complex phrase- and sentence-level analyses: words are interrelated and they mutually influence their affect-related interpretation. On the other hand, words can have more than one sense, and they can only"
W12-3711,strapparava-valitutti-2004-wordnet,0,0.475248,"Missing"
W12-3711,N03-1033,0,0.0404741,"Missing"
W12-3711,P02-1053,0,0.00534146,"tly that there has been a growing interest in automatic identification and extraction of sentiment, opinions and emotions in text. Sentiment analysis is the task of identifying positive and negative opinions, emotions and evaluations (Wilson, Wiebe, and Hoffmann, 2005). Most of the current work in sentiment analysis has focused on 70 determining the presence of sentiment in the given text, and on determining its polarity – the positive or negative orientation. The applications of sentiment analysis range from classifying positive and negative movie reviews (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002) to opinion question-answering (Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie, and Wiebe, 2005). The analysis of sentiment must, however, go beyond differentiating positive from negative emotions to give a systematic account of the qualitative differences among individual emotion (Ortony, Collins, and Clore, 1988). In this work, we deal with assigning fine-grained emotion classes to sentences in text. It might seem that these two tasks are strongly tied, but the higher level of classification in emotion recognition task and the presence of certain degrees of similarities between some emotion"
W12-3711,H05-1044,0,0.0403733,"emotion, with a focus on exploring features important for this task. 1 Introduction Recognition, interpretation and representation of affect have been investigated by researchers in the field of affective computing (Picard 1997). They consider a wide range of modalities such as affect in speech, facial display, posture and physiological activity. It is only recently that there has been a growing interest in automatic identification and extraction of sentiment, opinions and emotions in text. Sentiment analysis is the task of identifying positive and negative opinions, emotions and evaluations (Wilson, Wiebe, and Hoffmann, 2005). Most of the current work in sentiment analysis has focused on 70 determining the presence of sentiment in the given text, and on determining its polarity – the positive or negative orientation. The applications of sentiment analysis range from classifying positive and negative movie reviews (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002) to opinion question-answering (Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie, and Wiebe, 2005). The analysis of sentiment must, however, go beyond differentiating positive from negative emotions to give a systematic account of the qualitative differenc"
W12-3711,J09-3003,0,0.0606315,"Missing"
W12-3711,W03-1017,0,0.0953362,"automatic identification and extraction of sentiment, opinions and emotions in text. Sentiment analysis is the task of identifying positive and negative opinions, emotions and evaluations (Wilson, Wiebe, and Hoffmann, 2005). Most of the current work in sentiment analysis has focused on 70 determining the presence of sentiment in the given text, and on determining its polarity – the positive or negative orientation. The applications of sentiment analysis range from classifying positive and negative movie reviews (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002) to opinion question-answering (Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie, and Wiebe, 2005). The analysis of sentiment must, however, go beyond differentiating positive from negative emotions to give a systematic account of the qualitative differences among individual emotion (Ortony, Collins, and Clore, 1988). In this work, we deal with assigning fine-grained emotion classes to sentences in text. It might seem that these two tasks are strongly tied, but the higher level of classification in emotion recognition task and the presence of certain degrees of similarities between some emotion labels make categorization into distinct emotion classes mor"
W12-3711,baccianella-etal-2010-sentiwordnet,0,\N,Missing
W12-3711,W10-0210,0,\N,Missing
W14-0141,W14-0146,1,\N,Missing
W14-0142,C12-2101,1,\N,Missing
W14-0142,R13-1058,1,\N,Missing
W14-0142,kurc-etal-2012-constraint,1,\N,Missing
W14-0142,P13-1133,0,\N,Missing
W14-0146,J08-4004,0,0.0801973,"substantial. It is commonly assumed that only κ ≥ 0.8 guarantees reliable results in computational linguistics, and κ in 0.67-0.8 is tolerable. Reidsma and Carletta (2007) show that this rule of thumb does not always work. Sometimes lower κ makes the results reliable, sometimes even κ ≥ 0.8 does not suffice. The authors recommend checking whether differences between annotators are systematic or random,11 so we have decided also to put our data 10 The confidence interval was calculated by simple percentile bootstrap (DiCiccio and Efron, 1996; DiCiccio and Romano, 1988) suitable for Cohen’s κ (Artstein and Poesio, 2008). 11 The former is a real problem for computational methods, label system 10 labels 5 labels Cohen’s κ 0.645 0.722 confidence interval of κ 0.586-0.722 0.657-0.785 p-value of χ2 test 0.03962 0.02686 Table 1: Inter-rater agreement of two annotators assigning marking labels to nouns from plWordNet. Confidence intervals are calculated by the percentile bootstrap method, n = 10000 resamplings, α = 0.05. P-values are calculated for the χ2 tests of independence. The 10-label system was described in Section 4. The 5-label system equates compatible labels, as described in this section. through a non-p"
W14-0146,J08-3001,0,\N,Missing
W17-2201,S16-2003,0,0.0375765,"Missing"
W17-2201,E06-1042,0,0.0607526,"e sentences in the poems. We call this method ConceptNet Overlap. We assigned true if there was an overlap and false otherwise. This was used as one of the features in our rule-based model. 2.4 C(x,y).N ln C(x)C(y) N is the size of the corpus, C(x,y) is the frequency of x and y together, C(x) and C(y) are the frequencies of x and y in corpus, respectively. 3 Statistical-based Metaphor Detection The Results We applied our method to the sentences extracted from the 12,830 PoFo poems and annotated manually (see section 2.2). For training data, we used a combination of the datasets such as TroFi (Birke and Sarkar, 2006) and Shutova (Mohammad et al., 2016) with our own poetry dataset. We included other datasets annotated for metaphor, in addition to poetry, in order to increase the training set and thus get better classification predictions. We report all results explicitly for the test set throughout this paper. Table 1 shows the results for the class To capture the distortion of the context that a metaphor causes to a sentence, we computed the vector difference between the vectors for the head words. The underlying idea is this: the smaller the difference, the more connected the words would be. Conversely,"
W17-2201,D14-1162,0,0.0797271,"(Neuman et al., 2013). Type III has a tag sequence of Adjective-Noun (Neuman et al., 2013). We also propose two more metaphor types that we noticed in our poetry data: Type IV with a tag sequence of Noun-Verb, and Type V with a tag sequence of Verb-Verb. Here are examples: Neuman et al. (2013) propose to categorize metaphor by part-of-speech (POS) tag sequences such as Noun-Verb-Noun, Adjective-Noun, and so on. We follow the same methodology to extract the set of sentences that can be metaphorical in nature. Our method differs because we use word embeddings pre-trained on the Gigaword corpus (Pennington et al., 2014) to get word vector representations (vector difference and cosine similarity) of possible metaphorical word pairs. Another difference is the addition of two more types of POS sequences, which we have found to be metaphorical in our Poetry Foundation poetry corpus.1 We explain the types in section 2.1. Neuman et al. (2013) describe a statistical model based on Mutual Information and selectional preferences. They suggest using a largescale corpus to find the concrete nouns which most frequently occur with a specific word. Any word outside this small set denotes a metaphor. Our experiments do not"
W17-2201,de-marneffe-etal-2006-generating,0,0.00997025,"Missing"
W17-2201,N16-1020,0,0.0342629,"y occur with a specific word. Any word outside this small set denotes a metaphor. Our experiments do not involve finding selectional preference sets directly. Instead, we use word embeddings. We have found the selectional preference sets too limiting. The word span is to be set before the experiments. Some sentences exceed that limit, so the contextual meaning is lost. • As if the world were a taxi, you enter it [Type 1] (Koch, 1962) • I counted the echoes assembling, thumbing the midnight on the piers. [Type 2] (Crane, 2006) • The moving waters at their priestlike task [Type 3] (Keats, 2009) Shutova et al. (2016) introduce a statistical model which detects metaphor. So does our method, but their work is more verb-centered, in that verbs are a seed set for training data. Our work looks more into the possible applications for poetry, not generically. We also concentrate on nouns, because our initial experiments concerned Type I metaphor: a copular verb plays only an auxiliary role, so the focus is on the two nouns. • The yellow smoke slipped by the terrace, made a sudden leap [Type 4] (Eliot, 1915) • To die – to sleep [Type 5] (Shakespeare, 1904) In this paper, we focus on Type I metaphor. We will work"
W17-2201,speer-havasi-2012-representing,0,0.0148111,"e based on Pointwise Mutual Information in order to measure if a word pair is a collocation: Rule-based Metaphor Detection Firstly, we applied rule-based methods to our poetry dataset. We used the Abstract-Concrete (Turney et al., 2011) and Concrete Category Overlap rules (Assaf et al., 2013). The Abstract-Concrete rule needs the hypernym class of each noun; we find that in WordNet (Miller, 1995). We got all hypernyms of head nouns and checked for each parent till we reached the hypernym “abstract entity” or “physical entity”. Apart from the above rules, we used a feature based on ConceptNet (Speer and Havasi, 2012). For each noun in our sentence, we extracted the corresponding SurfaceText from ConceptNet. A SurfaceText contains some associations between the specific word and real-world knowledge. For example, “car” gives the following associations: • “drive” is related to “car” • You are likely to find “a car” in “the city” and so on. The entities are already highlighted in the SurfaceTexts. We parsed these associations and extracted all the entities. There can be action associations as well: • “a car” can “crash” • “a car” can “slow down” and so on. These entities and actions were used to establish an"
W17-2201,D11-1063,0,0.0193812,"etaphor instances commonly occurring in poetry. In this task, we were more concerned with the detection of all types of metaphor, not just poetic metaphor. In effect, distinguishing between common-speech and poetic metaphor has been left for our future work. We computed the cosine similarity for all word vector pairs, and made it another feature of our model. We also added a feature based on Pointwise Mutual Information in order to measure if a word pair is a collocation: Rule-based Metaphor Detection Firstly, we applied rule-based methods to our poetry dataset. We used the Abstract-Concrete (Turney et al., 2011) and Concrete Category Overlap rules (Assaf et al., 2013). The Abstract-Concrete rule needs the hypernym class of each noun; we find that in WordNet (Miller, 1995). We got all hypernyms of head nouns and checked for each parent till we reached the hypernym “abstract entity” or “physical entity”. Apart from the above rules, we used a feature based on ConceptNet (Speer and Havasi, 2012). For each noun in our sentence, we extracted the corresponding SurfaceText from ConceptNet. A SurfaceText contains some associations between the specific word and real-world knowledge. For example, “car” gives th"
