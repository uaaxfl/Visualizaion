2020.cmlc-1.4,W14-0617,1,0.684929,"cribe preliminary results obtained when processing this data. Keywords: text mining, geoparsing, historical text, Gazetteers of Scotland, distributed queries, Apache Spark, digital tools 1. Introduction In terms of geoparsing historical text, this area of research is relatively specialised, which means that there is limited related work. The Edinburgh Geoparser, one of the tools used for this work, has been previously adapted to work with historical and literary English text (Alex et al., 2015; Alex et al., 2019) and has been further modified or applied to a number of different text datasets (Grover and Tobin, 2014; Rupp et al., 2013; Rayson et al., 2017; Porter et al., 2018) Similar tools have applied geoparsing to historical text in other languages, e.g. historical French literary text (Moncla et al., 2017) and Swedish literary text (Borin et al., 2014). In the context of Scotland, there is not one comprehensive historical gazetteer available for research as a downloadable resource. There is an online resource called the Gazetteer for Scotland7 which allows users to search for and find out about places in Scotland but this data is limited to online search access only. Our challenge here is then threef"
2020.cmlc-1.4,H05-1109,0,0.103001,"analysis. We expect that geoparsing performance on this type of data is likely to be affected by the quality of the OCR, the use of historical place name variants or spelling variation and the use of Gaelic place names. The collection contains volumes published over the course of the 19th century during which type and quality of printing and use of language changed. This is undoubtedly going to be affected by OCR quality and consistency of spellings across the volumes. Previous work showed that OCRed text has a negative cascading effect on natural language processing tasks (Alex et al., 2012; Kolak and Resnik, 2005; Lopresti, 2005; Lopresti, 2008b; Alex et al., 2019) or information retrieval (Gotscharek et al., 2011; Hauser et al., 2007; Lopresti, 2008a; Reynaert, 2008) and those using NLP approaches to historical texts, in particular, have to take care regarding how the error rate of OCR can affect analysis (Ryan Cordell, 2017).This means All these graphs show that the Edinburgh Geoparser is able to recognise several locations more frequently for equivalent place names. Finally, we also explore which are the most frequent places names that have been identified but not resolved using the Edinburgh Geopa"
2020.louhi-1.4,W19-1909,0,0.065105,"Missing"
2020.louhi-1.4,P19-1091,0,0.348316,"ple, Smit et al. (2020) predict four labels per entity type (positive, negated, uncertain and blank). To scale such approaches to more entity types a lot of annotated data is needed, the absence of which is currently a limiting factor. Lastly, a significant drawback of end-to-end approaches is that no part of the system other than the encoder is reusable in any domain that has a different non-overlapping set of output labels. For that new domain, the labelling procedure needs to be initiated from scratch, leading to a duplication of effort. In this work, as in some previous neural approaches (Bhatia et al., 2019) and as is common in rule-based approaches (Cornegruta et al., 2016; Fu et al., 2019), we employ a bottom up approach to document labelling by factoring the problem into sub-tasks. This way document labels are interpretable as a sequence of decisions with some sub-tasks being extendable and reusable on other datasets. Our three IE systems, EdIE-R, EdIE-BiLSTM and EdIE-BERT (a rule-based and two neural models), recognise mentions of stroke, stroke sub-types and other related findings such as tumours and small vessel disease in text. They also identify related temporal modifiers (recent or old)"
2020.louhi-1.4,E17-2010,0,0.0825841,"hile not mutually exclusive, can broadly be categorised into the following: approaches leveraging lexicons, such as cTAKES (Savova et al., 2010) and RadLex (Langlotz, 2006); ontologies, such as MetaMap (Aronson and Lang, 2010); rule-based systems and pattern matching (Cornegruta et al., 2016); feature based machine learning such as Conditional Random Fields (CRFs) (Hassanpour and Langlotz, 2016); and more recently, deep learning (Cornegruta et al., 2016; Zhu et al., 2019). Negation detection is commonly framed as identifying negation or speculation cues and their matching scopes in sentences (Fancellu et al., 2017). In the clinical domain, however, it is common for approaches to tackle negation assertion, namely, to verify whether each identified entity mention in the text is negated or affirmed (Bhatia et al., 2019), and in some cases, whether it is uncertain (Peng et al., 2018), conditionally present, hypothetically present or relating to some other patient (Uzuner et al., 2011). As with NER, some of the earlier negation detection approaches were rule-based. NegEx (Chapman et al., 2001) relies on regular expressions to detect negation patterns, and has been successfully applied to discharge summaries."
2020.louhi-1.4,W19-5006,0,0.239026,"nalyses, and clinical decision support (Pons et al., 2016). Accurate IE from radiology reports has also received a surge of attention due to the insatiable demand of deep learning medical image classifiers for more labelled training data (Irvin et al., 2019). While IE from radiology reports is of increasing value, the scarcity of annotated data and limited transferability of previously developed models is currently hindering progress. Despite recent breakthroughs in learning contextual representations for clinical and biomedical text from large amounts of unlabelled text (Devlin et al., 2019; Peng et al., 2019; Alsentzer et al., 2019; Lee et al., 2019), labelled data scarcity remains the bottleneck to improvements and wider adoption of deep learning 24 Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis, pages 24–37 c November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 2 they are not data-efficient. For example, Smit et al. (2020) predict four labels per entity type (positive, negated, uncertain and blank). To scale such approaches to more entity types a lot of annotated data is needed, the absence of which is c"
2020.louhi-1.4,W02-2025,0,0.323432,"Missing"
2020.louhi-1.4,W03-0419,0,0.110595,"Missing"
2020.louhi-1.4,W17-2320,0,0.0650489,"Missing"
2020.louhi-1.4,W00-1427,0,0.042516,"Missing"
2020.louhi-1.4,N18-1100,0,0.0126795,"ument such as Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) are used as input to a multi-label classifier to label the report directly without first recognising named entities and negation. While such approaches make annotation simpler and faster and rely less on complex modelling decisions, they have various shortcomings. Firstly, they lack in interpretability, as it is hard to probe which parts of a document the model uses when making predictions. Some models employ an attention mechanism highlighting tokens in the input used to arrive at the decision (Mullenbach et al., 2018; Schrempf et al., 2020). However, they are opaque as to the exact sub-decisions that lead to the labels, which is unsatisfactory in the clinical domain where interpretability is of paramount importance. Secondly, We present an in-depth comparison of three clinical information extraction (IE) systems designed to perform entity recognition and negation detection on brain imaging reports: EdIER, a bespoke rule-based system, and two neural network models, EdIE-BiLSTM and EdIEBERT, both multi-task learning models with a BiLSTM and BERT encoder respectively. We compare our models both on an in-samp"
2020.louhi-1.4,D19-3024,0,0.0221389,"urgh Futures Institute, § Centre for Clinical Brain Sciences, § Usher Institute University of Edinburgh, United Kingdom § Nuffield Department of Population Health University of Oxford, United Kingdom {agrivas|balex|grover|richard}@inf.ed.ac.uk Abstract methods. Data scarcity is even more prominent in the general clinical domain with its vast quantity of possible entity labels. Existing approaches to overcome the lack of labelled data include using a rule-based system to annotate more data (Smit et al., 2020) or propose labels in an annotation tool (Nandhakumar et al., 2017; Alex et al., 2019; Searle et al., 2019), leveraging semi-supervised learning to speed up annotation (Wood et al., 2020) and creating artificial data (Schrempf et al., 2020). It is also common for rule-based systems to be developed alongside statistical models to contrast their performance (Cornegruta et al., 2016; Gorinski et al., 2019; Sykes et al., 2020). We need to understand the shortcomings and benefits of rule-based and neural models to improve annotation decisions and system evaluation, a comparison which we explore in this paper both on in- and out-of-sample data. The use of end-to-end learning for document labelling has be"
2020.louhi-1.4,D19-6221,0,0.0597815,"Missing"
2020.louhi-1.4,2020.emnlp-main.117,0,0.0318219,"Missing"
2020.louhi-1.4,W08-0606,0,0.0854454,"Missing"
2020.louhi-1.4,2020.emnlp-demos.6,0,0.0851926,"Missing"
alex-etal-2006-impact,W05-1306,0,\N,Missing
alex-etal-2006-impact,W03-0424,0,\N,Missing
alex-etal-2006-impact,W05-0304,0,\N,Missing
alex-etal-2006-impact,E03-1071,0,\N,Missing
alex-etal-2006-impact,grover-etal-2000-lt,1,\N,Missing
C88-1012,C86-1016,0,0.0285964,"ent, and describe the implementation of a system which has supported efficient development of a large computational grammar of English? 1. Tools for Grammar Development A number of researzh projects within the broad area of natural language processing (NLP) and theoretical linguistics make use of special purpose programs, which are beginning to be known under the general term of &quot;gm.nmar development environments&quot; (GDEs). Particularly well known examples are reported in Kaplan (1983) (see also Kiparsky, 1985), Shieber (1984), Evans (1985), Phillips and Thompson (1985), Jensen et al. (1986) and Karttunen (1986). In all instances the software packages cited above fall in the class of computational tools used in theoretical (rather than applied) Projects. Thus Kaplan's Grammar-writer's Workbench is an implementation of a particular linguistic theory (Lexical Functional Grammar;, Kaplan and Bresnan, 1982); similarly, Evans' ProGram incorporated an early version of Generalized Phrase Structure Grammar (GPSG, Gazdar and Pullum, 1982), whilst PATR-II is a &quot;virtual linguistic machine&quot;, developed by Shieber as a tool for experimenting with a variety of syntactic theories. These systems differ in their goals"
C88-1012,J85-2001,0,0.0293572,"or the notation is, it incorporates a handle for explicit intervention into the interpretation of the grammar at hand. Sometimes the nature of the task for which the g~ammar is being developed justifies a form~J notation incolporating 'hooks' for explicit procedures. Thus a number of matchine translation (MT) projects~ especially ones employing a ~ransfer strategy, make use of format systems for grammar specification, which, in addition to mapping surface strings into con~esponding language structures, identify operations to be associated with nodes and / or subtrees (Vauquois & Boitet, 1985; Nagao et al., 1985). In general, the effects of the temptation to allow, for example, the EVALuation of arbitrary LISP expressions on the ares of the ATN or the addition of &quot;procedural programming facilities&quot; to the rule-based skeleton of 1BM's PLNLP have been discussed at length in the recent literature addressing the issues of declarative formalisms from a theoretical perspective (see Shieber, 1986a, and references therein). However, from the point of view of developing a realistic grammar with substantial coverage, the opening of the procedural 'back door', while perhaps useful fo: 'patching' the inadequacies"
C88-1012,P85-1021,0,0.0297239,"l;~)rtance. For i~stance, it would bc inappropriate to adopt a direct imp!ementation of, say GPSG, since tire rate of change of the theory itself is likely to make such an implementation obsolete (or at least incapable of irmorporating subsequent linguistic analyses) quite rapidly - . file bdcf lifcspan of Ewms' ProGram is a case in point. ()nly when theou and grammar are beiug developed in very close collaboration, or even wifltin the same group -- - as in, for example, the })ewlctt.-Packard NLP project, whose cornerstotm is the linguistic framewolk of Head-Driven t'hrase Structm'e Grannnar (Proudian and Pollard, 1985; PollaN aud Sag, 1987) - - could such ~ul approach work. l}owever, itr mJ effint like om&quot;a, it is of critical impmtauce to strike the right balance between i)eit~g failhfu[ to the spirit nf a tbeo~y mid being uncommii:ted with respect to a particular vcrsien of it, as well as remaiuing tlexiNe within tile overall iianlcwoN of 'close' or related theories. Attempts to be too flexible, however, arc iikely to lead tit situations of wqich the PATII..II system is an example: the ability to model a wide t' rage of theoretical devices and mr(lyrical ti'amcworks is penalised by its unsuitability &quot;for"
C88-1012,P87-1034,0,0.0332838,"Missing"
C88-1012,C86-1066,0,0.06067,"Missing"
C88-1012,P84-1075,0,0.0908724,"this task, demonstrate how they influence the design of a suitable software environment, and describe the implementation of a system which has supported efficient development of a large computational grammar of English? 1. Tools for Grammar Development A number of researzh projects within the broad area of natural language processing (NLP) and theoretical linguistics make use of special purpose programs, which are beginning to be known under the general term of &quot;gm.nmar development environments&quot; (GDEs). Particularly well known examples are reported in Kaplan (1983) (see also Kiparsky, 1985), Shieber (1984), Evans (1985), Phillips and Thompson (1985), Jensen et al. (1986) and Karttunen (1986). In all instances the software packages cited above fall in the class of computational tools used in theoretical (rather than applied) Projects. Thus Kaplan's Grammar-writer's Workbench is an implementation of a particular linguistic theory (Lexical Functional Grammar;, Kaplan and Bresnan, 1982); similarly, Evans' ProGram incorporated an early version of Generalized Phrase Structure Grammar (GPSG, Gazdar and Pullum, 1982), whilst PATR-II is a &quot;virtual linguistic machine&quot;, developed by Shieber as a tool for"
C88-1012,J85-1003,0,0.079222,"ics: whatever the basis for the notation is, it incorporates a handle for explicit intervention into the interpretation of the grammar at hand. Sometimes the nature of the task for which the g~ammar is being developed justifies a form~J notation incolporating 'hooks' for explicit procedures. Thus a number of matchine translation (MT) projects~ especially ones employing a ~ransfer strategy, make use of format systems for grammar specification, which, in addition to mapping surface strings into con~esponding language structures, identify operations to be associated with nodes and / or subtrees (Vauquois & Boitet, 1985; Nagao et al., 1985). In general, the effects of the temptation to allow, for example, the EVALuation of arbitrary LISP expressions on the ares of the ATN or the addition of &quot;procedural programming facilities&quot; to the rule-based skeleton of 1BM's PLNLP have been discussed at length in the recent literature addressing the issues of declarative formalisms from a theoretical perspective (see Shieber, 1986a, and references therein). However, from the point of view of developing a realistic grammar with substantial coverage, the opening of the procedural 'back door', while perhaps useful fo: 'patch"
C88-1012,C86-1050,0,\N,Missing
E89-1035,J82-3004,0,0.0996764,"Missing"
E89-1035,C86-1066,0,0.0623422,"Missing"
E95-1035,P83-1007,0,0.0138238,"sentence is past perfect, it m a y be a continuation of a preceding thread or the start of a new thread itself. Consider: (11) a. S a m rang the bell. He had lost the key. It had fallen through a hole in his pocket. b. John got to work late. He had left the house at 8. He had eaten a big breakfast. new thread. 2 For the problem with multi-sentence discourses, and the &quot;threads&quot; that sentences continue, we use an implementation of temporM centering ( K a m e y a m a et al., 1993; Poesio, 1994). This is a technique similar to the type of centering used for nominal a n a p h o r a (Sidner, 1983; Grosz et al., 1983). Centering assumes that discourse understanding requires some notion of &quot;aboutness.&quot; While nominal centering assumes there is one object that the current discourse is &quot;about,&quot; temporal centering assumes that there is one thread that the discourse is currently following, and that, in addition to tense and aspect constraints, there is a preference for a new utterance to continue a thread which has a parallel tense or which is semantically related to it and a preference to continue the current thread rather than switching to another thread. K a m e y a m a et al. (1993) confirmed these preferenc"
E95-1035,P93-1010,0,0.246305,"temporal component were to yield a detailed representation of the temporal structure of the discourse, taking into account the effect of tense, aspect and temporal expressions while at the same time minimising unnecessary ambiguity in the temporal structure. The method combines a constraint-based approach with an approach based on preferences: we exploit the HPSG type hierarchy and unification to arrive at a temporal structure using constraints placed on that structure by tense, aspect, rhetorical structure and temporal expressions, and we use the temporal centering preferences described by (Kameyama et al., 1993; Poesio, 1994) to rate the possibilities for temporal structure and choose&apos; the best among them. The starting point for this work was Scha and Polanyi&apos;s discourse grammar (Scha Polanyi 1988; Priist et al 1994). For the implementation we extended the HPSG grammar (Pollard and Sag, 1994) which Gerald Penn and Bob Carpenter first encoded in ALE (Carpenter, 1993). This paper will focus on our temporal processing algorithm, and in particular on our analysis of narrative progression, rhetorical structure, perfects and temporal expressions• 2 C o n s t r a i n t s on n a r r a t i v e continuations"
E95-1035,P94-1046,0,0.0620575,"r, only one reading is possible, i.e. the one where John gave Mary her slice of pizza just after she stared or started to stare at him. It would be undesirable for the temporal processing mechanism to postulate an ambiguity in this case. Of course, sometimes it is possible to take advantage of certain cue words which either indicate or constrain the rhetorical relation. For example, in (5) the order of the events is understood to be the reverse of that in (1) due to the cue word because which signals a causal relationship between the events: (5) John entered the room because Mary stood up. As Kehler (1994) points out, if forward movement of time is considered a default with consecutive event sentences, then the use of &quot;because&quot; in (5) should cause a temporal clash-whereas it is perfectly felicitous. Temporal expressions such as at noon and the previous Thursday can have a similar effect: they too can override the default temporal relations and place constraints on tense. In (6), for example, the default interpretation would be that John&apos;s being in Detroit overlaps with his being in Boston, but the phrase the previous Thursday overrides this, giving the interpretation that John&apos;s being in Detroi"
E95-1035,P91-1008,0,0.012193,"(same-event), overlap, come just_after or precede. TENASP: Keeps track of the tense and syntactic aspect of the DCU (if the DCU is simple). TENSE: past, pres, fut ASPECT: simple, perf, prog, perf_prog and key. 257 To allow the above-mentioned types of information to mutually constrain each other, we employ a hierarchy of rhetorical and temporal relations (illustrated in Figure 1), using the ALE system in such a way that clues such as tense and cue words work together to reduce the number of possible temporal structures. This approach improves upon earlier work on discourse structure such as (Lascarides and Asher, 1991) and (Kehler, 1994) in reducing the number of possible ambiguities; it is also more precise than the Kamp/Hinrichs/Partee approach in that it takes into account ways in which the apparent defaults can be overridden and differentiates between events and activities, which behave differently in narrative progression. Tense, aspect, rhetorical relations and temporal expressions affect the value of the RHET..RELN t y p e that expresses the relationship between two I)CVs: cue words are lexicMly marked according to what rhetorical relation they specify, and this rel.ation is passed on to the DCU. Exp"
E95-1035,J91-1002,0,0.0469343,"Missing"
E95-1035,C88-2120,0,0.052541,"Missing"
E99-1001,M95-1011,0,0.00319979,"YPE=&apos;PERSON&apos; &gt;Flavel Donne&lt;/ENAMEX&gt; is an analyst with &lt;ENAMEX TYPE= &apos;ORGANIZATION &apos;&gt;General Trends &lt;/ENAMEX&gt;, which has been based in &lt;ENAMEX TYPE=&apos;LOCATION&apos;&gt;Little Spring&lt;/ENAMEX&gt; since &lt;TIMEX TYPE=&apos;DATE&apos; &gt;July 1998&lt;/TIMEX&gt;. In an articleon the Named Entity recognition competition (part of MUC-6) Sundheim (1995) remarks that &quot;common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks&quot; (Sundheim 1995: 16). In fact, participants in that competition from the University of Durham (Morgan et al., 1995) and from SRA (Krupka, 1995) report that gazetteers did not make that much of a difference to their system. Nevertheless, in a recent article Cucchiarelli et al. (1998) report that one of the bottlenecks in designing NE recognition systems is the limited availability of large gazetteers, particularly gazetteers for different languages (Cucchiarelli et al. 1998: 291). People also use gazetteers of very different sizes. The basic gazetteers in the Isoquest system for MUC°7 contain 110,000 names, but Krupka and Hausman (1998) show that system performance does not degrade much when the Proceedings of EACL &apos;99 gazetteers are"
E99-1001,M98-1021,1,0.640721,"Missing"
E99-1001,P98-2140,1,0.380974,"tial matching. We 4 Proceedings of EACL &apos;99 It then generates all possible partial orders of the composing words preserving their order, and marks them if found elsewhere in the text. For instance, if &quot;Adam Kluver Ltd&quot; had already been recognised as an organisation by the sure-fire rule, in this second step any occurrences of &quot;Kluver Ltd&quot;, &quot;Adam Ltd&quot; and &quot;Adam Kluver&quot; are also tagged as possible organizations. This assignment, however, is not definite since some of these words (such as &quot;Adam&quot;) could refer to a different entity. This information goes to a pre-trained maximum entropy model (see Mikheev (1998) for more details on this aproach). This model takes into account contextual information for named entities, such as their position in the sentence, whether they exist in lowercase in general, whether they were used in lowercase elsewhere in the same document, etc. These features are passed to the model as attributes of the partially matched words. If the model provides a positive answer for a partial match, the system makes a definite assignment. 3.5 Step 3. Rule R e l a x a t i o n Once this has been done, the system again applies the grammar rules. But this time the rules have much more rel"
E99-1001,P98-1045,0,0.029948,"ased in &lt;ENAMEX TYPE=&apos;LOCATION&apos;&gt;Little Spring&lt;/ENAMEX&gt; since &lt;TIMEX TYPE=&apos;DATE&apos; &gt;July 1998&lt;/TIMEX&gt;. In an articleon the Named Entity recognition competition (part of MUC-6) Sundheim (1995) remarks that &quot;common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks&quot; (Sundheim 1995: 16). In fact, participants in that competition from the University of Durham (Morgan et al., 1995) and from SRA (Krupka, 1995) report that gazetteers did not make that much of a difference to their system. Nevertheless, in a recent article Cucchiarelli et al. (1998) report that one of the bottlenecks in designing NE recognition systems is the limited availability of large gazetteers, particularly gazetteers for different languages (Cucchiarelli et al. 1998: 291). People also use gazetteers of very different sizes. The basic gazetteers in the Isoquest system for MUC°7 contain 110,000 names, but Krupka and Hausman (1998) show that system performance does not degrade much when the Proceedings of EACL &apos;99 gazetteers are reduced to 25,000 and 9,000 names; conversely, they also show that the addition of an extra 42 entries to the gazetteers improves performanc"
E99-1001,M98-1015,0,0.0801219,"6). In fact, participants in that competition from the University of Durham (Morgan et al., 1995) and from SRA (Krupka, 1995) report that gazetteers did not make that much of a difference to their system. Nevertheless, in a recent article Cucchiarelli et al. (1998) report that one of the bottlenecks in designing NE recognition systems is the limited availability of large gazetteers, particularly gazetteers for different languages (Cucchiarelli et al. 1998: 291). People also use gazetteers of very different sizes. The basic gazetteers in the Isoquest system for MUC°7 contain 110,000 names, but Krupka and Hausman (1998) show that system performance does not degrade much when the Proceedings of EACL &apos;99 gazetteers are reduced to 25,000 and 9,000 names; conversely, they also show that the addition of an extra 42 entries to the gazetteers improves performance dramatically. This raises several questions: how important are gazetteers? is it important that they are big? if gazetteers are important but their size isn&apos;t, then what are the criteria for building gazetteers? One might think that Named Entity recognition could be done by using lists of (e.g.) names of people, places and organisations, but that is not th"
E99-1001,M95-1007,0,0.0164119,"Named Entity information: &lt;ENAMEX TYPE=&apos;PERSON&apos; &gt;Flavel Donne&lt;/ENAMEX&gt; is an analyst with &lt;ENAMEX TYPE= &apos;ORGANIZATION &apos;&gt;General Trends &lt;/ENAMEX&gt;, which has been based in &lt;ENAMEX TYPE=&apos;LOCATION&apos;&gt;Little Spring&lt;/ENAMEX&gt; since &lt;TIMEX TYPE=&apos;DATE&apos; &gt;July 1998&lt;/TIMEX&gt;. In an articleon the Named Entity recognition competition (part of MUC-6) Sundheim (1995) remarks that &quot;common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks&quot; (Sundheim 1995: 16). In fact, participants in that competition from the University of Durham (Morgan et al., 1995) and from SRA (Krupka, 1995) report that gazetteers did not make that much of a difference to their system. Nevertheless, in a recent article Cucchiarelli et al. (1998) report that one of the bottlenecks in designing NE recognition systems is the limited availability of large gazetteers, particularly gazetteers for different languages (Cucchiarelli et al. 1998: 291). People also use gazetteers of very different sizes. The basic gazetteers in the Isoquest system for MUC°7 contain 110,000 names, but Krupka and Hausman (1998) show that system performance does not degrade much when the Proceedings"
E99-1001,A97-1028,0,0.0565007,"Missing"
E99-1001,M98-1004,0,\N,Missing
E99-1001,M98-1012,0,\N,Missing
E99-1001,M98-1014,0,\N,Missing
E99-1001,M95-1002,0,\N,Missing
E99-1001,M95-1018,0,\N,Missing
E99-1001,C98-1045,0,\N,Missing
E99-1001,C98-2135,1,\N,Missing
grover-etal-2000-lt,M98-1004,0,\N,Missing
grover-etal-2000-lt,M98-1012,0,\N,Missing
grover-etal-2000-lt,M98-1014,0,\N,Missing
grover-etal-2000-lt,M98-1021,1,\N,Missing
grover-etal-2000-lt,J97-3003,1,\N,Missing
grover-etal-2000-lt,E99-1001,1,\N,Missing
grover-etal-2000-lt,J93-1002,0,\N,Missing
grover-etal-2000-lt,M98-1001,0,\N,Missing
grover-etal-2000-lt,P98-2140,1,\N,Missing
grover-etal-2000-lt,C98-2135,1,\N,Missing
grover-etal-2002-multilingual,M98-1004,0,\N,Missing
grover-etal-2002-multilingual,M98-1012,0,\N,Missing
grover-etal-2002-multilingual,M98-1014,0,\N,Missing
grover-etal-2002-multilingual,M98-1021,1,\N,Missing
grover-etal-2002-multilingual,petasis-etal-2002-ellogon,1,\N,Missing
grover-etal-2002-multilingual,C00-2136,0,\N,Missing
grover-etal-2002-multilingual,grover-etal-2000-lt,1,\N,Missing
grover-etal-2002-multilingual,W99-0613,0,\N,Missing
grover-etal-2002-multilingual,M98-1001,0,\N,Missing
grover-etal-2008-named,E99-1001,1,\N,Missing
grover-etal-2008-named,W05-0619,0,\N,Missing
grover-etal-2008-named,grover-etal-2000-lt,1,\N,Missing
grover-etal-2008-named,W03-0419,0,\N,Missing
grover-etal-2008-named,alex-etal-2006-impact,1,\N,Missing
grover-tobin-2006-rule,J93-2004,0,\N,Missing
grover-tobin-2006-rule,W00-0728,0,\N,Missing
grover-tobin-2006-rule,W00-0736,0,\N,Missing
grover-tobin-2006-rule,W06-1807,0,\N,Missing
grover-tobin-2006-rule,A97-1054,0,\N,Missing
grover-tobin-2006-rule,E03-1071,0,\N,Missing
grover-tobin-2006-rule,W00-0726,0,\N,Missing
grover-tobin-2006-rule,P02-1060,0,\N,Missing
grover-tobin-2006-rule,grover-etal-2000-lt,1,\N,Missing
grover-tobin-2006-rule,W00-0727,0,\N,Missing
grover-tobin-2006-rule,W00-0730,0,\N,Missing
L16-1622,D10-1124,0,0.130785,"Missing"
L16-1622,P14-5007,0,0.0708164,"Missing"
L18-1483,chang-manning-2012-sutime,0,0.032419,"ic Expression Data Wrangling and Up-Cycling • create a list of entities with fields and types • extract common nouns (types) and proper nouns (entity names) for the OpenCCG lexicon • automatically add descriptions of instrument types and entities such as companies or people 3.1. Date and Name Normalization Many of the objects in the MIMEd data have fields which contain a date, but these have been annotated over many years by different authors, and are expressed in a wide variety of formats. There are date recognition software packages such as HeidelTime (Str¨otgen and Gertz, 2010) and SUTIME (Chang and Manning, 2012), but they were not able to parse many of the dates we found, as they are not geared towards historical dates with mis-spellings and expressions for uncertainty and vagueness. Blog posts from the Bentley Historical Library at the University of Michigan (Pillen, 2015a; Pillen, 2015b) describe techniques similar to ours. In addition, during the processing we have created a hierarchy of time expressions so that where possible we can use the Methodius algorithms to generate further meaningful comparisons between objects. For example, if a visitor looks at a clarinet made in 1874 and then another m"
L18-1483,W13-2715,0,0.0639171,"Missing"
L18-1483,W07-2322,0,0.0438071,"in a specific format containing up to a fifty objects and a few hundred triples describing attributes of the objects, created in collaboration with curators. The linguistic resources for each language available (English for ILEX, and English, Italian and Greek for M-PIRO) were also hand-created by computational linguists. The generated texts could be tailored to a museum visitor’s progress through an exhibition, allowing for comparisons between exhibits, and preventing the repetition of background information. More recent systems have generated texts from Semantic Web ontologies; NaturalOWL (Galanis and Androutsopoulos, 2007; Androutsopoulos et al., 2013) generated texts in English and Greek from OWL ontologies, and Dann´ells et al. (2013) generated texts from Semantic Web data in 15 languages, but in both cases expert input was still required to create the necessary linguistic resources. Sun and Mellish (2007), Mellish and Pan (2008), Mellish (2010) and Androutsopoulos et al. (2013) have experimented with performing NLG using OWL/RDF ontologies which do not have domain-dependent linguistic resources, using the relations provided by the ontologies as a starting point for the English presentation of the facts repr"
L18-1483,P17-1017,0,0.0230967,"ulos et al., 2013) generated texts in English and Greek from OWL ontologies, and Dann´ells et al. (2013) generated texts from Semantic Web data in 15 languages, but in both cases expert input was still required to create the necessary linguistic resources. Sun and Mellish (2007), Mellish and Pan (2008), Mellish (2010) and Androutsopoulos et al. (2013) have experimented with performing NLG using OWL/RDF ontologies which do not have domain-dependent linguistic resources, using the relations provided by the ontologies as a starting point for the English presentation of the facts represented, and Gardent et al. (2017) have used DBpedia (Lehmann et al., 2015) crowdsourcing methods to extract large numbers of linguistic resources which can be used by NLG systems. However, many museum databases contain information which is structured, but less regular than that found on the Semantic Web. Data may have been annotated over a number of years by multiple authors before being collected together, and the relation names used cannot always be relied upon to contain the information necessary to derive resources suited to NLG. From an NLG point of view, the museum data is often inconsistent, for example where the same"
L18-1483,L16-1273,1,0.935745,"cessary to derive resources suited to NLG. From an NLG point of view, the museum data is often inconsistent, for example where the same date or company appears in multiple versions, insufficient, for example where it is not clear how to express a given relation, and incomplete, in that there is further information which could be added from other sources to enrich the texts presented to a visitor. We aim to bridge this gap by using automatic methods which can be applied to any museum database in any domain to provide all of the resources needed to generate texts using the Methodius NLG system (Isard, 2016), which generates texts from structured data (described in Section 2.2.). We use the Edinburgh Musical Instrument Museum (MIMEd) as an example domain throughout the paper, but the techniques are designed to be used with any Cultural Heritage dataset. In Figure 1 we show parts of the current MIMEdh web pages for two cornets and a bassoon the information displayed comes directly from the database and is not very engaging for a museum visitor. In contrast, Figure 2 shows a mock-up of a potential visitor experience of a virtual web musem visit using texts generated by Methodius using the technique"
L18-1483,W08-1123,1,0.774283,"re 4. In order for this data to be used with the Methodius system, we need to normalize the representation of dates 1 http://collections.ed.ac.uk/mimed http://www.mimo-international.com/MIMO/ 3 http://dublincore.org 2 Figure 2: Web museum mock-up displaying Methodius generated texts (Section 3.1.), and acquire linguistic information so as to be able to generate sentences like “this bassoon was made by Buffet Crampon” (Section 3.2.). In addition, we can add information which is not present in the original database (Section 3.3.). 2.2. Methodius NLG system The Methodius NLG system (Isard, 2007; Marge et al., 2008; Isard, 2016) is a descendant of the Exprimo system, which was developed during the M-PIRO project (Isard et al., 2003). The M-PIRO web interface allowed users to navigate through a small collection of ancient Greek artefacts by clicking on thumbnail images of the objects. Methodius was designed to be a more robust and modular NLG system, which can deal with collections of at least a million objects, and can be used for any domain in which an ontology of objects and attributes exist. The system uses a typical NLG architecture based on the pipeline model described in Reiter and Dale (2000), wh"
L18-1483,W10-4209,0,0.0298397,"be tailored to a museum visitor’s progress through an exhibition, allowing for comparisons between exhibits, and preventing the repetition of background information. More recent systems have generated texts from Semantic Web ontologies; NaturalOWL (Galanis and Androutsopoulos, 2007; Androutsopoulos et al., 2013) generated texts in English and Greek from OWL ontologies, and Dann´ells et al. (2013) generated texts from Semantic Web data in 15 languages, but in both cases expert input was still required to create the necessary linguistic resources. Sun and Mellish (2007), Mellish and Pan (2008), Mellish (2010) and Androutsopoulos et al. (2013) have experimented with performing NLG using OWL/RDF ontologies which do not have domain-dependent linguistic resources, using the relations provided by the ontologies as a starting point for the English presentation of the facts represented, and Gardent et al. (2017) have used DBpedia (Lehmann et al., 2015) crowdsourcing methods to extract large numbers of linguistic resources which can be used by NLG systems. However, many museum databases contain information which is structured, but less regular than that found on the Semantic Web. Data may have been annota"
L18-1483,S10-1071,0,0.0657743,"Missing"
L18-1483,W07-2316,0,0.0479352,"putational linguists. The generated texts could be tailored to a museum visitor’s progress through an exhibition, allowing for comparisons between exhibits, and preventing the repetition of background information. More recent systems have generated texts from Semantic Web ontologies; NaturalOWL (Galanis and Androutsopoulos, 2007; Androutsopoulos et al., 2013) generated texts in English and Greek from OWL ontologies, and Dann´ells et al. (2013) generated texts from Semantic Web data in 15 languages, but in both cases expert input was still required to create the necessary linguistic resources. Sun and Mellish (2007), Mellish and Pan (2008), Mellish (2010) and Androutsopoulos et al. (2013) have experimented with performing NLG using OWL/RDF ontologies which do not have domain-dependent linguistic resources, using the relations provided by the ontologies as a starting point for the English presentation of the facts represented, and Gardent et al. (2017) have used DBpedia (Lehmann et al., 2015) crowdsourcing methods to extract large numbers of linguistic resources which can be used by NLG systems. However, many museum databases contain information which is structured, but less regular than that found on the"
L18-1483,2007.mtsummit-ucnlg.4,0,0.0279585,"Missing"
L18-1483,W06-1403,0,0.0768781,"Missing"
L18-1483,W14-4424,0,0.102388,"t”. It also states the noun to be used for 3057 &lt;type name=&quot;bassoon&quot;&gt; &lt;parents&gt; &lt;parent name=&quot;wind&quot;/&gt; &lt;/parents&gt; &lt;/type&gt; &lt;type name=&quot;wind&quot;&gt; &lt;parents&gt; &lt;parent name=&quot;instrument&quot;/&gt; &lt;/parents&gt; &lt;/type&gt; which describes the structure of the content to be generated. In Methodius, the logical form is created during the Microplanning stage, described above, and then passed to the OpenCCG generation component, which produces the final text. Previous Methodius domains have relied on hand-written lexicons, but as part of this research we will use the wide coverage grammar of English provided with OpenCCG (White, 2014) after extracting the necessary domain relations, as described in Section 3.2. 3. Figure 5: Extract of Methodius Type Hierarchy &lt;defobject type=&quot;bassoon&quot; is=&quot;object164&quot;&gt; &lt;role slot=&quot;maker&quot; filler=&quot;Buffet Crampon&quot;/&gt; &lt;role slot=&quot;creation-time&quot; filler=&quot;1921&quot;/&gt; &lt;role slot=&quot;original-location filler=&quot;Paris&quot;/&gt; &lt;/defobject&gt; From an NLG point of view, the museum data is often inconsistent, for example where the same date or company appears in multiple versions, insufficient, for example where it is not clear how to express a given relation, and incomplete, in that there is further information which cou"
llewellyn-etal-2014-using,E99-1015,0,\N,Missing
llewellyn-etal-2014-using,reed-etal-2008-language,0,\N,Missing
M98-1021,E95-1027,1,0.715162,"grammar P0.gr. This technique allows one to tailor resource grammars very precisely to particular parts of the text. For example, the reason for applying P0.gr to the rst sentence of a news wire is that that sentence often contains unusual information which occurs nowhere else in the article and which is very useful for the muc task: in particular, if the sentence starts with capitalised words followed by &MD; the capitalised words indicate a location, e.g. PASADENA, Calif. &MD;. We have used our tools in di erent language engineering tasks, such as information extraction in a medical domain [4], statistical text categorisation [2], collocation extraction for lexicography [1], etc. The tools include text annotation tools (a tokeniser, a lemmatiser, a tagger, etc.) as well as tools for gathering statistics and general purpose utilities. Combinations of these tools provide us with the means to explore corpora and to do fast prototyping of text processing applications. A detailed description of the tools, their interactions and application can be found in [4] and [5]; information can also be found at our website, http://www.ltg.ed.ac.uk/software/. This tool infrastructure was the starti"
M98-1021,A97-1054,1,0.764111,". &MD;. We have used our tools in di erent language engineering tasks, such as information extraction in a medical domain [4], statistical text categorisation [2], collocation extraction for lexicography [1], etc. The tools include text annotation tools (a tokeniser, a lemmatiser, a tagger, etc.) as well as tools for gathering statistics and general purpose utilities. Combinations of these tools provide us with the means to explore corpora and to do fast prototyping of text processing applications. A detailed description of the tools, their interactions and application can be found in [4] and [5]; information can also be found at our website, http://www.ltg.ed.ac.uk/software/. This tool infrastructure was the starting point for our muc campaign. LTG TOOLS IN MUC Amongst the tools used in our muc system is an existing ltg tokeniser, called lttok. Tokenisers take an input stream and divide it up into words&quot; or tokens, according to some agreed de nition of what a token is. This is not just a matter of nding white spaces between characters|for example, Tony Blair Jr&quot; could be treated as a single token. lttok is a tokeniser which looks at the characters in the input stream and bundles th"
M98-1021,J97-3003,1,0.620244,"&lt;/W&gt; &lt;W&gt;denied&lt;/W&gt; &lt;W&gt;this&lt;/W&gt;&lt;W C=`.&apos;&gt;.&lt;/W&gt; &lt;W&gt;But&lt;/W&gt;... Note how ltstop has added&quot; a nal stop to the rst sentence, making explicit that the period after Ltd&quot; has two distinct functions. Another standard ltg tool we used in our muc system was our part-of-speech tagger lt pos [7]. lt pos is sgml-aware: it reads a stream of sgml elements speci ed by the query and applies a Hidden Markov Modeling technique with estimates drawn from a trigram maximum entropy model to assign the most likely part of speech tags. An important feature of the tagger is an advanced module for handling unknown words [6], which proved to be crucial for name spotting. Some muc-speci c extensions were added at this point in the processing chain: for capitalised words, we added information as to whether the word exists in lowercase in the lexicon (marked as L=l) or whether it exists in lowercase elsewhere in the same document (marked as L=d). We also developed a model which assigns certain semantic&quot; tags which are particularly useful for muc processing. For example, words ending in -yst and -ist (analyst, geologist) as well as words occurring in a special list of words 3 (spokesman, director) are recognised as"
M98-1021,P98-2140,1,0.679519,"W&gt;Ltd.&lt;/W&gt; &lt;W&gt;He&lt;/W&gt; &lt;W&gt;denied&lt;/W&gt; &lt;W&gt;this.&lt;/W&gt; &lt;W&gt;But&lt;/W&gt; ... As the example shows, the tokeniser does not attempt to resolve whether a period is a full stop or part of an abbreviation. Depending on the choice of resource le for lttok, a period will either always be attached to the preceding word (as in this example) or it will always be split o . This creates an ambiguity where a sentence- nal period is also part of an abbreviation, as in the rst sentence of our example. To resolve this ambiguity we use a special program, ltstop, which applies a maximum entropy model pre-trained on a corpus [8]. To use ltstop the user must specify whether periods in the input are attached to or split o from the preceding words; in our case, they were attached to the words, and ltstop is used with the option -split. With this option, ltstop will split the period from regular words and create an end-of-sentence token &lt;W C=&quot;.&quot;&gt;.&lt;/W&gt;; or it will leave the period with the word if it is an abbreviation; or, in the case of sentence- nal abbreviations, it will leave the period with the abbreviation and in addition create a virtual full stop &lt;W C=&quot;.&quot;&gt;&lt;/W&gt; Like the other ltg tools ltstop can be targeted at pa"
M98-1021,C98-2135,1,\N,Missing
N03-4007,grover-etal-2002-multilingual,1,0.794948,"ris Souflis , Claire Grover 2003 , Edmonton, May-June Vangelis Karkaletsis , Constantine D. Spyropoulos , Ben Hachey , Maria Teresa Pazienza , Michele Vindigni , Emmanuel Cartier , Jos´e Coch Institute for Informatics and Telecommunications, NCSR “Demokritos” vangelis, costass @iit.demokritos.gr Velti S.A. Dsouflis@velti.net Division of Informatics, University of Edinburgh grover, bhachey @ed.ac.uk D.I.S.P., Universita di Roma Tor Vergata pazienza, vindigni @info.uniroma2.it Lingway emmanuel.cartier, Jose.Coch @lingway.com                    1 Introduction French, Italian) (Grover et al. 2002). Cross-lingual name matching techniques are also employed in order to link expressions referring to the same named entities across languages. - fact extraction to identify those named entities that fill the slots of the template specifying the information to be extracted from each web page. To achieve this the project combines wrapper-induction approaches for fact extraction with language-based information extraction in order to develop site independent wrappers for the domain examined. The EC-funded R&D project, CROSSMARC, is developing technology for extracting information from domainspecif"
N03-4007,M98-1021,1,0.810376,"ten to the blackboard and can be refined by the human administrator. The Spidering Agent is an autonomous software component, which retrieves sites to spider from the blackboard and locates interesting web pages within them by traversing their links. Again, status information is written to the blackboard. The multi-lingual IE system is a distributed one where the individual monolingual components are autonomous processors, which need not all be installed on the same machine. (These components have been developed using a wide range of base technologies: see, for example, Petasis et al. (2002), Mikheev et al. (1998), Pazienza and Vindigni (2000)). The IE systems are not offered as web services, therefore a proxy mechanism is required, utilising established remote access mechanisms (e.g. HTTP) to act as a front-end for every IE system in the project. In effect, this proxy mechanism turns every IE system into a web service. For this purpose, we have developed an Information Extraction Remote Invocation module (IERI) which takes XHTML pages as input and routes them to the corresponding monolingual IE system according to the language they are written in. The Information Extraction Agent retrieves pages store"
N03-4007,petasis-etal-2002-ellogon,1,0.821348,"e CROSSMARC system written to the blackboard and can be refined by the human administrator. The Spidering Agent is an autonomous software component, which retrieves sites to spider from the blackboard and locates interesting web pages within them by traversing their links. Again, status information is written to the blackboard. The multi-lingual IE system is a distributed one where the individual monolingual components are autonomous processors, which need not all be installed on the same machine. (These components have been developed using a wide range of base technologies: see, for example, Petasis et al. (2002), Mikheev et al. (1998), Pazienza and Vindigni (2000)). The IE systems are not offered as web services, therefore a proxy mechanism is required, utilising established remote access mechanisms (e.g. HTTP) to act as a front-end for every IE system in the project. In effect, this proxy mechanism turns every IE system into a web service. For this purpose, we have developed an Information Extraction Remote Invocation module (IERI) which takes XHTML pages as input and routes them to the corresponding monolingual IE system according to the language they are written in. The Information Extraction Agen"
N03-4007,M98-1004,0,\N,Missing
N03-4007,M98-1012,0,\N,Missing
N03-4007,M98-1014,0,\N,Missing
P01-1034,J97-3003,0,\N,Missing
P01-1034,W99-0629,0,\N,Missing
P01-1034,W98-1114,0,\N,Missing
P01-1034,A94-1009,0,\N,Missing
P01-1034,W00-1427,0,\N,Missing
P01-1034,H94-1020,0,\N,Missing
P01-1034,grover-etal-2000-lt,1,\N,Missing
P01-1034,J93-1002,0,\N,Missing
P01-1034,P99-1052,0,\N,Missing
P16-3007,Y09-2019,0,0.0325989,"d to aid summarisation, blogs are more variable. In particular they found that errors in blog summarisation are much higher than in news text summarisation. They determined that errors were often due to the candidate summary sentences being off topic and they suggest that blog summarisation needs to be improved in terms of topic detection. When investigating the summarisation of blogs and comments on blogs Balahur et al.(2009) found that it is very common to change topics between the original blog post and the comments, and from comment to comment. The research of Mithum and Koseim (2009) and Balahur et al. (2009) indicates that topic identification is a key area on which to concentrate efforts in the emerging field of comment summarisation. Comment Summarisation Abstractive summarisation is a very complex task, and because comment summarisation is a relatively new task, current work mostly focuses on extractive approaches. The general task involves clustering the comments into appropriate topics and then extracting comments, or parts of comments to represent those topics (Khabiri et al., 2011; Ma et al., 2012). Ma et al. (2012) summarise discussion on news articles from Yahoo!News and Khabiri et al (2"
P16-3007,llewellyn-etal-2014-using,1,0.930408,"mments. When studied closely and analysed effectively they provide multiple points of view and a wide range of experience and knowledge from diverse sources. However, the number of comments produced per article can prohibit close reading. Summarising the content of these comments allows users to interact with the data at a higher level, providing a transparency to the underlying data (Greene and Cross, 2015). The current state of the art within the comment summarisation field is to cluster comments using Latent Dirichlet Allocation (LDA) topic modelling (Khabiri et al., 2011; Ma et al., 2012; Llewellyn et al., 2014). The comments within 43 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics – Student Research Workshop, pages 43–50, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics 2 Related Work 2.1 grams. They found that the topic modelling approach was superior. Aker et al. (2016) looked at a graph based model that included information from DBpedia, finding that this approach out performed an un-optimised LDA model. They then labelled the clusters using LDA clustering and extracted keywords. Other work has been conducted in related"
P16-3007,C12-1047,0,0.0557785,"Missing"
P16-3007,W09-4301,0,0.033156,"Claire Grover Clare Llewellyn School of Informatics School of Informatics University of Edinburgh University of Edinburgh Edinburgh, UK Edinburgh, UK s1053147@sms.ed.ac.uk grover@inf.ed.ac.uk Abstract each topic cluster are ranked and comments are typically extracted to construct a summary of the cluster. In this paper we focus on the clustering subtask. It is important that the clustering is appropriate and robust as the subsequent extraction relies on a good set of clusters. Research in a related domain has found that topical mistakes were the largest source of error in summarising blogs – (Mithun and Kosseim, 2009) a similar data type. Comment data, as with many social media datasets, differs from other content types as each ‘document’ is very short. Previous studies have indicated that the number of documents and the number of words in the documents are limiting factors on the performance of topic modelling (Tang et al., 2014). Topic models built using longer documents and using more documents are more accurate. Short documents can be enriched with external data. In our corpus the number of comments on each newspaper article is finite and the topics discussed within each set have evolved from the origi"
P16-3007,W00-0405,0,0.0409448,". Aker et al. (2016) looked at a graph based model that included information from DBpedia, finding that this approach out performed an un-optimised LDA model. They then labelled the clusters using LDA clustering and extracted keywords. Other work has been conducted in related domains such as summarising blogs, microblogs and e-mail. Summarisation The summarisation domain is well developed. The earliest focus of the field was single document summarisation – for a survey paper see (Gupta and Lehal, 2010). This approach was extended into the summarisation of multiple documents on the same topic (Goldstein et al., 2000) and to summarising discussions such as email or Twitter conversations (Cselle et al., 2007; Sharifi et al., 2010; Inouye and Kalita, 2011). The basic idea behind the summarisation of textual data is the grouping together of similar information and describing those groups (Rambow et al., 2004). Once these groups are formed they are described using either an extractive or abstractive approach. Extractive summarisation uses units of text, generally sentences, from within the data in the group to represent the group. Abstractive summarisation creates a description of the data in the group as a wh"
P16-3007,N04-4027,0,0.0314067,"arising blogs, microblogs and e-mail. Summarisation The summarisation domain is well developed. The earliest focus of the field was single document summarisation – for a survey paper see (Gupta and Lehal, 2010). This approach was extended into the summarisation of multiple documents on the same topic (Goldstein et al., 2000) and to summarising discussions such as email or Twitter conversations (Cselle et al., 2007; Sharifi et al., 2010; Inouye and Kalita, 2011). The basic idea behind the summarisation of textual data is the grouping together of similar information and describing those groups (Rambow et al., 2004). Once these groups are formed they are described using either an extractive or abstractive approach. Extractive summarisation uses units of text, generally sentences, from within the data in the group to represent the group. Abstractive summarisation creates a description of the data in the group as a whole, analogous to the approach a human would take. 2.1.1 2.1.2 Blog Summarisation Comments are similar to blogs in that they are generated by multiple individuals who exhibit a vast array of writing styles. Mithum and Koseim (2009) found that whereas news articles have a generalisable structur"
P16-3007,N10-1100,0,0.0210441,"h out performed an un-optimised LDA model. They then labelled the clusters using LDA clustering and extracted keywords. Other work has been conducted in related domains such as summarising blogs, microblogs and e-mail. Summarisation The summarisation domain is well developed. The earliest focus of the field was single document summarisation – for a survey paper see (Gupta and Lehal, 2010). This approach was extended into the summarisation of multiple documents on the same topic (Goldstein et al., 2000) and to summarising discussions such as email or Twitter conversations (Cselle et al., 2007; Sharifi et al., 2010; Inouye and Kalita, 2011). The basic idea behind the summarisation of textual data is the grouping together of similar information and describing those groups (Rambow et al., 2004). Once these groups are formed they are described using either an extractive or abstractive approach. Extractive summarisation uses units of text, generally sentences, from within the data in the group to represent the group. Abstractive summarisation creates a description of the data in the group as a whole, analogous to the approach a human would take. 2.1.1 2.1.2 Blog Summarisation Comments are similar to blogs i"
P16-3007,C04-1079,0,0.10855,"Missing"
P87-1027,E85-1025,1,0.86709,"ated in Figure 4 can clearly be directly mapped into a feature d u s t e r within the features and feature set declarations used by the dictionary and grammar projects. A colnparison of the existing entries for ~oelieve~ in the hand crafted lexicon (Figure 1) and the third word sense for ~believem extracted from LDOCE demonstrates that much of the information available from L D O C E is of direct utility - - for example the SUBCAT values can be derived by an analysis of the T a k e s values and the O R a i e i n g logical type specification above. Indeed, we have demonstrated the feasibility (Alshawi et al., 1985) of driving a parsing system directly from the information av~lable in LDOCE by constructing dictionary entries for the PATR-H system (Shieber, Figure 4: A lexical template derived from LDOCE This resulting structure is a lexical template, designed as a formal representation for the kind of syntacrico-semantic information which can be extracted from the dictionary and which is relevant to a system for automatic morphological and syntactic analysis of English texts. The overall transformation strategy employed by our system attempts to derive both subcategorisation frames relevant to a particul"
P87-1027,J87-3002,1,\N,Missing
P87-1027,J87-3008,0,\N,Missing
P87-1027,E87-1011,1,\N,Missing
P87-1027,C86-1066,0,\N,Missing
P87-1027,P84-1075,0,\N,Missing
P94-1003,P90-1021,0,0.246776,"e case. Our solution to this problem is to dispense with the MSCD operation and to use generalization instead. However, we do propose that generalization should take inputs whose parallelism dependent anaphors have already been resolved. 1 In the case of the combination of (5a) and (5d), this will give 1As described in the next section, we use priority union to resolve these anaphors in both lists and contrasts. The use of generalization as a step towards checking that there is sufficient common ground is subsequent to the use of priority ration as the resolution mechanism. ~See, for example, Bouma (1990), Calder (1990), Carpenter (1994), Kaplan (1987). 19 default structure, hence our preference to refer to it by the n a m e priority union. Below we demonstrate the results of priority union for the examples in ( l a ) - ( l c ) . Note t h a t the target is the strict structure and the source is the defeasible one. (11) Hannah likes beetles. So does Thomas. Source: Target: (12) 5a 5b (15) Jessy likes her Priority[ AGENT th°mas ] Union: PATIENT beetle like Hannah likes beetles. She alsolikes caterpillars. Source: Target: Priority Union: (13) The situations where the credulous version of the oper"
P94-1003,P93-1009,0,0.0722164,"eetle) = like(hannah, beetle) b. P(like) = like(hannah, beetle) Source: Target: Equation: Sol.1 ($1): Sol.2 (S2): Apply SI: Apply $2: like(jessy, brother-of (jessy) ) P( hannah ) P(jessy) = like(jessy, brother-of (jessy) ) P = ~x.like(x, brother-of(jessy)) e = Ax.like(x, brother-of(x)) like(hannah, brother-of (jessy) ) like(hannah, brother-of(hannah)) Parallelism In the DSP approach to vP-ellipsis and in our approach too, the emphasis has been on semantic parallelism. It has often been pointed out, however, that there can be an additional requirement of syntactic parallelism (see for example, Kehler 1993 and Asher 1993). Kehler (1993) provides a useful discussion of the issue and argues convincingly that whether syntactic parallelism is required depends on the coherence relation involved. As the examples in (20) and (21) demonstrate, semantic parallelism is sufficient to establish a relation like contrast but it is not sufficient for building a coherent list. DSP claim that a significant attribute of their account is that they can provide the two readings in strict/sloppy ambiguities without having to postulate ambiguity in the source. They claim this as a virtue which is matched by few other"
P94-1003,P84-1085,0,0.0111088,"parallelism-dependent anaphors when they occur in list structures and must therefore be resolved to the corresponding fully referential subject/object in the first member of the list. Abstract 1 Marc (1) Grammar Working broadly within the sign-based paradigm exemplified by HPSG (Pollard and Sag in press) we have been exploring computational issues for a discourse level grammar by using the ALE system (Carpenter 1993) to implement a discourse grammar. Our central model of a discourse grammar is the Linguistic Discourse Model (LDM) most often associated with Scha, Polanyi, and their coworkers (Polanyi and Scha 1984, Scha and Polanyi 1988, Priist 1992, and most recently in Priist, Scha and van den Berg 1994). In LDM rules are defined which are, in a broad sense, unification grammar rules and which combine discourse constituent units (DCUS). These are simple clauses whose syntax and underresolved semantics have been determined by a sentence grammar but whose fully resolved final form can only be calculated by their integration into the current discourse and its context. The rules of the discourse grammar act to establish the rhetorical relations between constituents and to perform resolution of those anap"
P94-1003,C88-2120,0,0.0211539,"anaphors when they occur in list structures and must therefore be resolved to the corresponding fully referential subject/object in the first member of the list. Abstract 1 Marc (1) Grammar Working broadly within the sign-based paradigm exemplified by HPSG (Pollard and Sag in press) we have been exploring computational issues for a discourse level grammar by using the ALE system (Carpenter 1993) to implement a discourse grammar. Our central model of a discourse grammar is the Linguistic Discourse Model (LDM) most often associated with Scha, Polanyi, and their coworkers (Polanyi and Scha 1984, Scha and Polanyi 1988, Priist 1992, and most recently in Priist, Scha and van den Berg 1994). In LDM rules are defined which are, in a broad sense, unification grammar rules and which combine discourse constituent units (DCUS). These are simple clauses whose syntax and underresolved semantics have been determined by a sentence grammar but whose fully resolved final form can only be calculated by their integration into the current discourse and its context. The rules of the discourse grammar act to establish the rhetorical relations between constituents and to perform resolution of those anaphors whose interpretati"
S10-1074,E03-1071,0,0.0117849,"eospatial and temporal grounding of entities and it has been useful to participate in TempEval-2 to measure the performance of our system and to guide further development. We took part in Tasks A and B for English. 1 2 The Edinburgh IE System Our IE system is a modular pipeline system built around the LT-XML21 and LT-TTT22 toolsets. Documents are converted into our internal document format and are then passed through a sequence of linguistic components which each add XML mark-up. Early stages identify paragraphs, sentences and tokens. Part-of-speech (POS) tagging is done using the C&C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al., 2000). We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C&C maximum entropy NER tagger (Curran and Clark, 2003b). We are experimenting to find the best combination of the two different NER views but this is not an issue in the case of date and time entities since we have taken the decision to use the rule-based output for these. The main motivation for this decision arises from the need to ground (provide temporal values for) these entiti"
S10-1074,W03-0424,0,0.0191532,"eospatial and temporal grounding of entities and it has been useful to participate in TempEval-2 to measure the performance of our system and to guide further development. We took part in Tasks A and B for English. 1 2 The Edinburgh IE System Our IE system is a modular pipeline system built around the LT-XML21 and LT-TTT22 toolsets. Documents are converted into our internal document format and are then passed through a sequence of linguistic components which each add XML mark-up. Early stages identify paragraphs, sentences and tokens. Part-of-speech (POS) tagging is done using the C&C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al., 2000). We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C&C maximum entropy NER tagger (Curran and Clark, 2003b). We are experimenting to find the best combination of the two different NER views but this is not an issue in the case of date and time entities since we have taken the decision to use the rule-based output for these. The main motivation for this decision arises from the need to ground (provide temporal values for) these entiti"
S10-1074,grover-tobin-2006-rule,1,0.834732,"tending and adapting our IE pipeline to ground spatial and temporal entities. We have developed the Edinburgh Geoparser for georeferencing documents and have evaluated our system against the SpatialML corpus, as reported in Tobin et al. (2010). We are currently in the process of developing a rule-based date and time grounding component and it is this component that we used for Task A, which requires systems to identify the extents of temporal named entities and provide their interpretation. The TempEval-2 data also contains event entities and we have adapted the output of our inhouse chunker (Grover and Tobin, 2006) to identify events for Task B, which requires systems to identify event denoting words and to compute a range of attributes for them. In future work we will adapt our machine-learning-based relation extrac1 2 www.ltg.ed.ac.uk/software/ltxml2 www.ltg.ed.ac.uk/software/lt-ttt2 333 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 333–336, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics &lt;s id=&quot;s1&quot;&gt; &lt;ng&gt; &lt;w p=&quot;DT&quot; id=&quot;w13&quot;&gt;The&lt;/w&gt; &lt;w p=&quot;NN&quot; id=&quot;w17&quot; l=&quot;announcement&quot; vstem=&quot;announce&quot; headn=&quot;yes&quot;&gt;announcement&lt;/w&gt; &lt;/ng&gt; &lt;vg tense"
S10-1074,W08-0603,0,0.033339,"Missing"
S10-1074,W00-1427,0,0.0120156,"ul to participate in TempEval-2 to measure the performance of our system and to guide further development. We took part in Tasks A and B for English. 1 2 The Edinburgh IE System Our IE system is a modular pipeline system built around the LT-XML21 and LT-TTT22 toolsets. Documents are converted into our internal document format and are then passed through a sequence of linguistic components which each add XML mark-up. Early stages identify paragraphs, sentences and tokens. Part-of-speech (POS) tagging is done using the C&C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al., 2000). We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C&C maximum entropy NER tagger (Curran and Clark, 2003b). We are experimenting to find the best combination of the two different NER views but this is not an issue in the case of date and time entities since we have taken the decision to use the rule-based output for these. The main motivation for this decision arises from the need to ground (provide temporal values for) these entities and the rules for the grounding are most naturally implement"
W02-1706,J93-1002,0,0.029425,"Missing"
W02-1706,A97-1052,0,0.0294218,"Missing"
W02-1706,W01-1808,0,0.0444384,"Missing"
W02-1706,A94-1009,0,0.0258217,"Missing"
W02-1706,grover-etal-2000-lt,1,0.850189,"raven and Kumlien (1999) for discussion of methods for IE from MEDLINE. Our processing paradigm is XML-based. As a mark-up language for NLP tasks, XML is expressive and flexible yet constrainable. Furthermore, there exist a wide range of XML-based tools for NLP applications which lend themselves to a modular, pipelined approach to processing whereby linguistic knowledge is computed and added as XML annotations in an incremental fashion. In processing MEDLINE abstracts we have built a number of such pipelines using as key components the programs distributed with the LT TTT and LT XML toolsets (Grover et al., 2000; Thompson et al., 1997). We have also successfully integrated non-XML publicdomain tools into our pipelines and incorporated their output into the XML mark-up using the LT XML program xmlperl (McKelvie, 2000). In Section 2 we describe our use of XML-based tokenisation tools and techniques and in Sections 3 and 4 we describe two different approaches to analysing MEDLINE data which are built on top of the tokenisation. The first approach uses a handcoded grammar to give complete syntactic and semantic analyses of sentences. The second approach performs a shallower statistically-based analysis w"
W02-1706,P01-1034,1,0.692272,"Missing"
W02-1706,H94-1020,0,0.145023,"Missing"
W02-1706,J97-3003,0,0.0213577,"ields of an OHSUMED entry and convert them into XML mark-up: each abstract is put inside a RECORD element which contains sub-structure reflecting e.g. author, title, MESH code and the abstract itself. From this point on, all processing is directed at the ABSTRACT elements through the query “.*/ABSTRACT”1 . Steps 3 and 4 make calls to fsgmatch to identify S and W (word) elements as described above and after this point, in step 5, the S mark-up is discarded (using the LT TTT program sgdelmarkup) since it has now served its purpose. Step 6 contains a call to the other main LT TTT program, ltpos (Mikheev, 1997), which performs both sentence identification and POS tagging. The subquery (-qs) option picks out ABSTRACTs as the elements within RECORDs (-q option) that are to be processed; the -qw option indicates that the input has already been segmented into words marked 1 The query language that the LT TTT and LT XML tools use is a specialised XML query language which pinpoints the part of the XML tree-structure that is to be processed at that point. This query language pre-dates XPath and in expressiveness it constitutes a subset of XPath except that it also allows regular expressions over text conte"
W02-1706,W00-1427,0,\N,Missing
W02-1706,C02-1013,0,\N,Missing
W03-0505,J97-3003,0,\N,Missing
W03-0505,W01-0708,0,\N,Missing
W03-0505,J93-2004,0,\N,Missing
W03-0505,E99-1015,0,\N,Missing
W03-0505,W00-1302,0,\N,Missing
W03-0505,H94-1020,0,\N,Missing
W03-0505,J96-1002,0,\N,Missing
W03-0505,J02-3002,0,\N,Missing
W03-0505,grover-etal-2000-lt,1,\N,Missing
W03-0505,J02-4002,0,\N,Missing
W03-2406,M95-1012,0,\N,Missing
W03-2406,P97-1003,0,\N,Missing
W03-2406,P99-1042,0,\N,Missing
W03-2406,grover-etal-2000-lt,1,\N,Missing
W03-2406,W01-1201,0,\N,Missing
W04-1007,J96-1002,0,0.00337604,"up of the sentence was likely to provide important clues as to the rhetorical status of the sentence (e.g. a present tense active verb will correlate more highly with BACKGROUND or DIS POSAL sentences while a simple past tense sentence is more likely to be found in a FACT sentence). In order to find the main verb group of a sentence, however, we need to establish its clause structure. We do this with a clause identifier (Hachey, 2002) built using the CoNLL-2001 shared task data (Sang and D´ejean, 2001). Clause identification is performed in three steps. First, two maximum entropy classifiers (Berger et al., 1996) are applied, where the first predicts clause start labels and the second predicts clause end labels. In the the third step clause segmentation is inferred from the predicted starts and ends using a maximum entropy model whose sole purpose is to provide confidence values for potential clauses. The final stages of linguistic processing use handwritten LT TTT components to compute features of verb and noun groups. For all verb groups, attributes encoding tense, aspect, modality and negation are added to the mark-up: for example, might not have been brought is analysed as &lt;VG tense=‘pres’, aspect"
W04-1007,W03-0424,0,0.0336514,"the domain. Table 2 shows examples of the entities we have marked up in the corpus (in our annotation scheme these are noun groups (NG) with specific type and subtype attributes). In the top two blocks of the table are examples of domain-specific entities such as courts, judges, acts and judgments, while in the third block we show examples of non-domainspecific entity types. We use different strategies for the identification of the two classes of entities: for the domain-specific ones we use hand-crafted LT TTT rules, while for the non-domain-specific ones we use the C&C named entity tagger (Curran and Clark, 2003) trained on the MUC7 data set. For some entities, the two approaches provide competing analyses and in all cases the domain-specific label is to be preferred since it provides finer-grained information. However, while the rule-based recogniser can operate incrementally over data which already contains some entity markup, the C&C tagger is trained to operate over unlabelled sentences. For this reason we run the C&C tagger first and encode its results as attributes on the words. We then run the domain-specific tagger, encoding its results as XML elements enclosing the words, and finish with a si"
W04-1007,A97-1054,0,0.0191981,"andard for the HOLJ domain and evaluate the named entity recognition we are performing. For now, we can use rhetorical status classification as a task-based evaluation to estimate the utility of entity recognition. The generic C&C entity recognition together with the hand-crafted rules for the HOLJ domain prove to be the third most effective feature set after the cue phrase and location features (Table 3). The next stage in the linguistic analysis module performs noun group and verb group chunking using fsgmatch with the specialised hand-written rule sets which were the core part of LT CHUNK (Finch and Mikheev, 1997). The noun group and verb group mark-up plus POS tags provide the relevant &lt;NG type=‘enamex-pers’ subtype=‘committee-lord’&gt; &lt;NG type=‘caseent’ subtype=‘appellant’&gt; &lt;NG type=‘caseentsub’ subtype=‘appellant’&gt; &lt;NG type=‘caseent’ subtype=‘respondent’&gt; &lt;NG type=‘caseentsub’ subtype=‘respondent’&gt; &lt;NG type=‘enamex-pers’ subtype=‘judge’&gt; &lt;NG type=‘enamex-org’ subtype=‘court’&gt; &lt;NG type=‘legal-ent’ subtype=‘act’&gt; &lt;NG type=‘legal-ent’ subtype=‘section’&gt; &lt;NG type=‘legal-ent’ subtype=‘judgment’&gt; &lt;NG type=‘enamex-loc’ subtype=‘fromCC’&gt; &lt;NG type=‘enamex-pers’ subtype=‘fromCC’&gt; &lt;NG type=‘enamex-org’ subtype=‘"
W04-1007,grover-etal-2000-lt,1,0.85314,"we encode all the results of linguistic processing as XML annotations. Figure 1 shows the broad details of the automatic processing that we perform, with the processing divided into an initial tokenisation module and a later linguistic annotation module. The architecture of our system is one where a range of NLP tools is used in a modular, pipelined way to add linguistic knowledge to the XML document markup. In the tokenisation module we convert from the source HTML to HOLXML and then pass the data through a sequence of calls to a variety of XMLbased tools from the LT TTT and LT XML toolsets (Grover et al., 2000; Thompson et al., 1997). The core program in our pipelines is the LT TTT program fsgmatch, a general purpose transducer which processes an input stream and adds annotations using rules provided in a hand-written grammar file. The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module (Mikheev, 1997). The first step in the tokenisation modules uses fsgmatch to segment the contents of the paragraphs into word tokens encoded in the XML as W elements. Once the word tokens have been identified, the next step uses ltpos to"
W04-1007,W03-0505,1,0.872796,"Missing"
W04-1007,J97-3003,0,0.0625554,"e to the XML document markup. In the tokenisation module we convert from the source HTML to HOLXML and then pass the data through a sequence of calls to a variety of XMLbased tools from the LT TTT and LT XML toolsets (Grover et al., 2000; Thompson et al., 1997). The core program in our pipelines is the LT TTT program fsgmatch, a general purpose transducer which processes an input stream and adds annotations using rules provided in a hand-written grammar file. The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module (Mikheev, 1997). The first step in the tokenisation modules uses fsgmatch to segment the contents of the paragraphs into word tokens encoded in the XML as W elements. Once the word tokens have been identified, the next step uses ltpos to mark up the senHTML document TOKENISATION MODULE Conversion to HOLXML Tokenisation POS Tagging & Sentence Identification LINGUISTIC ANALYSIS MODULE Lemmati− sation Named Entity Recognition Chunking & Clause Identification Verb & subject features Automatically annotated HOLXML document Figure 1: HOLJ processing stages tences as SENT elements and to add part of speech attribut"
W04-1007,W00-1427,0,0.0492813,"Missing"
W04-1007,W01-0708,0,0.0524648,"Missing"
W04-1007,W99-0311,0,0.0511574,"Missing"
W04-1907,J97-3003,0,\N,Missing
W04-1907,W01-0708,0,\N,Missing
W04-1907,W03-0424,0,\N,Missing
W04-1907,W00-1427,0,\N,Missing
W04-1907,grover-etal-2000-lt,1,\N,Missing
W06-2703,grover-etal-2000-lt,1,0.661411,"op a method for recording cases where the tokenisation is inconsistent with an annotator’s desired action so that subsequent retokenisation does not require reannotation. Note that the standoff annotation is stored at the bottom of the annotated file, not in a separate file. This is principally to simplify file handling issues which might arise if the annotations were stored separately. Word tokens are wrapped in w elements and are assigned unique ids in the id attribute. The tokenisation is created using significantly improved upgrades of the XML tools described in Thompson et al. (1997) and Grover et al. (2000)1 . The ents element contains all the entities that the annotator has marked and the link between the ent elements and the words is encoded with the sw and ew attributes (start word and end word) which point at word ids. For example, the protein fragment entity with id e7 starts at the first character of the word with id w630 and ends at the last character of the word with id w644. Our annotation tool and the format for storing annotations that we have chosen are just one instance of a wide range of possible tools and formats for the NE annotation task. There are a number of decision points in"
W06-2703,W04-1907,1,0.819142,"aries conflict with the annotators’ desired actions, then either the annotation is inaccurate or expensive retokenisation and reannotation will be required. Here we describe the methods we have developed to address this problem. We also describe experiments which explore the effects of different granularities of tokenisation on NER tagger performance. 1 Introduction A primary consideration when designing an annotation tool for annotation tasks such as Named Entity (NE) annotation is to provide an interface that 19 Figure 1: Screenshot of the Annotation Tool Dingare et al., 2004), law reports (Grover et al., 2004), social science (Nissim et al., 2004), and astronomy and astrophysics (Becker et al., 2005; Hachey et al., 2005). We have worked with a number of XML-based annotation tools, including the the NXT annotation tool (Carletta et al., 2003; Carletta et al., in press). Since we are interested only in written text and focus on annotation for Information Extraction (IE), much of the complexity offered by the NXT tool is not required and we have therefore recently implemented our own IEspecific tool. This has much in common with NXT, in particular annotations are encoded as standoff with pointers to t"
W06-2703,W05-0619,0,0.0148761,"nisation and reannotation will be required. Here we describe the methods we have developed to address this problem. We also describe experiments which explore the effects of different granularities of tokenisation on NER tagger performance. 1 Introduction A primary consideration when designing an annotation tool for annotation tasks such as Named Entity (NE) annotation is to provide an interface that 19 Figure 1: Screenshot of the Annotation Tool Dingare et al., 2004), law reports (Grover et al., 2004), social science (Nissim et al., 2004), and astronomy and astrophysics (Becker et al., 2005; Hachey et al., 2005). We have worked with a number of XML-based annotation tools, including the the NXT annotation tool (Carletta et al., 2003; Carletta et al., in press). Since we are interested only in written text and focus on annotation for Information Extraction (IE), much of the complexity offered by the NXT tool is not required and we have therefore recently implemented our own IEspecific tool. This has much in common with NXT, in particular annotations are encoded as standoff with pointers to the indices of the word tokens. A screenshot of the tool being used for NE annotation of biomedical text is shown"
W06-2703,W04-1213,0,0.040544,"contain less actual context. For example, in the excerpt ... using a Tet-on LMP1 HNE2 cell line ... assuming a fine-grained tokenisation, the pair of tokens LMP and 1 make up a protein entity. The left context would be the sequence using a Tet - on and the right context would be HNE 2 cell line. Depending on the size of window used to capture context this may or may not provide useful information. To demonstrate the effect that a finer-grained tokenisation can have on POS and NER tagging, we performed a series of experiments on the NER annotated data provided for the Coling BioNLP evaluation (Kim et al., 2004), which was derived from the GENIA corpus (Kim et al., 2003). (The BioNLP data is annotated with five entities, protein, DNA, RNA, cell type and cell line.) We trained the C&C maximum entropy tagger (Curran and Clark, 2003) using default settings to obtain 5 Tokenisation for Multiple Components So far we have discussed the problem of finding the correct level of granularity of tokenisation purely in terms of obtaining the optimal basis for NER annotation. However, the reason for ob4 http://www.w3.org/TR/xptr-xpointer/ 24 training # sentences eval # sentences training # tokens eval # tokens Pre"
W06-2703,W03-0419,0,0.107349,"Missing"
W06-2703,W03-0424,0,0.120818,"Missing"
W06-2703,W02-2024,0,0.0320759,"ily based on XML are GATE (Cunningham et al., 2002) and the annotation graph model of Bird and Liberman (2001). The GATE system organises annotations in graphs where the start and end nodes have pointers into the source document character offsets. This is an adaptation of the TIPSTER architecture (Grishman, 1997). (The UIMA system from IBM (Ferrucci and Lally, 2004) also stores annotations in a TIPSTERlike format.) The annotation graph model en3 Tokenisation Issues The most widely known examples of the NER task are the MUC competitions (Chinchor, 1998) and the CoNLL 2002 and 2003 shared task (Sang, 2002; Sang and De Meulder, 2003). In both cases the domain is newspaper text and the entities are general ones such as person, location, organisation etc. For this kind of data there are unlikely to be conflicts between tokenisation and entity mark-up and a vanilla tokenisation that splits at whitespace and punctuation is adequate. When dealing with scientific text and entities which refer to technical concepts, on the other hand, much more care needs to be taken with tokenisation. In the SEER project we collected a corpus of abstracts of radio astronomical papers taken from the NASA Astrophysics"
W07-1009,W05-1306,0,0.0218436,"Missing"
W07-1009,W03-0424,0,0.0115224,"(Grover and Tobin, 2006). 5.3 Named Entity Tagging The C&C tagger, referred to earlier, forms the basis of the NER component of the TXM natural language processing (NLP) pipeline designed to detect entity relations and normalisations (Grover et al., 2007). The tagger, in common with many ML approaches to NER, reduces the entity recognition problem to a sequence tagging problem by using the BIO encoding of entities. As well as performing well on the CoNLL-2003 task, Maximum Entropy Markov Models have also been successful on biomedical NER tasks (Finkel et al., 2005). As the vanilla C&C tagger (Curran and Clark, 2003) is optimised for performance on newswire text, various modifications were applied to improve its performance for biomedical NER. Table 3 lists the extra features specifically designed for biomedical text. The C&C tagger was also extended using several gazetteers, including a protein, complex, experimental method and modification gazetteer, targeted at recognising entities occurring in the EPPI data. Further postprocessing specific to the EPPI data involves correcting boundaries of some hyphenated proteins and filtering out entities ending in punctuation. All experiments with the C&C tagger in"
W07-1009,grover-tobin-2006-rule,1,0.466158,"del (MEMM) tagger developed by Curran and 69 Clark (2003) (hereafter referred to as C&C ) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al., 2004). Information on lemmatisation, as well as abbreviations and their long forms, is added using the morpha lemmatiser (Minnen et al., 2000) and the ExtractAbbrev script of Schwartz and Hearst (2003), respectively. A lookup step uses ontological information to identify scientific and common English names of species. Finally, a rule-based chunker marks up noun and verb groups and their heads (Grover and Tobin, 2006). 5.3 Named Entity Tagging The C&C tagger, referred to earlier, forms the basis of the NER component of the TXM natural language processing (NLP) pipeline designed to detect entity relations and normalisations (Grover et al., 2007). The tagger, in common with many ML approaches to NER, reduces the entity recognition problem to a sequence tagging problem by using the BIO encoding of entities. As well as performing well on the CoNLL-2003 task, Maximum Entropy Markov Models have also been successful on biomedical NER tasks (Finkel et al., 2005). As the vanilla C&C tagger (Curran and Clark, 2003)"
W07-1009,W95-0107,0,0.0109149,"tuations, it may make sense to relax these restrictions, for example by allowing entities to be nested inside other entities, or allowing discontinuous entities. GENIA (Ohta et al., 2002) and BioInfer (Pyysalo et al., 2007) are examples of recently produced NE-annotated biomedical corpora where entities nest. Corpora in other domains, for example the ACE1 data, also contain nested entities. This paper compares techniques for recognising nested entities in biomedical text. The difficulty of this task is that the standard method for converting NER to a sequence tagging problem with BIOencoding (Ramshaw and Marcus, 1995), where each 1 http://www.nist.gov/speech/tests/ace/ index.htm token is assigned a tag to indicate whether it is at the beginning (B), inside (I), or outside (O) of an entity, is not directly applicable when tokens belong to more than one entity. Here we explore methods of reducing the nested NER problem to one or more BIO problems so that existing NER tools can be used. This paper is organised as follows. In Section 2, the problem of nested entities is introduced and motivated with examples from GENIA and our EPPI (enriched protein-protein interaction) data. Related work is reviewed in Sectio"
W07-1009,W06-2703,1,0.905139,"format.2 The outermost annotation of coordinated structures and any continuous entity mark-up within them is retained. For example, in “human interleukin-2 and -4” both the continuous embedded entity “human interleukin-2” and the entire string are marked as proteins. The markup for discontinuous embedded entities, like “human interleukin-4” in the previous example, is not retained, as they could be derived in a post-processing step once nested entities are recognised. 3 Related Work In previous work addressing nested entities, Shen et al. (2003), Zhang et al. (2004), Zhou et al. (2004), Zhou (2006), and Gu (2006) considered the GENIA 2 Both corpora are represented in XML with standoff annotation, potentionally allowing overlapping NE s. corpus, where nested entities are relatively frequent. All these studies ignore embedded entities occurring in coordinated structures and only retain their outermost annotation. Shen et al. (2003), Zhang et al. (2004), and Zhou et al. (2004) all report on a rulebased approach to dealing with nested NEs in the GENIA corpus (Version 3.0) in combination with a Hidden Markov Model (HMM) that first recognises innermost NEs. They use four basic hand-crafted pa"
W07-1009,W03-1307,0,0.0183391,"differently, but for this work they are all converted to a common format.2 The outermost annotation of coordinated structures and any continuous entity mark-up within them is retained. For example, in “human interleukin-2 and -4” both the continuous embedded entity “human interleukin-2” and the entire string are marked as proteins. The markup for discontinuous embedded entities, like “human interleukin-4” in the previous example, is not retained, as they could be derived in a post-processing step once nested entities are recognised. 3 Related Work In previous work addressing nested entities, Shen et al. (2003), Zhang et al. (2004), Zhou et al. (2004), Zhou (2006), and Gu (2006) considered the GENIA 2 Both corpora are represented in XML with standoff annotation, potentionally allowing overlapping NE s. corpus, where nested entities are relatively frequent. All these studies ignore embedded entities occurring in coordinated structures and only retain their outermost annotation. Shen et al. (2003), Zhang et al. (2004), and Zhou et al. (2004) all report on a rulebased approach to dealing with nested NEs in the GENIA corpus (Version 3.0) in combination with a Hidden Markov Model (HMM) that first recogni"
W07-1009,W06-3318,0,0.167316,"e outermost annotation of coordinated structures and any continuous entity mark-up within them is retained. For example, in “human interleukin-2 and -4” both the continuous embedded entity “human interleukin-2” and the entire string are marked as proteins. The markup for discontinuous embedded entities, like “human interleukin-4” in the previous example, is not retained, as they could be derived in a post-processing step once nested entities are recognised. 3 Related Work In previous work addressing nested entities, Shen et al. (2003), Zhang et al. (2004), Zhou et al. (2004), Zhou (2006), and Gu (2006) considered the GENIA 2 Both corpora are represented in XML with standoff annotation, potentionally allowing overlapping NE s. corpus, where nested entities are relatively frequent. All these studies ignore embedded entities occurring in coordinated structures and only retain their outermost annotation. Shen et al. (2003), Zhang et al. (2004), and Zhou et al. (2004) all report on a rulebased approach to dealing with nested NEs in the GENIA corpus (Version 3.0) in combination with a Hidden Markov Model (HMM) that first recognises innermost NEs. They use four basic hand-crafted patterns and a co"
W07-1009,W04-1213,0,0.12087,"Missing"
W07-1009,W04-3111,0,0.0140137,"Missing"
W07-1009,H05-1124,0,0.0607621,"Missing"
W07-1009,W00-1427,0,0.0109833,"through a sequence of preprocessing steps implemented using the LT- XML2 and LT- TTT2 tools (Grover et al., 2006) with the output of each step encoded in XML mark-up. Tokenisation and sentence splitting is followed by part-ofspeech tagging with the Maximum Entropy Markov Model (MEMM) tagger developed by Curran and 69 Clark (2003) (hereafter referred to as C&C ) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al., 2004). Information on lemmatisation, as well as abbreviations and their long forms, is added using the morpha lemmatiser (Minnen et al., 2000) and the ExtractAbbrev script of Schwartz and Hearst (2003), respectively. A lookup step uses ontological information to identify scientific and common English names of species. Finally, a rule-based chunker marks up noun and verb groups and their heads (Grover and Tobin, 2006). 5.3 Named Entity Tagging The C&C tagger, referred to earlier, forms the basis of the NER component of the TXM natural language processing (NLP) pipeline designed to detect entity relations and normalisations (Grover et al., 2007). The tagger, in common with many ML approaches to NER, reduces the entity recognition prob"
W07-1009,W03-0419,0,\N,Missing
W10-0514,mani-etal-2008-spatialml,0,0.0229449,"Missing"
W10-1804,alex-etal-2006-impact,1,0.827936,"tract This paper describes work testing agile data annotation by moving away from the traditional, linear phases of corpus creation towards iterative ones and by recognizing the potential for sources of error occurring throughout the annotation process. 1 Introduction Annotated data sets are an important resources for various research fields, including natural language processing (NLP) and text mining (TM). While the detection of annotation inconsistencies in different data sets has been investigated (e.g. Nov´ak and Raz´ımov´a, 2009) and their effect on NLP performance has been studied (e.g. Alex et al. 2006), very little work has been done on deriving better methods of annotation as a whole process in order to maximize both the quality and quantity of annotated data. This paper describes our annotation project in which we tested the relatively new approach of agile corpus annotation (Voormann and Gut, 2008) of moving away from the traditional, linear phases of corpus creation towards iterative ones and of recognizing the fact that sources of error can occur throughout the annotation process. We explain agile annotation and discuss related work in Section 2. Section 3 describes the entire annotati"
W10-1804,W08-0603,0,0.0248339,"Missing"
W10-1804,W09-3024,0,0.0655896,"Missing"
W10-4125,J05-4005,0,0.0253241,"acters have three different functionalities: delimiting, structuring and disambiguating. Space characters are natural delimiters in some languages. In English and many other Latin-based languages for example, spaces are used for separating words and certain punctuation marks (e.g. period and colon). However, in formal Chinese typesetting, spaces are not used to delimit words or characters. Hence the need for automatic word segmentation systems (Zhang et al., 2003). The current segmentation systems mainly focus on resolving ambiguities and detecting new words in segmenting text with no spaces (Gao et al., 2005). However, ambiguities can be caused not only by characters themselves, but also Disambiguating spaces occur where an unintentional ambiguity could result if the spaces were not there. Two types of ambiguities are usually caused by ignoring the effect of white space: Overlapping Ambiguity, where a set of tokens can either be appended to the previous set of tokens to form an entity, or precede the next set of tokens to form a different entity. For example, in a Chinese CV’s job history section, the following two situations could occur: 1999年10月1 日本公司会计 1999.10.1 A Japanese Company Accountant 19"
W10-4125,grover-tobin-2006-rule,1,0.718322,"f the document. This is especially true for semi-structured documents such as CVs. As our results indicate, integrating simple layout information with linguistic grammars can greatly improve the performance of information extraction. A further improvement can be achieved using the two filters introduced in the fourth section. Although Daxtra’s grammar formalism is chosen as the tool for information extraction, since it already includes treatment of space characters, other tools are also available to carry out the same job. For example, Edinburgh University Language Technology Group’s LT-TTT2 (Grover and Tobin, 2006) 3 . Our paper focuses mainly on Chinese CVs, but space layout information can be used widely in other languages and documents. In English for example, although words are separated by a single space, spaces are not always used as delimiters (e.g. constructing tables, columns), thus providing the need for integrating space layout. In terms of document types, plain paragraph based text (e.g. articles, blogs etc.) may not be affected too much by space characters, but integrating space layout information in parsing these documents should not decrease performance either. Furthermore, semi-structure"
W10-4125,C00-1049,0,0.0227876,"n Chitain layout, but also to signal a certain syntactic strucnese information extraction by parsing some ture. Some researchers have been seen to make use of semi-structured documents with two simispace characters, but they mainly use spaces to crelar grammars - one with treatment for space ate or recognise certain special layouts. For example, characters, the other ignoring it. This paper (Rus and Summers, 1994) used white spaces to reforalso introduces two post processing filters mat documents into somewhat structured styles; (Ng to further improve treatment of space charet al., 1999) and (Hurst and Nasukawa, 2000) used acters. Results show that the grammar that spaces to recognise tables in free text. Wrapper gentakes account of spaces clearly out-performs eration is more related to our research since it uses the one that ignores them, and so concludes layout to extract structured content from documents that space characters can play a useful role in (Irmak and Suel, 2006; Chen et al., 2003). Howinformation extraction. ever, wrapper generation is too high level, this paper is aimed at exploring the effects of space characters 1 Introduction at a lower level. It is well known that a snippet of text in C"
W10-4125,C94-1069,0,0.0550863,"disetc. In some cases, such structuring space characters ambiguation. Finally, we perform evaluation of the represent a relation between the elements that the tools on a set of real-world CVs and give proposals spaces are delimiting. For the following example, for future work. each line contains a label and a value separated using spaces to create a table. 2 Space Characters A space character, when considered as punctu姓名(Name) 李某某 年龄(Age) 25岁 ation, is a blank area devoid of content, serving to Email li25@gmail.com separate words, letters, numbers and other punctu籍贯(Place of Birth) 上海 ation. (Jones, 1994) found broadly three types of punctuation marks: delimiting, separating and disambiguating. Similarly, space characters have three different functionalities: delimiting, structuring and disambiguating. Space characters are natural delimiters in some languages. In English and many other Latin-based languages for example, spaces are used for separating words and certain punctuation marks (e.g. period and colon). However, in formal Chinese typesetting, spaces are not used to delimit words or characters. Hence the need for automatic word segmentation systems (Zhang et al., 2003). The current segme"
W10-4125,P99-1057,0,0.0988985,"Missing"
W10-4125,W03-1730,0,0.0159409,"lace of Birth) 上海 ation. (Jones, 1994) found broadly three types of punctuation marks: delimiting, separating and disambiguating. Similarly, space characters have three different functionalities: delimiting, structuring and disambiguating. Space characters are natural delimiters in some languages. In English and many other Latin-based languages for example, spaces are used for separating words and certain punctuation marks (e.g. period and colon). However, in formal Chinese typesetting, spaces are not used to delimit words or characters. Hence the need for automatic word segmentation systems (Zhang et al., 2003). The current segmentation systems mainly focus on resolving ambiguities and detecting new words in segmenting text with no spaces (Gao et al., 2005). However, ambiguities can be caused not only by characters themselves, but also Disambiguating spaces occur where an unintentional ambiguity could result if the spaces were not there. Two types of ambiguities are usually caused by ignoring the effect of white space: Overlapping Ambiguity, where a set of tokens can either be appended to the previous set of tokens to form an entity, or precede the next set of tokens to form a different entity. For"
W14-0617,grover-tobin-2006-rule,1,0.625846,"rpretation of the information. For this reason, OCR quality needs to be exceptionally high and we have been fortunate that our digitisation partner was able to ensure this high quality. As the survey was created over a period of decades under the supervision of several editors, there is some variability in format across the volumes. The most pertinent variation concerns grid references: early volumes either do not have any or use grid references that cannot be converted to 120 Figure 2: Processing pipeline which have been developed specifically for rulebased processing of text in NLP systems (Grover and Tobin (2006), Tobin et al. (2010)). Along with shell scripting these tools allow us to build up the components that comprise the pipeline. The output of step 2 contains XML elements for paragraphs, sentences and word/punctuation tokens. Font and style information is encoded as attributes on the tokens and line break hyphenation is repaired. Part-of-speech tagging is unnecessary: the named entity classes we recognise are primarily identified by position in the document or on the page in combination with font and style information. Once the tokens are marked up, finer-grained structural mark-up can be compu"
W14-4908,mani-etal-2008-spatialml,0,0.0282009,"f geo-resolution. It has been developed in step with our geo-resolution system, the Edinburgh Geoparser (Grover et al., 2010), but it could also be used to correct the output of other tools. In our work, we use the geo-annotator to create gold-standard material for geo-resolution evaluation and have produced accompanying scoring software.1 2 Related Work Within the field of NLP, SpatialML is probably the best known work in the area of geo-referencing. SpatialML is an annotation scheme for marking up natural language references to places and grounding them to coordinates. The SpatialML corpus (Mani et al., 2008) instantiates this annotation scheme and can be used as an evaluation corpus for geo-resolution (Tobin et al., 2010). Other researchers develop their own geo-annotated corpora and evaluate against these, e.g. Clough (2005), Leidner (2007). Within the field of Information Retrieval, there is an ACM special interest group on spatially-related information, SIGSPATIAL2 , with regular geographic IR conferences (GIR conferences) where georeferencing research is presented, see for example Purves et al. (2007). There are currently several geoparsing tools available, such as GeoLocate3 , and CLAVIN4 ,"
