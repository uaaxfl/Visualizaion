2002.tmi-papers.14,W99-0606,0,0.0689983,"Missing"
2002.tmi-papers.14,A00-2020,0,0.0829102,"achine translation. We describe the modality corpus in Section 2, the method of corpus correction in Section 3, and our experiments on corpus correction in Section 4. 2 Modality Corpus for Machine Translation In this section, we describe the modality corpus. A part of the modality corpus is shown in Figure 1. It is composed of a Japanese-English bilingual corpus; each English sentence can include two types of tags: 1 There is no previous paper on error correction in corpora. In terms of error detection in corpora, there has been research using boosting or anomaly detection (Abney et al. 1999; Eskin 2000). , kono kodomo wa aa ieba kou iu kara koniku-rashii This child always talks back to me, and this &lt;v>is&lt;/v> why I &lt;vj>hate&lt;/vj> him. d kare ga aa okubyou da to wa omowanakatta I &lt;v>did not think&lt;/v> he was so timid. c aa isogashikute wa yasumu hima mo nai hazu da Such a busy man as he &lt;v>cannot have&lt;/v> any spare time. Figure 1: Part of the modality corpus • The English main verb phrase is tagged with &lt;v>. • The English verb phrase corresponding to the Japanese main verb phrase is tagged with &lt;vj>. The symbols at the beginning of each Japanese sentence, such as “c” and “d”, indicate a category"
2002.tmi-papers.14,1999.tmi-1.7,1,0.874862,"Missing"
2002.tmi-papers.14,P94-1013,0,0.00968044,"f each category/feature pair as calculated from p˜(a, b) are the same as those from p(a, b) (this corresponds to Equation (1).) These estimated values are not so sparse. We can thus use the above assumption for calculating p(a, b). Furthermore, we maximize the entropy of the distribution of p˜(a, b) to obtain one solution of p˜(a, b), because using only Equation 1 produces several solutions for p˜(a, b). Maximizing the entropy has the effect of making the distribution more uniform and is considered to be a good solution for data sparseness problems. • Method based on the decision-list method (Yarowsky 1994) In this method, the probability of each category is calculated using one of the features, f j (∈ F,  ≤ j ≤ k). The probability that produces category a in context b is given by the following equation: p(a|b) = p(a|f max ), (3) such that f max is defined by f max = argmaxf j ∈F maxai ∈A p˜(ai |f j ), (4) where p˜(ai |f j ) is the occurrence rate of category a i when the context has feature f j. In this paper, we used the following items as features, which are the context when the probabilities are calculated; 26 (= 5 + 10 + 10 + 1) features appear in each English sentence: • the strings of 1-"
2007.mtsummit-papers.63,J93-2003,0,0.0367977,"Missing"
2007.mtsummit-papers.63,J93-1004,0,0.457946,"patent parallel corpus. We used simple pattern matching programs to extract the embodiment and background parts from the whole document pairs and obtained 77,014 embodiment part pairs and 72,589 background part pairs. We then applied the alignment procedure described in Section 3. to these 149,603 pairs. We call these embodiment and background parts documents. 3. 3.1. Alignment procedure Score of sentence alignment We used Utiyama and Isahara’s method (Utiyama and Isahara, 2003) to score sentence alignments. We first aligned sentences3 in each document by using a standard DP matching method (Gale and Church, 1993; Utsuro et al., 1994). We allowed 1-to-n, n-to-1 (0 ≤ n ≤ 5), or 2-to-2 alignments when aligning the sentences. A concise description of the algorithm used is described elsewhere (Utsuro et 2 Some USPTO patents have priority information that identify foreign applications for the same subject matters. Higuchi et al. (2001) have used such corresponding patents filed in both Japan and the United States to extract bilingual lexicons. 3 We split the Japanese documents into sentences by using simple heuristics and split the English documents into sentences by using a maximum entropy sentence splitt"
2007.mtsummit-papers.63,2001.mtsummit-papers.30,0,0.073559,"Missing"
2007.mtsummit-papers.63,W06-3114,0,0.0811281,"Missing"
2007.mtsummit-papers.63,2005.iwslt-1.8,0,0.0181508,"t in TEST that contained example 5R, even though these two patents were different. These examples show that even long and/or specific expressions are reused in patent documents. We think that this characteristic of patents contributed to the good translations. The middle and bottom examples (6 to 15) were generally not good translations. These examples adequately translated individual phrases. However, they failed to adequately reorder phrases. This suggests that we need more accurate models for reordering. Thus, our patent corpus will be a good corpus for comparing various reordering models (Koehn et al., 2005; Nagata et al., 2006; Xiong et al., 2006). 6. Discussion We have described the characteristic of our patent parallel corpus and showed that it could be a good corpus for promoting MT research. In this section, we describe three issues about ALL that we found during investigating it as described in Sections 3., 4., and 5. We want to resolve these issues when we extend it for the NTCIR-7 patent MT task. Issue 1. Our noise reduction procedure described in Section 3.2. reduced the number of sentences from about 4.2 million to about 3.9 million. This reduction could be too aggressive. We want to i"
2007.mtsummit-papers.63,koen-2004-pharaoh,0,0.0562327,"Missing"
2007.mtsummit-papers.63,2005.mtsummit-papers.11,0,0.132568,"Missing"
2007.mtsummit-papers.63,ma-cieri-2006-corpus,0,0.0478548,"Missing"
2007.mtsummit-papers.63,J05-4003,0,0.0346251,"ess in corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) has been supported by large parallel corpora, such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006) and the Europarl corpus (Koehn, 2005) consisting of 11 European languages. However, large parallel corpora do not exist for many language pairs. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Munteanu and Marcu (2005) have extracted parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Utiyama and Isahara (2003) have extracted JapaneseEnglish parallel sentences from a noisy-parallel corpus. We have recently aligned Japanese and English sentences in Japanese and US patent data provided for the NTCIR-6 patent retrieval task (Fujii et al., 2007). We used Utiyama and Isahara’s method to extract clean sentence alignments. The number of extracted sentence alignments was about 2 million. These sentence pairs and all alignment data that were produced during the alignment proced"
2007.mtsummit-papers.63,P06-1090,0,0.0239539,"ined example 5R, even though these two patents were different. These examples show that even long and/or specific expressions are reused in patent documents. We think that this characteristic of patents contributed to the good translations. The middle and bottom examples (6 to 15) were generally not good translations. These examples adequately translated individual phrases. However, they failed to adequately reorder phrases. This suggests that we need more accurate models for reordering. Thus, our patent corpus will be a good corpus for comparing various reordering models (Koehn et al., 2005; Nagata et al., 2006; Xiong et al., 2006). 6. Discussion We have described the characteristic of our patent parallel corpus and showed that it could be a good corpus for promoting MT research. In this section, we describe three issues about ALL that we found during investigating it as described in Sections 3., 4., and 5. We want to resolve these issues when we extend it for the NTCIR-7 patent MT task. Issue 1. Our noise reduction procedure described in Section 3.2. reduced the number of sentences from about 4.2 million to about 3.9 million. This reduction could be too aggressive. We want to investigate the effect"
2007.mtsummit-papers.63,J03-1002,0,0.0201032,"Missing"
2007.mtsummit-papers.63,E99-1010,0,0.0918638,"Missing"
2007.mtsummit-papers.63,P03-1021,0,0.0839448,"Missing"
2007.mtsummit-papers.63,P02-1040,0,0.0912947,"Missing"
2007.mtsummit-papers.63,J03-3002,0,0.0579747,"almost perfectly the contents of the corresponding Japanese sentences. 1. Introduction The rapid and steady progress in corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) has been supported by large parallel corpora, such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006) and the Europarl corpus (Koehn, 2005) consisting of 11 European languages. However, large parallel corpora do not exist for many language pairs. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Munteanu and Marcu (2005) have extracted parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Utiyama and Isahara (2003) have extracted JapaneseEnglish parallel sentences from a noisy-parallel corpus. We have recently aligned Japanese and English sentences in Japanese and US patent data provided for the NTCIR-6 patent retrieval task (Fujii et al., 2007). We used Utiyama and Isahara’s method to extract clean sentence alignments. The number of extracted sentence alignme"
2007.mtsummit-papers.63,P03-1010,1,0.951793,"the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006) and the Europarl corpus (Koehn, 2005) consisting of 11 European languages. However, large parallel corpora do not exist for many language pairs. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Munteanu and Marcu (2005) have extracted parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Utiyama and Isahara (2003) have extracted JapaneseEnglish parallel sentences from a noisy-parallel corpus. We have recently aligned Japanese and English sentences in Japanese and US patent data provided for the NTCIR-6 patent retrieval task (Fujii et al., 2007). We used Utiyama and Isahara’s method to extract clean sentence alignments. The number of extracted sentence alignments was about 2 million. These sentence pairs and all alignment data that were produced during the alignment procedure are planed to be used in the NTCIR-7 patent MT task.1 This is the largest Japanese-English parallel corpus, which will be availab"
2007.mtsummit-papers.63,C94-2175,0,0.0839682,". We used simple pattern matching programs to extract the embodiment and background parts from the whole document pairs and obtained 77,014 embodiment part pairs and 72,589 background part pairs. We then applied the alignment procedure described in Section 3. to these 149,603 pairs. We call these embodiment and background parts documents. 3. 3.1. Alignment procedure Score of sentence alignment We used Utiyama and Isahara’s method (Utiyama and Isahara, 2003) to score sentence alignments. We first aligned sentences3 in each document by using a standard DP matching method (Gale and Church, 1993; Utsuro et al., 1994). We allowed 1-to-n, n-to-1 (0 ≤ n ≤ 5), or 2-to-2 alignments when aligning the sentences. A concise description of the algorithm used is described elsewhere (Utsuro et 2 Some USPTO patents have priority information that identify foreign applications for the same subject matters. Higuchi et al. (2001) have used such corresponding patents filed in both Japan and the United States to extract bilingual lexicons. 3 We split the Japanese documents into sentences by using simple heuristics and split the English documents into sentences by using a maximum entropy sentence splitter available at http:/"
2007.mtsummit-papers.63,P06-1066,0,0.0226571,"though these two patents were different. These examples show that even long and/or specific expressions are reused in patent documents. We think that this characteristic of patents contributed to the good translations. The middle and bottom examples (6 to 15) were generally not good translations. These examples adequately translated individual phrases. However, they failed to adequately reorder phrases. This suggests that we need more accurate models for reordering. Thus, our patent corpus will be a good corpus for comparing various reordering models (Koehn et al., 2005; Nagata et al., 2006; Xiong et al., 2006). 6. Discussion We have described the characteristic of our patent parallel corpus and showed that it could be a good corpus for promoting MT research. In this section, we describe three issues about ALL that we found during investigating it as described in Sections 3., 4., and 5. We want to resolve these issues when we extend it for the NTCIR-7 patent MT task. Issue 1. Our noise reduction procedure described in Section 3.2. reduced the number of sentences from about 4.2 million to about 3.9 million. This reduction could be too aggressive. We want to investigate the effect of noise reduction o"
2008.amta-papers.8,fujii-etal-2006-test,1,0.857007,"e machine translation systems. Our test collection also includes search topics for cross-lingual patent retrieval, which can be used to evaluate the contribution of machine translation to retrieving patent documents across languages. This paper describes our test collection, methods for evaluating machine translation, and preliminary experiments. 1 Introduction Since the Third NTCIR Workshop in 20011 , which was an evaluation forum for research and development in information retrieval and natural language processing, the Patent Retrieval Task has been performed repeatedly (Fujii et al., 2004; Fujii et al., 2006; Fujii et al., 2007b; Iwayama et al., 2006). In the Sixth NTCIR Workshop (Fujii et al., 2007b), patent documents published over a 10-year period by the Japanese Patent Office (JPO) and the US Patent & Trademark Office (USPTO) were independently used as target document collections. 1 Having explored patent retrieval issues for a long time, we decided to address another issue in patent processing. From among a number of research issues related to patent processing (Fujii et al., 2007a), we selected Machine Translation (MT) of patent documents, which is useful for a number of applications and se"
2008.amta-papers.8,2001.mtsummit-papers.30,1,0.541065,"cations in foreign countries. Reflecting the rapid growth in the use of multilingual corpora, a number of data-driven MT methods have recently been explored, most of which are termed “Statistical Machine Translation (SMT)”. While large bilingual corpora for European languages, Arabic, and Chinese are available for research and development purposes, these corpora are rarely associated with Japanese and therefore it is difficult for explore SMT with respect to Japanese. However, we found that the patent documents used for the NTCIR Workshops can potentially alleviate this data scarcity problem. Higuchi et al. (2001) used “patent families” as a parallel corpus for extracting new translations. A patent family is a set of patent documents for the same or related inventions and these documents are usually filed in more than one country in various languages. Following Higuchi et al’s method, we can produce a bilingual corpus for Japanese and English. In addition, there are a number of SMT engines (decoders) available to the public, such as Pharaoh and Moses2 , which can be applied to bilingual corpora involving any pair of languages. Motivated by the above background, we de2 http://research.nii.ac.jp/ntcir/in"
2008.amta-papers.8,W04-3250,0,0.13242,"Missing"
2008.amta-papers.8,P02-1040,0,0.0861881,"train their MT system, whether it is a data-driven SMT or a conventional knowledge-intensive rule-based MT. Second, the organizers provide the groups with a test data set of sentences in either Japanese or English. Each group is requested to machine translate each sentence from its original language into the other language and submit their translation results to the organizers. Third, the organizers evaluate the submission from each group. We use both intrinsic and extrinsic evaluation methods. In the intrinsic evaluation, we independently use both the Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002), which was proposed as an automatic evaluation measure for MT, and human judgment. In the extrinsic evaluation, we investigate the contribution of the MT to CLPR. In the Patent Retrieval Task at NTCIR5, aimed at CLPR, search topics in Japanese were translated into English by human experts. We reuse these search topics for the evaluation of the MT. We also analyze the relationship between different evaluation measures. The use of extrinsic evaluation, which is not performed in existing MT-related evaluation activities, such as the NIST MetricsMATR Challenge3 and the IWSLT Workshop4 , is a dist"
2008.amta-papers.8,2007.mtsummit-papers.63,1,0.907449,"Missing"
2008.iwslt-evaluation.11,W08-0334,1,0.872755,"ation task, we integrated two strategies for pivot translation by linear interpolation. 1. Introduction This paper describes the NICT/ATR SMT system used in the International Workshop on Spoken Language Translation (IWSLT) 2008 evaluation campaign. We participated in the following translation tasks: Chinese–English (Challenge Task), English–Chinese (Challenge Task), Chinese–English (BTEC Task), Chinese–Spanish (BTEC Task), and Chinese– English–Spanish (PIVOT Task). Although our theme for each task was different, our systems were based on a fairly common phrase-based machine translation system [1], which was built within the framework of a feature-based exponential model. The model has the following features: • Phrase translation probability form source to target • Inverse phrase translation probability • Lexical weighting probability from source to target • Lexical reordering probability • Simple distance-based distortion model • Word penalty The decoder used for the training and decoding was the in-house multi-stack phrase-based decoder CleopATRa. The decoder can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapte"
2008.iwslt-evaluation.11,P07-2045,0,0.0149744,"e based on a fairly common phrase-based machine translation system [1], which was built within the framework of a feature-based exponential model. The model has the following features: • Phrase translation probability form source to target • Inverse phrase translation probability • Lexical weighting probability from source to target • Lexical reordering probability • Simple distance-based distortion model • Word penalty The decoder used for the training and decoding was the in-house multi-stack phrase-based decoder CleopATRa. The decoder can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapted from the MOSES decoder. We used GIZA++ [3] for word alignment and SRILM [4] for language modeling. We used 5-gram language models trained with modified Knesser–Ney smoothing. The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese tran"
2008.iwslt-evaluation.11,J03-1002,0,0.00224261,"a feature-based exponential model. The model has the following features: • Phrase translation probability form source to target • Inverse phrase translation probability • Lexical weighting probability from source to target • Lexical reordering probability • Simple distance-based distortion model • Word penalty The decoder used for the training and decoding was the in-house multi-stack phrase-based decoder CleopATRa. The decoder can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapted from the MOSES decoder. We used GIZA++ [3] for word alignment and SRILM [4] for language modeling. We used 5-gram language models trained with modified Knesser–Ney smoothing. The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese translation has been researched to a lesser extent than Chinese-English translation. Thus, we examined various fact"
2008.iwslt-evaluation.11,P03-1021,0,0.0223499,"r can operate on the same principles as the MOSES decoder [2]. For the training of SMT models, we used a training toolkit adapted from the MOSES decoder. We used GIZA++ [3] for word alignment and SRILM [4] for language modeling. We used 5-gram language models trained with modified Knesser–Ney smoothing. The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese translation has been researched to a lesser extent than Chinese-English translation. Thus, we examined various factors affecting English–Chinese translation. Table 1 summarizes the BLEU scores for correct recognition results (CRR). The BLEU scores [6] for “devset” are obtained with the small Challenge Task devset corpus (comprising 251 sentences). The devset corpus was also used for MERT.1 Thus, the results in Table 1 for devset were obtained from closed experiments. The results for “devset3” (506 sentences) were obtained by using the param"
2008.iwslt-evaluation.11,P02-1040,0,0.0774265,"The language models were trained with SMT training corpora on the target side. Minimum error rate training (MERT) was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score, and training was performed using the standard technique developed by Och [5]. 2. English–Chinese (Challenge Task) English–Chinese translation has been researched to a lesser extent than Chinese-English translation. Thus, we examined various factors affecting English–Chinese translation. Table 1 summarizes the BLEU scores for correct recognition results (CRR). The BLEU scores [6] for “devset” are obtained with the small Challenge Task devset corpus (comprising 251 sentences). The devset corpus was also used for MERT.1 Thus, the results in Table 1 for devset were obtained from closed experiments. The results for “devset3” (506 sentences) were obtained by using the parameters tuned on devset (open experiments). The BLEU scores were calculated based on Chinese character n-grams. When calculating BLEU scores, we removed out-of-vocabulary (OOV) words from the machine translated text and ignored punctuation. 1 We used 3-gram language models for performing MERT and used 5gra"
2008.iwslt-evaluation.11,W08-0335,1,0.86956,"Missing"
2008.iwslt-evaluation.11,N06-2049,1,0.880518,"Missing"
2008.iwslt-evaluation.11,I05-3017,0,0.087631,"Missing"
2008.iwslt-evaluation.11,D07-1054,1,0.880592,"Missing"
2008.iwslt-evaluation.11,N07-1061,1,0.865684,"of the language X to language Y SMT system by using corpus X of the “X-E corpus” and the newly created corpus Y’. After developing the models, as described above, we remove all the phrase table entries that have OOV words on the target side of the phrase table. We will call the system developed above the X2Y’ system and the strategy PseudoCorpusY. In this system, the target side of the phrase table is not completely reliable. For training these systems, we develop and use a language model using corpus Y of the “Y-E corpus.” 4.4. Phrase Table Composition This strategy was introduced by Utiyama [12]. In order to implement this strategy, we first develop the X2E system using the “X-E corpus” and the E2Y system using the “Y-E corpus.” Then, we compose a new phrase table from the phrase tables of the X2E and E2Y systems. For the purpose of integrating two models, we extend this strategy to include the lexicalized reordering model. 4.5. Linear Interpolation This strategy is used to develop new models from those described above, by linear interpolation: the phrase translation model and the lexicalized reordering model. First, we interpolate two PseudoCorpus models. These models are de- 82 - n"
2008.iwslt-evaluation.11,W05-0909,0,0.115895,"Missing"
2009.iwslt-evaluation.12,W08-0334,1,\N,Missing
2009.iwslt-evaluation.12,P07-2045,0,\N,Missing
2009.iwslt-evaluation.12,P03-1021,0,\N,Missing
2009.iwslt-evaluation.12,J03-1002,0,\N,Missing
2009.mtsummit-papers.18,J93-2003,0,0.029315,"e to mine parallel texts from mixedlanguage web pages. We define a mixedlanguage web page as a web page consisting of (at least) two languages. We mined Japanese-English parallel texts from mixedlanguage web pages. We presented the statistics for extracted parallel texts and conducted machine translation experiments. These statistics and experiments showed that mixedlanguage web pages are rich sources of parallel texts. 1 Introduction Parallel corpora are indispensable language resources for multi-lingual natural language processing, such as corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) and cross-lingual information retrieval. However, there are relatively few widely available parallel corpora. These include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and langu"
2009.mtsummit-papers.18,W08-0334,1,0.880155,"Missing"
2009.mtsummit-papers.18,W07-0717,0,0.0203036,"ure 1 and Table 4 show that the performance of the SMT systems trained with our extracted sentence alignments are inferior to that of the SMT system trained with the IWSLT training data. A likely reason is that the extracted alignments are out-ofdomain data with respect to the IWSLT testsets. In the following, we show that the extracted alignments are useful for improving the performance of the SMT system trained with the IWSLT training data, even though these alignments are not best suited to the testsets. 4.3 Interpolation of models We linearly interpolated8 language and translation models (Foster and Kuhn, 2007) to improve the performance of the SMT system. 4.3.1 Interpolation of language models We first interpolated language models (LMs). We interpolated the language model made from 900,000 sentences in Section 4.1 (hereafter LM(900k)) and that made from the IWSLT training data in Section 4.2 (hereafter LM(IWSLT)). The weight of LM(IWSLT) was 0.1, 0.2, ..., 0.9. In addition to these interpolated language models, we used the translation model made from the IWSLT training data in Section 4.2 for all of the weights. The figures in the 0.1, ..., 0.9 rows in Table 5 show the BLEU scores for set2, ..., se"
2009.mtsummit-papers.18,W04-3208,0,0.161396,"005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to our work, previous studie"
2009.mtsummit-papers.18,J93-1004,0,0.340799,"es into noisy parallel text files. That is, given a web page containing Japanese and English texts, we made a Japanese text file and an English text file from the web page.3 We regarded these two text files as a pair of noisy parallel text files and applied Utiyama and Isahara’s method to these. In the following, we briefly describe how we applied Utiyama and Isahara’s method to these parallel texts. See (Utiyama and Isahara, 2007) for details of their method. We first aligned the sentences in each pair of noisy parallel text files by using a standard dynamic programming (DP) matching method (Gale and Church, 1993; Utsuro et al., 1994). That is, let J and E be a Japanese text file and an English text file, respectively, we calculated the maximum similarity sen2 Our mining method will not be much affected by N because our method can extract parallel sentences very accurately as shown in Section 3. 3 We simply extracted Japanese (English) sentences from the web page and put them into a Japanese (English) text file. Pm AVSIM(J, E) = i=1 SIM(Ji , Ei ) R(J, E) = min( m |J ||E| , ) |E ||J| (1) (2) where |J |is the number of sentences in J, and |E |is the number of sentences in E. A high R(J, E) value occurs"
2009.mtsummit-papers.18,kawahara-kurohashi-2006-case,1,0.855263,"Missing"
2009.mtsummit-papers.18,P07-2045,0,0.0122038,"Missing"
2009.mtsummit-papers.18,2005.mtsummit-papers.11,0,0.0791756,"chine translation experiments. These statistics and experiments showed that mixedlanguage web pages are rich sources of parallel texts. 1 Introduction Parallel corpora are indispensable language resources for multi-lingual natural language processing, such as corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) and cross-lingual information retrieval. However, there are relatively few widely available parallel corpora. These include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung"
2009.mtsummit-papers.18,ma-cieri-2006-corpus,0,0.0118588,"extracted parallel texts and conducted machine translation experiments. These statistics and experiments showed that mixedlanguage web pages are rich sources of parallel texts. 1 Introduction Parallel corpora are indispensable language resources for multi-lingual natural language processing, such as corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) and cross-lingual information retrieval. However, there are relatively few widely available parallel corpora. These include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002"
2009.mtsummit-papers.18,1999.mtsummit-1.79,0,0.219583,"llel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to our work, previous studies have mined parallel texts from parallel web pages. A pair of parallel web pages consists of two monolingual web pages in different languages with almost the same meaning. For exa"
2009.mtsummit-papers.18,J05-4003,0,0.437791,"uropean languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to our work, previous studies have mined parallel texts fro"
2009.mtsummit-papers.18,J03-1002,0,0.00533169,"000 sentence alignments were effective for multi-lingual natural language processing. Based on the statistics presented in Sections 3.2 and 3.3, we concluded that we extracted a clean parallel corpus from the original web corpus. 4 Machine translation experiments We verify the usefulness of the extracted sentence alignments for SMT in this section. We used a state-of-the-art phrase-based SMT system (Finch and Sumita, 2008), which is comparable in performance to the MOSES system (Koehn et al., 2007). To train SMT models, we used a training toolkit adapted from the MOSES system. We used GIZA++ (Och and Ney, 2003) for word alignment and SRILM (Stolcke, 2002) for language modeling. We used 5-gram language models trained with modified Kneser–Ney smoothing. Minimum error rate training was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score (Papineni et al., 2002), and tuning was performed using the standard technique developed by Och (Och, 2003). We used the development data for the IWSLT2007 Japanese-English translation task (Fordyce, 2007) to verify the usefulness of the extracted sentence alignments. The development data consisted of five sets, devset1"
2009.mtsummit-papers.18,P03-1021,0,0.0112609,"system (Finch and Sumita, 2008), which is comparable in performance to the MOSES system (Koehn et al., 2007). To train SMT models, we used a training toolkit adapted from the MOSES system. We used GIZA++ (Och and Ney, 2003) for word alignment and SRILM (Stolcke, 2002) for language modeling. We used 5-gram language models trained with modified Kneser–Ney smoothing. Minimum error rate training was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score (Papineni et al., 2002), and tuning was performed using the standard technique developed by Och (Och, 2003). We used the development data for the IWSLT2007 Japanese-English translation task (Fordyce, 2007) to verify the usefulness of the extracted sentence alignments. The development data consisted of five sets, devset1, devset2, devset3, devset4, and devset5. Each of these data sets had about 500 sentences. The numbers of reference translations were 16 for devset1, devset2, and devset3 and 7 for devset4 and devset5. We used devset1 to tune the SMT system and used devset2, devset3, devset4, and devset5 as the testsets to evaluate the performance of the SMT system in terms of BLEU scores. Hereafter,"
2009.mtsummit-papers.18,P02-1040,0,0.079854,"tracted sentence alignments for SMT in this section. We used a state-of-the-art phrase-based SMT system (Finch and Sumita, 2008), which is comparable in performance to the MOSES system (Koehn et al., 2007). To train SMT models, we used a training toolkit adapted from the MOSES system. We used GIZA++ (Och and Ney, 2003) for word alignment and SRILM (Stolcke, 2002) for language modeling. We used 5-gram language models trained with modified Kneser–Ney smoothing. Minimum error rate training was used to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score (Papineni et al., 2002), and tuning was performed using the standard technique developed by Och (Och, 2003). We used the development data for the IWSLT2007 Japanese-English translation task (Fordyce, 2007) to verify the usefulness of the extracted sentence alignments. The development data consisted of five sets, devset1, devset2, devset3, devset4, and devset5. Each of these data sets had about 500 sentences. The numbers of reference translations were 16 for devset1, devset2, and devset3 and 7 for devset4 and devset5. We used devset1 to tune the SMT system and used devset2, devset3, devset4, and devset5 as the testse"
2009.mtsummit-papers.18,J03-3002,0,0.231052,"ese include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two"
2009.mtsummit-papers.18,P06-1062,0,0.286421,"gh these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to our work, previous studies have mined parallel texts from parallel web pages. A pair of parallel web pages consists of two monolingual web pages in different languages with almost the same meaning. For example, Shi et al. (2006) have aligned parall"
2009.mtsummit-papers.18,steinberger-etal-2006-jrc,0,0.0659403,"Missing"
2009.mtsummit-papers.18,P03-1010,1,0.869044,"he Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith, 2003; Shi et al., 2006). The novel contribution of our work compared to previous work is that we propose to mine parallel texts from mixed-language web pages. We define a mixed-language web page as a web page consisting of (at least) two languages. We mine Japanese-English parallel texts from mixedlanguage web pages consisting of Japanese and English texts. In contrast to o"
2009.mtsummit-papers.18,2007.mtsummit-papers.63,1,0.758763,"es for multi-lingual natural language processing, such as corpus-based machine translation (MT) (Nagao, 1981; Brown et al., 1993) and cross-lingual information retrieval. However, there are relatively few widely available parallel corpora. These include the ArabicEnglish and Chinese-English parallel corpora distributed by the Linguistic Data Consortium (Ma and Cieri, 2006); the Europarl corpus (Koehn, 2005), which consists of 11 European languages; the JRCAcquis corpus, which consists of more than 20 European languages (Steinberger et al., 2006); and a Japanese-English patent parallel corpus (Utiyama and Isahara, 2007). Although these parallel corpora are large scale, they are limited in the language registers and language pairs that they cover. Much work has been undertaken to overcome this lack of parallel corpora. For example, Resnik and Smith (2003) have proposed mining the web to collect parallel corpora for low-density language pairs. Zhao and Vogel (2002), Utiyama and Isahara (2003), Fung and Cheung (2004), and Munteanu and Marcu (2005) have extracted parallel sentences from comparable or non-parallel corpora. In this paper, we mine parallel texts from the web (Ma and Liberman, 1999; Resnik and Smith"
2009.mtsummit-papers.18,C94-2175,0,0.099231,"text files. That is, given a web page containing Japanese and English texts, we made a Japanese text file and an English text file from the web page.3 We regarded these two text files as a pair of noisy parallel text files and applied Utiyama and Isahara’s method to these. In the following, we briefly describe how we applied Utiyama and Isahara’s method to these parallel texts. See (Utiyama and Isahara, 2007) for details of their method. We first aligned the sentences in each pair of noisy parallel text files by using a standard dynamic programming (DP) matching method (Gale and Church, 1993; Utsuro et al., 1994). That is, let J and E be a Japanese text file and an English text file, respectively, we calculated the maximum similarity sen2 Our mining method will not be much affected by N because our method can extract parallel sentences very accurately as shown in Section 3. 3 We simply extracted Japanese (English) sentences from the web page and put them into a Japanese (English) text file. Pm AVSIM(J, E) = i=1 SIM(Ji , Ei ) R(J, E) = min( m |J ||E| , ) |E ||J| (1) (2) where |J |is the number of sentences in J, and |E |is the number of sentences in E. A high R(J, E) value occurs when |J |∼ |E|. Conseq"
2009.mtsummit-posters.10,2007.mtsummit-papers.63,1,0.920528,"tistical machine translation (SMT) experiments with the corpus and conﬁrmed that the corpus is useful for SMT. 1 Introduction Multilingual parallel corpora are required to support many tasks in natural language processing. For example, statistical machine translation (SMT) requires a parallel corpus for training, and crosslingual processing such as information retrieval and information extraction also use parallel corpora. There is no doubt on the importance of parallel corpora for any language pair. Specially, Japanese-English parallel corpora are very scarce. Although some parallel corpora (Utiyama and Isahara, 2007) are available, the domains and sizes of these corpora are limited. In general, European countries use multiple languages ofﬁcially. Based on this multilingual environment, Koehn (2005) has built a corpus by collecting parallel texts in eleven languages from the proceedings of the European Parliament, which are published on the Web. However, some countries such as Japan have no such language situation, that leads us difﬁculties for Masao Utiyama†† Eiichiro Sumita†† †† MASTAR Project National Institute of Information and Communications Tehnology 3-5, Hikaridai, Seika, Soraku, Kyoto 619-0289, Ja"
2009.mtsummit-posters.10,2005.mtsummit-papers.11,0,0.0444829,"al language processing. For example, statistical machine translation (SMT) requires a parallel corpus for training, and crosslingual processing such as information retrieval and information extraction also use parallel corpora. There is no doubt on the importance of parallel corpora for any language pair. Specially, Japanese-English parallel corpora are very scarce. Although some parallel corpora (Utiyama and Isahara, 2007) are available, the domains and sizes of these corpora are limited. In general, European countries use multiple languages ofﬁcially. Based on this multilingual environment, Koehn (2005) has built a corpus by collecting parallel texts in eleven languages from the proceedings of the European Parliament, which are published on the Web. However, some countries such as Japan have no such language situation, that leads us difﬁculties for Masao Utiyama†† Eiichiro Sumita†† †† MASTAR Project National Institute of Information and Communications Tehnology 3-5, Hikaridai, Seika, Soraku, Kyoto 619-0289, Japan {mutiyama,eiichiro.sumita} @nict.go.jp creating parallel corpora. Hence, more efforts are needed to collect them effectively. Available Japanese-English parallel corpora are scarce."
2009.mtsummit-posters.10,tiedemann-nygaard-2004-opus,0,0.0174316,",eiichiro.sumita} @nict.go.jp creating parallel corpora. Hence, more efforts are needed to collect them effectively. Available Japanese-English parallel corpora are scarce. However, there are a lot of translated texts on the Web. Specially, open source manuals are translated into Japanese from English by volunteer translators. We collected such English and Japanese texts. Then, the sentences in collected texts were automatically aligned, resulting in a parallel corpus made from open source software manuals. Manuals of open source software has been used for making a parallel corpus named OPUS (Tiedemann and Nygaard 2004), which was made from OpenOfﬁce.org documentation1 , KDE manuals including KDE messages2 , and PHP manuals 3 . However, the JapaneseEnglish part of OPUS is not large. In contrast, we collected about 500 thousand sentence pairs. In addition, our work involved extensive human efforts to ensure the quality of our parallel corpus. The original and translated texts often proscribe copy, distribute, display, and make derivative works. Our target texts are open source software manuals. Such open source software manuals are often published under open licenses under which we can modify and distribute t"
2009.mtsummit-posters.10,J93-1004,0,0.306217,", newlines are not deleted because they are regarded as sentence ends. 4.3 Aligning sentences We use Utiyama and Isahara’s alignment method, because their method has been successfully used in aligning noisy Japanese-English parallel texts (Utiyama and Isahara, 2007). Below is a concise description of their algorithm. We begin by obtaining the maximum similarity sentence alignments. Let J and E be a Japanese text ﬁle and an English text ﬁle, respectively. We calculate the maximum similarity sentence alignments (J1 ,E1 ), (J2 ,E2 ), . . ., (Jm ,Em ), using a dynamic programming matching method (Gale and Church, 1993), where (Ji , Ei ) is a Japanese and English sentence alignment pair in J and E. We allow 1-ton, n-to-1 (0 ≤ n ≤ 5), or 2-to-2 alignments when aligning sentences. The similarity between Ji and Ei is calculated based on word overlap (i.e., number of word pairs from Ji and Ei that are translations of each other based on a bilingual dictionary with 450,000+ entries). The similarity between a Japanese document, J, and an English document, E, (noted AVSIM(J,E)) is calculated using: ∑m AVSIM(J, E) = i=1 SIM(Ji , Ei ) m (1) A high AVSIM(J,E) value occurs when the sentence alignments in J and E take o"
2009.mtsummit-posters.10,P07-2045,0,0.0228859,"Missing"
2009.mtsummit-posters.10,P02-1040,0,0.0934553,"Missing"
2009.mtsummit-posters.10,P03-1021,0,0.016259,"ect without performing ﬁlling and painting operations. This includes BIOS settings, kernel conﬁguration and some simpliﬁcations in user land. This signal can be used to perform a remote checkpoint of a session. Xlib may choose to cache font data, loading it only as needed to draw text or compute text dimensions. Table 3: Example of parallel sentences formed to tune the decoder’s parameters on the basis of the bilingual evaluation understudy (BLEU) score (Papinei et al., 2002). The evaluation was done using a single reference. Tuning was performed using the standard technique developed by Och (Och, 2003). The test and development data were extracted from the aligned JF sentences. Each of test and development data consists of 500 sentences. In the following experiments, we simulated a situation where an SMT system was applied to help volunteer translators translate English JF documents into Japanese. We want to use all parallel sentences efﬁciently to help translators. This is a problem of domain adaptation. All of paralell sentences were translated from English to Japanese. Therefore we did MT experiments from English. In the ﬁrst experiment, we used all parallel sentences (excluding developm"
2009.mtsummit-posters.10,J03-1002,0,0.00480786,"Missing"
2009.mtsummit-posters.22,P07-2002,1,0.866754,"of documents every day, such as blogs, Wikipedia articles, open source software manuals, documents on nongovernmental organization (NGO) activities, and so on. These translations are read for pleasure, for practical purposes, for language learning, and many other reasons. Needless to say, volunteer translators contribute a great deal to the sharing and spreading of information around the world. Consequently, supporting their activities is a very important research issue. Volunteer translators translate a large number of documents everyday. However, they lack proper translation support tools (Abekawa and Kageura, 2007a). Thus, providing a good supporting environment should be of great assistance in improving volunteer translators’ efficiency and increasing the level of enjoyment they experience in translating. This is the motivation for our work in this paper. 2 Hosting volunteer translators Abekawa and Kageura (2007a) have developed a translation aid editor, QRedit, which has been experimentally provided to a limited number of volunteer translators. They report that QRedit is very effective for aiding their work, as described in Section 8. Based on the success of this translation aid editor, we have devel"
2009.mtsummit-posters.22,macklovitch-2006-transtype2,0,0.152808,"cement of translation candidate display [inside, left, right] of the SL area • Synchronized scroll [both directions, source→target, target→source, none] Figure 7 shows an example of a customized QRedit window. (See Figure 5 for a default window.) 8 User response As of 3 July, 2009 – three months after we made MNH and QRedit publicly available – there are about 600 users and 4 groups registered to MNH, including such major NGOs as Amnesty International Japan and Democracy Now! Japan. As quantitatively evaluating the benefit of using translation aid systems is technically a difficult task (cf. (Macklovitch, 2006)), and as we are dealing than before. We have not yet been able to obtain responses from group users, because they have only started using QRedit recently and thus have not accumulated sufficient experience to give an informed judgment on the system. We also have not been able to evaluate the usability of MNH because it is still under development and we are now improving its usability based on comments and suggestions from users. Figure 7: Customized QRedit window with volunteer translators who are not working on a “time is money” basis but rather wish to reduce the subjective burden of transl"
2009.mtsummit-posters.22,2001.mtsummit-papers.59,0,0.0378705,"uments; work that is concerned with hosting translated documents, often multilingually; and work that is addressed at aiding translators and translation communities. There are too many joint or collaborative online translation projects to mention here. GlobalVoices Online4 is perhaps one of the most well known, along with TUP5 (Translators United for Peace). Most projects do not provide translation aid facilities or collaborative working environments. They are rather projects defined by interested groups of people, using existing facilities. An example of the second category is Yakushite.net (Shimohata et al., 2001). It provides a collaborative translation environment in which users can use MT for translation, while contributing to collaborative terminology augmentation for the improvement of MT. Except for providing the MT engine, the translation aid functions are weak. Worldwide Lexicon (McConnell, 2007) is another example. Within the project a variety of mechanisms are provided that facilitate the sharing of translated documents world wide, with which one can (i) detect translated texts, if there are any; (ii) translate by oneself; (iii) subscribe to an RSS feed for translation; and (iv) use machine t"
2009.mtsummit-posters.22,P03-1010,1,0.803051,"ions and make derivative works public under certain conditions. Specifically, MNH uses four Creative Attribution Non-Commercial Share Alike This license lets others remix, tweak, and build upon the translator’s work non-commercially, as long as they credit the translator and license their new creations under identical terms. Translators can also use other licenses that are similar to the Creative Commons licenses listed above. In this way, translators can legally share their translations on MNH. These shared translations are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). Currently, MNH has a simple bilingual concordancer as shown in Figure 4. We plan to extend this concordancer in our future work because a bilingual concordancer is a very important tool for translation (Macklovitch et al., 2009). 6 High quality, comprehensive language resources Dictionaries and the web are the two main language resources that online volunteer translators use during translation. MNH, in cooperation with Sanseido, provides the “Grand Concise English Japanese Dictionary” (1) They are native-speakers of the target language (TL). (2) Most of them do not have a native-level comman"
2009.mtsummit-wpt.1,fujii-etal-2006-test,1,0.835438,"blem. Higuchi et al. (2001) used “patent families” as a parallel corpus for extracting translations. A patent family is a set of patent documents for the same or related inventions and -1- We used both intrinsic and extrinsic evaluation methods. In the intrinsic evaluation, we used both the Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002), which had been proposed as an automatic evaluation measure for MT, and human judgment. In the extrinsic evaluation, we evaluated the contribution of the MT to Cross-Lingual Information Retrieval (CLIR). In the Patent Retrieval Task at NTCIR-5 (Fujii et al., 2006), aimed at CLIR, search topics in Japanese were translated into English by human experts. We reused these search topics for the evaluation of the MT. We analyzed the relationship between different evaluation measures. The use of extrinsic evaluation, which is not performed in existing MT-related evaluation activities, such as the NIST MetricsMATR Challenge1 and the IWSLT Workshop2 , is a distinctive feature of our research. We executed a preliminary trial and the final evaluation, using the terms “dry run” and “formal run”, respectively. This paper describes only the formal run. 1 2 http://www"
2009.mtsummit-wpt.1,2001.mtsummit-papers.30,1,0.650448,"ation, Patent information, Cross-lingual information retrieval 1 Introduction Reflecting the rapid growth in the use of multilingual corpora, a number of data-driven Machine Translation (MT) methods have recently been explored, most of which are termed “Statistical Machine Translation (SMT)”. While large bilingual corpora for European languages, Arabic, and Chinese are available for research and development purposes, these corpora are rarely associated with Japanese and it is difficult to explore SMT with respect to Japanese. However, patent documents can alleviate this data scarcity problem. Higuchi et al. (2001) used “patent families” as a parallel corpus for extracting translations. A patent family is a set of patent documents for the same or related inventions and -1- We used both intrinsic and extrinsic evaluation methods. In the intrinsic evaluation, we used both the Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002), which had been proposed as an automatic evaluation measure for MT, and human judgment. In the extrinsic evaluation, we evaluated the contribution of the MT to Cross-Lingual Information Retrieval (CLIR). In the Patent Retrieval Task at NTCIR-5 (Fujii et al., 2006), aimed"
2009.mtsummit-wpt.1,W06-3114,0,0.0263028,"rinsic evaluation. ever, because the reference translations for MRB300 are independent of the counterpart sentences in the training data set, unlike RBMT systems, these SMT systems did not perform effectively. Figure 4 graphs the value for “Human” in Table 2, in which the order of groups is the same as Figures 1–3. In Figure 4, tsbmt and JAPIO, which were not effective in SRB, outperformed the other groups with respect to human rating. BLEU is generally suitable for comparing the effectiveness of SMT methods, but not suitable for evaluating other types of methods (Callison-Burch et al., 2006; Koehn and Monz, 2006). Figures 5 and 6 graph the value for adequacy and fluency, respectively. Although the relative superiority of the groups was almost the same in Figures 5 and 6, differences of the groups are more noticeable in Figure 5. To further analyze this tendency, Figure 7 shows -6- Figure 5: Adequacy for J–E intrinsic evaluation. Figure 6: Fluency for J–E intrinsic evaluation. Table 3: Results of E–J int/ext evaluation. Figure 7: Relationship between BLEU and human rating for J–E intrinsic evaluation. the correlation coefficient (“R”) between human rating and each BLEU type. The value of R for SRB is 0"
2009.mtsummit-wpt.1,P07-2045,0,0.0055351,"Missing"
2009.mtsummit-wpt.1,W04-3250,0,0.139508,"Missing"
2009.mtsummit-wpt.1,N03-2021,0,0.0130034,"E–J int/ext evaluation. Figure 7: Relationship between BLEU and human rating for J–E intrinsic evaluation. the correlation coefficient (“R”) between human rating and each BLEU type. The value of R for SRB is 0.814, which is smaller than those for MRB300 and MRB600. This is mainly due to the two outliers on the right side that correspond to the results for tsbmt and JAPIO. However, the values of R for MRB300 and MRB600 are more than 0.9, showing a high correlation between human rating and BLEU. By using multiple references, the evaluation result by BLEU became similar to that by human rating (Melamed et al., 2003). In such a case, while human judgments are not reusable, we need only reference translations, which are reusable, for evaluating MT methods. We also calculated the values of R for each BLEU type in terms of adequacy and fluency although these values are not shown in Figure 7. For adequacy, the values of R for SRB, MRB300, and MRB600 were 0.733, 0.846, and 0.887, respectively. For fluency, the values of R for SRB, MRB300, and MRB600 were 0.864, 0.940, and 0.951, respectively. This implies that BLEU is highly correlated with fluency more than adequacy. 4.3 E–J Intrinsic Evaluation Table 3 shows"
2009.mtsummit-wpt.1,P02-1040,0,0.079534,"r European languages, Arabic, and Chinese are available for research and development purposes, these corpora are rarely associated with Japanese and it is difficult to explore SMT with respect to Japanese. However, patent documents can alleviate this data scarcity problem. Higuchi et al. (2001) used “patent families” as a parallel corpus for extracting translations. A patent family is a set of patent documents for the same or related inventions and -1- We used both intrinsic and extrinsic evaluation methods. In the intrinsic evaluation, we used both the Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2002), which had been proposed as an automatic evaluation measure for MT, and human judgment. In the extrinsic evaluation, we evaluated the contribution of the MT to Cross-Lingual Information Retrieval (CLIR). In the Patent Retrieval Task at NTCIR-5 (Fujii et al., 2006), aimed at CLIR, search topics in Japanese were translated into English by human experts. We reused these search topics for the evaluation of the MT. We analyzed the relationship between different evaluation measures. The use of extrinsic evaluation, which is not performed in existing MT-related evaluation activities, such as the NIS"
2009.mtsummit-wpt.1,2007.mtsummit-papers.63,1,0.867471,"Missing"
2009.tc-1.4,P07-2002,1,0.458056,"Missing"
2011.iwslt-papers.11,N09-2038,0,0.0345077,"translation performance for its test set, and another data set giving the highest. In the experiments, we compare two methods for selecting data to be manually translated from the field data. Both of them use source side language models for data selection, but in different manners. According to the experimental results, either or both of the methods show larger improvements compared to a random data selection. 1. Introduction As a result of the drastic technical innovation advances in spoken language processing, speech-to-speech translation systems are now starting to be used in actual fields [1, 2]. In addition to the development of basic technologies, the efficient usage of field data is also an important challenge to be addressed for system performance improvement. A speech-to-speech translation system consists mainly of three subsystems: an automatic speech recognition (ASR) subsystem, a machine translation (MT) subsystem and a speech synthesis subsystem. While the simplest and most effective usage of field data is to annotate (transcribe and translate) all of the field data and use this for ASR and MT training, annotation is expensive and time consuming. In this paper, we propose a"
2011.iwslt-papers.11,P03-1021,0,0.0102454,"es) and a non-OOV part (non-OOV sentences). In addition to the training data used for the “Baseline 1”, “Baseline 2” uses OOV sentences and their manual translation. The baseline selection is a simple random selection. And, upper bound uses all of the field data and its manual translation. Table 2 shows the details of data sets used for the annotation data selection experiments. For each of the data sets (Hokkaido and Kyushu), we randomly sampled 1000 sentences from the field data to be used for the development sets and test sets. These development sets are used for minimum error rate training[9] and annotation data selection. The test sets are for the MT performance evaluation. In the experiments, we evaluate selection performance by computing the BLEU score [10] of the SMT system trained on the selected sentences including manual translation of the selected sentences. For the translation model and language model training, we use MOSES [11] and the SRI language model toolkit [12]. For the language model setting, we used a modified Kneser-Ney [13] 5-gram language model. For the data selection, we use 3-gram language model and set n (the number of selected sentence in each iteration) t"
2011.iwslt-papers.11,P02-1040,0,0.0809305,"Missing"
2011.iwslt-papers.11,P07-2045,0,0.0328106,"Missing"
2011.iwslt-papers.11,I08-2088,1,0.847105,"bsystem, a machine translation (MT) subsystem and a speech synthesis subsystem. While the simplest and most effective usage of field data is to annotate (transcribe and translate) all of the field data and use this for ASR and MT training, annotation is expensive and time consuming. In this paper, we propose a method for selecting useful field data to be manually translated in sentence units. In previous studies on ASR [3], positive results were obtained by selectively annotating (transcribing) field data. There have also been many studies on domain adaptation research handling data selection [1, 4, 5, 6] in ASR and MT researches, However, there has been little research done from a data annotation1 point of view. Typical MT domain adaptation research handles the selection of productive training sentences from out-of-domain monolingual or parallel corpus. In the task setting, we can 1 Different from ASR, annotation means manual translation here. use source and target language information to select training sentences from the parallel corpora. However, only source language information is available in our annotation data selection task. In this paper, we propose two methods that use source side l"
2011.iwslt-papers.11,N03-1017,0,0.0137249,"ion is shown Fig. 2. In the adaptation experiments, we used data sets from two areas. One is the data set from Hokkaido, which gave the lowest speech translation performance on its test set. The other data set is from Kyushu, which gave the highest. Translation memory 1 No output yes Translation Memory 2 2.2. System configuration of MT system Fig. 3 gives a flowchart of the MT subsystem. As shown in the figure, the MT system consists of 2 main components: the statistical based machine translation (SMT) and a translation memory. For the SMT, we employed a log-linear model as a phrase-based SMT [7]. This model expresses the probability of a target-language word sequence (e) for a given source language word sequence (f ) given by (∑ ) M exp i=1 λi hi (e, f ) (∑ ) P (e|f ) = ∑ (1) M ′ e′ exp i=1 λi hi (e , f ) where hi (e, f ) is the feature function, λi is the feature function’s weight, and M is the number of features. We can approximate Eq. 1 by regarding its denominator as constant. The translation results (ˆ e) are then obtained by eˆ(f, λM 1 ) = argmaxe M ∑ λi hi (e, f ) (2) i=1 We used the following eight features [7] for the translations. 3. Lexical weighting probability from the s"
2011.mtsummit-papers.37,P05-1074,0,0.0256923,"TF , not simply find a paraphrase for the input sentence. There are also two minor differences. One difference is the calculation of P (f |f  ). P (f |f  ) can be represented as (1) How to select one of eij is not discussed in this paper because we focus on the retrieval part of TMs. 3 326 P (fp |fp ) where fp and fp is one of the set of paraphrase pairs of f and f  , respectively, and P (fp |fp ) is the paraphrase probability of fp given fp . Quirk et al. calculated P (fp |fp ) from monolingual parallel corpora. In contrast, we calculate P (fp |fp ) from bilingual parallel corpora (Bannard and Callison-Burch, 2005). Another difference lies in the implementation. They used an in-house decoder which was very much like a phrase-based SMT monotone decoder. We use weighted finite state transducers (WFSTs) implemented with open-source software tools. 3.3 Acquiring the paraphrase list We acquire a paraphrase list using Bannard and Callison-Burch (2005)’s method. Their idea is, if two different phrases fp1 , fp2 in one language are aligned to the same phrase ep in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way. The procedure is as follows"
2011.mtsummit-papers.37,N06-1003,0,0.0147483,"m TMs for MT has been proposed by (Shimohata et al., 2003). They have proposed a method that retrieves sentences sharing the main meaning with input sentences despite lacking some unimportant information. In contrast, we aim to retrieve sentences with exactly the same meaning as input sentences, with no difference in information content. Our method uses a TM to perform MT. There are works that use MT for TM (He et al., 2010; Simard and Isabelle, 2009). Our method uses paraphrasing for retrieving sentences from a TM. Paraphrasing has also been used in a number of works on statistical MT (SMT) (Callison-Burch et al., 2006; Onishi et al., 2010). 3 Paraphrases for Retrieving Sentences from TMs P (f |f  ) = We first define a TM. A TM, T , is defined as: T = {fi , ei1 , . . . , eij , . . . , eiNi |1 ≤ i ≤ N } where fi is the i-th source language sentence, eij is the j-th translation of fi , Ni is the number of translations of fi , and N is the number of unique source sentences in T . We use TF to denote the set of source language sentences in T , i.e., TF = {fi |1 ≤ i ≤ N }. Given an input sentence f , we retrieve the fi from TF that receives the highest score according to a scoring function. Then, we use one o"
2011.mtsummit-papers.37,P09-1053,0,0.0242961,"state transducers (WFSTs) implemented with open-source software tools. 3.3 Acquiring the paraphrase list We acquire a paraphrase list using Bannard and Callison-Burch (2005)’s method. Their idea is, if two different phrases fp1 , fp2 in one language are aligned to the same phrase ep in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way. The procedure is as follows: (1) Build a phrase table: Build a phrase table from parallel corpus using standard SMT tech4 A statistical model for paraphrase detection has also been proposed (Das and Smith, 2009). Their system detects whether two input sentences are paraphrases of one another. However, it does not use paraphrases for searching TMs. niques. (We used the Moses toolkit (Koehn et al., 2007).) (2) Filter the phrase table by the sigtest-filter: The phrase table built in (1) has many inappropriate phrase pairs. Therefore, we filter the phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). (3) Calculate the paraphrase probability: Calculate the paraphrase probability P (fp2 |fp1 ) that fp2 is a paraphrase of fp1 . P (fp2 |fp1 ) =  ep P (fp2 |ep"
2011.mtsummit-papers.37,C10-2043,0,0.0382665,"Missing"
2011.mtsummit-papers.37,D07-1103,0,0.0294813,"(1) Build a phrase table: Build a phrase table from parallel corpus using standard SMT tech4 A statistical model for paraphrase detection has also been proposed (Das and Smith, 2009). Their system detects whether two input sentences are paraphrases of one another. However, it does not use paraphrases for searching TMs. niques. (We used the Moses toolkit (Koehn et al., 2007).) (2) Filter the phrase table by the sigtest-filter: The phrase table built in (1) has many inappropriate phrase pairs. Therefore, we filter the phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). (3) Calculate the paraphrase probability: Calculate the paraphrase probability P (fp2 |fp1 ) that fp2 is a paraphrase of fp1 . P (fp2 |fp1 ) =  ep P (fp2 |ep )P (ep |fp1 ) where P (fp2 |ep ) and P (ep |fp1 ) are phrase translation probabilities. (4) Acquire a paraphrase pair. Acquire (fp1 , fp2 ) as a paraphrase pair if P (fp2 |fp1 ) > P (fp1 |fp1 ). The purpose of this threshold is to keep highly-accurate paraphrase pairs. 3.4 Implementation using WFSTs We use WFSTs to retrieve sentences in a TM. Given an input sentence f , the best sentence fˆ in Equation (1) is represented in Equation (2"
2011.mtsummit-papers.37,P07-2045,0,0.00355409,"is, if two different phrases fp1 , fp2 in one language are aligned to the same phrase ep in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way. The procedure is as follows: (1) Build a phrase table: Build a phrase table from parallel corpus using standard SMT tech4 A statistical model for paraphrase detection has also been proposed (Das and Smith, 2009). Their system detects whether two input sentences are paraphrases of one another. However, it does not use paraphrases for searching TMs. niques. (We used the Moses toolkit (Koehn et al., 2007).) (2) Filter the phrase table by the sigtest-filter: The phrase table built in (1) has many inappropriate phrase pairs. Therefore, we filter the phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al., 2007). (3) Calculate the paraphrase probability: Calculate the paraphrase probability P (fp2 |fp1 ) that fp2 is a paraphrase of fp1 . P (fp2 |fp1 ) =  ep P (fp2 |ep )P (ep |fp1 ) where P (fp2 |ep ) and P (ep |fp1 ) are phrase translation probabilities. (4) Acquire a paraphrase pair. Acquire (fp1 , fp2 ) as a paraphrase pair if P (fp2 |fp1 ) > P (fp1 |fp1 )."
2011.mtsummit-papers.37,D09-1040,0,0.0434158,"Missing"
2011.mtsummit-papers.37,P10-2001,1,0.868418,"d by (Shimohata et al., 2003). They have proposed a method that retrieves sentences sharing the main meaning with input sentences despite lacking some unimportant information. In contrast, we aim to retrieve sentences with exactly the same meaning as input sentences, with no difference in information content. Our method uses a TM to perform MT. There are works that use MT for TM (He et al., 2010; Simard and Isabelle, 2009). Our method uses paraphrasing for retrieving sentences from a TM. Paraphrasing has also been used in a number of works on statistical MT (SMT) (Callison-Burch et al., 2006; Onishi et al., 2010). 3 Paraphrases for Retrieving Sentences from TMs P (f |f  ) = We first define a TM. A TM, T , is defined as: T = {fi , ei1 , . . . , eij , . . . , eiNi |1 ≤ i ≤ N } where fi is the i-th source language sentence, eij is the j-th translation of fi , Ni is the number of translations of fi , and N is the number of unique source sentences in T . We use TF to denote the set of source language sentences in T , i.e., TF = {fi |1 ≤ i ≤ N }. Given an input sentence f , we retrieve the fi from TF that receives the highest score according to a scoring function. Then, we use one of eij as the translati"
2011.mtsummit-papers.37,W04-3219,0,0.0224,"Missing"
2011.mtsummit-papers.37,W03-0311,1,0.801179,"t, the paraphrase retrieval proposed in this paper will use the translations of the retrieved sentences without modification. For example, if “is there a beauty parlor?” is retrieved when “is there a salon?” is given as an input, we simply output 2 Previous TM systems could use a thesaurus to detect paraphrases. However, large scale thesauruses do not exist for most languages. In this paper, we propose a method that uses only parallel corpora for getting paraphrases. the translation of “is there a beauty parlor?” without modification. Retrieving sentences from TMs for MT has been proposed by (Shimohata et al., 2003). They have proposed a method that retrieves sentences sharing the main meaning with input sentences despite lacking some unimportant information. In contrast, we aim to retrieve sentences with exactly the same meaning as input sentences, with no difference in information content. Our method uses a TM to perform MT. There are works that use MT for TM (He et al., 2010; Simard and Isabelle, 2009). Our method uses paraphrasing for retrieving sentences from a TM. Paraphrasing has also been used in a number of works on statistical MT (SMT) (Callison-Burch et al., 2006; Onishi et al., 2010). 3 Parap"
2011.mtsummit-papers.37,2009.mtsummit-papers.14,0,0.102253,"we propose a method that uses only parallel corpora for getting paraphrases. the translation of “is there a beauty parlor?” without modification. Retrieving sentences from TMs for MT has been proposed by (Shimohata et al., 2003). They have proposed a method that retrieves sentences sharing the main meaning with input sentences despite lacking some unimportant information. In contrast, we aim to retrieve sentences with exactly the same meaning as input sentences, with no difference in information content. Our method uses a TM to perform MT. There are works that use MT for TM (He et al., 2010; Simard and Isabelle, 2009). Our method uses paraphrasing for retrieving sentences from a TM. Paraphrasing has also been used in a number of works on statistical MT (SMT) (Callison-Burch et al., 2006; Onishi et al., 2010). 3 Paraphrases for Retrieving Sentences from TMs P (f |f  ) = We first define a TM. A TM, T , is defined as: T = {fi , ei1 , . . . , eij , . . . , eiNi |1 ≤ i ≤ N } where fi is the i-th source language sentence, eij is the j-th translation of fi , Ni is the number of translations of fi , and N is the number of unique source sentences in T . We use TF to denote the set of source language sentences in"
2011.mtsummit-papers.37,W01-1401,1,0.874099,"ntroduction Translation memories (TMs1 ) are very useful tools for translating texts in narrow domains, where replications of sentences are abundant. In such a case, a machine translation (MT) system can simply search for a match of the input sentence in the TM, and if a match is found, output its corresponding translation. TMs may also use soft matching, finding a sentence that is similar, but not identical to the input sentence. In this case, the translations of these similar sentences are modified to produce appropriate output translations. A number of MT systems have used TMs in this way (Sumita, 2001; Vogel et al., 2004; Zhechev and van Genabith, 2010). In this paper, we propose the use of paraphrases for searching TMs. By using paraphrases, we can retrieve sentences that have the same meaning as the input sentences even if the actual words of the sentences do not match exactly. Note that previous TM systems retrieve similar sentences based on the number of differing words in the sequence. For example, they would prefer “is 1 We use the term TM to refer to a set of parallel sentences. 325 there a pen?” over “is there a beauty parlor?”, when they are given “is there a salon?” as an input."
2011.mtsummit-papers.37,2004.iwslt-evaluation.11,0,0.180725,"anslation memories (TMs1 ) are very useful tools for translating texts in narrow domains, where replications of sentences are abundant. In such a case, a machine translation (MT) system can simply search for a match of the input sentence in the TM, and if a match is found, output its corresponding translation. TMs may also use soft matching, finding a sentence that is similar, but not identical to the input sentence. In this case, the translations of these similar sentences are modified to produce appropriate output translations. A number of MT systems have used TMs in this way (Sumita, 2001; Vogel et al., 2004; Zhechev and van Genabith, 2010). In this paper, we propose the use of paraphrases for searching TMs. By using paraphrases, we can retrieve sentences that have the same meaning as the input sentences even if the actual words of the sentences do not match exactly. Note that previous TM systems retrieve similar sentences based on the number of differing words in the sequence. For example, they would prefer “is 1 We use the term TM to refer to a set of parallel sentences. 325 there a pen?” over “is there a beauty parlor?”, when they are given “is there a salon?” as an input. This is because “is"
2011.mtsummit-papers.37,W10-3806,0,0.063305,"Missing"
2011.mtsummit-papers.40,C96-1009,0,0.116742,"Missing"
2011.mtsummit-papers.40,D07-1103,0,0.0336507,"English sentences of the given parallel corpus. (2) Build the phrase-table from the parallel corpus using the Moses toolkit (Koehn et al., 2007). (3) Extract bilingual term candidates from the phrase table that are included in the term candidates obtained in (1). (4) Calculate a statistical measure for each candidate term. (5) Rank the candidates according to the statistical measure, and extract the highly-ranked candidates as valid bilingual terms. We compare three statistical measures, ScoreF , ScoreL and ScoreC , for extracting correct bilingual terms. Fisher’s exact test has been used by Johnson et al. (2007) to select valid phrase pairs from the phrasetable for statistical machine translation. We use the statistic of Fisher’s exact test as ScoreF to measure the validity of each bilingual term candidate. The statistic used in Fisher’s exact test is deﬁned as ScoreF as follows. First, we obtain the contingency table, shown below, for a bilingual term candidate TJ,E consisting of Japanese term J and English term E C(J, E) C(E) − C(J, E) We use functions implemented in TermExtract 2 to extract term candidates. TermExtract is a Perl module for extracting terms. We slightly modiﬁed the POS patterns use"
2011.mtsummit-papers.40,P07-2045,0,0.00258597,"statistical measures. In addition, we compare three statistical measures for extracting bilingual terms. Note that Macken et al. (2008) have also used statistical measures to ﬁlter out invalid bilingual term candidates; however, they did not compare their statistical measures against other measures. 3 Bilingual term extraction (1) Extract the term candidates, which match speciﬁc part-of-speech(POS) patterns (e.g., a single noun or a noun sequence), from the Japanese and English sentences of the given parallel corpus. (2) Build the phrase-table from the parallel corpus using the Moses toolkit (Koehn et al., 2007). (3) Extract bilingual term candidates from the phrase table that are included in the term candidates obtained in (1). (4) Calculate a statistical measure for each candidate term. (5) Rank the candidates according to the statistical measure, and extract the highly-ranked candidates as valid bilingual terms. We compare three statistical measures, ScoreF , ScoreL and ScoreC , for extracting correct bilingual terms. Fisher’s exact test has been used by Johnson et al. (2007) to select valid phrase pairs from the phrasetable for statistical machine translation. We use the statistic of Fisher’s exa"
2011.mtsummit-papers.51,W06-2920,0,0.0180473,"ool did not include a POS tagger function, we used Tsuruoka’s English POS tagger (Tsuruoka and Tsujii, 2005) to get part-of-speech. CHARNIAK Charniak’s (2000) parser. The parser uses a lexicalized probabilistic CFG model. The model is based on the principle of maximum entropy. STANFORD Stanford’s parser (Klein and Manning, 2003). The parser uses an unlexicalized probabilistic CFG model. We used version 1.6.5. BERKELEY Berkeley’s parser (Petrov and Klein, 2007). The parser uses an unlexicalized probabilistic CFG model. We used release 1.1. 2.2 Dependency parser Owing to the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), research into dependency parsing have been active. Dependency structure is a tree structure in which a node is a word and an edge is the relation between a parent node and a child node. A child node modiﬁes its parent node. Fig. 2 shows an example of a dependency tree structure. In this research, we used the following parser: MST MacDonald and Pereira’s (2006) parser. Projective dependency parsing is based on Eisner’s algorithm (Eisner, 1996). We used version 0.4.3b. The tool did not contain a model. Domain Travel News Patent Table 1: Average sentence length in three dom"
2011.mtsummit-papers.51,W08-0309,0,0.0188608,"between a parent node and a child node. A child node modiﬁes its parent node. Fig. 2 shows an example of a dependency tree structure. In this research, we used the following parser: MST MacDonald and Pereira’s (2006) parser. Projective dependency parsing is based on Eisner’s algorithm (Eisner, 1996). We used version 0.4.3b. The tool did not contain a model. Domain Travel News Patent Table 1: Average sentence length in three domains. Sentence length is the number of words per English sentence. We used the IWSLT corpus (Eck and Hori, 2005) in the travel domain, the WMT08 News Commentary corpus (Callison-Burch et al., 2008) in the news domain, and the NTCIR-8 Patent machine translation corpus (Fujii et al., 2010) in the patent domain. Figure 1: Penn Treebank-style phrase structure Figure 2: Dependency tree structure We built a model using WSJ section 2 to 21 from Penn Treebank. Because the tool did not include a POS tagger function, we used Tsuruoka’s English POS tagger (Tsuruoka and Tsujii, 2005) to get part-of-speech. 2.3 HPSG parser There is a parser based on the HPSG (Pollard and Sag, 1994) theory. HPSG-based parsers analyze not only phrase structure but also deeper structures, such as the arguments of a pre"
2011.mtsummit-papers.51,A00-2018,0,0.0904273,"was used to obtain the syntax of input sentences. In addition, we examined the effects of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation are not clear in the commonly used evaluations of parsers. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) from the Penn Treebank corpus. Such parsers were evaluated by measuring bracketing precision and recall of the output using the WSJ from the Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the other domains such"
2011.mtsummit-papers.51,P08-1009,0,0.0388999,"hang et al., 2006). They did not use patent corpus, and only evaluated probabilistic CFG-based parsers. They used target side syntax and did not use source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machin"
2011.mtsummit-papers.51,P97-1003,0,0.0771229,"when the parser was used to obtain the syntax of input sentences. In addition, we examined the effects of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation are not clear in the commonly used evaluations of parsers. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) from the Penn Treebank corpus. Such parsers were evaluated by measuring bracketing precision and recall of the output using the WSJ from the Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the oth"
2011.mtsummit-papers.51,W06-1628,0,0.0507067,"Missing"
2011.mtsummit-papers.51,2005.iwslt-1.1,0,0.0135043,"e is a tree structure in which a node is a word and an edge is the relation between a parent node and a child node. A child node modiﬁes its parent node. Fig. 2 shows an example of a dependency tree structure. In this research, we used the following parser: MST MacDonald and Pereira’s (2006) parser. Projective dependency parsing is based on Eisner’s algorithm (Eisner, 1996). We used version 0.4.3b. The tool did not contain a model. Domain Travel News Patent Table 1: Average sentence length in three domains. Sentence length is the number of words per English sentence. We used the IWSLT corpus (Eck and Hori, 2005) in the travel domain, the WMT08 News Commentary corpus (Callison-Burch et al., 2008) in the news domain, and the NTCIR-8 Patent machine translation corpus (Fujii et al., 2010) in the patent domain. Figure 1: Penn Treebank-style phrase structure Figure 2: Dependency tree structure We built a model using WSJ section 2 to 21 from Penn Treebank. Because the tool did not include a POS tagger function, we used Tsuruoka’s English POS tagger (Tsuruoka and Tsujii, 2005) to get part-of-speech. 2.3 HPSG parser There is a parser based on the HPSG (Pollard and Sag, 1994) theory. HPSG-based parsers analyze"
2011.mtsummit-papers.51,C96-1058,0,0.0903811,"Missing"
2011.mtsummit-papers.51,C96-1009,0,0.0109805,"ut instead used the noun phrases determined by using the parse results of a context document, a document that contains an input sentence. This method can determine noun phrases by considering document-level consistency. The method is as follows: 1. The method parses a context document containing an input sentence. Set Training Development Test Number of sentences 3.2 million 2,000 1,119 Table 3: Statistics for the NTCIR-8 English to Japanese patent translation task dataset. 2. The method extracts all noun phrases from the parse results. 3. The method ranks the noun phrases based on a C-value (Frantzi and Ananiadou, 1996) that gives high rank to phrases with high termhood from nested candidates. 4. The method searches the list of noun phrases (in order of rank) for expressions that appear in the input sentence. 5. The method determines the searched expression to be a noun phrase and adds zone tags if the expression does not conﬂict with existing zone tags. The C-value of a phrase p is expressed in the following equation: C-value(p) =  (l(p)−1) n(p)  (l(p)−1) t(p) n(p)− c(p) (c(p) = 0) (c(p) &gt; 0) where l(p) is the length of a phrase p, n(p) is the frequency of p in a document, t(p) is the total frequency of p"
2011.mtsummit-papers.51,2006.amta-papers.8,0,0.0137765,"se source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machine translation by evaluating patent machine translation quality. Moreover, we also applied a method that used document-level context containing the input"
2011.mtsummit-papers.51,P03-1054,0,0.0791817,"ain the syntax of input sentences. In addition, we examined the effects of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation are not clear in the commonly used evaluations of parsers. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) from the Penn Treebank corpus. Such parsers were evaluated by measuring bracketing precision and recall of the output using the WSJ from the Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the other domains such as patent domain. Introdu"
2011.mtsummit-papers.51,W04-3250,0,0.111841,"ined directly by the parsers. “Baseline” indicates the result that did not use a parser and zone tags. The ﬁve parsers other than COLLINS had improved the BLEU scores over the baseline BLEU score. From these results, it can be seen that the CHARNIAK, STANFORD, BERKELEY, MST, and ENJU parsers were effective for patent machine translation. The “**” mark in Table 4 and Table 5 denotes a statistical signiﬁcant difference at the signiﬁcance level of α = 0.01 and the “*” mark denotes a statistical signiﬁcant difference at the signiﬁcance level of α = 0.05 according to the bootstrap resampling test (Koehn, 2004). The difference between BERKELY and the top parsers STANFORD, MST, and ENJU was not signiﬁcant at α = 0.05 and the difference between BERKELEY and CHARNIAK was signiﬁcant at α = 0.05. From these results, it can be seen that BERKELEY, STANFORD, MST, and ENJU were especially effective for patent machine translation among the six parsers when the noun phrases in an input sentence were obtained by parsing the input sentence directly. COLLINS CHARNIAK STANFORD BERKELEY MST ENJU BLEU 39.42 39.58 39.64 39.76 39.54 39.68 Gains from without context +2.45** +0.34* +0.08 +0.16 +0.10 +0.28* Table 5: Comp"
2011.mtsummit-papers.51,P06-1077,0,0.0177111,"ntax and did not use source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machine translation by evaluating patent machine translation quality. Moreover, we also applied a method that used document-level context"
2011.mtsummit-papers.51,P09-1063,0,0.0313516,"Missing"
2011.mtsummit-papers.51,J93-2004,0,0.0391607,"hese results showed how their method was effective with each parser. The rest of this paper is organized as follows: In section 2, we show the six parsers that we compared. In section 3, we explain the method of comparison. In section 4, we discuss the experiment results from the NTCIR-8 patent translation task data. In section 5, we conclude this paper. 449 2 Parsers We focused on six well-known publicly available parsers. The parsers are categorized by method into three groups: probabilistic CFG parser, dependency parser, and HPSG parser. 2.1 Probabilistic CFG parser Owing to Penn Treebank (Marcus et al., 1993), there has been a lot of research into parsers based on probabilistic CFG that output phrase structures. Fig. 1 shows an example of a phrase structure. The ways to parameterize the probabilistic models vary. In this research, we used the following four parsers: COLLINS Collins’ (1997) parser. The parser uses a lexicalized probabilistic CFG model. The tool includes three models: model 1, 2, and 3. We used model 3. Because the tool did not include a POS tagger function, we used Tsuruoka’s English POS tagger (Tsuruoka and Tsujii, 2005) to get part-of-speech. CHARNIAK Charniak’s (2000) parser. Th"
2011.mtsummit-papers.51,P08-1114,0,0.0689699,"006). They did not use patent corpus, and only evaluated probabilistic CFG-based parsers. They used target side syntax and did not use source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machine translation by evaluati"
2011.mtsummit-papers.51,E06-1011,0,0.0210896,"ts of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation are not clear in the commonly used evaluations of parsers. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) from the Penn Treebank corpus. Such parsers were evaluated by measuring bracketing precision and recall of the output using the WSJ from the Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the other domains such as patent domain. Introduction In recent years, demands for patent machine translation have increa"
2011.mtsummit-papers.51,J08-1002,0,0.0409543,"ontaining the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation are not clear in the commonly used evaluations of parsers. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) from the Penn Treebank corpus. Such parsers were evaluated by measuring bracketing precision and recall of the output using the WSJ from the Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the other domains such as patent domain. Introduction In recent years, demands for patent machine translation have increased. With globalization comes an increase in th"
2011.mtsummit-papers.51,P08-1006,0,0.142156,"he Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the other domains such as patent domain. Introduction In recent years, demands for patent machine translation have increased. With globalization comes an increase in the need for the international circulation of patent documents. It is, therefore, important to improve the quality of machine translation of patent sentences. Word ordering is the main issue in statistical machine translation of long patent sentences between language pairs with widely different word 448 There is a task-oriented evaluation (Miyao et al., 2008). Miyao et al. (2008) compared parsers based on the accuracy of identifying protein-protein interaction that used the parser’s output as features for machine learning models. This evaluation showed the effect of each parser for the protein-protein interaction task using biomedical papers. There is research that studied the relation between parse accuracy and translation quality (Quirk and Corston-Oliver, 2006). This showed the relationship between a parser’s training data size and the translation quality. They did not compare parsers, nor did they use a patent corpus. Research has also been do"
2011.mtsummit-papers.51,P11-2076,1,0.419665,"It is difﬁcult to select the appropriate parser for patent translation because the effects of each parser on patent translation are not clear. This paper provides comparative evaluation of several state-of-the-art parsers for English, focusing on the effects for patent machine translation from English to Japanese. We measured how much each parser contributed to improve translation quality when the parser was used to obtain the syntax of input sentences. In addition, we examined the effects of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation"
2011.mtsummit-papers.51,P02-1040,0,0.0828331,"translated by a phrase-based statistical machine translation with reordering constraints using syntax of input sentences. We translated from English to Japanese, whose word orders are widely different. In translation between languages with widely different word orders, it is difﬁcult to assign the proper word order, especially with 450 long input sentences. Input sentence syntax is useful in deciding a word order for the translated sentence. We parsed the input sentence and constrained the word order using these parsed results. The translation quality was measured using the 4-gram BLEU score (Papineni et al., 2002). There is a method that determines the noun phrases in an input sentence by using the parse results from document-level context that contains the input sentence. We applied this method and compared parsers based on the translation quality. We also examined the effects of this method on each parser and combinations of parsers. First, we show the issue of patent translation. Next, we explain the methods that deal with the issue by reordering constraints using syntax of input sentences. Finally, we explain the method that estimates noun phrases using document-level context. 3.1 Patent translatio"
2011.mtsummit-papers.51,N07-1051,0,0.234005,"entences. In addition, we examined the effects of a method using parsed documentlevel context containing the input sentence to determine noun phrases (Onishi et al., 2011). We conducted experiments using the NTCIR8 patent translation task dataset. Most of the parsers improved translation quality. When the method using document-level context was applied, all of the compared parsers improved translation quality. 1 • Parsing is a difﬁcult task, and several methods have been proposed in recent years. There are probabilistic CFG-based parser (Collins, 1997; Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007), dependency parser (McDonald and Pereira, 2006), and HPSGbased parser (Miyao and Tsujii, 2008). • The effects of each parser on patent translation are not clear in the commonly used evaluations of parsers. Most state-of-the-art parsers for English were trained with the Wall Street Journal (WSJ) from the Penn Treebank corpus. Such parsers were evaluated by measuring bracketing precision and recall of the output using the WSJ from the Penn Treebank corpus. From the evaluation, it is not clear how well these models work in the other domains such as patent domain. Introduction In recent years, de"
2011.mtsummit-papers.51,W06-1608,0,0.263524,"tent sentences. Word ordering is the main issue in statistical machine translation of long patent sentences between language pairs with widely different word 448 There is a task-oriented evaluation (Miyao et al., 2008). Miyao et al. (2008) compared parsers based on the accuracy of identifying protein-protein interaction that used the parser’s output as features for machine learning models. This evaluation showed the effect of each parser for the protein-protein interaction task using biomedical papers. There is research that studied the relation between parse accuracy and translation quality (Quirk and Corston-Oliver, 2006). This showed the relationship between a parser’s training data size and the translation quality. They did not compare parsers, nor did they use a patent corpus. Research has also been done on the relationship between four parsers and translation quality of syntax-based statistical machine translation (Zhang et al., 2006). They did not use patent corpus, and only evaluated probabilistic CFG-based parsers. They used target side syntax and did not use source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translati"
2011.mtsummit-papers.51,H05-1059,0,0.0464321,"HPSG parser. 2.1 Probabilistic CFG parser Owing to Penn Treebank (Marcus et al., 1993), there has been a lot of research into parsers based on probabilistic CFG that output phrase structures. Fig. 1 shows an example of a phrase structure. The ways to parameterize the probabilistic models vary. In this research, we used the following four parsers: COLLINS Collins’ (1997) parser. The parser uses a lexicalized probabilistic CFG model. The tool includes three models: model 1, 2, and 3. We used model 3. Because the tool did not include a POS tagger function, we used Tsuruoka’s English POS tagger (Tsuruoka and Tsujii, 2005) to get part-of-speech. CHARNIAK Charniak’s (2000) parser. The parser uses a lexicalized probabilistic CFG model. The model is based on the principle of maximum entropy. STANFORD Stanford’s parser (Klein and Manning, 2003). The parser uses an unlexicalized probabilistic CFG model. We used version 1.6.5. BERKELEY Berkeley’s parser (Petrov and Klein, 2007). The parser uses an unlexicalized probabilistic CFG model. We used release 1.1. 2.2 Dependency parser Owing to the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), research into dependency parsing have been active. Dependency"
2011.mtsummit-papers.51,N10-1016,0,0.0411628,"valuated probabilistic CFG-based parsers. They used target side syntax and did not use source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machine translation by evaluating patent machine translation quality. More"
2011.mtsummit-papers.51,W08-0401,1,0.865812,"tent corpus, and only evaluated probabilistic CFG-based parsers. They used target side syntax and did not use source side syntax. To the best of our knowledge, there has been no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machine translation by evaluating patent machine trans"
2011.mtsummit-papers.51,P08-1064,0,0.0117577,"no previous research comparing the effects of parsers on patent machine translation. In this paper, we compared the effects of several state-of-the-art parsers on patent machine translation. This research reveals how effective each parser is in patent machine translation. There are statistical machine translation methods that use input sentence syntax: reordering constraint methods (Cherry, 2008; Marton and Resnik, 2008; Yamamoto et al., 2008; Xiong et al., 2010; Onishi et al., 2011), tree-to-string methods (Liu et al., 2006; Huang et al., 2006), and tree-to-tree methods (Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In this research, we used a reordering constraint method, which directly controls word order using the syntax of an input sentence for phrase based statistical machine translation, one of the widely used statistical translation methods. The syntax structure was obtained using each parser to be compared. We evaluate the effects of each parser on patent machine translation by evaluating patent machine translation quality. Moreover, we also applied a method that used document-level context containing the input sentence to determine the noun phrases in the input sentence (Onis"
2012.iwslt-evaluation.15,D12-1037,0,0.0218568,"Missing"
2012.iwslt-evaluation.15,P07-2045,0,0.0268509,"Missing"
2012.iwslt-evaluation.15,P02-1040,0,\N,Missing
2012.iwslt-evaluation.15,N04-1022,0,\N,Missing
2012.iwslt-evaluation.15,P03-1021,0,\N,Missing
2012.iwslt-evaluation.15,J03-1002,0,\N,Missing
2012.iwslt-evaluation.15,2009.iwslt-evaluation.12,1,\N,Missing
2012.iwslt-evaluation.15,I05-3027,0,\N,Missing
2012.tc-1.1,P07-2002,1,0.672352,"ide-by-side with differences highlighted. Babych, Hartley, Kageura, Thomas, Utiyama 5 MNH-TT: a collaborative platform for translator training Translating and the Computer 34 29-30 November 2012, London, UK Figure 1: Dictionary lookup in QRedit MNH assumes translators work voluntarily and not upon request by customers, so the management of overall workflow in commercial settings is not explicitly provided, although it can be simulated using existing MNH functions. Translation work is carried out using the translation-aid editor QRedit, a two-pane editor which provides the following functions (Abekawa and Kageura, 2007):      lookup of dictionaries and terminologies (idiom variants are matched to their canonical forms; multi-word units and idioms are prioritised) seamless connection to bilingual corpora (TMs) seamless connection to Wikipedia monolingual and bilingual entries seamless connection to Google webpage and dictionary search registration of terms. These functions are triggered by mouse actions starting from the relevant words or phrases in the SL text area. Throughout these actions, the keyboard remains active in the TL text area in order to improve the efficiency of translation (Figure 1). The"
2012.tc-1.1,abekawa-kageura-2008-constructing,1,0.654492,"contribution. The poster may also specify their role and the object or ‘prop’ to which they are referring; these include: translation-brief, set-of-targetdocuments, research-data, glossary, tms, mt-raw-output, as well as text spans such as sentence or word. Figure 3 illustrates the bulletin board displaying a number of interactions between a project manager and volunteers signing up to play various roles in the project. Figure 3: Interaction in MNH-TT structured by dialogue act and referencing role 2.4 MNH-TT: revision categorisation The third extension provides a set of categories based on (Abekawa and Kageura, 2008; Castagnoli et al., 2006; Secara, 2005) to allow revisers to motivate and justify individual revisions (Table 3). The defined categories are grouped thematically.     content- revisions bear on the perceived transfer of ideas between the source and the target document lexis- revisions bear on the choice of words and terms grammar- revisions bear on the well-formedness of the target document text- revisions relate to departures from the conventions holding for the genre of the target document, or to clumsiness, or to a lack of cohesion. Revision categories are represented as a menu. Each t"
2012.tc-1.1,abekawa-etal-2010-community,1,0.806611,"Missing"
2012.tc-1.1,W07-0732,0,0.148105,"Missing"
2012.tc-1.1,2012.tc-1.1,1,0.0530913,"Missing"
2012.tc-1.1,2009.mtsummit-posters.22,1,0.370012,"Missing"
2012.tc-1.1,2009.tc-1.4,1,0.205718,"Missing"
2014.amta-researchers.9,P13-2068,0,0.023877,"Missing"
2014.amta-researchers.9,W09-2404,0,0.125788,", the well-developed techniques of SMT are mainly focused on the sentencelevel translation, i.e., the models are trained on the parallel corpus of sentence pairs, and the translation is conducted sentence-by-sentence. Because in practice sentences are usually contained in a document and surrounded by context, recent research has begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceed"
2014.amta-researchers.9,W12-3156,0,0.0734953,"loped techniques of SMT are mainly focused on the sentencelevel translation, i.e., the models are trained on the parallel corpus of sentence pairs, and the translation is conducted sentence-by-sentence. Because in practice sentences are usually contained in a document and surrounded by context, recent research has begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1:"
2014.amta-researchers.9,2012.eamt-1.60,0,0.0881271,"a baseline SMT system to introduce documentlevel information. In practice, if document-level information is no available, our approach degenerates to the baseline system (i.e. λtdoc = λst doc = 0); otherwise, the approach produces several sets of {λtdoc , λst }, which suggests that we should pay attention to the document-level doc features. 4 Experiment 4.1 Data and Settings We tested the proposed approach on French-to-English translation because this translation task has been handled well by state-of-the-art SMT systems. We used two different schemes. One is on the WIT3 corpus of TED talks4 (Cettolo et al., 2012), which contains a small training set with document-level parallel development set and test set. The other scheme is a relatively more realistic setting: using the Europarl corpus (Koehn, 2005) for model training and an in-domain development set for the weight tuning in the baseline SMT system. Then we selected document pairs from the Common Crawl (CC) Corpus5 of WMT2013 for document-level development and test set. The CC corpus has a lot of noise, with many document pairs only several sentences long – too short for our purposes. Thus, we selected relatively high-quality document pairs, with m"
2014.amta-researchers.9,J07-2003,0,0.749481,"ge of documentlevel information in an SMT system. 3 3.1 Proposed Approach Overview The proposed approach is essentially a re-ranking process in a document-level decoding (Fig. 1). We first use an off-the-shelf baseline SMT system to translate a document sentence-bysentence, obtaining the m-best translation candidates for each sentence. The baseline SMT system can be trained and tuned in a standard way with sentence-level parallel data. Then, we conduct a decoding on the document level to find good combinations among the m-best candidate sentences. The search is realized in a cube-pruning way (Chiang, 2007). Here, we use good to mean that the combinations are good for both sentence- and document-level metrics under the Pareto optimality (Duh et al., 2012). As far as we know, this is the first attempt to apply document-level re-ranking in an SMT system. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 112 tk-2 tk-1 tk tk+1 tk+2 rk-2 rk-1 rk rk+1 rk+2 Figure 2: In Seval (BLEU), every translation t will be compared with its reference r. 3.2 Notation In the following description, we use D to denote an input document on the source-side language c"
2014.amta-researchers.9,P12-1001,0,0.310384,"evel translation, where both the training and decoding are on the sentence level. Then we introduce two document-level features, one using the word-embedding technique to model the semantic relation of context on the target side and the other using a token-type ratio to model the consistency in translation. With the two document-level features, we conduct a further decoding on the document level to get a better combination of sentencelevel translation within a document. As to the weights of the introduced features, we utilize a multi-objective learning approach based on the Pareto optimality (Duh et al., 2012) to simultaneously optimize the sentence-level and document-level metrics. The proposed approach requires no word-net or document-level parallel corpus for model training. Instead, it requires a vector list of the target-side vocabulary by word embedding and a small development set of parallel document pairs to tune the weights of document-level features. The remainder of the paper is organized as follows. In Section 2, we mention the related work around using document-level information in translation. In Section 3, we describe our proposed approach. Section 4 presents experimental results, an"
2014.amta-researchers.9,P12-2023,0,0.130415,"). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest feature because it only considers the lexical words themselves. In contrast, lexical cohesion involves more semantic information, such as hypernyms and hyponyms, usually requiring a word-net. Approaches that use the document topic as a feature usually require training data, such as a document-level parallel corpus, in the training or decoding phases. In this paper, we propose an approach that considers both the lexical consistency and semantic relation on the document level. The app"
2014.amta-researchers.9,D11-1084,0,0.0913281,"er and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest feature because it only considers the lexical words themselves. In contrast, lexical cohesion involves more semantic information, such as hypernyms and hyponyms, usually requiring a word-net. Approaches that use the document topic as a feature usually require training data, such as a document-level parallel corpus, in the training or decoding phases. In this paper, we propose an approach that considers both the lexical consistency and semantic relation on the"
2014.amta-researchers.9,W13-3302,0,0.013329,"allel corpus of sentence pairs, and the translation is conducted sentence-by-sentence. Because in practice sentences are usually contained in a document and surrounded by context, recent research has begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent resea"
2014.amta-researchers.9,D12-1108,0,0.0185143,"ts a documentlevel decoding on the target-side m-best candidate lists of source-side sentences to find a better combination (marked by the dashed zigzag arrow). is statistical and needs to be trained on monolingual or bilingual document-level data. Along with the feature of lexical cohesion, the topic is a sophisticated feature that must be supported by extra resources. Many approaches using document-level features require to modify the decoder of a baseline system to adapt to their features in decoding. Research mainly focusing on the decoding and tuning algorithm, such as the series work of Hardmeier et al. (2012) and Stymne et al. (2013), extends the traditional sentence-based SMT system to be able to collaborate with documentlevel features. As to our approach, the features used can model the lexical consistency as well as semantic relation at a certain level while not being as rigid as the features/operations on the very lexical level that many previous approaches use. We assume that these features, combined with the multi-objective tuning, will provide a robust and stable way to take advantage of documentlevel information in an SMT system. 3 3.1 Proposed Approach Overview The proposed approach is es"
2014.amta-researchers.9,E14-1035,0,0.0703531,"many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest feature because it only considers the lexical words themselves. In contrast, lexical cohesion involves more semantic information, such as hypernyms and hyponyms, usually requiring a word-net. Approaches that use the document topic as a feature usually require training data, such as a document-level parallel corpus, in the training or decoding phases. In this paper, we propose an approach that considers both the lexical consistency and semantic relation on the document level. The approach first uses an off-the-shelf SMT system"
2014.amta-researchers.9,D11-1125,0,0.0266109,").11 The word embedding is over a vocabulary of 130, 000 words, with 50-dimension vectors. In the document-level decoding algorithm, we set the margin in cube-pruning to [−10, 10] to enlarge the search space. The search generated 100 document-level candidates for re-ranking. t feature and the Deval calculation, we set window-size to 2. That is, the context was In the fdoc defined as the two preceding and two succeeding sentences. For weight tuning on the document level, the multi-objective tuning can be combined with any tuning algorithm, such as MERT (Och, 2003), MIRA (Chiang, 2012), or PRO (Hopkins and May, 2011). Our approach contains only two free weights, λtdoc and λst doc ; thus, we used a greedy search for them in (−1.0, 1.0), with step of 0.1, to avoid any possible search errors in the tuning phase. We took the consistency verification approach (Xiao et al., 2011) as the comparison approach in our experiments. Similar to our approach, this approach takes advantage of the mbest translation candidates and uses a further decoding step to polish the baseline sentence-level 8 http://www.speech.sri.com/projects/srilm/ 9 As well as the word alignment of the highest-scoring candidate. 10 http://ml.nec-l"
2014.amta-researchers.9,W04-3250,0,0.0953589,"iable, especially in scheme-2. Consequently, the consistency verification method does not perform well in scheme-2. In Tables 4 and 5, we show the experimental results of the proposed approach in scheme-1 and scheme-2, respectively. Different sets of weights on the frontier of Pareto optimality are listed,15 with their corresponding Seval and Deval on development set and Seval on test set (i.e., test set BLEU). The first rows, λtdoc = 0 and λst doc = 0 are the performance of the baseline SMT system for scheme-1 and scheme-2. We conduct a statistical significance test via the bootstrap method (Koehn, 2004) using bleu-kit16 . For each row, + and − mean the result is better or worse than the baseline, respectively: a single mark means the difference is on the level of p < 0.05 and a double mark means on the p < 0.01 level. For the overall performance, in scheme-1, the change of test set BLEU is in the range of [−0.01, +0.48] points compared to the baseline; in scheme-2, the range of change is in [−0.26, +0.56]. Because the Pareto frontier offers multiple weights rather than a deterministic, the change on test set BLEU we report here is a range rather than a deterministic value. 12 Xiao et al. (20"
2014.amta-researchers.9,2005.mtsummit-papers.11,0,0.00716137,"rwise, the approach produces several sets of {λtdoc , λst }, which suggests that we should pay attention to the document-level doc features. 4 Experiment 4.1 Data and Settings We tested the proposed approach on French-to-English translation because this translation task has been handled well by state-of-the-art SMT systems. We used two different schemes. One is on the WIT3 corpus of TED talks4 (Cettolo et al., 2012), which contains a small training set with document-level parallel development set and test set. The other scheme is a relatively more realistic setting: using the Europarl corpus (Koehn, 2005) for model training and an in-domain development set for the weight tuning in the baseline SMT system. Then we selected document pairs from the Common Crawl (CC) Corpus5 of WMT2013 for document-level development and test set. The CC corpus has a lot of noise, with many document pairs only several sentences long – too short for our purposes. Thus, we selected relatively high-quality document pairs, with moderate lengths of 40–60 sentences to compose the baseline. The data used in the two schemes and the detailed information are listed in Tables 1 and 2, respectively. In experiments, as previous"
2014.amta-researchers.9,W13-3303,0,0.0131583,"sentence pairs, and the translation is conducted sentence-by-sentence. Because in practice sentences are usually contained in a document and surrounded by context, recent research has begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; E"
2014.amta-researchers.9,P03-1021,0,0.256712,"(doc) of scheme-1are an identical set. train dev. (snt.) dev. (doc.) test scheme-1 0.14M snt. 934 snt. 8 doc. / 934 snt. 11 doc. / 1, 664 snt. scheme-2 1.99M snt. 2, 000 snt. 14 doc. / 600 snt. 55 doc. / 2, 500 snt. model was an interpolated 5-gram model with modified Kneser-Ney discounting, trained by SRILM8 (Stolcke, 2002), on each scheme’s training data. In sentence-level decoding, the ttablelimit was 20; the stack size was 200; and the distortion-limit was 6, all of which followed the default settings of Moses’ decoder. The feature weights of the baseline PB SMT system were tuned by MERT (Och, 2003) to optimize the sentence-level development set BLEU (Papineni et al., 2002). The settings in tuning and translating on sentence-level were identical. For the document-level decoding of the proposed approach, we used the baseline system to generate a 1000-best translation candidate list for each sentence in a document. Each translation candidate was attached with the word alignment information in sentence-level decoding for the st fdoc calculation. Duplicate candidates in a 1000-best list were merged to one candidate taking t the highest score9 of the baseline SMT system. For the fdoc calculat"
2014.amta-researchers.9,J03-1002,0,0.00350855,"with moderate lengths of 40–60 sentences to compose the baseline. The data used in the two schemes and the detailed information are listed in Tables 1 and 2, respectively. In experiments, as previously described, a baseline SMT system was built from sentencelevel parallel data (the train row in Tables 1 and 2) and tuned on sentence-level development set (the dev. (snt.) row). We used the phrase-based statistical machine translation (PB SMT) system of Moses6 (Koehn et al., 2007) as the baseline SMT system. In model training, we used the grow-diag-final-and to symmetrize the output of GIZA++7 (Och and Ney, 2003). The maxphrase-length was set to 7 and the reordering model was msd-bidirectional-fe. The language 3 We always set λ snt to be positive. 4 https://wit3.fbk.eu/ λtdoc and λst doc can be either positive or negative. 5 http://www.statmt.org/wmt13/translation-task.html 6 http://www.statmt.org/moses/ 7 https://code.google.com/p/giza-pp/ Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 116 Table 1: Data used in experiment. train dev. (snt.) dev. (doc.) test scheme-1 WIT3 WIT3 WIT3 WIT3 scheme-2 Europarl WMT dev2006 CC CC Table 2: Number of sent"
2014.amta-researchers.9,P02-1040,0,0.0927951,"oc.) test scheme-1 0.14M snt. 934 snt. 8 doc. / 934 snt. 11 doc. / 1, 664 snt. scheme-2 1.99M snt. 2, 000 snt. 14 doc. / 600 snt. 55 doc. / 2, 500 snt. model was an interpolated 5-gram model with modified Kneser-Ney discounting, trained by SRILM8 (Stolcke, 2002), on each scheme’s training data. In sentence-level decoding, the ttablelimit was 20; the stack size was 200; and the distortion-limit was 6, all of which followed the default settings of Moses’ decoder. The feature weights of the baseline PB SMT system were tuned by MERT (Och, 2003) to optimize the sentence-level development set BLEU (Papineni et al., 2002). The settings in tuning and translating on sentence-level were identical. For the document-level decoding of the proposed approach, we used the baseline system to generate a 1000-best translation candidate list for each sentence in a document. Each translation candidate was attached with the word alignment information in sentence-level decoding for the st fdoc calculation. Duplicate candidates in a 1000-best list were merged to one candidate taking t the highest score9 of the baseline SMT system. For the fdoc calculation, we used a high-quality 10 English word embedding used in the SENNA tool"
2014.amta-researchers.9,W13-3308,0,0.0228699,"on the target-side m-best candidate lists of source-side sentences to find a better combination (marked by the dashed zigzag arrow). is statistical and needs to be trained on monolingual or bilingual document-level data. Along with the feature of lexical cohesion, the topic is a sophisticated feature that must be supported by extra resources. Many approaches using document-level features require to modify the decoder of a baseline system to adapt to their features in decoding. Research mainly focusing on the decoding and tuning algorithm, such as the series work of Hardmeier et al. (2012) and Stymne et al. (2013), extends the traditional sentence-based SMT system to be able to collaborate with documentlevel features. As to our approach, the features used can model the lexical consistency as well as semantic relation at a certain level while not being as rigid as the features/operations on the very lexical level that many previous approaches use. We assume that these features, combined with the multi-objective tuning, will provide a robust and stable way to take advantage of documentlevel information in an SMT system. 3 3.1 Proposed Approach Overview The proposed approach is essentially a re-ranking pr"
2014.amta-researchers.9,W10-2602,0,0.22849,"ounded by context, recent research has begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest featu"
2014.amta-researchers.9,N12-1046,0,0.0739994,"as begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest feature because it only considers the lexica"
2014.amta-researchers.9,2011.mtsummit-papers.13,0,0.251451,", recent research has begun to focus on enhancing SMT systems with the addition of document-level information. As to the features of document-level translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest feature because it only"
2014.amta-researchers.9,D13-1163,0,0.0464399,"l translation, a frequently discussed issue is lexical consistency in translation: i.e., words tend to be translated consistently in a document (Carpuat, 2009; Carpuat and Simard, 2012). There are also detailed discussions around the consistency of different parts of speech (Guillou, 2013; Meyer and Webber, 2013). On the basis of lexical consistency theory, many researchers focus on increasing the lexical consistency in translation (Tiedemann, 2010; Xiao et al., 2011; Ture et al., 2012). Beyond lexical consistency, there are attempts at using lexical cohesion in translation (Ben et al., 2013; Xiong et al., 2013a,b), which considers the semantic relation between words. Rather than the lexical features, the topic of Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 110 documents is also taken as a feature in some recent research (Gong et al., 2011; Eidelman et al., 2012; Xiong and Zhang, 2013; Hasler et al., 2014). Among the different features, lexical consistency is the simplest feature because it only considers the lexical words themselves. In contrast, lexical cohesion involves more semantic information, such as hypernyms and hyponyms, usually r"
2014.amta-researchers.9,P07-2045,0,\N,Missing
2014.iwslt-evaluation.20,2012.eamt-1.60,0,0.0123428,"statistical machine translation (SMT) systems. Our focus was in several areas, specifically system combination, word alignment, and various language modeling techniques including the use of neural network joint models. Our experiments on the test set from the 2013 shared task, showed that an improvement in BLEU score can be gained in translation performance through all of these techniques, with the largest improvements coming from using large data sizes to train the language model. 1. Introduction In the IWSLT 2014 machine translation evaluation campaign, the NICT team participated in the TED [1] translation shared-task for Chinese-English. This paper describes the machine translation approach adopted for this campaign. Our system was a combination of phrase-based and hierarchical SMT systems. The combination was performed by reranking the n-best hypotheses from these systems. A loglinear model which used the hypothesis scores of the component systems as features was used to calculate the score used in reranking. Additional features were also added into the log-linear model, for example features from a neural network model, or talk-level language model scores. In addition to system co"
2014.iwslt-evaluation.20,P14-1129,0,0.135961,"mple features from a neural network model, or talk-level language model scores. In addition to system combination, we put emphasis on language modeling. We used three approaches to improve the language modeling in the system. In the first approach we used a language model that was an interpolation of an indomain language model, and a language model trained on the GIGAWORD data. In the second approach, we incorporated a language model trained on the machine translations of each talk in the test dataset into the reranking procedure. In the third approach, a bilingual feed-forward neural network [2] was used in the reranker. Finally, we also improved the word alignment by using combining the alignments from two independent aligners: GIZA++ [3] and a modified version of the CICADA aligner [4]. 2. Data We used same Chinese-English data sets in all of the experiments in this paper. The supplied bilingual data consisted of 179901 sentence pairs. From this data we randomly selected a 3023-pair development set for tuning the decoder, and a 1553-pair development set for tuning the reranker. These development sets consisted of complete talks. All of the remaining talks were used as bilingual tra"
2014.iwslt-evaluation.20,J03-1002,0,0.00988323,"ling. We used three approaches to improve the language modeling in the system. In the first approach we used a language model that was an interpolation of an indomain language model, and a language model trained on the GIGAWORD data. In the second approach, we incorporated a language model trained on the machine translations of each talk in the test dataset into the reranking procedure. In the third approach, a bilingual feed-forward neural network [2] was used in the reranker. Finally, we also improved the word alignment by using combining the alignments from two independent aligners: GIZA++ [3] and a modified version of the CICADA aligner [4]. 2. Data We used same Chinese-English data sets in all of the experiments in this paper. The supplied bilingual data consisted of 179901 sentence pairs. From this data we randomly selected a 3023-pair development set for tuning the decoder, and a 1553-pair development set for tuning the reranker. These development sets consisted of complete talks. All of the remaining talks were used as bilingual training data for the component SMT systems. We used the IWSLT 2013 test set for evaluation. For some of the experiments we used language models train"
2014.iwslt-evaluation.20,2005.mtsummit-papers.11,0,0.0128404,"rom this data we randomly selected a 3023-pair development set for tuning the decoder, and a 1553-pair development set for tuning the reranker. These development sets consisted of complete talks. All of the remaining talks were used as bilingual training data for the component SMT systems. We used the IWSLT 2013 test set for evaluation. For some of the experiments we used language models trained on the English LDC Gigaword dataset, a collection of approximately 4 billion words of international newswire text. 2.1. Pre-processing The English data was tokenized by applying the EUROPARL tokenizer [5]. We also removed all case information from the English text to help to minimize issues of data sparseness in the models of the translation system. All punctuation was left in both source and target. We took the decision to generate target punctuation directly using the process of translation, rather than as a punctuation restoration step in post processing based on experiments carried out for the 2010 IWSLT shared evaluation [6]. 2.2. Post-processing The output of the translation system was subject to the following post-processing steps which were carried out in the following order: 1. In all"
2014.iwslt-evaluation.20,P03-1021,0,0.0455445,"Missing"
2014.iwslt-evaluation.20,2013.iwslt-evaluation.1,0,0.0146617,"cores of the component systems (MERT) [10]. The weights for the models were tuned using the development data supplied for the task. 3.5. Evaluation We evaluated each of these systems on the IWSLT 2013 test set, and the results are shown in Table 3.5. The evaluation in all of the experiments in this report was carried out on tokenized, lowercase data, using the “multi-bleu.perl” evaluation script included in release version 2.1 of the MOSES toolkit. The systems are roughly comparable in performance, and about 1.5 BLEU percentage points higher than the caseinsensitive MOSES baseline reported in [11], we believe this can be explained by differences in the tokenization used for evaluation, and also by differences in the development sets used for tuning. We found that when tuned and evaluated on different data sets, the relative rankings of the systems may vary. 4. Methodology 4.1. Language Modeling 4.1.1. Neural Network Model We implemented the neural network joint models proposed in [2] and used the output as a feature in the reranker. We ran a set of experiments to determine the optimal network architecture. We varied the size of the context on both source and sides, and also the scale o"
2014.iwslt-evaluation.20,D13-1140,0,0.0203032,"d the output as a feature in the reranker. We ran a set of experiments to determine the optimal network architecture. We varied the size of the context on both source and sides, and also the scale of the neural network. We found the settings used in [2] gave rise the highest performance, and we therefore adopted these settings in our system. These settings were: 11-word source context, 3-word target context, 192-unit shared embedding layer, and two additional 512unit hidden layers. We set both input and output vocabulary size to 32000. The neural network was implemented using the NPLM toolkit [12]. The results are shown in Table 4.3. The gain using from this approach was approximately 0.5 BLEU points. This was lower than the gains reported in [2], however, in their experiments the neural network was directly integrated into the decoding process. We integrated monolingual neural network model into the OCTAVIAN decoder, however, the experiments were not completed due to time limitations. 4.1.2. Gigaword We combined language models trained on the source of the parallel TED corpus, and the Gigaword newswire corpus by linear interpolation. The interpolated language model was then used direc"
2014.iwslt-evaluation.20,P96-1041,0,0.370762,"Missing"
2014.iwslt-evaluation.20,P07-2045,0,0.0124426,"the following order: 1. In all experiments, the out of vocabulary words (OOVs) were passed through the translation process unchanged, some of these OOVs were Chinese and some English. For the primary submission, we took 139 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the decision to delete only those OOVs containing Chinese characters not included in the ASCII character set and leave words containing only ASCII characters in the output. 2. The output was de-tokenized using the de-tokenizer included with the MOSES toolkit [7]. 3. The output was re-cased using the re-casing tool supplied with the MOSES toolkit. We trained the recasing tool on cased text from the TED talk training data. 3. The Base Systems 3.1. Decoders Our submission used two SMT systems within a system combination framework; these systems were: 1. OCTAVIAN, an in-house phrase-based decoder. 2. A hierarchical version of the MOSES decoder [7]. The OCTAVIAN decoder used in these experiments is an in-house phrase-based statistical machine translation decoder that can operate in a similar manner to the publicly available MOSES decoder [7]. The base dec"
2014.iwslt-papers.5,N03-1017,0,0.00933623,"Missing"
2014.iwslt-papers.5,P07-2045,0,0.0182188,"Missing"
2014.iwslt-papers.5,2005.mtsummit-papers.11,0,0.0574603,"Missing"
2014.iwslt-papers.5,W12-4207,0,0.0512408,"Missing"
2014.iwslt-papers.5,N09-1028,0,0.0455172,"Missing"
2014.iwslt-papers.5,D12-1077,0,0.0272006,"Missing"
2014.iwslt-papers.5,C04-1073,0,0.107251,"Missing"
2014.iwslt-papers.5,P05-1066,0,0.116818,"Missing"
2014.iwslt-papers.5,C10-1043,0,0.0500847,"Missing"
2014.iwslt-papers.5,P03-1056,0,0.0731239,"Missing"
2014.iwslt-papers.5,P13-1045,0,0.0374312,"Missing"
2014.iwslt-papers.5,N03-1033,0,0.0314199,"Missing"
2014.iwslt-papers.5,2008.jeptalnrecital-long.17,0,0.133128,"Missing"
2014.iwslt-papers.5,J03-1002,0,0.0078425,"Missing"
2014.iwslt-papers.5,N04-4026,0,0.0740724,"Missing"
2014.iwslt-papers.5,P03-1021,0,0.080262,"Missing"
2014.iwslt-papers.5,P02-1040,0,0.0911959,"Missing"
2014.iwslt-papers.5,D10-1092,0,0.0676014,"Missing"
2014.iwslt-papers.5,Y13-1026,0,0.0235984,"Missing"
2015.mtsummit-papers.1,P84-1068,0,0.64869,"Missing"
2015.mtsummit-papers.1,N12-1047,0,0.062571,"t (Koehn et al., 2007) with distortion limit of six as the baseline. We examined each of our SSSS transfer, and pre-ordering modules and their combination over the baseline. For reference, we investigated the performance of phrase-based SMT with a larger distortion limit 20, as well as hierarchical phrase-based SMT. Throughout the experiments, we used KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szał, 2010) for word alignment. We used the grow-diag-final method for obtaining phrase pairs. Weights of the models were tuned with n-best batch MIRA (Cherry and Foster, 2012) regarding BLEU (Papineni et al., 2002) as the objective. For each system, we performed weight tuning three times and selected for the test the setting that achieved the best BLEU on the development data. 4.3. Evaluation Metrics Each system is evaluated using two metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). Although our primary concern in this experiment is the effect of long distance relationship, in general, n-gram based metrics such as BLEU alone do not fully illustrate it. RIBES is therefore used alongside BLEU. RIBES is an automatic evaluation method based on r"
2015.mtsummit-papers.1,P05-1033,0,0.267749,"uilt a patent-adapted (not claim-adapted) parsing model by applying a self-learning procedure (Huang et al., 2009) to the above automatic parses. (ROOT (S (NP (PRP He)) (VP (VBZ likes) (NP (NNS apples))) (. .))) Figure 7. Parsing result of “He likes apples.” 4. Experiments We evaluated to what extent our SSSS transfer and pre-ordering improved the translation quality. As mentioned in Section 3, these methods are implemented as an add-on to off-the-shelf SMT systems. In particular, we used phrase-based SMT (Koehn et al., 2003) as the base system. We also regard it and its hierarchical version (Chiang, 2005) as baseline SMT systems. 4.1. Data The training data for SMT consists of two subcorpora. The first is the Japanese-English Patent Translation data comprising 3.2 million sentence pairs provided by the organizer of the Patent Machine Translation Task (PatentMT) at the NTCIR-9 Workshop (Goto et al., 2011). We randomly selected 3.0 million sentence pairs. Henceforth, we call this Corpus A. SMT systems trained on the corpus are reasonably good at lexical selection in translating claim sentences, because the vocabulary and phrases are commonly used in entire patent documents, and Corpus A is of a"
2015.mtsummit-papers.1,P05-1066,0,0.0440006,"most the entire gain in the Japanese-to-English setting. Conversely, English sentences are much more difficult to parse than Japanese. As a result, the pre-ordering module can sometimes fail to bring the English word order close to that in Japanese. Nevertheless, as a result of SSSS transfer, which divides an input English sentence into shorter pieces, pre-ordering became more accurate, and the RIBES score was further improved. 5. Related Work The quality of machine translation across distant languages has been improved as a result of the recent introduction of syntactic information into SMT (Collins et al., 2005; Quirk et al., 2005; Katz-Brown and Collins, 2008; Sudo et al., 2013; Hoshino et al., 2013; Cai et al., 2014; Goto Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 11 et al., 2015). One of the promising avenues for further improvement appears to be the incorporation of sublanguage-specific information (Buchmann et al., 1984; Luckhardt, 1991). This is particularly important for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an ea"
2015.mtsummit-papers.1,N15-1105,0,0.107415,"Missing"
2015.mtsummit-papers.1,2012.amta-caas14.1,0,0.0123933,"tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source and target sentences using minimal depth structures for facilitating the transfer between the source and target structures (Buchmann et al., 1984). Much of the recent work relating to document and sentence structures between close languages focuses on structures centered on discourse connectives (Miltsakaki et al., 2005; Pitler and Nenkova, 2009; Meyer et al., 2011; Hajlaoui and Popescu-Belis, 2012; Meyer et al., 2012) and on resolving the ambiguity of discourse connectives connecting structural components. Conversely, when dealing with structures across distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across dist"
2015.mtsummit-papers.1,P13-2121,0,0.0327298,"entence pairs was randomly divided into development and test data respectively consisting of 1,000 English-Japanese claim sentence pairs. 4.2. Systems In this experiment, we regard the implementation of phrase-based SMT in the Moses toolkit (Koehn et al., 2007) with distortion limit of six as the baseline. We examined each of our SSSS transfer, and pre-ordering modules and their combination over the baseline. For reference, we investigated the performance of phrase-based SMT with a larger distortion limit 20, as well as hierarchical phrase-based SMT. Throughout the experiments, we used KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szał, 2010) for word alignment. We used the grow-diag-final method for obtaining phrase pairs. Weights of the models were tuned with n-best batch MIRA (Cherry and Foster, 2012) regarding BLEU (Papineni et al., 2002) as the objective. For each system, we performed weight tuning three times and selected for the test the setting that achieved the best BLEU on the development data. 4.3. Evaluation Metrics Each system is evaluated using two metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010a). Although our primary c"
2015.mtsummit-papers.1,I13-1147,0,0.0147867,"much more difficult to parse than Japanese. As a result, the pre-ordering module can sometimes fail to bring the English word order close to that in Japanese. Nevertheless, as a result of SSSS transfer, which divides an input English sentence into shorter pieces, pre-ordering became more accurate, and the RIBES score was further improved. 5. Related Work The quality of machine translation across distant languages has been improved as a result of the recent introduction of syntactic information into SMT (Collins et al., 2005; Quirk et al., 2005; Katz-Brown and Collins, 2008; Sudo et al., 2013; Hoshino et al., 2013; Cai et al., 2014; Goto Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 11 et al., 2015). One of the promising avenues for further improvement appears to be the incorporation of sublanguage-specific information (Buchmann et al., 1984; Luckhardt, 1991). This is particularly important for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source a"
2015.mtsummit-papers.1,D10-1092,0,0.147645,"Missing"
2015.mtsummit-papers.1,P13-1048,0,0.0299936,"sive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies on pattern translat"
2015.mtsummit-papers.1,N03-1017,0,0.0190389,"ble. We first parsed 200,000 patent sentences using the initial parsing model. We then built a patent-adapted (not claim-adapted) parsing model by applying a self-learning procedure (Huang et al., 2009) to the above automatic parses. (ROOT (S (NP (PRP He)) (VP (VBZ likes) (NP (NNS apples))) (. .))) Figure 7. Parsing result of “He likes apples.” 4. Experiments We evaluated to what extent our SSSS transfer and pre-ordering improved the translation quality. As mentioned in Section 3, these methods are implemented as an add-on to off-the-shelf SMT systems. In particular, we used phrase-based SMT (Koehn et al., 2003) as the base system. We also regard it and its hierarchical version (Chiang, 2005) as baseline SMT systems. 4.1. Data The training data for SMT consists of two subcorpora. The first is the Japanese-English Patent Translation data comprising 3.2 million sentence pairs provided by the organizer of the Patent Machine Translation Task (PatentMT) at the NTCIR-9 Workshop (Goto et al., 2011). We randomly selected 3.0 million sentence pairs. Henceforth, we call this Corpus A. SMT systems trained on the corpus are reasonably good at lexical selection in translating claim sentences, because the vocabula"
2015.mtsummit-papers.1,C94-2183,0,0.0489313,"cross distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. Ther"
2015.mtsummit-papers.1,E91-1054,0,0.456166,"tactic parsing (Isozaki et al., 2010b; de Gispert et al., 2015), with growing volumes of parallel patent corpora available, have brought significant improvements in the performance of statistical machine translation (SMT) for translating patent documents across distant language pairs (Goto et al., 2012; Goto et al., 2015). However, among various sentences within a patent document, patent claim sentences still pose difficulties for SMT resulting in low translation quality, despite their utmost legal importance. A patent claim sentence is written in a kind of sublanguage (Buchmann et al., 1984; Luckhardt, 1991) in the sense that it has the following two characteristics: (i) comprising a patent claim by itself with an extreme length and (ii) having a typical sentence structure composed of a fixed set of components irrespective of language, such as those illustrated in Figures 1 and 2. The difficulties in patent claim translation lie in these two characteristics. Regarding the first characteristic, the extreme lengths cause syntactic parsers to fail with consequent low Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 1 reordering accuracy. Regarding the second"
2015.mtsummit-papers.1,2006.eamt-1.24,0,0.551983,"Missing"
2015.mtsummit-papers.1,2012.amta-papers.20,0,0.0173078,"document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source and target sentences using minimal depth structures for facilitating the transfer between the source and target structures (Buchmann et al., 1984). Much of the recent work relating to document and sentence structures between close languages focuses on structures centered on discourse connectives (Miltsakaki et al., 2005; Pitler and Nenkova, 2009; Meyer et al., 2011; Hajlaoui and Popescu-Belis, 2012; Meyer et al., 2012) and on resolving the ambiguity of discourse connectives connecting structural components. Conversely, when dealing with structures across distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in whi"
2015.mtsummit-papers.1,P02-1040,0,0.0931704,"(2) Claim sentences are translated according to the sentence structure, producing structurally natural translation outputs. We manually extracted a set of language independent claim components. Moreover, using these components, we constructed a set of synchronous rules for English and Japanese to transfer the SSSS in the source language to the target language. The results of an experiment demonstrate these two major effects of our SSSS transfer method. Regarding the first effect, when used in conjunction with pre-ordering, our method improves translation quality by five points in BLEU score (Papineni et al., 2002), in both English-to-Japanese and Japanese-to-English translations. Regarding the second effect, gains in RIBES score (Isozaki et al., 2010a) of over 30 points are obtained, indicating that our SSSS transfer is effective in transferring an input sentence structure to the output sentence. Components Preamble Transitional phrase Body Element Element Element Example strings An apparatus, comprising: a pencil; an eraser attached to the pencil; and a light attached to the pencil. Figure 1. Example of an English patent claim (WIPO, 2014) Components Element Body Element Element Transitional phrase Pr"
2015.mtsummit-papers.1,P09-2004,0,0.0322502,"tant for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublanguage introduced the notion of flat trees which represents both source and target sentences using minimal depth structures for facilitating the transfer between the source and target structures (Buchmann et al., 1984). Much of the recent work relating to document and sentence structures between close languages focuses on structures centered on discourse connectives (Miltsakaki et al., 2005; Pitler and Nenkova, 2009; Meyer et al., 2011; Hajlaoui and Popescu-Belis, 2012; Meyer et al., 2012) and on resolving the ambiguity of discourse connectives connecting structural components. Conversely, when dealing with structures across distant language pairs, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this di"
2015.mtsummit-papers.1,P06-1055,0,0.0259631,"“having:”, ”備える”〉 〈“wherein:”, ”ことを特徴とする”〉 〈“wherein:”, ”する”〉 3.2. Pre-ordering Another major issue in patent claim translation is that the extreme lengths cause syntactic parsers to fail with consequent low reordering accuracy. To evaluate the effect of introducing our SSSS transfer on the translation quality, we also implemented a pre-ordering tool using state-of-the-art techniques (Isozaki et al., 2010b; Goto et al., 2012; Goto et al., 2015). Our pre-ordering method is based on syntactic parsing. First, the input sentence is parsed into a binary tree structure by using the Berkeley Parser (Petrov et al., 2006). For example, when “He likes apples.” is inputted into our English-to-Japanese translation system, it is parsed as shown in Figure 7. Second, the nodes in the parse tree are reordered using a classifier. For example, according to the classifier's decision, the two children of the “VP” node, i.e., “VBZ” and “NP”, are swapped, whereas the order of the two children of the “S” node, i.e., “NP” and “VP”, is retained. Once such a decision is made for every node with two children (henceforth, binary mode), the word order of the entire sentence becomes very similar to that in Japanese, i.e., “He (kar"
2015.mtsummit-papers.1,P05-1034,0,0.049209,"n the Japanese-to-English setting. Conversely, English sentences are much more difficult to parse than Japanese. As a result, the pre-ordering module can sometimes fail to bring the English word order close to that in Japanese. Nevertheless, as a result of SSSS transfer, which divides an input English sentence into shorter pieces, pre-ordering became more accurate, and the RIBES score was further improved. 5. Related Work The quality of machine translation across distant languages has been improved as a result of the recent introduction of syntactic information into SMT (Collins et al., 2005; Quirk et al., 2005; Katz-Brown and Collins, 2008; Sudo et al., 2013; Hoshino et al., 2013; Cai et al., 2014; Goto Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 11 et al., 2015). One of the promising avenues for further improvement appears to be the incorporation of sublanguage-specific information (Buchmann et al., 1984; Luckhardt, 1991). This is particularly important for translating formalized documents that tend to form sublanguage-specific document structures and sentence structures. In dealing with structures across close language pairs, an early study of sublang"
2015.mtsummit-papers.1,P13-2066,0,0.0184134,"re appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies on pattern translation (Xia et al., 2"
2015.mtsummit-papers.1,2007.mtsummit-papers.63,1,0.700657,"abulary and phrases are commonly used in entire patent documents, and Corpus A is of a substantial size to cover a large portion of them. However, the claim-specific sentence structure would never be taken into account, as Corpus A does not contain any claim sentences. To bring claim-specific characteristics into the SMT training, even for the baseline systems, we also used Corpus B comprising 1.0 million parallel sentences of patent claims. These were automatically extracted from pairs of English and Japanese patent documents published between 1999 and 2012 using a sentence alignment method (Utiyama and Isahara, 2007). The concatenation of Corpora A and B was used to train baseline SMT systems, as well as those for our extensions. 2 Note that Goto et al. (2015) learned the SWAP/STRAIGHT classification problem jointly with the parsing source sentences. 3 https://www.cis.upenn.edu/~treebank/ 4 https://www2.nict.go.jp/out-promotion/techtransfer/EDR/index.html Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 7 Development and test data were constructed separately from the training data in the following manner. First, we randomly extracted English patent documents from p"
2015.mtsummit-papers.1,N09-2004,0,0.0270482,"s, a more comprehensive approach is more appropriate. A wide range of research has been conducted in this direction. A study by Marcu et al. (2000) proposed a method for improving Japanese-to-English translation by transforming the source structure generated by a rhetorical structure theory (RST) parser, to the corresponding target structure. Some work in this direction has been conducted in translations across distant languages, in which the source text is parsed using an RST parser, and translation rules are automatically extracted from the source and target pair (Kurohashi and Nagao, 1994; Wu and Fung, 2009; Joty et al., 2013; Tu et al., 2013). There are also approaches of simplifying long sentences by capturing the overall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies"
2015.mtsummit-papers.1,C04-1073,0,0.141971,"Missing"
2015.mtsummit-papers.1,P14-2092,0,0.218298,"Missing"
2015.mtsummit-papers.1,P09-2035,0,0.0194868,"rall structure of a sentence, or a group of sentences. The skeleton-based approach (Mellebeek et al., 2006; Xiao, 2014) attempts to extract the key elements/structure (or skeleton) from the input sentence using a syntactic parser. The divide-and-translate approach (Shinhori et al., 2003; Sudo et al., 2010; Hung et al., 2012) also makes use of syntactically motivated features, such as phrases and clauses, for extracting subcomponents to be translated by SMT. There are also studies on pattern translation (Xia et al., 2004; Murakami et al., 2009; Murakami et al., 2013) and sentence segmentation (Xiong et al., 2009; Jin and Liu, 2010) for dealing with long input sentences with complex structures. Our approach is similar to the above models in the sense that it incorporates structural information into SMT, but differs in that it uses sublanguage-specific sentence structures, rather than syntactically motivated structures. This results in significant improvement in translation quality for the claim sublanguage using only a handful of rules. 6. Conclusion In this paper, we described a method for transferring sublanguage-specific sentence structure for English-to-Japanese and Japanese-to-English patent clai"
2020.acl-main.324,N19-1388,0,0.065454,"e attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using a combination of divers"
2020.acl-main.324,D18-1549,0,0.307174,"UNMT performance. Our experiments on a dataset with English translated to and from twelve other languages (including three language families and six language branches) show remarkable results, surpassing strong unsupervised individual baselines while achieving promising performance between non-English language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs. 1 Introduction Recently, neural machine translation (NMT) has been adapted to the unsupervised scenario in which NMT is trained without any bilingual data. Unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a) requires only monolingual corpora. UNMT achieves remarkable results by using a combination of diverse mechanisms (Lample et al., 2018b) such as an initialization with bilingual word embeddings, denoising auto-encoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. More recently, Lample and Conneau (2019) achieves better UNMT performance by introducing the pretrained language model. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language"
2020.acl-main.324,W19-5301,0,0.0735941,"Missing"
2020.acl-main.324,C18-1263,0,0.0276317,"e pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et a"
2020.acl-main.324,P15-1166,0,0.0246013,"7.58 25.05 14.09 9.75 25.84 10.90 23.80 10.07 13.09 28.82 12.41 15.79 19.57 27.59 16.62 11.05 28.56 12.77 25.25 10.92 14.33 32.38 14.78 15.47 19.28 26.79 15.62 10.57 27.78 12.03 25.52 11.11 14.33 31.28 13.83 15.93 20.00 27.80 17.21 11.58 28.62 13.12 25.98 11.22 15.17 32.43 15.30 Average 15.61 17.15 19.13 18.63 19.53 Table 5: The +FT column shows BLEU scores from further training of the MUNMT and LBKD model on the English to non-English language pairs. The other columns show results from Table 2. 7 Related Work Multilingual NMT has attracted much attention in the machine translation community. Dong et al. (2015) first extended NMT from the translation of a single language pair to multiple language pairs, using a shared encoder and multiple decoders and 3532 Corpus SM MUNMT +FT LBKD +FT Cs-En De-En Es-En Et-En Fi-En Fr-En Hu-En It-En Lt-En Lv-En Ro-En Tr-En 20.62 21.31 25.53 19.48 7.62 25.86 14.48 24.33 1.72 0.95 28.52 12.99 20.09 21.95 25.37 19.60 7.19 25.41 14.54 24.77 14.04 14.90 28.38 15.65 21.50 22.41 26.24 21.61 8.06 26.30 15.99 25.54 15.27 15.57 29.61 18.47 21.25 22.81 26.59 21.31 7.80 26.48 15.34 25.35 15.84 15.33 30.18 17.35 22.17 23.07 26.78 22.61 8.34 26.76 16.07 25.86 16.86 15.87 30.39 19."
2020.acl-main.324,N16-1101,0,0.0218044,"15.99 25.54 15.27 15.57 29.61 18.47 21.25 22.81 26.59 21.31 7.80 26.48 15.34 25.35 15.84 15.33 30.18 17.35 22.17 23.07 26.78 22.61 8.34 26.76 16.07 25.86 16.86 15.87 30.39 19.48 Average 16.95 19.32 20.55 20.47 21.19 Table 6: The +FT column shows BLEU scores from further training of the MUNMT and LBKD model on the non-English to English language pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) an"
2020.acl-main.324,R19-1050,0,0.0409383,"Missing"
2020.acl-main.324,N16-1162,0,0.0752553,"Missing"
2020.acl-main.324,Q17-1024,0,0.0420139,"Missing"
2020.acl-main.324,P07-2045,0,0.00631129,"n of the MUNMT and LBUNMT models, respectively, after encoding M j (Xi1 ) generated by the previous MUNMT model in the L1 → Lj direction. X j (M 1 (Xij )) and LB j (M 1 (Xij )) denote the softened Lj sentence probability distribution of the MUNMT and LBUNMT models, respectively, after encoding M 1 (Xij ) generated by the previous MUNMT model in the Lj → L1 direction. 5 5.1 Experiments Datasets To establish an MUNMT system, we considered 13 languages from WMT monolingual news crawl datasets: Cs, De, En, Es, Et, Fi, Fr, Hu, It, Lt, Lv, Ro, and Tr. For preprocessing, we used the Moses tokenizer (Koehn et al., 2007). For cleaning, we only applied the Moses script clean-corpus-n.perl to remove lines in the monolingual data containing more than 50 words. We then used a shared vocabulary for all languages, with 80,000 sub-word tokens based on BPE (Sennrich et al., 2016b). The statistics of the data are presented in Table 1. For Cs,De,En, we randomly extracted 50M monolingual news crawl data after cleaning; For other languages, we used all news crawl data after cleaning as shown in Table 1. Language Cs De En Es Et Fi Fr Hu It Lt Lv Ro Tr Sentences Words Sub-words 50.00M 50.00M 50.00M 36.33M 3.00M 15.31M 50.0"
2020.acl-main.324,C18-1054,0,0.023553,"ing of the MUNMT and LBKD model on the non-English to English language pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem o"
2020.acl-main.324,W18-6309,0,0.0284044,"ns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has be"
2020.acl-main.324,W19-5330,1,0.703395,") concatenated two bilingual corpora as one monolingual corpus, and used monolingual embedding pretraining in the initialization step, to achieve remarkable results with some similar language pairs. Lample and Conneau (2019) achieved better UNMT performance by introducing a pretrained language model. Sun et al. (2019, 2020) proposed to train UNMT with cross-lingual language representation agreement, to further improve UNMT performance. Moreover, an unsupervised translation task that evaluated in the WMT19 news translation task (Barrault et al., 2019) attracted many researchers to participate (Marie et al., 2019; Li et al., 2019). For Multilingual UNMT, Xu et al. (2019) exploited multiple auxiliary languages for jointly boosting UNMT models via the Polygon-Net framework. Sen et al. (2019) proposed an MUNMT scheme that jointly trains multiple languages with a shared encoder and multiple decoders. In contrast with their use of multiple decoders, we have constructed a simpler MUNMT model with one encoder and one decoder. Further, we have extended the four or five languages used in their work to thirteen languages, for training our MUNMT model. 8 Conclusion and Future Work In this paper, we have introduc"
2020.acl-main.324,D18-1039,0,0.0403498,"LBKD model on the non-English to English language pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs tra"
2020.acl-main.324,P18-1006,0,0.0664673,"(2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using a combination of diverse mechanisms such as initialization with bilingual word embeddings, denoising autoencoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. Lample et al. (2018b) concatenated two bilingual corpora as one monolingual corpus, and used monolingual embedding pr"
2020.acl-main.324,P19-1117,0,0.0411944,"rom Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using"
2020.acl-main.324,W18-6327,0,0.027368,"nglish to English language pairs. The other columns show results from Table 3. multiple attention mechanisms, for each language. Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe e"
2020.acl-main.324,P19-1297,0,0.455981,"mple and Conneau (2019) achieves better UNMT performance by introducing the pretrained language model. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time (Wang et al., 2020). Multilingual UNMT (MUNMT) translating multiple languages at the same time can save substantial training time and resources. Moreover, the performance of MUNMT in similar languages can promote each other. Research on MUNMT has been limited and there are only a few pioneer studies. For example, Xu et al. (2019) and Sen et al. (2019) proposed a multilingual scheme that jointly trains multiple languages with multiple decoders. However, the performance of their MUNMT is much worse than our re-implemented individual baselines (shown in Tables 2 and 3) and the scale of their study is modest (i.e., 4-5 languages). In this paper, we empirically introduce an unified framework to translate among thirteen languages (including three language families and six language branches) using a single encoder and single decoder, making use of multilingual data to improve UNMT for all languages. On the basis of these empirical findings, we pr"
2020.acl-main.324,P18-1005,0,0.0216021,"bulary. The entire training of UNMT needs to consider back-translation between the two languages and their respective denoising processes. In summary, the entire UNMT model can be optimized by minimizing: Lall = LD + LB . 3 Denoising Auto-encoder LD = 2.3 2.4 A cross-lingual masked language model, which can encode two monolingual sentences into a shared latent space, is first trained. The pretrained crosslingual encoder is then used to initialize the whole UNMT model (Lample and Conneau, 2019). Compared with previous bilingual embedding pretraining (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019), this pretraining can provide much more crosslingual information, causing the UNMT model to achieve better performance and faster convergence. 2.2 where {C(Xi1 )} and {C(Xi2 )} are noisy sentences. PL1 →L1 and PL2 →L2 denote the reconstruction probability in language L1 and L2 , respectively. 3.1 (3) Multilingual UNMT (MUNMT) Multilingual Pretraining Motivated by Lample and Conneau (2019), we construct a multilingual masked language model, using a single encoder. For each language, the language model is trained by encoding the masked input and revertin"
2020.acl-main.324,P16-1009,0,0.571282,"language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs. 1 Introduction Recently, neural machine translation (NMT) has been adapted to the unsupervised scenario in which NMT is trained without any bilingual data. Unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a) requires only monolingual corpora. UNMT achieves remarkable results by using a combination of diverse mechanisms (Lample et al., 2018b) such as an initialization with bilingual word embeddings, denoising auto-encoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. More recently, Lample and Conneau (2019) achieves better UNMT performance by introducing the pretrained language model. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time (Wang et al., 2020). Multilingual UNMT (MUNMT) translating multiple languages at the same time can save substantial training time and resources. Moreover, the performance of MUNMT in similar languages can promote each other. Research on MUNMT has been limited and there are only a"
2020.acl-main.324,P16-1162,0,0.832951,"language pairs in zero-shot translation scenarios and alleviating poor performance in low-resource language pairs. 1 Introduction Recently, neural machine translation (NMT) has been adapted to the unsupervised scenario in which NMT is trained without any bilingual data. Unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a) requires only monolingual corpora. UNMT achieves remarkable results by using a combination of diverse mechanisms (Lample et al., 2018b) such as an initialization with bilingual word embeddings, denoising auto-encoder (Vincent et al., 2010), back-translation (Sennrich et al., 2016a), and shared latent representation. More recently, Lample and Conneau (2019) achieves better UNMT performance by introducing the pretrained language model. However, conventional UNMT can only translate between a single language pair and cannot produce translation results for multiple language pairs at the same time (Wang et al., 2020). Multilingual UNMT (MUNMT) translating multiple languages at the same time can save substantial training time and resources. Moreover, the performance of MUNMT in similar languages can promote each other. Research on MUNMT has been limited and there are only a"
2020.acl-main.324,P19-1119,1,0.531451,"to consider back-translation between the two languages and their respective denoising processes. In summary, the entire UNMT model can be optimized by minimizing: Lall = LD + LB . 3 Denoising Auto-encoder LD = 2.3 2.4 A cross-lingual masked language model, which can encode two monolingual sentences into a shared latent space, is first trained. The pretrained crosslingual encoder is then used to initialize the whole UNMT model (Lample and Conneau, 2019). Compared with previous bilingual embedding pretraining (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019), this pretraining can provide much more crosslingual information, causing the UNMT model to achieve better performance and faster convergence. 2.2 where {C(Xi1 )} and {C(Xi2 )} are noisy sentences. PL1 →L1 and PL2 →L2 denote the reconstruction probability in language L1 and L2 , respectively. 3.1 (3) Multilingual UNMT (MUNMT) Multilingual Pretraining Motivated by Lample and Conneau (2019), we construct a multilingual masked language model, using a single encoder. For each language, the language model is trained by encoding the masked input and reverting it with this encoder. This pretrained m"
2020.acl-main.324,D19-1089,0,0.0734585,"Missing"
2020.acl-main.324,P19-1583,0,0.0192416,"Luong et al. (2016) translated multiple source languages to multiple target languages using a combination of multiple encoders and multiple decoders. Firat et al. (2016) used a shared attention mechanism but multiple encoders and decoders for each language. Ha et al. (2016) and Johnson et al. (2017) proposed a simpler method to use one encoder and one decoder to translate between multiple languages. Recently, many methods (Lakew et al., 2018; Platanios et al., 2018; Sachan and Neubig, 2018; Blackwood et al., 2018; Lu et al., 2018; Wang et al., 2019a; Aharoni et al., 2019; Wang et al., 2019b; Wang and Neubig, 2019) have been proposed to boost multilingual NMT performance. In particular, Tan et al. proposed a knowledge distillation method (Tan et al., 2019b) and a language clustering method (Tan et al., 2019a) to improve the performance of multilingual NMT. Ren et al. (2018) propose a triangular architecture to tackle the problem of low-resource pairs translation by introducing another rich language. To further tackle the problem of low-resource pairs translation, UNMT (Artetxe et al., 2018; Lample et al., 2018a) has been proposed, using a combination of diverse mechanisms such as initialization with bil"
2020.acl-main.324,W19-5325,0,\N,Missing
2020.acl-main.34,P05-1066,0,0.156159,"Missing"
2020.acl-main.34,P17-1012,0,0.0231835,"ule (ATT), and the second sub-layer is a position-wise fully connected feed-forward network (FNN). A residual connection (He et al., 2016) is applied between the sub-layers, and layer normalization (LN) (Ba et al., 2016) is performed. Formally, the l-th identical layer of this stack is as follows: l l−1 l−1 l−1 H = LN(ATTle (Ql−1 ) e , Ke , Ve ) + H l l Hl = LN(FFNle (H ) + H ). (1) l−1 l−1 {Ql−1 e , Ke , Ve } are query, key, and value vectors that are transformed from the (l-1)-th layer Hl−1 . For example, {Q0 , K 0 , V 0 } are packed from the H0 learned by the positional encoding mechanism (Gehring et al., 2017). Similarly, the decoder is composed of a stack of L identical layers. Compared with the stacked encoder, it contains an additional attention sublayer to compute alignment weights for the output of the encoder stack HL : l l−1 l−1 l−1 Si = LN(ATTld (Ql−1 i , Ki , Vi ) + Si ), l l L Cli = LN(ATTlc (Si , KL e , Ve ) + Si ), (2) Sli = LN(FFNld (Cli ) + Cli ), l−1 l−1 where Ql−1 are query, key, and d , Kd , and Vd value vectors, respectively, that are transformed from the (l-1)-th layer Sl−1 in time-step i. L {KL e , Ve } are transformed from the L-th layer of the encoder. The top layer of the dec"
2020.acl-main.34,N19-1122,0,0.0409928,"Missing"
2020.acl-main.34,D19-1088,0,0.0341217,"Missing"
2020.acl-main.34,N19-4009,0,0.0250087,"Missing"
2020.acl-main.34,P16-1162,0,0.0697775,"to further improve translation performance. 5 5.1 Experiments Setup The proposed methods were evaluated on the WMT14 English-to-German (EN-DE), WMT14 English-to-French (EN-FR), and WMT17 Chineseto-English (ZH-EN) tasks. The EN-DE corpus consists of 4M sentence pairs, the ZH-EN corpus of 22M sentence pairs, and the EN-FR corpus of 36M sentence pairs. We used the case-sensitive 4gram BLEU score as evaluation metric. The results of the newstest2014 test sets are reported for the EN-DE and EN-FR tasks, and the newstest2017 test set is reported for the ZH-EN task. The byte pair encoding algorithm (Sennrich et al., 2016) was applied to encode all sentences to limit the size of the vocabulary to 40K. The other configurations were identical to those in (Vaswani et al., 2017). The poposed models were implemented by using 360 EN-DE BLEU #Speed #Param Existing NMT systems Trans.base (Vaswani et al., 2017) 27.3 N/A 65.0M +Context-Aware SANs (Yang et al., 2019a) 28.26 N/A 106.9M +Convolutional SANs (Yang et al., 2019b) 28.18 N/A 88.0M +BIARN (Hao et al., 2019) 28.21 N/A 97.4M Trans.big (Vaswani et al., 2017) 28.4 N/A 213.0M +Context-Aware SANs (Yang et al., 2019a) 28.89 N/A 339.6M +Convolutional SANs (Yang et al., 2"
2020.acl-main.34,P07-1090,0,0.0422181,"tent or function words with UNK in a source sentence. Figure 1 shows that the BLEU scores of the test set decreased much ∗ Corresponding author Transformer (base) -Function Words -Content Words 20 more substantially when parts of content words were randomly replaced with UNK on the WMT14 English-to-German task, which is in line with the findings in He et al. (2019)’s work. To address this limitation, we propose a content word-aware NMT model that exploits the results of translation using a sequence of content words learned by a simple content word recognition method. Inspired by the works of (Setiawan et al., 2007, 2009; Zhang and Zhao, 2013), we first divide words in a sentence into content words and other function words depending on term frequencyinverse document frequency (TF-IDF) constraints. Two methods are designed to utilize the sequence of content word on the source and target sides: 1) We encode the content words of the source sentence as a new source representation, and learn an additional content word context vector based on it to improve translation performance; 2) A specific loss for content words of the target sentence is introduced to compensate for the original training objection, to ob"
2020.acl-main.34,P09-1037,0,0.0880705,"Missing"
2020.acl-main.34,N19-1407,0,0.0385394,"Missing"
2020.acl-main.44,2005.mtsummit-papers.11,0,0.14949,". Therefore, an extension of the original Zipf’s/power law requires at least two parameters. In this study, a three-parameter formulation of f ∝ r−α (r + γ)−β is derived based on the observation and analysis of multilingual corpora. It is a natural generalization of the power law and the Zipf-Mandelbrot law. The third parameter provides a depiction of the rigidness of different coefficients of proportionality. The proposed formulation can also fit non-Zipfian phenomena in natural languages, such as the r-f relation on Chinese characters. Figure 1 shows examples on English words from Europarl (Koehn, 2005) 1 and Chinese characters of Academia Sinica from the data of Sproat and Emerson (2003).2 2 Proposed and Related Formulation Under a logarithmic form, the Zipf’s law states that x + y = C, where (x, y) = (log r, log f ), and C is roughly a constant. We further investigate the 1 http://www.statmt.org/europarl/v8/ europarl.tgz 2 http://sighan.cs.uchicago.edu/ bakeoff2005/data/icwb2-data.zip 460 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 460–464 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 0 1 2 3 4 5 English Word Chinese"
2020.acl-main.44,P07-2045,0,0.00616617,"bg el fr β + γnorm bg cs da de el en es et fi fr hu it lt lv nl pl pt ro sk sl sv β γnorm α Experiment and Discussion We used the proposed formulation to fit data of various European languages and typical Asian languages. The Europarl corpus (Koehn, 2005) and data from the Second International Chinese Word Segmentation Bakeoff (ICWB2) (Sproat and Emerson, 2003) were mentioned in Section 1. We also used English-Japanese patent data from the 7th NTCIR Workshop (Fujii et al., 2008). The Europarl data and English data from NTCIR were lower-cased and tokenized using the toolkit provided by MOSES5 (Koehn et al., 2007). Fitting was performed under a logarithmic scale using the fit function6 in gnuplot.7 Specifically, relation-frequency data were used to fit (α, β, γ) and C in y = C −αx−β log10 (10x +10γ ). For the initialization, (α, β, γ) = (1, 1, rmax 2 ) and C = 3γ were applied. Table 1 lists the fitting results for all the languages8 in the Europarl corpus. The (α, β, γ) with Greek (el), English (en), Spanish (es), Estonian (et), Finnish (fi), French (fr), Hungarian (hu), Italian (it), Lithuanian (lt), Latvian (lv), Dutch (nl), Polish (pl), Portuguese (pt), Romanian (ro), Slovak (sk), Slovene (sl), and"
2020.acl-main.44,W98-1218,0,0.083013,". The simple proportionality of the Zipf’s/power law can be observed on randomly generated textual data (Li, 1992) and it only roughly depicts the r-f relation in real textual data. A two-parameter generalization of the Zipf’s/power law is the Zipf-Mandelbrot law, where f ∝ (r + β)−α (Mandelbrot, 1965). Li et al. (2010) considered the reversed rank of rmax +1−r, where rmax is the maximum of ranking index, and proposed a two-parameter formulation of f ∝ r−α (rmax + 1 − r)β . As a straightforward observation, the coefficients of proportionality should be distinguished for common and rear words (Powers, 1998; Li et al., 2010). Therefore, an extension of the original Zipf’s/power law requires at least two parameters. In this study, a three-parameter formulation of f ∝ r−α (r + γ)−β is derived based on the observation and analysis of multilingual corpora. It is a natural generalization of the power law and the Zipf-Mandelbrot law. The third parameter provides a depiction of the rigidness of different coefficients of proportionality. The proposed formulation can also fit non-Zipfian phenomena in natural languages, such as the r-f relation on Chinese characters. Figure 1 shows examples on English wor"
2020.acl-main.44,W03-1719,0,0.0340993,"st two parameters. In this study, a three-parameter formulation of f ∝ r−α (r + γ)−β is derived based on the observation and analysis of multilingual corpora. It is a natural generalization of the power law and the Zipf-Mandelbrot law. The third parameter provides a depiction of the rigidness of different coefficients of proportionality. The proposed formulation can also fit non-Zipfian phenomena in natural languages, such as the r-f relation on Chinese characters. Figure 1 shows examples on English words from Europarl (Koehn, 2005) 1 and Chinese characters of Academia Sinica from the data of Sproat and Emerson (2003).2 2 Proposed and Related Formulation Under a logarithmic form, the Zipf’s law states that x + y = C, where (x, y) = (log r, log f ), and C is roughly a constant. We further investigate the 1 http://www.statmt.org/europarl/v8/ europarl.tgz 2 http://sighan.cs.uchicago.edu/ bakeoff2005/data/icwb2-data.zip 460 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 460–464 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 0 1 2 3 4 5 English Word Chinese Character Artificial gi00 = 0 gi0 − gi−1 . xi − xi−1 (1) exp(bc − d) ∝ r−α (r + γ)−β ,"
2020.coling-main.374,D18-1549,0,0.0786934,"e the robustness of the UNMT based systems. First of all, we clearly defined two types of noises in training sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios. 1 Introduction Recently, unsupervised neural machine translation (UNMT) has attracted great interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Sun et al., 2020b). Typically, UNMT relies solely on monolingual corpora rather than bilingual parallel data in supervised neural machine translation (SNMT) to model translations between the source language and target language and has achieved remarkable results on several translation tasks (Conneau and Lample, 2019). However, previous work only focus on how to build stateof-the-art UNMT systems on the clean data and ignore the robustness of UNMT on the noisy data. In the real-world scenario, there often exists"
2020.coling-main.374,P18-1163,0,0.0194499,"put sentences, for example, word character misspelling, replacement, or word position misordering, etc. The translation model is sensitive to these perturbations, leading to various errors even the perturbations are small. The existing neural translation system, which lacks of robustness, is difficult to be widely applied to the noisy-data scenario (denoted as noisy scenario in the following sections). Therefore, the robustness of neural translation system is not only worthy of being studied, but also very essential in the real-world scenarios. The robustness of SNMT (Belinkov and Bisk, 2018; Cheng et al., 2018; Cheng et al., 2019; Karpukhin et al., 2019) has been well-studied. However, most previous work only focus on the effect of the word substitution for translation performance, and ignore the effect of word order for translation performance. Moreover, the noisy robustness of UNMT is much more difficult since the noisy input data may be relieved in some degree by the SNMT due to its supervised check in training. Currently, there is no study considering the noisy robustness of the UNMT. In this paper, we first define two types of noises which cover the noise types mentioned above, i.e., word nois"
2020.coling-main.374,P19-1425,0,0.025815,"xample, word character misspelling, replacement, or word position misordering, etc. The translation model is sensitive to these perturbations, leading to various errors even the perturbations are small. The existing neural translation system, which lacks of robustness, is difficult to be widely applied to the noisy-data scenario (denoted as noisy scenario in the following sections). Therefore, the robustness of neural translation system is not only worthy of being studied, but also very essential in the real-world scenarios. The robustness of SNMT (Belinkov and Bisk, 2018; Cheng et al., 2018; Cheng et al., 2019; Karpukhin et al., 2019) has been well-studied. However, most previous work only focus on the effect of the word substitution for translation performance, and ignore the effect of word order for translation performance. Moreover, the noisy robustness of UNMT is much more difficult since the noisy input data may be relieved in some degree by the SNMT due to its supervised check in training. Currently, there is no study considering the noisy robustness of the UNMT. In this paper, we first define two types of noises which cover the noise types mentioned above, i.e., word noise and word order noi"
2020.coling-main.374,P18-2006,0,0.0213336,"ask of WMT19 by combining UNMT and unsupervised statistical machine translation. However, previous work only focuses on how to build state-of-the-art UNMT systems and ignore the robustness of UNMT on the noisy data. In this paper, we propose adversarial training methods with denoising process in UNMT training to improve the robustness of the UNMT systems. Moreover, our proposed methods could improve the UNMT performance even in clean scenarios. Actually, Belinkov and Bisk (2018) pointed out that synthetic and natural noise both influenced the translation performance. Belinkov and Bisk (2018), Ebrahimi et al. (2018), and Karpukhin et al. (2019) designed character-level noise, which affects the spelling of a single word, to improve the model robustness. Meanwhile, both textual and phonetic embeddings were used to improve the robustness of SNMT to homophone noises (Liu et al., 2019). Adversarial examples, generated by gradient-based method, attacked the translation model to improve the robustness of SNMT (Cheng et al., 2019). In contrast with this work, we applied adversarial perturbation to the denoising training of UNMT , instead of translation training, to enhance the learning ability of UNMT model. 424"
2020.coling-main.374,D17-1215,0,0.08645,"Missing"
2020.coling-main.374,D19-5506,0,0.0949022,"er misspelling, replacement, or word position misordering, etc. The translation model is sensitive to these perturbations, leading to various errors even the perturbations are small. The existing neural translation system, which lacks of robustness, is difficult to be widely applied to the noisy-data scenario (denoted as noisy scenario in the following sections). Therefore, the robustness of neural translation system is not only worthy of being studied, but also very essential in the real-world scenarios. The robustness of SNMT (Belinkov and Bisk, 2018; Cheng et al., 2018; Cheng et al., 2019; Karpukhin et al., 2019) has been well-studied. However, most previous work only focus on the effect of the word substitution for translation performance, and ignore the effect of word order for translation performance. Moreover, the noisy robustness of UNMT is much more difficult since the noisy input data may be relieved in some degree by the SNMT due to its supervised check in training. Currently, there is no study considering the noisy robustness of the UNMT. In this paper, we first define two types of noises which cover the noise types mentioned above, i.e., word noise and word order noise. Then we empirically ∗"
2020.coling-main.374,P07-2045,0,0.00911683,"nal embedding (Position AT), and the combination of word and positional adversarial training (Both AT), all of which enrich robust information via adversarial perturbation. 5 Experiments 5.1 Datasets We considered two language pairs to do simulated experiments on the Fr↔En and German(De)↔En translation tasks. We used 50 million sentences from WMT monolingual news crawl datasets for each language. To make our experiments comparable with previous work (Conneau and Lample, 2019), we reported results on newstest2014 for Fr↔En and newstest2016 for De↔En. For preprocessing, we used Moses tokenizer (Koehn et al., 2007)1 for all languages. For cleaning, we only applied the Moses script clean-corpus-n.perl to remove lines in the monolingual data containing more than 50 tokens. For BPE (Sennrich et al., 2016b), we used a shared vocabulary for every language pair with 60K subword tokens based on BPE. 5.2 UNMT Settings We used a transformer-based XLM toolkit2 and followed settings of Conneau and Lample (2019) for UNMT: 6 layers for the encoder and the decoder. The dimension of hidden layers was set to 1024. The Adam optimizer (Kingma and Ba, 2015) was used to optimize the model parameters. The initial learning r"
2020.coling-main.374,P19-1291,0,0.0385075,"Missing"
2020.coling-main.374,W19-5330,1,0.743883,"noisy input. 6 Related Work Recently, UNMT (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019) that relies solely on monolingual corpora in each language via bilingual word embedding initialization, denoising auto-encoder, back-translation and sharing latent representations. More recently, Conneau and Lample (2019) and Song et al. (2019) introduced the pretrained cross-lingual language model to achieve state-of-the-art UNMT performance. Sun et al. (2020a) extended UNMT to the multilingual UNMT training on a large scale of European languages. Marie et al. (2019) won the first place in the unsupervised translation task of WMT19 by combining UNMT and unsupervised statistical machine translation. However, previous work only focuses on how to build state-of-the-art UNMT systems and ignore the robustness of UNMT on the noisy data. In this paper, we propose adversarial training methods with denoising process in UNMT training to improve the robustness of the UNMT systems. Moreover, our proposed methods could improve the UNMT performance even in clean scenarios. Actually, Belinkov and Bisk (2018) pointed out that synthetic and natural noise both influenced t"
2020.coling-main.374,D18-1050,0,0.0202074,"ut with different level of noise was 22.76 BLEU scores more in average for the word noise scenario, and 24.09 BLEU scores more in average for the word order noise scenario, compared with the UNMT system. These further demonstrate that our proposed Both AT mechanism is robust and can effectively alleviate the impact of two types of noise on translation performance. 5.5 Evaluation on MTNT dataset To better assess the effectiveness of our proposed adversarial training methods, we investigated the performance of UNMT with Both AT framework on the MTNT dataset, which is a noisy dataset proposed by Michel and Neubig (2018). The detailed statistics of MTNT data set is presented as shown in Table 3. To make our experiments comparable with previous work (Michel and Neubig, 2018; Zhou et al., 2019), we used the same MTNT parallel training data to fine-tune our proposed +Both AT system and used sacreBLEU (Post, 2018) to evaluate the translation performance. Corpus en-fr fr-en Training set Valid set Test set 36,058 852 1,020 19,161 886 1,022 Methods en-fr fr-en Michel and Neubig (2018) +Fine-tuning Zhou et al. (2019) +Fine-tuning 21.77 29.73 n/a n/a 23.27 30.29 24.50 31.70 31.60 39.00 33.80 41.30 +Both AT +Fine-tunin"
2020.coling-main.374,W18-6319,0,0.0183283,"e impact of two types of noise on translation performance. 5.5 Evaluation on MTNT dataset To better assess the effectiveness of our proposed adversarial training methods, we investigated the performance of UNMT with Both AT framework on the MTNT dataset, which is a noisy dataset proposed by Michel and Neubig (2018). The detailed statistics of MTNT data set is presented as shown in Table 3. To make our experiments comparable with previous work (Michel and Neubig, 2018; Zhou et al., 2019), we used the same MTNT parallel training data to fine-tune our proposed +Both AT system and used sacreBLEU (Post, 2018) to evaluate the translation performance. Corpus en-fr fr-en Training set Valid set Test set 36,058 852 1,020 19,161 886 1,022 Methods en-fr fr-en Michel and Neubig (2018) +Fine-tuning Zhou et al. (2019) +Fine-tuning 21.77 29.73 n/a n/a 23.27 30.29 24.50 31.70 31.60 39.00 33.80 41.30 +Both AT +Fine-tuning Table 3: Statistics of MTNT data set. Table 4: BLEU score on the En-Fr MTNT test set. As shown in Table 4, Our proposed +Both AT system significantly outperformed the previous work(Michel and Neubig, 2018; Zhou et al., 2019) by approximately 10 BLEU scores. The performance of our proposed sys"
2020.coling-main.374,P16-1009,0,0.166098,"e the model learning ability by introducing noise in the form of random token deleting and swapping in this input sentence. The denoising auto-encoder, which encodes a noisy version and reconstructs it with the decoder in the same language, acts as a language model during UNMT training. It is optimized by minimizing the objective function: LD = |X| X − log PL1 →L1 (Xi |C(Xi )) + |Y | X − log PL2 →L2 (Yi |C(Yi )), (2) i=1 i=1 where {C(Xi )} and {C(Yi )} are noisy sentences. PL1 →L1 and PL2 →L2 denote the reconstruction probability in the language L1 and L2 , respectively. Back-translation: It (Sennrich et al., 2016a) is adapted to train a translation system across different languages based on monolingual corpora. The pseudo-parallel sentence pairs {(YM (Xi ), Xi )} and {(XM (Yi ), Yi )} produced by the model at the previous iteration would be used to train the new translation model. The UNMT model would be improved through iterative back-translation. Therefore, the back-translation probability would be optimized by minimizing LB = |X| X − log PL2 →L1 (Xi |YM (Xi )) + i=1 |Y | X − log PL1 →L2 (Yi |XM (Yi )), (3) i=1 where PL1 →L2 and PL2 →L1 denote the translation probability across the two languages. Sh"
2020.coling-main.374,P16-1162,0,0.368756,"e the model learning ability by introducing noise in the form of random token deleting and swapping in this input sentence. The denoising auto-encoder, which encodes a noisy version and reconstructs it with the decoder in the same language, acts as a language model during UNMT training. It is optimized by minimizing the objective function: LD = |X| X − log PL1 →L1 (Xi |C(Xi )) + |Y | X − log PL2 →L2 (Yi |C(Yi )), (2) i=1 i=1 where {C(Xi )} and {C(Yi )} are noisy sentences. PL1 →L1 and PL2 →L2 denote the reconstruction probability in the language L1 and L2 , respectively. Back-translation: It (Sennrich et al., 2016a) is adapted to train a translation system across different languages based on monolingual corpora. The pseudo-parallel sentence pairs {(YM (Xi ), Xi )} and {(XM (Yi ), Yi )} produced by the model at the previous iteration would be used to train the new translation model. The UNMT model would be improved through iterative back-translation. Therefore, the back-translation probability would be optimized by minimizing LB = |X| X − log PL2 →L1 (Xi |YM (Xi )) + i=1 |Y | X − log PL1 →L2 (Yi |XM (Yi )), (3) i=1 where PL1 →L2 and PL2 →L1 denote the translation probability across the two languages. Sh"
2020.coling-main.374,P19-1119,1,0.906137,"s of noises in training sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios. 1 Introduction Recently, unsupervised neural machine translation (UNMT) has attracted great interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Sun et al., 2020b). Typically, UNMT relies solely on monolingual corpora rather than bilingual parallel data in supervised neural machine translation (SNMT) to model translations between the source language and target language and has achieved remarkable results on several translation tasks (Conneau and Lample, 2019). However, previous work only focus on how to build stateof-the-art UNMT systems on the clean data and ignore the robustness of UNMT on the noisy data. In the real-world scenario, there often exists noises or perturbations in the input sentences, for example, word character missp"
2020.coling-main.374,2020.acl-main.324,1,0.900456,"ining sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios. 1 Introduction Recently, unsupervised neural machine translation (UNMT) has attracted great interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Sun et al., 2020b). Typically, UNMT relies solely on monolingual corpora rather than bilingual parallel data in supervised neural machine translation (SNMT) to model translations between the source language and target language and has achieved remarkable results on several translation tasks (Conneau and Lample, 2019). However, previous work only focus on how to build stateof-the-art UNMT systems on the clean data and ignore the robustness of UNMT on the noisy data. In the real-world scenario, there often exists noises or perturbations in the input sentences, for example, word character misspelling, replacemen"
2020.coling-main.374,P18-1005,0,0.100191,"First of all, we clearly defined two types of noises in training sentences, i.e., word noise and word order noise, and empirically investigate its effect in the UNMT, then we propose adversarial training methods with denoising process in the UNMT. Experimental results on several language pairs show that our proposed methods substantially improved the robustness of the conventional UNMT systems in noisy scenarios. 1 Introduction Recently, unsupervised neural machine translation (UNMT) has attracted great interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Sun et al., 2020b). Typically, UNMT relies solely on monolingual corpora rather than bilingual parallel data in supervised neural machine translation (SNMT) to model translations between the source language and target language and has achieved remarkable results on several translation tasks (Conneau and Lample, 2019). However, previous work only focus on how to build stateof-the-art UNMT systems on the clean data and ignore the robustness of UNMT on the noisy data. In the real-world scenario, there often exists noises or perturbations in the input sent"
2020.coling-main.374,W19-5368,0,0.0186829,"with the UNMT system. These further demonstrate that our proposed Both AT mechanism is robust and can effectively alleviate the impact of two types of noise on translation performance. 5.5 Evaluation on MTNT dataset To better assess the effectiveness of our proposed adversarial training methods, we investigated the performance of UNMT with Both AT framework on the MTNT dataset, which is a noisy dataset proposed by Michel and Neubig (2018). The detailed statistics of MTNT data set is presented as shown in Table 3. To make our experiments comparable with previous work (Michel and Neubig, 2018; Zhou et al., 2019), we used the same MTNT parallel training data to fine-tune our proposed +Both AT system and used sacreBLEU (Post, 2018) to evaluate the translation performance. Corpus en-fr fr-en Training set Valid set Test set 36,058 852 1,020 19,161 886 1,022 Methods en-fr fr-en Michel and Neubig (2018) +Fine-tuning Zhou et al. (2019) +Fine-tuning 21.77 29.73 n/a n/a 23.27 30.29 24.50 31.70 31.60 39.00 33.80 41.30 +Both AT +Fine-tuning Table 3: Statistics of MTNT data set. Table 4: BLEU score on the En-Fr MTNT test set. As shown in Table 4, Our proposed +Both AT system significantly outperformed the previo"
2020.coling-main.374,W18-6401,0,\N,Missing
2020.coling-main.374,N19-1120,0,\N,Missing
2020.coling-main.374,W19-5301,0,\N,Missing
2020.coling-main.376,D17-1209,0,0.0287748,"Missing"
2020.coling-main.376,2020.acl-main.147,0,0.0221429,"ng strategies using RNN namely parallel, hierarchical and mixed, are tried to integrate the dependency information. Wu et al. (2018) used dependency information of both the source and the target languages. As their model needs multiple encoders and decoders, so it is not worthy for use under low-resource condition. In the work of Zhang et al. (2019), the authors employed a supervised encoder-decoder dependency parser and used the outputs from the encoder as a syntax-aware representations of words, which in turn, are concatenated to the input embeddings of the translation model. Most recently, Bugliarello and Okazaki (2020) proposed dependency-aware self-attention in the Transformer that needs no extra parameter. For a pivot word, its self-attention scores with other words are weighted considering their distances from the dependency parent of the pivot word. Apart from the above works, there are some studies which use the factors in the target side (Burlot et al., 2017; Garc´ıa-Mart´ınez et al., 2016a; Garc´ıa-Mart´ınez et al., 2016b). In general, their approach is to predict the roots and other morphological tags of the target words instead of producing the surface forms. Additionally, a morphological analyzer"
2020.coling-main.376,W17-4703,0,0.042239,"Missing"
2020.coling-main.376,P17-1177,0,0.0183396,"anguage Treebank (Riza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and fou"
2020.coling-main.376,P16-1078,0,0.020133,"s having diverse morphological variations taken from the Asian Language Treebank (Riza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a"
2020.coling-main.376,J82-2005,0,0.664999,"Missing"
2020.coling-main.376,D17-1012,0,0.0119381,"iza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and found that its different layers r"
2020.coling-main.376,U16-1001,0,0.0175246,"ords due to the inflectional nature of the language. Hence, for morphologically rich languages, ideally, the use of language specific knowledge should improve the translation quality. To the best of our knowledge, there are not an adequate amount of research works on effectively incorporating arbitrary syntactic information into NMT. One possible reason could be that in high resource scenario the network learns from the large amount of training data to handle the problem caused by polysemy and morphological variants. In this direction, the notable works are done by (Sennrich and Haddow, 2016; Hoang et al., 2016; Li et al., 2018). Sennrich and Haddow (2016) incorporated several features at the source side by employing a separate embedding matrix for each component of a source token including the word and its associated features. Finally, all embeddings are concatenated to enrich the representation. Inspired by this work, Hoang et al. (2016) developed a method to process feature sequences of the source sentence by separate recurrent neural networks (RNNs) and combined the output of all RNNs using a hybrid global-local attention strategy. Li et al. (2018) proposed a complex RNN architecture to model so"
2020.coling-main.376,P17-4012,0,0.0292268,"p 15 512 15 15 15 Table 2: Embedding dimensions of the components. Baseline models Proposed models Base Concat Add Linear Self-rel Word-rel en-bg 4.97 5.56 4.66 4.89 6.10 6.25 en-fi 25.59 23.75 22.02 24.26 26.26 26.01 en-hi 18.54 20.69 15.45 20.65 21.27 21.63 en-id 27.93 27.99 24.78 27.17 30.41 26.53 en-khm 22.88 23.53 21.65 23.42 24.76 25.13 en-ms 32.40 32.92 30.45 32.64 34.71 33.20 en-my 13.93 14.92 11.86 13.79 16.53 15.62 en-vi 24.99 26.50 22.78 25.36 27.74 27.66 Table 3: BLEU scores of the models for all reference language pairs. Hyperparameters: We use the OpenNMT PyTorch implementation (Klein et al., 2017) to build our models and mostly follow the Transformer-base hyperparameter setting mentioned there4 . There are 6 layers in each of the encoder and the decoder stacks. The number of multi-heads used is 8. The dimension of the fully-connected-feed-forward network is 2, 048. Total number of training steps is set to 200, 000 and after each 10, 000 steps validation checking is performed. We use the early-stopping strategy in training. If the validation accuracy does not improve for 5 consecutive validation checking steps, then training stops. Following (Sennrich and Haddow, 2016), we keep the dime"
2020.coling-main.376,P17-1064,0,0.0608027,"sis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and found that its different layers represent differen"
2020.coling-main.376,D15-1166,0,0.23826,") word-based relevance, to improve the representation of features for NMT. Experiments are conducted on translation tasks from English to eight Asian languages, with no more than twenty thousand sentences for training. The proposed methods improve translation quality for all tasks by up to 3.09 BLEU points. Discussions with visualization provide the explainability of the proposed methods where we show that the relevance methods provide weights to features thereby enhancing their impact on low-resource machine translation. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017; Kitaev et al., 2020) is known to give state-of-the-art translation quality for language pairs having abundance of parallel corpora. In case of resource poor scenarios, additional translation knowledge is acquired either through transfer learning in the form of pre-trained model parameters or by supplying external monolingual corpora. However, exploiting linguistic information effectively in low-resource conditions is still an under-researched field. Annotating the source side with various syntactic features e.g. part-of-speech (POS), lemma, dependency labels etc. can he"
2020.coling-main.376,P14-5010,0,0.00251552,"ymbols. Linguistic Features Used: We use three linguistic features of the source language in our experiments. They are - (i) lemma, (ii) POS tag and (iii) dependency label. For morphologically rich languages where roots have multiple variants, there tagging the raw text with these three features helps to disambiguate homonymy and polysemy. In particular, if experiments are done at subword-level, then annotating each subword with the word-level features is expected to feed into the model’s performance. As the source side is fixed to English, we annotate the English data using Stanford CoreNLP (Manning et al., 2014) toolkit. The vocabulary sizes of the three features are 26, 414; 43 and 45 respectively. Subword Tags: All experiments are done at subword-level to reduce the out-of-vocabulary cases during inference. We segment the datasets into subword units using byte-pair encoding (BPE) (Sennrich et al., 2016) technique keeping the number of merge operations to be 10, 000. Note that in BPE segmentation there is no explicit word boundary and a symbol may form either of the beginning/inside/end/whole of a word. Hence, following Sennrich and Haddow (2016), we add an extra feature to each subword in the sourc"
2020.coling-main.376,P02-1040,0,0.110315,"ation checking steps, then training stops. Following (Sennrich and Haddow, 2016), we keep the dimension of the final embedding which is fed to the Transformer, comparable across the models without and with using features so that the number of model parameters does not influence the performance. In Table 2 we list the embedding dimensions of the subword and its features for all experimental settings. Inference is done keeping beam size equal to 5. We carry out our experiments using single GPU with the specification of 32 GB Tesla V100-SXM2. 4.1 Results BLEU Scores: We present the BLEU scores5 (Papineni et al., 2002) of our proposed methods and the baselines in Table 3. The scores are computed after undoing the BPE segmentation of the translations. For en-fi, en-id, en-ms, en-my and en-vi, the self relevance checking strategy yields the best results (26.26, 30.41, 34.71, 16.53 and 27.74 respectively). For en-bg, en-hi and en-khm, the wordbased relevance method outperforms others (6.25, 21.63 and 25.13 respectively). Compared to the base configuration, maximum improvement is obtained for en-hi (18.54 → 21.63) and minimum for en-fi (25.59 → 26.26). Compared to (Sennrich and Haddow, 2016) i.e. the concat com"
2020.coling-main.376,W16-2209,0,0.275768,"shared by different root words due to the inflectional nature of the language. Hence, for morphologically rich languages, ideally, the use of language specific knowledge should improve the translation quality. To the best of our knowledge, there are not an adequate amount of research works on effectively incorporating arbitrary syntactic information into NMT. One possible reason could be that in high resource scenario the network learns from the large amount of training data to handle the problem caused by polysemy and morphological variants. In this direction, the notable works are done by (Sennrich and Haddow, 2016; Hoang et al., 2016; Li et al., 2018). Sennrich and Haddow (2016) incorporated several features at the source side by employing a separate embedding matrix for each component of a source token including the word and its associated features. Finally, all embeddings are concatenated to enrich the representation. Inspired by this work, Hoang et al. (2016) developed a method to process feature sequences of the source sentence by separate recurrent neural networks (RNNs) and combined the output of all RNNs using a hybrid global-local attention strategy. Li et al. (2018) proposed a complex RNN arch"
2020.coling-main.376,P16-1162,0,0.0584049,"elps to disambiguate homonymy and polysemy. In particular, if experiments are done at subword-level, then annotating each subword with the word-level features is expected to feed into the model’s performance. As the source side is fixed to English, we annotate the English data using Stanford CoreNLP (Manning et al., 2014) toolkit. The vocabulary sizes of the three features are 26, 414; 43 and 45 respectively. Subword Tags: All experiments are done at subword-level to reduce the out-of-vocabulary cases during inference. We segment the datasets into subword units using byte-pair encoding (BPE) (Sennrich et al., 2016) technique keeping the number of merge operations to be 10, 000. Note that in BPE segmentation there is no explicit word boundary and a symbol may form either of the beginning/inside/end/whole of a word. Hence, following Sennrich and Haddow (2016), we add an extra feature to each subword in the source side in addition to the three linguistic features stated above. Every subword is annotated with one of the four markers - B (beginning), I (inside), E (end), S (single). The annotation is done with the help of the script provided in the corresponding url3 . Table 1 depicts the structure of a samp"
2020.coling-main.376,D16-1159,0,0.0269826,"logical variations taken from the Asian Language Treebank (Riza et al., 2016). Our hypothesis is empirically validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of trai"
2020.coling-main.376,W18-6459,0,0.0831158,"y validated showing the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and found that its different layers represent different types of syntax"
2020.coling-main.376,N19-1118,0,0.0608803,"ng the fruitfulness of the relevance checking mechanisms in low-resource scenario. We achieve up to 3.09 BLEU points gain over the standard baseline models of Sennrich and Haddow (2016) and Vaswani et al. (2017). In the next section, the related works are briefly described. 2 Related Works Incorporating morphological information for NMT is a challenging area of research. A significant number of works involve dependency structure at the source side (Eriguchi et al., 2016; Shi et al., 2016; Bastings et al., 2017; Chen et al., 2017; Hashimoto and Tsuruoka, 2017; Li et al., 2017; Wu et al., 2018; Zhang et al., 2019). Eriguchi et al. (2016) proposed a syntax-aware encoding mechanism that encodes the source sentence maintaining the hierarchy of its dependency tree. A Long-Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to encode the constituent phrases recursively in bottom-up direction. On the contrary, Shi et al. (2016) claimed that an RNN encoder can capture the inherent syntactic properties automatically from a source sentence as a by-product of training. They used a multi-layer LSTM and found that its different layers represent different types of syntax. This work gives the"
2020.coling-main.378,P18-2049,0,0.0215536,"stically dropping merged characters. Note that BPE-dropout cannot obtain k-best candidates based on likelihood like P (x|X). Cherry et al. (2018) have shown that NMT that translates character sequences has achieved higher translation performance than word-based and subword-based NMT. However, they have mentioned that character-based NMT causes problems of modeling and computational time. We believe that our proposed method maintains balance between the advantages and disadvantages of character-based NMT (i.e., translation performance vs. modeling and computational cost). Ataman et al. (2017), Ataman and Federico (2018b), and Huck et al. (2017) have proposed linguisticbased subword segmentation algorithms. Ataman et al. (2017) and Ataman and Federico (2018b) have shown that their proposed “Linguistically Motivated Vocabulary Reduction (LMVR),” which is based on unsupervised morphology learning, outperforms BPE. Huck et al. (2017) have shown that incorporating linguistic knowledge, such as stemming and compound words, into subword segmentation improves NMT performance. Ataman and Federico (2018a) have further shown that compositional representations learned from character n-grams improve translation performa"
2020.coling-main.378,W18-1810,0,0.0176152,"stically dropping merged characters. Note that BPE-dropout cannot obtain k-best candidates based on likelihood like P (x|X). Cherry et al. (2018) have shown that NMT that translates character sequences has achieved higher translation performance than word-based and subword-based NMT. However, they have mentioned that character-based NMT causes problems of modeling and computational time. We believe that our proposed method maintains balance between the advantages and disadvantages of character-based NMT (i.e., translation performance vs. modeling and computational cost). Ataman et al. (2017), Ataman and Federico (2018b), and Huck et al. (2017) have proposed linguisticbased subword segmentation algorithms. Ataman et al. (2017) and Ataman and Federico (2018b) have shown that their proposed “Linguistically Motivated Vocabulary Reduction (LMVR),” which is based on unsupervised morphology learning, outperforms BPE. Huck et al. (2017) have shown that incorporating linguistic knowledge, such as stemming and compound words, into subword segmentation improves NMT performance. Ataman and Federico (2018a) have further shown that compositional representations learned from character n-grams improve translation performa"
2020.coling-main.378,D18-1461,0,0.0739323,"re, the training process for NMT needs to be modiﬁed to incorporate the method. In addition, a sufﬁciently large number of epochs is required to obtain this method’s effectiveness. In contrast, our proposed method does not require changing the NMT training process and does not need a large number of epochs. BPE-dropout (Provilkov et al., 2020) is a method that extends BPE to use subword regularization. In this method, multiple subword candidates are obtained by probabilistically dropping merged characters. Note that BPE-dropout cannot obtain k-best candidates based on likelihood like P (x|X). Cherry et al. (2018) have shown that NMT that translates character sequences has achieved higher translation performance than word-based and subword-based NMT. However, they have mentioned that character-based NMT causes problems of modeling and computational time. We believe that our proposed method maintains balance between the advantages and disadvantages of character-based NMT (i.e., translation performance vs. modeling and computational cost). Ataman et al. (2017), Ataman and Federico (2018b), and Huck et al. (2017) have proposed linguisticbased subword segmentation algorithms. Ataman et al. (2017) and Atama"
2020.coling-main.378,W17-4706,0,0.0615828,"rs. Note that BPE-dropout cannot obtain k-best candidates based on likelihood like P (x|X). Cherry et al. (2018) have shown that NMT that translates character sequences has achieved higher translation performance than word-based and subword-based NMT. However, they have mentioned that character-based NMT causes problems of modeling and computational time. We believe that our proposed method maintains balance between the advantages and disadvantages of character-based NMT (i.e., translation performance vs. modeling and computational cost). Ataman et al. (2017), Ataman and Federico (2018b), and Huck et al. (2017) have proposed linguisticbased subword segmentation algorithms. Ataman et al. (2017) and Ataman and Federico (2018b) have shown that their proposed “Linguistically Motivated Vocabulary Reduction (LMVR),” which is based on unsupervised morphology learning, outperforms BPE. Huck et al. (2017) have shown that incorporating linguistic knowledge, such as stemming and compound words, into subword segmentation improves NMT performance. Ataman and Federico (2018a) have further shown that compositional representations learned from character n-grams improve translation performance for morphologically-ri"
2020.coling-main.378,W04-3250,0,0.578892,"Missing"
2020.coling-main.378,D18-2012,0,0.0472157,"ntil they exceed the given vocabulary size. BPE is widely used in many NMT systems; however, since BPE is a greedy and deterministic algorithm, obtaining multiple subword candidates is not possible. The unigram language model is a likelihood-based subword segmentation algorithm. Each subword occurrence probability is estimated by the EM algorithm. The unigram language model has a more complicated algorithm than BPE, but it has the advantages that it can obtain multiple subword candidates based on likelihood and that it can be learned from raw sentences without pre-tokenization. SentencePiece (Kudo and Richardson, 2018) is an implementation of the unigram language model we used. Subword regularization (Kudo, 2018) is an NMT training method that uses multiple subword candidates obtained by the unigram language model and maximizes the marginal likelihood of sampled multiple subword candidates. This method requires on-the-ﬂy subword sampling in training; therefore, the training process for NMT needs to be modiﬁed to incorporate the method. In addition, a sufﬁciently large number of epochs is required to obtain this method’s effectiveness. In contrast, our proposed method does not require changing the NMT traini"
2020.coling-main.378,P18-1007,0,0.166217,"er Excerpt Corpus (ASPEC) English-to-Japanese and Japanese-to-English translation tasks and WMT14 English-to-German and German-to-English translation tasks show that our bilingual subword segmentation improves the performance of Transformer neural machine translation (up to +0.81 BLEU). 1 Introduction Subword units have recently been widely used in neural machine translation (NMT) to solve open vocabulary problems. Byte Pair Encoding (BPE) (Sennrich et al., 2016) is a dominant subword segmentation method for NMT, but it is designed for segmented languages in which words are divided by spaces. Kudo (2018) has proposed a subword segmentation method based on a unigram language model, that can be applied to non-segmented languages such as Chinese and Japanese. Both BPE and the unigram language model tokenize sentences by minimizing the number of segments under a limitation on subword vocabulary size, which relies on a data compression principle. In these existing segmentations, a sentence is segmented without considering its translation, and therefore the segmented sentence might not be optimal for NMT. This paper proposes a new subword segmentation method for NMT, “Bilingual Subword Segmentation"
2020.coling-main.378,P02-1040,0,0.108274,"as set to 0.1, and the batch size was set to 256 sentences. In subword regularization, we used 1-best decoding, which translates a segment sequence with the highest score of the unigram language model for a fair comparison with our proposed method because an NMT model with our segmentation method translates one segmented sequence. 4.2 Results Table 1 shows our experimental results: “Unigram LM,” “Subword Regularization,” and “BiSW” indicate NMT models using the unigram language model, subword regularization, and our proposed method, respectively. Translation performance was evaluated by BLEU (Papineni et al., 2002). We followed WAT Automatic Evaluation Systems5 . The statistical signiﬁcance test was performed by paired bootstrap re2 https://github.com/google/sentencepiece http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 4 http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2019/baseline/dataPreparationJE.html 5 http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index.html#automatic_evaluation_ systems.html 3 4291 Ja En Precision 97.05 98.82 Recall 97.44 99.22 F-measure 97.24 99.02 BiSW Oracle Ja-En 29.39 29.49 En-Ja 43.29 43.49 (a) Segmentation performance of the character-based BiLSTM (b) Comparison with the translation using go"
2020.coling-main.378,2020.acl-main.170,0,0.0139625,"regularization (Kudo, 2018) is an NMT training method that uses multiple subword candidates obtained by the unigram language model and maximizes the marginal likelihood of sampled multiple subword candidates. This method requires on-the-ﬂy subword sampling in training; therefore, the training process for NMT needs to be modiﬁed to incorporate the method. In addition, a sufﬁciently large number of epochs is required to obtain this method’s effectiveness. In contrast, our proposed method does not require changing the NMT training process and does not need a large number of epochs. BPE-dropout (Provilkov et al., 2020) is a method that extends BPE to use subword regularization. In this method, multiple subword candidates are obtained by probabilistically dropping merged characters. Note that BPE-dropout cannot obtain k-best candidates based on likelihood like P (x|X). Cherry et al. (2018) have shown that NMT that translates character sequences has achieved higher translation performance than word-based and subword-based NMT. However, they have mentioned that character-based NMT causes problems of modeling and computational time. We believe that our proposed method maintains balance between the advantages an"
2020.coling-main.378,P16-1162,0,0.0818193,"by using subword units induced from bilingual sentences; this method could be more favorable to machine translation. Evaluations on WAT Asian Scientiﬁc Paper Excerpt Corpus (ASPEC) English-to-Japanese and Japanese-to-English translation tasks and WMT14 English-to-German and German-to-English translation tasks show that our bilingual subword segmentation improves the performance of Transformer neural machine translation (up to +0.81 BLEU). 1 Introduction Subword units have recently been widely used in neural machine translation (NMT) to solve open vocabulary problems. Byte Pair Encoding (BPE) (Sennrich et al., 2016) is a dominant subword segmentation method for NMT, but it is designed for segmented languages in which words are divided by spaces. Kudo (2018) has proposed a subword segmentation method based on a unigram language model, that can be applied to non-segmented languages such as Chinese and Japanese. Both BPE and the unigram language model tokenize sentences by minimizing the number of segments under a limitation on subword vocabulary size, which relies on a data compression principle. In these existing segmentations, a sentence is segmented without considering its translation, and therefore the"
2020.findings-emnlp.371,abdelali-etal-2014-amara,0,0.0286749,"erence agreement. Following previous studies, newstest 2016 was used to evaluate the en-ro language pair. For fr-ro, we sampled 5K sentence pairs from OPUS (Tiedemann, 2012) for evaluation, while for zh-ro, we use the religious and educational parallel data for out-of-domain evaluation and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we used GlobalVoices2 , OpenSubtitles (Lison and Tiedemann, 2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennrich et al. (2016), we removed diacritics from ro. For zh, to avoid confusion between Hong Kong Standard Traditional Chinese (zh hk: QED), Taiwan Standard Traditional Chinese"
2020.findings-emnlp.371,N19-1388,0,0.0312615,"2005)). PBSMT + NMT: (Lample et al., 2018b), XLM: (Conneau and Lample, 2019), MASS: (Song et al., 2019). In the form x[y], x and y respectively indicate results on in-domain and out-of-domain sets. Note, the BLEU used in ro→zh is based on Chinese words segmented by the jieba toolkit. en-ro fr-ro zh-ro en-fr-ro en-zh-ro en fr ro zh 6.5 / 64.3 6.9 / 60.1 7.4 / 53.8 4.1 / 68.7 4.2 / 68.4 - 4.9 / 68.3 4.9 / 68.5 5.3 / 65.8 5.0 / 68.1 5.5 / 64.9 11.5 / 52.9 11.4 / 53.4 Table 2: Perplexity / Accuracy for masked language modeling in different languages joint pre-training. UNMT Lample et al. (2018a); Aharoni et al. (2019); Song et al. (2019) have demonstrated the importance of pre-training, which is a key ingredient of UNMT. Conneau and Lample (2019) used masked language modeling (MLM) to pretrain the full model for the initialization step before applying a denoising autoencoder and BT training step. Therefore, we take the XLM architecture proposed by Conneau and Lample (2019) as our backbone baseline model. MUNMT Our method studies the impact of adding a reference language to the existing UNMT language pair, which makes our model essentially multilingual. Therefore, MUNMT is the baseline for comparison. We ad"
2020.findings-emnlp.371,N19-1121,0,0.059622,"irs, including unseen language pairs, transfer learning should be considered when low-resource languages are trained together with rich-resource ones. As discussed by Arivazhagan et al. (2019), MUNMT usually performs worse than pivot-based supervised NMT; however, the pivot-based method easily experiences a computationally expensive quadratic growth in the number of source languages and suffers from the error propagation problem. Arivazhagan et al. (2019) addressed the zeroshot generalization problem that some translation directions have not been optimized well due to a lack of parallel data. Al-Shedivat and Parikh (2019) introduced a consistent agreement-based training method that encourages the model to produce equivalent translations of parallel sentences in zero-shot translation, which share similarities with our RAT approach. However, in terms of a specific implementation, because of the differences between UNMT and NMT, we have provided three new UNMT methods, and have alleviated the problem of uncontrollable intermediate BT quality in UNMT. Arivazhagan et al. (2019) addressed the issue of transfer learning between language pairs with parallel data where there is a lack of parallel corpora in multilingua"
2020.findings-emnlp.371,P17-1042,0,0.0856606,"Missing"
2020.findings-emnlp.371,1981.tc-1.7,0,0.709837,"Missing"
2020.findings-emnlp.371,C18-1233,1,0.900017,"Missing"
2020.findings-emnlp.371,P05-1066,0,0.0305915,"Missing"
2020.findings-emnlp.371,2020.findings-emnlp.283,0,0.202912,"ize is set to 60K, and the model hyperparameters are consistent with those of XLM. The smoothing value  in RAT is set to 0.1. 4.3 Main Results and Analysis This section examines the effectiveness of the proposed RUNMT framework6 . The main results7 are presented in Table 1. Row #4 reports the replicated results of the XLM architecture (Conneau and Lample, 2019) based on the training of each language pair individually. Our UNMT basically reproduces XLM’s results, and it also 6 Code available at https://github.com/ bcmi220/runmt. 7 Notably, concurrent works (Liu et al., 2020; Bai et al., 2020; Garcia et al., 2020) also explore the case of using auxiliary parallel data effects under the MUNMT setting, where all of these works share similarities in multilingualism motivation. Due to the inconsistency of the parallel corpora used, the results are not directly comparable, so we don’t include their results in the table. 4156 makes some improvements over the original (probably because of differences in data sampling). Thus, our approach offers a strong baseline performance. Compared with the current stateof-the-art method MASS (Song et al., 2019), our baseline performance is slightly lower. This is because M"
2020.findings-emnlp.371,P18-1192,1,0.906335,"Missing"
2020.findings-emnlp.371,D19-1080,0,0.0441941,"Missing"
2020.findings-emnlp.371,W17-3204,0,0.0440239,"and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we used GlobalVoices2 , OpenSubtitles (Lison and Tiedemann, 2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennrich et al. (2016), we removed diacritics from ro. For zh, to avoid confusion between Hong Kong Standard Traditional Chinese (zh hk: QED), Taiwan Standard Traditional Chinese (zh tw: Bibleuedin), and Simplified Chinese (zh: Tanzil and monolingual training data), we used opencc5 to convert zh hk and zh tw to simplified Chinese. 4.2 Baselines Our baseline models follow XLM (Conneau and Lample, 2019), with the following refinements: 4155 2 http://casmacat.eu/cor"
2020.findings-emnlp.371,J82-2005,0,0.623357,"Missing"
2020.findings-emnlp.371,P19-1017,0,0.0906893,"ich share similarities with our RAT approach. However, in terms of a specific implementation, because of the differences between UNMT and NMT, we have provided three new UNMT methods, and have alleviated the problem of uncontrollable intermediate BT quality in UNMT. Arivazhagan et al. (2019) addressed the issue of transfer learning between language pairs with parallel data where there is a lack of parallel corpora in multilingual supervised NMT. As for the agreement in UNMT, (Sun et al., 2019) investigate the enhancement of unsupervised bilingual word embedding agreement in the UNMT training. Leng et al. (2019) propose a multi-hop UNMT that automatically selects a good translation path for a distant language pair during UNMT. Baijun et al. (2019) proposed a cross-lingual pre-training approach that makes use of the source–pivot data to pre-train the language model. As for the multilingualism, Liu et al. (2020) proposes a multilingual denoising pre-training technique to improve machine translation tasks. Bai et al. (2020) and Garcia et al. (2020) both studied the agreement across language pairs. Their method is much the same as one of our proposed approaches, XBT, which relies on the supervision signa"
2020.findings-emnlp.371,D18-1262,1,0.884281,"Missing"
2020.findings-emnlp.371,L16-1147,0,0.020699,"language pair parallel dataset is about 10M. In both scenarios, we evaluated each language pair except for en-fr and en-zh, for which the relevant parallel data was used for reference agreement. Following previous studies, newstest 2016 was used to evaluate the en-ro language pair. For fr-ro, we sampled 5K sentence pairs from OPUS (Tiedemann, 2012) for evaluation, while for zh-ro, we use the religious and educational parallel data for out-of-domain evaluation and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we used GlobalVoices2 , OpenSubtitles (Lison and Tiedemann, 2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennr"
2020.findings-emnlp.371,2020.tacl-1.47,0,0.167936,"the byte pair encoding (BPE) code size is set to 60K, and the model hyperparameters are consistent with those of XLM. The smoothing value  in RAT is set to 0.1. 4.3 Main Results and Analysis This section examines the effectiveness of the proposed RUNMT framework6 . The main results7 are presented in Table 1. Row #4 reports the replicated results of the XLM architecture (Conneau and Lample, 2019) based on the training of each language pair individually. Our UNMT basically reproduces XLM’s results, and it also 6 Code available at https://github.com/ bcmi220/runmt. 7 Notably, concurrent works (Liu et al., 2020; Bai et al., 2020; Garcia et al., 2020) also explore the case of using auxiliary parallel data effects under the MUNMT setting, where all of these works share similarities in multilingualism motivation. Due to the inconsistency of the parallel corpora used, the results are not directly comparable, so we don’t include their results in the table. 4156 makes some improvements over the original (probably because of differences in data sampling). Thus, our approach offers a strong baseline performance. Compared with the current stateof-the-art method MASS (Song et al., 2019), our baseline performa"
2020.findings-emnlp.371,N09-2056,1,0.895514,"Missing"
2020.findings-emnlp.371,D18-1549,0,0.061576,"benchmarks has achieved great success (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) because of advances in deep learning and the availability of large-scale parallel corpora; however, the applicability of MT systems is limited because of their reliance on large parallel corpora for the majority of language pairs. In real-world situations, the majority of language pairs have very little parallel data, although large volumes of monolingual data are available for each language. UNMT removes the dependence on parallel corpora, relying only on monolingual corpora in each language (Reddi et al., 2018; Lample et al., 2018a,b; Conneau and Lample, 2019; Li et al., 2019b). UNMT uses translation symmetry for dual learning in each language direction. Existing UNMT models are mainly built on the encoder– decoder schema. The essence of UNMT is to 4151 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4151–4162 c November 16 - 20, 2020. 2020 Association for Computational Linguistics learn unsupervised cross-lingual word alignment and/or sentence alignment. For unsupervised word alignment, the most popular methods are word embedding mapping (Conneau et al., 2017; Lample e"
2020.findings-emnlp.371,W16-2323,0,0.058197,"2016), and MultiParaCrawl3 , whereas for zh-ro, Bible-uedin (Christodouloupoulos and Steedman, 2015), Tanzil, and the QCRI Educational Domain Corpus (QED) (Abdelali et al., 2014) were used. Because these parallel corpora between zh-ro are in religious and educational domains only, which are far away from the news domain of training data, we also collected a parallel corpus (2K in size) of zh-ro for in-domain evaluation. The Moses scripts (Koehn and Knowles, 2017) were used for tokenization of en, fr, and ro, and the jieba toolkit4 was used for word segmentation on zh. In particular, following Sennrich et al. (2016), we removed diacritics from ro. For zh, to avoid confusion between Hong Kong Standard Traditional Chinese (zh hk: QED), Taiwan Standard Traditional Chinese (zh tw: Bibleuedin), and Simplified Chinese (zh: Tanzil and monolingual training data), we used opencc5 to convert zh hk and zh tw to simplified Chinese. 4.2 Baselines Our baseline models follow XLM (Conneau and Lample, 2019), with the following refinements: 4155 2 http://casmacat.eu/corpus/global-voices.html http://paracrawl.eu 4 https://github.com/fxsjy/jieba 5 https://github.com/BYVoid/OpenCC 3 en-fr-ro en-zh-ro en→ro ro→en fr→ro ro→fr"
2020.findings-emnlp.371,P19-1119,1,0.807626,"Missing"
2020.findings-emnlp.371,2020.acl-main.324,1,0.653027,"ation models (as the generation direction is the same as the training direction), but also train the BT models, i.e., T → S and T → R. This gives the RABT training approach shown in Figure 2(c). The learning objective of RABT can be described as: LRABT (S, T , R) = L(θT →S ) + L(θT →R ). 3.4 (8) Cross-lingual Back-translation The traditional BT analyzed in Section 2 and illustrated in Figure 2(a) allows us to train a T → S model with the help of an S → T model, and vice versa; however, this mutually beneficial training is performed entirely within one language pair. Multilingual UNMT (MUNMT) (Sun et al., 2020) is a special case of UNMT that is capable of translating between multiple source and target languages. Although multiple language pairs are trained jointly in MUNMT, there is an obvious shortcoming for BT: translating between language pairs that do not occur together during training, i.e., lack of optimization across language pairs. Joint training across language pairs can be performed through forced high-order BT in UNMT, which takes the form L1 → L2 → ... → LO+1 → L1 , where O is the translation order indicating the number of bridge languages in BT. This approach may fail because decoding t"
2020.findings-emnlp.371,N07-1061,1,0.653675,"Missing"
2020.findings-emnlp.371,P07-1108,0,0.24056,"Missing"
2020.findings-emnlp.371,P19-1230,1,0.787194,"Missing"
2020.findings-emnlp.371,L16-1561,0,0.0615968,"olingual sentences as those extracted from the WMT News Crawl datasets for the period 2007–2017 by Conneau and Lample (2019) for a fair comparison and limited the maximum number of sentences in each language to 50 million(M), which results in 50M, 50M, and 14M sentences, respectively. For Chinese, we combined all of the sentences available in the WMT News Crawl datasets with the source sentences from the WMT’17 Chinese–English translation task, leading to 26M sentences. For the parallel data of en-fr and en-zh introduced by the two experimental settings, we only use those provided by MultiUN (Ziemski et al., 2016). Finally, the size of the resulting language pair parallel dataset is about 10M. In both scenarios, we evaluated each language pair except for en-fr and en-zh, for which the relevant parallel data was used for reference agreement. Following previous studies, newstest 2016 was used to evaluate the en-ro language pair. For fr-ro, we sampled 5K sentence pairs from OPUS (Tiedemann, 2012) for evaluation, while for zh-ro, we use the religious and educational parallel data for out-of-domain evaluation and collected 2K news parallel sentences for in-domain evaluation. In detail, as data for fr-ro, we"
2020.findings-emnlp.371,tiedemann-2012-parallel,0,\N,Missing
2020.lrec-1.364,D14-1179,0,0.0409666,"Missing"
2020.lrec-1.364,W16-2711,1,0.828506,"on the character/grapheme level rather than the word/phrase level, with no (or few) reordering operations. The technical background has been well established in the field of NLP. A PBSMT system (Koehn et al., 2003) can be used as an off-the-shelf tool once adequate data are provided. In recent years, NNbased frameworks such as the LSTM-RNN (Cho et al., 2014) have been widely applied to many NLP tasks. The Transformer model (Vaswani et al., 2017), which introduces a self-attention mechanism, is a state-of-the-art NN architecture in the NLP field. Regarding specific studies on transliteration, Finch et al. (2016) proposed an agreement model of bidirectional RNN, which outperformed PBSMT on various language pairs. Wu and Yarowsky (2018) compared several machine translation methods for the transliteration of 591 languages into English. Their conclusion was that a PBSMT system outperformed other systems including NN-based approaches. Regarding the case of Myanmar processing, there are few previous works. Ding et al. (2017) first attempted a Myanmar name Romanization task, where NN-based approaches did not outperform the traditional approaches of the conditional random field and support vector machine. It"
2020.lrec-1.364,P17-4012,0,0.0161047,"n this study, we focus on transliteration between Myanmar (Burmese) and English. To facilitate the application of data-driven approaches, we manually collected a dictionary containing more than eighty thousand MyanmarEnglish transliteration instances. The data have been released under a CC BY-NC-SA license for research purposes. 1 Based on the dictionary, we conducted experiments on automatic transliteration between Myanmar and English. Specifically, we conducted experiments using two neural network (NN)-based approaches: the Transformer model using the OpenNMT system 2 (Vaswani et al., 2017; Klein et al., 2017) and a joint agreement bidirectional long short-term memory (LSTM)-based recurrent NN (RNN) using the JANUS3 tool (Liu et al., 2016). A traditional phrase-based statistical machine translation (PBSMT) system using the Moses4 toolkit (Koehn et al., 2007) was set as a baseline. The experimental results were evaluated using the BLEU score (Papineni et al. 2002) on the character level. The experimental approaches performed well on transliteration tasks. The NN-based approaches outperformed the traditional PBSMT by large gains. The effect of using units at different granularities in the Myanmar scr"
2020.lrec-1.364,N03-1017,0,0.0396493,"ded and future work is presented. 2. Related Work Many Asian languages apply special writing systems, and efforts have been made on transliteration processing for major languages such as Chinese, Japanese, and Korean (Merhav and Ash, 2018). However, studies are required on understudied languages with limited resources. Generally, the transliteration task can be modeled as a simplified translation task on the character/grapheme level rather than the word/phrase level, with no (or few) reordering operations. The technical background has been well established in the field of NLP. A PBSMT system (Koehn et al., 2003) can be used as an off-the-shelf tool once adequate data are provided. In recent years, NNbased frameworks such as the LSTM-RNN (Cho et al., 2014) have been widely applied to many NLP tasks. The Transformer model (Vaswani et al., 2017), which introduces a self-attention mechanism, is a state-of-the-art NN architecture in the NLP field. Regarding specific studies on transliteration, Finch et al. (2016) proposed an agreement model of bidirectional RNN, which outperformed PBSMT on various language pairs. Wu and Yarowsky (2018) compared several machine translation methods for the transliteration o"
2020.lrec-1.364,P07-2045,0,0.0859893,"he data have been released under a CC BY-NC-SA license for research purposes. 1 Based on the dictionary, we conducted experiments on automatic transliteration between Myanmar and English. Specifically, we conducted experiments using two neural network (NN)-based approaches: the Transformer model using the OpenNMT system 2 (Vaswani et al., 2017; Klein et al., 2017) and a joint agreement bidirectional long short-term memory (LSTM)-based recurrent NN (RNN) using the JANUS3 tool (Liu et al., 2016). A traditional phrase-based statistical machine translation (PBSMT) system using the Moses4 toolkit (Koehn et al., 2007) was set as a baseline. The experimental results were evaluated using the BLEU score (Papineni et al. 2002) on the character level. The experimental approaches performed well on transliteration tasks. The NN-based approaches outperformed the traditional PBSMT by large gains. The effect of using units at different granularities in the Myanmar script was also investigated. To the best of our knowledge, this study is the first systematic work on the topic of Myanmar-English transliteration driven by a relatively large-scale dataset. * Corresponding author http://www2.nict.go.jp/astrec-att/member/"
2020.lrec-1.364,J03-1002,0,0.0365005,"uly.” Dictionary To collect the transliteration instances, we began with two English-Myanmar parallel corpora, and then moved to resources on the Internet to enlarge the scale of the data. Specifically, we used the ALT corpus (Riza et al., 2016; Ding et al., 2018; Ding et al., 2019; Ding et al., 2020)5 and UCSY corpus (Sin et al., 2018). 6 The ALT corpus consists of twenty thousand parallel sentences from news articles and the UCSY corpus contains two hundred thousand parallel sentences collected from different domains, including local news articles and textbooks. We used the GIZA++ toolkit7 (Och and Ney, 2003) to obtain the raw alignments between the source and target language, based on which the transliteration instances were filtered. From the Internet, we further collected instances of places, organizations, and person names. The dictionary was encoded in Unicode. For experimental investigation, we divided it into three parts by taking the first one thousand lines as the test data and the final one thousand lines as the development data. Statistics for the data are listed in Table 1, where #Char. denotes Unicode characters, #Syl. denotes syllables, and #Sub-Syl. denotes the sub-syllable units pr"
2020.lrec-1.364,P03-1021,0,0.0592599,",884,108 13,604 14,253 Table 1: Statistics for the data. Char. Sub-Syl. Syl. <ဆ><ွွ><စ><ွ><ဇ><ွာ><လ><န><ွ> <ဆ><ွွစ><ဇ><ွာ><လ><န> <ဆွစ><ဇာ><လန> Table 2: Different units in Myanmar processing. 4. Experiment The Myanmar data were segmented into different units, as shown in Table 2, and all English data were lowercased. Moses was used to train the baseline PBSMT system. Character alignment was generated using GIZA++ and symmetrized using the grow-diag-final-and heuristics. A 5-gram language model on character was trained using the SRILM toolkit (Stolcke, 2002).8 Minimum error rate training (Och, 2003) was used to tune the weight of the features. All other parameters were adapted using the default setting of Moses. Regarding the JANUS toolkit, which implements a bidirectional agreement LSTM-RNN model, the hyperparameters followed the original paper (Liu et 5 http://www2.nict.go.jp/astrecatt/member/mutiyama/ALT/index.html 6 http://lotus.kuee.kyoto-u.ac.jp/WAT/my-en-data/ 7 http://www.statmt.org/moses/giza/GIZA++.html 8 http://www.speech.sri.com/projects/srilm/ 2981 al., 2016). Specifically, the settings were 500 for embedding, 500 for the hidden unit dimensions, and 16 for the batch size. Ad"
2020.lrec-1.364,P02-1040,0,0.107849,"we conducted experiments on automatic transliteration between Myanmar and English. Specifically, we conducted experiments using two neural network (NN)-based approaches: the Transformer model using the OpenNMT system 2 (Vaswani et al., 2017; Klein et al., 2017) and a joint agreement bidirectional long short-term memory (LSTM)-based recurrent NN (RNN) using the JANUS3 tool (Liu et al., 2016). A traditional phrase-based statistical machine translation (PBSMT) system using the Moses4 toolkit (Koehn et al., 2007) was set as a baseline. The experimental results were evaluated using the BLEU score (Papineni et al. 2002) on the character level. The experimental approaches performed well on transliteration tasks. The NN-based approaches outperformed the traditional PBSMT by large gains. The effect of using units at different granularities in the Myanmar script was also investigated. To the best of our knowledge, this study is the first systematic work on the topic of Myanmar-English transliteration driven by a relatively large-scale dataset. * Corresponding author http://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/my-e n-transliteration.zip 2 http://opennmt.net/OpenNMT-py/ 3 https://github.com/lemaoliu/Agta"
2020.lrec-1.364,L18-1150,0,0.0146687,"background has been well established in the field of NLP. A PBSMT system (Koehn et al., 2003) can be used as an off-the-shelf tool once adequate data are provided. In recent years, NNbased frameworks such as the LSTM-RNN (Cho et al., 2014) have been widely applied to many NLP tasks. The Transformer model (Vaswani et al., 2017), which introduces a self-attention mechanism, is a state-of-the-art NN architecture in the NLP field. Regarding specific studies on transliteration, Finch et al. (2016) proposed an agreement model of bidirectional RNN, which outperformed PBSMT on various language pairs. Wu and Yarowsky (2018) compared several machine translation methods for the transliteration of 591 languages into English. Their conclusion was that a PBSMT system outperformed other systems including NN-based approaches. Regarding the case of Myanmar processing, there are few previous works. Ding et al. (2017) first attempted a Myanmar name Romanization task, where NN-based approaches did not outperform the traditional approaches of the conditional random field and support vector machine. It can be considered that the transliteration task may be sensitive to the quality and quantity of training data, in addition t"
2020.wmt-1.22,D18-1399,0,0.0372296,"Missing"
2020.wmt-1.22,N18-1118,0,0.0256328,"al., 2017), sentence-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky, 2000) elements were ignored. This results in that the translations may be perfect at the sentence-level but lack crucial properties of the text, hindering understanding (Maruf et al., 2019). To help to resolve ambiguities and inconsistencies in translations, some MT pioneers (Bar-Hillel, 1960; Xiong et al., 2013; Sennrich, 2018) exploit the underlying discourse structure information of a text to address this issue, while others (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019; Wang et al., 2019; Scherrer et al., 2019) extend the translation units with the context or use an additional context encoder and attention. It is worth noting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and tractability concerns, the document-enhanced NMT models mostly used document embedding, document topic information, and limited past or fu"
2020.wmt-1.22,P18-1192,1,0.83718,"019) is adopted. In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT mo"
2020.wmt-1.22,D19-5603,1,0.814693,"rmance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the n-best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output. 2 2.1 Methodology XLM-enhanced NMT Pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLM (Conneau et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) etc. have recently demonstrated a very dominant effect on natural language processing tasks. Several works (Clinchant et al., 2019; Imamura and Sumita, 2019; Zhu et al., 2020) leveraged a pre-trained BERT model for improving NMT and found that BERT can bring significantly better results over the baseline. Since BERT and other pre-trained language models are trained on large scale corpus beyond the data provided by the WMT20 organizers, the direct use of BERT will make the system submitted unconstrained. Using an XLM model, a variant of BERT, pre-trained from scratch on the monolingual data provided by the official to enhance our NMT model, is a good choice to keep the system constrained. Moreover, the XLM model has the advantages of simple traini"
2020.wmt-1.22,P19-4007,0,0.0555909,"Missing"
2020.wmt-1.22,J82-2005,0,0.698509,"Missing"
2020.wmt-1.22,P19-1285,0,0.0196835,"ting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and tractability concerns, the document-enhanced NMT models mostly used document embedding, document topic information, and limited past or future context sentences, etc., rather than the truly whole document information. Recently, with the increase in computational power available to us and the well-designed neural network structures (Dai et al., 2019; Kitaev et al., 2019; Beltagy et al., 2020) for long sequence encoding, we are finally in a position to employ the whole document information for enhancing sentence-level NMT. In addition, we argue that since long sequences encoding is easier than decoding, truly whole document-level translation is still a long way off, since the bidirectional context is available in the encoder, but only the past is visible by the decoder. Longformer To make the long documents processed with Transformer (Vaswani et al., 2017) architecture feasible or easier, a modified Transformer architecture named Longform"
2020.wmt-1.22,P19-1120,0,0.0384069,"Missing"
2020.wmt-1.22,N19-1423,0,0.0103748,"the TF-IDF algorithm is employed to filter the training set according to the input of the test set, a training subset whose domain is more similar to the test set is obtained, and then used to finetune the model for reducing the performance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the n-best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output. 2 2.1 Methodology XLM-enhanced NMT Pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLM (Conneau et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) etc. have recently demonstrated a very dominant effect on natural language processing tasks. Several works (Clinchant et al., 2019; Imamura and Sumita, 2019; Zhu et al., 2020) leveraged a pre-trained BERT model for improving NMT and found that BERT can bring significantly better results over the baseline. Since BERT and other pre-trained language models are trained on large scale corpus beyond the data provided by the WMT20 organizers, the direct use of BERT will make the system submitted unconstrained. Using an"
2020.wmt-1.22,J93-1004,0,0.207938,"ull model with the obtained model. The PLM-encoder attention attnP and PLM-decoder attention attnPC are randomly initialized. EN-PL On the language pair EN-PL, we explored performance in two training data settings. The first is base data, including Europarl v10, Tilde Rapid corpus, and WikiMatrix bitext data, whose raw data is on the sentence-level. In the second setting base data + paracrawl, we converted the paragraph-level alignment data in Paracrawl to sentence-level alignment and incorporated it with the base data. In the conversion process, we adopted the method and program proposed by (Gale and Church, 1993) for aligning sentences based on a simple statistical model of character lengths, which uses the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter 224 Systems Transformer big +D2GPo XLM-enhanced Document-enhanced Ensemble ++TF-IDF finetune ++Re-ranking 19test Test BLEU BLEU chrF 37.2 37.7 38.9 39.2 40.0 40.2 40.5 48.6 48.8 49.1 0.418 0.422 0.427 (NSP) classification model provided by Google for document interval prediction to recover the documents. DE-HSB In RUNMT on EN-DE-"
2020.wmt-1.22,P07-2045,0,0.0121171,"s the adjustable parameters. ∀t ∈ Dterms ], (12) where Dterms indicates the all terms set in corpus D. We calculate the cosine similarity as final scores between the query and every source sentence in corpus, and ranked on the scores to get the topK pairs (K=1000 in our experiments) as the subtraining set for finetuning. 3 Data Preprocessing and Model Setup Before model training, we preprocessed the data uniformly and customized the processing according to the requirements of each model. We normalized punctuation, remove non-printing characters, and tokenize all data with the Moses tokenizer (Koehn et al., 2007) except for the Chinese. For Chinese, we removed the segmentation space in some training data and then use PKUSeg (Luo et al., 2019) toolkit to cut all Chinese sentences, so as to obtain unified word segmentation annotations. We use joint byte pair encodings (BPE) with 40K split operations for subword segmentation (Sennrich et al., 2016). In XLM-enhanced NMT and Documentenhanced NMT, we first train a basic NMT (Transformer big) model on the sentence-level data until convergence, then initialize the encoder and decoder of the XLM-enhanced NMT and Document-enhanced NMT full model with the obtain"
2020.wmt-1.22,C18-1271,1,0.818947,"In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from"
2020.wmt-1.22,N19-4009,0,0.0819852,"(U1836222 and 61733011), Huawei-SJTU Long Term AI Project, Cuttingedge Machine Reading Comprehension and Language Model. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenuretrack researcher startup fund “Toward Intelligent Machine Translation”. (DE) ↔ Upper Sorbian (HSB) both directions are focused. Our baseline system in supervised track is based on the Transformer big architecture proposed by Vaswani et al. (2017), in which its opensource implementation version Fairseq (Ott et al., 2019) is adopted. In the unsupervised and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He e"
2020.wmt-1.22,N09-2056,1,0.713764,"eference agreement mechanism. Specifically, we proposed three kinds of reference agreement utilization approaches in (Li et al., 2020b): reference agreement translation (RAT), reference agreement back-translation (RABT), and cross-lingual back-translation (XBT). (6) LRABT (S, T , R) = L(θT →S ) + L(θT →R ). (9) XBT The parallel corpus between languages S and R can not only bring agreement in the translations of the same target language T , but also cross-lingual agreement, that is, using the target language as the bridge to form pivot translation (Wu and Wang, 2007; Utiyama and Isahara, 2007; Paul et al., 2009) patterns: S → T → R and R → T → S. In XBT, paired sentences s and r are translated to language T : t˜s and t˜r , and forms two new pseudo-parallel pairs: ht˜s , ri and ht˜r , si, which promote the training of translation T → R and T → S. The objective function of XBT is: RAT RAT utilizes the principle for translating paired sentences into the target language T of the source S and reference R language. Since the input the parallel, the both translation outputs should be the same. Given a parallel sentence pair hs, ri between language S and R, we would ideally have P(·|s; θS→T ) = P(·|r; θR→T )"
2020.wmt-1.22,N18-1202,0,0.00847445,"model training is finished, the TF-IDF algorithm is employed to filter the training set according to the input of the test set, a training subset whose domain is more similar to the test set is obtained, and then used to finetune the model for reducing the performance degradation caused by domain inconsistency. For the final submission, an ensemble of several different trained models outputs the n-best predictions, and used the decoder trained with Marian toolkit to performs reranking to get the final system output. 2 2.1 Methodology XLM-enhanced NMT Pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLM (Conneau et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et al., 2019) etc. have recently demonstrated a very dominant effect on natural language processing tasks. Several works (Clinchant et al., 2019; Imamura and Sumita, 2019; Zhu et al., 2020) leveraged a pre-trained BERT model for improving NMT and found that BERT can bring significantly better results over the baseline. Since BERT and other pre-trained language models are trained on large scale corpus beyond the data provided by the WMT20 organizers, the direct use of BERT will make the system submi"
2020.wmt-1.22,2020.findings-emnlp.371,1,0.64243,"Missing"
2020.wmt-1.22,2021.ccl-1.108,0,0.100832,"Missing"
2020.wmt-1.22,P12-3005,0,0.0427849,"gned to each proposed correspondence of sentences, based on the scaled difference of lengths of the two sentences (in characters) and the variance of this difference. This probabilistic score is used in a dynamic programming framework to find the maximum likelihood alignment of sentences. For the Polish pre-trained XLM language model, we used all NewsCrawl monolingual data and some CommonCrawl monolingual data. Since the CommonCrawl data is very large and noisy and can potentially decrease the performance of LM if it is used in its raw form. We apply language identification filtering (langid; Lui and Baldwin (2012)), keeping sentences with correct languages. In order to filter out the sentences shorter than 5 words or longer than 150 words more precisely, we re-split sentences using Spacy (Honnibal and Montani, 2017) toolkit. EN-ZH In EN-ZH, the pre-training of Longformer as a document encoder is unique. As described in (Beltagy et al., 2020), the Longformer needs a large number of gradient updates to learn the local context first; before learning to utilize longer context. In the first phase of the staged training procedure, an initial RoBERTa (Liu et al., 2019) model implemented in Fairseq (Ott et al."
2020.wmt-1.22,2020.acl-main.571,1,0.800756,", 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT (Zhu et al., 2020). Besides, we trained a bidirectional translation model of EN-"
2020.wmt-1.22,W18-6319,0,0.0144898,"uent phase, we trained the model on the paragraph text, doubled the window size and the sequence length, and halve the learning rate. For the paragraph text, the Wikidumps and NewsCommentary v15 have document intervals and can be used directly, while UN v1.0 has no document intervals but the sentence order is not interrupted. Therefore, we use the BERT Next Sentence Prediction Results and Analysis Results and ablations for PL→EN2 are shown in Table 1, EN→ZH in Table 2, unsupervised DE↔HSB in Table 3 and low-resource DE↔HSB in Table 4. We report case-sensitive SacreBLEU scores using SacreBLEU (Post, 2018) for EN-PL, DE-HSB, and BLEU based on characters for ENZH. In the results, “+” means addition based on baseline, and “++” means cumulative addition based on the previous one. In PL→EN, the introduction of ParaCrawl data improves the baseline performance on the dev dataset by about 4.2 BLEU. +D2GPo, XLM-enhanced NMT, Bidirectional NMT, and ensembling outperforms our strong baseline by 2 BLEU point. Finally, finetuning and reranking further gives another 0.5 BLEU. For EN→ZH, as with PL→EN, we see similar improvements with +D2GPo, XLM-enhanced NMT, ensembling and reranking. We also observe that t"
2020.wmt-1.22,D19-6506,0,0.0216909,"irections, the differences are also very obvious. To further expose the model to the direction difference and improve the effect of unidirectional translation, we further finetune the 220 bidirectional pre-trained model on the bilingual data. Take S2T translation as an example; the model is optimized as follows: L(θS→T ) = N X log p(y (n) |x(n) ), (5) n=1 where θS→T is the parameters of child model which is initialized with θparent . Similarly, the T2S child model can also be obtained. Due to the introduction of bidirectional translation in one model, follow the practice of Conneau and Lample (2019), shared subword vocabulary and shared encoder-decoder (source and target) embedding were employed to improves the alignment of embedding spaces across languages. In addition, since the encoder and decoder need to be able to handle two languages simultaneously, a language embedding was used to indicate the language being processed, so as to reduce confusion of the model. 2.3 Document-enhanced NMT In spite of its success (Vaswani et al., 2017), sentence-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky"
2020.wmt-1.22,P16-1162,0,0.0703717,"cessing and Model Setup Before model training, we preprocessed the data uniformly and customized the processing according to the requirements of each model. We normalized punctuation, remove non-printing characters, and tokenize all data with the Moses tokenizer (Koehn et al., 2007) except for the Chinese. For Chinese, we removed the segmentation space in some training data and then use PKUSeg (Luo et al., 2019) toolkit to cut all Chinese sentences, so as to obtain unified word segmentation annotations. We use joint byte pair encodings (BPE) with 40K split operations for subword segmentation (Sennrich et al., 2016). In XLM-enhanced NMT and Documentenhanced NMT, we first train a basic NMT (Transformer big) model on the sentence-level data until convergence, then initialize the encoder and decoder of the XLM-enhanced NMT and Document-enhanced NMT full model with the obtained model. The PLM-encoder attention attnP and PLM-decoder attention attnPC are randomly initialized. EN-PL On the language pair EN-PL, we explored performance in two training data settings. The first is base data, including Europarl v10, Tilde Rapid corpus, and WikiMatrix bitext data, whose raw data is on the sentence-level. In the secon"
2020.wmt-1.22,2021.naacl-main.311,1,0.626638,"Missing"
2020.wmt-1.22,P95-1026,0,0.836605,"e unlabeled dataset U = {x(j) }L j=1 is used for the synthesis of pseudo-parallel corpora. While in UNMT, since the model is trained with backtranslation on unpaired monolingual data, the pseudo-parallel corpora is synthesized by the monolingual data, i.e., U = {x(m) }M m=1 . Considering the translation quality can’t effectively be evaluated across languages in machine translation with only the monolingual data, therefore the selection of the subset Q, is one of the key factors for self-training. It is usually selected based on some confidence scores (e.g. log probability or perplexity, PPL) (Yarowsky, 1995), but it is also possible for S to be the whole pseudo parallel data (Zhu and Goldberg, 2009). In the backward translation based on the pseudo-parallel data, the DAE method widely used in UNMT can alleviate the impact of the noise resulted from the synthesized sentences on model training, since the synthesized sentences are only used as input. However, in the forward translation training, the quality of noisy targets will directly affect the success of the model training. Therefore, the selection of synthetic parallel corpus becomes particularly critical. fθT →S 5: Calculate BT-BLEU B for two"
2020.wmt-1.22,N07-1061,1,0.7001,"Missing"
2020.wmt-1.22,P18-1117,0,0.0215559,"-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky, 2000) elements were ignored. This results in that the translations may be perfect at the sentence-level but lack crucial properties of the text, hindering understanding (Maruf et al., 2019). To help to resolve ambiguities and inconsistencies in translations, some MT pioneers (Bar-Hillel, 1960; Xiong et al., 2013; Sennrich, 2018) exploit the underlying discourse structure information of a text to address this issue, while others (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019; Wang et al., 2019; Scherrer et al., 2019) extend the translation units with the context or use an additional context encoder and attention. It is worth noting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and tractability concerns, the document-enhanced NMT models mostly used document embedding, document topic information, and limited past or future context sentenc"
2020.wmt-1.22,P07-1108,0,0.0458195,"truction training of UNMT through a proposed reference agreement mechanism. Specifically, we proposed three kinds of reference agreement utilization approaches in (Li et al., 2020b): reference agreement translation (RAT), reference agreement back-translation (RABT), and cross-lingual back-translation (XBT). (6) LRABT (S, T , R) = L(θT →S ) + L(θT →R ). (9) XBT The parallel corpus between languages S and R can not only bring agreement in the translations of the same target language T , but also cross-lingual agreement, that is, using the target language as the bridge to form pivot translation (Wu and Wang, 2007; Utiyama and Isahara, 2007; Paul et al., 2009) patterns: S → T → R and R → T → S. In XBT, paired sentences s and r are translated to language T : t˜s and t˜r , and forms two new pseudo-parallel pairs: ht˜s , ri and ht˜r , si, which promote the training of translation T → R and T → S. The objective function of XBT is: RAT RAT utilizes the principle for translating paired sentences into the target language T of the source S and reference R language. Since the input the parallel, the both translation outputs should be the same. Given a parallel sentence pair hs, ri between language S and R, we w"
2020.wmt-1.22,P19-1298,1,0.820418,"successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT (Zhu et al., 2020)."
2020.wmt-1.22,D13-1163,0,0.0303897,"dicate the language being processed, so as to reduce confusion of the model. 2.3 Document-enhanced NMT In spite of its success (Vaswani et al., 2017), sentence-level NMT has been based on strong independence and locality assumptions generally, in which the interrelations among these discourse (Jurafsky, 2000) elements were ignored. This results in that the translations may be perfect at the sentence-level but lack crucial properties of the text, hindering understanding (Maruf et al., 2019). To help to resolve ambiguities and inconsistencies in translations, some MT pioneers (Bar-Hillel, 1960; Xiong et al., 2013; Sennrich, 2018) exploit the underlying discourse structure information of a text to address this issue, while others (Bawden et al., 2018; Voita et al., 2018; Jean and Cho, 2019; Wang et al., 2019; Scherrer et al., 2019) extend the translation units with the context or use an additional context encoder and attention. It is worth noting that the essence of the document-level NMT claimed with additional context and attention is still sentence-level MT, whose translation is still output sentence by sentence. We named it as documentenhanced NMT more precisely. Due to computational efficiency and"
2020.wmt-1.22,C18-1317,1,0.818967,"ed and low-resource track, we draw on the successful experience of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorpo"
2020.wmt-1.22,P19-1230,1,0.815888,"ce of the XLM framework (Conneau et al., 2019), and used the two-stage training mode of masked language modeling (MLM) pre-training + back-translation (BT) finetune to obtain a very strong baseline performance. Marian (JunczysDowmunt et al., 2018) toolkit is utilized for training the decoder in reranking using machine translation targets instead of common GPT-style language modeling targets. In order to better play the role of WMT evaluation in polishing the methods proposed or improved by our team (He et al., 2018; Li et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2018; Xiao et al., 2019; Zhou and Zhao, 2019; Li et al., 2019b; Luo and Zhao, 2020), we divided the three language pairs we participated in into three categories: 1. Traditional language pair with rich parallel corpus: EN-PL, 2. Language pair with document-level information: EN-ZH, 3. Language pair with no or low parallel resources: DE-HSB. In the supervised PL→EN translation direction, we based on the XLM framework to pre-train a Polish language model using common crawl and news crawl monolingual data, and proposed the XLM enhanced NMT model inspired from the idea of incorporating BERT into NMT (Zhu et al., 2020). Besides, we trained a"
2020.wmt-1.22,D16-1163,0,0.0162403,"t , where [0, pnet 2 ) is the probability of attending to L, the final sum for the first attn in HEL and HD pnet pnet [ 2 , 1 − 2 ) is the probability for the whole HEL L equation, [1 − pnet , 1] is the probability and HD 2 L. for the second attn in in HEL and HD 2.2 Bidirectional NMT Machine translation, in general, is unidirectional, that is, from the source language to the target language. The encoder-decoder framework for NMT has been shown effective in large data scenarios, and the more high-quality bilingual training data, the better performance the model tends to achieve. Recent works (Zoph et al., 2016; Kim et al., 2019) on translation transfer learning (Torrey and Shavlik, 2010; Pan and Yang, 2009) from rich-resource language pairs to low-resource language pairs demonstrate that translation has some universal nature in essence between different language pairs. As the sourceto-target (S2T) forward translation and target-tosource (T2S) backward translation can be seen as two special language pairs in bilingual translation, it can make use of the translation universal nature to improve each other, i.e., dual learning (He et al., 2016). Based on this motivation, we developed a bidirectional NM"
2021.emnlp-demo.1,D18-1045,0,0.01542,"ation of the translation y is always k tokens behind reading x; that is, at the t-th decoding step, we generate token yt based on x ≤ t − k + 1. We thus adopt a Transformerbased NMT model with the wait-k strategy, aiming for balance between translation performance and efficiency. 2.3 translation’ of a translated sequence back into its original language – is a potential method of generating reference sentences for comparison that utilizes the duality of direction in translation (He et al., 2016). Back-translation is currently mainly used as a data-enhancement method for supervised NMT systems (Edunov et al., 2018) and as a crucial training method for unsupervised NMT systems (Conneau and Lample, 2019), though it has been more controversial as a method of assessing translations. According to (Behr, 2017)’s conclusion, while back-translation can give some evaluation of the translation, it often raises issues not noted by human assessors, and more importantly, is less reliable in general, as many problems remain hidden. These shortcomings are mainly are a result of commonly used automatic evaluation methods (like BLEU) using only surface-level similarity; they do not strictly measure , Semantic Equivalenc"
2021.emnlp-demo.1,W19-4822,0,0.0146487,"tics as Google and Baidu have introduced simultaneous translation feature, due to the integration of simultaneous translation and whole-sentence translation, users cannot easily control whether the system uses simultaneous translation or whole-sentence translation, and the automated control of commercial systems sometimes does not follow the user’s wishes. Since user input errors are unavoidable for any human-computer interaction system, the quality of NMT system also has been shown to significantly degrade when confronted with source-side noise (Heigold et al., 2018; Belinkov and Bisk, 2018; Anastasopoulos, 2019). The previous grammatical error detection and correction work focused on computer-aided writing systems. Some existing computer-aided writing systems (Grammarly2 and Pigai3 , Write&Improve4 , and LinggleWrite5 ) detect and correct grammatical errors; however, systems such as these have had little attention when considered in the context of input error detection or correction for commercial machine translation systems, as their main focus is generally posttranslation editing. High quality domain specific machine translation systems are in high demand whereas general purpose MT has limited appl"
2021.emnlp-demo.1,D19-1435,0,0.0143281,"ain better translation sequences. We wanted to emphasize efficient inference, so we adopted a Transformer (Base) setting with fewer parameters. The training data used was the same as that in the multi-style NMT model. We formulated the GEC task as a sequence labeling problem and thus adopted a neural sequence tagging model to handle the task. We followed (Omelianchuk et al., 2020)’s model architecture, which was an encoder consisting of a pre-trained BERT-like transformer stacked with two linear layers with softmax layers on the top - one for error detection and one for error labeling. As in (Awasthi et al., 2019), the architecture uses an iterative correction strategy in which predicted transformations are applied to the input sequence successively. After errors are detected and predicted, a modified Levenshtein distance guides the generation of a corrected sentence. We limit the maximum number of inference iterations to 4 to speed up the overall correction process while still maintaining good correction accuracy. The training data we used for GEC is shown in Table 3. We trained our English GEC model at the word level and our Chinese and Japanese models at the character level. We used pre-trained lang"
2021.emnlp-demo.1,W18-1807,0,0.0687644,"Missing"
2021.emnlp-demo.1,P18-4024,1,0.847809,"apan charlee@sjtu.edu.cn, {mutiyama, eiichiro.sumita}@nict.go.jp, zhaohai@cs.sjtu.edu.cn Abstract and machine translation has correspondingly risen in popularity (Hutchins and Somers, 1992). Recently, Neural Machine Translation (NMT), especially Transformer-based NMT, has emerged as a promising approach with the potential to address many of the shortcomings of traditional rulebased or statistics-based machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). This has significantly improved the performance of machine translation and other related tasks (Huang et al., 2018; Li et al., 2018a,b). Although neural machine translation has made tremendous improvements and is relatively highperforming, because human language is so complex, machine translation is often still only used as an assistance tool rather than the sole entity responsible for translation. There are several popular and large existing commercial machine translation systems that provide users with effective translation (e.g., Google Translator, Bing Translator, Amazon Translate, and Baidu Translate). As NMT is still very imprecise, however, these web services fall short, as they do not provide suff"
2021.emnlp-demo.1,N12-1067,0,0.0355332,"016; Omelianchuk et al., 2020) and used precision (P), recall (R), and F0 .5 to evaluate our models on all three languages. We evaluated English at the word level and Chinese and Japanese at the character level. We chose the test set of the CoNLL-2014 shared task as our evaluation set for our English GEC model. For Chinese and Japanese, we extracted 5000 sentences from the original training set for the development set and 5000 sentences for the test set and used the rest as the training set. ERRANT6 was used to convert parallel files to the m2 format for subsequent scoring with the M2 Scorer (Dahlmeier and Ng, 2012). The results of our models for standard NMT and simultaneous NMT are shown in Table 4. First, for the evaluation results of standard NMT, we found that the joint training of multiple styles of data does not bring performance improvement compared to separate training, especially when the corpora sizes of the two styles are similar. The translation performance gap between different styles demonstrates that the level difficulty of translation in different styles is different. Since style essentially refers to deviation from standard textual norms, the greater the deviation, the greater the trans"
2021.emnlp-demo.1,P19-1289,0,0.0633091,"Missing"
2021.emnlp-demo.1,2016.amta-researchers.9,0,0.0252712,"I SS, users can get real-time translations while writing, flexible control in switching between real-time translation and whole-sentence translation, informative back-translation feedback and scoring, and input error detection and revision suggestions. In addition, the system also supports user interactions that modify the translations or inputs, which provides crowdsourced data for further improving the performance of our machine translation and grammatical error correction. Notably, there were also several interactive translation systems in the past, such as CASMACAT (Alabau et al., 2014), (Knowles and Koehn, 2016), (Peris et al., 2017), and INMT (Santy et al., 2019). The distinctions lie in the abilities of the systems and the features to adapt to the latest user needs. 2 The M I SS System There are 5 features in our M I SS translation system: simultaneous translation, back-translation for quality evaluation, grammatical error correction, multi-style translation, and crowdsourcing data collection. The system is available at http: //miss.x2brain.com/ until November 12, 2021. We show a screenshot of the system in Figure 1. In the following subsections, we will describe each component of the system. 2.1 B"
2021.emnlp-demo.1,W17-3204,0,0.0152215,"nd LinggleWrite5 ) detect and correct grammatical errors; however, systems such as these have had little attention when considered in the context of input error detection or correction for commercial machine translation systems, as their main focus is generally posttranslation editing. High quality domain specific machine translation systems are in high demand whereas general purpose MT has limited applications because different machine translation users want to generate translations that can be used in the scenario. On the one hand, general purpose translation systems usually perform poorly (Koehn and Knowles, 2017). On the other hand, appropriate translation is also a very important goal to pursue. There are two typical methods to achieve this goal. One is to use the domain adaptation method to obtain a domainspecific model from the existing general machine translation model through transfer learning. The other is to adopt an conditional translation decoder to integrate various domains into the same model and generate translations according to different input conditions (Keskar et al., 2019). At present, the commercial machine translation system mainly adopts the former one, but it also brings the addit"
2021.emnlp-demo.1,D18-1512,0,0.0469907,"Missing"
2021.emnlp-demo.1,C18-1271,1,0.743867,"u.cn, {mutiyama, eiichiro.sumita}@nict.go.jp, zhaohai@cs.sjtu.edu.cn Abstract and machine translation has correspondingly risen in popularity (Hutchins and Somers, 1992). Recently, Neural Machine Translation (NMT), especially Transformer-based NMT, has emerged as a promising approach with the potential to address many of the shortcomings of traditional rulebased or statistics-based machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). This has significantly improved the performance of machine translation and other related tasks (Huang et al., 2018; Li et al., 2018a,b). Although neural machine translation has made tremendous improvements and is relatively highperforming, because human language is so complex, machine translation is often still only used as an assistance tool rather than the sole entity responsible for translation. There are several popular and large existing commercial machine translation systems that provide users with effective translation (e.g., Google Translator, Bing Translator, Amazon Translate, and Baidu Translate). As NMT is still very imprecise, however, these web services fall short, as they do not provide sufficient informatio"
2021.emnlp-demo.1,2020.bea-1.16,0,0.389675,"NMT model, making this decoder also conditioned on a variety of control codes (Pfaff, 1979; Poplack, 2000). We call our system CTRL-NMT. Formally speaking, the target distribution of CTRL-NMT can be decomposed using the chain rule of probability and trained with a loss that takes the control code into account: Feature #3: Grammatical Error Correction Detecting potential grammatical errors and offering corrective suggestions for them sentence is also a very important feature in M I SS. We chose the tag-based modeling approach for this feature based on the fresearch field’s latest achievements (Omelianchuk et al., 2020) and our recent work (Parnow et al., 2020, 2021) in the Grammatical Error Correction (GEC). Specifically, the g-transformations developed by (Omelianchuk et al., 2020) were included in our system in the hopes of providing learners more specific suggestions (i.e., the edit type of an error) to revise the users’ input. Predicting edits rather than tokens also increases the generalization of our GEC model. G-transformations are based on several basic transformations: $KEEP (keep the current token unchanged), $DELETE (delete current token), $APPEND_t1 (append new token t1 next to the current token"
2021.emnlp-demo.1,D18-1262,1,0.848954,"u.cn, {mutiyama, eiichiro.sumita}@nict.go.jp, zhaohai@cs.sjtu.edu.cn Abstract and machine translation has correspondingly risen in popularity (Hutchins and Somers, 1992). Recently, Neural Machine Translation (NMT), especially Transformer-based NMT, has emerged as a promising approach with the potential to address many of the shortcomings of traditional rulebased or statistics-based machine translation systems (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017). This has significantly improved the performance of machine translation and other related tasks (Huang et al., 2018; Li et al., 2018a,b). Although neural machine translation has made tremendous improvements and is relatively highperforming, because human language is so complex, machine translation is often still only used as an assistance tool rather than the sole entity responsible for translation. There are several popular and large existing commercial machine translation systems that provide users with effective translation (e.g., Google Translator, Bing Translator, Amazon Translate, and Baidu Translate). As NMT is still very imprecise, however, these web services fall short, as they do not provide sufficient informatio"
2021.emnlp-demo.1,N19-4009,0,0.0214697,"models. As (Zhang et al., 2019, 2020) observed that fine-tuning the pre-trained conSE X X X Table 1: User operations used for our crowdsourcing data collection in our M I SS system. 3 Style Implementation and Training The full system consists of 4 neural models: (1) a multi-style NMT model, (2) a simultaneous NMT model, (3) a grammatical error correction model, and (4) a BERT model. In our current M I SS release, we translate between three languages (English (EN), Chinse (ZH), and Japanese (JA)) for demonstration. For the multi-style NMT model, we implement CTRL-NMT using the public fairseq (Ott et al., 2019) toolkit. In our system, we adopt the Transformer (big) setting as in (Vaswani et al., 2017). We did not choose a deeper or wider Transformer (Wang et al., 2019; Sun et al., 2019) model because we wanted to balance performance and efficiency. As in (Li et al., 2019), we used a data-dependent gaussian prior objective (D2GPo) during the NMT model training process for better generalization. Due to resource constraints, our currently deployed model does not perform back-translation of larger sentences. Table 2 lists all our training corpora and their sizes. For the simultaneous translation model,"
2021.emnlp-demo.1,2021.findings-acl.290,1,0.806502,"Missing"
2021.emnlp-demo.1,2020.findings-emnlp.371,1,0.876743,"Missing"
2021.emnlp-demo.1,P16-1112,0,0.0293237,"ous translation model, we implemented the wait-k strategy and replaced the bi5 Provider Num. PIE-synthetic Lang-8 NUCLE FCE W&I+LOCNESS 9M 947K 56K 34K 34K ZH NLPCC2018-GEC HSK+Lang8 CGED 1.3M JA Lang8 3.1M EN EN ZH JA PrLM Dict P R F0.5 − +XLNet − − +BERT − − +BERT Word Word Word Char Char Word Char Char 53.46 76.92 38.72 45.06 50.34 36.83 45.68 46.56 37.45 41.03 15.07 19.55 33.46 20.52 16.49 27.34 54.22 65.47 29.47 35.73 45.72 31.78 33.74 40.82 Table 5: The performance of our GEC models Table 3: The GEC training data Models For the GEC component, we followed common practice in the GEC task (Rei and Yannakoudakis, 2016; Omelianchuk et al., 2020) and used precision (P), recall (R), and F0 .5 to evaluate our models on all three languages. We evaluated English at the word level and Chinese and Japanese at the character level. We chose the test set of the CoNLL-2014 shared task as our evaluation set for our English GEC model. For Chinese and Japanese, we extracted 5000 sentences from the original training set for the development set and 5000 sentences for the test set and used the rest as the training set. ERRANT6 was used to convert parallel files to the m2 format for subsequent scoring with the M2 Scorer (Dah"
2021.emnlp-demo.1,D19-3018,0,0.0188754,"flexible control in switching between real-time translation and whole-sentence translation, informative back-translation feedback and scoring, and input error detection and revision suggestions. In addition, the system also supports user interactions that modify the translations or inputs, which provides crowdsourced data for further improving the performance of our machine translation and grammatical error correction. Notably, there were also several interactive translation systems in the past, such as CASMACAT (Alabau et al., 2014), (Knowles and Koehn, 2016), (Peris et al., 2017), and INMT (Santy et al., 2019). The distinctions lie in the abilities of the systems and the features to adapt to the latest user needs. 2 The M I SS System There are 5 features in our M I SS translation system: simultaneous translation, back-translation for quality evaluation, grammatical error correction, multi-style translation, and crowdsourcing data collection. The system is available at http: //miss.x2brain.com/ until November 12, 2021. We show a screenshot of the system in Figure 1. In the following subsections, we will describe each component of the system. 2.1 Basis: Transformer-based NMT Transformer (Vaswani et a"
2021.emnlp-demo.1,W19-5341,0,0.017823,"em. 3 Style Implementation and Training The full system consists of 4 neural models: (1) a multi-style NMT model, (2) a simultaneous NMT model, (3) a grammatical error correction model, and (4) a BERT model. In our current M I SS release, we translate between three languages (English (EN), Chinse (ZH), and Japanese (JA)) for demonstration. For the multi-style NMT model, we implement CTRL-NMT using the public fairseq (Ott et al., 2019) toolkit. In our system, we adopt the Transformer (big) setting as in (Vaswani et al., 2017). We did not choose a deeper or wider Transformer (Wang et al., 2019; Sun et al., 2019) model because we wanted to balance performance and efficiency. As in (Li et al., 2019), we used a data-dependent gaussian prior objective (D2GPo) during the NMT model training process for better generalization. Due to resource constraints, our currently deployed model does not perform back-translation of larger sentences. Table 2 lists all our training corpora and their sizes. For the simultaneous translation model, we implemented the wait-k strategy and replaced the bi5 Provider Num. PIE-synthetic Lang-8 NUCLE FCE W&I+LOCNESS 9M 947K 56K 34K 34K ZH NLPCC2018-GEC HSK+Lang8 CGED 1.3M JA Lang8"
2021.emnlp-demo.1,P19-1176,0,0.0137142,"in our M I SS system. 3 Style Implementation and Training The full system consists of 4 neural models: (1) a multi-style NMT model, (2) a simultaneous NMT model, (3) a grammatical error correction model, and (4) a BERT model. In our current M I SS release, we translate between three languages (English (EN), Chinse (ZH), and Japanese (JA)) for demonstration. For the multi-style NMT model, we implement CTRL-NMT using the public fairseq (Ott et al., 2019) toolkit. In our system, we adopt the Transformer (big) setting as in (Vaswani et al., 2017). We did not choose a deeper or wider Transformer (Wang et al., 2019; Sun et al., 2019) model because we wanted to balance performance and efficiency. As in (Li et al., 2019), we used a data-dependent gaussian prior objective (D2GPo) during the NMT model training process for better generalization. Due to resource constraints, our currently deployed model does not perform back-translation of larger sentences. Table 2 lists all our training corpora and their sizes. For the simultaneous translation model, we implemented the wait-k strategy and replaced the bi5 Provider Num. PIE-synthetic Lang-8 NUCLE FCE W&I+LOCNESS 9M 947K 56K 34K 34K ZH NLPCC2018-GEC HSK+Lang8"
2021.emnlp-main.261,D18-1024,0,0.0183798,"erformance even declines compared to baseline. C ONST BTLM can adapt to larger γ and higher phrase utilization proportions, it achieves better results. Method Cosine sim. L2 dist. Pearson cor. Concat Fasttext MUSE 0.36 0.38 4.89 5.13 0.52 0.65 XLM + C ONST BTLM‡ 0.55 0.60 2.64 2.55 0.69 0.71 Table 4: Unsupervised cross-lingual alignment evaluation with word embedding Cosine similarity (Cosine sim.), L2 distance (L2 dist.), and Pearson correlation (Pearson cor.) between source words and their translations. We adopted the same vocabulary size for Concat Fasttext (Bojanowski et al., 2016), MUSE (Alaux et al., 2018), and XLM baselines, and our best EnDe UNMT model and extracted the embeddings for comparison. The results are shown in Table 4. As the results show, our method is not only better than pure embedding training methods, Concat Fasttest and MUSE, on the three evaluation metrics, but also surpasses our strong XLM baseline, which demonstrates that the alignment of the UGUNMT model is indeed improved with the weak alignment information from syntactic categories. 4.4 Universal Constituent Labels To illustrate the universal nature of the phrase grammar, we calculate statistics on the labels of the con"
2021.emnlp-main.261,P17-1042,0,0.0443604,"Missing"
2021.emnlp-main.261,D18-1399,0,0.0182256,"in machine translation. On the one hand, the development of deep neural networks such as Transformer (Vaswani et al., 2017; Li et al., 2021a) has played a significant role in NMT’s improvements. On the other hand, large-scale parallel corpora like the UN corpus (Ziemski et al., 2016) have also played an important role. Despite the recent success of NMT in standard benchmarks, the need for large-scale parallel corpora has limited the effectiveness of NMT in many language pairs, especially in low-resource language pairs (Koehn and Knowles, 2017). Unsupervised Neural Machine Translation (UNMT) (Artetxe et al., 2018b) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Unsupervised machine translation does not need the parallel information from parallel sentences; rather, it generally uses embedding alignments, initializes parameters with pretrained language models, and uses iterative backtranslation between two languages to synthesize pseudo parallel corpora for model training (Lample et al., 2018a,c; Yang et al., 2018; Sun et al., 2019; Conneau and Lample, 2019;"
2021.emnlp-main.261,J82-2005,0,0.644622,"Missing"
2021.emnlp-main.261,D17-1209,0,0.0602426,"Missing"
2021.emnlp-main.261,S17-2002,0,0.0139469,"on label UNMT model is obtained using UG and our pro- proportions, indicating the UD annotation’s uniposed training approaches, we conducted an ex- versality and the effectiveness of our conversion perimental exploration of embedding alignment in preserving grammatical features. This does not according to the experimental settings of (Conneau explain more complicated issues such as language and Lample, 2019) and evaluated models on the similarity or commonality but rather indicates the SemEval’17 En-De cross-lingual semantic word overlap of grammatical phenomena and universal similarity task (Camacho-Collados et al., 2017). features in the annotations and parser predictions. 3256 4.5 Effects of SpanBERT, LIMIT-BERT, and C ONST BTLM for UNMT From the main experiments, the UNMT performance is improved, especially for the small-scale data setting. To find out that if the improvements are caused by C ONST MLM/C ONST BTLM and the syntactic information is really necessary, we compare our approaches with LIMIT-BERT which apply a linguistically guided span based MLM objective during UNMT training, and SpanBERT which is with a non-syntax based span masking strategy. Compared with SpanBERT and LIMIT-BERT in our UNMT fram"
2021.emnlp-main.261,D19-1538,1,0.900451,"Missing"
2021.emnlp-main.261,P18-1192,1,0.892196,"Missing"
2021.emnlp-main.261,2020.acl-main.703,0,0.0326422,"Missing"
2021.emnlp-main.261,D18-1262,1,0.8931,"Missing"
2021.emnlp-main.261,2020.findings-emnlp.371,1,0.856979,"Missing"
2021.emnlp-main.261,H94-1020,0,0.851239,"Missing"
2021.emnlp-main.261,J93-2004,0,0.0748203,"rged IWSLT’14 En-Fr and En-De tasks. † means the to evaluate the En-Fr translation model and encoder-only version is adopted, and ‡ means the dev2010, dev2012, tst2010, tst2011, and tst2012 in encoder-decoder version is adopted. IWSLT14.TED to evaluate the En-De model. To acquire constituent parse trees for monolingual sentences, we adopted the current state-of-the- 3.2 Results and Analysis art Berkeley Neural Parser (Kitaev and Klein, 2018) The results of the UNMT experiment are mainly as our parsing model and trained an En parser us- shown in Table 1. When a large-scale monolingual ing PTB (Marcus et al., 1993), Fr and De parsers corpus is used, our baseline model outperforms using the SPMRL14 multilingual constituent tree- XLM’s reported results. This may be due to the use bank (Seddah et al., 2014), and a Zh Parser using of the larger epoch size, which makes for more adeCTB (Xue et al., 2005). Since a constituent tree- quate training. Based on our strong baseline model, bank is not available in Ro and for the consistency the four implementations of our C ONST MLM and 3254 In the small-scale monolingual training data scenario, the performance of the baseline model has a large decline compared with"
2021.emnlp-main.261,W17-4707,0,0.0590694,"Missing"
2021.emnlp-main.261,W08-2119,0,0.078955,"Missing"
2021.emnlp-main.261,P11-1002,0,0.0926002,"Missing"
2021.emnlp-main.261,P16-1009,0,0.23002,"s compared to the baselines in these tasks. We also present a significantly boosted performance on several low-resource semisupervised tasks. These results verify that universal grammar commonalities can bring additional supervision information to bolster the training of unsupervised and low-resource translation models. 2 2.1 The Proposed Approaches Background We formally present the background of our baseline UNMT system in terms of unsupervised machine translation between languages L1 and L2 . Our UNMT model follows an encoder-decoder architecture as in standard NMT. We use a joint subword (Sennrich et al., 2016b) vocabulary shared between languages and share parameters between source→target and target→source models to take advantage of multilingualism (Edwards, 2002). In this framework, three training methods are indispensable for the feasibility of unsupervised machine translation: initialization, denoising generation, and iterative back-translation. UNMT models typically use denoising generation and iterative back translation simultaneously by alternating between the two methods in a single phase rather Masked Language Modeling (MLM) is a com- than separately in multiple phases. The model is 3250"
2021.emnlp-main.261,P16-1162,0,0.135289,"s compared to the baselines in these tasks. We also present a significantly boosted performance on several low-resource semisupervised tasks. These results verify that universal grammar commonalities can bring additional supervision information to bolster the training of unsupervised and low-resource translation models. 2 2.1 The Proposed Approaches Background We formally present the background of our baseline UNMT system in terms of unsupervised machine translation between languages L1 and L2 . Our UNMT model follows an encoder-decoder architecture as in standard NMT. We use a joint subword (Sennrich et al., 2016b) vocabulary shared between languages and share parameters between source→target and target→source models to take advantage of multilingualism (Edwards, 2002). In this framework, three training methods are indispensable for the feasibility of unsupervised machine translation: initialization, denoising generation, and iterative back-translation. UNMT models typically use denoising generation and iterative back translation simultaneously by alternating between the two methods in a single phase rather Masked Language Modeling (MLM) is a com- than separately in multiple phases. The model is 3250"
2021.emnlp-main.261,P19-1119,1,0.827825,"ne Translation (UNMT) (Artetxe et al., 2018b) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Unsupervised machine translation does not need the parallel information from parallel sentences; rather, it generally uses embedding alignments, initializes parameters with pretrained language models, and uses iterative backtranslation between two languages to synthesize pseudo parallel corpora for model training (Lample et al., 2018a,c; Yang et al., 2018; Sun et al., 2019; Conneau and Lample, 2019; Li et al., 2020a). The pseudo parallel data created by iterative back-translation is the key to the success of unsupervised NMT model training (Kim et al., 2020). ∗ Corresponding author. † This paper was finished when It takes advantage of the equivalence of translaZuchao Li was a fixed term technical researcher at NICT. tion languages to bring supervision (albeit weak This work was supported by Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). supervision) to model training. Recent results in 3249 Proceedings of the 2021 Conferen"
2021.emnlp-main.261,2020.acl-main.324,1,0.803643,"Missing"
2021.emnlp-main.261,P15-1150,0,0.0327086,"Missing"
2021.emnlp-main.261,J10-2004,0,0.0915598,"Missing"
2021.emnlp-main.261,W18-6459,0,0.0630501,"Missing"
2021.emnlp-main.261,P17-1179,0,0.0596818,"Missing"
2021.emnlp-main.261,P08-1064,0,0.109236,"Missing"
2021.emnlp-main.261,2020.findings-emnlp.398,1,0.838011,"T training. The loss, LB , is defined as the sequence after masking. The masked positions 3251 set M consists of randomly sampled discrete positions, that is, M = TopK([randi (0, 1)]ni=1 ). Here, TopK is a function that selects positions by probability until the masking budget has been spent. In span-based MLM like (Joshi et al., 2020), a span of length ` is first sampled from a geometric distribution ` ∼ Geo(p), and the start position of a span is sampled in the same manner as in MLM, giving final masked span set MS = {(Mi , `i )}. In another linguistically guided language modeling approach, Zhou et al. (2020b) proposed Syntactic/Semantic Phrase Masking (SPM) for their model LIMIT-BERT. In SPM, the masked positions set consists of tuples randomly sampled from the linguistic span set instead of the discrete token position set. Only the span boundary information, however, is used in SPM; the linguistic label is ignored, so we remedy this and propose C ONST MLM. In C ONST MLM, we first extract and filter the constituent span set CS = {(s, e, c)i }m i=1 , where s, e, and c represent the start position, end position, and syntactic category, respectively. During filtering, constituent parse trees with a"
2021.emnlp-main.261,2020.findings-emnlp.399,1,0.781794,"T training. The loss, LB , is defined as the sequence after masking. The masked positions 3251 set M consists of randomly sampled discrete positions, that is, M = TopK([randi (0, 1)]ni=1 ). Here, TopK is a function that selects positions by probability until the masking budget has been spent. In span-based MLM like (Joshi et al., 2020), a span of length ` is first sampled from a geometric distribution ` ∼ Geo(p), and the start position of a span is sampled in the same manner as in MLM, giving final masked span set MS = {(Mi , `i )}. In another linguistically guided language modeling approach, Zhou et al. (2020b) proposed Syntactic/Semantic Phrase Masking (SPM) for their model LIMIT-BERT. In SPM, the masked positions set consists of tuples randomly sampled from the linguistic span set instead of the discrete token position set. Only the span boundary information, however, is used in SPM; the linguistic label is ignored, so we remedy this and propose C ONST MLM. In C ONST MLM, we first extract and filter the constituent span set CS = {(s, e, c)i }m i=1 , where s, e, and c represent the start position, end position, and syntactic category, respectively. During filtering, constituent parse trees with a"
2021.emnlp-main.261,L16-1561,0,0.020456,"f constituent trees from English Penn Treebank (PTB) and German dataset of SPMRL14 shared task. The dotted box indicates the constituents that can be masked for prediction. Introduction Recently, Neural Machine Translation (NMT) (Bahdanau et al., 2014; Sutskever et al., 2014) has been greatly developed and become the dominant paradigm in machine translation. On the one hand, the development of deep neural networks such as Transformer (Vaswani et al., 2017; Li et al., 2021a) has played a significant role in NMT’s improvements. On the other hand, large-scale parallel corpora like the UN corpus (Ziemski et al., 2016) have also played an important role. Despite the recent success of NMT in standard benchmarks, the need for large-scale parallel corpora has limited the effectiveness of NMT in many language pairs, especially in low-resource language pairs (Koehn and Knowles, 2017). Unsupervised Neural Machine Translation (UNMT) (Artetxe et al., 2018b) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Unsupervised machine translation does not need the parallel informa"
2021.emnlp-main.261,P18-1005,0,0.0165051,"rvised Neural Machine Translation (UNMT) (Artetxe et al., 2018b) was proposed to alleviate this issue by completely removing the need for parallel data and training an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Unsupervised machine translation does not need the parallel information from parallel sentences; rather, it generally uses embedding alignments, initializes parameters with pretrained language models, and uses iterative backtranslation between two languages to synthesize pseudo parallel corpora for model training (Lample et al., 2018a,c; Yang et al., 2018; Sun et al., 2019; Conneau and Lample, 2019; Li et al., 2020a). The pseudo parallel data created by iterative back-translation is the key to the success of unsupervised NMT model training (Kim et al., 2020). ∗ Corresponding author. † This paper was finished when It takes advantage of the equivalence of translaZuchao Li was a fixed term technical researcher at NICT. tion languages to bring supervision (albeit weak This work was supported by Key Projects of National Natural Science Foundation of China (U1836222 and 61733011). supervision) to model training. Recent results in 3249 Proceedings of"
2021.emnlp-main.261,N19-1118,0,0.0470832,"Missing"
2021.emnlp-main.299,L16-1432,0,0.0159602,"f possible edges derived from the discourse parsing, namely, default-in, default-out, reverse-in, reverse-out, self, and global. Furthermore, to build the relationship with the background user scenario, we add an extra global vertex of the user scenario that connects all the other vertices. As a result, there are three types of vertices, including the rule conditions, discourse relations, and the global scenario vertex. For rule condition and user scenario vertices, 2 This discourse parser gives a state-of-the-art performance on STAC so far. There are 16 discourse relations according to STAC (Asher et al., 2016), including comment, clarificationquestion, elaboration, acknowledgment, continuation, explanation, conditional, question-answer, alternation, questionelaboration, result, background, narration, correction, parallel, and contrast. we fetch the contextualized representation of the special tokens [RULE] and [CLS] before the corresponding sequences, respectively. For relation vertices, they are initialized as the conventional embedding layer, whose representations are obtained through a lookup table. For each rule document that is composed of multiple rule conditions, i.e., EDUs, let hp denote th"
2021.emnlp-main.299,N19-1124,0,0.0608009,"Missing"
2021.emnlp-main.299,D18-1241,0,0.0170919,"framework where the dialogue states for decision making are employed for question generation, in contrast to the independent models or pipeline systems in previous studies. Besides, a variety of strategies are empirically studied for smoothing the two dialogue states in only one decoder. 3) Experiments on the ShARC dataset show the effectiveness of our model, which achieves the new state-of-the-art results. A series of analyses show the contributing factors. 2 Related Work Most of the current conversation-based reading comprehension tasks are formed as either spanbased QA (Reddy et al., 2019; Choi et al., 2018) or multi-choice tasks (Sun et al., 2019; Cui et al., 2020), both of which neglect the vital process of question generation for confirmation during the human-machine interaction. In this work, we are interested in building a machine that can not only make the right decisions but also raise questions when necessary. The related task is called conversational machine reading (Saeidi et al., 2018) which consists of two separate subtasks: decision making and question generation. Compared with conversation-based reading comprehension tasks, our concerned CMR task is more challenging as it involves r"
2021.emnlp-main.299,2020.acl-main.130,0,0.0166896,"employed for question generation, in contrast to the independent models or pipeline systems in previous studies. Besides, a variety of strategies are empirically studied for smoothing the two dialogue states in only one decoder. 3) Experiments on the ShARC dataset show the effectiveness of our model, which achieves the new state-of-the-art results. A series of analyses show the contributing factors. 2 Related Work Most of the current conversation-based reading comprehension tasks are formed as either spanbased QA (Reddy et al., 2019; Choi et al., 2018) or multi-choice tasks (Sun et al., 2019; Cui et al., 2020), both of which neglect the vital process of question generation for confirmation during the human-machine interaction. In this work, we are interested in building a machine that can not only make the right decisions but also raise questions when necessary. The related task is called conversational machine reading (Saeidi et al., 2018) which consists of two separate subtasks: decision making and question generation. Compared with conversation-based reading comprehension tasks, our concerned CMR task is more challenging as it involves rule documents, scenarios, asking clarifi1 Our source codes"
2021.emnlp-main.299,N19-1423,0,0.00556007,"are packed from Hc . Then a gate control λ is computed ˆ + Uλ H e ) to get the final as sigmoid(Wλ H ˆ representation H = H e + λH. (7) The overall loss for decision making is: Ld = Ldecision + λLentail . (8) Question Generation If the decision is made to be Inquire, the machine needs to ask a follow-up question to further clarify. Question generation in this part is mainly based on the uncovered information in the rule document, and then that information will be rephrased into a question. We predict the position of an under-specified span within a rule document in a supervised way. Following Devlin et al. (2019), our model learns a start vector ws ∈ Rd and end vector we ∈ Rd to indicate the start and end positions of the desired span: span = arg min(wsT tk,i + weT tk,j ), H is then passed to the BART decoder to generate the follow-up question. At the i-th time-step, H is used to generate the target token yi by P (yi |y&lt;i , x; θ) ∝ exp(Wd tanh(Ww H)), (10) where θ denotes all the trainable parameters. Wd and Ww are projection matrices. The training objective is computed by Lg = arg max (9) I X log P (yi |y&lt;i , x; θ). (11) i=1 i,j,k where tk,i denote the i-th token in the k-th rule sen- is tence. The g"
2021.emnlp-main.299,2020.acl-main.88,0,0.257863,"eventually hintion questions. ders the model performance. In this work, we propose an effective gating strategy by smoothA variety of methods have been proposed for ing the two dialogue states in only one decoder the CMR task, including 1) sequential models that and bridge decision making and question genencode all the elements and model the matching reeration to provide a richer dialogue state referlationships with attention mechanisms (Zhong and ence. Experiments on the OR-ShARC dataset Zettlemoyer, 2019; Lawrence et al., 2019; Verma show the effectiveness of our method, which et al., 2020; Gao et al., 2020a,b); 2) graph-based achieves new state-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given befor"
2021.emnlp-main.299,2020.emnlp-main.191,0,0.35027,"eventually hintion questions. ders the model performance. In this work, we propose an effective gating strategy by smoothA variety of methods have been proposed for ing the two dialogue states in only one decoder the CMR task, including 1) sequential models that and bridge decision making and question genencode all the elements and model the matching reeration to provide a richer dialogue state referlationships with attention mechanisms (Zhong and ence. Experiments on the OR-ShARC dataset Zettlemoyer, 2019; Lawrence et al., 2019; Verma show the effectiveness of our method, which et al., 2020; Gao et al., 2020a,b); 2) graph-based achieves new state-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given befor"
2021.emnlp-main.299,2021.acl-long.285,0,0.0126044,"ate-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given before the system interFan et al., 2020; Gu et al., 2021). It usually adopts acts with users, which is in a closed-book style. the form of question answering (QA) according to In real-world applications, the machines are ofthe user’s query along with the dialogue context (Sun et al., 2019; Reddy et al., 2019; Choi et al., ten required to retrieve supporting information to 2018). The machine may also actively ask ques- respond to incoming high-level queries in an intertions for confirmation (Wu et al., 2018; Cai et al., active manner, which results in an open-retrieval setting (Gao et al., 2021). The comparison of the 2019; Zhang et al., 2020b; Gu et"
2021.emnlp-main.299,D19-1001,0,0.255904,"actilogue context, and make decisions or ask clarificavate question generation, which eventually hintion questions. ders the model performance. In this work, we propose an effective gating strategy by smoothA variety of methods have been proposed for ing the two dialogue states in only one decoder the CMR task, including 1) sequential models that and bridge decision making and question genencode all the elements and model the matching reeration to provide a richer dialogue state referlationships with attention mechanisms (Zhong and ence. Experiments on the OR-ShARC dataset Zettlemoyer, 2019; Lawrence et al., 2019; Verma show the effectiveness of our method, which et al., 2020; Gao et al., 2020a,b); 2) graph-based achieves new state-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant e"
2021.emnlp-main.299,2020.emnlp-main.120,0,0.0111371,"eak the rule text into clauselike units called elementary discourse units (EDUs) to extract the in-line rule conditions from the rule texts. Embedding We employ pre-trained language model (PrLM) model as the backbone of the encoder. As shown in the figure, the input of our model includes rule document which has already be parsed into EDUs with explicit discourse relation tagging, user initial question, user scenario and the dialog history. Instead of inserting a [CLS] token before each rule condition to get a sentence-level representation, we use [RULE] which is proved to enhance performance (Lee et al., 2020). Formally, the sequence is organized as: {[RULE] EDU0 [RULE] EDU1 [RULE] EDUk [CLS] Question [CLS] Scenario [CLS] History [SEP]}. Then we feed the sequence to the PrLM to obtain the contextualized representation. Interaction To explicitly model the discourse structure among the rule conditions, we first annotate the discourse relationships between the rule conditions and employ a relational graph convolutional network following Ouyang et al. (2021) by regarding the rule conditions as the vertices. The graph is formed as a Levi graph (Levi, 1942) that regards the relation edges as additional v"
2021.emnlp-main.299,2021.findings-acl.279,1,0.782265,"e encoder together with other necessary information. Lawrence et al., 2019; Verma et al., 2020; Gao et al., 2020a,b; Ouyang et al., 2021) have made progress in modeling the matching relationships between the rule document and other elements such as user scenarios and questions. These studies are based on the hypothesis that the supporting information for answering the question is provided, which does not meet the real-world applications. Therefore, we are motivated to investigate the open-retrieval settings (Qu et al., 2020), where the retrieved background knowledge would be noisy. Gao et al. (2021) makes the initial attempts of open-retrieval for CMR. However, like previous studies, the common solution is training independent or pipeline systems for the two subtasks and does not consider the information flow between decision making and question generation, which would eventually hinder the model performance. Compared to existing methods, our method makes the first attempt to bridge the gap between decision making and question generation, by smoothing the two dialogue states in only one decoder. In addition, we improve the retrieval process by taking advantage of the traditional TF-IDF m"
2021.emnlp-main.299,H90-1020,0,0.0999002,"Tong University 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University 3 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 4 National Institute of Information and Communications Technology (NICT), Kyoto, Japan {zhangzs,oysr0926}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn {mutiyama,eiichiro.sumita}@nict.go.jp Abstract with people according to the dialogue states, and completes specific tasks, such as ordering meals Conversational machine reading (CMR) re(Liu et al., 2013) and air tickets (Price, 1990). In quires machines to communicate with humans real-world scenario, annotating data such as inthrough multi-turn interactions between two tents and slots is expensive. Inspired by the studies salient dialogue states of decision making and of reading comprehension (Rajpurkar et al., 2016, question generation processes. In open CMR 2018; Zhang et al., 2020c, 2021), there appears a settings, as the more realistic scenario, the retrieved background knowledge would be noisy, more general task — conversational machine readwhich results in severe challenges in the ining (CMR) (Saeidi et al., 2018):"
2021.emnlp-main.299,2020.emnlp-main.550,0,0.10908,"R. However, like previous studies, the common solution is training independent or pipeline systems for the two subtasks and does not consider the information flow between decision making and question generation, which would eventually hinder the model performance. Compared to existing methods, our method makes the first attempt to bridge the gap between decision making and question generation, by smoothing the two dialogue states in only one decoder. In addition, we improve the retrieval process by taking advantage of the traditional TF-IDF method and the latest dense passage retrieval model (Karpukhin et al., 2020). and a generator G(·, ·) on {R, Us , Uq , C} for question generation. 4 Model Our model is composed of three main modules: retriever, encoder, and decoder. The retriever is employed to retrieve the related rule texts for the given user scenario and question. The encoder takes the tuple {R, Us , Uq , C} as the input, encodes the elements into vectors and captures the contextualized representations. The decoder makes a decision or generates a question once the decision is “inquiry”. Figure 1 overviews the model architecture, we will elaborate the details in the following part. 4.1 Retrieval To"
2021.emnlp-main.299,P18-2124,0,0.0621738,"Missing"
2021.emnlp-main.299,D16-1264,0,0.0496147,"Communications Technology (NICT), Kyoto, Japan {zhangzs,oysr0926}@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn {mutiyama,eiichiro.sumita}@nict.go.jp Abstract with people according to the dialogue states, and completes specific tasks, such as ordering meals Conversational machine reading (CMR) re(Liu et al., 2013) and air tickets (Price, 1990). In quires machines to communicate with humans real-world scenario, annotating data such as inthrough multi-turn interactions between two tents and slots is expensive. Inspired by the studies salient dialogue states of decision making and of reading comprehension (Rajpurkar et al., 2016, question generation processes. In open CMR 2018; Zhang et al., 2020c, 2021), there appears a settings, as the more realistic scenario, the retrieved background knowledge would be noisy, more general task — conversational machine readwhich results in severe challenges in the ining (CMR) (Saeidi et al., 2018): given the inquiry, formation transmission. Existing studies comthe machine is required to retrieve relevant supmonly train independent or pipeline systems porting rule documents, the machine should judge for the two subtasks. However, those methods whether the goal is satisfied according"
2021.emnlp-main.299,2020.emnlp-main.589,0,0.0933312,"ques. BART-base *relations obtained by tagging model &lt;rule&gt; user scenario e1 Document k ... e2 e2 e3 ... e4 e3 ... ...... Rule Conditions Tagging Double-channel Decoder relations e1 e3 e1 Comment us GCN layer e 4 Document 1 ...... r3 e4 &lt;rule&gt; ... Rule e5 &lt;/s&gt; ... Span e6 &lt;/s&gt; ... BART-base &lt;rule&gt; EDU0 &lt;rule&gt; EDU1 &lt;rule&gt; EDU... &lt;s&gt; Scen. &lt;/s&gt; Ques. &lt;/s&gt; QA... &lt;/s&gt; Figure 2: The overall structure of our model O SCAR. The left part introduces the retrieval and tagging process for rule documents, which is then fed into the encoder together with other necessary information. Lawrence et al., 2019; Verma et al., 2020; Gao et al., 2020a,b; Ouyang et al., 2021) have made progress in modeling the matching relationships between the rule document and other elements such as user scenarios and questions. These studies are based on the hypothesis that the supporting information for answering the question is provided, which does not meet the real-world applications. Therefore, we are motivated to investigate the open-retrieval settings (Qu et al., 2020), where the retrieved background knowledge would be noisy. Gao et al. (2021) makes the initial attempts of open-retrieval for CMR. However, like previous studies, t"
2021.emnlp-main.299,Q19-1016,0,0.115503,"hallenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given before the system interFan et al., 2020; Gu et al., 2021). It usually adopts acts with users, which is in a closed-book style. the form of question answering (QA) according to In real-world applications, the machines are ofthe user’s query along with the dialogue context (Sun et al., 2019; Reddy et al., 2019; Choi et al., ten required to retrieve supporting information to 2018). The machine may also actively ask ques- respond to incoming high-level queries in an intertions for confirmation (Wu et al., 2018; Cai et al., active manner, which results in an open-retrieval setting (Gao et al., 2021). The comparison of the 2019; Zhang et al., 2020b; Gu et al., 2020). closed-book setting and open-retrieval setting is In the classic spoken language understanding tasks (Tur and De Mori, 2011; Zhang et al., 2020a; shown in Figure 1. 2) The gap between decision making and quesRen et al., 2018; Qin et al., 2"
2021.emnlp-main.299,D18-1299,0,0.0339546,"Missing"
2021.emnlp-main.299,D18-1233,0,0.202927,"tickets (Price, 1990). In quires machines to communicate with humans real-world scenario, annotating data such as inthrough multi-turn interactions between two tents and slots is expensive. Inspired by the studies salient dialogue states of decision making and of reading comprehension (Rajpurkar et al., 2016, question generation processes. In open CMR 2018; Zhang et al., 2020c, 2021), there appears a settings, as the more realistic scenario, the retrieved background knowledge would be noisy, more general task — conversational machine readwhich results in severe challenges in the ining (CMR) (Saeidi et al., 2018): given the inquiry, formation transmission. Existing studies comthe machine is required to retrieve relevant supmonly train independent or pipeline systems porting rule documents, the machine should judge for the two subtasks. However, those methods whether the goal is satisfied according to the diaare trivial by using hard-label decisions to actilogue context, and make decisions or ask clarificavate question generation, which eventually hintion questions. ders the model performance. In this work, we propose an effective gating strategy by smoothA variety of methods have been proposed for ing"
2021.emnlp-main.299,Q19-1014,0,0.0881508,"logue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given before the system interFan et al., 2020; Gu et al., 2021). It usually adopts acts with users, which is in a closed-book style. the form of question answering (QA) according to In real-world applications, the machines are ofthe user’s query along with the dialogue context (Sun et al., 2019; Reddy et al., 2019; Choi et al., ten required to retrieve supporting information to 2018). The machine may also actively ask ques- respond to incoming high-level queries in an intertions for confirmation (Wu et al., 2018; Cai et al., active manner, which results in an open-retrieval setting (Gao et al., 2021). The comparison of the 2019; Zhang et al., 2020b; Gu et al., 2020). closed-book setting and open-retrieval setting is In the classic spoken language understanding tasks (Tur and De Mori, 2011; Zhang et al., 2020a; shown in Figure 1. 2) The gap between decision making and quesRen et al.,"
2021.emnlp-main.299,2020.acl-demos.30,0,0.237205,"u.cn,zhaohai@cs.sjtu.edu.cn {mutiyama,eiichiro.sumita}@nict.go.jp Abstract with people according to the dialogue states, and completes specific tasks, such as ordering meals Conversational machine reading (CMR) re(Liu et al., 2013) and air tickets (Price, 1990). In quires machines to communicate with humans real-world scenario, annotating data such as inthrough multi-turn interactions between two tents and slots is expensive. Inspired by the studies salient dialogue states of decision making and of reading comprehension (Rajpurkar et al., 2016, question generation processes. In open CMR 2018; Zhang et al., 2020c, 2021), there appears a settings, as the more realistic scenario, the retrieved background knowledge would be noisy, more general task — conversational machine readwhich results in severe challenges in the ining (CMR) (Saeidi et al., 2018): given the inquiry, formation transmission. Existing studies comthe machine is required to retrieve relevant supmonly train independent or pipeline systems porting rule documents, the machine should judge for the two subtasks. However, those methods whether the goal is satisfied according to the diaare trivial by using hard-label decisions to actilogue con"
2021.emnlp-main.299,C18-1317,1,0.893958,"Missing"
2021.emnlp-main.299,P19-1223,0,0.0225589,"Missing"
2021.emnlp-main.299,C18-2024,1,0.833381,"eriments on the OR-ShARC dataset Zettlemoyer, 2019; Lawrence et al., 2019; Verma show the effectiveness of our method, which et al., 2020; Gao et al., 2020a,b); 2) graph-based achieves new state-of-the-art results. methods that capture the discourse structures of the 1 Introduction rule texts and user scenario for better interactions (Ouyang et al., 2021). However, there are two sides The ultimate goal of multi-turn dialogue is to enof challenges that have been neglected: able the machine to interact with human beings and 1) Open-retrieval of supporting evidence. The solve practical problems (Zhu et al., 2018; Zhang above existing methods assume that the relevant et al., 2018; Zaib et al., 2020; Huang et al., 2020; rule documents are given before the system interFan et al., 2020; Gu et al., 2021). It usually adopts acts with users, which is in a closed-book style. the form of question answering (QA) according to In real-world applications, the machines are ofthe user’s query along with the dialogue context (Sun et al., 2019; Reddy et al., 2019; Choi et al., ten required to retrieve supporting information to 2018). The machine may also actively ask ques- respond to incoming high-level queries in an"
2021.naacl-main.311,D18-1549,0,0.0207702,"adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-re"
2021.naacl-main.311,P07-2045,0,0.0126746,"gual WMT news crawl datasets3 for each language. For the high-resource languages En and Fr, we randomly extracted 50M sentences. For the low-resource languages Ro and Et, we used all available monolingual news crawl training data. To make our experiments comparable with previous work (Lample and Conneau, 2019), we report the results on newstest2014 for Fr–En, newstest2016 for Ro–En, and newstest2018 for Et–En. Language En Fr Ro Et Sentences Words 50.00M 50.00M 8.92M 3.00M 1.15B 1.19B 207.07M 51.39M Table 2: Statistics of the monolingual corpora. For preprocessing, we used the Moses tokenizer (Koehn et al., 2007). To clean the data, we only applied the Moses script clean-corpus-n.perl to remove lines from the monolingual data containing more than 50 words. We used a shared vocabulary for all language pairs, with 60,000 subword tokens based on BPE (Sennrich et al., 2016b). learning rate was 0.0001, β1 = 0.9, and β2 = 0.98. We trained a specific cross-lingual language model for each different training dataset. The language model was used to initialize the full parameters of the UNMT system. Eight V100 GPUs were used to train all UNMT models. We used the casesensitive 4-gram BLEU score computed by the mu"
2021.naacl-main.311,P16-1009,0,0.302263,"hat the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian. Data size (sentences) En-Fr Fr-En 50M En and 50M Fr (Baseline) 25M En and 25M Fr 50M En and 2M Fr 2M En and 50M Fr 2M En and 2M Fr 36.63 36.59 31.01 31.84 30.91 34.38 34.34 31.06 30.21 29.86 Table 1: UNMT performance (BLEU score) for different training data sizes on En–Fr language pairs. T"
2021.naacl-main.311,P16-1162,0,0.856645,"hat the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian. Data size (sentences) En-Fr Fr-En 50M En and 50M Fr (Baseline) 25M En and 25M Fr 50M En and 2M Fr 2M En and 50M Fr 2M En and 2M Fr 36.63 36.59 31.01 31.84 30.91 34.38 34.34 31.06 30.21 29.86 Table 1: UNMT performance (BLEU score) for different training data sizes on En–Fr language pairs. T"
2021.naacl-main.311,P95-1026,0,0.892071,"∗ (Y |X) logPM U (X|Y ) logPM U (Y |X), U ∗ (X|Y ) (1) where P (X) and P (Y ) are the empirical data distribution from monolingual corpora {X}, {Y }, and PM U (Y |X) and PM U (X|Y ) are the conditional distributions generated by the UNMT model. In ∗ addition, M U denotes the model at the previous iteration for generating new pseudo-parallel sentence pairs to update the UNMT model. Self-training proposed by Scudder (1965), is a semi-supervised approach that utilizes unannotated data to create better models. Self-training has been successfully applied to many natural language processing tasks (Yarowsky, 1995; McClosky et al., 2006; Zhang and Zong, 2016; He et al., 2020). Recently, He et al. (2020) empirically found that noisy self-training could improve the performance of supervised machine translation and synthetic data could play a positive role, even as a target. 4 Self-training Mechanism for UNMT Based on these previous empirical findings and analyses, we propose a self-training mechanism to generate synthetic training data for UNMT to alleviate poor performance in the unbalanced training data scenario. The synthetic data increases the diversity of low-resource language data, further enhancin"
2021.naacl-main.311,D16-1160,0,0.0127253,", U ∗ (X|Y ) (1) where P (X) and P (Y ) are the empirical data distribution from monolingual corpora {X}, {Y }, and PM U (Y |X) and PM U (X|Y ) are the conditional distributions generated by the UNMT model. In ∗ addition, M U denotes the model at the previous iteration for generating new pseudo-parallel sentence pairs to update the UNMT model. Self-training proposed by Scudder (1965), is a semi-supervised approach that utilizes unannotated data to create better models. Self-training has been successfully applied to many natural language processing tasks (Yarowsky, 1995; McClosky et al., 2006; Zhang and Zong, 2016; He et al., 2020). Recently, He et al. (2020) empirically found that noisy self-training could improve the performance of supervised machine translation and synthetic data could play a positive role, even as a target. 4 Self-training Mechanism for UNMT Based on these previous empirical findings and analyses, we propose a self-training mechanism to generate synthetic training data for UNMT to alleviate poor performance in the unbalanced training data scenario. The synthetic data increases the diversity of low-resource language data, further enhancing the performance of the translation, even 3"
2021.naacl-main.311,P19-1119,1,0.628784,"raining data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian. Data size (sentences) En-Fr Fr-En 50M En and 50M Fr (Baseline)"
2021.naacl-main.311,2020.acl-main.324,1,0.817815,"improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems. 1 Introduction Recently, unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has attracted a high level of interest in the machine translation community (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Wu et al., 2019; Sun et al., 2019, 2020b). With the help of cross-lingual language model pretraining (Lample and Conneau, 2019; Song et al., 2019; Sun et al., 2020a), the denoising auto-encoder (Vincent et al., 2010), and backtranslation (Sennrich et al., 2016a), UNMT has achieved remarkable results in several translation tasks. However, in real-world scenarios, in contrast to the many large corpora available for high-resource languages such as English and French, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian. Data size (sentences) En-Fr Fr-En 50M En and 50M Fr (Baseline) 25M En and 25M Fr 50M En and 2M Fr 2M En and 50M Fr 2M En and 2M Fr 36.63 36.59 31.01 31.84 30.91 34.38 34.34 31.06 30.21 29.86 Ta"
2021.naacl-main.311,N19-1120,0,0.040071,"Missing"
2021.naacl-main.438,W15-4319,0,0.048961,"Missing"
2021.naacl-main.438,2020.lrec-1.773,0,0.0611393,"Missing"
2021.naacl-main.438,P11-1038,0,0.0383248,"al. (2015)’s classification tic and phonetic similarity. is most similar to our types of vocabulary (shown For English and Chinese, various classification in Table 2), whereas we provide more detailed defimethods for normalization of informal words (Li nitions of categories and criteria for standard and and Yarowsky, 2008; Wang et al., 2013; Han and non-standard forms. Other work on Japanese MA Baldwin, 2011; Jin, 2015; van der Goot, 2019) and LN did not consider diverse phenomena in have been developed based on, for example, string, UGT (Sasano et al., 2013; Saito et al., 2014). For English, Han and Baldwin (2011) classi- phonetic, semantic similarity, or co-occurrence frefied ill-formed English words on Twitter into ex- quency. Qian et al. (2015) proposed a transitiontra/missing letters and/or number substitution (e.g., based method with append(x), separate(x), and separate_and_substitute(x,y) operations for the “b4” for “before”), slang (e.g., “lol” for “laugh out loud” ), and “others”. van der Goot et al. (2018) joint word segmentation, POS tagging, and normalization of Chinese microblog text. Dekker defined a more comprehensive taxonomy with 14 and van der Goot (2020) automatically generated catego"
2021.naacl-main.438,W15-4313,0,0.0214366,"ion of pe- Saito et al. (2017) extracted formal-informal word culiar expressions is most similar to our types of pairs from unlabeled Twitter data based on semanvariant forms and Kaji et al. (2015)’s classification tic and phonetic similarity. is most similar to our types of vocabulary (shown For English and Chinese, various classification in Table 2), whereas we provide more detailed defimethods for normalization of informal words (Li nitions of categories and criteria for standard and and Yarowsky, 2008; Wang et al., 2013; Han and non-standard forms. Other work on Japanese MA Baldwin, 2011; Jin, 2015; van der Goot, 2019) and LN did not consider diverse phenomena in have been developed based on, for example, string, UGT (Sasano et al., 2013; Saito et al., 2014). For English, Han and Baldwin (2011) classi- phonetic, semantic similarity, or co-occurrence frefied ill-formed English words on Twitter into ex- quency. Qian et al. (2015) proposed a transitiontra/missing letters and/or number substitution (e.g., based method with append(x), separate(x), and separate_and_substitute(x,y) operations for the “b4” for “before”), slang (e.g., “lol” for “laugh out loud” ), and “others”. van der Goot et a"
2021.naacl-main.438,D14-1011,0,0.0999202,"emmatization because the Japanese language has no explicit word delimiters. Although MA methods for well-formed text (Kudo et al., 2004; Neubig et al., 2011) have been actively developed taking advantage of the existing annotated corpora of newswire domains, they perform poorly on usergenerated text (UGT), such as social media posts and blogs. Additionally, because of the frequent occurrence of informal words, lexical normalization (LN), which identifies standard word forms, is another important task in UGT. Several studies have been devoted to both tasks in Japanese UGT (Sasano et al., 2013; Kaji and Kitsuregawa, 2014; Saito et al., 2014, 2017) to achieve the robust performance for noisy text. Previous researchers 1 Our corpus will be available at https://github. have evaluated their own systems using in-house com/shigashiyama/jlexnorm. data created by individual researchers, and thus 2 Twitter could be a candidate for a data source. However, it is difficult to compare the performance of dif- redistributing original tweets collected via the Twitter Streaming APIs is not permitted by Twitter, Inc., and an alternative ferent systems and discuss what issues remain in approach to distributing tweet URLs has th"
2021.naacl-main.438,W04-3230,0,0.164412,"Missing"
2021.naacl-main.438,D08-1108,0,0.0604526,"al. (2018) joint word segmentation, POS tagging, and normalization of Chinese microblog text. Dekker defined a more comprehensive taxonomy with 14 and van der Goot (2020) automatically generated categories for a detailed evaluation of English LN systems. It includes phrasal abbreviation (e.g., “idk” pseudo training data from English raw tweets using noise insertion operations to achieve comparable for “I don’t know”), repetition (e.g., “soooo” for “so”), and phonetic transformation (e.g., “hackd” performance without manually annotated data to an existing LN system. for “hacked”). For Chinese, Li and Yarowsky (2008) classified informal words in Chinese webpages into four 19 types: homophone (informal words with similar Pinyin pronunciation is shown in “hi”. 5539 7 Conclusion We presented a publicly available Japanese UGT corpus annotated with morphological and normalization information. Our corpus enables the performance comparison of existing and future systems and identifies the main remaining issues of MA and LN of UGT. Experiments on our corpus demonstrated the limited performance of the existing systems for non-general words and non-standard forms mainly caused by two types of difficult examples: co"
2021.naacl-main.438,P11-2093,0,0.0259792,"entence IDs and annotation information, including word boundaries, POS, lemmas, standard forms of non-standard word tokens, and word categories. We will release the annotation information that enables BCCWJ applicants to replicate the full BQNC data from the original BCCWJ data.3 Using the BQNC, we evaluated two existing Japanese morphological analysis (MA) is a fundamental and important task that involves word segmentation, part-of-speech (POS) tagging and lemmatization because the Japanese language has no explicit word delimiters. Although MA methods for well-formed text (Kudo et al., 2004; Neubig et al., 2011) have been actively developed taking advantage of the existing annotated corpora of newswire domains, they perform poorly on usergenerated text (UGT), such as social media posts and blogs. Additionally, because of the frequent occurrence of informal words, lexical normalization (LN), which identifies standard word forms, is another important task in UGT. Several studies have been devoted to both tasks in Japanese UGT (Sasano et al., 2013; Kaji and Kitsuregawa, 2014; Saito et al., 2014, 2017) to achieve the robust performance for noisy text. Previous researchers 1 Our corpus will be available a"
2021.naacl-main.438,D15-1211,0,0.0186263,"classification in Table 2), whereas we provide more detailed defimethods for normalization of informal words (Li nitions of categories and criteria for standard and and Yarowsky, 2008; Wang et al., 2013; Han and non-standard forms. Other work on Japanese MA Baldwin, 2011; Jin, 2015; van der Goot, 2019) and LN did not consider diverse phenomena in have been developed based on, for example, string, UGT (Sasano et al., 2013; Saito et al., 2014). For English, Han and Baldwin (2011) classi- phonetic, semantic similarity, or co-occurrence frefied ill-formed English words on Twitter into ex- quency. Qian et al. (2015) proposed a transitiontra/missing letters and/or number substitution (e.g., based method with append(x), separate(x), and separate_and_substitute(x,y) operations for the “b4” for “before”), slang (e.g., “lol” for “laugh out loud” ), and “others”. van der Goot et al. (2018) joint word segmentation, POS tagging, and normalization of Chinese microblog text. Dekker defined a more comprehensive taxonomy with 14 and van der Goot (2020) automatically generated categories for a detailed evaluation of English LN systems. It includes phrasal abbreviation (e.g., “idk” pseudo training data from English ra"
2021.naacl-main.438,I17-1094,0,0.0705833,"o¯ ‘oh’ and サラサラ〜 was normalized to サラサラ sarasara ‘smoothly’). However, we assessed these as errors based on our criterion that interjections have no (non-)standard forms and the BCCWJ guidelines that regards onomatopoeia with and without long sound insertion as different lemmas. malization errors into two types: complicated variant forms and unknown words of specific vocabulary types such as emoticons and neologisms/slang. The effective use of linguistic resources may be required to build more accurate systems, for example, discovering variant form candidates from large raw text similar to (Saito et al., 2017), and constructing/using term dictionaries of specific vocabulary types. 6 Related Work UGT Corpus for MA and LN Hashimoto et al. (2011) developed a Japanese blog corpus with morphological, grammatical, and sentiment information, but it contains only 38 non-standard forms and 102 misspellings as UGT-specific examples. Osaki et al. (2017) constructed a Japanese Twitter corpus annotated with morphological information and standard word forms. Although they published tweet URLs along with annotation information, we could only restore parts of sentences because of the deletion of the original tweet"
2021.naacl-main.438,P19-3032,0,0.0256965,"Missing"
2021.naacl-main.438,L18-1109,0,0.0396716,"Missing"
2021.naacl-main.438,I13-1015,0,0.0257161,"irs of formal and informal words on Twitter. variants. Ikeda et al. (2010)’s classification of pe- Saito et al. (2017) extracted formal-informal word culiar expressions is most similar to our types of pairs from unlabeled Twitter data based on semanvariant forms and Kaji et al. (2015)’s classification tic and phonetic similarity. is most similar to our types of vocabulary (shown For English and Chinese, various classification in Table 2), whereas we provide more detailed defimethods for normalization of informal words (Li nitions of categories and criteria for standard and and Yarowsky, 2008; Wang et al., 2013; Han and non-standard forms. Other work on Japanese MA Baldwin, 2011; Jin, 2015; van der Goot, 2019) and LN did not consider diverse phenomena in have been developed based on, for example, string, UGT (Sasano et al., 2013; Saito et al., 2014). For English, Han and Baldwin (2011) classi- phonetic, semantic similarity, or co-occurrence frefied ill-formed English words on Twitter into ex- quency. Qian et al. (2015) proposed a transitiontra/missing letters and/or number substitution (e.g., based method with append(x), separate(x), and separate_and_substitute(x,y) operations for the “b4” for “befo"
2021.naacl-main.438,D13-1007,0,0.0741565,"Missing"
2021.naacl-main.438,C14-1167,0,0.0805651,"anese language has no explicit word delimiters. Although MA methods for well-formed text (Kudo et al., 2004; Neubig et al., 2011) have been actively developed taking advantage of the existing annotated corpora of newswire domains, they perform poorly on usergenerated text (UGT), such as social media posts and blogs. Additionally, because of the frequent occurrence of informal words, lexical normalization (LN), which identifies standard word forms, is another important task in UGT. Several studies have been devoted to both tasks in Japanese UGT (Sasano et al., 2013; Kaji and Kitsuregawa, 2014; Saito et al., 2014, 2017) to achieve the robust performance for noisy text. Previous researchers 1 Our corpus will be available at https://github. have evaluated their own systems using in-house com/shigashiyama/jlexnorm. data created by individual researchers, and thus 2 Twitter could be a candidate for a data source. However, it is difficult to compare the performance of dif- redistributing original tweets collected via the Twitter Streaming APIs is not permitted by Twitter, Inc., and an alternative ferent systems and discuss what issues remain in approach to distributing tweet URLs has the disadvantage that"
2021.naacl-main.438,I13-1019,0,0.0948969,"h (POS) tagging and lemmatization because the Japanese language has no explicit word delimiters. Although MA methods for well-formed text (Kudo et al., 2004; Neubig et al., 2011) have been actively developed taking advantage of the existing annotated corpora of newswire domains, they perform poorly on usergenerated text (UGT), such as social media posts and blogs. Additionally, because of the frequent occurrence of informal words, lexical normalization (LN), which identifies standard word forms, is another important task in UGT. Several studies have been devoted to both tasks in Japanese UGT (Sasano et al., 2013; Kaji and Kitsuregawa, 2014; Saito et al., 2014, 2017) to achieve the robust performance for noisy text. Previous researchers 1 Our corpus will be available at https://github. have evaluated their own systems using in-house com/shigashiyama/jlexnorm. data created by individual researchers, and thus 2 Twitter could be a candidate for a data source. However, it is difficult to compare the performance of dif- redistributing original tweets collected via the Twitter Streaming APIs is not permitted by Twitter, Inc., and an alternative ferent systems and discuss what issues remain in approach to di"
2021.wat-1.4,D17-1098,0,0.0211247,"(Chu and Wang, 2018), which is also crucial for the practical application of NMT. Since there is still a need for manual interventions for the new NMT paradigm, much effort is spent in studying how to incorporate this explicit control into the end-to-end neural translation (Arthur et al., 2016). Among these efforts, Constrained Decoding (CD) has gained a lot of attention in this research field, which is a modification to commonly adopted beam search in ordinary NMT models. Hokamp and Liu (2017) proposed grid beam search, which expands beam search to include pre-specified lexical constraints. Anderson et al. (2017) used constrained beam search to force the inclusion of restricted words in the output, and employed fixed pre-trained word embeddings to facilitate vocabulary expansion to unseen words in training. While these works accomplish the goal of explicit translation control, the time complexity of their decoding algorithm and resultant decoding speed falls short of the expectations. The complexity of grid beam search and constrained beam search is linear and exponential to the number of constraints, respectively. These algorithms are thus too inefficient to be practical for large-scale use. To allev"
2021.wat-1.4,N19-1090,0,0.0512166,"Missing"
2021.wat-1.4,D16-1162,0,0.0114004,"of Information and Communications Technology (NICT), Kyoto, Japan 1 charlee@sjtu.edu.cn, {mutiyama, eiichiro.sumita}, zhaohai@cs.sjtu.edu.cn Abstract control over translation output, which is effective in a variety of translation settings, including interactive machine translation (Peris et al., 2017) and domain adaptation (Chu and Wang, 2018), which is also crucial for the practical application of NMT. Since there is still a need for manual interventions for the new NMT paradigm, much effort is spent in studying how to incorporate this explicit control into the end-to-end neural translation (Arthur et al., 2016). Among these efforts, Constrained Decoding (CD) has gained a lot of attention in this research field, which is a modification to commonly adopted beam search in ordinary NMT models. Hokamp and Liu (2017) proposed grid beam search, which expands beam search to include pre-specified lexical constraints. Anderson et al. (2017) used constrained beam search to force the inclusion of restricted words in the output, and employed fixed pre-trained word embeddings to facilitate vocabulary expansion to unseen words in training. While these works accomplish the goal of explicit translation control, the"
2021.wat-1.4,C18-1111,0,0.0241363,"ong University 2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China 3 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 4 National Institute of Information and Communications Technology (NICT), Kyoto, Japan 1 charlee@sjtu.edu.cn, {mutiyama, eiichiro.sumita}, zhaohai@cs.sjtu.edu.cn Abstract control over translation output, which is effective in a variety of translation settings, including interactive machine translation (Peris et al., 2017) and domain adaptation (Chu and Wang, 2018), which is also crucial for the practical application of NMT. Since there is still a need for manual interventions for the new NMT paradigm, much effort is spent in studying how to incorporate this explicit control into the end-to-end neural translation (Arthur et al., 2016). Among these efforts, Constrained Decoding (CD) has gained a lot of attention in this research field, which is a modification to commonly adopted beam search in ordinary NMT models. Hokamp and Liu (2017) proposed grid beam search, which expands beam search to include pre-specified lexical constraints. Anderson et al. (2017"
2021.wat-1.4,J10-4005,0,0.0298736,"ese sampled positions to achieve the goal of restrict translation with soft constraints on the model: Convk (H) = DepthConvk (HW V )W out DepthConvk (H) = k X Softmax( d X Q Wj,c Hi,c ) c=1 j=1  · Hi+j−d k+1 e,c , 2 Conv(H) = K X i=1 LSCC = − exp (αi ) Convki (X) n P exp (αj ) (1 + γ 1(yi ∈ C)) i=1  logP (yi |X; C; y<i ; θ) , j=1 where 1(·) is the indicator function and γ is the penalty factor. in which DepthConv(·) is the depth convolution structure proposed in Wu et al. (2019). And Pointwise(·) refers to a position-wise feed-forward network: 2.4 Lexically Constrained Decoding Beam search (Koehn, 2010) is a common approximate search algorithm for sequence generation task. Lexically constrained decoding is a modification to the beam search algorithm, which is proposed to enforce hard constraints that force a given constrained sequence to appear in the generated sequence. Specifically, beam search maintains a beam Bt on time step t, which contains only the b most likely partial sequences, where b is known as the beam size. The beam Bt is updated by retaining the b most likely sequences in the candidate set Et generated by considering all possible next word predictions: Pointwise(H) = max(0, H"
2021.wat-1.4,N03-1017,0,0.112871,"Missing"
2021.wat-1.4,W19-6721,0,0.0563198,"Missing"
2021.wat-1.4,D18-1262,1,0.831361,"ECT En→Ja test sets. ∗ indicates that the official evaluation results are reported. Dataset Sentences ParaCrawl-v5.1 Wiki Titles v2 ASPEC 10.12M 3.64M 3.01M Table 2: Training data statistics. [  s0 (Yˆt−1 , w) |Yˆt−1 ∈ Bt−1 , w ∈ V, δ(s0 , w) = s , where δ : S × V 7→ S is the FSM state-transition function that maps states and predicted words to states. System Details Our implementation of the Transformer models and lexically constrained decoding algorithm are based on the Fairseq toolkit1 . We follow the settings and pre-processing methods in our previous models and systems (He et al., 2018; Li et al., 2018; He et al., 2019; Li et al., 2019; Zhou et al., 2020; Li et al., 2020b,d,c; Zhang et al., 2020). We use Transformer-big as our basic model, which has 6 layers in both the encoder and decoder, respectively. For each layer, it consists of a multi-head attention sublayer with 16 heads and a feed-forward sublayer with an inner dimension 4096. The word embedding dimensions and the hidden state dimensions are set to 1024 for both the encoder and decoder. In the training phase, the dropout rate is set to 0.1. Our model training consists of two phases. In the first NMT pre-training phase, the ParaCra"
2021.wat-1.4,D19-1538,1,0.835069,"ts. ∗ indicates that the official evaluation results are reported. Dataset Sentences ParaCrawl-v5.1 Wiki Titles v2 ASPEC 10.12M 3.64M 3.01M Table 2: Training data statistics. [  s0 (Yˆt−1 , w) |Yˆt−1 ∈ Bt−1 , w ∈ V, δ(s0 , w) = s , where δ : S × V 7→ S is the FSM state-transition function that maps states and predicted words to states. System Details Our implementation of the Transformer models and lexically constrained decoding algorithm are based on the Fairseq toolkit1 . We follow the settings and pre-processing methods in our previous models and systems (He et al., 2018; Li et al., 2018; He et al., 2019; Li et al., 2019; Zhou et al., 2020; Li et al., 2020b,d,c; Zhang et al., 2020). We use Transformer-big as our basic model, which has 6 layers in both the encoder and decoder, respectively. For each layer, it consists of a multi-head attention sublayer with 16 heads and a feed-forward sublayer with an inner dimension 4096. The word embedding dimensions and the hidden state dimensions are set to 1024 for both the encoder and decoder. In the training phase, the dropout rate is set to 0.1. Our model training consists of two phases. In the first NMT pre-training phase, the ParaCrawlv5.1 (Espl`a et"
2021.wat-1.4,D18-1458,0,0.0396717,"Missing"
2021.wat-1.4,2020.wmt-1.22,1,0.828689,"Missing"
2021.wat-1.4,2020.findings-emnlp.371,1,0.890157,"Missing"
2021.wat-1.4,K19-2004,1,0.848833,"hat the official evaluation results are reported. Dataset Sentences ParaCrawl-v5.1 Wiki Titles v2 ASPEC 10.12M 3.64M 3.01M Table 2: Training data statistics. [  s0 (Yˆt−1 , w) |Yˆt−1 ∈ Bt−1 , w ∈ V, δ(s0 , w) = s , where δ : S × V 7→ S is the FSM state-transition function that maps states and predicted words to states. System Details Our implementation of the Transformer models and lexically constrained decoding algorithm are based on the Fairseq toolkit1 . We follow the settings and pre-processing methods in our previous models and systems (He et al., 2018; Li et al., 2018; He et al., 2019; Li et al., 2019; Zhou et al., 2020; Li et al., 2020b,d,c; Zhang et al., 2020). We use Transformer-big as our basic model, which has 6 layers in both the encoder and decoder, respectively. For each layer, it consists of a multi-head attention sublayer with 16 heads and a feed-forward sublayer with an inner dimension 4096. The word embedding dimensions and the hidden state dimensions are set to 1024 for both the encoder and decoder. In the training phase, the dropout rate is set to 0.1. Our model training consists of two phases. In the first NMT pre-training phase, the ParaCrawlv5.1 (Espl`a et al., 2019) and W"
2021.wat-1.4,N18-1119,0,0.0866255,"usion of restricted words in the output, and employed fixed pre-trained word embeddings to facilitate vocabulary expansion to unseen words in training. While these works accomplish the goal of explicit translation control, the time complexity of their decoding algorithm and resultant decoding speed falls short of the expectations. The complexity of grid beam search and constrained beam search is linear and exponential to the number of constraints, respectively. These algorithms are thus too inefficient to be practical for large-scale use. To alleviate the shortcomings in constrained decoding, Post and Vilar (2018) proposed a new constrained decoding algorithm with a claimed complexity of O(1) in the number of constraints - dynamic beam allocation which allocates the slots in a fixed-size beam. However, their approach still processes sentence constraints sequentially rather than batch processing, limiting the GPU’s parallel processing capabilities. Based on Post and Vilar (2018), a vectorized dynamic beam allocation approach was proposed in Hu et al. (2019), which which vectorThis paper describes our system (Team ID: nictrb) for participating in the WAT’21 restricted machine translation task. In our sub"
2021.wat-1.4,2020.findings-emnlp.398,1,0.72695,"evaluation results are reported. Dataset Sentences ParaCrawl-v5.1 Wiki Titles v2 ASPEC 10.12M 3.64M 3.01M Table 2: Training data statistics. [  s0 (Yˆt−1 , w) |Yˆt−1 ∈ Bt−1 , w ∈ V, δ(s0 , w) = s , where δ : S × V 7→ S is the FSM state-transition function that maps states and predicted words to states. System Details Our implementation of the Transformer models and lexically constrained decoding algorithm are based on the Fairseq toolkit1 . We follow the settings and pre-processing methods in our previous models and systems (He et al., 2018; Li et al., 2018; He et al., 2019; Li et al., 2019; Zhou et al., 2020; Li et al., 2020b,d,c; Zhang et al., 2020). We use Transformer-big as our basic model, which has 6 layers in both the encoder and decoder, respectively. For each layer, it consists of a multi-head attention sublayer with 16 heads and a feed-forward sublayer with an inner dimension 4096. The word embedding dimensions and the hidden state dimensions are set to 1024 for both the encoder and decoder. In the training phase, the dropout rate is set to 0.1. Our model training consists of two phases. In the first NMT pre-training phase, the ParaCrawlv5.1 (Espl`a et al., 2019) and Wiki Titles v2 datas"
2021.wnut-1.9,2021.findings-acl.84,0,0.0123695,"ons that aggregate character-level edit operations. Recently, text editing models based on Transformer and BERT (Malmi et al., 2019; Mallinson et al., 2020; Stahlberg and Kumar, 2020) have been proposed for monolingual sequence transduction tasks, such as grammatical error correction and text normalization for speech synthesis, because of their sample-efficient and fast inference characteristics 74 compared to sequence-to-sequence models. References Data Synthesis. Data synthesis and augmentation methods have been explored for various NLP tasks, to increase the diversity of training examples (Feng et al., 2021) and for lexical normalization to address the deficiency of training data. Ikeda et al. (2016) synthesized Japanese formal-informal sentence pairs by hand-crafted rules to convert standard forms to nonstandard forms. Zhang et al. (2017) synthesized training data for Chinese informal word detection by random substitution of formal words in segmented sentences by informal words in a dictionary of formal-informal word pairs. To train statistical and neural MT models for Turkish text normalization, Çolako˘glu et al. (2019) generated a pseudo-parallel corpus where nonstandard words in original twee"
2021.wnut-1.9,2021.naacl-main.438,1,0.897268,"ers. For this reason, the problem of Japanese lexical normalization has been solved by predicting word boundaries, part-of-speech (POS) tags, and normalized word forms simultaneously (Sasano et al., 2013; Saito et al., 2014). Similarly to previous work, we tackle the joint task comprising Japanese word Segmentation, POS tagging, and lexical Normalization (SPN). A critical problem in lexical normalization is the lack of labeled data. Manual annotation of normalized forms is a time-consuming task; therefore, the size of the available annotated corpora is quite small (Kaji and Kitsuregawa, 2014; Higashiyama et al., 2021). A prospective solution to this problem 2 Task Definition As shown in Table 2, a training instance for the SPN task is defined as a pair, comprising a sentence x = (x1 , . . . , xn ) and its label sequence t = {(fj , lj , pj , Sj )}m j=1 , where n and m (≤ n) are the numbers of characters and words in x, fj and lj are the indexes of the first and last character in j-th word wj , and pj is the POS tag of wj . The set of standard forms Sj is equal to the empty set ∅ when wj is a standard form, whereas Sj consists of one or more standard forms when wj is a nonstandard form. A system is required"
2021.wnut-1.9,2020.findings-emnlp.111,0,0.0245681,"(2008) extracted formal-informal word pairs using websearched sentences defining informal words and a conditional log-linear ranking model. Wang and Text Editing. Text editing methods have also been applied to English lexical normalization. Chrupała (2014) used character embeddings based on a recurrent neural network LM and trained CRFs to predict character-level edit operations. Min and Mott (2015) proposed an LSTM-based model to perform word-level edit operations that aggregate character-level edit operations. Recently, text editing models based on Transformer and BERT (Malmi et al., 2019; Mallinson et al., 2020; Stahlberg and Kumar, 2020) have been proposed for monolingual sequence transduction tasks, such as grammatical error correction and text normalization for speech synthesis, because of their sample-efficient and fast inference characteristics 74 compared to sequence-to-sequence models. References Data Synthesis. Data synthesis and augmentation methods have been explored for various NLP tasks, to increase the diversity of training examples (Feng et al., 2021) and for lexical normalization to address the deficiency of training data. Ikeda et al. (2016) synthesized Japanese formal-informal sente"
2021.wnut-1.9,P82-1020,0,0.785309,"Missing"
abekawa-etal-2010-community,abekawa-kageura-2008-constructing,1,\N,Missing
abekawa-etal-2010-community,P01-1008,0,\N,Missing
abekawa-etal-2010-community,P07-2002,1,\N,Missing
abekawa-etal-2010-community,I08-1032,1,\N,Missing
abekawa-etal-2010-community,2007.mtsummit-papers.60,1,\N,Missing
C00-2128,P99-1008,0,0.0150643,"in corpora, i.e. noun phrases of the form A no B can be found in corpora but their semantic relations are not. If we need such semantic relations, we must semantically analyze the noun phrases (Kurohashi and Sakai, 1999). Applicability to other languages Japanese noun phrases of the form A no B are specific to Japanese. The proposed method, however, could easily be extended to other languages. For example, in English, noun phrases B of A could be used to extract semantically related nouns. Nouns related by is-a relations or part-of relations could also be extracted from corpora (Hearst, 1992; Berland and Charniak, 1999). If such semantically related nouns are extracted, Candidates beer, cola, mizu (water) yu (hot water), oyu (hot water), nettˆo (boiling water) zyˆoyˆ osya (car), best seller, kuruma (vehicle) e (painting), image,aizin (lover) gensaku (original work), meisaku (famous story), daihyˆosaku (important work) menuetto (minuet), kyoku (music), piano si (poem), tyosyo (writings), tyosaku (writings) care,kyˆ usoku (rest), kaigo (nursing) hito (person),tomodati (friend), byˆonin (sick person) Nihon (Japan),ziko (accident), kigyˆo (company) zikanho (assistant vice-minister), seikai (political world), gik"
C00-2128,C96-1025,0,0.271611,"rocessing applications, especially for machine translation (Kamei and Wakao, 1992; Fass, 1997). A metonymy may be acceptable in a source language but unacceptable in a target language. For example, a direct translation of ‘he read Mao’, which is acceptable in English and Japanese, is completely unacceptable in Chinese (Kamei and Wakao, 1992). In such cases, the machine translation system has to interpret metonymies to generate acceptable translations. Previous approaches to processing metonymy have used hand-constructed ontologies or semantic networks (Fass, 1988; Iverson and Helmreich, 1992; Bouaud et al., 1996; Fass, 1997).1 1 As for metaphor processing, Ferrari (1996) used texSuch approaches are restricted by the knowledge bases they use, and may only be applicable to domain-specific tasks because the construction of large knowledge bases could be very difficult. The method outlined in this paper, on the other hand, uses corpus statistics to interpret metonymy, so that a variety of metonymies can be handled without using hand-constructed knowledge bases. The method is quite promising as shown by the experimental results given in section 5. 2 Recognition and Interpretation Two main steps, recogniti"
C00-2128,J92-4003,0,0.0133448,"below).3 Examples of these and similar types of metonymic concepts (Lakoff and Johnson, 1980; Fass, 1997) are given below. Container for contents • glass no mizu (water) • nabe (pot) no ryˆ ori (food) Artist for artform • Beethoven no kyoku (music) • Picasso no e (painting) Object for user • ham sandwich no kyaku (customer) • sax no sˆosya (performer) Whole for part • kuruma (car) no tire Information Source We use a large corpus to extract nouns which can be syntactically related to the explicit term of a metonymy. A large corpus is valuable as a source of such nouns (Church and Hanks, 1990; Brown et al., 1992). We used Japanese noun phrases of the form A no B to extract nouns that were syntactically related to A. Nouns in such a syntactic relation are usually close semantic relatives of each other (Murata et al., 1999), and occur relatively infrequently. We thus also used an A near B relation, i.e. identifying the other nouns within the target sentence, to extract nouns that may be more loosely related to A, but occur more frequently. These two types of syntactic relation are treated differently by the statistical measure which we will discuss in section 4. The Japanese noun phrase A no B roughly c"
C00-2128,J90-1003,0,0.0136441,"s and artist for artform below).3 Examples of these and similar types of metonymic concepts (Lakoff and Johnson, 1980; Fass, 1997) are given below. Container for contents • glass no mizu (water) • nabe (pot) no ryˆ ori (food) Artist for artform • Beethoven no kyoku (music) • Picasso no e (painting) Object for user • ham sandwich no kyaku (customer) • sax no sˆosya (performer) Whole for part • kuruma (car) no tire Information Source We use a large corpus to extract nouns which can be syntactically related to the explicit term of a metonymy. A large corpus is valuable as a source of such nouns (Church and Hanks, 1990; Brown et al., 1992). We used Japanese noun phrases of the form A no B to extract nouns that were syntactically related to A. Nouns in such a syntactic relation are usually close semantic relatives of each other (Murata et al., 1999), and occur relatively infrequently. We thus also used an A near B relation, i.e. identifying the other nouns within the target sentence, to extract nouns that may be more loosely related to A, but occur more frequently. These two types of syntactic relation are treated differently by the statistical measure which we will discuss in section 4. The Japanese noun ph"
C00-2128,C88-1036,0,0.0563287,"metonymy is vital for natural language processing applications, especially for machine translation (Kamei and Wakao, 1992; Fass, 1997). A metonymy may be acceptable in a source language but unacceptable in a target language. For example, a direct translation of ‘he read Mao’, which is acceptable in English and Japanese, is completely unacceptable in Chinese (Kamei and Wakao, 1992). In such cases, the machine translation system has to interpret metonymies to generate acceptable translations. Previous approaches to processing metonymy have used hand-constructed ontologies or semantic networks (Fass, 1988; Iverson and Helmreich, 1992; Bouaud et al., 1996; Fass, 1997).1 1 As for metaphor processing, Ferrari (1996) used texSuch approaches are restricted by the knowledge bases they use, and may only be applicable to domain-specific tasks because the construction of large knowledge bases could be very difficult. The method outlined in this paper, on the other hand, uses corpus statistics to interpret metonymy, so that a variety of metonymies can be handled without using hand-constructed knowledge bases. The method is quite promising as shown by the experimental results given in section 5. 2 Recogn"
C00-2128,P96-1048,0,0.0173931,"and Wakao, 1992; Fass, 1997). A metonymy may be acceptable in a source language but unacceptable in a target language. For example, a direct translation of ‘he read Mao’, which is acceptable in English and Japanese, is completely unacceptable in Chinese (Kamei and Wakao, 1992). In such cases, the machine translation system has to interpret metonymies to generate acceptable translations. Previous approaches to processing metonymy have used hand-constructed ontologies or semantic networks (Fass, 1988; Iverson and Helmreich, 1992; Bouaud et al., 1996; Fass, 1997).1 1 As for metaphor processing, Ferrari (1996) used texSuch approaches are restricted by the knowledge bases they use, and may only be applicable to domain-specific tasks because the construction of large knowledge bases could be very difficult. The method outlined in this paper, on the other hand, uses corpus statistics to interpret metonymy, so that a variety of metonymies can be handled without using hand-constructed knowledge bases. The method is quite promising as shown by the experimental results given in section 5. 2 Recognition and Interpretation Two main steps, recognition and interpretation, are involved in the processing of met"
C00-2128,C92-2082,0,0.00414663,"tly expressed in corpora, i.e. noun phrases of the form A no B can be found in corpora but their semantic relations are not. If we need such semantic relations, we must semantically analyze the noun phrases (Kurohashi and Sakai, 1999). Applicability to other languages Japanese noun phrases of the form A no B are specific to Japanese. The proposed method, however, could easily be extended to other languages. For example, in English, noun phrases B of A could be used to extract semantically related nouns. Nouns related by is-a relations or part-of relations could also be extracted from corpora (Hearst, 1992; Berland and Charniak, 1999). If such semantically related nouns are extracted, Candidates beer, cola, mizu (water) yu (hot water), oyu (hot water), nettˆo (boiling water) zyˆoyˆ osya (car), best seller, kuruma (vehicle) e (painting), image,aizin (lover) gensaku (original work), meisaku (famous story), daihyˆosaku (important work) menuetto (minuet), kyoku (music), piano si (poem), tyosyo (writings), tyosaku (writings) care,kyˆ usoku (rest), kaigo (nursing) hito (person),tomodati (friend), byˆonin (sick person) Nihon (Japan),ziko (accident), kigyˆo (company) zikanho (assistant vice-minister),"
C00-2128,P92-1047,0,0.423418,"on Metonymy is a figure of speech in which the name of one thing is substituted for that of something to which it is related. The explicit term is ‘the name of one thing’ and the implicit term is ’the name of something to which it is related’. A typical example of metonymy is He read Shakespeare. (1) ‘Shakespeare’ is substituted for ‘the works of Shakespeare’. ‘Shakespeare’ is the explicit term and ‘works’ is the implicit term. Metonymy is pervasive in natural language. The correct treatment of metonymy is vital for natural language processing applications, especially for machine translation (Kamei and Wakao, 1992; Fass, 1997). A metonymy may be acceptable in a source language but unacceptable in a target language. For example, a direct translation of ‘he read Mao’, which is acceptable in English and Japanese, is completely unacceptable in Chinese (Kamei and Wakao, 1992). In such cases, the machine translation system has to interpret metonymies to generate acceptable translations. Previous approaches to processing metonymy have used hand-constructed ontologies or semantic networks (Fass, 1988; Iverson and Helmreich, 1992; Bouaud et al., 1996; Fass, 1997).1 1 As for metaphor processing, Ferrari (1996) u"
C00-2128,P99-1062,0,0.0621513,"lly related to A. Nouns in such a syntactic relation are usually close semantic relatives of each other (Murata et al., 1999), and occur relatively infrequently. We thus also used an A near B relation, i.e. identifying the other nouns within the target sentence, to extract nouns that may be more loosely related to A, but occur more frequently. These two types of syntactic relation are treated differently by the statistical measure which we will discuss in section 4. The Japanese noun phrase A no B roughly corresponds to the English noun phrase B of A, but it has a much broader range of usage (Kurohashi and Sakai, 1999). In fact, A no B can express most of the possible types of semantic relation between two nouns including metonymic • door no knob These examples suggest that we can extract semantically related nouns by using the A no B relation. 4 Statistical Measure A metonymy ‘Noun A Case-Marker R Predicate V ’ can be regarded as a contraction of ‘Noun A Syntactic-Relation Q Noun B CaseMarker R Predicate V ’, where A has relation Q to B (Yamamoto et al., 1998). For example, Shakespeare wo yomu (read) (read Shakespeare) is regarded as a contraction of Shakespeare no sakuhin (works) wo yomu (read the works o"
C00-2128,W99-0205,1,0.768329,"artform • Beethoven no kyoku (music) • Picasso no e (painting) Object for user • ham sandwich no kyaku (customer) • sax no sˆosya (performer) Whole for part • kuruma (car) no tire Information Source We use a large corpus to extract nouns which can be syntactically related to the explicit term of a metonymy. A large corpus is valuable as a source of such nouns (Church and Hanks, 1990; Brown et al., 1992). We used Japanese noun phrases of the form A no B to extract nouns that were syntactically related to A. Nouns in such a syntactic relation are usually close semantic relatives of each other (Murata et al., 1999), and occur relatively infrequently. We thus also used an A near B relation, i.e. identifying the other nouns within the target sentence, to extract nouns that may be more loosely related to A, but occur more frequently. These two types of syntactic relation are treated differently by the statistical measure which we will discuss in section 4. The Japanese noun phrase A no B roughly corresponds to the English noun phrase B of A, but it has a much broader range of usage (Kurohashi and Sakai, 1999). In fact, A no B can express most of the possible types of semantic relation between two nouns inc"
C00-2129,W99-0211,0,\N,Missing
C00-2129,P98-1012,0,\N,Missing
C00-2129,C98-1012,0,\N,Missing
C00-2129,P99-1071,0,\N,Missing
C00-2129,P98-2151,1,\N,Missing
C00-2129,C98-2146,1,\N,Missing
C16-1291,D16-1162,0,0.0517274,"NMT1 and NMT2 are much worse than Moses with a substantial gap. This result is not difficult to understand: neural network systems typically require sufficient data to boost their performance, and thus low resource translation tasks are very challenging for them. Secondly, the proposed SA-NMT gains much over NMT2 similar to the case in the large scale task, and the gap towards Moses is narrowed substantially. While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work. 5 Related Work Many recent works have led to notable improvements in the attention mechanism for neural machine trans"
C16-1291,J16-2001,0,0.0929956,"ved by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006; Koehn et al., 2007; Bisazza and Federico, 2016). Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the This work is licensed under a Creative Commons Attribution 4.0 International Licence. http://crea"
C16-1291,J93-2003,0,0.0944848,"igures out those source words will be translated next, even though the next target word yt is unavailable. From this point of view, the attention mechanism plays a role in reordering and thus can be considered as a reordering model. Unlike this attention model, conventional alignment models define the alignment α directly over x and y as follows: exp(F (x, y, α)) 0 α0 exp(F (x, y, α )) p(α |x, y) = P where F denotes a feature function over a pair of sentences x and y together with their word alignment α, and it is either a log-probability log p(y, α |x) for a generative model like IBM models (Brown et al., 1993) or a well-designed feature function for discriminative models (Liu and Sun, 2015). In order to infer αt , alignment models can readily use the entire y, of course including yt as well, thereby they can model the alignment between x and y more sufficiently. As a result, the attention based NMT might not deliver satisfying alignments, as reported in (Cheng et al., 2016), compared to conventional alignment models. This may be a sign that the potential of attention-based NMT is limited in end-to-end translation. 3 Supervised Attention In this section, we introduce supervised attention to improve"
C16-1291,2016.amta-researchers.10,0,0.330705,"vel reordering model. The main difference is that in our approach the reordering model and translation model are trained jointly rather than separately as theirs. Supervising the attention variables for attention-based neural networks is pioneered by Liu et al. 3100 (2016). On image caption task, Liu et al. (2016) supervise the attention with external guidances in either a strong or a weak supervision manner. Their method requires the training data to be associated with direct annotation or indirect annotation. In parallel to our work, particularly on machine translation, Mi et al. (2016) and Chen et al. (2016) guide the attention for NMT from conventional word alignment models as teachers without any annotation on machine translation task. The differences of our work lie in that: we consider the attention as a form of a reordering model, which is thereby straightforward to be learned from conventional word alignment models; and we also provide a theoretical explanation why the attention leads to the worse alignment accuracy than the conventional word alignment models, standing upon the point view of reordering. 6 Conclusion It has been shown that attention mechanism in NMT is worse than conventiona"
C16-1291,P14-1129,0,0.0162995,"e original paper, αt is not explicitly dependent on the yt−1 in Eq.(2), but this dependency was explicitly retained in our direct baseline NMT2. 5 Although the alignment is loosely related to the downstream translation (Liu and Sun, 2015), substantial improvements in alignment usually leads to the improvements in translation as observed in our experiments. 3095 Therefore, we apply the following heuristics to preprocess the hard alignment: if a target word does not align to any source words, we inherit its affiliation from the closest aligned word with preference given to the right, following (Devlin et al., 2014); if a target word is aligned to multiple source words, we assume it aligns to each one evenly. In addition, in the implementation of NMT, there are two special tokens ‘eol’ added to both source and target sentences. We assume they are aligned to each other. In this way, we can obtain the final supervision of attention, denoted as α ˆ. 3.2 Jointly Supervising Translation and Attention We propose a soft constraint method to jointly supervise the translation and attention as follows: − X log p(yi |xi ; θ) + λ × ∆(αi , α ˆ i ; θ) (4) i where αi is as defined in Eq. (1), ∆ is a loss function that"
C16-1291,N13-1073,0,0.108639,"allowing the use of fewer hidden layers while still maintaining high levels of translation performance. An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more details). In contrast, conventional alignment models are able to use the target word to infer its alignments (Och and Ney, 2000; Dyer et al., 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervise"
C16-1291,P07-2045,0,0.0608892,"the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006; Koehn et al., 2007; Bisazza and Federico, 2016). Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the This work is licensed under a Creative Commons Attribution 4.0 Inter"
C16-1291,N06-1014,0,0.0172418,"because the supervision of α is more close to Ex than y as in Figure 1(b). In order to quantify the disagreement between αi and α ˆ i , three different methods are investigated in our experiments: • Mean Squared Error (MSE) ∆(αi , α ˆ i ; θ) = XX 1 m 2 n i α(θ)im,n − α ˆ m,n 2 MSE is widely used as a loss for regression tasks (Lehmann and Casella, 1998), and it directly i encourages α(θ)im,n to be equal to α ˆ m,n . • Multiplication (MUL) ∆(αi , α ˆ i ; θ) = − log XX m n i α(θ)im,n × α ˆ m,n  MUL is particularly designed for agreement in word alignment and it has been shown to be effective (Liang et al., 2006; Cheng et al., 2016). Note that different from those in (Cheng et al., 2016), α ˆ is not a parametrized variable but a constant in this paper. • Cross Entropy (CE) ∆(αi , α ˆ i ; θ) = − XX m n i α ˆ m,n × log α(θ)im,n Since for each t, α(θ)t is a distribution, it is natural to use CE as the metric to evaluate the disagreement (Rubinstein and Kroese, 2004). 4 Experiments We conducted experiments on two Chinese-to-English translation tasks: one is the NIST task oriented to NEWS domain, which is a large scale task and suitable to NMT; and the other is the speech translation oriented to travel do"
C16-1291,2015.iwslt-evaluation.11,0,0.28045,"1. The first row shows the alignments of the sentence pair from the training set while the second row shows the alignments from test sets. Methods GIZA++ NMT2 SA-NMT AER 30.6∗ 50.6 43.3∗ Table 4: Results on word alignment task for the large scale data. The evaluation metric is Alignment Error Rate (AER). ‘*’ denotes that the corresponding result is significanly better than NMT2 with p &lt; 0.01. Table 4 shows the overall alignment results on word alignment task in terms of the metric, alignment error rate. We used the manually-aligned dataset as in (Liu and Sun, 2015) as the test set. Following (Luong and Manning, 2015), we force-decode both the bilingual sentences including source and reference sentences to obtain the alignment matrices, and then for each target word we extract one-to-one alignments by picking up the source word with the highest alignment confidence as the hard alignment. From Table 4, we can see clearly that standard NMT (NMT2) is far behind GIZA++ in alignment quality. This shows that it is possible and promising to supervise the attention with GIZA++. With the help from GIZA++, our supervised attention based NMT (SA-NMT) significantly reduces the AER, compared with the unsupervised count"
C16-1291,D15-1166,0,0.100348,"ver the standard attention based NMT. 1 Introduction Neural Machine Translation (NMT) has achieved great successes on machine translation tasks recently (Bahdanau et al., 2015; Sutskever et al., 2015). Generally, it relies on a recurrent neural network under the Encode-Decode framework: it firstly encodes a source sentence into context vectors and then generates its translation token-by-token, selecting from the target vocabulary. Among different variants of NMT, attention based NMT, which is the focus of this paper,1 is attracting increasing interests in the community (Bahdanau et al., 2015; Luong et al., 2015). One of its advantages is that it is able to dynamically make use of the encoded context through an attention mechanism thereby allowing the use of fewer hidden layers while still maintaining high levels of translation performance. An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more"
C16-1291,D16-1249,0,0.218565,"network based word-level reordering model. The main difference is that in our approach the reordering model and translation model are trained jointly rather than separately as theirs. Supervising the attention variables for attention-based neural networks is pioneered by Liu et al. 3100 (2016). On image caption task, Liu et al. (2016) supervise the attention with external guidances in either a strong or a weak supervision manner. Their method requires the training data to be associated with direct annotation or indirect annotation. In parallel to our work, particularly on machine translation, Mi et al. (2016) and Chen et al. (2016) guide the attention for NMT from conventional word alignment models as teachers without any annotation on machine translation task. The differences of our work lie in that: we consider the attention as a form of a reordering model, which is thereby straightforward to be learned from conventional word alignment models; and we also provide a theoretical explanation why the attention leads to the worse alignment accuracy than the conventional word alignment models, standing upon the point view of reordering. 6 Conclusion It has been shown that attention mechanism in NMT is"
C16-1291,W15-5003,0,0.0260557,"Moses is narrowed substantially. While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work. 5 Related Work Many recent works have led to notable improvements in the attention mechanism for neural machine translation. Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT. Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies. Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn"
C16-1291,P00-1056,0,0.102849,"mechanism thereby allowing the use of fewer hidden layers while still maintaining high levels of translation performance. An attention mechanism is designed to predict the alignment of a target word with respect to source words (Bahdanau et al., 2015). In order to facilitate incremental decoding, it tries to make this alignment prediction without the information about the target word itself, and thus this attention can be considered to be a form of a reordering model (see §2 for more details). In contrast, conventional alignment models are able to use the target word to infer its alignments (Och and Ney, 2000; Dyer et al., 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering mo"
C16-1291,P14-1138,1,0.532331,"). This shows that the proposed approach is able to realize our intuition: the alignment is improved, leading to better translation performance. Note that there is still a gap between SA-NMT and GIZA++ as indicated in Table 4. Since SA-NMT was trained for machine translation instead of word alignment, it is possible to reduce its AER if we aim to the word alignment task only. For example, we can enlarge λ in Eq.(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014). 3099 Systems Moses NMT1 NMT2 SA-NMT CSTAR03 44.1 33.4 36.5 39.8∗ IWSLT04 45.1 33.0 35.9 40.7∗ Table 5: BLEU comparison for low-resource translation task. CSTAR03 is the development set while IWSLT04 is the test set. ‘*’ denotes that SA-NMT is significantly better than both NMT1 and NMT2 with p &lt; 0.01. 4.2 Results on the Low Resource Translation Task For the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 and IWSLT04 held out set"
C16-1291,P16-1008,0,0.0286294,"over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size. Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work. 5 Related Work Many recent works have led to notable improvements in the attention mechanism for neural machine translation. Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT. Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies. Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itse"
C16-1291,P06-1066,0,0.0297014,"p in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-to-English as reported in (Cheng et al., 2016)). This discrepancy might be an indication that the potential of attentionbased NMT is limited. In addition, the attention in NMT is learned in an unsupervised manner without explicit prior knowledge about alignment.2 However, in conventional statistical machine translation (SMT), it is standard practice to learn reordering models in a supervised manner with the guidance from conventional alignment models (Xiong et al., 2006; Koehn et al., 2007; Bisazza and Federico, 2016). Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run offthe-shelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the This work is licensed under a Creative Commons A"
C16-1291,P13-1017,0,0.0104401,"d counterpart (NMT2). This shows that the proposed approach is able to realize our intuition: the alignment is improved, leading to better translation performance. Note that there is still a gap between SA-NMT and GIZA++ as indicated in Table 4. Since SA-NMT was trained for machine translation instead of word alignment, it is possible to reduce its AER if we aim to the word alignment task only. For example, we can enlarge λ in Eq.(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014). 3099 Systems Moses NMT1 NMT2 SA-NMT CSTAR03 44.1 33.4 36.5 39.8∗ IWSLT04 45.1 33.0 35.9 40.7∗ Table 5: BLEU comparison for low-resource translation task. CSTAR03 is the development set while IWSLT04 is the test set. ‘*’ denotes that SA-NMT is significantly better than both NMT1 and NMT2 with p &lt; 0.01. 4.2 Results on the Low Resource Translation Task For the low resource translation task, we used the BTEC corpus as the training data, which consists of 30k sentence pairs with 0.27M Chinese words and 0.33M English words. As development and test sets, we used the CSTAR03 an"
C16-1291,C14-1179,0,\N,Missing
C16-1295,D11-1033,0,0.575583,"corpora, which are also called in-domain or related-domain corpora, can enhance the performance of SMT effectively. Otherwise the irrelevant additional corpora, which are also called outof-domain corpora, may not benefit SMT (Koehn and Schroeder, 2007). SMT adaptation means selecting useful part from mix-domain (mixture of in-domain and out-ofdomain) data, for SMT performance enhancement. The core task in adaptation is about how to select the useful data. Existing works have considered selection strategies with various granularities, though most of them only focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al."
C16-1295,2011.iwslt-evaluation.18,0,0.251266,"ion probabilities of connecting phrases calculated by NN can also be used to enhance SMT, and the experimental results will be shown in Section 5.4. 3.3 Integration into SMT The thresholds of Pop and Dminus are tuned using development data. Selected phrase pairs are added into the in-domain PT. Because they are not so useful as the in-domain ones, a penalty score is added. For in-domain phrase pairs, the penalty is set as 1; for the out-of-domain ones the penalty is set as e (= 2.71828...). Other phrase scores (lexical weights et. al.) are used as they are. This penalty setting is similar to (Bisazza et al., 2011). Penalty weights, together with all of existing score weights, will be further tuned by MERT (Och, 2003). The phrase pairs in re-ordering model are selected using the same way as PT. The selected monolingual n-grams are added to the original LM, and the probabilities are re-normalized by SRILM (Stolcke, 2002; Stolcke et al., 2011). 4 Experiments 4.1 Data sets The proposed methods are evaluated on two data sets. 1) IWSLT 2014 French (FR) to English (EN) corpus4 is used as in-domain data and dev2010 and test2010/2011 (Niehues and Waibel, 2012), are selected as development (dev) and test data, r"
C16-1295,P16-1039,1,0.833935,"Missing"
C16-1295,P13-1141,0,0.0645321,"Missing"
C16-1295,N13-1114,0,0.0207534,"Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific mo"
C16-1295,P13-1126,0,0.0188404,"Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific mo"
C16-1295,P08-1010,0,0.178554,"focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and mono"
C16-1295,P13-2119,0,0.636704,"elated-domain corpora, can enhance the performance of SMT effectively. Otherwise the irrelevant additional corpora, which are also called outof-domain corpora, may not benefit SMT (Koehn and Schroeder, 2007). SMT adaptation means selecting useful part from mix-domain (mixture of in-domain and out-ofdomain) data, for SMT performance enhancement. The core task in adaptation is about how to select the useful data. Existing works have considered selection strategies with various granularities, though most of them only focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et"
C16-1295,2015.mtsummit-papers.10,0,0.834828,"ild lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B. L. Lu were partially supported by Cai Yuanpei Program (CSC No. 201304490199 and 201304490171), National Natural Science Foundation of China (No. 61672343, 61170114 and 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shanghai Science and Technolog"
C16-1295,D10-1044,0,0.0735489,"et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the abov"
C16-1295,D14-1062,0,0.238959,"Missing"
C16-1295,C14-1182,0,0.301219,"Missing"
C16-1295,D15-1147,0,0.484005,"selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B. L. Lu were partially supported by Cai Yuanpei Program (CSC No. 201304490199 and 201304490171), National Natural Science Foundation of China (No. 61672343, 61170114 and 61272248), National Basic Research Program of China (No. 2013CB329401), Major Basic Research Program of Shangha"
C16-1295,W07-0733,0,0.496687,"ed method is evaluated on IWSLT/NIST data sets, and the results show that phrase based SMT performances are significantly improved (up to +1.6 in comparison with phrase based SMT baseline system and +0.9 in comparison with existing methods). 1 Introduction Large corpora are important for Statistical Machine Translation (SMT) training. However only the relevant additional corpora, which are also called in-domain or related-domain corpora, can enhance the performance of SMT effectively. Otherwise the irrelevant additional corpora, which are also called outof-domain corpora, may not benefit SMT (Koehn and Schroeder, 2007). SMT adaptation means selecting useful part from mix-domain (mixture of in-domain and out-ofdomain) data, for SMT performance enhancement. The core task in adaptation is about how to select the useful data. Existing works have considered selection strategies with various granularities, though most of them only focus on sentence-level selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it i"
C16-1295,W04-3250,0,0.163106,"methods have been applied to a series of NLP tasks, such as Chinese word segmentation and parsing (Cai and Zhao, 2016; Zhang et al., 2016). 4 https://wit3.fbk.eu/mt.php?release=2014-01 5 http://statmt.org/wmt15/translation-task.html 6 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 3138 4.2 Common Setting The basic settings of IWSLT-2014 FR to EN and NIST-06 CN to EN phrase based translation baseline systems are followed. 5-gram interpolated KN (Kneser and Ney, 1995) LMs are trained. Translation performances are measured by case-insensitive BLEU (Papineni et al., 2002) with significance test (Koehn, 2004) and METEOR (Lavie and Agarwal, 2007). MERT (Och, 2003) (BLEU based) is run three times for each system and the average BLEU/METEOR scores are recorded. 4-layer CSTM (Schwenk, 2012) are applied to NN translation models: phrase length limit is set as seven, shared projection layer of dimension 320 for each word (that is 2240 for seven words), projection layer of dimension 768, hidden layer of dimension 512. The dimensions of input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. That is 72K/57K for IWSLT and 149/112K f"
C16-1295,W07-0734,0,0.0163639,"d to a series of NLP tasks, such as Chinese word segmentation and parsing (Cai and Zhao, 2016; Zhang et al., 2016). 4 https://wit3.fbk.eu/mt.php?release=2014-01 5 http://statmt.org/wmt15/translation-task.html 6 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 3138 4.2 Common Setting The basic settings of IWSLT-2014 FR to EN and NIST-06 CN to EN phrase based translation baseline systems are followed. 5-gram interpolated KN (Kneser and Ney, 1995) LMs are trained. Translation performances are measured by case-insensitive BLEU (Papineni et al., 2002) with significance test (Koehn, 2004) and METEOR (Lavie and Agarwal, 2007). MERT (Och, 2003) (BLEU based) is run three times for each system and the average BLEU/METEOR scores are recorded. 4-layer CSTM (Schwenk, 2012) are applied to NN translation models: phrase length limit is set as seven, shared projection layer of dimension 320 for each word (that is 2240 for seven words), projection layer of dimension 768, hidden layer of dimension 512. The dimensions of input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. That is 72K/57K for IWSLT and 149/112K for NIST. Since out-of-domain corpora"
C16-1295,D12-1088,0,0.0610705,"Missing"
C16-1295,N13-1074,0,0.0171997,"ee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT p"
C16-1295,C14-1105,0,0.0122047,"problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B. L. Lu were partially"
C16-1295,P10-2041,0,0.0842928,"evel selection (Axelrod et al., 2011; Banerjee et al., 2012; Duh et al., 2013; Hoang and Sima’an, 2014a; Hoang and Sima’an, 2014b). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwh"
C16-1295,2012.iwslt-papers.3,0,0.0240338,"e used as they are. This penalty setting is similar to (Bisazza et al., 2011). Penalty weights, together with all of existing score weights, will be further tuned by MERT (Och, 2003). The phrase pairs in re-ordering model are selected using the same way as PT. The selected monolingual n-grams are added to the original LM, and the probabilities are re-normalized by SRILM (Stolcke, 2002; Stolcke et al., 2011). 4 Experiments 4.1 Data sets The proposed methods are evaluated on two data sets. 1) IWSLT 2014 French (FR) to English (EN) corpus4 is used as in-domain data and dev2010 and test2010/2011 (Niehues and Waibel, 2012), are selected as development (dev) and test data, respectively. Out-of-domain corpora contain Common Crawl, Europarl v7, News Commentary v10 and United Nation (UN) FR-EN parallel corpora5 . 2) NIST 2006 Chinese (CN) to English corpus6 is used as in-domain corpus, which follows the setting of (Wang et al., 2014b) and mainly consists of news and blog texts. Chinese to English UN data set (LDC2013T06) and NTCIR-9 (Goto et al., 2011) patent data are used as out-of-domain bilingual (Bil) parallel corpora. The English patent data in NTCIR-8 (Fujii et al., 2010) is also used as additional out-of-dom"
C16-1295,P03-1021,0,0.0187056,"ts will be shown in Section 5.4. 3.3 Integration into SMT The thresholds of Pop and Dminus are tuned using development data. Selected phrase pairs are added into the in-domain PT. Because they are not so useful as the in-domain ones, a penalty score is added. For in-domain phrase pairs, the penalty is set as 1; for the out-of-domain ones the penalty is set as e (= 2.71828...). Other phrase scores (lexical weights et. al.) are used as they are. This penalty setting is similar to (Bisazza et al., 2011). Penalty weights, together with all of existing score weights, will be further tuned by MERT (Och, 2003). The phrase pairs in re-ordering model are selected using the same way as PT. The selected monolingual n-grams are added to the original LM, and the probabilities are re-normalized by SRILM (Stolcke, 2002; Stolcke et al., 2011). 4 Experiments 4.1 Data sets The proposed methods are evaluated on two data sets. 1) IWSLT 2014 French (FR) to English (EN) corpus4 is used as in-domain data and dev2010 and test2010/2011 (Niehues and Waibel, 2012), are selected as development (dev) and test data, respectively. Out-of-domain corpora contain Common Crawl, Europarl v7, News Commentary v10 and United Nati"
C16-1295,P02-1040,0,0.0955457,"ics on data sets (‘B’ for billions). 3 NN based methods have been applied to a series of NLP tasks, such as Chinese word segmentation and parsing (Cai and Zhao, 2016; Zhang et al., 2016). 4 https://wit3.fbk.eu/mt.php?release=2014-01 5 http://statmt.org/wmt15/translation-task.html 6 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 3138 4.2 Common Setting The basic settings of IWSLT-2014 FR to EN and NIST-06 CN to EN phrase based translation baseline systems are followed. 5-gram interpolated KN (Kneser and Ney, 1995) LMs are trained. Translation performances are measured by case-insensitive BLEU (Papineni et al., 2002) with significance test (Koehn, 2004) and METEOR (Lavie and Agarwal, 2007). MERT (Och, 2003) (BLEU based) is run three times for each system and the average BLEU/METEOR scores are recorded. 4-layer CSTM (Schwenk, 2012) are applied to NN translation models: phrase length limit is set as seven, shared projection layer of dimension 320 for each word (that is 2240 for seven words), projection layer of dimension 768, hidden layer of dimension 512. The dimensions of input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. Tha"
C16-1295,C12-2104,0,0.173123,"and Pout (E|F ) by N N T Mout should be lower. This hypothesis is partially motivated by (Axelrod et al., 2011), which use bilingual cross-entropy difference to distinguish in-domain and out-of-domain data. The translation probability of a phrase-pair is estimated as, P (E|F ) = P (e1 , ..., eq |f1 , ..., fp ), (1) where fs (s ∈ [1, p]) and et (t ∈ [1, q]) are source and target words, respectively. Originally, P (e1 , ..., eq |f1 , ..., fp ) = q ∏ P (ek |e1 , ..., ek−1 , f1 , ...fp ). (2) k=1 The structure of NN based translation model is similar to Continuous Space Translation Model (CSTM) (Schwenk, 2012). For the purpose of adaptation, the dependence between target words is dropped2 and the probabilities of different length target phrase are normalized. For an incomplete source phrase, i.e. with less than seven words, we set the projections of the missing words to zero. The normalized translation probability Q(E|F ) can be approximately computed by the following equation, v u q u∏ q P (ek |f1 , ...fp ). Q(E|F ) ≈ t (3) k=1 Finally, the minus Dminus (E|F ) is used to rank connecting phrase pairs from mix-domain PT, Dminus (E|F ) = Qin (E|F ) − Qin (E|F ). (4) where Qin (E|F ) and Qin (E|F ) ar"
C16-1295,P13-1082,0,0.203536,"). There is a potential problem for sentence level adaptation: different parts of a sentence may belong to different domains. That is, it is possible that a sentence is overall out-of-domain, although part of it can be in-domain. Therefore a few works consider more granular level for selection. They build lexicon, Translation Models (TMs), reordering models or Language Models (LMs) to select fragment or directly adapt the models (Bellegarda, 2004; Deng et al., 2008; Moore and Lewis, 2010; Foster et al., 2010; Mansour and Ney, 2013; Carpuat et al., 2013; Chen et al., 2013a; Chen et al., 2013b; Sennrich et al., 2013; Mathur et al., 2014; Shi et al., 2015). One typical example of these methods is to train two Neural Network (NN) models (one from in-domain and the other from out-of-domain) and penalize the sentences/phrases similar to out-of-domain corpora (Duh et al., 2013; Joty et al., 2015; Durrani et al., 2015). As we know, Phrase Based SMT (PBSMT) mainly contains two models: translation model and LM, whose components are bilingual phrase pairs and monolingual n-grams. Meanwhile, most of the above methods enhance SMT performance by adapting single specific model. ∗ Corresponding authors. H. Zhao and B."
C16-1295,E12-1055,0,0.0662449,"input/output layers for both in/out-of-domain CSTMs follows the size of vocabularies of source/target words from in-domain corpora. That is 72K/57K for IWSLT and 149/112K for NIST. Since out-of-domain corpora are huge, part of them are resampled (resample coefficient 0.01 for IWSLT and NIST). Several related existing methods are selected as baselines7 : Koehn and Schroeder (2007)’s method for using two (in and out-of-domain) TMs and LMs together, entropy based method for TM (Ling et al., 2012) and LM (Stolcke, 1998) adaptation (pruning), (Duh et al., 2013) for NNLM based sentence adaptation, (Sennrich, 2012) for TM weights combination, and (Bisazza et al., 2011) for TM fill-up. In Table Tables 2 and 3, ‘in-domain’, ‘out-of-domain’ and ‘mix-domain’ indicate training all models using corresponding corpora, ‘in+NN’ indicates applying NN based adaptation directly for all phrases, and ‘in+connect’ indicates adding all connecting phrases and n-grams to in-domain PT and LM, respectively. For tuning methods, ‘in+connect+OP/NN’ indicates tuning connecting phrase pairs and n-grams using Occurring Probability (OP) and NN, respectively. Only the best preforming systems (for both the baselines and proposed me"
C16-1295,D14-1023,1,0.921449,"Missing"
C16-1295,P14-2122,1,0.886787,"ails: http://creativecommons.org/licenses/by/4.0/ 3135 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3135–3145, Osaka, Japan, December 11-17 2016. Instead of focusing on sentence selection or single model adaptation, we propose a phrase adaptation method, which is applied to both bilingual phrase pair and monolingual n-gram selection. It is based on a linguistic observation that the translation hypotheses of a phrase-based SMT system are concatenations of phrases from Phrase Table (PT), which has been applied to LM growing (Wang et al., 2014a; Wang et al., 2015). As a straightforward linear method, it is much efficient in comparison with NN based non-linear methods. The remainder of this paper is organized as follows. Section 2 will introduce the connecting phrase based adaptation method. The size of adapted connecting phrase will be tuned in Section 3. Empirical results will be shown in Section 4. We will discuss the methods and conduct extension experiments in Section 5. The last section will conclude this paper. 2 Connecting Phrase based Adaptation Suppose that two phrases ‘would like to learn’ and ‘Chinese as second language’"
C16-1295,P16-1131,1,0.811994,"Missing"
C16-1295,W14-3348,0,\N,Missing
C16-1295,C12-1010,0,\N,Missing
C16-2007,2014.iwslt-papers.8,1,0.731983,"s can be used to monitor the running of the system: the speech recognition log and the machine translation log (Figure 2). The speech recognition log shows the recognized words from the speakers. The machine translation log shows the recognized sentences and their translations. The content of both logs is updated in realtime. 4 Performance The performance of our method was measured in (Wang et al., 2016a). Experiments were performed on translation between Japanese and English in both directions. The time efficiency was measured by average latency per source word using the definition given in (Finch et al., 2014). The translation quality was measured by the BLEU of end-to-end translation. Because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The results of the measurement are presented in table 1. Different sentence segmentation methods were compared. Our system adopted the threshold-latency method which generally outperformed the other methods on both time efficiency and translation quality. 5 Example Analysis Here is an example of interpreting a TED ta"
C16-2007,2005.iwslt-1.19,0,0.0456124,"The content of both logs is updated in realtime. 4 Performance The performance of our method was measured in (Wang et al., 2016a). Experiments were performed on translation between Japanese and English in both directions. The time efficiency was measured by average latency per source word using the definition given in (Finch et al., 2014). The translation quality was measured by the BLEU of end-to-end translation. Because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The results of the measurement are presented in table 1. Different sentence segmentation methods were compared. Our system adopted the threshold-latency method which generally outperformed the other methods on both time efficiency and translation quality. 5 Example Analysis Here is an example of interpreting a TED talk from English to Japanese by the system. The talk is ”Your elusive creative genius “ given by Elizabeth Gilbert in 20097 . The oracle transcript is, I am a writer. Writing books is my profession but it’s more than that, of course. It is also my great lifelong love and fascinati"
C16-2007,2006.iwslt-papers.1,0,0.0264925,"mage is caused. The Online Sentence Segmenter converts the stream of words into sentences. The implementation is based on the method proposed in (Wang et al., 2016a). The implementation uses a linear combination of a language model, a length model and a prosodic model to calculate the confidence of segmentation boundaries, and uses a threshold-latency-based heuristic to make decisions. The Punctuation Predictor converts an un-punctuated sentence into a punctuated sentence. The implementation is based on the findings in (Wang et al., 2016b). It uses a hidden N-gram model (Stolcke et al., 1998; Matusov et al., 2006), which is available in the toolkit of SRILM (Stolcke, 2002), to insert punctuation. The Machine Translation Engine translates a source-language sentence into a target-language sentence. The implementation is our in-house pre-ordering translation system, called the General Purpose Machine Translation (GPMT) engine. The system is publicly accessible through a Web API 5 The Speech Synthesizer converts sentences into speech. The implementation is based on the HTS open-source toolkit (Tokuda et al., 2013)6 4 https://github.com/kaldi-asr/kaldi https://mt-auto-minhon-mlt.ucri.jgn-x.jp/ 6 http://hts."
C16-2007,N16-3017,0,0.0615103,"Missing"
C16-2007,W16-4613,1,0.864082,"Missing"
C16-2008,W02-2117,1,0.549993,"Missing"
C16-2008,J14-1005,0,0.0222758,"TUAL, which aims to help writers create multilingual texts. The highlighted feature of the system is that it enables machine translation (MT) to generate outputs appropriate to their functional context within the target document. Our system is operational online, implementing core mechanisms for document structuring and controlled writing. These include a topic template and a controlled language authoring assistant, linked to our statistical MT system. 1 Introduction For improved machine translatability, a wide variety of controlled language (CL) rule sets have been proposed (Kittredge, 2003; Kuhn, 2014). Evidence of reduced post-editing costs when a CL is employed is provided (Bernth and Gdaniec, 2001; O’Brien and Roturier, 2007), and several controlled authoring support tools, such as Acrolinx1 and MAXIT2 , have been developed. The fundamental limitation of the CLs proposed hitherto is, however, that they are defined at the level of the sentence rather than at the level of the document (Hartley and Paris, 2001). In fact, the notion of functional document element (see Section 2.1) does figure in some CL rule sets. ASD Simplified Technical English (ASD, 2013), for example, specifies writing p"
C16-2008,2003.eamt-1.10,0,0.0917482,"pic template is the core interface for authoring self-contained topics in a structured manner. The left pane in Figure 3 provides the basic DITA Task topic structure for composing municipal procedural documents. • CL authoring assistant analyses each sentence in the text box and highlights any segment that violates a local CL rule or controlled terminology, together with diagnostic comments and suggestions for rewriting (shown at bottom centre in Figure 3) (Miyata et al., 2016). In addition, we have implemented a preliminary rewriting support function with several of the features advocated by Mitamura et al. (2003). For a particular CL-noncompliant segment, the function offers alternative expressions; clicking one of the suggestions automatically replaces the offending segment in the text box above. • Pre-translation processing automatically modifies source segments in the background following transformation rules defined for each functional element, and then MT produces the translation and back-translation at the same time. 3 We used a Japanese morphological analyser MeCab. http://taku910.github.io/mecab/ 37 MT and back translation DITA task topic CL authoring assistant Figure 3: Task topic template fo"
C16-2008,2015.mtsummit-papers.8,1,0.857596,"Missing"
D13-1082,P96-1041,0,0.290232,"translations. However, CSLMs have not been used in the first pass decoding of SMT, because using CSLMs in decoding takes a lot of time. In contrast, we propose a method for converting CSLMs into back-off n-gram language models (BNLMs) so that we can use converted CSLMs in decoding. We show that they outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in prac"
D13-1082,W04-3250,0,0.0751864,"development data. After we selected the interpolation weight, we applied MERT again to the 2,000 sentence development data to tune the weight parameters.2 We call this BNLM CONV42. We also obtained CONV746 by re-writing BNLM746 with CSLM42 2 We aware that the interpolation weight might be determined by minimizing the perplexity on the development data. However, we opted to directly maximize the BLEU score. LMs BNLM42 CONV42 BNLM746 CONV746 1st pass 31.60 32.58 32.83 33.22 rerank 32.44 32.98 33.36 33.54 Table 1: Comparison of BLEU scores We also performed the paired bootstrap resampling test (Koehn, 2004).3 We sampled 2000 samples for each significance test. Table 2 shows the results of a statistical significance test, in which the “1st” is short for the “1st pass”. The marks indicate whether the LM to the left of a mark is significantly better than that above the mark at a certain level. (“≫”: significantly better at α = 0.01, “>”: α = 0.05, “−”: not significantly better at α = 0.05) First, as shown in the tables, the reranking by applying CSLM42 increased the BLEU scores for all language models. This observation is in accordance with those of previous work (Schwenk, 2010; Huang et al., 2013)"
D13-1082,2012.iwslt-papers.3,0,0.182408,"CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing using costs. Since the using costs of CSLMs are very high, it is dif"
D13-1082,J03-1002,0,0.01,"n that case, we use the probabilities in the BNLM as they are. 4 Experiments 4.1 Common settings We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data consisted of 1 M, 2,000, and 2,000 sentences, respectively. We followed the settings of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) except that we used various language models to compare them. We used the MOSES phrasebased SMT system (Koehn et al., 2003), together with Giza++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. The translation performance was measured by the case-insensitive BLEU scores on the tokenized test data. We used mteval-v13a.pl for calculating BLEU scores.1 1 It is available at http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ 847 We used the 14 standard SMT features: five translation model scores, one word penalty score, seven distortion scores and one language model score. Each of the different language models was used to calculate the language model score. As the baseline BNLM, we trained a 5-gram BNLM with modified Kne"
D13-1082,P03-1021,0,0.0144364,"e BNLM as they are. 4 Experiments 4.1 Common settings We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data consisted of 1 M, 2,000, and 2,000 sentences, respectively. We followed the settings of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) except that we used various language models to compare them. We used the MOSES phrasebased SMT system (Koehn et al., 2003), together with Giza++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. The translation performance was measured by the case-insensitive BLEU scores on the tokenized test data. We used mteval-v13a.pl for calculating BLEU scores.1 1 It is available at http://www.itl.nist.gov/iad/ mig/tests/mt/2009/ 847 We used the 14 standard SMT features: five translation model scores, one word penalty score, seven distortion scores and one language model score. Each of the different language models was used to calculate the language model score. As the baseline BNLM, we trained a 5-gram BNLM with modified Kneser-Ney smoothing using the English"
D13-1082,P02-1040,0,0.0964055,"e processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing using costs. Since the using costs of CSLMs are very high, it is difficult to use CSLMs in decoding directly. A common approach in SMT using CSLMs i"
D13-1082,P06-2093,0,0.271464,"ey outperform the original BNLMs and are comparable with the traditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little"
D13-1082,W12-2702,0,0.0213596,"comparable with the traditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing using costs. Since the"
D13-1082,D10-1076,0,0.552441,"inal BNLMs and are comparable with the traditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing u"
D13-1082,N12-1005,0,0.0565609,"raditional use of CSLMs in reranking. 1 Introduction Language models are important in natural language processing tasks such as speech recognition and statistical machine translation. Traditionally, backoff n-gram language models (BNLMs) (Chen and Goodman, 1996; Chen and Goodman, 1998; Stolcke, 2002) are being widely used for these tasks. Recently, neural network language models, or continuous-space language models (CSLMs) (Bengio et al., 2003; Schwenk, 2007; Le et al., 2011) are being used in statistical machine translation (SMT) (Schwenk et al., 2006; Son et al., 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). These works have shown that CSLMs can improve the BLEU (Papineni et al., 2002) scores of SMT when compared with BNLMs, on the condition that the training data for language modeling are the same size. However, in practice, CSLMs have not been widely used in SMT. One reason is that the computational costs of training and using CSLMs are very high. Various methods have been proposed to tackle the training cost issues (Son et al., 2010; Schwenk et al., 2012; Mikolov et al., 2011). However, there has been little work on reducing using costs. Since the using costs of CSL"
D13-1082,N03-1017,0,\N,Missing
D14-1022,C10-2044,0,0.0352255,"translation spans, respectively. A source word is marked as beginning (ending) boundary if it is the first (last) word of a translation span. However, a source span whose first and last words are both boundaries is not always a translation span. In Figure 1, “I” is a beginning boundary since it is the first word of translation span “I will” and “experiment” is an ending boundary since it is the last word of translation span “finish this experiment” , but “I will finish this experiment” is not a translation span. This happens because the translation spans are nested or hierarchical. Note that (He et al., 2010) also learned phrase boundaries to constrain decoding, but their approach identified boundaries only for monotone translation. In this paper, taking fully into account that translation spans being nested, we propose an approach to learn hierarchical translation spans directly from an aligned parallel corpus that makes more accurate identification over translation spans. The rest of the paper is structured as follows: In Section 2, we briefly review the HPB translation model. Section 3 describes our approach. We describe experiments in Section 4 and conclude in Section 5. 2 w (X → hγ, α, ∼i) ="
D14-1022,N03-1017,0,0.0383474,"118M 104M 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default Classifiers We compare two machine learning methods for learning"
D14-1022,P07-2045,0,0.0296419,"Missing"
D14-1022,W04-3250,0,0.0165102,"word order difference of JE task is much more significant than that of CE task, there are more negative Japanese translation span instances than Chinese. In JE tasks, the ME classifiers C8 , C9 and C10 predicted all new instances to be negative due to the heavily unbalanced instance distribution. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sentences, a source span can be applied with translaAs shown in Table 2, NN outperformed ME approach for our classification tasks. As the span length growing, the advantage of NN became more significant. Since the class"
D14-1022,P10-1052,0,0.0701371,"Missing"
D14-1022,2011.mtsummit-papers.28,0,0.029711,"ine system. 1 我 会 在 明天 之前 完成 这个 实验 Figure 1: A translation example. model, which represents if the source span covered by a translation rule is a syntactic constituent. However, the experimental results showed this feature gave no significant improvement. Instead of using the undifferentiated constituency feature, (Marton and Resnik, 2008) defined different soft syntactic features for different constituent types and obtained substantial performance improvement. Later, (Mylonakis and Sima’an, 2011) introduced joint probability synchronous grammars to integrate flexible linguistic information. (Liu et al., 2011) proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model. (Cui et al., 2010) focused on hierarchical rule selection using many features including syntax constituents. These works have demonstrated the benefits of using syntactic features in the HPB model. However, high quality syntax parsers are not always easily obtained for many languages. Without this problem, word alignment constraints can also be used to guide the application of the rules. Suppose that we want to translate the English"
D14-1022,P08-1114,0,0.0223738,"t require parsers. Rich source side contextual features and advanced machine learning methods were utilized for this learning task. The proposed approach was evaluated on NTCIR-9 Chinese-English and Japanese-English translation tasks and showed significant improvement over the baseline system. 1 我 会 在 明天 之前 完成 这个 实验 Figure 1: A translation example. model, which represents if the source span covered by a translation rule is a syntactic constituent. However, the experimental results showed this feature gave no significant improvement. Instead of using the undifferentiated constituency feature, (Marton and Resnik, 2008) defined different soft syntactic features for different constituent types and obtained substantial performance improvement. Later, (Mylonakis and Sima’an, 2011) introduced joint probability synchronous grammars to integrate flexible linguistic information. (Liu et al., 2011) proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model. (Cui et al., 2010) focused on hierarchical rule selection using many features including syntax constituents. These works have demonstrated the benefits of using"
D14-1022,J96-1002,0,0.0295998,"ta for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default Classifiers We compare two machine learning methods for learning a series of binary classifiers. For the first method, each Ck is individually learned using the maximum entropy (ME) approach (Berger et al., 1996):  P exp t µt ht (v, f (D) , i, j)  P Pk (v|f (D) , i, j) = P 0 v 0 exp t µt ht (v , f (D) , i, j) (5) where ht is a feature function and µt is weight of ht . We use rich source contextual features: unigram, bigram and trigram of the phrase [fi−3 , ..., fj+3 ]. As the second method, these classification tasks are learned in the continuous space using feedforward neural networks (NNs). Each Ck has the similar structure with the NN language model (Vaswani et al., 2013). The inputs to the NN are indices of the words: [fi−3 , ..., fj+3 ]. Each source word is projected into an N dimensional vecto"
D14-1022,P05-1033,0,0.124086,"wever, high quality syntax parsers are not always easily obtained for many languages. Without this problem, word alignment constraints can also be used to guide the application of the rules. Suppose that we want to translate the English sentence into the Chinese sentence in Figure 1, a translation rule can be applied to the source span “finish this experiment by tomorrow”. Nonetheless, if a rule is applied to “experiment by”, then the Chinese translation can not be correctly obtained, because the target span projected from “exIntroduction The hierarchical phrase-based (HPB) translation model (Chiang, 2005) has been widely adopted in statistical machine translation (SMT) tasks. The HPB translation rules based on the synchronous context free grammar (SCFG) are simple and powerful. One drawback of the HPB model is the applications of translation rules to the input sentence are highly ambiguous. For example, a rule whose English side is “X1 by X2” can be applied to any word sequence that has “by” in them. In Figure 1, this rule can be applied to the whole sentence as well as to “experiment by tomorrow”. In order to tackle rule application ambiguities, a few previous works used syntax trees. Chiang"
D14-1022,P11-1065,0,0.036752,"Missing"
D14-1022,P10-2002,0,0.0140531,"experimental results showed this feature gave no significant improvement. Instead of using the undifferentiated constituency feature, (Marton and Resnik, 2008) defined different soft syntactic features for different constituent types and obtained substantial performance improvement. Later, (Mylonakis and Sima’an, 2011) introduced joint probability synchronous grammars to integrate flexible linguistic information. (Liu et al., 2011) proposed the soft syntactic constraint model based on discriminative classifiers for each constituent type and integrated all of them into the translation model. (Cui et al., 2010) focused on hierarchical rule selection using many features including syntax constituents. These works have demonstrated the benefits of using syntactic features in the HPB model. However, high quality syntax parsers are not always easily obtained for many languages. Without this problem, word alignment constraints can also be used to guide the application of the rules. Suppose that we want to translate the English sentence into the Chinese sentence in Figure 1, a translation rule can be applied to the source span “finish this experiment by tomorrow”. Nonetheless, if a rule is applied to “expe"
D14-1022,P02-1038,0,0.10792,"0.97 0.14 0.98 0 1 0 1 0 1 NN P N 0.86 0.80 0.71 0.87 0.63 0.90 0.54 0.93 0.47 0.95 0.41 0.96 0.33 0.97 0.32 0.97 0.25 0.98 0.23 0.99 Table 2: Classification accuracies. The Rate column represents ratio of positive instances to negative instances; the P and N columns give classification accuracies for positive and negative instances. LM Toolkit 3 with improved Kneser-Ney smoothing. {C1 , ..., C10 } were integrated into the baseline with different weights, which were tuned by MERT (Och, 2003) together with other feature weights (language model, word penalty,...) under the log-linear framework (Och and Ney, 2002). values. The training time of one classifier on a 12-core 3.47GHz Xeon X5690 machine was 0.5h (2.5h) using ME (NN) approach for CE task; 1h (4h) using ME (NN) approach for JE task . The classification results are shown in Table 2. Instead of the undifferentiated classification accuracy, we present separate classification accuracies for positive and negative instances. The big difference between classification accuracies for positive and negative instances was caused by the unbalanced rate of positive and negative instances in the training corpus. For example, if there are more positive traini"
D14-1022,J03-1002,0,0.0125398,"rds #Vocab SOURCE TARGET 954k 37.2M 40.4M 288k 504k 3.14M 118M 104M 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default Classi"
D14-1022,P03-1021,0,0.0214277,".08 0.73 0.52 0.36 0.26 0.20 0.16 0.13 0.10 0.08 JE ME P N 0.85 0.79 0.69 0.84 0.56 0.89 0.48 0.93 0.30 0.96 0.25 0.97 0.14 0.98 0 1 0 1 0 1 NN P N 0.86 0.80 0.71 0.87 0.63 0.90 0.54 0.93 0.47 0.95 0.41 0.96 0.33 0.97 0.32 0.97 0.25 0.98 0.23 0.99 Table 2: Classification accuracies. The Rate column represents ratio of positive instances to negative instances; the P and N columns give classification accuracies for positive and negative instances. LM Toolkit 3 with improved Kneser-Ney smoothing. {C1 , ..., C10 } were integrated into the baseline with different weights, which were tuned by MERT (Och, 2003) together with other feature weights (language model, word penalty,...) under the log-linear framework (Och and Ney, 2002). values. The training time of one classifier on a 12-core 3.47GHz Xeon X5690 machine was 0.5h (2.5h) using ME (NN) approach for CE task; 1h (4h) using ME (NN) approach for JE task . The classification results are shown in Table 2. Instead of the undifferentiated classification accuracy, we present separate classification accuracies for positive and negative instances. The big difference between classification accuracies for positive and negative instances was caused by the"
D14-1022,P02-1040,0,0.0924408,"erence at the p < 0.01 level and - represents a significant difference at the p < 0.05 level against the BLM. Since the word order difference of JE task is much more significant than that of CE task, there are more negative Japanese translation span instances than Chinese. In JE tasks, the ME classifiers C8 , C9 and C10 predicted all new instances to be negative due to the heavily unbalanced instance distribution. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sentences, a source span can be applied with translaAs shown in Table 2, NN outperformed ME approach for our"
D14-1022,2006.amta-papers.25,0,0.020084,"- represents a significant difference at the p < 0.05 level against the BLM. Since the word order difference of JE task is much more significant than that of CE task, there are more negative Japanese translation span instances than Chinese. In JE tasks, the ME classifiers C8 , C9 and C10 predicted all new instances to be negative due to the heavily unbalanced instance distribution. We compare our method with the baseline and the boundary learning method (BLM) (Xiong et al., 2010) based on Maximum Entropy Markov Models with Markov order 2. Table 3 reports BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores. Significance tests are conducted using bootstrap sampling (Koehn, 2004). Our ME classifiers achieve comparable translation improvement with the BLM and NN classifiers enhance translation system significantly compared to others. Table 3 also shows that the relative gain was higher for higher n-grams, which is reasonable since the higher n-grams have higher ambiguities in the translation rule application. It is true that because of multiple parallel sentences, a source span can be applied with translaAs shown in Table 2, NN outperformed ME approach for our classification tasks. As the s"
D14-1022,D13-1140,0,0.074174,"ab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical quasi-newton optimization algorithm with limited memory. The NNs are trained by the toolkit NPLM (Vaswani et al., 2013). We chose “rectifier” as the activation function and the logarithmic loss function for NNs. The number of epochs was set to 20. Other parameters were set to default Classifiers We compare two machine learning methods for learning a series of binary classifiers. For the first method, each Ck is individually learned using the maximum entropy (ME) approach (Berger et al., 1996):  P exp t µt ht (v, f (D) , i, j)  P Pk (v|f (D) , i, j) = P 0 v 0 exp t µt ht (v , f (D) , i, j) (5) where ht is a feature function and µt is weight of ht . We use rich source contextual features: unigram, bigram and t"
D14-1022,W06-0127,1,0.879402,"i to n do 3: if ∃eqp , 1 ≤ p ≤ q ≤ m & ∃ (k, t) ∈ A, i ≤ k ≤ j, p ≤ t ≤ q & ∀ (k, t) ∈ A, i ≤ k ≤ j ↔ p ≤ t ≤ q then 4: fij is a positive instance for Cj−i+1 5: else 6: fij is a negative instance for Cj−i+1 7: end if 8: end for 9: end for 3.3 Experiment CE JE #Sents #Words #Vocab #Sents #Words #Vocab SOURCE TARGET 954k 37.2M 40.4M 288k 504k 3.14M 118M 104M 150k 273k Table 1: Data sets. sets were both provided for CE task while only the test set was provided for JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab 2 for Japanese. To learn the classifiers for each translation task, the training set and development set were put together to obtain symmetric word alignment using GIZA++ (Och and Ney, 2003) and the growdiag-final-and heuristic (Koehn et al., 2003). The source span instances extracted from the aligned training and development sets were used as the training and validation data for the classifiers. The toolkit Wapiti (Lavergne et al., 2010) was adopted to train ME classifiers using the classical"
D14-1022,N10-1016,0,\N,Missing
D14-1023,P14-1142,1,0.292552,"i ∈ V0 Pb (wi |hi ) otherwise 0 (1) where V0 is the short-list,∑Pc (·) is the probability calculated by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calc"
D14-1023,W07-0733,0,0.109437,"Missing"
D14-1023,P07-2045,0,0.00563399,"ased and the BLEU scores trended to increase. These indicated that our proposed method can give better probability estimation for LM and better performance for SMT. (2) In comparison with the grown LMs in ArGrown n-grams Input Corpus CSLM Interpolate BNLM Output Grown n-grams with Probabilities Grown LM Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences,"
D14-1023,D13-1106,0,0.0135132,"accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translation model for SMT (Devlin et al., 2014; Liu et al., 2014; Auli et al., 2013). However, the decoding speed using n-gram LM is still state-ofthe-art one. Some approaches calculate the probabilities of the n-grams n-grams before decoding, and store them in the n-gram format (Wang et al., 2013a; Arsoy et al., 2013; Arsoy et al., 2014). The ‘converted CSLM’ can be directly used in SMT. Though more n-grams which are not in the trainSince larger n-gram Language Model (LM) usually performs better in Statistical Machine Translation (SMT), how to construct efficient large LM is an important topic in SMT. However, most of the existing LM growing methods need an extra monolingual"
D14-1023,W04-3250,0,0.0727048,"est lists of SMT. Our previous converted LM, Arsoy’s grown LMs and bilingual grown LMs were interpolated with the original BNLMs, using default setting of SRILM5 . To reduce the randomness of MERT, we used two methods for tuning the weights of different SMT features, and two BLEU scores are corresponding to these two methods. The BLEU-s indicated that the same weights of the BNLM (BN) features were used for all the SMT systems. The BLEU-i indicated that the MERT was run independently by three times and the average BLEU scores were taken. We also performed the paired bootstrap resampling test (Koehn, 2004)6 . Two thousands samples were sampled for each significance test. The marks at the right of the BLEU score indicated whether the LMs were significantly better/worse than the Arsoy’s grown LMs with the same IDs for SMT (“++/−−”: significantly better/worse at α = 0.01, “+/−”: α = 0.05, no mark: not significantly better/worse at α = 0.05). From the results shown in Table 1, we can get the following observations: (1) Nearly all the bilingual grown LMs outperformed both BNLM and our previous converted LM on PPL and BLEU. As the size of grown LMs is increased, the PPL always decreased and the BLEU"
D14-1023,2012.eamt-1.60,0,0.0244855,"200240, China 2 Multilingual Translation Laboratory, MASTAR Project, National Institute of Information and Communications Technology, 3-5 Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japan 3 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China wangrui.nlp@gmail.com, {zhaohai, blu}@cs.sjtu.edu.cn, {mutiyama, eiichiro.sumita}@nict.go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, whic"
D14-1023,P14-1140,0,0.0260099,"Missing"
D14-1023,P14-1129,0,0.0325731,"Missing"
D14-1023,2012.iwslt-papers.3,0,0.044911,"s very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural network based LM or translati"
D14-1023,J03-1002,0,0.00542098,"e. These indicated that our proposed method can give better probability estimation for LM and better performance for SMT. (2) In comparison with the grown LMs in ArGrown n-grams Input Corpus CSLM Interpolate BNLM Output Grown n-grams with Probabilities Grown LM Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002;"
D14-1023,I13-1170,1,0.794008,"ed by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our previous approach. That is, the proposed new method is the"
D14-1023,P03-1021,0,0.0161605,"can give better probability estimation for LM and better performance for SMT. (2) In comparison with the grown LMs in ArGrown n-grams Input Corpus CSLM Interpolate BNLM Output Grown n-grams with Probabilities Grown LM Figure 1: NN based bilingual LM growing. 4 Experiments and Results 4.1 Experiment Setting up The same setting up of the NTCIR-9 Chinese to English translation baseline system (Goto et al., 2011) was followed, only with various LMs to compare them. The Moses phrase-based SMT system was applied (Koehn et al., 2007), together with GIZA++ (Och and Ney, 2003) for alignment and MERT (Och, 2003) for tuning on the development data. Fourteen standard SMT features were used: five translation model scores, one word penalty score, seven distortion scores, and one LM score. The translation performance was measured by the case-insensitive BLEU on the tokenized test data. We used the patent data for the Chinese to English patent translation subtask from the NTCIR-9 patent translation task (Goto et al., 2011). The parallel training, development, and test data sets consist of 1 million (M), 2,000, and 2,000 sentences, respectively. Using SRILM (Stolcke, 2002; Stolcke et al., 2011), we trained"
D14-1023,P95-1030,0,0.0389552,"rpus in SMT. The results show that our method can improve both the perplexity score for LM evaluation and BLEU score for SMT, and significantly outperforms the existing LM growing methods without extra corpus. 1 Introduction ‘Language Model (LM) Growing’ refers to adding n-grams outside the corpus together with their probabilities into the original LM. This operation is useful as it can make LM perform better through letting it become larger and larger, by only using a small training corpus. There are various methods for adding n-grams selected by different criteria from a monolingual corpus (Ristad and Thomas, 1995; Niesler and Woodland, 1996; Siu and Ostendorf, 2000; Siivola et al., 2007). However, all of these approaches need additional corpora. Meanwhile the extra corpora from different domains will not result in better LMs (Clarkson and Robinson, 1997; Iyer et al., 1997; Bellegarda, 2004; Koehn and Schroeder, ∗ Part of this work was done as Rui Wang visited in NICT. 189 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189–195, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics ing corpus can be generated by using so"
D14-1023,D13-1140,0,0.0268942,"Missing"
D14-1023,P06-2093,0,0.0965172,", blu}@cs.sjtu.edu.cn, {mutiyama, eiichiro.sumita}@nict.go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram"
D14-1023,D13-1082,1,0.598465,"Missing"
D14-1023,W12-2702,0,0.0279048,".go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to i"
D14-1023,I13-1069,1,0.740408,"Missing"
D14-1023,C12-2131,1,0.530178,"ist,∑Pc (·) is the probability calculated by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our previous approach."
D14-1023,C12-3067,1,0.224044,"re V0 is the short-list,∑Pc (·) is the probability calculated by CSLM, w∈V0 Pc (w|hi ) is the summary of probabilities of the neuron for all the words in the short-list, Pb (·) is the probability calculated by the BNLM, and Ps (hi ) = ∑ Pb (v|hi ). (2) v∈V0 We may regard that CSLM redistributes the probability mass of all words in the short-list, which is calculated by using the n-gram LM. Existing CSLM Converting Methods 2.2 Existing Converting Methods Traditional Backoff N -gram LMs (BNLMs) have been widely used in many NLP tasks (Zhang and Zhao, 2013; Jia and Zhao, 2014; Zhao et al., 2013; Zhang et al., 2012; Xu and Zhao, 2012; Wang et al., 2013b; Jia and Zhao, 2013; Wang et al., 2014). Recently, CSLMs become popular because they can obtain more accurate probability estimation. As baseline systems, our approach proposed in (Wang et al., 2013a) only re-writes the probabilities from CSLM into the BNLM, so it can only conduct a convert LM with the same size as the original one. The main difference between our proposed method in this paper and our previous approach is that n-grams outside the corpus are generated firstly and the probabilities using CSLM are calculated by using the same method as our"
D14-1023,D10-1076,0,0.35607,"{mutiyama, eiichiro.sumita}@nict.go.jp Abstract 2007). In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are seve"
D14-1023,N12-1005,0,0.0314071,"In addition, it is very difficult or even impossible to collect an extra large corpus for some special domains such as the TED corpus (Cettolo et al., 2012) or for some rare languages. Therefore, to improve the performance of LMs, without assistance of extra corpus, is one of important research topics in SMT. Recently, Continues Space Language Model (CSLM), especially Neural Network based Language Model (NNLM) (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011), is being actively used in SMT (Schwenk et al., 2006; Son et al., 2010; Schwenk, 2010; Schwenk et al., 2012; Son et al., 2012; Niehues and Waibel, 2012). One of the main advantages of CSLM is that it can more accurately predict the probabilities of the n-grams, which are not in the training corpus. However, in practice, CSLMs have not been widely used in the current SMT systems, due to their too high computational cost. Vaswani and colleagues (2013) propose a method for reducing the training cost of CSLM and apply it to SMT decoder. However, they do not show their improvement for decoding speed, and their method is still slower than the n-gram LM. There are several other methods for attempting to implement neural ne"
D14-1173,li-etal-2010-enriching,0,0.0242748,"rst briefly describes the GALE WA corpus, then presents an analysis of the WS arising from a CTB-standard word segmenter with reference to the segmentation of the atomic blocks in the GALE WA corpus, finally the impact of the findings on SMT is discussed. 2.1 GALE WA corpus The GALE WA corpus was developed by the LDC, and was used as training data in the DARPA GALE global autonomous language exploitation program 5 . The corpus incorporates linguistic knowledge into word aligned text to help improve automatic WA and translation quality. It employs two annotation schemes: alignment and tagging (Li et al., 2010). Alignment identifies minimum translation units and translation relations; tagging adds contextual, syntactic and languagespecific features to the alignment annotation. For example, the sample shown in Figure 1 carries tags on both alignment edges and tokens. The GALE WA corpus contains 18,057 manually word aligned Chinese and English parallel sentences which are extracted from newswire and web blogs. Table 1 presents the statistics on the corpus. One third of the sentences are approximately newswire text, and the remainder consists of web blogs. 2.2 Analysis of WS In order to produce a Chine"
D14-1173,W08-0336,0,0.0775486,"s. Automated word segmenters built through supervised-learning methods, after decades of intensive research, have emerged as effective solutions to WS tasks and become widely used in many NLP applications. For example, the Stanford word segmenter (Xue et al., 2002)1 which is based on conditional random field (CRF) is employed to prepare the official corpus for NTCIR9 Chinese-English patent translation task (Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently"
D14-1173,D09-1075,0,0.0804234,"the boundaries in the CTB WS scheme. The extended features consists of four types – named entities, word frequency, word length and character-level unsupervised WA. For each type of the feature, the value and value concatenated with previous or current character are taken as sparse features (see Table 4 for details). The real values of word frequency, word length and characterlevel unsupervised WA are converted into sparse features due to the routine of CRF model. The character-level unsupervised alignment feature is inspired by the related works of unsupervised bilingual WS (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The idea is that the character-level WA can approximately capture the counterpart English expression of each Chinese token, and source tokens aligned to different target expressions should be split into different words (see Figure 4 for an illustration). The values of the character-level alignment features are obtained through building a dictionary. First, unsupervised WA is performed on the SMT training corpus where the Chinese sentences are treated as sequences of characters; then, the Chinese sentences are segmented by CTB segmenter and a dictio"
D14-1173,P07-1003,0,0.0245675,"re often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1 http://nlp.stanford.edu/software/ segmenter.shtml 2 http://catalog.ldc.upenn.edu 3 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 1654 Proceedings of the 2014 Conference on Empirical Methods"
D14-1173,P05-1045,0,0.00487485,"es were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7 http://nlp.stanford.edu/software/ corenlp.shtml The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9 . The length tuner reused the CRF model of CTB segmenter. The parameter λ0 was tuned through the grid search in (Chang et al., 2008), that is, observing the BLEU score on the SMT development set varing from λ0 = 0 to λ0 = 32. The grid search showed that λ0 = 2 was optimal, agreeing with the value in (Chang et al., 2008). Moses (Koehn et al., 2007)10 , a state-of-the-art phrase"
D14-1173,P09-1104,0,0.0131808,"T. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1 http://nlp.stanford.edu/software/ segmenter.shtml 2 http://catalog.ldc.upenn.edu 3 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 1654 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Pro"
D14-1173,P07-2045,0,0.00855791,"ord named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9 . The length tuner reused the CRF model of CTB segmenter. The parameter λ0 was tuned through the grid search in (Chang et al., 2008), that is, observing the BLEU score on the SMT development set varing from λ0 = 0 to λ0 = 32. The grid search showed that λ0 = 2 was optimal, agreeing with the value in (Chang et al., 2008). Moses (Koehn et al., 2007)10 , a state-of-the-art phrase-based SMT system, was employed to perform end-to-end SMT experiments. GIZA++ was employed to perform unsupervised WA. 4.2 Experimental Results 4.2.1 Word Segmentation The WS performance of CTB segmenter, length tuner and the proposed lexical splitter are presented in Table 6. The proposed method achieves the highest scores on all the criterion of F1 , precision and recall. The length tuner outperforms the CTB segmenter in terms of recall, but with lower precision. 8 http://nlp.stanford.edu/software/ CRF-NER.shtml 9 http://www.statmt.org/moses/giza/ GIZA++.html 10"
D14-1173,N06-1014,0,0.651297,"t the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1 http://nlp.stanford.edu/software/ segmenter.shtml 2 http://catalog.ldc.upenn.edu 3 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 1654 Proceedings of the 2014 Confere"
D14-1173,E09-1063,0,0.0167738,"There is large volume of research using bilingual unsupervised and semi-supervised WS to address the problem of optimizing WS for SMT (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The main difference with our approach is that they use automatic WA results, most often obtained using the same tools as are used in training SMT systems. One of the main problems of using unsupervised WA is that it is noisy, and therefore, employing iterative optimization methods to refine the results of unsupervised WA is a key issue in their research, for example boosting (Ma and Way, 2009; Michael et al., 2011), expectation maximization (Chung and Gildea, 2009), Bayesian sampling (Xu et al., 2008; Nguyen et al., 2010), or heuristic search (Zhao et al., 2013). Nevertheless, noisy WA makes both analyzing WS and improving SMT quality quite hard. In contrast, by using manual WA, we can clearly analyze the segmentation problems (Section 2), and train supervised models to solve the problem (Section 3). As far as we are aware, among related work on WS, our method achieves the highest BLEU improvement relative to the start-of-the-art WS – the Stanford Chinese word segmenter – on the C"
D14-1173,W03-0301,0,0.0319011,"orpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,137 English words, and 421,763 1 http://nlp.stanford.edu/software/ segmenter.shtml 2 http://catalog.ldc.upenn.edu 3 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012"
D14-1173,C10-1092,0,0.107692,"B WS scheme. The extended features consists of four types – named entities, word frequency, word length and character-level unsupervised WA. For each type of the feature, the value and value concatenated with previous or current character are taken as sparse features (see Table 4 for details). The real values of word frequency, word length and characterlevel unsupervised WA are converted into sparse features due to the routine of CRF model. The character-level unsupervised alignment feature is inspired by the related works of unsupervised bilingual WS (Xu et al., 2008; Chung and Gildea, 2009; Nguyen et al., 2010; Michael et al., 2011). The idea is that the character-level WA can approximately capture the counterpart English expression of each Chinese token, and source tokens aligned to different target expressions should be split into different words (see Figure 4 for an illustration). The values of the character-level alignment features are obtained through building a dictionary. First, unsupervised WA is performed on the SMT training corpus where the Chinese sentences are treated as sequences of characters; then, the Chinese sentences are segmented by CTB segmenter and a dictionary of segmented wor"
D14-1173,C00-2163,0,0.802782,"Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually WA corpora are precious resources for SMT research, but they used to be only available in small volumes due to the production cost. For example, (Och and Ney, 2000) initially annotated 447 English-French sentence pairs, which later became the test data set in ACL 2003 shared task on word alignment (Mihalcea and Pedersen, 2003), and was used frequently thereafter (Liang et al., 2006; DeNero and Klein, 2007; Haghighi et al., 2009) For Chinese and English, the shortage of manually WA corpora has recently been relieved by the linguistic data consortium (LDC) 2 GALE Chinese-English word alignment and tagging training corpus (the GALE WA corpus)3 . The corpus is considerably large, containing 4,735 documents, 18,507 sentence pairs, 620,189 Chinese tokens, 518,"
D14-1173,J03-1002,0,0.00860593,"posed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9 . The length tuner reused the CRF model of CTB segmenter. The parameter λ0 was tuned through the grid search in (Chang et al., 2008), that is, observing the BLEU score on the SMT development set varing from λ0 = 0 to λ0 = 32. The grid search showed that λ0 = 2 was optimal, agreeing with the value in (Chang et al., 2008). Moses (Koehn et al., 2007)10 , a state-of-the-art phrase-based SMT system, was employed to perform end-to-end SMT experiments. GIZA++ was employed to perform unsupervised WA. 4.2 Experimental Results 4.2.1 Word Segmentation The WS performance of CTB segmenter, length tuner a"
D14-1173,P03-1021,0,0.039167,"nce samples which contain one Chinese sentence and four English reference sentences. The experimental corpus for unsupervised WA was the union set of the NIST OpenMT training set and the 2000 test sentence pairs from GALE WA corpus. We removed the United Nations corpus from the NIST OpenMT constraint training resources because it is out of domain. The main result of this paper is the evaluation of the end-to-end performance of an SMT system. The experimental corpus for this task was the NIST OpenMT corpus. The data set of the NIST evaluation 2002 was used as a development set for MERT tuning (Och, 2003), and the remaining data sets of the NIST evaluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The"
D14-1173,P02-1040,0,0.0990795,"aluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7 http://nlp.stanford.edu/software/ corenlp.shtml The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The characte"
D14-1173,C04-1081,0,0.0484789,"roach This paper employs a condition random field (CRF) to solve this sequence labeling task (Lafferty et al., 2001). A linear-chain CRF defines the conditional probability of y given x as, T 1 XX λk fk (yt−1 , yt , x, t)), ( Zx t=1 k (2) where Λ = {λ1 , . . .} are parameters, Zx is a perinput normalization that makes the probability of all state sequences sum to one; fk (yt−1 , yt , x, t) is a feature function which is often a binary-valued sparse feature. The training of CRF model is to maximize the likelihood of training data together with a regularization penalty to avoid over-fitting as (Peng et al., 2004; Peng and McCallum, 2006), PΛ (y|x) = X X λ2 k Λ∗ = argmax( logPΛ (yi |xi ) − 2 ), 2δ Λ k i k (3) where (x,y) are training samples; the hyperparameter δk can be understood as the variance of the prior distribution of λk . When predicting the labels of test samples, the CRF decoder searches for the optimal label sequence y ∗ that maximizes the conditional probability, y∗ = argmax PΛ (y|x). y (4) In (Chang et al., 2008) a method is proposed to select an appropriate level of segmentation granularity (in practical terms, to encourage smaller segments). We call their method “length tuner”. The fol"
D14-1173,W03-1719,0,0.02299,"nMT constraint training resources because it is out of domain. The main result of this paper is the evaluation of the end-to-end performance of an SMT system. The experimental corpus for this task was the NIST OpenMT corpus. The data set of the NIST evaluation 2002 was used as a development set for MERT tuning (Och, 2003), and the remaining data sets of the NIST evaluation from 2003 to 2006 were used as test sets. The English sentences were tokenized by Stanford toolkit 7 and converted to lowercase. 4.1.2 Evaluation The performance of WS was measured by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7 http://nlp.stanford.edu/software/ corenlp.shtml The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the"
D14-1173,I05-3027,0,0.0191438,"d by precision, recall and F1 of gold words (Sproat and Emerson, 2003), The performance of unsupervised WA in the SMT training procedure was measured through alignment error rate (AER)(Och and Ney, 2000; Liang et al., 2006). Sure alignment edges and possible alignment edges were not distinguished in this paper as no such tags are found in GALE manual WA corpus. The performance of SMT was measured using BLEU (Papineni et al., 2002). 7 http://nlp.stanford.edu/software/ corenlp.shtml The proposed lexical word splitter was implemented on the CRF model toolkit released with the Stanford segmenter (Tseng et al., 2005). The regularity parameters δk are set to be 3, the same as the Stanford segmenter, because no significant performance improvements were observed by tuning that parameter. To extract features for the word splitter, the Stanford named entity recognizer (Finkel et al., 2005)8 was employed to obtain the tags of named entities. Word frequencies were caculated from the source side of SMT training corpus. The character-level unsupervised alignment was conducted using GIZA++ (Och and Ney, 2003)9 . The length tuner reused the CRF model of CTB segmenter. The parameter λ0 was tuned through the grid sear"
D14-1173,C08-1128,0,0.0713041,"Missing"
D14-1173,C02-1145,0,0.0532368,"on those languages that have no explicit space between words, such as Arabic, Chinese and Japanese. As the first processing step, WS affects all successive steps, thus it has a large potential impact on the final performance. For SMT, the unsupervised WA, building translation models and reordering models, and decoding are all based on segmented words. Automated word segmenters built through supervised-learning methods, after decades of intensive research, have emerged as effective solutions to WS tasks and become widely used in many NLP applications. For example, the Stanford word segmenter (Xue et al., 2002)1 which is based on conditional random field (CRF) is employed to prepare the official corpus for NTCIR9 Chinese-English patent translation task (Goto et al., 2011). However, one problem with applying these supervised-learning word segmenters to SMT is that the WS scheme of annotating the training corpus may not be optimal for SMT. (Chang et al., 2008) noticed that the words in CTB are often too long for SMT. For example, a full Chinese personal name which consists of a family name and a given name is always taken as a single word, but its counterpart in English is usually two words. Manually"
D14-1173,2002.tmi-tutorials.2,0,0.0201863,"ET (determiner) are tags of tokens. Genre Newswire Web blog Total # Files 2,175 2,560 4,735 # Sentences† 6,218 11,839 18,057 # CN tokens 246,371 373,818 620,189 # EN tokens 205,281 312,856 518,137 # Alignment edges 164,033 257,730 421,763 Table 1: GALE WA corpus. † Sentences rejected by the annotators are excluded. four atomic blocks; the CTB segmenter produces five words which all locate within the blocks, so they are all small enough. • Alignment inconsistent: the word aligns to more than one atomic block, but the target expression is contiguous, allowing for correct phrase pair extraction (Zens et al., 2002). For example, in Figure 2(b), the characters in the word “shuang fang”, which is produced by the CTB segmenter, contains two atomic blocks, but the span of the target “to both side” is continuous, therefore the phrase pair • Alignment inconsistent and extraction hindered: the word aligned to more than one atomic block, and the target expression is not contiguous, which hinders correct phrase pair extractions. For example, in Figure 2(c), the word “zeng chan” has to be split in order to match the target language. Table 2 shows the statistics of the three categories of CTB WS on the GALE WA cor"
D14-1173,W03-1726,0,\N,Missing
D15-1119,N15-1105,0,0.0443516,"Missing"
D15-1119,P10-4002,0,0.060743,"., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14 . We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the proposed approach, we set δ = 2 and M = 4 in Algorithm 4. Note that δ can be set to a larger value and seg rev could be applied repeatedly until no additional reordering is possible. As mentioned, the word alignment is noisy and our intention is a robust and rough process; therefore, we restricted seg rev to two applications and did not consider the difference in sentence lengths or different languages during training. Within each iteration, fast alig"
D15-1119,N13-1073,0,0.208935,"), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2 , Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1 http://www.statmt.org/moses/giza/ GIZA++.html 2 https://code.google.com/p/ berkeleyaligner/ 3 http://jasonriesa.github.io/nile/ 4 http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-order alignment model (Och and Ney, 2003). Taking advantage of the simplicity of the IBM model 2, fast align introduces a “tension” parameter to model the overall accordance of word orders and an efficient parameter re-estimation algorithm is devised. It has been reported that the fast align approach is more than 10 times faster than baseline GIZA++, with comparable results in end-to-end French-, Chinese-, and Arabic-to-English translation experiments. Howe"
D15-1119,P15-2023,0,0.0125535,"ing time becomes 2–4 times that of a baseline fast align, which is still at least 2 – 4 times faster than the training time required by baseline GIZA++. Results for German-, French-, and Chinese-English translations are also reported. 2 Segmenting-Reversing Reordering The seg rev is inspired by the “REV preorder” (Katz-Brown and Collins, 2008), which is a simple pre-reordering approach originally designed for the Japanese-to-English translation task. More efficient pre-reordering approaches usually require trained parsers and sophisticated machine learning frameworks (de Gispert et al., 2015; Hoshino et al., 2015). We adopt the REV method in KatzBrown and Collins (2008) considering it is the simplest and lightest pre-reordering approach (to our knowledge), which may bring a minimal effect on the efficiency of fast align. An example seg rev process, where the word alignment is generated by fast align, is illustrated in Fig. 1. The example we selected has relatively correct word alignment and seg rev performs well. In general cases, the alignment has significant noise and the reordering is rougher . Algorithm 1 describes the repeated (δ times) application of the seg rev process, and Algorithm 2 describes"
D15-1119,N03-1017,0,0.065222,"oblem by alternately applying fast align and reordering source sentences during training. Experimental results with JapaneseEnglish translation demonstrate that the proposed approach improves the performance of fast align significantly without the loss of efficiency. Experiments using other languages are also reported. 1 Introduction Aligning words in a parallel corpus is a basic task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2 , Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1 http://www.statmt.org/moses/giza/ GIZA++.html 2 https://code.google.com/p/ berkeleyaligner/ 3 http://jasonriesa.github.io/nile/ 4 http://www.phontron.com/pialign/ fast align5 (D"
D15-1119,P07-2045,0,0.0216472,"tion 4 Experiments and Discussion We applied the proposed approach to JapaneseEnglish translation, a language pair with dramatically different word orders. In addition, we applied the approach to German-English translation, a language pair with relatively different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14 . We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et"
D15-1119,W04-3250,0,0.0329281,"T data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14 . We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the proposed approach, we set δ = 2 and M = 4 in Algorithm 4. Note that δ can be set to a larger value and seg rev could be applied repeatedly until no additional reordering is possible. As mentioned,"
D15-1119,2005.mtsummit-papers.11,0,0.0460474,"ligible. Note that seg rev processes are accelerated easily by parallel processing. 3 GIZA++ FAλini =4.0 FAλini =0.1 iteration 2 iteration 3 iteration 4 Experiments and Discussion We applied the proposed approach to JapaneseEnglish translation, a language pair with dramatically different word orders. In addition, we applied the approach to German-English translation, a language pair with relatively different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14 . We compared GIZA++ and fast a"
D15-1119,P06-1077,0,0.0458507,"t the proposed approach improves the performance of fast align significantly without the loss of efficiency. Experiments using other languages are also reported. 1 Introduction Aligning words in a parallel corpus is a basic task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2 , Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1 http://www.statmt.org/moses/giza/ GIZA++.html 2 https://code.google.com/p/ berkeleyaligner/ 3 http://jasonriesa.github.io/nile/ 4 http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-o"
D15-1119,J06-4004,0,0.0845468,"Missing"
D15-1119,P11-1064,1,0.905317,"Missing"
D15-1119,J03-1002,0,0.0215403,"align significantly without the loss of efficiency. Experiments using other languages are also reported. 1 Introduction Aligning words in a parallel corpus is a basic task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2 , Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1 http://www.statmt.org/moses/giza/ GIZA++.html 2 https://code.google.com/p/ berkeleyaligner/ 3 http://jasonriesa.github.io/nile/ 4 http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-order alignment model (Och and Ney, 2003). Taking advantag"
D15-1119,P03-1021,0,0.0453797,"he approach to German-English translation, a language pair with relatively different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14 . We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the proposed approach"
D15-1119,P02-1040,0,0.0977428,"ly different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14 . We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the proposed approach, we set δ = 2 and M = 4 in Algorithm 4. Note that δ can be set to a larger value an"
D15-1119,D11-1046,0,0.117983,"task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2 , Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1 http://www.statmt.org/moses/giza/ GIZA++.html 2 https://code.google.com/p/ berkeleyaligner/ 3 http://jasonriesa.github.io/nile/ 4 http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-order alignment model (Och and Ney, 2003). Taking advantage of the simplicity of the IBM model 2, fast align introduces a “tension” parameter to model the overall accordance of word orders and an efficient parameter re-estimati"
D15-1119,J07-2003,0,\N,Missing
D15-1128,P07-2045,0,0.0216808,"lish-Spanish) and (English-French), and also language pairs that required a greater amount of word re-ordering for 1 http://www.ted.com example (English-Chinese). The Chinese corpus was segmented using the Stanford Chinese word segmenter (Tseng et al., 2005) according to the Chinese Penn Treebank standard. 3.2 Experimental Methodology Our stream decoder was implemented within the framework of the AUGUSTUS decoder, a hierarchical statistical machine translation decoder (Chiang, 2007) that operates in a similar manner to the moses-chart decoder provided in the Moses machine translation toolkit (Koehn et al., 2007). The training procedure was quite typical: 5-gram language models were used, trained with modified 1092 English input stream: ... we want to encourage a world of creators of inventors of contributors because this world that we live in this interactive world is ours ... Sequence of translated segments: Segment 1: Segment 2: Segment 3: Segment 4: Segment 5: Segment 6: Segment 7: Segment 8: queremos animar a un mundo de creadores de inventores de colaboradores porque este mundo en el que vivimos este interactiva mundo es la nuestra [we want to] [encourage a world of] [creators of inventors] [of"
D15-1128,2008.iwslt-papers.5,0,0.161611,"sing the number of segmentation boundaries to be inserted, prior to the segmentation process. In (Matusov et al., 2007) it was shown that the prediction and use of soft boundaries in the source language text, when used as re-ordering constraints can improve the quality of a speech translation system. (Siahbani et al., 2014) used a pre-segmenter in combination with a left-to-right hierarchical decoder (Watanabe et al., 2006) to achieve a considerably faster decoder in return for a small cost in terms of BLEU score. A phrase-based incremental decoder called the stream decoder was introduced in (Kolss et al., 2008b), and further studied in (Finch et al., 2014). Their results, conducted on translation between European languages, and also on English-Chinese, showed that this approach was able to maintain a high level of translation quality for practically useful levels of latency. The hierarchical decoding strategy proposed here is based on this work. 2.1 Stream Decoding The reader is referred to the original paper (Kolss et al., 2008a) for a complete description of the stream decoding process; in this section we provide a brief summary. Figure 1 depicts a stream decoding process, and the figure applies"
D15-1128,N13-1023,0,0.132338,"segmentation. This approach has the advantage that it can be implemented without the need to modify the machine translation decoding software. In the second type of strategy, which we will call incremental decoding, the segmentation process is performed during the decoding of the input stream. In this approach the segmentation process is able to exploit segmentation cues arising from the decoding process itself. That is to say, the order in which the decoder would prefer to generate the target sequence is taken into account. A number of diverse strategies for presegmentation were studied in (Sridhar et al., 2013). They studied both non-linguistic techniques, that included fixed-length segments, and a “hold-output” method which identifies contiguous blocks of text that do not contain alignments to words outside them, and linguistically-motivated segmentation techniques beased on segmenting on 1089 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1089–1094, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. conjunctions, sentence boundaries and commas. Commas were the most effective segmentation cue in their investigatio"
D15-1128,I05-3027,0,0.0491287,"Missing"
D15-1128,P06-1098,0,0.456205,"rching for segmentation points while optimizing the BLEU score was presented. An attractive characteristic of this approach is that the granularity of the segmentation could be controlled by choosing the number of segmentation boundaries to be inserted, prior to the segmentation process. In (Matusov et al., 2007) it was shown that the prediction and use of soft boundaries in the source language text, when used as re-ordering constraints can improve the quality of a speech translation system. (Siahbani et al., 2014) used a pre-segmenter in combination with a left-to-right hierarchical decoder (Watanabe et al., 2006) to achieve a considerably faster decoder in return for a small cost in terms of BLEU score. A phrase-based incremental decoder called the stream decoder was introduced in (Kolss et al., 2008b), and further studied in (Finch et al., 2014). Their results, conducted on translation between European languages, and also on English-Chinese, showed that this approach was able to maintain a high level of translation quality for practically useful levels of latency. The hierarchical decoding strategy proposed here is based on this work. 2.1 Stream Decoding The reader is referred to the original paper ("
D15-1128,P03-1021,0,0.146861,"Missing"
D15-1128,P14-2090,0,0.161343,"studied both non-linguistic techniques, that included fixed-length segments, and a “hold-output” method which identifies contiguous blocks of text that do not contain alignments to words outside them, and linguistically-motivated segmentation techniques beased on segmenting on 1089 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1089–1094, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. conjunctions, sentence boundaries and commas. Commas were the most effective segmentation cue in their investigation. In (Oda et al., 2014) a strategy for segmentation prior to decoding based on searching for segmentation points while optimizing the BLEU score was presented. An attractive characteristic of this approach is that the granularity of the segmentation could be controlled by choosing the number of segmentation boundaries to be inserted, prior to the segmentation process. In (Matusov et al., 2007) it was shown that the prediction and use of soft boundaries in the source language text, when used as re-ordering constraints can improve the quality of a speech translation system. (Siahbani et al., 2014) used a pre-segmenter"
D15-1128,2001.mtsummit-papers.68,0,0.0256782,"radores porque este mundo en el que vivimos este interactiva mundo es la nuestra [we want to] [encourage a world of] [creators of inventors] [of collaborators] [because this world] [in which we live] [this interactive world] [is ours] Figure 4: Example translation segmentation from the English-Spanish task (Lmax = 8 and Lmin = 4). Kneser-Ney smoothing; MERT (Och, 2003) was used to train the log-linear weights of the models; the decoding was performed with a distortion limit of 20 words. To allow the results to be directly comparable to those in (Finch et al., 2014), the talk level BLEU score (Papineni et al., 2001) was used to evaluate the machine translation quality in all experiments. 3.3 Results The results for decoding with various values of the latency parameters are shown in Figure 3 for English-French, English-Spanish, English-Arabic, English-Hebrew, English-Russian and EnglishChinese. Overall the behavior of the system was quite similar in character to the published results for phrase-based stream decoding for EnglishSpanish (Kolss et al., 2008b; Finch et al., 2014). The hierarchical system seemed to be more sensitive to small values of minimum latency, and less sensitive to larger values. The r"
D15-1128,P02-1040,0,\N,Missing
D15-1128,J07-2003,0,\N,Missing
D15-1128,D08-1076,0,\N,Missing
D15-1209,W13-2201,0,0.0199167,"ing the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a real sentence pair, denoted s, from the GALE ChineseEnglish Word Alignment and Tagging Training corpus (GALE WA corpus)1 with it’s humanannotated word alignment. The Chinese word “HE ZHANG,” denoted wr , which means river custodian, only occurs once in the whole corpus. We performe"
D15-1209,H93-1039,0,0.474031,"Missing"
D15-1209,J93-2003,0,0.167506,"and BLEU scores of end-to-end translation were raised by 0.03 – 1.30. The proposed method also outperformed l0 -normalized GIZA++ and Kneser-Ney smoothed GIZA++. 1 Introduction Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects"
D15-1209,2013.iwslt-evaluation.1,0,0.0234888,"Missing"
D15-1209,P07-1003,0,0.0292356,"essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a real sentence pair, denoted"
D15-1209,D08-1033,0,0.0481387,"Missing"
D15-1209,N13-1073,0,0.0288713,"st statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a real sentence pair, denoted s, from the GALE ChineseEnglish"
D15-1209,P08-1112,0,0.0529177,"Missing"
D15-1209,P07-2045,0,0.00687099,"nd converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out. GIZA++ was run with the default Moses settings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Wat"
D15-1209,2005.mtsummit-papers.11,0,0.0582682,"GALE WA corpus and the OpenMT corpus. They are from the same domain, both contain newswire texts and web blogs. The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set. The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6 . The corpus contains a set of 1,235 sentence pairs that are manually word aligned. The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out. GIZA++ was run with the default Moses settings"
D15-1209,N06-1014,0,0.168812,"e pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Grac¸a et al., 2010). Figure 1(a) shows a rea"
D15-1209,P11-1064,1,0.897734,"Missing"
D15-1209,C00-2163,0,0.833427,"nd-to-end translation were raised by 0.03 – 1.30. The proposed method also outperformed l0 -normalized GIZA++ and Kneser-Ney smoothed GIZA++. 1 Introduction Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a"
D15-1209,J03-1002,0,0.26461,"d word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000). ∗ The author now is affiliated with Google, Japan. The EM algorithm for WA has a great influence in SMT. Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013). However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.” Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Gra"
D15-1209,P03-1021,0,0.015174,"d was tested on two language pairs: Chinese-English and JapaneseEnglish (Table 2). Performance was measured both directly using the agreement with reference to manual WA annotations, and indirectly using the BLEU score in end-to-end machine translation tasks. GIZA++ and our own implementation of standard EM were used as baselines. 4.1 Experimental Settings The Chinese-English experimental data consisted of the GALE WA corpus and the OpenMT corpus. They are from the same domain, both contain newswire texts and web blogs. The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set. The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6 . The corpus contains a set of 1,235 sentence pairs that are manually word aligned. The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server i"
D15-1209,C02-1145,0,0.0241296,"nMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set. The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6 . The corpus contains a set of 1,235 sentence pairs that are manually word aligned. The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7 ; the Japanese texts 4 We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out. GIZA++ was run with the default Moses settings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We imp"
D15-1209,P14-1072,0,0.0607884,"ing phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al., 2012). Our work differs from theirs by addressing the over-fitting directly in the EM algorithm by adopting a leave-one-out approach. Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al., 3 The probability distribution of generating target language words from wr . The description here is only based on IBM model1 for simplicity, and the other alignment models are similar.                    (a)"
D15-1209,P13-1083,1,0.86032,"el 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9 . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS. In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision, recall and F1 (Och and Ney, 2003). T"
D15-1209,J10-3007,0,0.035402,"Missing"
D15-1209,P12-1033,0,0.0623327,"r et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al., 2012). Our work differs from theirs by addressing the over-fitting directly in the EM algorithm by adopting a leave-one-out approach. Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al., 3 The probability distribution of generating target language words from wr . The description here is only based on IBM model1 for simplicity, and the other alignment models are similar.                    (a)                (b)"
D15-1209,P14-2122,1,0.835056,"ly based on IBM model1 for simplicity, and the other alignment models are similar.                    (a)                (b)                    (c) Figure 1: Examples of supervised word alignment. (a) gold alignment; (b) standard EM (GIZA++); (c) Leave-one-out alignment (proposed). 2011), also attempt to address the issue of overfitting, however EM algorithms related to the proposed method have been shown to be more efficient (Wang et al., 2014). 3 Methodology This section first formulates the standard EM for WA, then presents the leave-one-out EM for WA, and finally briefly discusses handling singletons and effecient implementation. The main notation used in this section is shown in Table 1. 3.1 Standard EM for IBM Models 1, 2 and HMM Model To perform WA through EM, the parallel corpus is taken as observed data, the alignments are taken as latent data. In order to maximize the likelihood of the alignment model θ given the data S, the following two steps are conducted iteratively (Brown et al., 1993b; Och and Ney, 2000; Och and Ney,"
D15-1209,P11-1125,1,0.848372,"settings (Koehn et al., 2007). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9 . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS. In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision"
D15-1209,N12-1026,1,0.860338,"07). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4. In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations. The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9 . We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS. In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003). 4.2 Word Alignment Accuracy Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision, recall and F1"
D15-1209,P10-1049,0,0.0224644,"e propose a leave-one-out EM algorithm for WA in this paper. Recently this technique has been applied to avoid over-fitting in kernel density estimation (Roux and Bach, 2011); instead of performing maximum likelihood estimation, maximum leaveone-out likelihood estimation is performed. Figure 1(c) shows the effect of using our technique on the example. The garbage collection has not occurred, and the alignment of the word “HE ZHANG” is identical to the human annotation. 2 Related Work The most related work to this paper is training phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of proba"
D15-1209,W12-3158,0,0.0196466,"out EM algorithm for WA in this paper. Recently this technique has been applied to avoid over-fitting in kernel density estimation (Roux and Bach, 2011); instead of performing maximum likelihood estimation, maximum leaveone-out likelihood estimation is performed. Figure 1(c) shows the effect of using our technique on the example. The garbage collection has not occurred, and the alignment of the word “HE ZHANG” is identical to the human annotation. 2 Related Work The most related work to this paper is training phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012). The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a foundation of most MT systems, our method have a wider application. Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al.,"
D15-1250,N15-1027,0,0.0153685,"sh training data from noise by maximize the conditional likelihood, L = log P (v = 1|C, ti ) + k P j=1 log P (v = 0|C, tik ). The normalization cost can be avoided by using p (ti |C) as an approximation of P (ti |C).2 1 If ti aligns to exactly one source word, ai is the index of this source word; If ti aligns to multiple source words, ai is the index of the aligned word in the middle; If ti is unaligned, they inherit its affiliation from the closest aligned word. 2 The theoretical properties of self-normalization techniques, including NCE and Devlin et al. (2014)’s method, are investigated by Andreas and Klein (2015). 3 Binarized NNJM In this paper, we propose a new framework of the binarized NNJM (BNNJM), which is similar to the NNJM but learns not to predict the next word given the context, but solves a binary classification problem by adding a variable v ∈ {0, 1} that stands for whether the current target word ti is correctly/wrongly produced in terms of source cona +(m−1)/2 text words saii −(m−1)/2 and target history words i−1 ti−n+1 ,   a +(m−1)/2 P v|saii −(m−1)/2 , ti−1 i−n+1 , ti . The BNNJM is learned by a feedforward neural network o with m + n inputs n ai +(m−1)/2 i−1 sai −(m−1)/2 , ti−n+1 ,"
D15-1250,D13-1106,0,0.0273384,"classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost dur"
D15-1250,P14-1129,0,0.0837573,"Missing"
D15-1250,E14-1003,0,0.0168384,"native to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to"
D15-1250,P07-2045,0,0.00974293,"9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5. The NNJM was trained by NCE using UPD and TPD as noise distributions. The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples. The number of noise samples"
D15-1250,P02-1040,0,0.0928294,"tion task, as it did for the other translation tasks. We found that using the BNNJM instead of the NNJM on the JE task did improve translation quality significantly for infrequent words, but not for frequent words. First, we describe how we estimate translation quality for infrequent words. Suppose we have a test set S, a reference set R and a translation set T with I sentences, Si (1 ≤ i ≤ I) , Ri (1 ≤ i ≤ I) , Ti (1 ≤ i ≤ I) Ti contains J individual words, To (Wij ) is how many times Wij occurs in Ti and Ro (Wij ) is how many times Wij occurs in Ri . The general 1-gram translation accuracy (Papineni et al., 2002) is calculated as, I P J P Pg = i=1 j=1 min(To (Wij ),Ro (Wij )) I P J P To (Wij ) i=1 j=1 This general 1-gram translation accuracy does not distinguish word frequency. We use a modified 1-gram translation accuracy that weights infrequent words more heavily, I P J P Pc = i=1 j=1 min(To (Wij ),Ro (Wij ))· I P J P i=1 j=1 1 Occur Wij ( ) To (Wij ) where Occur (Wij ) is how many times Wij occurs in the whole reference set. Note Pc will not be 1 even in the case of completely accurate translations, but it can approximately reflect infrequent word translation accuracy, since correct frequent word t"
D15-1250,C12-2104,0,0.0869208,"kes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. H"
D15-1250,D14-1003,0,0.0152779,"JM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks. 1 ti-n+1~ti-1 P(ti=N) (a) m-word source context P(ti is correct) P(ti is wrong) ti-n+1~ti (b) Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM Introduction Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy (Hu et al., 2014; Devlin et al., 2014; Sundermeyer et al., 2014; Auli et al., 2013; Schwenk, 2012; Sutskever et al., 2014; Bahdanau et al., 2015). Notably, Devlin et al. (2014) proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in Figure 1a. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive nor"
D15-1250,D13-1140,0,0.562022,"t of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, Devlin et al. (2014) presented a technique to train the NNJM to be selfnormalized and avoided the expensive normalization cost during decoding. However, they also note that this self-normalization technique sacrifices neural network accuracy, and the training process for the self-normalized neural network is very slow, as with standard maximum likelihood estimation (MLE). To remedy the problem of long training times in the context of NNLMs, Vaswani et al. (2013) used a method called noise contrastive estimation (NCE). Compared with MLE, NCE does not require repeated summations over the whole vocabulary and performs nonlinear logistic regression to discriminate between the observed data and artificially generated noise. This paper proposes an alternative framework of binarized NNJMs (BNNJM), which are similar to the NNJM, but use the current target word not as the output, but as the input of the neural network, estimating whether the target word under examination is correct or not, as shown in Figure 1b. Because the BNNJM uses the current target word"
D15-1250,D11-1104,0,0.0257476,"g 68.2 68.4 0.29 JE Pc 4.15 4.30 3.6 Pg 61.2 61.7 0.81 FE Pc 6.70 6.86 2.4 Table 5: 1-gram precisions and improvements. grammatical features of Japanese and English are quite different.8 Wrong function word alignments will make noise sampling less effective and therefore lower the BNNJM performance for function word translations. Although wrong word alignments will also make noise sampling less effective for the NNJM, the BNNJM only uses one noise sample for each positive example, so wrong word alignments affect the BNNJM more than the NNJM. 6 Wij ∈ W ords (Ti ) Pg 70.3 70.9 0.85 Related Work Xu et al. (2011) proposed a method to use binary classifiers to learn NNLMs. But they also used the current target word in the output, similarly to NCE. The BNNJM uses the current target word as input, so the information about the current target word can be combined with the context word information and processed in hidden layers. Mauser et al. (2009) presented discriminative lexicon models to predict target words. They train a separate classifier for each target word, as these lexicon models use discrete representations of words and different classifiers do not share features. In contrast, the BNNJM uses rea"
D15-1250,W06-0127,0,0.144311,"P align(sai ,ti 0 ) ti 00 ∈U (sai ) align(sai ,ti Experiments We evaluated the effectiveness of the proposed approach for Chinese-to-English (CE), Japanese-toEnglish (JE) and French-to-English (FE) translation tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The wo"
D15-1250,W04-3250,0,0.0992848,"(E) and time (T) in minutes per epoch for each task. Base NNJM BNNJM UPD TPD UPD TPD CE 32.95 34.36+ 34.60+ 32.89 35.05+* JE 30.13 31.30+ 31.50+ 30.04 31.42+ FE 24.56 24.68 24.80 24.50 25.84+* Table 3: Translation examples. Here, S: source; R: reference; T1 uses NNJM; T2 uses BNNJM. 该− &gt;the 移动− &gt;mobile 持续− &gt;continues 到− &gt;to SUM 该− &gt;this 移动− &gt;movement null− &gt;is 持续− &gt;continued 到− &gt;until SUM Table 2: Translation results. The symbol + and * represent significant differences at the p &lt; 0.01 level against Base and NNJM+UPD, respectively. Significance tests were conducted using bootstrap resampling (Koehn, 2004). the whole neural network (not just the output layer like the NNJM) for each noise sample and thus noise computation is more expensive. However, for different epochs, we resampled the negative example for each positive example, so the BNNJM can make use of different negative examples. 5.2 Results and Discussion Table 1 shows how many epochs these two models needed and the training time for each epoch on a 10-core 3.47GHz Xeon X5690 machine.7 Translation results are shown in Table 2. We can see that using TPD instead of UPD as a noise distribution for the NNJM trained by NCE can speed up the t"
D15-1250,D09-1022,0,0.0791321,"Missing"
D15-1250,P03-1021,0,0.105131,"Zhao et al., 2006) for Chinese and Mecab4 for Japanese. For the FE language pair, we used standard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively. For each translation task, a recent version of Moses HPB decoder (Koehn et al., 2007) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the target side of the training corpus using the IRSTLM Toolkit5 with improved Kneser-Ney smoothing. Feature weights were tuned by MERT (Och, 2003). The word-aligned training set was used to learn the NNJM and the BNNJM.6 For both NNJM and BNNJM, we set m = 7 and n = 5. The NNJM was trained by NCE using UPD and TPD as noise distributions. The BNNJM was trained by standard MLE using UPD and TPD to generate negative examples. The number of noise samples for NCE was set to be 100. For the BNNJM, we used only one negative example for each positive example in each training epoch, as the BNNJM needs to calculate 3 00 ) where align (sai , ti 0 ) is how many times ti 0 is aligned to sai in the parallel corpus. Note that ti could be unaligned, in"
D17-1155,D11-1033,0,0.732683,"two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT EnglishGerman/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points. 1 Introduction In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as"
D17-1155,2015.iwslt-evaluation.1,0,0.0573695,"Missing"
D17-1155,2016.amta-researchers.8,0,0.201897,"it some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rous"
D17-1155,P17-2061,0,0.26518,"anslation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (Jean et al., 2015). ii) an NMT further training (fine-tuning) method (Luong and Manning, 2015). The training is performed in two steps: first, the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Recently, Chu et al. (2017) make an empirical comparison of NMT further training (Luong and Manning, 2015) and domain control (Kobus et al., 2016), which applied word-level domain features to word embedding layer. This approach provides natural baselines for comparison. To the best of our knowledge, there is no existing work concerning instance weighting in 1482 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1482–1488 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics NMT. The main challenge is that NMT is not a liner model or combin"
D17-1155,P13-2119,0,0.0568442,"corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT"
D17-1155,C16-1299,0,0.0285091,"Missing"
D17-1155,2015.mtsummit-papers.10,0,0.103141,"een shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foste"
D17-1155,D10-1044,0,0.0589565,"2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain mod"
D17-1155,D14-1062,0,0.0330635,"Missing"
D17-1155,C14-1182,0,0.0385476,"Missing"
D17-1155,P07-1034,0,0.35209,"by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directl"
D17-1155,kobus-etal-2017-domain,0,0.0736135,"Missing"
D17-1155,W04-3250,0,0.163481,"Missing"
D17-1155,P07-2045,0,0.024644,"Missing"
D17-1155,2015.iwslt-evaluation.11,0,0.461478,"technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT EnglishGerman/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points. 1 Introduction In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translati"
D17-1155,D15-1166,0,0.185744,"ies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT EnglishGerman/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points. 1 Introduction In Statistical Machine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translati"
D17-1155,D09-1074,0,0.0172604,"Missing"
D17-1155,P10-2041,0,0.135114,"hine Translation (SMT), unrelated additional corpora, known as out-ofdomain corpora, have been shown not to benefit some domains and tasks, such as TED-talks and IWSLT tasks (Axelrod et al., 2011; Luong and Manning, 2015). Several Phrase-based SMT (PBSMT) domain adaptation methods have been proposed to overcome this problem of the lack of substantial data in some specific domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity"
D17-1155,P02-1040,0,0.116309,"Missing"
D17-1155,2011.iwslt-evaluation.10,0,0.120719,"Missing"
D17-1155,E12-1055,0,0.0316873,"domains and languages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural M"
D17-1155,E17-3017,0,0.0358134,"Missing"
D17-1155,P13-1082,0,0.0411855,"uages: i) Data selection. The main idea is to score the out-of-domain data using models trained from the in-domain and out-of-domain data, respectively. Then select training data by using these ranked scores (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Hoang and Sima’an, 2014a,b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT"
D17-1155,W10-1759,0,0.0888214,",b; Durrani et al., 2015; Chen et al., 2016). ii) Model Linear Interpolation. Several PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model"
D17-1155,P17-2089,1,0.366616,"Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (Jean et al., 2015). ii) an NMT further training (fine-tuning) method (Luong and Manning, 2015). The training is performed in two steps: first, the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Recently, Chu et al. (2017) make an empirical comparison of NMT further training (Luong and Manning, 2015) and domain contr"
D17-1155,C16-1295,1,0.845483,"ral PBSMT models, such as language models, translation models, and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015, 2016; Imamura and Sumita, 2016). iii) Instance Weighting. Instance Weighting has been applied to several NLP domain adaptation tasks (Jiang and Zhai, 2007), such as POS tagging, entity type classification and especially PBSMT (Matsoukas et al., 2009; Shah et al., 2010; Foster et al., 2010; Rousseau et al., 2011; Zhou et al., 2015; Wang et al., 2016; Imamura and Sumita, 2016). They firstly score each instance/domain by using rules or statistical methods as a weight, and then train PBSMT models by giving each instance/domain the weight. For Neural Machine Translation (NMT) domain adaptation, the sentence selection can also be used (Chen et al., 2016; Wang et al., 2017). Meanwhile, the model linear interpolation is not easily applied to NMT directly, because NMT is not a linear model. There are two methods for model combination of NMT: i) the in-domain model and out-of-domain model can be ensembled (Jean et al., 2015). ii) an NMT further t"
D17-1304,W09-2307,0,0.0166967,"annotation vectors H and dependency annotation vectors D. The current context vector csi and cdi are compute by eq.(4), respectively: J X … CNN yi-1 … Figure 2: SDRNMT-1 for the i-th time step. csi = … … x7 CNN ci … x6 VU2 ? i,1 ? i,1 ? ? i,2 i,2 … i,J x5 U2=&lt;x3, x1, x4, x7 , ε&gt; CNN d1 x4 (14) Experiment Setting up We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M 2 λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We als"
D17-1304,P05-1066,0,0.206266,"Experiment Setting up We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M 2 λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-implement the baseline methods on Nematus toolkit 4 (Sennrich et al.,"
D17-1304,P14-1129,0,0.0116576,"or VUj for Uj . In our experiment, the output of the output layer is 1 × d-dimension vector. It should be noted that the dependency unit is similar to the source dependency feature of Sennrich and Haddow (2016) and the SDR is the same to the source-side representation of Chen et al. (2017). In comparison with Sennrich and Haddow (2016), who concatenate the source dependency labels and word together to enhance the Encoder of NMT, we adapt a separate attention mechanism together with a CNN dependency Encoder. Compared with Chen et al. (2017), which expands the famous neural network joint model (Devlin et al., 2014) with source dependency information to improve the phrase pair translation probability estimation for SMT, we focus on source dependency information to enhance attention probability estimation and to learn corresponding dependency context and RNN hidden state for improving translation. 4 NMT with SDR In this section, we propose two novel NMT models SDRNMT-1 and SDRNMT-2, both of which can make use of source dependency information SDR to enhance Encoder and Decoder of NMT. 4.1 greatly tackle the sparsity issues associated with large dependency units. Motivated by (Sennrich and Haddow, 2016), we"
D17-1304,P16-1078,0,0.0619476,"h translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). I"
D17-1304,P17-2012,0,0.0146372,"ovements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (S"
D17-1304,N16-1101,0,0.0209728,"cially on long sentences. Empirical results on NIST Chinese-toEnglish translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; H"
D17-1304,D14-1176,0,0.0464241,"Missing"
D17-1304,P17-1177,0,0.0492407,"achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent pr"
D17-1304,D13-1176,0,0.0524365,"successfully introduced into statistical machine translation. However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together. In this paper, we propose a novel attentional NMT with source dependency representation to improve translation performance of NMT, especially on long sentences. Empirical results on NIST Chinese-toEnglish translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency"
D17-1304,W15-4906,0,0.0209798,"6; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (Sennrich and Haddow, 2016), in which vector representations of source word and its dependency label are simply concatenated as source input, achieving state-ofthe-art performance in NMT (Bojar et al., 2016). In this paper, we propose a novel NMT with source dependency representation to improve translation performance. Compared with the simple approach of vector concatenation, we learn the Source Dependency Representation (SDR) to compute dependency context vect"
D17-1304,P07-2045,0,0.0114868,"data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-implement the baseline methods on Nematus toolkit 4 (Sennrich et al., 2017). For all NMT systems, we limit the source and target vocabularies to 30K, and the maximum sentence length is 80. The word embedding dimension is 620,5 and the hidden l"
D17-1304,P17-1064,0,0.0668486,"that our method achieves 1.6 BLEU improvements on average over a strong NMT system. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b). Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has"
D17-1304,P02-1040,0,0.11802,"? i,1 ? ? i,2 i,2 … i,J x5 U2=&lt;x3, x1, x4, x7 , ε&gt; CNN d1 x4 (14) Experiment Setting up We carry out experiments on Chinese-to-English translation. The training dataset consists of 1.42M 2 λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments. sentence pairs extract from LDC corpora.3 We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese. We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively. Case-insensitive 4gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test. The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized. We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016). For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly. We try our best to re-"
D17-1304,E17-3017,0,0.043072,"Missing"
D17-1304,W16-2209,0,0.149106,"), thus improving the performance of NMT. In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction. Actually, source dependency information has been shown greatly effective in ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017). In NMT, there has been a quite recent preliminary exploration (Sennrich and Haddow, 2016), in which vector representations of source word and its dependency label are simply concatenated as source input, achieving state-ofthe-art performance in NMT (Bojar et al., 2016). In this paper, we propose a novel NMT with source dependency representation to improve translation performance. Compared with the simple approach of vector concatenation, we learn the Source Dependency Representation (SDR) to compute dependency context vectors and alignment matrices in a more sophisticated manner, which has the potential to make full use of source dependency information. To this end, we create a de"
D17-1304,N16-1004,0,0.0215619,"Missing"
D17-1304,W16-2301,0,\N,Missing
D18-2023,D17-1151,0,0.0576179,"ern. Slightly faster training can make the difference between plausible and impossible experiments (Klein et al., 2017). • RNNsearch-LV (Jean et al., 2015)2 • Luong-NMT (Luong et al., 2015a)3 • DL4MT by Kyunghyun Cho et al.4 • BPE-char (Chung et al., 2016)5 • The researchers using these toolkits may be constrained by the platforms. Unexplored computations or operations may become disallowed or unnecessarily inefficient on a third-party platform, which lowers the chances of developing novel neural network techniques. • Nematus (Sennrich et al., 2017)6 • OpenNMT (Klein et al., 2017)7 • Seq2seq (Britz et al., 2017)8 1 https://github.com/arthurxlw/cytonMt https://github.com/sebastien-j/LV groundhog 3 https://github.com/lmthang/nmt.hybrid 4 https://github.com/nyu-dl/dl4mt-tutorial 5 https://github.com/nyu-dl/dl4mt-cdec 6 https://github.com/EdinburghNLP/nematus 7 https://github.com/OpenNMT/OpenNMT-py 8 https://github.com/google/seq2seq 2 9 https://github.com/paarthneekhara/byteNet-tensorflow (unofficial) and others. 10 https://github.com/facebookresearch/fairseq 11 https://github.com/tensorflow/tensor2tensor 12 https://github.com/marian-nmt/marian 133 Proceedings of the 2018 Conference on Empirical Methods"
D18-2023,E17-3017,0,0.0368643,"Missing"
D18-2023,P15-1001,0,0.0387061,"Wu et al., 2016). Just like Moses (Koehn et al., 2007) does for statistic machine translation (SMT), open-source NMT toolkits contribute greatly to this progress, including but not limited to, • The running efficiency drops, and profiling and optimization also become difficult, as the direct access to GPUs is blocked by the language interpreters or the platforms. NMT systems typically require days or weeks to train, so training efficiency is a paramount concern. Slightly faster training can make the difference between plausible and impossible experiments (Klein et al., 2017). • RNNsearch-LV (Jean et al., 2015)2 • Luong-NMT (Luong et al., 2015a)3 • DL4MT by Kyunghyun Cho et al.4 • BPE-char (Chung et al., 2016)5 • The researchers using these toolkits may be constrained by the platforms. Unexplored computations or operations may become disallowed or unnecessarily inefficient on a third-party platform, which lowers the chances of developing novel neural network techniques. • Nematus (Sennrich et al., 2017)6 • OpenNMT (Klein et al., 2017)7 • Seq2seq (Britz et al., 2017)8 1 https://github.com/arthurxlw/cytonMt https://github.com/sebastien-j/LV groundhog 3 https://github.com/lmthang/nmt.hybrid 4 https://g"
D18-2023,P18-4020,0,0.10183,"Missing"
D18-2023,P17-4012,0,0.0268699,"et al., 2014; Bahdanau et al., 2014; Wu et al., 2016). Just like Moses (Koehn et al., 2007) does for statistic machine translation (SMT), open-source NMT toolkits contribute greatly to this progress, including but not limited to, • The running efficiency drops, and profiling and optimization also become difficult, as the direct access to GPUs is blocked by the language interpreters or the platforms. NMT systems typically require days or weeks to train, so training efficiency is a paramount concern. Slightly faster training can make the difference between plausible and impossible experiments (Klein et al., 2017). • RNNsearch-LV (Jean et al., 2015)2 • Luong-NMT (Luong et al., 2015a)3 • DL4MT by Kyunghyun Cho et al.4 • BPE-char (Chung et al., 2016)5 • The researchers using these toolkits may be constrained by the platforms. Unexplored computations or operations may become disallowed or unnecessarily inefficient on a third-party platform, which lowers the chances of developing novel neural network techniques. • Nematus (Sennrich et al., 2017)6 • OpenNMT (Klein et al., 2017)7 • Seq2seq (Britz et al., 2017)8 1 https://github.com/arthurxlw/cytonMt https://github.com/sebastien-j/LV groundhog 3 https://githu"
D18-2023,P07-2045,0,0.0193419,"ftware. However, there is a common issue – they are all written in script languages with dependencies on third-party GPU platforms (see Table 1) except Marian, which is developed simultaneously with our toolkit. Using script languages and third-party GPU platforms is a two-edged sword. On one hand, it greatly reduces the workload of coding neural networks. On the other hand, it also causes two problems as follows, Introduction Neural Machine Translation (NMT) has made remarkable progress over the past few years (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016). Just like Moses (Koehn et al., 2007) does for statistic machine translation (SMT), open-source NMT toolkits contribute greatly to this progress, including but not limited to, • The running efficiency drops, and profiling and optimization also become difficult, as the direct access to GPUs is blocked by the language interpreters or the platforms. NMT systems typically require days or weeks to train, so training efficiency is a paramount concern. Slightly faster training can make the difference between plausible and impossible experiments (Klein et al., 2017). • RNNsearch-LV (Jean et al., 2015)2 • Luong-NMT (Luong et al., 2015a)3"
D18-2023,1983.tc-1.13,0,0.384524,"Missing"
D18-2023,D15-1166,0,0.231977,"s (Koehn et al., 2007) does for statistic machine translation (SMT), open-source NMT toolkits contribute greatly to this progress, including but not limited to, • The running efficiency drops, and profiling and optimization also become difficult, as the direct access to GPUs is blocked by the language interpreters or the platforms. NMT systems typically require days or weeks to train, so training efficiency is a paramount concern. Slightly faster training can make the difference between plausible and impossible experiments (Klein et al., 2017). • RNNsearch-LV (Jean et al., 2015)2 • Luong-NMT (Luong et al., 2015a)3 • DL4MT by Kyunghyun Cho et al.4 • BPE-char (Chung et al., 2016)5 • The researchers using these toolkits may be constrained by the platforms. Unexplored computations or operations may become disallowed or unnecessarily inefficient on a third-party platform, which lowers the chances of developing novel neural network techniques. • Nematus (Sennrich et al., 2017)6 • OpenNMT (Klein et al., 2017)7 • Seq2seq (Britz et al., 2017)8 1 https://github.com/arthurxlw/cytonMt https://github.com/sebastien-j/LV groundhog 3 https://github.com/lmthang/nmt.hybrid 4 https://github.com/nyu-dl/dl4mt-tutorial 5"
D18-2023,Q16-1027,0,0.0394418,"Missing"
D18-2023,P15-1002,0,0.167553,"s (Koehn et al., 2007) does for statistic machine translation (SMT), open-source NMT toolkits contribute greatly to this progress, including but not limited to, • The running efficiency drops, and profiling and optimization also become difficult, as the direct access to GPUs is blocked by the language interpreters or the platforms. NMT systems typically require days or weeks to train, so training efficiency is a paramount concern. Slightly faster training can make the difference between plausible and impossible experiments (Klein et al., 2017). • RNNsearch-LV (Jean et al., 2015)2 • Luong-NMT (Luong et al., 2015a)3 • DL4MT by Kyunghyun Cho et al.4 • BPE-char (Chung et al., 2016)5 • The researchers using these toolkits may be constrained by the platforms. Unexplored computations or operations may become disallowed or unnecessarily inefficient on a third-party platform, which lowers the chances of developing novel neural network techniques. • Nematus (Sennrich et al., 2017)6 • OpenNMT (Klein et al., 2017)7 • Seq2seq (Britz et al., 2017)8 1 https://github.com/arthurxlw/cytonMt https://github.com/sebastien-j/LV groundhog 3 https://github.com/lmthang/nmt.hybrid 4 https://github.com/nyu-dl/dl4mt-tutorial 5"
D18-2023,P16-1160,0,\N,Missing
D18-2023,W17-4717,0,\N,Missing
D19-1139,P19-1174,1,0.410366,"er than the baseline Transformer (base or big) at the significance level p &lt;0.01 (Collins et al., 2005). BLEU Transformer (base) +RPEHead the highest BLEU scores at dr =320 and dr =256, respectively. This means that the original partial input representation and our RPE can complement each other to improve translation performance. +MPRHead 28 27.5 5.3 0 64 128 192 256 320 384 448 512 dr Figure 2: The BLEU scores on the different dr . et al., 2018). Besides, we reported results of the existing works (Vaswani et al., 2017; Chen et al., 2018; Shaw et al., 2018; Hao et al., 2019; Liu et al., 2019; Chen et al., 2019). We reimplemented the baseline Transformer, Relative PEs, and DiSAN models on the OpenNMT toolkit (Klein et al., 2017). All the models were trained for 200k batches and evaluated on a single V100 GPU. The multi-bleu.perl was used as the evaluation metric to obtain the case-sensitive 4gram BLEU score of EN-DE and ZH-EN tasks. 5.2 Effect of RPEs In this work, we extracted dr dimensions of each word vector to learn recurrent embeddings. To explore the relation between dr and translation performance, Figure 2 shows the translation performance on the different dr . For +RPEHead (or +MPRHead), with"
D19-1139,P18-1008,0,0.205951,"the input to selfattention networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue, and reported improvements on the WMT’14 English-to-German translation task. Inspired by their works (Chen et al., 2018; Hao et al., 2019), we propose an simple and efficient recurrent positional embedding approach to capture order dependencies based on word content in a sentence, thus learning a more effective sentence representation for the Transformer. In addition, we designed two simple multi-head self-attentions to introduce these learned RPEs and"
D19-1139,D18-1317,0,0.0146913,"e a recurrent positional embedding approach based on part of word embedding (6) where Wr ∈ Rdr ×dr is a parameter matrix and br ∈ Rdr is a bias item.1 Note that the xrj is derived from part of the word embedding xj . Finally, there is a sequence R ={r1 , · · · , rJ }, called as recurrent positional embeddings (RPEs). In this work, a bidirectional RNN and a forward RNN (Bahdanau et al., 2015) are used to learn source RPEs and target RPEs, respectively. Noth that the RNN is also replaced by other neural networks for learning order dependency information, such as GRU (Cho et al., 2014), and SRU (Li et al., 2018a). In addition, other sub-sequence {xp1 , · · · , xpJ } is used to gain the reduced dimension input representation P={p1 , · · · , pJ } according to the Section 2.1. Both of R and P will be together as the input to the encoder (or decoder) to learn a more effective source (or target) representation for the Transformer. 4 Neural Machine Translation with RPE To make use of these learned RPEs, we propose two simple methods: RPE head (RPEHead) selfattention and mixed positional representation head (MPRHead) self-attention. Both of RPEHead and MPRHead can utilize RPEs to learn sentence representat"
D19-1139,C18-1271,0,0.0316368,"e a recurrent positional embedding approach based on part of word embedding (6) where Wr ∈ Rdr ×dr is a parameter matrix and br ∈ Rdr is a bias item.1 Note that the xrj is derived from part of the word embedding xj . Finally, there is a sequence R ={r1 , · · · , rJ }, called as recurrent positional embeddings (RPEs). In this work, a bidirectional RNN and a forward RNN (Bahdanau et al., 2015) are used to learn source RPEs and target RPEs, respectively. Noth that the RNN is also replaced by other neural networks for learning order dependency information, such as GRU (Cho et al., 2014), and SRU (Li et al., 2018a). In addition, other sub-sequence {xp1 , · · · , xpJ } is used to gain the reduced dimension input representation P={p1 , · · · , pJ } according to the Section 2.1. Both of R and P will be together as the input to the encoder (or decoder) to learn a more effective source (or target) representation for the Transformer. 4 Neural Machine Translation with RPE To make use of these learned RPEs, we propose two simple methods: RPE head (RPEHead) selfattention and mixed positional representation head (MPRHead) self-attention. Both of RPEHead and MPRHead can utilize RPEs to learn sentence representat"
D19-1139,P19-1352,0,0.0214787,"significantly better than the baseline Transformer (base or big) at the significance level p &lt;0.01 (Collins et al., 2005). BLEU Transformer (base) +RPEHead the highest BLEU scores at dr =320 and dr =256, respectively. This means that the original partial input representation and our RPE can complement each other to improve translation performance. +MPRHead 28 27.5 5.3 0 64 128 192 256 320 384 448 512 dr Figure 2: The BLEU scores on the different dr . et al., 2018). Besides, we reported results of the existing works (Vaswani et al., 2017; Chen et al., 2018; Shaw et al., 2018; Hao et al., 2019; Liu et al., 2019; Chen et al., 2019). We reimplemented the baseline Transformer, Relative PEs, and DiSAN models on the OpenNMT toolkit (Klein et al., 2017). All the models were trained for 200k batches and evaluated on a single V100 GPU. The multi-bleu.perl was used as the evaluation metric to obtain the case-sensitive 4gram BLEU score of EN-DE and ZH-EN tasks. 5.2 Effect of RPEs In this work, we extracted dr dimensions of each word vector to learn recurrent embeddings. To explore the relation between dr and translation performance, Figure 2 shows the translation performance on the different dr . For +RPEHead"
D19-1139,D14-1179,0,0.090697,"Missing"
D19-1139,W19-5330,1,0.891821,"Missing"
D19-1139,P05-1066,0,0.659276,"Architecture Existing NMT systems Transformer (base) Transformer (big) RNMT+SAN Transformer (base)+BiARN Transformer (big)+BiARN Our NMT systems Transformer (base) +Relative PE +DiSAN +RPEHead +MPRHead Transformer (big) +MPRHead newstest2014 #Param 27.3 28.4 28.49 28.21 28.98 65M 213M 378.9M 97.4M 323.5M 27.25 27.60 27.66 28.11* 28.35* 28.22 29.11* 97.35M 97.42M 97.39M 97.84M 97.72M 272.6M 289.1M Table 1: Results for EN-DE translation task. The mark “*” after scores indicates that the model was significantly better than the baseline Transformer (base or big) at the significance level p &lt;0.01 (Collins et al., 2005). BLEU Transformer (base) +RPEHead the highest BLEU scores at dr =320 and dr =256, respectively. This means that the original partial input representation and our RPE can complement each other to improve translation performance. +MPRHead 28 27.5 5.3 0 64 128 192 256 320 384 448 512 dr Figure 2: The BLEU scores on the different dr . et al., 2018). Besides, we reported results of the existing works (Vaswani et al., 2017; Chen et al., 2018; Shaw et al., 2018; Hao et al., 2019; Liu et al., 2019; Chen et al., 2019). We reimplemented the baseline Transformer, Relative PEs, and DiSAN models on the Op"
D19-1139,W18-6419,1,0.850701,"t and convolutional neural networks, rely on a positional embedding (PE) approach to encode order information into the input representation. PE is typically learned based on the position index of each word and is added to corresponding word embedding. This allows the Transformer to encode order dependencies between words in addition to the words themselves. Finally, the Transformer uses these combined vectors as the input to selfattention networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue, and reported improvements on the WMT’14"
D19-1139,D18-1457,0,0.0176619,"Vaswani et al., 2017), without recurrent and convolutional neural networks, rely on a positional embedding (PE) approach to encode order information into the input representation. PE is typically learned based on the position index of each word and is added to corresponding word embedding. This allows the Transformer to encode order dependencies between words in addition to the words themselves. Finally, the Transformer uses these combined vectors as the input to selfattention networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue"
D19-1139,P16-1162,0,0.0431505,"r architecture. 5 5.1 Experiments Experimental Setup The proposed methods were evaluated on the WMT’14 English to German (EN-DE) and NIST Chinese-to-English (ZH-EN) translation tasks. The ZH-EN training set includes 1.28 million bilingual sentence pairs from the LDC corpora, where the NIST06 and the NIST02/NIST03/NIST04 data sets were used as the development and test sets, respectively. The EN-DE training set includes 4.43 million bilingual sentence pairs of the WMT’14 corpora, where the newstest2013 and newstest2014 data sets were used as the development and test sets, respectively. The BPE (Sennrich et al., 2016) was adopted and the vocabulary size was set as 32K. The dimension of all input and output layers was set to 512, and that of the inner feedforward neural network layer was set to 2048. The total heads of all multi-head modules were set to 8 in both encoder and decoder layers. In each training batch, there was a set of sentence pairs containing approximately 4096*4 source tokens and 4096*4 target tokens. For the other setting not mentioned, we followed the setting in Vaswani et al. (2017). Baseline systems included a vanilla Transformer (Vaswani et al., 2017), Relative PEs (Shaw et al., 2018),"
D19-1139,N19-1122,0,0.0571523,"ion networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue, and reported improvements on the WMT’14 English-to-German translation task. Inspired by their works (Chen et al., 2018; Hao et al., 2019), we propose an simple and efficient recurrent positional embedding approach to capture order dependencies based on word content in a sentence, thus learning a more effective sentence representation for the Transformer. In addition, we designed two simple multi-head self-attentions to introduce these learned RPEs and original input repres"
D19-1139,N18-2074,0,0.243236,"nrich et al., 2016) was adopted and the vocabulary size was set as 32K. The dimension of all input and output layers was set to 512, and that of the inner feedforward neural network layer was set to 2048. The total heads of all multi-head modules were set to 8 in both encoder and decoder layers. In each training batch, there was a set of sentence pairs containing approximately 4096*4 source tokens and 4096*4 target tokens. For the other setting not mentioned, we followed the setting in Vaswani et al. (2017). Baseline systems included a vanilla Transformer (Vaswani et al., 2017), Relative PEs (Shaw et al., 2018), and directional SAN (DiSAN) (Shen 1363 System Vaswani et al. (2017) Chen et al. (2018) Hao et al. (2019) This work Architecture Existing NMT systems Transformer (base) Transformer (big) RNMT+SAN Transformer (base)+BiARN Transformer (big)+BiARN Our NMT systems Transformer (base) +Relative PE +DiSAN +RPEHead +MPRHead Transformer (big) +MPRHead newstest2014 #Param 27.3 28.4 28.49 28.21 28.98 65M 213M 378.9M 97.4M 323.5M 27.25 27.60 27.66 28.11* 28.35* 28.22 29.11* 97.35M 97.42M 97.39M 97.84M 97.72M 272.6M 289.1M Table 1: Results for EN-DE translation task. The mark “*” after scores indicates th"
D19-1139,P19-1119,1,0.863476,"Missing"
D19-1139,P18-1166,0,0.130583,"Missing"
D19-1139,C18-1153,0,0.027204,"17), without recurrent and convolutional neural networks, rely on a positional embedding (PE) approach to encode order information into the input representation. PE is typically learned based on the position index of each word and is added to corresponding word embedding. This allows the Transformer to encode order dependencies between words in addition to the words themselves. Finally, the Transformer uses these combined vectors as the input to selfattention networks (SANs), achieving state-of-theart translation performance with several language pairs (Vaswani et al., 2017; Dou et al., 2018; Zhang et al., 2018a; Marie et al., 2018, 2019). ∗ In spite of their success, the input representation only involves static order dependencies based on discrete numerical information. That is, any word in the entire vocabulary has the same PE on the same position index. As a result, the dependencies encoded by the original PEs are independent of word content, which may further hinder the improvement of translation capacity. Recently, Chen et al. (2018) and Hao et al. (2019) introduced the additional source representation learned by an RNN-based encoder into Transformer to alleviate this issue, and reported impro"
D19-5206,P07-2045,0,0.00846795,"Missing"
D19-5206,P18-1073,0,0.151243,"orpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual word embeddings, and the 300 target phrases with the highest scores were kept in the phrase table for each source phrase. As a result, the induced phrase table contains a total of 90M (300k×300) phrase pairs. For this induction, bilingual word embeddings of 300 dimensions were obtained using word embeddings trained with fastText13 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018a). This alignment is the most critical step for unsupervised MT since it is used for initializing the training. It is expected to be extremely difficult for distant languages such as Myanmar, Khmer, and English, as reported by previous work (Søgaard et al., 2018). For each phrase pair, a total of four scores, to be used as features in the phrase tables were computed to mimic phraseReranking Framework and Features We rescored all the hypotheses in the list with a reranking framework using features to better model the fluency and the adequacy of each hypothesis. This method can find a better hy"
D19-5206,J82-2005,0,0.725497,"Missing"
D19-5206,D18-1399,0,0.253018,"orpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual word embeddings, and the 300 target phrases with the highest scores were kept in the phrase table for each source phrase. As a result, the induced phrase table contains a total of 90M (300k×300) phrase pairs. For this induction, bilingual word embeddings of 300 dimensions were obtained using word embeddings trained with fastText13 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018a). This alignment is the most critical step for unsupervised MT since it is used for initializing the training. It is expected to be extremely difficult for distant languages such as Myanmar, Khmer, and English, as reported by previous work (Søgaard et al., 2018). For each phrase pair, a total of four scores, to be used as features in the phrase tables were computed to mimic phraseReranking Framework and Features We rescored all the hypotheses in the list with a reranking framework using features to better model the fluency and the adequacy of each hypothesis. This method can find a better hy"
D19-5206,W18-1811,1,0.810257,"oduction This paper describes neural (NMT) and statistical machine translation systems (SMT) built for the participation of the National Institute of Information and Communications Technology (NICT) in the WAT2019 (Nakazawa et al., 2019) Myanmar-English (my-en) and Khmer-English (km-en) translation tasks.1 We present supervised systems built using the parallel data provided by the organizers and external additional monolingual data. For all the translation directions, we trained supervised NMT and SMT systems, and combined them through n-best list reranking using several informative features (Marie and Fujita, 2018a), as in our previous participation to WAT2018 (Marie et al., 2018). This simple combination method achieved the best results among the submitted MT systems for these tasks according to BLEU (Papineni et al., 1 2 Data preprocessing To train our systems, we used all the bilingual data provided by the organizers. The provided bilingual data comprises different types of corpora: the training data provided by the ALT project2 and additional training data. These additional data are the UCSY corpus, constructed by the University of Computer Studies, Yangon (UCSY),3 for the my-en task, and the ECCC"
D19-5206,Y18-3007,1,0.640277,"slation systems (SMT) built for the participation of the National Institute of Information and Communications Technology (NICT) in the WAT2019 (Nakazawa et al., 2019) Myanmar-English (my-en) and Khmer-English (km-en) translation tasks.1 We present supervised systems built using the parallel data provided by the organizers and external additional monolingual data. For all the translation directions, we trained supervised NMT and SMT systems, and combined them through n-best list reranking using several informative features (Marie and Fujita, 2018a), as in our previous participation to WAT2018 (Marie et al., 2018). This simple combination method achieved the best results among the submitted MT systems for these tasks according to BLEU (Papineni et al., 1 2 Data preprocessing To train our systems, we used all the bilingual data provided by the organizers. The provided bilingual data comprises different types of corpora: the training data provided by the ALT project2 and additional training data. These additional data are the UCSY corpus, constructed by the University of Computer Studies, Yangon (UCSY),3 for the my-en task, and the ECCC corpus, collected by National Institute of Posts, Telecoms & ICT (NI"
D19-5206,N12-1047,0,0.0345309,"ur MT systems. 8 We used a Myanmar dictionary that contains a list of unique 41,343 Myanmar words from https: //github.com/chanmratekoko/Awesome-Myanmar. 70 right) lexicalized reordering models. We also used the default distortion limit of 6. We trained two 4-gram language models, one on the WMT monolingual data for English, on the Common Crawl corpus for Khmer, and on the Wikipedia data for Myanmar, concatenated to the target side of the parallel data, and another one on the target side of the parallel data, using LMPLZ (Heafield et al., 2013). To tune the SMT model weights, we used kb-mira (Cherry and Foster, 2012) and selected the weights giving the best BLEU score for the development data during 15 iterations. --type transformer --max-length 80 --mini-batch-fit --valid-freq 5000 --save-freq 5000 --workspace 10000 --disp-freq 500 --beam-size 12 --normalize 1 --valid-mini-batch 16 --overwrite --early-stopping 5 --cost-type ce-mean-words --valid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 4 --dec-depth 4 --transformer-dropout 0.1 --learn-rate 0.001 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 4"
D19-5206,W19-5330,1,0.784097,"contrasted when ensembling 7 NMT models during decoding (#4). While we observe an improvement of 3.3 BLEU points for (my→en), the improvements for the other directions were limited to 1.0 BLEU points or less. Considering the cost of independently training 7 NMT models and the cost of decoding with 7 models, ensembling does not seem to offer a cost-effective solution. Finally, combining SMT and NMT (#5) provides the best results with improvements over #4 ranging from 0.9 (en→km) to 2.4 BLEU points (my→en). Our results for unsupervised SMT (#6) follow the same trend as the results presented by Marie et al. (2019) for English-Gujarati and English-Kazakh at WMT19: while unsupervised MT has shown promising results for European languages, it is far from being useful for real-world applications, i.e., truly low-resource distant language pairs. We assume that training useful bilingual weaklysupervised/unsupervised bilingual word embeddings for initializing the system remains one of the main challenges. Results Table 6 presents the results for different versions of our SMT and NMT systems. We can observe that NMT (#2) is significantly better than SMT (#1) for my-en while we can observe the reverse for km-en."
D19-5206,P13-2121,0,0.0393413,"Missing"
D19-5206,P02-1040,0,0.117309,"Missing"
D19-5206,P16-1009,0,0.0322957,"alid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 4 --dec-depth 4 --transformer-dropout 0.1 --learn-rate 0.001 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 4 5 6 7 --dim-vocabs 8000 8000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --sync-sgd --exponential-smoothing 4 Back-Translation of Monolingual Data for NMT Parallel data for training NMT can be augmented with synthetic parallel data, generated through a so-called back-translation, to significantly improve translation quality (Sennrich et al., 2016a). We used an NMT system, trained on the parallel data provided by the organizers, to translate target monolingual sentences into the source language. Then, these back-translated sentences were simply mixed with the original parallel data to train from scratch a new source-to-target NMT system. We back-translated 2M sentences randomly sampled from WMT18 English data for my→en and km→en, our Myanmar Wikipedia corpus for en→my, and our Khmer Common Crawl corpus for en→km. Table 4: Parameters of Marian used for training our NMT systems. 3 Supervised MT Systems 3.1 NMT To build competitive NMT sy"
D19-5206,P16-1162,0,0.048767,"alid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 4 --dec-depth 4 --transformer-dropout 0.1 --learn-rate 0.001 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 4 5 6 7 --dim-vocabs 8000 8000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --sync-sgd --exponential-smoothing 4 Back-Translation of Monolingual Data for NMT Parallel data for training NMT can be augmented with synthetic parallel data, generated through a so-called back-translation, to significantly improve translation quality (Sennrich et al., 2016a). We used an NMT system, trained on the parallel data provided by the organizers, to translate target monolingual sentences into the source language. Then, these back-translated sentences were simply mixed with the original parallel data to train from scratch a new source-to-target NMT system. We back-translated 2M sentences randomly sampled from WMT18 English data for my→en and km→en, our Myanmar Wikipedia corpus for en→my, and our Khmer Common Crawl corpus for en→km. Table 4: Parameters of Marian used for training our NMT systems. 3 Supervised MT Systems 3.1 NMT To build competitive NMT sy"
D19-5206,P18-1072,0,0.0254507,"highest scores were kept in the phrase table for each source phrase. As a result, the induced phrase table contains a total of 90M (300k×300) phrase pairs. For this induction, bilingual word embeddings of 300 dimensions were obtained using word embeddings trained with fastText13 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018a). This alignment is the most critical step for unsupervised MT since it is used for initializing the training. It is expected to be extremely difficult for distant languages such as Myanmar, Khmer, and English, as reported by previous work (Søgaard et al., 2018). For each phrase pair, a total of four scores, to be used as features in the phrase tables were computed to mimic phraseReranking Framework and Features We rescored all the hypotheses in the list with a reranking framework using features to better model the fluency and the adequacy of each hypothesis. This method can find a better hypothesis in these merged n-best lists than the one-best hypothesis originated by the individual systems. We chose kb-mira as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018a). All the following features we used are descri"
D19-5209,D18-1549,0,0.0213276,"train from scratch a new source-to-target NMT system. TLM Before training NMT, we used all training corpora including parallel data and monolingual data to train a translation language model (TLM) using XLM3 in order to pretrain the NMT model on 8 GPUs4 . The parameters for training the language model were set as listed in Table 3. --lgs ’en-my’ --mlm steps ’en,my,en-my,my-en’ --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --batch size 32 --bptt 256 --optimizer adam,lr=0.0001 3.4 UNMT To the best of our knowledge, unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Lample and Conneau, 2019) has achieved remarkable results on some similar language pairs. To obtain a better picture of the feasibility of UNMT, we also set up a UNMT system for one truly low-resource and distant language pair: En-My. We tried to train a Transformer-based UNMT model that relies solely on monolingual corpora, with the pre-trained cross-lingual language model using XLM toolkit. Note that this cross-lingual language model was trained solely on monolingual corpora shown in Section 2. We used these m"
D19-5209,W19-5330,1,0.708168,"r for English. The truecaser was trained on the English data, after tokenization. For Myanmar, we used the original tokens. For cleaning, we only applied the Moses script clean-n-corpus.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. Data Preprocessing As parallel data to train our systems, we used all the provided parallel data for all our targeted ∗ Rui and Haipeng have equal contribution to this paper. This work was conductd when Haipeng visited NICT as an internship student. 1 This system is based on our WMT-2019 system (Marie et al., 2019). 2 http://data.statmt.org/news-crawl/ 90 Proceedings of the 6th Workshop on Asian Translation, pages 90–93 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics 3 MT Systems --lgs ’en-my’ --encoder only false --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --tokens per batch 2000 --batch size 32 --bptt 256 --optimizer adam inverse sqrt,beta1=0.9, beta2=0.98,lr=0.0001 --eval bleu true To build competitive NMT systems, we chose to rely on the Transformer architecture (Vaswani et al., 2017) since it has been show"
D19-5209,P02-1040,0,0.110492,"oolkit. Note that this cross-lingual language model was trained solely on monolingual corpora shown in Section 2. We used these monolingual corpora to train the UNMT model for 50000 iterations. The En-My UNMT system was trained on 8 GPUs, with the parameters listed in Table 6. Table 3: Parameters for training TLM. 3.2 Back-translation NMT We trained a Transformer-based NMT model with the pre-trained TLM using XLM toolkit. Our NMT system was consistently trained on 8 GPUs, with the following parameters listed in Table 4. We performed NMT decoding with a single model according to the best BLEU (Papineni et al., 2002) and the perplexity scores. 3 https://github.com/facebookresearch/ XLM 4 NVIDIA @ Tesla @ V100 32Gb. 91 Systems UNMT NMT NMT NMT+TLM NMT+TLM NMT+TLM+back-translation ALT UCSY X X X X X X X X X MONO My-En En-My X 0.81 8.06 14.97 18.42 21.33 29.89 0.31 10.50 14.15 16.12 19.73 19.01 X X Table 5: Results (BLEU-cased) of our MT systems on the test set. ALT denotes that ALT training data was used in this system; UCSY denotes that UCSY training data was used in this system; MONO denotes monolingual training data was used in this system. +TLM denotes that language model pretraining was used in this sy"
D19-5209,P16-1009,0,0.035489,"al., 2017) since it has been shown to outperform, in quality and efficiency, the two other mainstream architectures for NMT known as deep recurrent neural network (deep-RNN) and convolutional neural network (CNN). We chose to rely on the Transformer-based NMT initialized by a pretrained cross-lingual language model (Lample and Conneau, 2019) to train our NMT systems since it had been shown to be efficient in the low-resource language pairs. In order to limit the size of the vocabulary of the NMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training data for English and Myanmar, and used a shared vocabulary for both languages with 60k tokens based on BPE. 3.1 Table 4: Parameters for training NMT. 3.3 We also tried back-translation method (Sennrich et al., 2016a) to make use of monolingual corpora for English-to-Myanmar translation task. Parallel data for training NMT can be augmented with synthetic parallel data, generated through backtranslation, to significantly improve translation quality. For back-translation generation, we used an NMT system, trained on the parallel data pr"
D19-5209,P16-1162,0,0.0522179,"al., 2017) since it has been shown to outperform, in quality and efficiency, the two other mainstream architectures for NMT known as deep recurrent neural network (deep-RNN) and convolutional neural network (CNN). We chose to rely on the Transformer-based NMT initialized by a pretrained cross-lingual language model (Lample and Conneau, 2019) to train our NMT systems since it had been shown to be efficient in the low-resource language pairs. In order to limit the size of the vocabulary of the NMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training data for English and Myanmar, and used a shared vocabulary for both languages with 60k tokens based on BPE. 3.1 Table 4: Parameters for training NMT. 3.3 We also tried back-translation method (Sennrich et al., 2016a) to make use of monolingual corpora for English-to-Myanmar translation task. Parallel data for training NMT can be augmented with synthetic parallel data, generated through backtranslation, to significantly improve translation quality. For back-translation generation, we used an NMT system, trained on the parallel data pr"
D19-5209,P19-1119,1,0.8583,"Missing"
D19-5209,P18-1005,0,0.0212656,"MT system. TLM Before training NMT, we used all training corpora including parallel data and monolingual data to train a translation language model (TLM) using XLM3 in order to pretrain the NMT model on 8 GPUs4 . The parameters for training the language model were set as listed in Table 3. --lgs ’en-my’ --mlm steps ’en,my,en-my,my-en’ --emb dim 1024 --n layers 6 --n heads 8 --dropout 0.1 --attention dropout 0.1 --gelu activation true --batch size 32 --bptt 256 --optimizer adam,lr=0.0001 3.4 UNMT To the best of our knowledge, unsupervised NMT (UNMT) (Artetxe et al., 2018; Lample et al., 2018a; Yang et al., 2018; Lample et al., 2018b; Sun et al., 2019; Lample and Conneau, 2019) has achieved remarkable results on some similar language pairs. To obtain a better picture of the feasibility of UNMT, we also set up a UNMT system for one truly low-resource and distant language pair: En-My. We tried to train a Transformer-based UNMT model that relies solely on monolingual corpora, with the pre-trained cross-lingual language model using XLM toolkit. Note that this cross-lingual language model was trained solely on monolingual corpora shown in Section 2. We used these monolingual corpora to train the UNMT mode"
fujii-etal-2008-producing,P02-1040,0,\N,Missing
fujii-etal-2008-producing,P03-1010,1,\N,Missing
fujii-etal-2008-producing,fujii-etal-2006-test,1,\N,Missing
I17-1002,P05-1066,0,0.210454,"Missing"
I17-1002,P16-2058,0,0.0359599,"Missing"
I17-1002,W04-3208,0,0.0171983,"the OOV’s semantic information. Second, we also extend the contextaware smoothing method to in-vocabulary words, which enhances encoder and decoder of NMT by more effectively utilizing context information by the learned CAR. To this end, we proposed two unique neural networks to learn the contextaware representation for each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focu"
I17-1002,P15-1001,0,0.0317148,"n 5 reports the experimental results obtained in the Chineseto-English task. Finally, we conclude the contributions of the paper and discuss the further work in Section 6. 12 proposed method can smooth the representation of word and reduce the unk’s negative effect in attention model, context annotations and decoding hidden states, thus improving the performance of NMT. these methods improved the translation of OOV, they must learn external bilingual dictionary information in advance. From the point of vocabulary size, many works tried to use a large vocabulary size, thus covering more words. Jean et al. (2015) proposed a method based on importance sampling that allowed NMT model to use a very large target vocabulary for relieving the OOV phenomenon in NMT, which are only designed to reduce the computational complexity in training, not for decoding. Arthur et al. (2016) introduced discrete translation lexicons into NMT to imrpove the translations of these low-frequency words. Mi et al. (2016) proposed a vocabulary manipulation approach by limiting the number of vocabulary being predicted by each batch or sentence, to reduce both the training and the decoding complexity. These methods focused on the"
I17-1002,P02-1051,0,0.0320588,"a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Compared with PBSMT, due to high computational cost, NMT has a more limited vocabulary size and severe OOV phenomenon. The existing PBSMT methods that used external resources to translate unknown words for SMT are hard to be directly intro"
I17-1002,D16-1162,0,0.0215161,"ect in attention model, context annotations and decoding hidden states, thus improving the performance of NMT. these methods improved the translation of OOV, they must learn external bilingual dictionary information in advance. From the point of vocabulary size, many works tried to use a large vocabulary size, thus covering more words. Jean et al. (2015) proposed a method based on importance sampling that allowed NMT model to use a very large target vocabulary for relieving the OOV phenomenon in NMT, which are only designed to reduce the computational complexity in training, not for decoding. Arthur et al. (2016) introduced discrete translation lexicons into NMT to imrpove the translations of these low-frequency words. Mi et al. (2016) proposed a vocabulary manipulation approach by limiting the number of vocabulary being predicted by each batch or sentence, to reduce both the training and the decoding complexity. These methods focused on the translation of OOV itself and ignored the other negative effect caused by the OOV, such as the translations of the words around the OOV. 3 Context-Aware Representation Intuitively, when one understands natural language sentence, especially including polysemy words"
I17-1002,D13-1176,0,0.372214,"representation of unk. To alleviate this problem, we propose a novel contextaware smoothing method to dynamically learn a sentence-specific vector for each word (including OOV words) depending on its local context words in a sentence. The learned context-aware representation is integrated into the NMT to improve the translation performance. Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves 1.78 BLEU improvements on average over a strong attentional NMT, and outperforms some existing systems. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has shown prominent performances in comparison with the conventional Phrase Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003). In NMT, a source sentence is converted into a vector representation by an RNN called encoder, then another RNN ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. 11 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 11–20, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP x1 x2 x3 x4 xu … xJ v1"
I17-1002,P97-1017,0,0.104382,"each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Compared with PBSMT, due to high computational cost, NMT has a more limited vocabulary size and severe OOV phenomenon. The existing PBSMT methods that used external resources to translate u"
I17-1002,P07-2045,0,0.0134503,"Missing"
I17-1002,P09-1089,0,0.0223355,"effectively utilizing context information by the learned CAR. To this end, we proposed two unique neural networks to learn the contextaware representation for each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Compared with PBSMT, due to"
I17-1002,N03-1017,0,0.0655639,"cal context words in a sentence. The learned context-aware representation is integrated into the NMT to improve the translation performance. Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves 1.78 BLEU improvements on average over a strong attentional NMT, and outperforms some existing systems. 1 Introduction Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015), has shown prominent performances in comparison with the conventional Phrase Based Statistical Machine Translation (PBSMT) (Koehn et al., 2003). In NMT, a source sentence is converted into a vector representation by an RNN called encoder, then another RNN ∗ Kehai Chen was an internship research fellow at NICT when conducting this work. † Corresponding author. 11 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 11–20, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP x1 x2 x3 x4 xu … xJ v1 v2 v3 v4 vu … vJ 1 2 3 5 … J 4 … They are beating each other for a dispute x1 x2 x3 v1 v2 v3 1 2 3 (a) … Attention α x4 x5 … v4 v5 … vJ 5 … J 4 xJ … Decoder Encoder Trg2 Encoder Src1 他们 想 通过 打"
I17-1002,P02-1040,0,0.100942,"Missing"
I17-1002,E17-3017,0,0.069622,"Missing"
I17-1002,P16-1100,0,0.0170641,"y, suppose there is a source language sentence, {x1 , x2 , . . . , xj , . . . , xJ }. If the context window is set as 2n (n = 2), the context of each word xi is defined as its historical n words and future n words: Recently, many works exploited the granularity translation unit from words to smaller subwords or characters. Sennrich et al. (2016) introduced a simpler and more effective approach to encode rare and unknown words as sequences of subword units by Byte Pair Encoding (Gage, 1994). This is based on the intuition that various word classes are translatable via smaller units than words. Luong and Manning (2016) segmented the known words into character sequence, and learned the unknown word representation by characterlevel recurrent neural networks, thus achieving open vocabulary NMT. Li et al. (2016) replaced OOVs with in-vocabulary words by semantic similarity to reduce the negative effect for words around the OOVs. Costa-juss`a and Fonollosa (2016) presented a character-based NMT, in which character-level embeddings were in combination with convolutional and highway layers to replace the standard lookup-based word representations. These methods extended the vocabulary to a larger or unlimited voca"
I17-1002,P16-1162,0,0.370792,"presentation (CAR) for each word. 3.1 Feedforward Context-of-Words Model Inspired by the representation learning of word (Bengio et al., 2003), the proposed FCWM includes an input layer, a projection layer, and a non-linear output layer, as shown in Figure 2 (a). Specifically, suppose there is a source language sentence, {x1 , x2 , . . . , xj , . . . , xJ }. If the context window is set as 2n (n = 2), the context of each word xi is defined as its historical n words and future n words: Recently, many works exploited the granularity translation unit from words to smaller subwords or characters. Sennrich et al. (2016) introduced a simpler and more effective approach to encode rare and unknown words as sequences of subword units by Byte Pair Encoding (Gage, 1994). This is based on the intuition that various word classes are translatable via smaller units than words. Luong and Manning (2016) segmented the known words into character sequence, and learned the unknown word representation by characterlevel recurrent neural networks, thus achieving open vocabulary NMT. Li et al. (2016) replaced OOVs with in-vocabulary words by semantic similarity to reduce the negative effect for words around the OOVs. Costa-juss"
I17-1002,P15-1002,0,0.0275471,"d the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Compared with PBSMT, due to high computational cost, NMT has a more limited vocabulary size and severe OOV phenomenon. The existing PBSMT methods that used external resources to translate unknown words for SMT are hard to be directly introduced into NMT, because of NMT’s soft-alignment mechanism (Bahdanau et al., 2015). To relieve the negative effect of unknown words for NMT, Luong et al. (2015) proposed a word alignment algorithm, allowing the NMT system to emit, for each OOV word in the target sentence, the position of its corresponding word in the source sentence, and to translate every OOV in a post-processing step using a external bilingual dictionary. Although The remainder of the paper is organized as follows. Section 2 introduces the related work in the NMT. Section 3 presents two novel neural models to learn the CAR for each word. Section 4 integrates the CAR into the NMT by using smoothing strategies. Section 5 reports the experimental results obtained in the Chineseto-Engl"
I17-1002,C04-1089,0,0.0293475,"ion. Second, we also extend the contextaware smoothing method to in-vocabulary words, which enhances encoder and decoder of NMT by more effectively utilizing context information by the learned CAR. To this end, we proposed two unique neural networks to learn the contextaware representation for each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the corr"
I17-1002,D09-1040,0,0.0346227,"nd decoder of NMT by more effectively utilizing context information by the learned CAR. To this end, we proposed two unique neural networks to learn the contextaware representation for each word depending on its context words in a fixed-size window. We then design four NMT models with CAR to improve translation performance by smoothing the encoder and decoder. Related Work In traditional SMT, there are many research works to improve the translations of OOVs. Fung and Cheung (2004) and Shao and Ng (2004) adopte comparable corpora and web resources to extract translations for each unknown word. Marton et al. (2009) and Mirkin et al. (2009) applied paraphrase model and entailment rules to replace unknown words with in-vocabulary synonyms before translation. A series of works (Knight and Graehl, 1997; Jiang et al., 2007; Al-Onaizan and Knight, 2002) utilized transliteration and web mining techniques with external monolingual/bilingual corpora, comparable data and the web resource to find the translation of the unknown words. Nearly most of the related PBSMT researches focused on finding the correct translation of the unknown words with external resources and ignored the negative effect for other words. Co"
I17-1002,P16-2021,0,0.0221027,"oved the translation of OOV, they must learn external bilingual dictionary information in advance. From the point of vocabulary size, many works tried to use a large vocabulary size, thus covering more words. Jean et al. (2015) proposed a method based on importance sampling that allowed NMT model to use a very large target vocabulary for relieving the OOV phenomenon in NMT, which are only designed to reduce the computational complexity in training, not for decoding. Arthur et al. (2016) introduced discrete translation lexicons into NMT to imrpove the translations of these low-frequency words. Mi et al. (2016) proposed a vocabulary manipulation approach by limiting the number of vocabulary being predicted by each batch or sentence, to reduce both the training and the decoding complexity. These methods focused on the translation of OOV itself and ignored the other negative effect caused by the OOV, such as the translations of the words around the OOV. 3 Context-Aware Representation Intuitively, when one understands natural language sentence, especially including polysemy words or OOVs, one often inferences the meaning of these words depending on its context words. Context plays an important role in"
I17-1016,W16-2206,0,0.0245607,"er- translation under-translation Reduce over-translation unrelated translation No difference 2 11 10 4 73 Table 10: Human evaluation results. Reranking PBMT Outputs with NMT We also did experiments that use the NMT score as an additional feature to rerank PBMT outputs (unique 1, 000-best list). The results are shown 159 reason why NMT is more likely to produce completely unrelated translations, over-translation or under-translation compared to traditional SMT. To relieve NMT of these problems, there are methods that modify the NMT neural network structure (Tu et al., 2016; Meng et al., 2016; Alkhouli et al., 2016) while we rerank NMT outputs by exploiting knowledge from traditional SMT. There are also existing methods that rerank NMT outputs by using target-bidirectional NMT models (Liu et al., 2016; Sennrich et al., 2016a). Their reranking method aims to overcome the issue of unbalanced accuracy in NMT outputs while our reranking method aims to solve the inadequacy problem of NMT. in Table 11. We also copy results of baseline PBMT and NMT from Table 4 for direct comparison. As we can see, using NMT to rerank PBMT outputs achieved improvements over the baseline PBMT system. However, when the baseline N"
I17-1016,D16-1162,1,0.935856,"ct There are a number of methods that combine the two paradigms to address their respective weaknesses. For example, it is possible to incorporate neural features into traditional SMT models to disambiguate hypotheses (Neubig et al., 2015; Stahlberg et al., 2016). However, the search space of traditional SMT is usually limited by translation rule tables, reducing the ability of these models to generate hypotheses on the same level of fluency as NMT, even after reranking. There are also methods that incorporate knowledge from traditional SMT into NMT, such as lexical translation probabilities (Arthur et al., 2016; He et al., 2016), phrase memory (Tang et al., 2016; Zhang et al., 2017), and n-gram posterior probabilities based on traditional SMT translation lattices (Stahlberg et al., 2017). These improve the adequacy of NMT outputs, but do not impose hard alignment constraints like traditional SMT systems and therefore cannot effectively solve all over-translation or under-translation problems. In this paper, we propose a method that exploits an existing phrase-based translation model to compute the phrase-based decoding cost for a given NMT translation.1 That is, we force a phrase-based translation s"
I17-1016,J93-2003,0,0.0533469,"er all source words and does not provides exact mutually-exclusive word or phrase level alignments. As a result, it is known that attentional NMT systems make mistakes in over- or undertranslation (Cohn et al., 2016; Mi et al., 2016). 3 3.1 Phrase-based SMT In phrase-based SMT (Koehn et al., 2003), a phrase-based translation rule r includes a source phrase, a target phrase and a translation score S (r). Phrase-based translation rules can be extracted from the word-aligned training set and then used to translate new sentences. Word alignments for the training set can be obtained by IBM models (Brown et al., 1993). Phrase-based decoding uses a list of translation rules to translate source phrases in the input sentence and generate target phrases from left to right. A basic concept in phrase-based decoding is hypotheses. As shown in Figure 1, the hypothesis H1 consists of two rules r1 and r2 . The score of a hypothesis S (H) can be calculated as the product of the scores of all applied rules.3 An existing hypothesis can be expanded into a new hypothesis by applying a new rule. As shown in Figure 1, H1 can be expanded into H2 , H3 and H4 . H2 cannot be further expanded, because it covers all source words"
I17-1016,W15-5003,1,0.831286,"Eiichro Sumita1 Graham Neubig3,2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Japan 3 Language Technologies Institute, Carnegie Mellon University, USA jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp gneubig@cs.cmu.edu, s-nakamura@is.naist.jp Abstract There are a number of methods that combine the two paradigms to address their respective weaknesses. For example, it is possible to incorporate neural features into traditional SMT models to disambiguate hypotheses (Neubig et al., 2015; Stahlberg et al., 2016). However, the search space of traditional SMT is usually limited by translation rule tables, reducing the ability of these models to generate hypotheses on the same level of fluency as NMT, even after reranking. There are also methods that incorporate knowledge from traditional SMT into NMT, such as lexical translation probabilities (Arthur et al., 2016; He et al., 2016), phrase memory (Tang et al., 2016; Zhang et al., 2017), and n-gram posterior probabilities based on traditional SMT translation lattices (Stahlberg et al., 2017). These improve the adequacy of NMT out"
I17-1016,J03-1002,0,0.0122746,"#Words #Sents #Words #Sents #Words #Vocab #Sents #Words #Sents #Words SOURCE TARGET 1.90M 52.2M 49.7M 113K 376K 3,003 67.6K 63.0K 2,169 46.8K 44.0K 1.99M 54.4M 60.4M 114K 137K 3,003 71.1K 81.1K 1.5K 27.1K 29.8K 954K 40.4M 37.2M 504K 288K 2K 77.5K 75.4K 2K 58.1K 55.5K 3.14M 104M 118M 273K 150K 2K 66.5K 74.6K 2K 70.6K 78.5K Table 3: Data sets. are both 512. We used Byte-pair encoding (BPE) (Sennrich et al., 2016b) and set the vocabulary size to be 50K. We used the Adam algorithm for optimization. To obtain a phrase-based translation rule table for our forced decoding algorithm, we used GIZA++ (Och and Ney, 2003) and grow-diagfinal-and heuristic to obtain symmetric word alignments for the training set. Then we extracted the rule table using Moses (Koehn et al., 2007). Experiments Settings We evaluated the proposed approach for Englishto-Chinese (en-zh), English-to-Japanese (en-ja), English-to-German (en-de) and English-to-French (en-fr) translation tasks. For the en-zh and enja tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 For the en-de and en-fr tasks, we used version 7 of the Europarl corpus as training data, WMT 2014 test sets as our devel"
I17-1016,W16-2323,0,0.404803,"opose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs. 1 Introduction Neural machine translation (NMT), which uses a single large neural network to model the entire translation process, has recently been shown to outperform traditional statistical machine translation (SMT) such as phrase-based machine translation (PBMT) on several translation tasks (Koehn et al., 2003; Bahdanau et al., 2015; Sennrich et al., 2016a). Compared to traditional SMT, NMT generally produces more fluent translations, but often sacrifices adequacy, such as translating source words into completely unrelated target words, over-translation or under-translation (Koehn and Knowles, 2017). 1 In fact, our method can take in the output of any upstream system, but we experiment exclusively with using it to rerank NMT output. 152 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 152–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP rule table. To solve this problem, we propose"
I17-1016,P16-1162,0,0.763028,"opose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs. 1 Introduction Neural machine translation (NMT), which uses a single large neural network to model the entire translation process, has recently been shown to outperform traditional statistical machine translation (SMT) such as phrase-based machine translation (PBMT) on several translation tasks (Koehn et al., 2003; Bahdanau et al., 2015; Sennrich et al., 2016a). Compared to traditional SMT, NMT generally produces more fluent translations, but often sacrifices adequacy, such as translating source words into completely unrelated target words, over-translation or under-translation (Koehn and Knowles, 2017). 1 In fact, our method can take in the output of any upstream system, but we experiment exclusively with using it to rerank NMT output. 152 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 152–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP rule table. To solve this problem, we propose"
I17-1016,W04-3250,0,0.0605453,"anese. We built attentional NMT systems with Lamtram7 . Word embedding size and hidden layer size 5.2 Results and Analysis Table 4 shows results of the phrase-based SMT system8 , the baseline NMT system, the lexicon integration method (Arthur et al., 2016) and the proposed reranking method. We tested three features for reranking: the NMT score Pn , the forced decoding score Sd and a word penalty (WP) feature, which is the length of the translation. The best NMT system and the systems that have no significant difference from the best NMT system at the p &lt; 0.05 level using bootstrap resampling (Koehn, 2004) are shown in bold font. As we can see, integrating lexical translation probabilities improved the baseline NMT system 5 Note that NTCIR-9 only contained a Chinese-to-English translation task, we used English as the source language in our experiments. In NTCIR-9, the development and test sets were both provided for the zh-en task while only the test set was provided for the en-ja task. We used the sentences from the NTCIR-8 en-ja and ja-en test sets as the development set in our experiments. 6 http://sourceforge.net/projects/mecab/files/ 7 https://github.com/neubig/lamtram 8 We used the defaul"
I17-1016,P16-1159,0,0.0418421,"Missing"
I17-1016,P07-2045,0,0.0120866,"4.4M 60.4M 114K 137K 3,003 71.1K 81.1K 1.5K 27.1K 29.8K 954K 40.4M 37.2M 504K 288K 2K 77.5K 75.4K 2K 58.1K 55.5K 3.14M 104M 118M 273K 150K 2K 66.5K 74.6K 2K 70.6K 78.5K Table 3: Data sets. are both 512. We used Byte-pair encoding (BPE) (Sennrich et al., 2016b) and set the vocabulary size to be 50K. We used the Adam algorithm for optimization. To obtain a phrase-based translation rule table for our forced decoding algorithm, we used GIZA++ (Och and Ney, 2003) and grow-diagfinal-and heuristic to obtain symmetric word alignments for the training set. Then we extracted the rule table using Moses (Koehn et al., 2007). Experiments Settings We evaluated the proposed approach for Englishto-Chinese (en-zh), English-to-Japanese (en-ja), English-to-German (en-de) and English-to-French (en-fr) translation tasks. For the en-zh and enja tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 For the en-de and en-fr tasks, we used version 7 of the Europarl corpus as training data, WMT 2014 test sets as our development sets and WMT 2015 test sets as our test sets. The detailed statistics for training, development and test sets are given in Table 3. The word segmentat"
I17-1016,E17-2058,0,0.093465,"Missing"
I17-1016,W17-3204,0,0.0413309,"nguage pairs. 1 Introduction Neural machine translation (NMT), which uses a single large neural network to model the entire translation process, has recently been shown to outperform traditional statistical machine translation (SMT) such as phrase-based machine translation (PBMT) on several translation tasks (Koehn et al., 2003; Bahdanau et al., 2015; Sennrich et al., 2016a). Compared to traditional SMT, NMT generally produces more fluent translations, but often sacrifices adequacy, such as translating source words into completely unrelated target words, over-translation or under-translation (Koehn and Knowles, 2017). 1 In fact, our method can take in the output of any upstream system, but we experiment exclusively with using it to rerank NMT output. 152 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 152–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP rule table. To solve this problem, we propose a soft forced decoding algorithm, which is based on the standard phrase-based decoding algorithm and integrates new types of translation rules (deleting a source word or inserting a target word). The proposed forced decoding algorithm can always su"
I17-1016,P16-2049,0,0.0730172,"am Neubig3,2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Japan 3 Language Technologies Institute, Carnegie Mellon University, USA jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp gneubig@cs.cmu.edu, s-nakamura@is.naist.jp Abstract There are a number of methods that combine the two paradigms to address their respective weaknesses. For example, it is possible to incorporate neural features into traditional SMT models to disambiguate hypotheses (Neubig et al., 2015; Stahlberg et al., 2016). However, the search space of traditional SMT is usually limited by translation rule tables, reducing the ability of these models to generate hypotheses on the same level of fluency as NMT, even after reranking. There are also methods that incorporate knowledge from traditional SMT into NMT, such as lexical translation probabilities (Arthur et al., 2016; He et al., 2016), phrase memory (Tang et al., 2016; Zhang et al., 2017), and n-gram posterior probabilities based on traditional SMT translation lattices (Stahlberg et al., 2017). These improve the adequacy of NMT outputs, but do not impose h"
I17-1016,N03-1017,0,0.218879,"phrase-based translation rule table. We propose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs. 1 Introduction Neural machine translation (NMT), which uses a single large neural network to model the entire translation process, has recently been shown to outperform traditional statistical machine translation (SMT) such as phrase-based machine translation (PBMT) on several translation tasks (Koehn et al., 2003; Bahdanau et al., 2015; Sennrich et al., 2016a). Compared to traditional SMT, NMT generally produces more fluent translations, but often sacrifices adequacy, such as translating source words into completely unrelated target words, over-translation or under-translation (Koehn and Knowles, 2017). 1 In fact, our method can take in the output of any upstream system, but we experiment exclusively with using it to rerank NMT output. 152 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 152–162, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP"
I17-1016,N16-1046,1,0.898579,"Missing"
I17-1016,P16-1008,0,0.0263944,"nder translation. both under- and over- translation under-translation Reduce over-translation unrelated translation No difference 2 11 10 4 73 Table 10: Human evaluation results. Reranking PBMT Outputs with NMT We also did experiments that use the NMT score as an additional feature to rerank PBMT outputs (unique 1, 000-best list). The results are shown 159 reason why NMT is more likely to produce completely unrelated translations, over-translation or under-translation compared to traditional SMT. To relieve NMT of these problems, there are methods that modify the NMT neural network structure (Tu et al., 2016; Meng et al., 2016; Alkhouli et al., 2016) while we rerank NMT outputs by exploiting knowledge from traditional SMT. There are also existing methods that rerank NMT outputs by using target-bidirectional NMT models (Liu et al., 2016; Sennrich et al., 2016a). Their reranking method aims to overcome the issue of unbalanced accuracy in NMT outputs while our reranking method aims to solve the inadequacy problem of NMT. in Table 11. We also copy results of baseline PBMT and NMT from Table 4 for direct comparison. As we can see, using NMT to rerank PBMT outputs achieved improvements over the baselin"
I17-1016,C16-1205,0,0.0455265,"both under- and over- translation under-translation Reduce over-translation unrelated translation No difference 2 11 10 4 73 Table 10: Human evaluation results. Reranking PBMT Outputs with NMT We also did experiments that use the NMT score as an additional feature to rerank PBMT outputs (unique 1, 000-best list). The results are shown 159 reason why NMT is more likely to produce completely unrelated translations, over-translation or under-translation compared to traditional SMT. To relieve NMT of these problems, there are methods that modify the NMT neural network structure (Tu et al., 2016; Meng et al., 2016; Alkhouli et al., 2016) while we rerank NMT outputs by exploiting knowledge from traditional SMT. There are also existing methods that rerank NMT outputs by using target-bidirectional NMT models (Liu et al., 2016; Sennrich et al., 2016a). Their reranking method aims to overcome the issue of unbalanced accuracy in NMT outputs while our reranking method aims to solve the inadequacy problem of NMT. in Table 11. We also copy results of baseline PBMT and NMT from Table 4 for direct comparison. As we can see, using NMT to rerank PBMT outputs achieved improvements over the baseline PBMT system. Howe"
I17-1016,W12-3158,0,0.0325086,"Missing"
I17-1016,D16-1096,0,0.0382651,"ling method to obtain a more diverse n-best list. We test the proposed method on English-toChinese, English-to-Japanese, English-to-German and English-to-French translation tasks, obtaining large improvements over a strong NMT baseline that already incorporates discrete lexicon features. 2 As we can see, NMT only learns an attention (alignment) distribution for each target word over all source words and does not provides exact mutually-exclusive word or phrase level alignments. As a result, it is known that attentional NMT systems make mistakes in over- or undertranslation (Cohn et al., 2016; Mi et al., 2016). 3 3.1 Phrase-based SMT In phrase-based SMT (Koehn et al., 2003), a phrase-based translation rule r includes a source phrase, a target phrase and a translation score S (r). Phrase-based translation rules can be extracted from the word-aligned training set and then used to translate new sentences. Word alignments for the training set can be obtained by IBM models (Brown et al., 1993). Phrase-based decoding uses a list of translation rules to translate source phrases in the input sentence and generate target phrases from left to right. A basic concept in phrase-based decoding is hypotheses. As"
I17-1016,P10-1049,0,0.157145,"to generate new hypotheses with phrase-based SMT, but instead use the phrase-based model to calculate scores for NMT output. In order to do so, we can perform forced decoding, which is very similar to the algorithm in the previous section but discards all partial hypotheses that do not match the NMT output. However, the NMT output is not limited by the phrase-based rule table, so there may be no decoding path that completely matches the NMT output when using only the phrase-based rules. To remedy this problem, inspired by previous work in forced decoding for training phrase-based SMT systems (Wuebker et al., 2010, 2012) we propose a soft forced decoding algorithm that can always successfully find a decoding path for a source sentence F and an NMT translation E. First, we introduce two new types of rules R1 and R2 . s (null → e) = unalign (e) |T | (6) where unalign (e) is how many times e is unaligned in T . One motivation for Equations 5 and 6 is that function words usually have high frequencies, but do not have as clear a correspondence with a word in the other language as content words. As a result, in the training set function words are more often unaligned than content words. As an example, Table"
I17-1016,D13-1112,0,0.0602895,"Missing"
I17-1016,P17-1139,0,0.0346417,"s their respective weaknesses. For example, it is possible to incorporate neural features into traditional SMT models to disambiguate hypotheses (Neubig et al., 2015; Stahlberg et al., 2016). However, the search space of traditional SMT is usually limited by translation rule tables, reducing the ability of these models to generate hypotheses on the same level of fluency as NMT, even after reranking. There are also methods that incorporate knowledge from traditional SMT into NMT, such as lexical translation probabilities (Arthur et al., 2016; He et al., 2016), phrase memory (Tang et al., 2016; Zhang et al., 2017), and n-gram posterior probabilities based on traditional SMT translation lattices (Stahlberg et al., 2017). These improve the adequacy of NMT outputs, but do not impose hard alignment constraints like traditional SMT systems and therefore cannot effectively solve all over-translation or under-translation problems. In this paper, we propose a method that exploits an existing phrase-based translation model to compute the phrase-based decoding cost for a given NMT translation.1 That is, we force a phrase-based translation system to take in the source sentence and generate an NMT translation. The"
I17-1016,W06-0127,0,0.0264745,"evaluated the proposed approach for Englishto-Chinese (en-zh), English-to-Japanese (en-ja), English-to-German (en-de) and English-to-French (en-fr) translation tasks. For the en-zh and enja tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 For the en-de and en-fr tasks, we used version 7 of the Europarl corpus as training data, WMT 2014 test sets as our development sets and WMT 2015 test sets as our test sets. The detailed statistics for training, development and test sets are given in Table 3. The word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab6 for Japanese. We built attentional NMT systems with Lamtram7 . Word embedding size and hidden layer size 5.2 Results and Analysis Table 4 shows results of the phrase-based SMT system8 , the baseline NMT system, the lexicon integration method (Arthur et al., 2016) and the proposed reranking method. We tested three features for reranking: the NMT score Pn , the forced decoding score Sd and a word penalty (WP) feature, which is the length of the translation. The best NMT system and the systems that have no significant difference from the best NMT system at the p &lt; 0.05 lev"
I17-2049,D14-1179,0,0.044685,"Missing"
I17-2049,D15-1166,0,0.28351,"h and h are concatenated into the hidden states h = (h1 , ..., hM ) ∈ RK×M as − → ← −⊤ ⊤ hi = We [ h ⊤ (3) i ; hi ] , where We ∈ RK×2K is a matrix for the affine transform. Each hidden state, represented as a single vector, can be seen a memory vector that includes not only the lexical information at its source position, but also information about the left and right contexts. Then, the decoder predicts the target sentence y using a conditional probability calculated as bellow: Method 3.1 NMT with Attention p(yj |y1,j−1 , x) = softmax(Wo ej + bo ), Our work is based on an attention-based NMT (Luong et al., 2015), which generates a target sentence y = (y1 , ..., yN ) ∈ RVt ×N from the source sentence x = (x1 , ..., xM ) ∈ RVs ×M . Vs and Vt denote the vocabulary size of the source and target side, respectively. The attention-based model comprises two components, an encoder and a decoder. The encoder embeds the source sentence x into vectors through an embedding matrix and produces the hidden states using a bidirectional RNN, which represents a forward and a backward sequence. Thus, we have − → − → h i = enc1 (Ws xi , h i−1 ), ← − ← − h i = enc2 (Ws xi , h i+1 ). ?"" (4) where Wo ∈ RVt ×K and bo ∈ RVt a"
I17-2049,D16-1147,0,0.416559,"calculated as the inner product of the source hidden-state and the target word hidden-state. Note that the source hiddenstate acts as the key to weight itself. It also acts as the value to predict the target word through the context vector. Daniluk et al. (2017) suppose that the dual use of a single vector makes training the model difficult and propose the use of a key-value paired structure, which is a generalized way of storing content in the vector. In this paper, we propose splitting the matrix of the source hidden-states into two parts, an approach suggested by Daniluk et al. (2017) and Miller et al. (2016). The first part refers to the key used to calculate the attention distribution or weights. The second part refers to the value for the source-side context representation. We empirically demonstrate that the separation of the source-side context vector into the key and value significantly improves the performance of an NMT using three different English-to-Japanese translation tasks. In this paper, we propose a neural machine translation (NMT) with a key-value attention mechanism on the source-side encoder. The key-value attention mechanism separates the source-side content vector into two type"
I17-2049,D15-1044,0,0.0448359,"ory cells with a recurrently self-connected linear unit ∗ This work was performed while the first author was affiliated with National Institute of Information and Communication Technology, Kyoto, Japan. 290 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 290–295, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP have been proposed. Attention-based neural networks with soft or hard attention over an input have shown successful results in a wide range of tasks including machine translation (Bahdanau et al., 2015), sentence summarization (Rush et al., 2015), and image captioning (Xu et al., 2015). These attention-based networks use an encoded memory for both as the key and value as described in the Introduction to calculate the output. In contrast to the dual use of a single memory vector, Miller et al. (2016) have proposed keyvalue memory networks with key- and valuememory vectors to solve question-answering tasks, which use a generalized approach to store content in the memory. The key-memory vectors are used to calculate the attention weights, which address relevant memories with respect to the question, whereas the value-memory vectors are u"
isahara-etal-2008-application,P02-1040,0,\N,Missing
isahara-etal-2008-application,P07-2045,0,\N,Missing
isahara-etal-2008-application,P03-1010,1,\N,Missing
isahara-etal-2008-development,vossen-etal-2008-kyoto,1,\N,Missing
isahara-etal-2008-development,kanzaki-etal-2008-extraction,1,\N,Missing
isahara-etal-2008-development,kaji-watanabe-2006-automatic,0,\N,Missing
K19-2004,P18-2058,0,0.0637148,"rsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored"
K19-2004,P13-1023,0,0.215914,"of transducing natural language text into AMR, which is a graph-based formalism used for capturing sentence-level semantics. The AMR framework backgrounds notions of compositionality and derivation, therefore, without explicit correspondence between graph nodes and lexical units. In the representation of AMR framework, the graph nodes are obtained by composition, derivation, lexical decomposition, normalization towards verb senses and so on. The features of the AMR graphs built on these graph nodes is similar UCCA UCCA is a multi-layer linguistic framework for semantic annotation proposed by Abend and Rappoport (2013). UCCA aims to recognize the level of semantic granularity which abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion and does not need to rely on language-specific resources. In the representation of the UCCA framework, some nodes have a one-to-one correspondence with the span in the sentence, which is called 3 MRP-transformed UCCA graph differs from on the terminal nodes from the original UCCA graph. In the original UCCA graph representation, terminal nodes refer to words, and in the MRP-transformed UCCA graph, terminal nodes refer to the lowest lay"
K19-2004,D19-1538,1,0.812819,"2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored (UCCA): treats it as a special constituent tree parsing task a"
K19-2004,W13-2322,0,0.120818,"Missing"
K19-2004,P18-1192,1,0.922788,"rsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored"
K19-2004,W12-3602,0,0.323392,"BERT as the encoder. In the training phase, in order to prevent the nodes from falling into local optimum and the edges unable to get enough training, we use the random sampling method on the golden graph nodes to push as many correct nodes as possible to join the edge training. According to the official results of the evaluation, our system ranked second place in the overall F1 metric among the 16 participating systems. On the DM framework, our system achieved the best results. Our system on other 4 frameworks (PSD, EDS, UCCA, and AMR) are all ranked the third place. 2 2.1 DM and PSD The DM (Ivanova et al., 2012) and PSD (Hajic et al., 2012; Miyao et al., 2014) are two independently developed syntactic-semantic annotations which project semantic forms onto bilexical dependencies in a lossy manner. In the representation of the DM and PSD frameworks, the graph nodes and surface lexical units are strictly anchored. There is an explicit, one-to-many anchoring onto sub-strings of the underlying sentence. These graphs are neither fully connected nor rooted. The graphs of DM and PSD have the following features: • There is only a one-to-one correspondence1 between the graph node and the span in the sentence."
K19-2004,C18-1233,1,0.869457,"Missing"
K19-2004,S19-2002,0,0.248133,"ersal Scenarios” and NICT tenuretrack researcher startup fund “Toward Intelligent Machine Translation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored ("
K19-2004,N19-1423,0,0.0142873,"edge target label. Overall, we use multi-tasking learning strategy, shared hidden representation, The top node uses the same mechanism as node scoring, using binary crossentropy as loss implementation. The node pos tag and node frame label use independent feed-forward classifier, using cross-entropy as loss implementation. The edge source label and edge target label use a biaffine scorer consistent with the edge label, using cross-entropy loss as well. We accumulate the loss of all goals together. Neural Architecture Our model builds the candidate graph nodes representation based on the BERT (Devlin et al., 2019) encoder outputs, i.e., for each token wi , the contextualized vector from BERT encoder is denoted as xi . The candidate span (i, j) representation h consists of two endpoint contextualized vectors (xi , xj ) where i and j are the start and end position of the span in the sentence: h = [xi ; xj ]. 4.2 For the UCCA framework, we directly adopt the minimal span-based parser of Stern et al. (2017) on the converted constituent trees. A constituency tree can be regarded as a collection of labeled spans over a sentence. There are two components in the constituent parsing model: one is to assign the"
K19-2004,C18-1271,1,0.840306,"lation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge tr"
K19-2004,D18-1262,1,0.847064,"lation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge tr"
K19-2004,flickinger-etal-2014-towards,0,0.0124799,"ection, we will introduce this shared task and our modeling approach. Our key idea is to use a graph-based approach rather than a transition-based one; therefore, all the modeling and optimization methods we have on these frameworks are graph-based. The CoNLL shared 46 terminal nodes3 . Other nodes do not have any corresponding relationship with the span, which is introduced as a notion of a semantic constituency that transcends the pure dependency graphs to represent the semantic granularity. The UCCA graph has the following features: (2006) which encode the English Resource Semantics (ERS) (Flickinger et al., 2014). The EDS conversion from under-specified logical forms of the full ERS to variable-free graphs discards partial semantic information which makes the graph abstractly. In the representation of the EDS framework, the graph nodes are independent of surface lexical units. For each graph node, there is an explicit, many-to-many anchoring onto sub-strings of the underlying sentence. The EDS graph has the following features: • There is a one-to-one correspondence between the terminal nodes and the spans in the sentence. • Graph nodes may have multiple parents, among which one is annotated as the pri"
K19-2004,D18-1198,0,0.0419396,"ch span s ∈ Sd do 6: for each word w ∈ s do 7: find a maximum range parent node np of word w whose range size is less than s; 8: move node np to be the child of n(t), and concatenate the original edge label with “ancestor-d” where d represents the original number of edges between the ancestor of np and n(t); 9: remove all the children words of np from s; 10: end for 11: end for 12: until T (t) is a constituent tree 13: set Tc = T (t) Anonymization in AMR Framework Anonymization is an important AMR preprocessing method to reduce the data sparsity issue (Werling et al., 2015; Peng et al., 2017; Guo and Lu, 2018). Following the practice of Zhang et al. (2019a), we first remove senses, wiki links, and polarity attributes in the training dataset. Secondly, we anonymize sub-graphs of named entities which is labeled by one of AMR’s finegrained entity types that contain a name role, and other entities which end with -entity6 . 4 Models To handle different flavors of representation, our system has three types of models: Anchoring-based Pruning Parsing Model, Constituent Parsing Model, Seq2seq-based Parsing Model. 4.1 Anchoring-based Pruning Parsing Model The anchoring-based pruning parsing model is suitable"
K19-2004,K18-2006,1,0.849176,"lation”. 45 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge tr"
K19-2004,K18-2001,0,0.0750259,"Missing"
K19-2004,hajic-etal-2012-announcing,0,0.427424,"Missing"
K19-2004,P14-5010,0,0.00246285,"ne correspondence with its usage pattern string (like “ACT PAT”) in the case of word determination, and the usage pattern has duplicates among different words, the number is much smaller than all item ids size; thus it is more suitable as a learning goal. In the subsequent recovery process, we can use lemma and the usage pattern to restore to the item id. Tokenization, Lemmatization, and Anchor conversion Since the sentence in the training dataset is the original text and no tokenization is performed, and the subsequent processing requires the word root form, we use the Stanford NLP toolkit4 (Manning et al., 2014) to tokenize and lemmatize the original text. As the graph node anchor in the original data is defined at the character level, we need to convert the anchor to the word level. In this process, due to the difference in tokenization criteria and the existence of tokenizing errors, some graph nodes will be converted into the same one in the process of conversion to word-level anchor. Therefore, we performed some post-processing modifications on the tokenization results of the Stanford NLP toolkit to ensure that the graph nodes after the conversion to the word level anchor correspond to the previo"
K19-2004,P19-1298,1,0.811368,"ing representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expression of the candidate graph nodes, (2) Non-strictly anchored (UCCA): treats it as a special constituent tree parsing task and uses an additional component to recover the remote edges, and (3) Completely unanchored (i.e., AMR): uses the Seq2seq model to generate the nodes and the"
K19-2004,J93-2004,0,0.0644555,"propbank/accept-v.xml :: accept.01 :: 7] ACT()[accept.01 :: 0 :: :: 3]{} PAT()[accept.01 :: 1 :: :: 6]{} ORIG()[accept.01 :: 2 :: :: 3, accept.01 :: 1 :: :: 2]{} ev-w21f1 ACT PAT [propbank/access-v.xml :: access.01 :: 2] ACT()[access.01 :: 0 :: :: 1]{} PAT()[access.01 :: 1 :: :: 2]{} acclaim ev-w22f1 ACT PAT [propbank/acclaim-v.xml :: acclaim.01 :: 1] ACT(sub)[]{} PAT(obj1, ving)[acclaim.01 :: 1 :: :: 1]{} Data and Preprocessing ev-w22f2 3.1 Data The CoNLL shared task provides a training dataset of 5 subtasks, of which DM, PSD, and EDS are from Wall Street Journal (WSJ) text of Penn Treebank (Marcus et al., 1993) and contain 35,656 sentences. The UCCA training data comes from the English Web Treebank’s reviews text (Bies et al., 2012) and the English Wikipedia celebrity articles, with a total data volume of 5,672 sentences. AMR annotation data are drawn from a variety of texts, including online discussion forums, newswires, folktales, novels, and Wikipedia articles, which contain a total of 56,240 sentences. 3.2 ACT PAT ?CAUS ACT(sub)[]{} PAT(obj1)[]{} CAUS(for[objpp, ving])[]{} Figure 1: Examples of the most frequent frame-toframeset mapping extracted from “rng pb links.txt”. 3.3 Frame Label Projecti"
K19-2004,S14-2056,0,0.515659,"r to prevent the nodes from falling into local optimum and the edges unable to get enough training, we use the random sampling method on the golden graph nodes to push as many correct nodes as possible to join the edge training. According to the official results of the evaluation, our system ranked second place in the overall F1 metric among the 16 participating systems. On the DM framework, our system achieved the best results. Our system on other 4 frameworks (PSD, EDS, UCCA, and AMR) are all ranked the third place. 2 2.1 DM and PSD The DM (Ivanova et al., 2012) and PSD (Hajic et al., 2012; Miyao et al., 2014) are two independently developed syntactic-semantic annotations which project semantic forms onto bilexical dependencies in a lossy manner. In the representation of the DM and PSD frameworks, the graph nodes and surface lexical units are strictly anchored. There is an explicit, one-to-many anchoring onto sub-strings of the underlying sentence. These graphs are neither fully connected nor rooted. The graphs of DM and PSD have the following features: • There is only a one-to-one correspondence1 between the graph node and the span in the sentence. • Graph nodes can have multiple in-edges or out-e"
K19-2004,P19-1009,0,0.09286,"Missing"
K19-2004,P05-1013,0,0.103018,"transformation is carried out: the pseudo node has a one-to-one relationship with the span in the sentence. The edge between nodes in the graph is transformed into the edge of the pseudo node, and two attributes are added for the edge: the source node label and the target node label which are used to indicate the node label in the original EDS graph. In this way, the many-to-one relationship is converted into a one-to-one relationship. After conversion, we can model the problem using in the same way as DM and PSD as described in Subsection 2.1. 2.3 Based on the above features and inspired by Nivre and Nilsson (2005), we transform the tree composed of primary edges (and nodes) into a constituent syntax tree structure, which is modeled using the constituent syntax tree parsing schema. Use an additional classifier for the remote edges prediction and recovery. 2.4 AMR Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is the task of transducing natural language text into AMR, which is a graph-based formalism used for capturing sentence-level semantics. The AMR framework backgrounds notions of compositionality and derivation, therefore, without explicit correspondence between graph nodes a"
K19-2004,K19-2001,0,0.0683072,"Missing"
K19-2004,P19-1230,1,0.841784,"s of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 45–54 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2004 task combines the following five frameworks for graph-based meaning representation: DM, PSD, EDS, UCCA, and AMR. corresponding graph of the directed graph of EDS, our system further treats EDS as one type, and DM and PSD as another type. Based on the experiences of Jiang et al. (2019) and Zhang et al. (2019a) and our previous works on the Dependency Parsing (Li et al., 2018a,b,d; Zhou and Zhao, 2019; Zhou et al., 2019), Semantic Role Labeling (He et al., 2018b; Cai et al., 2018; Li et al., 2018c, 2019b; He et al., 2019), Universal Conceptual Cognitive Annotation (Jiang et al., 2019), Abstract Mean Representation (Zhang et al., 2019a), Machine Translation (Xiao et al., 2019; Sun et al., 2019; Chen et al., 2019), Language Modeling (Li et al., 2019a; Zhang et al., 2019c,b) tasks, we create three graph parsing models based on the semantic graph flavors: (1) Strictly anchored (DM, PSD, EDS): scores the surface lexical units as nodes of the graph, and performs edge training based on the expres"
K19-2004,E17-1035,0,0.0249758,"of n(t); 5: for each span s ∈ Sd do 6: for each word w ∈ s do 7: find a maximum range parent node np of word w whose range size is less than s; 8: move node np to be the child of n(t), and concatenate the original edge label with “ancestor-d” where d represents the original number of edges between the ancestor of np and n(t); 9: remove all the children words of np from s; 10: end for 11: end for 12: until T (t) is a constituent tree 13: set Tc = T (t) Anonymization in AMR Framework Anonymization is an important AMR preprocessing method to reduce the data sparsity issue (Werling et al., 2015; Peng et al., 2017; Guo and Lu, 2018). Following the practice of Zhang et al. (2019a), we first remove senses, wiki links, and polarity attributes in the training dataset. Secondly, we anonymize sub-graphs of named entities which is labeled by one of AMR’s finegrained entity types that contain a name role, and other entities which end with -entity6 . 4 Models To handle different flavors of representation, our system has three types of models: Anchoring-based Pruning Parsing Model, Constituent Parsing Model, Seq2seq-based Parsing Model. 4.1 Anchoring-based Pruning Parsing Model The anchoring-based pruning parsin"
K19-2004,P17-1076,0,0.0518095,"ent with the edge label, using cross-entropy loss as well. We accumulate the loss of all goals together. Neural Architecture Our model builds the candidate graph nodes representation based on the BERT (Devlin et al., 2019) encoder outputs, i.e., for each token wi , the contextualized vector from BERT encoder is denoted as xi . The candidate span (i, j) representation h consists of two endpoint contextualized vectors (xi , xj ) where i and j are the start and end position of the span in the sentence: h = [xi ; xj ]. 4.2 For the UCCA framework, we directly adopt the minimal span-based parser of Stern et al. (2017) on the converted constituent trees. A constituency tree can be regarded as a collection of labeled spans over a sentence. There are two components in the constituent parsing model: one is to assign the scores directly to span existence which determines the tree structure, and the other one assigns scores to span labels which provides the labeled outputs. (1) The node unary scorer φnode (·) is implemented with feed-forward networks based on the candidate graph nodes representation h: φnode (·) = sigmoid(MLPnode (h)). Constituent Parsing Model (2) Neural Architecture In this model, we also buil"
K19-2004,P19-1119,1,0.795869,"Missing"
K19-2004,P15-1095,0,0.0223824,"spans Sd in the range of n(t); 5: for each span s ∈ Sd do 6: for each word w ∈ s do 7: find a maximum range parent node np of word w whose range size is less than s; 8: move node np to be the child of n(t), and concatenate the original edge label with “ancestor-d” where d represents the original number of edges between the ancestor of np and n(t); 9: remove all the children words of np from s; 10: end for 11: end for 12: until T (t) is a constituent tree 13: set Tc = T (t) Anonymization in AMR Framework Anonymization is an important AMR preprocessing method to reduce the data sparsity issue (Werling et al., 2015; Peng et al., 2017; Guo and Lu, 2018). Following the practice of Zhang et al. (2019a), we first remove senses, wiki links, and polarity attributes in the training dataset. Secondly, we anonymize sub-graphs of named entities which is labeled by one of AMR’s finegrained entity types that contain a name role, and other entities which end with -entity6 . 4 Models To handle different flavors of representation, our system has three types of models: Anchoring-based Pruning Parsing Model, Constituent Parsing Model, Seq2seq-based Parsing Model. 4.1 Anchoring-based Pruning Parsing Model The anchoring-b"
kuroda-etal-2006-getting,A00-2008,0,\N,Missing
kuroda-etal-2006-getting,P98-1013,0,\N,Missing
kuroda-etal-2006-getting,C98-1013,0,\N,Missing
kuroda-etal-2006-getting,P03-1010,1,\N,Missing
kuroda-etal-2006-getting,P03-1068,0,\N,Missing
kuroda-etal-2006-getting,I05-6002,1,\N,Missing
L16-1249,N03-1017,0,0.0633206,"ASTREC, plans to coordinate the development of the ALT Corpus between 2014 to 2018. As a first step, the corpus is scheduled to cover: Indonesian, Japanese, Khmer, Laos, Malay, Myanmar, Philippine, Thai and Vietnamese languages by the end of this time span. In 2014, the project commenced development for the Japanese and Myanmar langauges. The domain is news and 1888 articles were randomly selected from English Wikinews (Wikinews, 2014). 20,000 sentences for building the corpus. Although preparing a parallel corpus may be sufficient for building a standard statistical phrase-based SMT system (Koehn et al., 2003), we also added manual alignment, POS tagging and constituency trees to facilitate further study on SMT and also for other NLP fundamental research. In order to create the corpus, we implemented a web-based tool. This tool will be used in collaboration with research institutions of several Asian countries. The data was represented in XML format for all development steps. The following is an example of the XML data for English sentence “Visitors at the hotel were evacuated to the exhibition hall at street level.”: &lt;source&gt; &lt;text&gt;&lt;![CDATA[Work began in 1900.]]&gt;&lt;/text&gt; &lt;words&gt; &lt;word&gt;&lt;![CDATA[Work"
L16-1249,petrov-etal-2012-universal,0,0.0113014,"ds to align to. For example, the alignment of “the last” in “the last cars to finish” would be aligned to ေနာကဆုံး and မှ in ေနာကဆုံး မှ ပနးဝငေသာ ကားများ and likewise, the alignment of “to finish” would align to ပနးဝငေသာ. All words on both sides were required to be aligned to words in the other language. Unaligned words (null alignments) were not allowed (see Figure 1). 3.4. POS Tagging For the ALT Project, we did not use existing POS tagsets for Myanmar such as (Phyu Hninn et al., 2011). Our POS tag set was intended to be simple and universal similar to proposal of (Slav et al., 2011) (Petrov et al., 2012). The main difference with the Universal POS Tagset is we added necessary language specific POS tags to a core tagset that will be shared with other languages. Myanmar parts of speech are different from English since the grammatical structure is subject, object, verb. Some English parts of speech like determiners, prepositions and auxiliary verbs are not used in Myanmar and some Myanmar parts of speech like post positional markers are not used in English. Although the Myanmar Thadda (a book on Myanmar grammar) 1575 (Commission, 2005) defined 10 parts of speech (adjectives, adverbs, conjunction"
L16-1350,W14-7001,1,0.377751,"her unit belonging to the same paper in the translated data are extracted. Therefore, there is no sentence pairs sharing the same paper across the training, development, development-test and test sets. This is a practical setting of the machine translation for scientific papers in the future where the input sentences are not in the training data. Application: Workshop on Asian Translation (WAT) 4.1. Overview of WAT The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages hosted by JST, NICT and Kyoto University. The first workshop was held in 2014 (Nakazawa et al., 2014) where the ASPEC was centered as the official dataset for the scientific paper translation subtasks. ASPEC was again used in the workshop in 2015 (Nakazawa et al., 2015) to observe the contiguous development of machine translation technologies together with the newly added dataset. WAT will keep growing as the leader of the machine translation technology development in Asia. WAT is working toward the practical use of machine translation among all Asian countries. WAT tries to understand the essence of machine translation and the problems to be solved by collecting and sharing the knowledge acq"
L16-1350,W15-5001,1,0.860841,"development-test and test sets. This is a practical setting of the machine translation for scientific papers in the future where the input sentences are not in the training data. Application: Workshop on Asian Translation (WAT) 4.1. Overview of WAT The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages hosted by JST, NICT and Kyoto University. The first workshop was held in 2014 (Nakazawa et al., 2014) where the ASPEC was centered as the official dataset for the scientific paper translation subtasks. ASPEC was again used in the workshop in 2015 (Nakazawa et al., 2015) to observe the contiguous development of machine translation technologies together with the newly added dataset. WAT will keep growing as the leader of the machine translation technology development in Asia. WAT is working toward the practical use of machine translation among all Asian countries. WAT tries to understand the essence of machine translation and the problems to be solved by collecting and sharing the knowledge acquired in the workshop. WAT is unique in the following points: As described in Section 2., ASPEC-JC includes only 8 scientific fields. The distribution of the fields is s"
L16-1350,2007.mtsummit-papers.63,1,0.812108,"Missing"
N07-1061,J93-2003,0,0.0264764,"h sentences and then translate these n sentences into target language sentences separately. Then, we select the highest scoring sentence from these target sentences. We conducted controlled experiments using the Europarl corpus to evaluate the performance of these pivot strategies as compared to directly trained SMT systems. The phrase translation strategy significantly outperformed the sentence translation strategy. Its relative performance was 0.92 to 0.97 compared to directly trained SMT systems. 1 Introduction The rapid and steady progress in corpus-based machine translation (Nagao, 1981; Brown et al., 1993) has been supported by large parallel corpora such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium and the Europarl corpus (Koehn, 2005), which consists of 11 European languages. However, large parallel corpora do not exist for many language pairs. For example, there are no publicly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to c"
N07-1061,N06-1003,0,0.490064,"rpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Researchers can also make the best use of existing (small) parallel corpora. For example, Nießen and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary in translation. Callison-Burch et al. (2006a) use paraphrases to deal with unknown source language phrases to improve coverage and translation quality. In this paper, we focus on situations where no parallel corpus is available (except a few hundred parallel sentences for tuning parameters). To tackle these extremely scarce training data situations, we propose using a pivot language (English) to bridge the 484 Proceedings of NAACL HLT 2007, pages 484–491, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics source and target languages in translation. We first translate source language sentences or phrases into En"
N07-1061,E06-1032,0,0.041808,"rpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Researchers can also make the best use of existing (small) parallel corpora. For example, Nießen and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary in translation. Callison-Burch et al. (2006a) use paraphrases to deal with unknown source language phrases to improve coverage and translation quality. In this paper, we focus on situations where no parallel corpus is available (except a few hundred parallel sentences for tuning parameters). To tackle these extremely scarce training data situations, we propose using a pivot language (English) to bridge the 484 Proceedings of NAACL HLT 2007, pages 484–491, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics source and target languages in translation. We first translate source language sentences or phrases into En"
N07-1061,W06-3114,0,0.0236121,"is a parallel corpus consisting of the source language and English as well as one consisting of English and the target language. Selecting English as a pivot language is a reasonable pragmatic choice because English is included in parallel corpora more often than other languages are, though any language can be used as a pivot language. In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. 2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely available software, as described in the introduction. The system segments the source sentence into socalled phrases (a number of sequences of consecutive words). Each phrase is translated into a target language phrase. Phrases"
N07-1061,2005.iwslt-1.8,0,0.017247,"the SMT system outˆ that satisfies puts an e ˆ = arg max Pr(e|f ) e e = arg max e M X λm hm (e, f ) (1) (2) m=1 where hm (e, f ) is a feature function and λm is a weight. The system uses a total of eight feature functions: a trigram language model probability of the target language, two phrase translation probabilities (both directions), two lexical translation prob1 http://www.isi.edu/licensed-sw/carmel/ 485 abilities (both directions), a word penalty, a phrase penalty, and a linear reordering penalty. For details on these feature functions, please refer to (Koehn et al., 2003; Koehn, 2004; Koehn et al., 2005). To set the weights, λm , we carried out minimum error rate training (Och, 2003) using BLEU (Papineni et al., 2002) as the objective function. 3 Pivot methods We use the phrase-based SMT system described in the previous section to develop pivot methods. We use English e as the pivot language. We use French f and German g as examples of the source and target languages in this section. We describe two types of pivot strategies, namely phrase translation and sentence translation. The phrase translation strategy means that we directly construct a French-German phrase translation table (phrase-tab"
N07-1061,koen-2004-pharaoh,0,0.0335287,"English as well as one consisting of English and the target language. Selecting English as a pivot language is a reasonable pragmatic choice because English is included in parallel corpora more often than other languages are, though any language can be used as a pivot language. In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. 2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely available software, as described in the introduction. The system segments the source sentence into socalled phrases (a number of sequences of consecutive words). Each phrase is translated into a target language phrase. Phrases may be reordered. Let f be a source sentence (e.g"
N07-1061,E99-1010,0,0.0122897,"electing English as a pivot language is a reasonable pragmatic choice because English is included in parallel corpora more often than other languages are, though any language can be used as a pivot language. In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. 2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely available software, as described in the introduction. The system segments the source sentence into socalled phrases (a number of sequences of consecutive words). Each phrase is translated into a target language phrase. Phrases may be reordered. Let f be a source sentence (e.g, French) and e be a target sentence (e.g., English), the SMT system o"
N07-1061,2005.mtsummit-papers.11,0,0.0238744,"s using the Europarl corpus to evaluate the performance of these pivot strategies as compared to directly trained SMT systems. The phrase translation strategy significantly outperformed the sentence translation strategy. Its relative performance was 0.92 to 0.97 compared to directly trained SMT systems. 1 Introduction The rapid and steady progress in corpus-based machine translation (Nagao, 1981; Brown et al., 1993) has been supported by large parallel corpora such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium and the Europarl corpus (Koehn, 2005), which consists of 11 European languages. However, large parallel corpora do not exist for many language pairs. For example, there are no publicly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract pa"
N07-1061,P03-1021,0,0.0489827,"hm (e, f ) (1) (2) m=1 where hm (e, f ) is a feature function and λm is a weight. The system uses a total of eight feature functions: a trigram language model probability of the target language, two phrase translation probabilities (both directions), two lexical translation prob1 http://www.isi.edu/licensed-sw/carmel/ 485 abilities (both directions), a word penalty, a phrase penalty, and a linear reordering penalty. For details on these feature functions, please refer to (Koehn et al., 2003; Koehn, 2004; Koehn et al., 2005). To set the weights, λm , we carried out minimum error rate training (Och, 2003) using BLEU (Papineni et al., 2002) as the objective function. 3 Pivot methods We use the phrase-based SMT system described in the previous section to develop pivot methods. We use English e as the pivot language. We use French f and German g as examples of the source and target languages in this section. We describe two types of pivot strategies, namely phrase translation and sentence translation. The phrase translation strategy means that we directly construct a French-German phrase translation table (phrase-table for short) from a French-English phrase-table and an English-German phrase-tab"
N07-1061,J05-4003,0,0.016542,"nd the Europarl corpus (Koehn, 2005), which consists of 11 European languages. However, large parallel corpora do not exist for many language pairs. For example, there are no publicly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Researchers can also make the best use of existing (small) parallel corpora. For example, Nießen and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary in translation. Callison-Burch et al. (2006a) use paraphrases to deal with unknown source language phrases to improve coverage and translation quality. In this paper, we focus on s"
N07-1061,P02-1040,0,0.0794409,"where hm (e, f ) is a feature function and λm is a weight. The system uses a total of eight feature functions: a trigram language model probability of the target language, two phrase translation probabilities (both directions), two lexical translation prob1 http://www.isi.edu/licensed-sw/carmel/ 485 abilities (both directions), a word penalty, a phrase penalty, and a linear reordering penalty. For details on these feature functions, please refer to (Koehn et al., 2003; Koehn, 2004; Koehn et al., 2005). To set the weights, λm , we carried out minimum error rate training (Och, 2003) using BLEU (Papineni et al., 2002) as the objective function. 3 Pivot methods We use the phrase-based SMT system described in the previous section to develop pivot methods. We use English e as the pivot language. We use French f and German g as examples of the source and target languages in this section. We describe two types of pivot strategies, namely phrase translation and sentence translation. The phrase translation strategy means that we directly construct a French-German phrase translation table (phrase-table for short) from a French-English phrase-table and an English-German phrase-table. We assume that these French-Eng"
N07-1061,J03-3002,0,0.0116956,"chine translation (Nagao, 1981; Brown et al., 1993) has been supported by large parallel corpora such as the Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium and the Europarl corpus (Koehn, 2005), which consists of 11 European languages. However, large parallel corpora do not exist for many language pairs. For example, there are no publicly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Researchers can also make the best use of existing (small) parallel corpora. For example, Nießen and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to suff"
N07-1061,W02-2026,0,0.0522002,"ot significantly affected by this low precision, as is shown in Table 1. This indicates that recall is more important than precision in building phrase-tables. 5 Related work Pivot languages have been used in rule-based machine translation systems. Boitet (1988) discusses the pros and cons of the pivot approaches in multilingual machine translation. Schubert (1988) argues that a pivot language needs to be a natural language, due to the inherent lack of expressiveness of artificial languages. Pivot-based methods have also been used in other related areas, such as translation lexicon induction (Schafer and Yarowsky, 2002), word alignment (Wang et al., 2006), and cross language information retrieval (Gollins and Sanderson, 2001). The translation disambiguation techniques used in these studies could be used for improving the quality of phrase translation tables. In contrast to these, very little work has been done on pivot-based methods for SMT. Kauers et al. (2002) used an artificial interlingua for spoken language translation. Gispert and Mari˜no (2006) created an English-Catalan parallel corpus by automatically translating the Spanish part of an EnglishSpanish parallel corpus into Catalan with a SpanishCatala"
N07-1061,C88-2125,0,0.70696,"ose sentences were aligned to each other across all four languages, as described in Section 4.1. Thus, there is a lot of room for improvement with respect to recall. Precision, on the other hand, was very low. However, translation performance was not significantly affected by this low precision, as is shown in Table 1. This indicates that recall is more important than precision in building phrase-tables. 5 Related work Pivot languages have been used in rule-based machine translation systems. Boitet (1988) discusses the pros and cons of the pivot approaches in multilingual machine translation. Schubert (1988) argues that a pivot language needs to be a natural language, due to the inherent lack of expressiveness of artificial languages. Pivot-based methods have also been used in other related areas, such as translation lexicon induction (Schafer and Yarowsky, 2002), word alignment (Wang et al., 2006), and cross language information retrieval (Gollins and Sanderson, 2001). The translation disambiguation techniques used in these studies could be used for improving the quality of phrase translation tables. In contrast to these, very little work has been done on pivot-based methods for SMT. Kauers et a"
N07-1061,J04-2003,0,0.0102487,"scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Researchers can also make the best use of existing (small) parallel corpora. For example, Nießen and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary in translation. Callison-Burch et al. (2006a) use paraphrases to deal with unknown source language phrases to improve coverage and translation quality. In this paper, we focus on situations where no parallel corpus is available (except a few hundred parallel sentences for tuning parameters). To tackle these extremely scarce training data situations, we propose using a pivot language (Engl"
N07-1061,P02-1038,0,0.0158062,"t language. In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. 2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely available software, as described in the introduction. The system segments the source sentence into socalled phrases (a number of sequences of consecutive words). Each phrase is translated into a target language phrase. Phrases may be reordered. Let f be a source sentence (e.g, French) and e be a target sentence (e.g., English), the SMT system outˆ that satisfies puts an e ˆ = arg max Pr(e|f ) e e = arg max e M X λm hm (e, f ) (1) (2) m=1 where hm (e, f ) is a feature function and λm is a weight. The system uses a total of eight feature function"
N07-1061,P03-1010,1,0.610487,"e Arabic-English and Chinese-English parallel corpora distributed by the Linguistic Data Consortium and the Europarl corpus (Koehn, 2005), which consists of 11 European languages. However, large parallel corpora do not exist for many language pairs. For example, there are no publicly available Arabic-Chinese large-scale parallel corpora even though there are Arabic-English and Chinese-English parallel corpora. Much work has been done to overcome the lack of parallel corpora. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. Munteanu and Marcu (2005) extract parallel sentences from large Chinese, Arabic, and English non-parallel newspaper corpora. Researchers can also make the best use of existing (small) parallel corpora. For example, Nießen and Ney (2004) use morpho-syntactic information to take into account the interdependencies of inflected forms of the same lemma in order to reduce the amount of bilingual data necessary to sufficiently cover the vocabulary in translation. Callison-Burch et al. (2006a) use paraphrases to deal with unkno"
N07-1061,J03-1002,0,0.0159768,"and the target language. Selecting English as a pivot language is a reasonable pragmatic choice because English is included in parallel corpora more often than other languages are, though any language can be used as a pivot language. In Section 2, we describe a phrase-based statistical machine translation (SMT) system that was used to develop the pivot methods described in Section 3. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. 2 Phrase-based SMT We use a phrase-based SMT system, Pharaoh, (Koehn et al., 2003; Koehn, 2004), which is based on a log-linear formulation (Och and Ney, 2002). It is a state-of-the-art SMT system with freely available software, as described in the introduction. The system segments the source sentence into socalled phrases (a number of sequences of consecutive words). Each phrase is translated into a target language phrase. Phrases may be reordered. Let f be a source sentence (e.g, French) and e be a target sentence (e.g., English"
N07-1061,P06-2112,0,0.228586,"aligned with the phrase e¯ in the parallel corpus. Eq. 7 means that φ(f¯|¯ e) is calculated using maximum likelihood estimation. The definition of the lexical translation probability is pw (f¯|¯ e) = max pw (f¯|¯ e, a) (8) pw (f¯|¯ e, a) = a n Y Ew (fi |¯ e, a) (9) i=1 Ew (fi |¯ e, a) = X 1 w(fi |ej ) |{j|(i, j) ∈ a} |∀(i,j)∈a (10) 2 Feature functions scores are calculated using these probabilities. For example, for a translation probability of a French sentence f = f¯1 .Q . . f¯K and a German sentence g = g¯1 . . . g¯K , K h(g, f ) = log i=1 φ(f¯i |¯ gi ), where K is the number of phrases. 3 Wang et al. (2006) use essentially the same definition to induce the translation probability of the source and target language word alignment that is bridged by an intermediate language. Callison-Burch et al. (2006a) use a similar definition for a paraphrase probability. 486 count(f, e) 0 f 0 count(f , e) w(f |e) = P (11) where count(f, e) gives the total number of times the word f is aligned with the word e in the parallel corpus. Thus, w(f |e) is the maximum likelihood estimation of the word translation probability of f given e. Ew(fi |¯ e, a) is calculated from a word alignment a between a phrase pair f¯ = f"
N07-1061,J04-4002,0,0.0256699,"ir f¯ = f1 f2 . . . fn and e¯ = e1 e2 . . . em where fi is connected to several (|{j|(i, j) ∈ a}|) English words. Thus, Ew(fi |¯ e, a) is the average (or mixture) of w(fi |ej ). This means that Ew(fi |¯ e, a) is an estimation of the probability of fi in a. Consequently, pw (f¯|¯ e, a) estimates the probability of f¯ given e¯ and a using the product of the probabilities Ew(fi |¯ e, a). This assumes that the probability of fi is independent given e¯ and a. pw (f¯|¯ e) takes the highest pw (f¯|¯ e, a) if there are multiple alignments a. This discussion, which is partly based on Section 4.1.2 of (Och and Ney, 2004), means that the lexical translation probability pw (f¯|¯ e) is another probability estimated using the word translation probability w(f |e). The justification of Eqs. 3–6 is straightforward. From the discussion above, we know that the probabilities, φ(f¯|¯ e), φ(¯ e|f¯), φ(¯ g |¯ e), φ(¯ e|¯ g ), pw (f¯|¯ e), ¯ pw (¯ e|f ), pw (¯ g |¯ e), and pw (¯ e|¯ g ) are probabilities in the ordinary sense. Thus, we can derive φ(f¯|¯ g ), ¯ ¯ ¯ φ(¯ g |f ), pw (f |¯ g ), and pw (¯ g |f ) by assuming that these probabilities are independent given an English phrase e¯ (e.g., φ(f¯|¯ g , e¯) = φ(f¯|¯ e)). We"
N07-1061,N03-1017,0,\N,Missing
N16-1046,P15-1033,0,0.0135531,"Missing"
N16-1046,D09-1117,1,0.942985,"Missing"
N16-1046,P15-1001,0,0.0309557,"Missing"
N16-1046,P07-2045,0,0.0575108,"Missing"
N16-1046,N06-1014,0,0.0285511,"Missing"
N16-1046,D14-1209,1,0.815238,"Missing"
N16-1046,P15-1002,0,0.0282201,"Missing"
N16-1046,P00-1056,0,0.0407959,"Missing"
N16-1046,C02-1050,1,0.742069,"Missing"
N16-1046,P15-1113,1,0.580203,"Missing"
N16-1046,N13-1002,0,0.091757,"Missing"
N16-1046,P13-1016,1,\N,Missing
N16-1046,2002.tmi-tutorials.2,0,\N,Missing
N16-1046,P14-1129,0,\N,Missing
N18-1120,D16-1162,1,0.868289,"based method with respect to accuracy, speed, and simplicity of implementation. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translatio"
N18-1120,W17-4716,0,0.0252546,"l corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of transla"
N18-1120,D17-1148,0,0.0329483,"ety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Gre"
N18-1120,W17-4713,0,0.397612,"-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help NMT in translating the input sen1 Note that there are existing retrieval-based methods for phrase-based and hierarchical phrase-based translation (Lopez, 2007; Germann, 2015). However, these methods do not improve translation quality but rather aim to improve the efficiency of the translation models. 1325 Proceedings of NAACL-HLT 2018, pages 1325–1335 c New Orleans, Loui"
N18-1120,1999.tc-1.8,0,0.191708,"17), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help NMT in translating the input sen1 Note that there are existing"
N18-1120,P07-2045,0,0.00831454,"43.76 50.15 METEOR 36.69 39.50 36.57 39.18 en-fr BLEU 57.26 62.60 57.67 63.27 METEOR 43.51 45.83 43.66 46.24 en-es BLEU 55.76 60.51 55.78 60.54 METEOR 42.53 44.58 42.55 44.64 Table 2: Translation results. TRAIN DEV TEST Average Length en-de 674K 1,636 1,689 31 en-fr 665K 1,733 1,710 29 en-es 663K 1,662 1,696 29 dev test Table 3: Data sets. The last line is the average length of English sentences. directions: English-to-German (en-de), Englishto-French (en-fr) and English-to-Spanish (en-es). We cleaned the data by removing repeated sentences and used the train-truecaser.perl script from Moses (Koehn et al., 2007) to truecase the corpus. Then we selected 2000 sentence pairs as development and test sets, respectively. The rest was used as the training set. We removed sentences longer than 80 and 100 from the training and development/test sets respectively. The final numbers of sentence pairs contained in the training, development and test sets are shown in Table 3.5 We applied byte pair encoding (Sennrich et al., 2016b) and set the vocabulary size to be 20K. For translation piece collection, we use GIZA++ (Och and Ney, 2003) and the grow-diag-final-and heuristic (Koehn et al., 2003) to obtain symmetric"
N18-1120,W17-3204,0,0.0504375,"Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT usi"
N18-1120,N03-1017,0,0.0776975,"translation time, and compares favorably to another alternative retrievalbased method with respect to accuracy, speed, and simplicity of implementation. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information abou"
N18-1120,L18-1146,0,0.239139,"Missing"
N18-1120,D07-1104,0,0.0403502,"translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help NMT in translating the input sen1 Note that there are existing retrieval-based methods for phrase-based and hierarchical phrase-based translation (Lopez, 2007; Germann, 2015). However, these methods do not improve translation quality but rather aim to improve the efficiency of the translation models. 1325 Proceedings of NAACL-HLT 2018, pages 1325–1335 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Input: requirements Retrieved: requirements Vorschriften in relation in für to the relation die operational to Eignung the von suitability suitability Um@@ of of bulk carriers terminals schlags@@ anlagen Figure 1: A word-aligned sentence pair retrieved for an input sentence. Red words are unedited words obtained"
N18-1120,N18-1031,0,0.0217635,"Missing"
N18-1120,J03-1002,0,0.00854324,"moving repeated sentences and used the train-truecaser.perl script from Moses (Koehn et al., 2007) to truecase the corpus. Then we selected 2000 sentence pairs as development and test sets, respectively. The rest was used as the training set. We removed sentences longer than 80 and 100 from the training and development/test sets respectively. The final numbers of sentence pairs contained in the training, development and test sets are shown in Table 3.5 We applied byte pair encoding (Sennrich et al., 2016b) and set the vocabulary size to be 20K. For translation piece collection, we use GIZA++ (Och and Ney, 2003) and the grow-diag-final-and heuristic (Koehn et al., 2003) to obtain symmetric word alignments for the training set. We trained an attentional NMT model as our baseline system. The settings for NMT are shown in Table 4. We also compared our method with the search engine guided NMT model (SGNMT, Gu et al. (2017)) in Section 4.5. Word embedding GRU dimension Optimizer Initial learning rate Beam size 512 1024 adam 0.0001 5 Table 4: NMT settings. 5 We put the datasets used in our experiments on Github https://github.com/jingyiz/Data-sampled-preprocessed NMT Ours NMT Ours en-de 1.000 1.005 0.995 1"
N18-1120,P16-1162,0,0.472084,", an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces. We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient. It also causes little increase in the translation time, and compares favorably to another alternative retrievalbased method with respect to accuracy, speed, and simplicity of implementation. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Sennrich et al., 2016a; Wang et al., 2017b) is now the state-of-the-art in machine translation, due to its ability to be trained end-toend on large parallel corpora and capture complex parameterized functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problem"
N18-1120,E17-2058,0,0.0499781,"Missing"
N18-1120,2011.mtsummit-papers.37,1,0.902992,"ables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target technical terms or domain-specific words. Currently there are two main approaches to doing so. Li et al. (2016) and Farajian et al. (2017) use the retrieved sentence pairs to fine tune the parameters of the NMT model which is pre-trained on the whole training corpus. Gu et al. (2017) uses the retrieved sentence pairs as additional inputs to the NMT model to help"
N18-1120,P17-2089,1,0.897275,"Missing"
N18-1120,W17-4742,0,0.0321322,"Missing"
N18-1120,P17-1139,0,0.0211901,"functions that generalize across a variety of syntactic and semantic phenomena. However, it has also been noted that compared to alternatives such as phrase-based translation (Koehn et al., 2003), NMT has trouble with lowfrequency words or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or"
N18-1120,C16-1170,0,0.0180518,"s or phrases (Arthur et al., 2016; Kaiser et al., 2017), and also generalizing across domains (Koehn and Knowles, 2017). A number of methods have been proposed to ameliorate these problems, including methods that incorporate symbolic knowledge such as discrete translation lexicons (Arthur et al., 2016; He et al., 2016; Chatterjee et al., 2017) and phrase tables (Zhang et al., 2017; Tang et al., 2016; Dahlmann et al., 2017), adjust model structures to be more conducive to generalization (Nguyen and Chiang, 2017), or incorporate additional information about domain (Wang et al., 2017a) or topic (Zhang et al., 2016) in translation models. In particular, one paradigm of interest is recent work that augments NMT using retrieval-based models, retrieving sentence pairs from the training corpus that are most similar to the sentence that we want to translate, and then using these to bias the NMT model.1 These methods – reminiscent of translation memory (Utiyama et al., 2011) or example-based translation (Nagao, 1984; Grefenstette, 1999) – are effective because they augment the parametric NMT model with a non-parametric translation memory that allows for increased capacity to measure features of the target tech"
N18-1120,W16-2323,0,\N,Missing
N19-1205,P15-1033,0,0.0276095,"training Ja–En models, and (2) LDC,5 which contains about 1.2M sentence pairs, for training En–Ch and Ch–En models. To tackling the problem of memory consumption, sentences longer than 150 were filtered out, so that models can be trained successfully. Chinese sentences were segmented by the Stanford segmentation tool.6 For Japanese sentences, we followed the preprocessing steps recommended in WAT 2017.7 The test set is a concatenation of NIST MT 2003, 2004, and 2005. Constituent trees are generated by the parser of Kitaev and Klein (2018)8 , and dependency trees are generated by the parser of Dyer et al. (2015)9 . Note that although we only used syntactic information of English in our experiments, our method is also applicable to other languages. We implemented our method on OpenNMT10 (Klein et al., 2017), and used the Transformer as our baseline. As far as we know, there are no previous studies on using syntactic informations in the Transformer. The vocabulary sizes for all languages are 50, 000. Both the encoder and decoder have 6 layers. The dimensions of hidden vectors and word embeddings are 512. The multi-head attention has 5 LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07,"
N19-1205,P16-1078,0,0.0191314,", and +1.0 (Ch–En) BLEU). 1 Introduction In recent years, neural machine translation (NMT) has been developing rapidly and has become the de facto approach for machine translation. To improve the performance of the conventional NMT models (Sutskever et al., 2014; Bahdanau et al., 2014), one effective approach is to incorporate syntactic information into the encoder and/or decoder of the baseline model. Based on how the syntactic information is represented, there are two categories of syntactic NMT methods: (1) those that use treestructured neural networks (NNs) to represent syntax structures (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017), and (2) those that use linear-structured NNs to represent linearized syntax structures (Li et al., 2017; Ma et al., 2017, 2018). For the first category, there is a direct corresponding relationship between the syntactic structure and the NN structure, but the complexity of NN structures usually makes training in∗ Corresponding author efficient. In contrast, for the second category, syntactic structures are linearized and represented using linear-structured recurrent neural networks (RNNs), but the linearized sequence can generally be quite long and therefore tr"
N19-1205,D16-1026,0,0.0365324,"Missing"
N19-1205,D18-1162,0,0.0255237,"Missing"
N19-1205,W16-2209,0,0.0925436,"sent a constituent tree as a sequence whose length is identical to the number of words in the sentence (almost) without losing syntactic information. However, there are no previous studies that use NSD in NMT. Moreover, as demonstrated by our experiments, using NSD in NMT is far from straightforward, so we propose five strategies and verify the effects empirically. The strategies are summarized below. • Extend NSD to dependency trees, which is inspired by the dependency language model (Shen et al., 2010). • Use NSDs as input sequences1 , where an NSD is regarded as a linguistic input feature (Sennrich and Haddow, 2016). • Use NSDs as output sequences, where the NMT and prediction of the NSD are simultaneously trained through multi-task learning (Firat et al., 2016). • Use NSD as positional encoding (PE), which is a syntactic extension of the PE of the Transformer (Vaswani et al., 2017). 1 Throughout this paper, ”input” means the input of an encoder or a decoder rather than the input of the NMT model (i.e., only source sentences), and ”output” is similar. • Add a loss function for NSD to achieve distance-aware training (Shen et al., 2018). 2 S S’ VP Neural Syntactic Distance (NSD) VP The NSD was firstly prop"
N19-1205,D17-1012,0,0.0351546,"Missing"
N19-1205,J10-4005,0,0.0369885,"constituent parsing (Shen et al., 2018; G´omez-Rodr´ıguez and Vilares, 2018). NSD makes it possible to represent a constituent tree as a sequence whose length is identical to the number of words in the sentence (almost) without losing syntactic information. However, there are no previous studies that use NSD in NMT. Moreover, as demonstrated by our experiments, using NSD in NMT is far from straightforward, so we propose five strategies and verify the effects empirically. The strategies are summarized below. • Extend NSD to dependency trees, which is inspired by the dependency language model (Shen et al., 2010). • Use NSDs as input sequences1 , where an NSD is regarded as a linguistic input feature (Sennrich and Haddow, 2016). • Use NSDs as output sequences, where the NMT and prediction of the NSD are simultaneously trained through multi-task learning (Firat et al., 2016). • Use NSD as positional encoding (PE), which is a syntactic extension of the PE of the Transformer (Vaswani et al., 2017). 1 Throughout this paper, ”input” means the input of an encoder or a decoder rather than the input of the NMT model (i.e., only source sentences), and ”output” is similar. • Add a loss function for NSD to achie"
N19-1205,P18-1249,0,0.02509,"op 100K sentence pairs for training En–Ja models and top 1M sentence pairs for training Ja–En models, and (2) LDC,5 which contains about 1.2M sentence pairs, for training En–Ch and Ch–En models. To tackling the problem of memory consumption, sentences longer than 150 were filtered out, so that models can be trained successfully. Chinese sentences were segmented by the Stanford segmentation tool.6 For Japanese sentences, we followed the preprocessing steps recommended in WAT 2017.7 The test set is a concatenation of NIST MT 2003, 2004, and 2005. Constituent trees are generated by the parser of Kitaev and Klein (2018)8 , and dependency trees are generated by the parser of Dyer et al. (2015)9 . Note that although we only used syntactic information of English in our experiments, our method is also applicable to other languages. We implemented our method on OpenNMT10 (Klein et al., 2017), and used the Transformer as our baseline. As far as we know, there are no previous studies on using syntactic informations in the Transformer. The vocabulary sizes for all languages are 50, 000. Both the encoder and decoder have 6 layers. The dimensions of hidden vectors and word embeddings are 512. The multi-head attention"
N19-1205,P17-4012,0,0.0361245,"tered out, so that models can be trained successfully. Chinese sentences were segmented by the Stanford segmentation tool.6 For Japanese sentences, we followed the preprocessing steps recommended in WAT 2017.7 The test set is a concatenation of NIST MT 2003, 2004, and 2005. Constituent trees are generated by the parser of Kitaev and Klein (2018)8 , and dependency trees are generated by the parser of Dyer et al. (2015)9 . Note that although we only used syntactic information of English in our experiments, our method is also applicable to other languages. We implemented our method on OpenNMT10 (Klein et al., 2017), and used the Transformer as our baseline. As far as we know, there are no previous studies on using syntactic informations in the Transformer. The vocabulary sizes for all languages are 50, 000. Both the encoder and decoder have 6 layers. The dimensions of hidden vectors and word embeddings are 512. The multi-head attention has 5 LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06. 6 https://nlp.stanford.edu/software/ stanford-segmenter-2017-06-09.zip 7 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2017/baseline/dataPreparationJE.html 8 https://github.com"
N19-1205,P17-1064,0,0.0196163,"acto approach for machine translation. To improve the performance of the conventional NMT models (Sutskever et al., 2014; Bahdanau et al., 2014), one effective approach is to incorporate syntactic information into the encoder and/or decoder of the baseline model. Based on how the syntactic information is represented, there are two categories of syntactic NMT methods: (1) those that use treestructured neural networks (NNs) to represent syntax structures (Eriguchi et al., 2016; Hashimoto and Tsuruoka, 2017), and (2) those that use linear-structured NNs to represent linearized syntax structures (Li et al., 2017; Ma et al., 2017, 2018). For the first category, there is a direct corresponding relationship between the syntactic structure and the NN structure, but the complexity of NN structures usually makes training in∗ Corresponding author efficient. In contrast, for the second category, syntactic structures are linearized and represented using linear-structured recurrent neural networks (RNNs), but the linearized sequence can generally be quite long and therefore training efficiency is still a problem. Although using a shorter sequence may improve the efficiency, some syntactic information is lost."
N19-1205,I17-1003,0,0.0192217,"he depth of the LCA. 4 dS (wn ) and dG (wn ) are undefined in both of the original papers. We give the definitions here to enable the use of NSD in NMT later. NP d d d S G R NP PRP VBZ VBG NN . She enjoys playing tennis . 4 1 1 2 3 2 1 4 1 3 2 -2 5 0 -2 Figure 1: Example of different NSDs. This example is from Shen et al. (2018). # d D She enjoys playing tennis . -1 2 1 1 3 Figure 2: Example of dependency NSDs. “#” is the root. Dependency labels are omitted. 3 Strategies to improve NMT with NSD 3.1 Dependency NSD There are many previous studies on using dependency trees to improve NMT (Nguyen Le et al., 2017; Wu et al., 2017). Therefore, we extend NSD to dependency trees. Formally, the dependency NSD between two nodes is defined as follows: dD (wi ) = i − h(i), (4) where h(i) is the index of the head of wi , and we let the index of root be 0. Note that dD (wi ) can be either positive or negative, representing the directional information. Figure 2 gives an example. 3.2 NSDs as Input Sequences It is easy to see that for w = (w1 , . . . , wn ), the lengths of dS , dG , dR and dD are all n. Denoting the NSD sequence as d = (d1 , . . . , dn ), we can see that di ∈ Z, i ∈ [1, n], so we can obtain a seq"
N19-1205,P02-1040,0,0.103502,"0.1 (Srivastava et al., 2014). The number of training epochs was fixed to 50, and we used the model which performs the best on the development set for testing. As for optimization, we used the Adam optimizer (Kingma and Ba, 2014), with β1 = 0.9, β2 = 0.998, and  = 10−9 . Warmup and decay strategy for learning rate of Vaswani et al. (2017) are also used, with 8, 000 warmup steps. We also used the label smoothing strategy (Szegedy et al., 2016) with ls = 0.1. 4.2 Experimental Results Table 1 compares the effects of the strategies. We evaluate the proposed strategies using characterlevel BLEU (Papineni et al., 2002) for Chinese and Japanese, and case-insensitive BLEU for English. Comparison of different NSDs. The first five rows of Table 1 compare the results of using different NSDs. When NSD was used at the source side (En–Ja/En–Ch), all kinds of NSDs improved translation performance. This indicates that NSD can be regarded as a useful linguistic feature to improve NMT. In contrast, when NSD was used at the target side (Ja–En/Ch–En), dS and dG hurt the performance. This is because the values of dS and dG are volatile. A tiny change of syntactic structure often causes a big change of dS and dG . Since th"
N19-1205,P18-1108,0,0.300875,"or efficient. In contrast, for the second category, syntactic structures are linearized and represented using linear-structured recurrent neural networks (RNNs), but the linearized sequence can generally be quite long and therefore training efficiency is still a problem. Although using a shorter sequence may improve the efficiency, some syntactic information is lost. We propose a method of using syntactic information in NMT that overcomes the disadvantages of both methods. The basis of our method is the neural syntactic distance (NSD), a recently proposed concept used for constituent parsing (Shen et al., 2018; G´omez-Rodr´ıguez and Vilares, 2018). NSD makes it possible to represent a constituent tree as a sequence whose length is identical to the number of words in the sentence (almost) without losing syntactic information. However, there are no previous studies that use NSD in NMT. Moreover, as demonstrated by our experiments, using NSD in NMT is far from straightforward, so we propose five strategies and verify the effects empirically. The strategies are summarized below. • Extend NSD to dependency trees, which is inspired by the dependency language model (Shen et al., 2010). • Use NSDs as input"
N19-1205,P17-1065,0,0.0341685,"CA. 4 dS (wn ) and dG (wn ) are undefined in both of the original papers. We give the definitions here to enable the use of NSD in NMT later. NP d d d S G R NP PRP VBZ VBG NN . She enjoys playing tennis . 4 1 1 2 3 2 1 4 1 3 2 -2 5 0 -2 Figure 1: Example of different NSDs. This example is from Shen et al. (2018). # d D She enjoys playing tennis . -1 2 1 1 3 Figure 2: Example of dependency NSDs. “#” is the root. Dependency labels are omitted. 3 Strategies to improve NMT with NSD 3.1 Dependency NSD There are many previous studies on using dependency trees to improve NMT (Nguyen Le et al., 2017; Wu et al., 2017). Therefore, we extend NSD to dependency trees. Formally, the dependency NSD between two nodes is defined as follows: dD (wi ) = i − h(i), (4) where h(i) is the index of the head of wi , and we let the index of root be 0. Note that dD (wi ) can be either positive or negative, representing the directional information. Figure 2 gives an example. 3.2 NSDs as Input Sequences It is easy to see that for w = (w1 , . . . , wn ), the lengths of dS , dG , dR and dD are all n. Denoting the NSD sequence as d = (d1 , . . . , dn ), we can see that di ∈ Z, i ∈ [1, n], so we can obtain a sequence of embedding"
N19-1276,P16-1039,0,0.142523,"tion decisions. The experimental results show that our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets.1 1 Introduction Word segmentation is the first step of natural language processing (NLP) for most East Asian languages, such as Japanese and Chinese. In recent years, neural network models have been widely applied to word segmentation, especially Chinese, because of their ability to minimize the effort in feature engineering. These models are categorized as character-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labeling. These models typi"
N19-1276,P17-2096,0,0.0592388,"experimental results show that our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets.1 1 Introduction Word segmentation is the first step of natural language processing (NLP) for most East Asian languages, such as Japanese and Chinese. In recent years, neural network models have been widely applied to word segmentation, especially Chinese, because of their ability to minimize the effort in feature engineering. These models are categorized as character-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labeling. These models typically predict opti"
N19-1276,P15-1168,0,0.356081,"acter-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labeling. These models typically predict optimal label sequences while considering adjacent labels. Limited efforts have been devoted to leveraging the advantages of both types of models, such as utilizing word information and conducting exact inference, which are complementary characteristics. In particular, the candidate word information for a character is beneficial to disambiguate word boundaries because a character in the sentence has multiple candidate words that contain the character. For example, there are three or four candidate words for charact"
N19-1276,D15-1141,0,0.0715249,"Missing"
N19-1276,I05-3017,0,0.636732,"Missing"
N19-1276,P82-1020,0,0.817381,"Missing"
N19-1276,Y18-1033,0,0.0225075,"s using word boundary information from auto-segmented texts. Wang and Xu (2017) explicitly introduced word information into their CNN-based model. They concatenated embeddings of a character and multiple words corresponding to n-grams (n ranging from 1 to 4) that include the target character. For Japanese, less work employed neural models for word segmentation than for Chinese. Morita et al. (2015) integrated an RNN language model into a statistical Japanese morphological analysis framework, which simultaneously segments a sentence into words and predicts word features, such as POS and lemma. Kitagawa and Komachi (2018) applied a pure neural model based on LSTM and achieved a better performance than a popular statistical Japanese segmenter (Neubig et al., 2011). Around the same time as our work, two other character-based models for word segmentation have been proposed. Ma et al. (2018) showed a standard BiLSTM model can achieve state-of-theart results when combined with deep learning best practices, including dropout to recurrent connections (Gal and Ghahramani, 2016) and pre-trained embeddings of character bigrams. These techniques can also be applied to and can further boost performance of our model. Yang"
N19-1276,W04-3230,0,0.355922,"Missing"
N19-1276,P16-1200,0,0.0313472,"and Hovy (2016) and Rei et al. (2016) introduced the internal character information of words on word-level labeling tasks in contrast to our work introducing candidate word information of characters in the character-level labeling task. Attention Mechanism An attention mechanism (Bahdanau et al., 2015) was first introduced in machine translation to focus on appropriate parts of a source sentence during decoding. This mechanism has been widely applied to various NLP tasks, including question answering (Sukhbaatar et al., 2015), constituency parsing (Vinyals et al., 2015), relation extraction (Lin et al., 2016) and natural language inference (Parikh et al., 2016). Rei et al. (2016) introduced a gate-like attention mechanism on their word-based sequence labeling model to determine the importance between the word itself and the internal characters for each word. 7 Conclusion and Future Work In this paper, we proposed a word segmentation model that integrates word-level information into a character-based framework, aiming to take the advantages of both character- and word-based models. The experimental results show that our model with an attention-based composition function outperforms the state-of-the"
N19-1276,D18-1529,0,0.375472,"Missing"
N19-1276,P16-1101,0,0.0514462,"Yang et al. (2018) proposed a lattice LSTM-based model with subsequence (word or subword) information. Their model also considers the importance of multiple words by integrating character and word information into an LSTM cell vector using a gatemechanism. However, their model might not fully exploit word information, since word information is given to only the first and last characters of the word. 2706 LSTM-CRF LSTM-CRF is a popular neural architecture, which has been applied to various tagging tasks, including word segmentation (Chen et al., 2015b), POS tagging and NER (Huang et al., 2015; Ma and Hovy, 2016; Rei et al., 2016). Ma and Hovy (2016) and Rei et al. (2016) introduced the internal character information of words on word-level labeling tasks in contrast to our work introducing candidate word information of characters in the character-level labeling task. Attention Mechanism An attention mechanism (Bahdanau et al., 2015) was first introduced in machine translation to focus on appropriate parts of a source sentence during decoding. This mechanism has been widely applied to various NLP tasks, including question answering (Sukhbaatar et al., 2015), constituency parsing (Vinyals et al., 2015)"
N19-1276,I13-1181,0,0.541426,"g. These models are categorized as character-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labeling. These models typically predict optimal label sequences while considering adjacent labels. Limited efforts have been devoted to leveraging the advantages of both types of models, such as utilizing word information and conducting exact inference, which are complementary characteristics. In particular, the candidate word information for a character is beneficial to disambiguate word boundaries because a character in the sentence has multiple candidate words that contain the character. For example, there are thre"
N19-1276,D15-1276,0,0.0187771,"r not to segment two consecutive characters, using a deep CNN consisting of more than ten layers. Recent works utilized word information on a character-based framework. Zhou et al. (2017) pre-trained character embeddings using word boundary information from auto-segmented texts. Wang and Xu (2017) explicitly introduced word information into their CNN-based model. They concatenated embeddings of a character and multiple words corresponding to n-grams (n ranging from 1 to 4) that include the target character. For Japanese, less work employed neural models for word segmentation than for Chinese. Morita et al. (2015) integrated an RNN language model into a statistical Japanese morphological analysis framework, which simultaneously segments a sentence into words and predicts word features, such as POS and lemma. Kitagawa and Komachi (2018) applied a pure neural model based on LSTM and achieved a better performance than a popular statistical Japanese segmenter (Neubig et al., 2011). Around the same time as our work, two other character-based models for word segmentation have been proposed. Ma et al. (2018) showed a standard BiLSTM model can achieve state-of-theart results when combined with deep learning be"
N19-1276,P11-2093,0,0.449114,"ur preliminary experiments. Hyperparameter Setting Table 1 gives the hyperparameters for the proposed model. The same dropout strategy as in Zaremba et al. (2015) was applied to non-recurrent connections of recurrent layers. We used word vector dropout, which ran5 We restored provided auto-segmented texts to the original raw sentences and used them as unlabeled texts. 6 https://catalog.ldc.upenn.edu/ ldc2011t13 Method BCCWJ Our WCON model◦ 98.9 (Kitagawa and Komachi, 2018) 98.4 (Zhao and Kit, 2008)? – – (Zhou et al., 2017)◦ (Wang and Xu, 2017)◦ – (Liu et al., 2016)◦ – (Zhang et al., 2016)◦ – (Neubig et al., 2011)? 98.2∗ – (Sun et al., 2017)◦• CTB6 96.4 – – 96.2 – 95.5 96.0 – 96.3 MSR 97.8 – 97.6 97.8 98.0 97.6 97.7 – 97.9 Table 3: Comparison with state-of-the-art characterbased (top) and word-based (middle) and other types of models (bottom) on the test sets. Models marked with a symbol indicate ones based on linear statistical algorithms (?), ones with additional unlabeled texts (◦) and ones replacing specific characters as preprocessing (•). The result with ∗ is from our run on their released implementation. domly replaces a word embedding ew to a zero vector when calculating a word summary vector i"
N19-1276,D16-1244,0,0.109068,"Missing"
N19-1276,P14-1028,0,0.367972,"ategorized as character-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labeling. These models typically predict optimal label sequences while considering adjacent labels. Limited efforts have been devoted to leveraging the advantages of both types of models, such as utilizing word information and conducting exact inference, which are complementary characteristics. In particular, the candidate word information for a character is beneficial to disambiguate word boundaries because a character in the sentence has multiple candidate words that contain the character. For example, there are three or four candidat"
N19-1276,C04-1081,0,0.404506,"model learned the incorrect weights likely due to the infrequent occurrence of the correct words; the single words hiru and yoru occur in the training set tens or hundreds of times while the compound word ch¯uya occurs only twice. We may reduce these errors due to no or infrequent occurrences of gold words by increasing word vocabulary size, e.g., using larger texts to pre-train word embeddings. 6 Related Work Word Segmentation For both Chinese and Japanese, word segmentation has been traditionally addressed by applying linear statistical algorithms, such as maximum entropy (Xue, 2003), CRF (Peng et al., 2004; Kudo et al., 2004; Zhao and Kit, 2008), and logistic regression (Neubig et al., 2011). Various neural network architectures have been explored for Chinese word segmentation to reduce the burden of manual feature engineering. Specifically, character-based neural models have been developed to model the task as a sequence labeling problem, starting with earlier work by (Zheng et al., 2013) and (Mansur et al., 2013), which applied feed-forward neural networks. Pei et al. (2014) used a neural tensor network to capture interactions between tags and characters. More sophisticated architectures have"
N19-1276,C16-1030,0,0.0225395,"proposed a lattice LSTM-based model with subsequence (word or subword) information. Their model also considers the importance of multiple words by integrating character and word information into an LSTM cell vector using a gatemechanism. However, their model might not fully exploit word information, since word information is given to only the first and last characters of the word. 2706 LSTM-CRF LSTM-CRF is a popular neural architecture, which has been applied to various tagging tasks, including word segmentation (Chen et al., 2015b), POS tagging and NER (Huang et al., 2015; Ma and Hovy, 2016; Rei et al., 2016). Ma and Hovy (2016) and Rei et al. (2016) introduced the internal character information of words on word-level labeling tasks in contrast to our work introducing candidate word information of characters in the character-level labeling task. Attention Mechanism An attention mechanism (Bahdanau et al., 2015) was first introduced in machine translation to focus on appropriate parts of a source sentence during decoding. This mechanism has been widely applied to various NLP tasks, including question answering (Sukhbaatar et al., 2015), constituency parsing (Vinyals et al., 2015), relation extracti"
N19-1276,I17-1018,0,0.0329611,"characters. More sophisticated architectures have also been used as standard components of word segmentation models to derive effective features automatically. Chen et al. (2015a) proposed gated recursive neural networks to model complicated combinations of characters. Chen et al. (2015b) used LSTM to capture long distance dependencies. Xu and Sun (2016) combined LSTM and GRNN to capture long term information better by utilizing chain and tree structures. CNNs have been used to extract complex features such as character n-grams (Chen et al., 2017) and graphical features of Chinese characters (Shao et al., 2017). On the other hand, word-based neural models have also been proposed. Typical word-based models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) sequentially determine whether or not to segment each character on the basis of word-level features and segmentation history, while keeping multiple segmentation candidates by beam search decoding. Liu et al. (2016) combined neural architectures for segment (i.e., word) representations into a semi-CRF framework, which searches for an optimal segmentation sequence consisting of variable length segments. Sun et al. (2017) p"
N19-1276,I17-1017,0,0.660438,"odels, such as utilizing word information and conducting exact inference, which are complementary characteristics. In particular, the candidate word information for a character is beneficial to disambiguate word boundaries because a character in the sentence has multiple candidate words that contain the character. For example, there are three or four candidate words for characters x3 , x4 and x5 in a sentence x1:5 in Figure 1. A feasible solution to develop a model with both characteristics is to incorporate word information into a character-based framework. An example of such work is that of Wang and Xu (2017). They concatenated embeddings of a character and candidate words and used it in their convolutional neural network (CNN)-based model. They treated candidate words equivalently, although the plausibility of a candidate word differs in the context of a target character. In this paper, we propose a character-based word segmentation model that utilizes word information. Our model is based on a BiLSTM-CRF architecture that has been successfully applied to sequence labeling tasks (Huang et al., 2015; Chen et al., 2015b). Differing from the work of Wang and Xu (2017), our model learns and distinguis"
N19-1276,P16-2092,0,0.0319066,"to model the task as a sequence labeling problem, starting with earlier work by (Zheng et al., 2013) and (Mansur et al., 2013), which applied feed-forward neural networks. Pei et al. (2014) used a neural tensor network to capture interactions between tags and characters. More sophisticated architectures have also been used as standard components of word segmentation models to derive effective features automatically. Chen et al. (2015a) proposed gated recursive neural networks to model complicated combinations of characters. Chen et al. (2015b) used LSTM to capture long distance dependencies. Xu and Sun (2016) combined LSTM and GRNN to capture long term information better by utilizing chain and tree structures. CNNs have been used to extract complex features such as character n-grams (Chen et al., 2017) and graphical features of Chinese characters (Shao et al., 2017). On the other hand, word-based neural models have also been proposed. Typical word-based models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) sequentially determine whether or not to segment each character on the basis of word-level features and segmentation history, while keeping multiple segmentation c"
N19-1276,P16-1040,0,0.349286,"e of it for segmentation decisions. The experimental results show that our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets.1 1 Introduction Word segmentation is the first step of natural language processing (NLP) for most East Asian languages, such as Japanese and Chinese. In recent years, neural network models have been widely applied to word segmentation, especially Chinese, because of their ability to minimize the effort in feature engineering. These models are categorized as character-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labelin"
N19-1276,I08-4017,0,0.254677,"ndomly initialized all character embeddings, since pre-trained character embeddings did not improve performance in our preliminary experiments. Hyperparameter Setting Table 1 gives the hyperparameters for the proposed model. The same dropout strategy as in Zaremba et al. (2015) was applied to non-recurrent connections of recurrent layers. We used word vector dropout, which ran5 We restored provided auto-segmented texts to the original raw sentences and used them as unlabeled texts. 6 https://catalog.ldc.upenn.edu/ ldc2011t13 Method BCCWJ Our WCON model◦ 98.9 (Kitagawa and Komachi, 2018) 98.4 (Zhao and Kit, 2008)? – – (Zhou et al., 2017)◦ (Wang and Xu, 2017)◦ – (Liu et al., 2016)◦ – (Zhang et al., 2016)◦ – (Neubig et al., 2011)? 98.2∗ – (Sun et al., 2017)◦• CTB6 96.4 – – 96.2 – 95.5 96.0 – 96.3 MSR 97.8 – 97.6 97.8 98.0 97.6 97.7 – 97.9 Table 3: Comparison with state-of-the-art characterbased (top) and word-based (middle) and other types of models (bottom) on the test sets. Models marked with a symbol indicate ones based on linear statistical algorithms (?), ones with additional unlabeled texts (◦) and ones replacing specific characters as preprocessing (•). The result with ∗ is from our run on their"
N19-1276,D13-1061,0,0.394171,"Missing"
N19-1276,D17-1079,0,0.121156,"segment each character on the basis of word-level features and segmentation history, while keeping multiple segmentation candidates by beam search decoding. Liu et al. (2016) combined neural architectures for segment (i.e., word) representations into a semi-CRF framework, which searches for an optimal segmentation sequence consisting of variable length segments. Sun et al. (2017) proposed a gap-based model to predict whether or not to segment two consecutive characters, using a deep CNN consisting of more than ten layers. Recent works utilized word information on a character-based framework. Zhou et al. (2017) pre-trained character embeddings using word boundary information from auto-segmented texts. Wang and Xu (2017) explicitly introduced word information into their CNN-based model. They concatenated embeddings of a character and multiple words corresponding to n-grams (n ranging from 1 to 4) that include the target character. For Japanese, less work employed neural models for word segmentation than for Chinese. Morita et al. (2015) integrated an RNN language model into a statistical Japanese morphological analysis framework, which simultaneously segments a sentence into words and predicts word f"
N19-1276,O03-4002,0,0.41713,"e vocabulary. The model learned the incorrect weights likely due to the infrequent occurrence of the correct words; the single words hiru and yoru occur in the training set tens or hundreds of times while the compound word ch¯uya occurs only twice. We may reduce these errors due to no or infrequent occurrences of gold words by increasing word vocabulary size, e.g., using larger texts to pre-train word embeddings. 6 Related Work Word Segmentation For both Chinese and Japanese, word segmentation has been traditionally addressed by applying linear statistical algorithms, such as maximum entropy (Xue, 2003), CRF (Peng et al., 2004; Kudo et al., 2004; Zhao and Kit, 2008), and logistic regression (Neubig et al., 2011). Various neural network architectures have been explored for Chinese word segmentation to reduce the burden of manual feature engineering. Specifically, character-based neural models have been developed to model the task as a sequence labeling problem, starting with earlier work by (Zheng et al., 2013) and (Mansur et al., 2013), which applied feed-forward neural networks. Pei et al. (2014) used a neural tensor network to capture interactions between tags and characters. More sophisti"
N19-1276,P17-1078,0,0.250174,"ts show that our model achieves better performance than the state-of-the-art models on both Japanese and Chinese benchmark datasets.1 1 Introduction Word segmentation is the first step of natural language processing (NLP) for most East Asian languages, such as Japanese and Chinese. In recent years, neural network models have been widely applied to word segmentation, especially Chinese, because of their ability to minimize the effort in feature engineering. These models are categorized as character-based or word-based. Wordbased models (Zhang et al., 2016; Cai and Zhao, 2016; Cai et al., 2017; Yang et al., 2017) directly segment a character sequence into words and can easily achieve the benefits of word-level information. However, these models cannot usually conduct exact inference because of strategies, such as beam-search decoding and constraints of maximum word length, which are necessary as the number of candidate segmentations increases exponentially with the sentence length. On the other hand, character-based models (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) treat word segmentation as sequence labeling. These models typically predict optimal label sequences"
N19-1276,N19-1278,0,0.0907116,"Missing"
N19-1276,P12-1083,0,0.455803,"Missing"
P01-1064,A00-2004,0,0.63446,"00). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000). A major characteristic of the methods used in this research is that they do not require training data to segment given texts. Hearst (1994), for example, used only the similarity of word distributions in a given text to segment the text. Consequently, these methods can be applied to any text in any domain, even if training data do not exist. This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with domain-independent documents. Another application of text segmentation is the segmentation of a continuous"
P01-1064,P94-1002,0,0.864752,"earst and Plaunt, 1993; Salton et al., 1996) and summarization (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000). A major characteristic of the methods used in this research is that they do not require training data to segment given texts. Hearst (1994), for example, used only the similarity of word distributions in a given text to segment the text. Consequently, these methods can be applied to any text in any domain, even if training data do not exist. This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with domain-independent"
P01-1064,P98-2244,0,0.264207,"ntation L 12 ¯ 1 2 b¨« ¬± 3 «  Step 2. Find the minimum-cost path from    . of edge for Step 1. Calculate the cost by using Equation (16). to Algorithms for finding the minimum-cost path in a graph are well known. An algorithm that can provide a solution for Step 2 will be a simpler version of the algorithm used to find the maximumprobability solution in Japanese morphological analysis (Nagata, 1994). Therefore, a solution can be obtained by applying a dynamic programming (DP) algorithm.4 DP algorithms have also been used for text segmentation by other researchers (Ponte and Croft, 1997; Heinonen, 1998). The path thus obtained represents the when edges minimum-cost segmentation in correspond with segments. In Figure 1, for example, if is the minimum-cost path, then is the minimum-cost segmentation. The algorithm automatically determines the number of segments. But the number of segments can also be specified explicitly by specifying the number of edges in the minimum-cost path. The algorithm allows the text to be segmented anywhere between words; i.e., all the positions ¯ &¯¦?´¯""µ ¶ ¨?· ¶ ¸ 9· ¶ ¸µ9· 4 A program that implements the algorithm described in this section is av"
P01-1064,W98-1123,0,0.751156,"Missing"
P01-1064,P93-1041,0,0.703934,"n retrieval (Hearst and Plaunt, 1993; Salton et al., 1996) and summarization (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000). A major characteristic of the methods used in this research is that they do not require training data to segment given texts. Hearst (1994), for example, used only the similarity of word distributions in a given text to segment the text. Consequently, these methods can be applied to any text in any domain, even if training data do not exist. This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with doma"
P01-1064,maekawa-etal-2000-spontaneous,1,0.345853,"Missing"
P01-1064,C94-1032,0,0.0196192,"boundaries are at the ends of sentences. Given these definitions, we describe the algorithm to find the minimum-cost segmentation or maximum-probability segmentation as follows: 3.2 Properties of the segmentation L 12 ¯ 1 2 b¨« ¬± 3 «  Step 2. Find the minimum-cost path from    . of edge for Step 1. Calculate the cost by using Equation (16). to Algorithms for finding the minimum-cost path in a graph are well known. An algorithm that can provide a solution for Step 2 will be a simpler version of the algorithm used to find the maximumprobability solution in Japanese morphological analysis (Nagata, 1994). Therefore, a solution can be obtained by applying a dynamic programming (DP) algorithm.4 DP algorithms have also been used for text segmentation by other researchers (Ponte and Croft, 1997; Heinonen, 1998). The path thus obtained represents the when edges minimum-cost segmentation in correspond with segments. In Figure 1, for example, if is the minimum-cost path, then is the minimum-cost segmentation. The algorithm automatically determines the number of segments. But the number of segments can also be specified explicitly by specifying the number of edges in the minimum-cost path. The algori"
P01-1064,P00-1039,0,0.247342,"thod does not require training data because it estimates probabilities from the given text. Therefore, it can be applied to any text in any domain. An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system. 1 Introduction Documents usually include various topics. Identifying and isolating topics by dividing documents, which is called text segmentation, is important for many natural language processing tasks, including information retrieval (Hearst and Plaunt, 1993; Salton et al., 1996) and summarization (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Cho"
P01-1064,C94-2121,0,0.00963597,"nt, 1993; Salton et al., 1996) and summarization (Kan et al., 1998; Nakao, 2000). In information retrieval, users are often interested in particular topics (parts) of retrieved documents, instead of the documents themselves. To meet such needs, documents should be segmented into coherent topics. Summarization is often used for a long document that includes multiple topics. A summary of such a document can be composed of summaries of the component topics. Identification of topics is the task of text segmentation. A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000). A major characteristic of the methods used in this research is that they do not require training data to segment given texts. Hearst (1994), for example, used only the similarity of word distributions in a given text to segment the text. Consequently, these methods can be applied to any text in any domain, even if training data do not exist. This property is important when text segmentation is applied to information retrieval or summarization, because both tasks deal with domain-independent documents. Another appli"
P01-1064,P94-1050,0,0.366264,"requires only the given documents for segmentation. It can, however, incorporate training data when they are available, as discussed in Section 5. The algorithm selects the optimum segmentation in terms of the probability defined by a statistical model. This is a new approach for domain-independent text segmentation. Previous approaches usually used lexical cohesion to segment texts into topics. Kozima (1993), for example, used cohesion based on the spreading activation on a semantic network. Hearst (1994) used the similarity of word distributions as measured by the cosine to gauge cohesion. Reynar (1994) used word repetition as a measure of cohesion. Choi (2000) used the rank of the cosine, rather than the cosine itself, to measure the similarity of sentences. The statistical model for the algorithm is described in Section 2, and the algorithm for obtaining the maximum-probability segmentation is described in Section 3. Experimental results are presented in Section 4. Further discussion and our conclusions are given in Sections 5 and 6, respectively. 2 Statistical Model for Text Segmentation We first define the probability of a segmentation of a given text in this section. In the next section"
P01-1064,P99-1046,0,0.0156788,"(in words). Another major difference from their algorithm is that our algorithm does not require training data to estimate probabilities, while their algorithm does. Therefore, our algorithm can be applied to domain-independent texts, while their algorithm is restricted to domains for which training data are available. It would be interesting, however, to compare our algorithm with their algorithm for the case when training data are available. In such a case, our model should be extended to incorporate various features such as the average segment length, clue words, named entities, and so on (Reynar, 1999; Beeferman et al., 1999). Our proposed algorithm naturally estimates the probabilities of words in segments. These probabilities, which are called word densities, have been used to detect important descriptions of words in texts (Kurohashi et al., 1997). This method is based on the assumption that the density of a word is high in a segment in which the word is discussed (defined and/or explained) in some depth. It would be interesting to apply our method to this application. 6 Conclusion We have proposed a statistical model for domainindependent text segmentation. This method finds the maximu"
P01-1064,C98-2239,0,\N,Missing
P03-1010,P98-1041,0,0.12065,"pus available to the public. 1 Introduction A large-scale Japanese-English parallel corpus is an invaluable resource in the study of natural language processing (NLP) such as machine translation and cross-language information retrieval (CLIR). It is also valuable for language education. However, no such corpus has been available to the public. We recently have obtained a noisy parallel corpus of Japanese and English newspapers consisting of issues published over more than a decade and have tried to align their articles and sentences. We first aligned the articles using a method based on CLIR (Collier et al., 1998; Matsumoto and Tanaka, 2002) and then aligned the sentences in these articles by using a method based on dynamic programming (DP) matching (Gale and Church, 1993; Utsuro et al., 1994). However, the results included many incorrect alignments due to noise in the corpus. To remove these, we propose two measures (scores) that evaluate the validity of article and sentence alignments. Using these, we can selectively extract valid alignments. In this paper, we first discuss the basic statistics on the Japanese and English newspapers. We next explain methods and measures used for alignment. We then e"
P03-1010,H92-1022,0,0.00854577,"ment. Let Ji and Ei be the words of Japanese and English sentences for i-th alignment. The similarity6 between Ji and Ei is: SIM(Ji , Ei ) = where l(X) = co(Ji × Ei ) + 1 l(Ji ) + l(Ei ) − 2co(Ji × Ei ) + 2 P x∈X f (x) f (x) is the frequency of x in the sentences. co(Ji × Ei ) = P (j,e)∈Ji ×Ei min(f (j), f (e)) Ji × Ei = {(j, e)|j ∈ Ji , e ∈ Ei } and Ji × Ei is a one-to-one correspondence between Japanese and English words. Ji and Ei are obtained as follows. We use ChaSen to morphologically analyze the Japanese sentences and extract content words, which consists of Ji . We use Brill’s tagger (Brill, 1992) to POS-tag the English sentences, extract content words, and use WordNet’s library7 to obtain lemmas of the words, which consists of Ei . We use simple heuristics to obtain Ji × Ei , i.e., a one-to-one correspondence between the words in Ji and Ei , by looking up JapaneseEnglish and English-Japanese dictionaries made up by combining entries in the EDR Japanese-English bilingual dictionary and the EDR English-Japanese bilingual dictionary. Each of the constructed dictionaries has over 300,000 entries. We evaluated the implemented program against a corpus consisting of manually aligned Japanese"
P03-1010,J93-1004,0,0.929982,"(NLP) such as machine translation and cross-language information retrieval (CLIR). It is also valuable for language education. However, no such corpus has been available to the public. We recently have obtained a noisy parallel corpus of Japanese and English newspapers consisting of issues published over more than a decade and have tried to align their articles and sentences. We first aligned the articles using a method based on CLIR (Collier et al., 1998; Matsumoto and Tanaka, 2002) and then aligned the sentences in these articles by using a method based on dynamic programming (DP) matching (Gale and Church, 1993; Utsuro et al., 1994). However, the results included many incorrect alignments due to noise in the corpus. To remove these, we propose two measures (scores) that evaluate the validity of article and sentence alignments. Using these, we can selectively extract valid alignments. In this paper, we first discuss the basic statistics on the Japanese and English newspapers. We next explain methods and measures used for alignment. We then evaluate the effectiveness of the proposed measures. Finally, we show that our aligned corpus has attracted people both inside and outside the NLP community. 2 New"
P03-1010,matsumoto-tanaka-2002-automatic,0,0.0426027,"ublic. 1 Introduction A large-scale Japanese-English parallel corpus is an invaluable resource in the study of natural language processing (NLP) such as machine translation and cross-language information retrieval (CLIR). It is also valuable for language education. However, no such corpus has been available to the public. We recently have obtained a noisy parallel corpus of Japanese and English newspapers consisting of issues published over more than a decade and have tried to align their articles and sentences. We first aligned the articles using a method based on CLIR (Collier et al., 1998; Matsumoto and Tanaka, 2002) and then aligned the sentences in these articles by using a method based on dynamic programming (DP) matching (Gale and Church, 1993; Utsuro et al., 1994). However, the results included many incorrect alignments due to noise in the corpus. To remove these, we propose two measures (scores) that evaluate the validity of article and sentence alignments. Using these, we can selectively extract valid alignments. In this paper, we first discuss the basic statistics on the Japanese and English newspapers. We next explain methods and measures used for alignment. We then evaluate the effectiveness of"
P03-1010,A97-1004,0,0.0113373,"mmarize, we first translate each of the Japanese articles into a set of English words. We then use each of the English articles as a query and search for the most similar Japanese article in terms of BM25 and assume that it corresponds to the English article. 3.2 Sentence alignment The sentences5 in the aligned Japanese and English articles are aligned by a method based on DP matching (Gale and Church, 1993; Utsuro et al., 1994). 4 http://trec.nist.gov/ We split the Japanese articles into sentences by using simple heuristics and split the English articles into sentences by using MXTERMINATOR (Reynar and Ratnaparkhi, 1997). 5 We allow 1-to-n or n-to-1 (1 ≤ n ≤ 6) alignments when aligning the sentences. Readers are referred to Utsuro et al. (1994) for a concise description of the algorithm. Here, we only discuss the similarities between Japanese and English sentences for alignment. Let Ji and Ei be the words of Japanese and English sentences for i-th alignment. The similarity6 between Ji and Ei is: SIM(Ji , Ei ) = where l(X) = co(Ji × Ei ) + 1 l(Ji ) + l(Ei ) − 2co(Ji × Ei ) + 2 P x∈X f (x) f (x) is the frequency of x in the sentences. co(Ji × Ei ) = P (j,e)∈Ji ×Ei min(f (j), f (e)) Ji × Ei = {(j, e)|j ∈ Ji , e"
P03-1010,C94-2175,0,0.476751,"ranslation and cross-language information retrieval (CLIR). It is also valuable for language education. However, no such corpus has been available to the public. We recently have obtained a noisy parallel corpus of Japanese and English newspapers consisting of issues published over more than a decade and have tried to align their articles and sentences. We first aligned the articles using a method based on CLIR (Collier et al., 1998; Matsumoto and Tanaka, 2002) and then aligned the sentences in these articles by using a method based on dynamic programming (DP) matching (Gale and Church, 1993; Utsuro et al., 1994). However, the results included many incorrect alignments due to noise in the corpus. To remove these, we propose two measures (scores) that evaluate the validity of article and sentence alignments. Using these, we can selectively extract valid alignments. In this paper, we first discuss the basic statistics on the Japanese and English newspapers. We next explain methods and measures used for alignment. We then evaluate the effectiveness of the proposed measures. Finally, we show that our aligned corpus has attracted people both inside and outside the NLP community. 2 Newspapers Aligned The Ja"
P03-1010,C98-1041,0,\N,Missing
P05-3030,Y04-1017,1,0.796124,"On the contrary, 1 http://www.cnn.com/ http://www.time.com/time/ 3 http://www.bbc.co.uk/ 4 http://www.gradedreading.pwp.blueyonder.co.uk/ EFL teachers have to carefully select texts, if they want their students to learn a specialized vocabulary through reading in a particular discipline such as medicine, engineering, or economics. However, it is problematic for teachers to select materials for learning a target vocabulary with short authentic texts. It is possible to automate this selection process given the target vocabulary to be learned and the target corpus from which texts are gathered (Utiyama et al., 2004). In this research (Utiyama et al., 2004), we used a specialized vocabulary for an English certification test as the target vocabulary and used newspaper articles from The Daily Yomiuri as the target corpus. We then organized a set of reading materials, which we called courseware5 , using the algorithm in Section 2. The courseware consisted of 116 articles and contained all the target vocabulary. We used the courseware in university English classes from May 2004 to January 2005. We found that the courseware was effective in learning vocabulary (Tanimura and Utiyama, in preparation). Based on t"
P10-2001,D07-1103,0,0.0130933,"Missing"
P10-2001,P05-1074,0,0.128063,"is shown in Figure 1. In advance, we automatically acquire a paraphrase list from a parallel corpus. In order to acquire paraphrases of unknown phrases, this parallel corpus is different from the parallel corpus for training. Given an input sentence, we build a lattice which represents paraphrases of the input sentence using the paraphrase list. We call this lattice a paraphrase lattice. Then, we give the paraphrase lattice to the lattice decoder. • is there a beauty salon ? • is there a beauty parlor ? • is there a salon ? 3.1 Acquiring the paraphrase list We acquire a paraphrase list using Bannard and Callison-Burch (2005)’s method. Their idea is, if two different phrases e1 , e2 in one language are aligned to the same phrase c in another language, they are hypothesized to be paraphrases of each other. Our paraphrase list is acquired in the same way. The procedure is as follows: In the paraphrase lattice, each node consists of a token, the distance to the next node and features for lattice decoding. We use following four features for lattice decoding. • Paraphrase probability (p) A paraphrase probability p(e2 |e1 ) calculated when acquiring the paraphrase. hp = p(e2 |e1 ) 1. Build a phrase table. • Language mod"
P10-2001,2005.mtsummit-papers.11,0,0.00371744,"(p) and (p, L) in EJ translation and (L) and (p, l) in EC translation. Since features related to the source-side language model were chosen in each direction, using the source-side language model is useful for decoding paraphrase lattices. We also tried a combination of Proposed Method and CCB, which is a method of decoding paraphrase lattices with an augmented phrase table. However, the result showed no significant improvements. This is because the proposed method includes the effect of augmenting the phrase table. Moreover, we conducted German-English translation using the Europarl corpus (Koehn, 2005). We used the WMT08 dataset1 , which consists of 1M sentences for training and 2K sentences for development and testing. We acquired 5.3M pairs of German-German paraphrases from a 1M German-Spanish parallel corpus. We conducted experiments with various sizes of training corpus, using 10K, 20K, 40K, 80K, 160K and 1M. Figure 3 shows the proposed method consistently get higher score than Moses and CCB. 4.2 Proposed method In the proposed method, we conducted experiments with various settings for paraphrasing and lattice decoding. Then, we chose the best setting according to the result of the dev2"
P10-2001,2008.iwslt-papers.2,0,0.615261,"Missing"
P10-2001,D09-1040,0,0.161716,"Missing"
P10-2001,N06-1003,0,0.301119,"araphrase pair is applied. As these features can penalize paraphrases which are not appropriate to the context, appropriate paraphrases are chosen and appropriate translations are output in lattice decoding. The features related to the sentence length, such as (L) and (d), are added to penalize the language model score in case the paraphrased sentence length is shorter than the original sentence length and the language model score is unreasonably low. In experiments, we use four combinations of these features, (p), (p, l), (p, L) and (p, l, d). 3.3 4.1 Baseline As baselines, we used Moses and Callison-Burch et al. (2006)’s method (hereafter CCB). In Moses, we used default settings without paraphrases. In CCB, we paraphrased the phrase table using the automatically acquired paraphrase list. Then, we augmented the phrase table with paraphrased phrases which were not found in the original phrase table. Moreover, we used an additional feature whose value was the paraphrase probability (p) if the entry was generated by paraphrasing and Lattice decoding We use Moses (Koehn et al., 2007) as a decoder for lattice decoding. Moses is an open source 3 EJ EC Moses (w/o Paraphrases) 38.98 25.11 CCB 39.24 (+0.26) 26.14 (+1"
P10-2001,N09-1046,0,0.00902598,"mentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets. 1 Introduction Lattice decoding in SMT is useful in speech translation and in the translation of German (Bertoldi et al., 2007; Dyer, 2009). In speech translation, by using lattices that represent not only 1-best result but also other possibilities of speech recognition, we can take into account the ambiguities of speech recognition. Thus, the translation quality for lattice inputs is better than the quality for 1best inputs. In this paper, we show that lattice decoding is also useful for handling input variations. “Input variations” refers to the differences of input texts with the same meaning. For example, “Is there a beauty salon?” and “Is there a beauty parlor?” have the same meaning with variations in “beauty salon” and “be"
P10-2001,P03-1021,0,0.00279147,"""beauty"" , 0.250, 1.172, 1, 1) (""salon"" , 0.133, 0.537, 0.367, 3) 4 -- (""parlor"" , 1, 1, 1, 2) Paraphrase probability (p) 5 -- (""salon"" , 1, 1, 1, 1) 6 -- (""?"" Language model score (l) , 1, 1, 1, 1) Paraphrase length (d) Figure 2: An example of a paraphrase lattice, which contains three features of (p, l, d). • Normalized language model score (L) SMT system which allows lattice decoding. In lattice decoding, Moses selects the best path and the best translation according to features added in each node and other SMT features. These weights are optimized using Minimum Error Rate Training (MERT) (Och, 2003). A language model score where the language model probability is normalized by the sentence length. The sentence length is calculated as the number of tokens. hL = LM (para) LM (orig) , 1 4 Experiments where LM (sent) = lm(sent) length(sent) • Paraphrase length (d) In order to evaluate the proposed method, we conducted English-to-Japanese and English-toChinese translation experiments using IWSLT 2007 (Fordyce, 2007) dataset. This dataset contains EJ and EC parallel corpus for the travel domain and consists of 40k sentences for training and about 500 sentences sets (dev1, dev2 and dev3) for dev"
P10-2001,2007.iwslt-1.1,0,\N,Missing
P10-2001,P07-2045,0,\N,Missing
P11-2076,A00-2018,0,0.0172986,") in J-E/E-J translation. (w/ Context) and the method of specifying reordering constraints without a context document (w/o Context). In both methods, the feature weights used in decoding are the same value as those for the baseline (dl = −1). 4.2.1 Proposed method (w/ Context) In the proposed method, reordering constraints were defined with a context document. For J-E translation, we used the CaboCha parser (Kudo and Matsumoto, 2002) to analyze the context document. As coherent phrase candidates, we extracted all subtrees whose heads are noun. For E-J translation, we used the Charniak parser (Charniak, 2000) and extracted all noun phrases, labeled “NP”, as coherent phrase candidates. The parsers are used only when extracting coherent phrase candidates. When specifying zones for each source sentence, strings which match the coherent phrase candidates are defined to be zones. Therefore, the proposed method is robust against parsing errors. We tried various thresholds of the C-value and selected the value that yielded the highest BLEU score for the development set. 4.2.2 w/o Context In this method, reordering constraints were defined without a context document. For J-E translation, we converted the"
P11-2076,P08-1009,0,0.223081,"Missing"
P11-2076,C96-1009,0,0.772989,"g zones. We then give the zone-tagged sentence, an example is shown in Table 1, as a decoder input. 3.2.2 Ranking with C-value The candidates which have been extracted are nested and have different lengths. A naive method cannot rank these candidates properly. For example, ranking by frequency cannot pick up an important phrase which has a long length, yet, ranking by length may give a long but unimportant phrase a high rank. In order to select the appropriate coherent phrases, measurements which give high rank to phrases with high termhood are needed. As one such measurement, we use C-value (Frantzi and Ananiadou, 1996). C-value is a measurement of automatic term recognition and is suitable for extracting important phrases from nested candidates. The C-value of a phrase p is expressed in the following equation: In decoding, reorderings which violate zones, such as the baseline output in Table 1, are restricted and we get a more appropriate translation, such as the proposed output in Table 1. We use the Moses decoder (Koehn et al., 2007; Koehn and Haddow, 2009), which can specify reordering constraints using &lt;zone&gt; and &lt;/zone&gt; tags. Moses restricts reorderings which violate zones and translates zones as singl"
P11-2076,W10-1736,0,0.0543091,"Missing"
P11-2076,P03-1040,0,0.116281,"with similar word orders. However, it has problems with longdistance reordering when translating between languages with different word orders, such as JapaneseEnglish. These problems are especially crucial when translating long sentences, such as patent sentences, because many combinations of word orders cause high computational costs and low translation quality. In order to address these problems, various methods which use syntactic information have been proposed. These include methods where source sentences are divided into syntactic chunks or clauses and the translations are merged later (Koehn and Knight, 2003; Sudoh et al., 2010), methods where syntactic constraints or penalties for reordering are added to a decoder (Yamamoto et al., 2008; Cherry, 2008; Marton and Resnik, 2008; Xiong et al., 2010), and methods where source sentences are reordered into a similar word order as the target language in advance (Katz-Brown and Collins, 2008; Isozaki et al., 2010). However, these methods did not use document-level context to constrain reorderings. Document-level context is often available in real-life situations. We think it is a promising clue to improving translation quality. In this paper, we propose"
P11-2076,P07-2045,0,0.00562533,"rder to select the appropriate coherent phrases, measurements which give high rank to phrases with high termhood are needed. As one such measurement, we use C-value (Frantzi and Ananiadou, 1996). C-value is a measurement of automatic term recognition and is suitable for extracting important phrases from nested candidates. The C-value of a phrase p is expressed in the following equation: In decoding, reorderings which violate zones, such as the baseline output in Table 1, are restricted and we get a more appropriate translation, such as the proposed output in Table 1. We use the Moses decoder (Koehn et al., 2007; Koehn and Haddow, 2009), which can specify reordering constraints using &lt;zone&gt; and &lt;/zone&gt; tags. Moses restricts reorderings which violate zones and translates zones as single blocks. { C-value(p) = (l(p)−1) n(p) ( (l(p)−1) t(p) n(p)− c(p) ) (c(p) = 0) (c(p) &gt; 0) where l(p) is the length of a phrase p, n(p) is the frequency of p in a document, t(p) is the total frequency of phrases which contain p as a subphrase, c(p) is the number of those phrases. Since phrases which have a large C-value frequently occur in a context document, these phrases are considered to be a significant unit, i.e., a"
P11-2076,W04-3250,0,0.199744,"Missing"
P11-2076,W02-2016,0,0.130618,"Missing"
P11-2076,P08-1114,0,0.190549,"Missing"
P11-2076,P03-1021,0,0.00888357,"ains the patent specifications from which sentence pairs are extracted. We used these patent specifications as context documents. 4.1 Baseline We used Moses as a baseline system, with all the settings except distortion limit (dl) at the default. The distortion limit is a maximum distance of reordering. It is known that an appropriate distortion-limit can improve translation quality and decoding speed. Therefore, we examined the effect of a distortionlimit. In experiments, we compared dl = 6, 10, 20, 30, 40, and −1 (unlimited). The feature weights were optimized to maximize BLEU score by MERT (Och, 2003) using the development set. 4.2 Compared methods We compared two methods, the method of specifying reordering constraints with a context document w/o Context w/ Context in ( this case ) , ( the leading end ) 15f of ( the segment operating body ) ( ( 15 swings ) in ( a direction opposite ) ) to ( the a arrow direction ) . in ( this case ) , ( ( the leading end ) 15f ) of ( ( ( the segment ) operating body ) 15 ) swings in a direction opposite to ( the a arrow direction ) . Table 3: An example of the zone-tagged source sentence. &lt;zone&gt; and &lt;/zone&gt; are replaced by “(” and “)”. System dl 6 10 20 B"
P11-2076,P02-1040,0,0.0801919,"Missing"
P11-2076,W10-1762,0,0.0518035,"Missing"
P11-2076,W08-0401,1,0.853494,"Missing"
P11-2076,N10-1016,0,\N,Missing
P11-2076,W09-0429,0,\N,Missing
P12-2061,N09-1029,0,0.0274937,"Missing"
P12-2061,J07-2003,0,0.0775997,"rce language structure, and 2) transferring the obtained syntax structures into the syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh"
P12-2061,P05-1066,0,0.315048,"roblem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, simply moves syntactic heads to the end of corresponding syntactic constituents (e.g.,"
P12-2061,D11-1018,0,0.0458521,"irst, we produce N-best HFE sentences using Japanese-to-HFE monotone phrase-based SMT. Next, we produce K-best parse trees for each HFE sentence by parsing, and produce English sentences by swapping any nodes annotated with “ SW”. Then we score the English sentences and select the English sentence with the highest score. For the score of an English sentence, we use the sum of the log-linear SMT model score for Japanese-to-HFE and the logarithm of the language model probability of the English sentence. 2 There are works using the ITG model in SMT: ITG was used for training pre-ordering models (DeNero and Uszkoreit, 2011); hierarchical phrase-based SMT (Chiang, 2007), which is an extension of ITG; and reordering models using ITG (Chen et al., 2009; He et al., 2010). These methods are not post-ordering methods. 5.1 Setup We used patent sentence data for the Japanese to English translation subtask from the NTCIR-9 and 8 (Goto et al., 2011; Fujii et al., 2010). There were 2,000 test sentences for NTCIR-9 and 1,251 for NTCIR-8. XML entities included in the data were decoded to UTF-8 characters before use. We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. Mecab 3 v0.98"
P12-2061,P05-1067,0,0.024275,"yntax structures into the syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called hea"
P12-2061,N04-1035,0,0.0556164,"erring the obtained syntax structures into the syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The meth"
P12-2061,N10-1127,0,0.0611272,"ween languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, simply moves syntactic heads to the end of corresponding syntactic constituents (e.g., phrases and clauses). This method fi"
P12-2061,C10-1051,0,0.0113551,"and produce English sentences by swapping any nodes annotated with “ SW”. Then we score the English sentences and select the English sentence with the highest score. For the score of an English sentence, we use the sum of the log-linear SMT model score for Japanese-to-HFE and the logarithm of the language model probability of the English sentence. 2 There are works using the ITG model in SMT: ITG was used for training pre-ordering models (DeNero and Uszkoreit, 2011); hierarchical phrase-based SMT (Chiang, 2007), which is an extension of ITG; and reordering models using ITG (Chen et al., 2009; He et al., 2010). These methods are not post-ordering methods. 5.1 Setup We used patent sentence data for the Japanese to English translation subtask from the NTCIR-9 and 8 (Goto et al., 2011; Fujii et al., 2010). There were 2,000 test sentences for NTCIR-9 and 1,251 for NTCIR-8. XML entities included in the data were decoded to UTF-8 characters before use. We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. Mecab 3 v0.98 was used for the Japanese morphological analysis. The translation model was trained using sentences of 64 words or less from the training corpus a"
P12-2061,D10-1092,0,0.265016,"Missing"
P12-2061,W10-1736,0,0.581409,"ages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, simply moves syntactic heads to the end of corresponding syntactic constituents (e.g., phrases and clauses). This method first changes the Englis"
P12-2061,N03-1017,0,0.0113125,"Missing"
P12-2061,P06-1077,0,0.0779107,"he syntax structures of the target language. 1 Introduction The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, si"
P12-2061,P09-1063,0,0.0332901,"Missing"
P12-2061,J08-1002,0,0.0611705,"of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Japanese HFE Japanese: English HFE: monotone translation kare he wa _va0 kinou hon yesterday wo post-ordering katta _va2 books bought Parsing Figure 1: Post-ordering framework. S_ST VP_SW English words into Japanese. There are two key reasons why this pre-ordering method works for estimating Japanese word order. The first reason is that Japanese is a typical headfinal language. That is, a syntactic head word comes after nonhead (dependent) words. Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. Consequently, the parsed English input sentences can be pre-ordered into a Japaneselike word order using the head finalization rule. Pre-ordering using the head finalization rule naturally cannot be applied to Japanese-English translation, because English is not a head-final language. If we want to pre-order Japanese sentences into an English-like word order, we therefore have to build complex rules (Sudoh et al., 2011b). VP_SW NP_ST HFE: he _va0 NP_ST yesterday _va2 books bought Reordering S VP VP NP English: he _va0) ( NP bought books _va2) ( yesterday Figure"
P12-2061,N07-1051,0,0.21339,"d with “ ST” (indicating “Straight”). A node with only one child is not annotated with either “ ST” or “ SW”. The result is an HFE sentence in a binary tree annotated with “ SW” and “ ST” suffixes. Observe that the HFE sentences can be regarded as binary trees annotated with syntax tags augmented with swap/straight suffixes. Therefore, the structures of these binary trees can be learnable by using an off-the-shelf grammar learning algorithm. The learned parsing model can be regarded as an ITG model (Wu, 1997) between the HFE and English sentences. 2 In this paper, we used the Berkeley Parser (Petrov and Klein, 2007) for learning these structures. The HFE sentences can be parsed by using the learned parsing model. Then the parsed structures can be converted into their corresponding English structures by swapping the “ SW” nodes. Note that this parsing model jointly learns how to parse and swap the HFE sentences. 4 Detailed Explanation of Our Method This section explains the proposed method, which is based on the post-ordering framework using the parsing model. 4.2 HFE and Articles This section describes the details of HFE sentences. In HFE sentences: 1) Heads are final except for coordination. 2) Pseudo-p"
P12-2061,D09-1105,0,0.0460596,"g one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, simply moves syntactic heads to the end of corresponding syntactic constituents (e.g., phrases and clauses). This"
P12-2061,2011.mtsummit-papers.34,0,0.0832434,"Missing"
P12-2061,J97-3002,0,0.424822,"e swapped nodes are annotated with “ SW”. When the two nodes are not swapped, they are annotated with “ ST” (indicating “Straight”). A node with only one child is not annotated with either “ ST” or “ SW”. The result is an HFE sentence in a binary tree annotated with “ SW” and “ ST” suffixes. Observe that the HFE sentences can be regarded as binary trees annotated with syntax tags augmented with swap/straight suffixes. Therefore, the structures of these binary trees can be learnable by using an off-the-shelf grammar learning algorithm. The learned parsing model can be regarded as an ITG model (Wu, 1997) between the HFE and English sentences. 2 In this paper, we used the Berkeley Parser (Petrov and Klein, 2007) for learning these structures. The HFE sentences can be parsed by using the learned parsing model. Then the parsed structures can be converted into their corresponding English structures by swapping the “ SW” nodes. Note that this parsing model jointly learns how to parse and swap the HFE sentences. 4 Detailed Explanation of Our Method This section explains the proposed method, which is based on the post-ordering framework using the parsing model. 4.2 HFE and Articles This section desc"
P12-2061,C04-1073,0,0.136648,"The word reordering problem is a challenging one when translating between languages with widely different word orders such as Japanese and English. Many reordering methods have been proposed in statistical machine translation (SMT) research. Those methods can be classified into the following three types: Type-1: Conducting the target word selection and reordering jointly. These include phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Galley et al., 2004; Ding and Palmer, 2005; Liu et al., 2006; Liu et al., 2009). Type-2: Pre-ordering (Xia and McCord, 2004; Collins et al., 2005; Tromble and Eisner, 2009; Ge, 2010; Isozaki et al., 2010b; DeNero and Uszkoreit, Before explaining our method, we explain the preordering method for English to Japanese used in the post-ordering framework. In English-Japanese translation, Isozaki et al. (2010b) proposed a simple pre-ordering method that achieved the best quality in human evaluations, which were conducted for the NTCIR-9 patent machine translation task (Sudoh et al., 2011a; Goto et al., 2011). The method, which is called head finalization, simply moves syntactic heads to the end of corresponding syntacti"
P12-2061,P07-2045,0,\N,Missing
P13-1016,J96-1002,0,0.0215273,"i, and sj , which is the word specified at j, or both the context of i and the context of j simultaneously. Distance is considered using the distance class d. Distortion is represented by distance and orientation. The pair model considers distortion using six joint classes of d and o. 3.2 Pair Model The pair model utilizes the word at the CP, the word at an NPC, and the context of the CP and the NPC simultaneously to estimate the NP. This can be done by our distortion model definition and the learning strategy described in the previous section. In this work, we use the maximum entropy method (Berger et al., 1996) as a discriminative machine learning method. The reason for this is that a model based on the maximum entropy method can calculate probabilities. However, if we use scores as an approximation of the distortion probabilities, various discriminative machine learning methods can be applied to build the distortion model. Let s be a source word and sn1 = s1 s2 ...sn be a source sentence. We add a beginning of sentence (BOS) marker to the head of the source sentence and an end of sentence (EOS) marker to the end, so the source sentence S is expressed as sn+1 0 (s0 = BOS, sn+1 = EOS). Our distortion"
P13-1016,J07-2003,0,0.673825,"for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, 1 A language model also supports the estimation. In this paper, reordering models for phrase-based SMT, which are intended to estimate the source word position to be translated next in decoding, are called distortion models. This estimation is used to produce a hypothesis in the target language word order sequentially from left to right. 2 155 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 155–165, c"
P13-1016,C10-2033,0,0.528996,"Missing"
P13-1016,D08-1089,0,0.0586676,"he NPC. 3 Proposed Method In this section, we first define our distortion model and explain our learning strategy. Then, we describe two proposed models: the pair model and the sequence model that is the further improved model. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP and its context. Their mod"
P13-1016,P06-1077,0,0.0741182,"nces appropriately from the training data.16 together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to use distortion scores considering the distances for decoders that do not generate hypotheses from left to right. If it could be applied to these methods, our distortion model might contribute to tree-based SMT methods. Investigating the effects will be for future work. 6 Conclusion This paper described our distortion models for phrase-based SMT. Our sequence model simply consists of only one probabilistic model, but it can consider"
P13-1016,N04-1035,0,0.0561875,"n the effect of distances appropriately from the training data.16 together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to use distortion scores considering the distances for decoders that do not generate hypotheses from left to right. If it could be applied to these methods, our distortion model might contribute to tree-based SMT methods. Investigating the effects will be for future work. 6 Conclusion This paper described our distortion models for phrase-based SMT. Our sequence model simply consists of only one probabilistic model,"
P13-1016,P09-2061,0,0.018513,"n if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP and its context. Their model does not simultaneously consider both the word specified at the CP and the word specified at the NPCs. Green et al. (2010) proposed distortion models that used context. Their model (the outbound model) estimates how far the NP should be from the CP using the word at the CP and its context.5 Their model does not simultaneously con5 3.1 Distortion Model and Learning Strategy First, we define our distortion model. Let i be a CP, j be an NPC, S be a source sentence, and X be the random"
P13-1016,P03-1021,0,0.0383462,"ors, we removed articles {a, an, the} in English and particles {ga, wo, wa} in Japanese before performing word alignments because these function words do not correspond to any words in the other languages. After word alignment, we restored the removed words and shifted the word alignment positions to the original word positions. We used 5gram language models that were trained using the English side of each set of bilingual training data. We used an in-house standard phrase-based SMT system compatible with the Moses decoder (Koehn et al., 2007). The SMT weighting parameters were tuned by MERT (Och, 2003) using the development data. To stabilize the MERT results, we tuned three times by MERT using the first half of the development data and we selected the SMT weighting parameter set that performed the best on the second half of the development data based on the BLEU scores from the three SMT weighting parameter sets. We compared systems that used a common SMT feature set from standard SMT features and different distortion model features. The common SMT feature set consists of: four translation model features, phrase penalty, word penalty, and a language model feature. The compared different di"
P13-1016,N10-1129,0,0.272219,"at the CP but also the word at a NP candidate (NPC) should be considered simultaneously. In (c) and (d) in Figure 2, the word (kare) at the CP is the same and karita (borrowed) and katta (bought) are at the NPCs. Karita is the word at the NP and katta is not the word at the NP for (c), while katta is the word at the NP and karita is not the word at the NP for (d). From these examples, considering what the word is at the NP 3 NP is not always one position, because there may be multiple correct hypotheses. 4 This definition is slightly different from that of existing methods such as Moses and (Green et al., 2010). In existing methods, CP is the rightmost position of the last translated source phrase and NP is the leftmost position of the source phrase to be translated next. Note that existing methods do not consider word-level correspondences. 156 sider both the word specified at the CP and the word specified at an NPC. For example, the outbound model considers the word specified at the CP, but does not consider the word specified at an NPC. Their models also do not consider relative word order. In contrast, our distortion model solves the aforementioned problems. Our distortion models utilize the wor"
P13-1016,P02-1040,0,0.0864299,"010). The relative source sentence position is discretized into five bins, one for each quintile of the sentence. For the inbound model13 , i of the feature templates was changed to j. Features occurring four or more times in the training sentences were used. The maximum entropy method with Gaussian prior smoothing was used to estimate the model parameters. The MSD bidirectional lexical distortion model was built using all of the data used to build the translation model. 4.4 Results and Discussion We evaluated translation quality based on the caseinsensitive automatic evaluation score BLEU-4 (Papineni et al., 2002). We used distortion limits of 10, 20, 30, and unlimited (∞), which limited the number of words for word reordering to a maximum number. Table 3 presents our main results. The proposed SEQUENCE outperformed the baselines for both Japanese to English and Chinese to English translation. This demonstrates the effectiveness of the proposed SEQUENCE. The scores of the proposed SEQUENCE were higher than those 4.2 Training for the Proposed Models Our distortion model was trained as follows: We used 0.2 million sentence pairs and their word alignments from the data used to build the translation model"
P13-1016,N04-4026,0,0.287066,"Missing"
P13-1016,2005.iwslt-1.8,0,0.323302,"between the CP and the NPC. 3 Proposed Method In this section, we first define our distortion model and explain our learning strategy. Then, we describe two proposed models: the pair model and the sequence model that is the further improved model. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP and the word at an NPC. However, their model did not use context, relative word order, or words between the CP and the NPC. Ni et al. (2009) proposed a method that adjusts the linear distortion cost using the word at the CP"
P13-1016,J97-3002,0,0.0396293,"istance class feature used in the model was the same (e.g., distortions from 5 to 20 were the same distance class feature), PAIR produced average distortion probabilities that were almost the same. In contrast, the average distortion probabilities for SEQUENCE decreased when the lengths of the distortions increased, even if the distance class feature was the same, and this behavior was the same as that of CORPUS. This confirms that the proposed SEQUENCE could learn the effect of distances appropriately from the training data.16 together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering dista"
P13-1016,P07-2045,0,0.0157849,"ummary, in order to estimate the NP, the following should be considered simultaneously: the word at the NP, the word at the CP, the relative word order among the NPCs, the words surrounding NP and CP (context), and the words between the CP and the NPC. 3 Proposed Method In this section, we first define our distortion model and explain our learning strategy. Then, we describe two proposed models: the pair model and the sequence model that is the further improved model. There are distortion models that do not require a parser for phrase-based SMT. The linear distortion cost model used in Moses (Koehn et al., 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. The MSD lexical reordering model (Tillman, 2004; Koehn et al., 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. Thus, these models are not sufficient for long distance word reordering. Al-Onaizan and Papineni (2006) proposed a distortion model that used the word at the CP"
P13-1016,C04-1073,0,0.12617,"9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, 1 A language model also supports the estimation. In this paper, reordering models for phrase-based SMT, which are intended to estimate the source word position to be translated next in decoding, are called distortion models. This estimation is used to produce a hypothesis in the target language word order sequentially from left to right. 2 155 Proceedings of the 51st Annual Meeting of the Association fo"
P13-1016,P12-1095,0,0.0139432,"models for phrase-based SMT. Our sequence model simply consists of only one probabilistic model, but it can consider rich context. Experiments indicate that our models achieved better performance and the sequence model could learn the effect of distances appropriately. Since our models do not require a parser, they can be applied to many languages. Future work includes application to other language pairs, incorporation into ITG constraint methods and other reordering methods, and application to tree-based SMT methods. 5 Related Works We discuss related works other than discussed in Section 2. Xiong et al. (2012) proposed a model predicting the orientation of an argument with respect to its verb using a parser. Syntactic structures and predicate-argument structures are useful for reordering. However, orientations do not handle distances. Thus, our distortion model does not compete against the methods predicting orientations using a parser and would assist them if used References 16 Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association"
P13-1016,W04-3250,0,0.119941,"Missing"
P13-1016,W08-0401,1,0.819648,"e (e.g., distortions from 5 to 20 were the same distance class feature), PAIR produced average distortion probabilities that were almost the same. In contrast, the average distortion probabilities for SEQUENCE decreased when the lengths of the distortions increased, even if the distance class feature was the same, and this behavior was the same as that of CORPUS. This confirms that the proposed SEQUENCE could learn the effect of distances appropriately from the training data.16 together. There are word reordering constraint methods using ITG (Wu, 1997) for phrase-based SMT (Zens et al., 2004; Yamamoto et al., 2008; Feng et al., 2010). These methods consider sentence level consistency with respect to ITG. The ITG constraint does not consider distances of reordering and was used with other distortion models. Our distortion model does not consider sentence level consistency, so our distortion model and ITG constraint methods are thought to be complementary. There are tree-based SMT methods (Chiang, 2007; Galley et al., 2004; Liu et al., 2006). In many cases, tree-based SMT methods do not use the distortion models that consider reordering distance apart from translation rules because it is not trivial to u"
P13-1016,C04-1030,1,0.952204,"experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models. 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders. To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al., 2004), pre-ordering (Xia and McCord, 2004), hierarchical phrase-based SMT (Chiang, 2007), and syntax-based SMT (Yamada and Knight, 2001). In general, source language syntax is useful for handling long distance word reordering. However, 1 A language model also supports the estimation. In this paper, reordering models for phrase-based SMT, which are intended to estimate the source word position to be translated next in decoding, are called distortion models. This estimation is used to produce a hypothesis in the target language word order sequentially from left to right. 2 155 Proceedings of the 51st"
P13-1016,P06-1067,0,\N,Missing
P13-1016,P01-1067,0,\N,Missing
P14-2026,W13-2239,0,0.0189713,"-ordering rules. They are: (b) Stanford typed dependency parse tree Figure 1: A constituent parse tree and its corresponding Stanford typed dependency parse tree for the same Chinese sentence. spent more than two months discovering the rules introduced in this paper. By applying our rules and Wang et al.’s rules, one can use both dependency and constituency parsers for pre-ordering in Chinese-English PBSMT. This is especially important on the point of the system combination of PBSMT systems, because the diversity of outputs from machine translation systems is important for system combination (Cer et al., 2013). By using both our rules and Wang et al.’s rules, one can obtain diverse machine translation results because the pre-ordering results of these two rule sets are generally different. Another similar work is that of (Xu et al., 2009). They created a pre-ordering rule set for dependency parsers from English to several SOV languages. In contrast, our rule set is for ChineseEnglish PBSMT. That is, the direction of translation is opposite. Because there are a lot of language specific decisions that reflect specific aspects of the source language and the language pair combination, our rule set provi"
P14-2026,C10-1071,0,0.0152835,"procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence. As shown in the figure, the number of nodes in the dependency parse tree (i.e. 9) is much fewer than that in its corresponding constituen"
P14-2026,W09-2307,0,0.0252693,"parse tree Chinese sentence. As shown in the figure, the number of nodes in the dependency parse tree (i.e. 9) is much fewer than that in its corresponding constituent parse tree (i.e. 17). Because dependency parse trees are generally more concise than the constituent ones, they can conduct longdistance reorderings in a finer way. Thus, we attempted to conduct pre-ordering based on dependency parsing. There are two widely-used dependency systems – Stanford typed dependencies and CoNLL typed dependencies. For Chinese, there are 45 types of grammatical relations for Stanford typed dependencies (Chang et al., 2009) and 25 for CoNLL typed dependencies. As we thought that Stanford typed dependencies could describe language phenomena more meticulously owing to more types of grammatical relations, we preferred to use it for searching candidate preordering rules. We designed two types of formats in our dependency-based pre-ordering rules. They are: (b) Stanford typed dependency parse tree Figure 1: A constituent parse tree and its corresponding Stanford typed dependency parse tree for the same Chinese sentence. spent more than two months discovering the rules introduced in this paper. By applying our rules a"
P14-2026,P02-1040,0,0.0985432,",752 547,084 Table 1: The comparison of four systems, including the performance (BLEU) on the test set, the total count of each rule set and the number of sentences they were applied to on the training set. tracted from the Linguistic Data Consortium’s parallel news corpora. Our development set was the official NIST MT evaluation data from 2002 to 2005, consisting of 4476 Chinese-English sentences pairs. Our test set was the NIST 2006 MT evaluation data, consisting of 1664 sentence pairs. We employed the Stanford Segmenter1 to segment all of the data sets. For evaluation, we used BLEU scores (Papineni et al., 2002). We implemented the constituent-based preordering rule set in Wang et al. (2007) for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences. For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0. We conducted our dependency-based preordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al., 2012). First, we converted the constituent parse trees in the results of the Berkeley Parser into dependency"
P14-2026,P06-1055,0,0.00873215,"were applied to on the training set. tracted from the Linguistic Data Consortium’s parallel news corpora. Our development set was the official NIST MT evaluation data from 2002 to 2005, consisting of 4476 Chinese-English sentences pairs. Our test set was the NIST 2006 MT evaluation data, consisting of 1664 sentence pairs. We employed the Stanford Segmenter1 to segment all of the data sets. For evaluation, we used BLEU scores (Papineni et al., 2002). We implemented the constituent-based preordering rule set in Wang et al. (2007) for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences. For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0. We conducted our dependency-based preordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al., 2012). First, we converted the constituent parse trees in the results of the Berkeley Parser into dependency parse trees by employing a tool in the Stanford Parser (Klein and Manning, 2003). For the Mate Parser, POS tagged inputs are required both in training and in inferenc"
P14-2026,P12-2003,0,0.0130816,"yed the Stanford Segmenter1 to segment all of the data sets. For evaluation, we used BLEU scores (Papineni et al., 2002). We implemented the constituent-based preordering rule set in Wang et al. (2007) for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences. For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0. We conducted our dependency-based preordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al., 2012). First, we converted the constituent parse trees in the results of the Berkeley Parser into dependency parse trees by employing a tool in the Stanford Parser (Klein and Manning, 2003). For the Mate Parser, POS tagged inputs are required both in training and in inference. Thus, we then extracted the POS information from the results of the Berkeley Parser and used these as the pre-specified POS tags for the Mate Parser. Finally, we applied our dependency-based pre-ordering rule set to the dependency parse trees created from the converted Berkeley Parser and the Mate Parser, respectively. Table"
P14-2026,P05-1066,0,0.155941,"pairs. Previous work has shown that the approaches tackling the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence. As shown in the figure, the number of n"
P14-2026,D07-1077,0,0.265653,"approaches tackling the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence. As shown in the figure, the number of nodes in the dependency parse tree (i"
P14-2026,W12-4207,0,0.229959,"Missing"
P14-2026,I11-1004,0,0.148624,"Missing"
P14-2026,2007.mtsummit-papers.29,0,0.39493,"Missing"
P14-2026,W10-1736,0,0.108857,"Missing"
P14-2026,C04-1073,0,0.257529,"in SMT systems between distant language pairs. Previous work has shown that the approaches tackling the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence."
P14-2026,D11-1017,0,0.0265976,"Missing"
P14-2026,N09-1028,0,0.380798,"munications Technology joycetsai99@gmail.com {mutiyama, eiichiro.sumita}@nict.go.jp yjzhang@bjtu.edu.cn Abstract As a kind of constituent structure, HPSG (Pollard and Sag, 1994) parsing-based pre-ordering showed improvements in SVO-SOV translations, such as English-Japanese (Isozaki et al., 2010; Wu et al., 2011) and Chinese-Japanese (Han et al., 2012). Since dependency parsing is more concise than constituent parsing in describing sentences, some research has used dependency parsing in pre-ordering approaches for language pairs such as Arabic-English (Habash, 2007), and EnglishSOV languages (Xu et al., 2009; Katz-Brown et al., 2011). The pre-ordering rules can be made manually (Collins et al., 2005; Wang et al., 2007; Han et al., 2012) or extracted automatically from a parallel corpus (Xia and McCord, 2004; Habash, 2007; Zhang et al., 2007; Wu et al., 2011). The purpose of this paper is to introduce a novel dependency-based pre-ordering approach through creating a pre-ordering rule set and applying it to the Chinese-English PBSMT system. Experiment results showed that our pre-ordering rule set improved the BLEU score on the NIST 2006 evaluation data by 1.61. Moreover, this rule set substantially"
P14-2026,C08-1137,0,0.0190934,"g the problem by introducing a pre-ordering procedure into phrase-based SMT (PBSMT) were effective. These pre-ordering approaches first parse the source language sentences to create parse trees. Then, syntactic reordering rules are applied to these parse trees with the goal of reordering the source language sentences into the word order of the target language. Syntax-based pre-ordering by employing constituent parsing have demonstrated effectiveness in many language pairs, such as English-French (Xia and McCord, 2004), German-English (Collins et al., 2005), Chinese-English (Wang et al., 2007; Zhang et al., 2008), and English-Japanese (Lee et al., 2010). ∗ This work was done when the first author was on an internship in NICT. 155 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 155–160, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Figure 2: An example of a preposition phrase with a plmod structure. The phrase translates into “in front of the US embassy”. (a) A constituent parse tree Chinese sentence. As shown in the figure, the number of nodes in the dependency parse tree (i.e. 9) is much fewer"
P14-2026,P03-1054,0,0.00382741,"et in Wang et al. (2007) for comparison, which is called WR07 below. The Berkeley Parser (Petrov et al., 2006) was employed for parsing the Chinese sentences. For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0. We conducted our dependency-based preordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al., 2012). First, we converted the constituent parse trees in the results of the Berkeley Parser into dependency parse trees by employing a tool in the Stanford Parser (Klein and Manning, 2003). For the Mate Parser, POS tagged inputs are required both in training and in inference. Thus, we then extracted the POS information from the results of the Berkeley Parser and used these as the pre-specified POS tags for the Mate Parser. Finally, we applied our dependency-based pre-ordering rule set to the dependency parse trees created from the converted Berkeley Parser and the Mate Parser, respectively. Table 1 presents a comparison of the system without pre-ordering, the constituent system using WR07 and two dependency systems employing the converted Berkeley Parser and the Mate Parser, re"
P14-2026,P07-2045,0,0.00682969,"Missing"
P14-2026,W07-0401,0,\N,Missing
P14-2122,J93-2003,0,0.0813025,"h E, formulated as: ′ P ∗ (i|j, I, J)PB (fj |ei ) (9) i=1 ∗ 2.1.2 Bilingual Expectation P (Fkk |F, E, B) = I X 2.2 Maximization P (aj |j, I, J)PB (fj |eaj ), Inspired by (Teh, 2006; Mochihashi et al., 2009; Neubig et al., 2010; Teh and Jordan, 2010), we employ a Pitman-Yor process model to build the segmentation model M or B. The monolingual model M is F ∈F j=1 a ′ fjk =Fkk (6) where J and I are the number of foreign and English words, respectively, and aj is the position of the English word that is aligned to fj in the alignment a. For the alignment we employ an approximation to IBM model 2 (Brown et al., 1993; Och and Ney, 2003) described below. We define the conditional probability of fj given the corresponding English sentence E and the model B as: PM (fj ) = ¡ ¢ max n(fj ) − d, 0 + (θ + d · nM )G0 (fj ) P ′ fj′ n(fj ) + θ ¯ ¯ nM = ¯{fj |n(fj ) &gt; d}¯, (11) (7) where fj is a foreign language word, and n(fj ) is the observed counts of fj , θ is named the strength parameter, G0 (fj ) is named the base distribution of fj , and d is the discount. The bilingual model is Then, the previous dynamic programming method can be extended to the bilingual expectation PB (fj |ei ) = ¡ ¢ max n(fj , ei ) − d, 0"
P14-2122,C10-1092,0,0.0157702,"being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model bilingual UWS under a similar framework with monolingual UWS in order to improve efficiency, and replace Gibbs sampling with expectation maximization (EM) in training. We aware that variational bayes (VB) may be used for speeding up the training of DP-based Unsupervised word segmentation (UWS) can provide domain-adaptive segmentation for statistical machine transl"
P14-2122,W08-0336,0,0.0239815,"able to supervised segmenters on the in-domain NIST OpenMT corpus, and yields a 0.96 BLEU relative increase on NTCIR PatentMT corpus which is out-of-domain. 1 Introduction Many languages, especially Asian languages such as Chinese, Japanese and Myanmar, have no explicit word boundaries, thus word segmentation (WS), that is, segmenting the continuous texts of these languages into isolated words, is a prerequisite for many natural language processing applications including SMT. Though supervised-learning approaches which involve training segmenters on manually segmented corpora are widely used (Chang et al., 2008), yet the criteria for manually annotating words are arbitrary, and the available annotated corpora are limited in both quantity and genre variety. For example, in machine translation, there are various parallel corpora such as 1 http://ntcir.nii.ac.jp/PatentMT 752 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752–758, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Monolingual and bilingual WS can be formulated as follows, respectively, or PYP-based bilingual UWS. However, VB requires f"
P14-2122,J03-1002,0,0.00915462,"′ P ∗ (i|j, I, J)PB (fj |ei ) (9) i=1 ∗ 2.1.2 Bilingual Expectation P (Fkk |F, E, B) = I X 2.2 Maximization P (aj |j, I, J)PB (fj |eaj ), Inspired by (Teh, 2006; Mochihashi et al., 2009; Neubig et al., 2010; Teh and Jordan, 2010), we employ a Pitman-Yor process model to build the segmentation model M or B. The monolingual model M is F ∈F j=1 a ′ fjk =Fkk (6) where J and I are the number of foreign and English words, respectively, and aj is the position of the English word that is aligned to fj in the alignment a. For the alignment we employ an approximation to IBM model 2 (Brown et al., 1993; Och and Ney, 2003) described below. We define the conditional probability of fj given the corresponding English sentence E and the model B as: PM (fj ) = ¡ ¢ max n(fj ) − d, 0 + (θ + d · nM )G0 (fj ) P ′ fj′ n(fj ) + θ ¯ ¯ nM = ¯{fj |n(fj ) &gt; d}¯, (11) (7) where fj is a foreign language word, and n(fj ) is the observed counts of fj , θ is named the strength parameter, G0 (fj ) is named the base distribution of fj , and d is the discount. The bilingual model is Then, the previous dynamic programming method can be extended to the bilingual expectation PB (fj |ei ) = ¡ ¢ max n(fj , ei ) − d, 0 + (θ + d · nei )G0 ("
P14-2122,D09-1075,0,0.0283316,"r the monolingual bigram model, the number of states in the HMM is U times more than that of the monolingual unigram model, as the states at specific position of F are not only related to the length of the current word, but also related to the length of the word before it. Thus its complexity is U 2 times the unigram model’s complexity: Omonoling = O(Ni |F|KU 4 ). Type Mono. Mono. Biling. Biling. (17) 4.1.3 Parameter settings The parameters are tuned on held-out data sets. The maximum length of foreign language words is set to 4. For the PYP model, the base distribution adopts the formula in (Chung and Gildea, 2009), and the strength parameter is set to 1.0, and the discount is set to 1.0 × 10−6 . For bilingual segmentation,the size of the alignment window is set to 6; the probability λφ of foreign language words being generated by an empty Experiments In this section, the proposed method is first validated on monolingual segmentation tasks, and then evaluated in the context of SMT to study whether the translation quality, measured by BLEU, can be improved. 4.1 Experimental Settings 4.1.1 Experimental Corpora Two monolingual corpora and two bilingual corpora are used (Table 2). CHILDES (MacWhinney and Sn"
P14-2122,P03-1021,0,0.0177603,"noling = O(Ni |F|KU 2 ), unigram bigram unigram 4 = O(Ni |F|KU 2 δb ). # Characters 95,809 4,234,824 19,692,605 63,130,757 corpus for UWS methods. The SIGHAN-MSR corpus (Emerson, 2005) consists of manually segmented simplified Chinese news text, released in the SIGHAN bakeoff 2005 shared tasks. The first bilingual corpus: OpenMT06 was used in the NIST open machine translation 2006 Evaluation 2 . We removed the United Nations corpus and the traditional Chinese data sets from the constraint training resources. The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). This data set mainly consists of news text 3 . PatentMT9 is from the shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences. (15) 4.1.2 Performance Measurement and Baseline Methods For the monolingual tasks, the F1 score against the gold annotation is adopted to measure the accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with d"
P14-2122,I05-3017,0,0.194131,"t al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model bilingual UWS under a similar framework with monolingual UWS"
P14-2122,P02-1040,0,0.0934044,"shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences. (15) 4.1.2 Performance Measurement and Baseline Methods For the monolingual tasks, the F1 score against the gold annotation is adopted to measure the accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al., 2002) was used to evaluate the quality. Character-based segmentation, LDC segmenter and Stanford Chinese segmenters were used as the baseline methods. (16) The bilingual expectation is given by Eq. 8, whose complexity is the same as the monolingual case. However, the complexity of calculating the transition probability, in Eqs. 9 and 10, is O(δb ). Thus its overall complexity is: Obiling # Sentences 9,790 90,903 437,004 1,004,000 Table 2: Experimental Corpora where Ni is the number of iterations, K is the average number of characters per sentence, and U is the predefined maximum length of words. Fo"
P14-2122,P06-1085,0,0.0420447,") which is trained on a small amount of annotated news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy"
P14-2122,P07-2045,0,0.0109645,"or MERT tuning (Och, 2003). This data set mainly consists of news text 3 . PatentMT9 is from the shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences. (15) 4.1.2 Performance Measurement and Baseline Methods For the monolingual tasks, the F1 score against the gold annotation is adopted to measure the accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al., 2002) was used to evaluate the quality. Character-based segmentation, LDC segmenter and Stanford Chinese segmenters were used as the baseline methods. (16) The bilingual expectation is given by Eq. 8, whose complexity is the same as the monolingual case. However, the complexity of calculating the transition probability, in Eqs. 9 and 10, is O(δb ). Thus its overall complexity is: Obiling # Sentences 9,790 90,903 437,004 1,004,000 Table 2: Experimental Corpora where Ni is the number of iterations, K is"
P14-2122,P06-1124,0,0.0821838,"Missing"
P14-2122,I05-3027,0,0.0342293,"Missing"
P14-2122,P09-1012,0,0.285732,"d news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilin"
P14-2122,J01-3002,0,0.456396,"Missing"
P14-2122,C08-1128,0,0.047334,"Missing"
P14-2122,I08-1002,0,0.179395,"le to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model"
P14-2122,2008.iwslt-evaluation.1,0,\N,Missing
P15-2089,C14-1108,0,0.0124885,"approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Algorithm 1 Extract training instances. could not be used in hierarchical phrase-based model directly. Nguyen and Vogel (2013) and Cao et al. (2014) proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT. However, their work limited to learning the reordering of continuous phrases. For short phrases, in extreme cases, when phrase length is one, their model only learned reordering for continuous word pairs like Feng et al. (2013)’s work, while our model can be applied to word pairs with longer distances. 2 Require: A pair of parallel sentence f1l and em 1 with word alignments. Ensure: Training examples for M1 , M2 , . . . , MN . for i = 1 to l − 1 do for j = i + 1 to l do if j − i ≤ N then for u = 1 to πi d"
P15-2089,P05-1033,0,0.134178,"machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality. Compared with previous work, our method may more effectively and efficiently exploit helpful word reordering information. 1 Introduction The hierarchical phrase-based model (Chiang, 2005) is capable of capturing rich translation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual informati"
P15-2089,D08-1010,0,0.0211256,"eordering information. 1 Introduction The hierarchical phrase-based model (Chiang, 2005) is capable of capturing rich translation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Algorithm 1 Extract training instances. could not be used in hierarchical phrase-based model directly. Nguyen"
P15-2089,J07-2003,0,0.23551,"Missing"
P15-2089,P13-1032,0,0.21843,"niversity, Shanghai 200240, China 4 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai 200240, China jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp zhaohai@cs.sjtu.edu.cn 1 Abstract Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating. Feng et al. (2013) proposed a word reordering model to learn reorderings only for continuous words, which reduced computation cost a lot compared with Tromble and Eisner (2009)’s model and still achieved significant reordering improvement over the baseline system. In this paper, we incorporate word reordering information into hierarchical phrase-based SMT by training a series of separate reordering submodels for word pairs with different distances. We will demonstrate that the translation performance achieves consistent improvement as more sub-models for longer distance reorderings being integrated, but the imp"
P15-2089,P13-1156,0,0.0142256,"(2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Algorithm 1 Extract training instances. could not be used in hierarchical phrase-based model directly. Nguyen and Vogel (2013) and Cao et al. (2014) proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT. However, their work limited to learning the reordering of continuous phrases. For short phrases, in extreme cases, when phrase length is one, their model only learned reordering for continuous word pairs like Feng et al. (2013)’s work, while our model can be applied to word pairs with longer distances. 2 Require: A pair of parallel sentence f1l and em 1 with word alignments. Ensure: Training examples for M1 , M2 , . . . , MN . for i = 1 to l − 1 do for j = i + 1 to l do if j − i ≤ N"
P15-2089,P09-2061,0,0.01765,"significantly. Compared with previous models (Tromble and Eisner, 2009; Feng et al., 2013), our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings. Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and Statistical models for reordering source words have been used to enhance the hierarchical phrase-based statistical machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality. Compared with previous work, our met"
P15-2089,C10-1050,0,0.314597,"al Institute of Information and Communications Technology, 3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan 3 Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai 200240, China 4 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai 200240, China jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp zhaohai@cs.sjtu.edu.cn 1 Abstract Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating. Feng et al. (2013) proposed a word reordering model to learn reorderings only for continuous words, which reduced computation cost a lot compared with Tromble and Eisner (2009)’s model and still achieved significant reordering improvement over the baseline system. In this paper, we incorporate w"
P15-2089,J03-1002,0,0.0123034,"se that r is applied to the input sentence f1l , where CE JE In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task. The word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ (Och and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE task, training instances extracted from all the 1M sentence pairs were used to train neural reordering models. For JE task, training instances were from 1M sentence pairs that were randomly selected from all the 3.14M sentence pairs. We also implemented Hayashi et al. (2"
P15-2089,P03-1021,0,0.0151868,"cial datasets for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used. The detailed statistics for training, development and test sets are given in Table 1. Integration into the Decoder In the hierarchical phrase-based model, a translation rule r is like: X → hγ, α, ∼i TRAINING where X is a nonterminal, γ and α are respectively source and target strings of terminals and nonterminals, and ∼ is the alignment between nonterminals and terminals in γ and α. Each rule has several features and the feature weights are tuned by the minimum error rate training (MERT) algorithm (Och, 2003). To integrate our model into the hierarchical phrase-based translation system, a new feature scoren (r) is added to each rule r for each Mn . The score of this feature is calculated during decoding. Note that these scores are correspondingly calculated for different sub-models Mn and the sub-model weights are tuned separately. Suppose that r is applied to the input sentence f1l , where CE JE In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the developm"
P15-2089,C08-1041,0,0.0210894,"xploit helpful word reordering information. 1 Introduction The hierarchical phrase-based model (Chiang, 2005) is capable of capturing rich translation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Algorithm 1 Extract training instances. could not be used in hierarchical phrase-based m"
P15-2089,D09-1105,0,0.152786,"e School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan 3 Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai 200240, China 4 Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai 200240, China jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp zhaohai@cs.sjtu.edu.cn 1 Abstract Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating. Feng et al. (2013) proposed a word reordering model to learn reorderings only for continuous words, which reduced computation cost a lot compared with Tromble and Eisner (2009)’s model and still achieved significant reordering improvement over the baseline system. In this paper, we incorporate word reordering information into hierarchical phrase-based SMT by training a series of separate reordering submodels for word pairs with different"
P15-2089,N03-1017,0,0.104813,"less than a specific threshold are useful to improve translation quality. Compared with previous work, our method may more effectively and efficiently exploit helpful word reordering information. 1 Introduction The hierarchical phrase-based model (Chiang, 2005) is capable of capturing rich translation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, c Beijing, China, July"
P15-2089,D13-1140,0,0.124363,"ion of f1l = f1 , . . . , fl and A be word alignments bel tween em 1 and f1 , our model estimates the reordering probability of the source sentence as follows: Pr f1l , em 1 ,A ≈ N Q to learn reorderings for word pairs with different distances. That means, for the word pair hfi , fj i with distance j − i = n, its reordering   probability Pr oijuv |fi−3 , ..., fj+3 , eaiu , eajv is estimated by Mn . Different sub-models are trained and integrated into the translation system separately. Each sub-model Mn is implemented by an FNN, which has the same structure with the neural language model in (Vaswani et al., 2013). The input to Mn is a sequence of n + 9 words: fi−3 , ..., fj+3 , eaiu , eajv . The input layer projects each word into a high dimensional vector using a matrix of input word embeddings. Two hidden layers can combine all input data1. The output layer has two neurons  that give Pr oijuv = 1|fi−3 , ..., fj+3 , eaiu , eajv and  Q n=1 i,j:1≤i&lt;j≤l,j−i=n  Pr f1l , em 1 , A, i, j  (1)  where Pr f1l , em 1 , A, i, j is the reordering probability of the word pair hfi , fj i during translating; N is the maximum distance for source word reordering, which is empirically determined by supposing that"
P15-2089,2005.iwslt-1.8,0,0.0474646,"shold do not improve translation quality significantly. Compared with previous models (Tromble and Eisner, 2009; Feng et al., 2013), our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings. Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and Statistical models for reordering source words have been used to enhance the hierarchical phrase-based statistical machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality."
P15-2089,W06-3108,0,0.0271019,"translation quality significantly. Compared with previous models (Tromble and Eisner, 2009; Feng et al., 2013), our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings. Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and Statistical models for reordering source words have been used to enhance the hierarchical phrase-based statistical machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality. Compared with previ"
P15-2089,P07-2045,0,0.00963672,"Missing"
P15-2089,W06-0127,1,0.737146,"slation system, a new feature scoren (r) is added to each rule r for each Mn . The score of this feature is calculated during decoding. Note that these scores are correspondingly calculated for different sub-models Mn and the sub-model weights are tuned separately. Suppose that r is applied to the input sentence f1l , where CE JE In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task. The word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ (Och and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE"
P15-2089,W04-3250,0,0.0722837,"男生(guy)” is reversed. This is the reason why translation performance improves as more sub-models being integrated. As shown in Table 2, with 4 sub-models being integrated, our model improved baseline system significantly and also outperformed Hayashi model clearly. It is easy to understand, since our model was trained by feed-forward neural network on a high dimensional space and incorporated rich context information, while Hayashi model used the averaged perceptron algorithm and simple features. Table 3b shows the prediction accuracies (b) Significance test results using bootstrap sampling (Koehn, 2004) w.r.t. BLEU scores. The symbol  represents a significant difference at the p &lt; 0.01 level; &gt; represents a significant difference at the p &lt; 0.05 level; − means not significantly different at p = 0.05. Table 2: Translation results. For each translation task, the recent version of the Moses hierarchical phrase-based decoder (Koehn et al., 2007) with the training scripts was used as the baseline system Base. We used the default parameters for Moses. A 5-gram language model was trained on the target side of the training corpus by IRST LM Toolkit3 with the improved Kneser-Ney smoothing. We integr"
P15-2089,C14-1179,0,0.0221259,"ompared with previous models (Tromble and Eisner, 2009; Feng et al., 2013), our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings. Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and Statistical models for reordering source words have been used to enhance the hierarchical phrase-based statistical machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality. Compared with previous work, our method may more effec"
P15-2089,D11-1079,0,\N,Missing
P15-2089,Q13-1027,0,\N,Missing
P15-2089,P08-1114,0,\N,Missing
P15-2089,W13-2258,0,\N,Missing
P15-2089,P02-1038,0,\N,Missing
P15-2089,P10-2002,0,\N,Missing
P15-2089,D15-1164,0,\N,Missing
P15-2089,D11-1125,0,\N,Missing
P16-1130,P10-2002,0,0.0185124,"ver, the information contained in Rule4 will be considered as context features for Rule1. Therefore, this is no longer an advantage for the CSRS model as long as we use rich enough context features, which could be the reason why using both the CSRS and CSRS-MINI models cannot further improve the translation quality compared to using only the CSRS-MINI model. 5 Related Work The rule selection problem for syntax-based SMT has received much attention. He et al. (2008) proposed a lexicalized rule selection model to perform context-sensitive rule selection for hierarchical phrase-base translation. Cui et al. (2010) introduced a joint rule selection model for hierarchical phrase-based translation, which also approximated the rule selection problem by a binary classification problem like our approach. However, these two models adopted linear classifiers similar to those used in the MERS model (Liu et al., 2008), which suffers more from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model. Namely, these methods estimated translation probabili"
P16-1130,P14-1129,0,0.0707017,"Missing"
P16-1130,N13-1001,0,0.0201378,"Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minimal rules for modeling. Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on paral"
P16-1130,P16-1078,0,0.0171099,"tion models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on plain source and target sentences without considering any syntactic information while our neural model learns rule selection for tree-based translation rules and makes use of the tree structure of natural language for better translation. There is also a new syntactic NMT model (Eriguchi et al., 2016), which extends the original sequence-to-sequence NMT model with the source-side phrase structure. Although this model takes source-side syntax into consideration, it still produces target words one by one as a sequence. In contrast, the tree-based translation rules used in our model can take advantage of the hierarchical structures of both source and target languages. 6 Conclusion In this paper, we propose a CSRS model for syntax-based SMT, which is learned by a feedforward neural network on a continuous space. Compared with the previous MERS model that used discrete representations of words"
P16-1130,N04-1035,0,0.0816271,"n rules used in translations down into minimal rules and multiply all probabilities to calculate the necessary features. PP NP IN DT NN on the table 在 桌子 上 extract Rule1 Source tree NN Target string table NP 桌子 DT Rule2 NN the DT Rule3 桌子 table NP 4 Experiments 4.1 NN the x0 PP x0 NP IN DT NN Rule4 on the PP table 在 桌子 上 IN NP 在 x0 上 on x0 PP Rule5 NP IN DT NN Rule6 on the x0 在 x0 上 Figure 3: Rules. and many may only appear a few times in the corpus. To reduce these problems of sparsity, we propose another improvement to the model, specifically through the use of minimal rules. Minimal rules (Galley et al., 2004) are translation rules that cannot be split into two smaller rules. For example, in Figure 3, Rule2 is not a minimal rule, since Rule2 can be split into Rule1 and Rule3. In the same way, Rule4 and Rule6 are not minimal while Rule1, Rule3 and Rule5 are minimal. Minimal rules are more frequent than nonminimal rules and have richer training data. Hence, we can expect that a rule selection model trained on minimal rules will suffer less from data sparsity problems. Besides, without non-minimal rules, the rule selection model will need less memSetting We evaluated the proposed approach for Englisht"
P16-1130,P06-1121,0,0.0645963,".1K 1.5K 27.1K 29.8K 954K 40.4M 37.2M 504K 288K 2K 77.5K 75.4K 2K 58.1K 55.5K 3.14M 104M 118M 273K 150K 2K 66.5K 74.6K 2K 70.6K 78.5K Base MERS CSRS MERS-MINI CSRS-MINI CSRS vs. MERS CSRS-MINI vs. MERS-MINI MERS-MINI vs. MERS CSRS-MINI vs. CSRS EC 29.42 29.75 30.12 30.53 31.63 EJ 37.10 37.76 37.83 38.14 38.32 ED &gt;&gt; &gt;&gt; − &gt; EF &gt;&gt; − &gt;&gt; − EC &gt; &gt;&gt; &gt;&gt; &gt;&gt; EJ − − &gt;&gt; &gt;&gt; Table 3: Significance test results. The symbol &gt;&gt; (&gt;) represents a significant difference at the p < 0.01 (p < 0.05) level and the symbol - represents no significant difference at the p < 0.05 level. performed using the GHKM algorithm (Galley et al., 2006) and the maximum numbers of nonterminals and terminals contained in one rule were set to 2 and 10 respectively. Note that when extracting minimal rules, we release this limit. The decoding algorithm is the bottom-up forest-to-string decoding algorithm of Mi et al. (2008). For English parsing, we used Egret8 , which is able to output packed forests for decoding. We trained the CSRS models (CSRS and CSRSMINI) on translation rules extracted from the training set. Translation rules extracted from the development set were used as validation data for model training to avoid over-fitting. For differe"
P16-1130,P14-1066,0,0.0278847,"8), which suffers more from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model. Namely, these methods estimated translation probabilities for phrase pairs extracted from the parallel corpus. Schwenk (2012) proposed a continuous space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair. Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minimal rules for modeling. Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2"
P16-1130,N04-1014,0,0.0657252,"ules. We tested our model on different translation tasks and the CSRS model outperformed a baseline without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively. 1 S NP VP NP PRP VBD DT NN I caught a thief 我 抓 了 一个 贼 NP NP PRP VBD DT NN I caught a cold 我 得 了 感冒 Rule Extraction S S VP VP NP VBD NP NP VBD NP x0 caught x1 x0 caught x1 x0 抓 了 x1 x0 得 了 x1 Figure 1: An ambiguous source subtree with different translations (English-to-Chinese). Introduction In syntax-based statistical machine translation (SMT), especially tree-to-string (Liu et al., 2006; Graehl and Knight, 2004) and forest-to-string (Mi et al., 2008) SMT, a source tree or forest is used as input and translated by a series of tree-based translation rules into a target sentence. A tree-based translation rule can perform reordering and translation jointly by projecting a source subtree into a target string, which can contain both terminals and nonterminals. One of the difficulties in applying this model is the ambiguity existing in translation rules: a source subtree can have different target translations extracted from the parallel corpus as shown in Figure 1. Selecting correct rules during decoding is"
P16-1130,C08-1041,0,0.018049,"tion than minimal rules. For example, in Figure 3, Rule4 contains more information than Rule1, which could be an advantage for rule selection. However, the information contained in Rule4 will be considered as context features for Rule1. Therefore, this is no longer an advantage for the CSRS model as long as we use rich enough context features, which could be the reason why using both the CSRS and CSRS-MINI models cannot further improve the translation quality compared to using only the CSRS-MINI model. 5 Related Work The rule selection problem for syntax-based SMT has received much attention. He et al. (2008) proposed a lexicalized rule selection model to perform context-sensitive rule selection for hierarchical phrase-base translation. Cui et al. (2010) introduced a joint rule selection model for hierarchical phrase-based translation, which also approximated the rule selection problem by a binary classification problem like our approach. However, these two models adopted linear classifiers similar to those used in the MERS model (Liu et al., 2008), which suffers more from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to lea"
P16-1130,P15-1001,0,0.0379898,"a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on pl"
P16-1130,W04-3250,0,0.140364,"owing their work, the iteration number was set to be 100 and the Gaussian prior was set to be 1. We also compared the original MERS model and the MERS model trained only on minimal rules (MERS-MINI) to test the benefit of using minimal rules for model training. The MERS and CSRS models were both used to calculate features used to rerank unique 1,000best outputs of the baseline system. Tuning is performed to maximize BLEU score using minimum error rate training (Och, 2003). 4.2 Results Table 2 shows the translation results and Table 3 shows significance test results using bootstrap resampling (Koehn, 2004): “Base” stands for the baseline system without any; “MERS”, “CSRS”, “MERS-MINI” and “CSRS-MINI” means the outputs of the baseline system were reranked using features from the MERS, CSRS, MERS-MINI and CSRS-MINI models respectively. Generally, the CSRS model outperformed the MERS model and the CSRS-MINI model outperformed the MERSMINI model on different translation tasks. In addition, using minimal rules for model training benefitted both the MERS and CSRS models. Table 4 shows translation examples in the EC task to demonstrate the reason why our approach improved accuracy. Among all translati"
P16-1130,P08-1023,0,0.171613,"ion tasks and the CSRS model outperformed a baseline without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively. 1 S NP VP NP PRP VBD DT NN I caught a thief 我 抓 了 一个 贼 NP NP PRP VBD DT NN I caught a cold 我 得 了 感冒 Rule Extraction S S VP VP NP VBD NP NP VBD NP x0 caught x1 x0 caught x1 x0 抓 了 x1 x0 得 了 x1 Figure 1: An ambiguous source subtree with different translations (English-to-Chinese). Introduction In syntax-based statistical machine translation (SMT), especially tree-to-string (Liu et al., 2006; Graehl and Knight, 2004) and forest-to-string (Mi et al., 2008) SMT, a source tree or forest is used as input and translated by a series of tree-based translation rules into a target sentence. A tree-based translation rule can perform reordering and translation jointly by projecting a source subtree into a target string, which can contain both terminals and nonterminals. One of the difficulties in applying this model is the ambiguity existing in translation rules: a source subtree can have different target translations extracted from the parallel corpus as shown in Figure 1. Selecting correct rules during decoding is a major challenge for SMT in general,"
P16-1130,P13-4016,1,0.856935,"on tasks. For the ED and EF tasks, the translation systems are trained on Europarl v7 parallel corpus and tested on the WMT 2015 translation task.4 The test sets for the WMT 2014 translation task were used as development sets in our experiments. For the EC and EJ tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 The detailed statistics for training, development and test sets are given in Table 1. The word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab6 for Japanese. For each translation task, we used Travatar (Neubig, 2013) to train a forest-to-string translation system. GIZA++ (Och and Ney, 2003) was used for word alignment. A 5-gram language model was trained on the target side of the training corpus using the IRST-LM Toolkit7 with modified Kneser-Ney smoothing. Rule extraction was 4 The WMT tasks provided other training corpora. We used only the Europarl corpus, because training a large-scale system on the whole data set requires large amounts of time and computational resources. 5 Note that NTCIR-9 only contained a Chinese-to-English translation task. Because we want to test the proposed approach with a simi"
P16-1130,J03-1002,0,0.00539023,"on Europarl v7 parallel corpus and tested on the WMT 2015 translation task.4 The test sets for the WMT 2014 translation task were used as development sets in our experiments. For the EC and EJ tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 The detailed statistics for training, development and test sets are given in Table 1. The word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab6 for Japanese. For each translation task, we used Travatar (Neubig, 2013) to train a forest-to-string translation system. GIZA++ (Och and Ney, 2003) was used for word alignment. A 5-gram language model was trained on the target side of the training corpus using the IRST-LM Toolkit7 with modified Kneser-Ney smoothing. Rule extraction was 4 The WMT tasks provided other training corpora. We used only the Europarl corpus, because training a large-scale system on the whole data set requires large amounts of time and computational resources. 5 Note that NTCIR-9 only contained a Chinese-to-English translation task. Because we want to test the proposed approach with a similarly accurate parsing model across our tasks, we used English as the sourc"
P16-1130,P03-1021,0,0.0729483,"stems. Table 1: Data sets. 8 ED 15.00 15.62 16.15 15.77 16.49 ing instances for their model were extracted from the training set. Following their work, the iteration number was set to be 100 and the Gaussian prior was set to be 1. We also compared the original MERS model and the MERS model trained only on minimal rules (MERS-MINI) to test the benefit of using minimal rules for model training. The MERS and CSRS models were both used to calculate features used to rerank unique 1,000best outputs of the baseline system. Tuning is performed to maximize BLEU score using minimum error rate training (Och, 2003). 4.2 Results Table 2 shows the translation results and Table 3 shows significance test results using bootstrap resampling (Koehn, 2004): “Base” stands for the baseline system without any; “MERS”, “CSRS”, “MERS-MINI” and “CSRS-MINI” means the outputs of the baseline system were reranked using features from the MERS, CSRS, MERS-MINI and CSRS-MINI models respectively. Generally, the CSRS model outperformed the MERS model and the CSRS-MINI model outperformed the MERSMINI model on different translation tasks. In addition, using minimal rules for model training benefitted both the MERS and CSRS mod"
P16-1130,C12-2104,0,0.0248686,"phrase-based translation, which also approximated the rule selection problem by a binary classification problem like our approach. However, these two models adopted linear classifiers similar to those used in the MERS model (Liu et al., 2008), which suffers more from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model. Namely, these methods estimated translation probabilities for phrase pairs extracted from the parallel corpus. Schwenk (2012) proposed a continuous space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair. Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minima"
P16-1130,P15-1004,0,0.0145628,"odel for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on plain source and target sentences without considering any syntactic information while our neural model learns rule selection for tree-based translation rules and makes use of the tree structure of natural language for better translation. There is also a new syntactic NMT model (Eriguchi et al., 2016), which extends the original sequence-to-sequence NMT model with the source-side phrase structure. Although this model tak"
P16-1130,P11-1086,0,0.0154809,"space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair. Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minimal rules for modeling. Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al."
P16-1130,D13-1140,0,0.0199508,"MERS probability feature, and, h2 is a penalty feature counting the number of predictions made by the MERS model. 3 where v ∈ {0, 1} is an indicator of whether t˜ is translated into e˜. This is in contrast to the MERS model, which treated the rule selection problem as a multi-class classification task. If instead we attempted to estimate output probabilities for all different e˜, the cost of estimating the normalization coefficient would be prohibitive, as the number of unique output-side word strings e˜ is large. There are a number of remedies to this, including noise contrastive estimation (Vaswani et al., 2013), but the binary approximation method has been reported to have better performance (Zhang et al., 2015). To learn this model, we use a feed-forward neural network with structure similar to neural network language models (Vaswani et al., 2013). The input of the neural rule selection model is a vector representation for t˜, another vector representation for e˜, and a set of ξ vector representations for both source-side and target-side context words of r: In our model, C (r) is calculated differently depending on the number of nonterminals included in the rule. Specifically, Equation 7 defines Co"
P16-1130,P06-1077,0,0.310903,"d to non-minimal rules. We tested our model on different translation tasks and the CSRS model outperformed a baseline without rule selection and the previous MERS model by up to 2.2 and 1.1 points of BLEU score respectively. 1 S NP VP NP PRP VBD DT NN I caught a thief 我 抓 了 一个 贼 NP NP PRP VBD DT NN I caught a cold 我 得 了 感冒 Rule Extraction S S VP VP NP VBD NP NP VBD NP x0 caught x1 x0 caught x1 x0 抓 了 x1 x0 得 了 x1 Figure 1: An ambiguous source subtree with different translations (English-to-Chinese). Introduction In syntax-based statistical machine translation (SMT), especially tree-to-string (Liu et al., 2006; Graehl and Knight, 2004) and forest-to-string (Mi et al., 2008) SMT, a source tree or forest is used as input and translated by a series of tree-based translation rules into a target sentence. A tree-based translation rule can perform reordering and translation jointly by projecting a source subtree into a target string, which can contain both terminals and nonterminals. One of the difficulties in applying this model is the ambiguity existing in translation rules: a source subtree can have different target translations extracted from the parallel corpus as shown in Figure 1. Selecting correc"
P16-1130,P14-1011,0,0.01589,"from the data sparsity 1379 problem compared to the CSRS model. There are also existing works that exploited neural networks to learn translation probabilities for translation rules used in the phrase-based translation model. Namely, these methods estimated translation probabilities for phrase pairs extracted from the parallel corpus. Schwenk (2012) proposed a continuous space translation model, which calculated the translation probability for each word in the target phrase and then multiplied the probabilities together as the translation probability of the phrase pair. Gao et al. (2014) and Zhang et al. (2014) proposed methods to learn continuous space phrase representations and use the similarity between the source and target phrases as translation probabilities for phrase pairs. All these three methods can only be used for the phrase-based translation model, not for syntaxbased translation models. There are also works that used minimal rules for modeling. Vaswani et al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method t"
P16-1130,D08-1010,0,0.241026,"tion rules: a source subtree can have different target translations extracted from the parallel corpus as shown in Figure 1. Selecting correct rules during decoding is a major challenge for SMT in general, and syntax-based models are no exception. There have been several methods proposed to resolve this ambiguity. The most simple method, used in the first models of tree-to-string translation (Liu et al., 2006), estimated the probability of a translation rule by relative frequencies. For example, in Figure 1, the rule that occurs more times in the training data will have a higher score. Later, Liu et al. (2008) proposed a maximum entropy based rule selection (MERS, Section 2) model for syntax-based SMT, which used contextual information for rule selection, such as words surrounding a rule and words covered by nonterminals in a rule. For example, to choose the correct rule from the two rules in Figure 1 for decoding a particular input sentence, if the source phrase covered by “x1” is “a thief” and this child phrase 1372 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1372–1381, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Ling"
P16-1130,D15-1250,1,0.828569,"S model. 3 where v ∈ {0, 1} is an indicator of whether t˜ is translated into e˜. This is in contrast to the MERS model, which treated the rule selection problem as a multi-class classification task. If instead we attempted to estimate output probabilities for all different e˜, the cost of estimating the normalization coefficient would be prohibitive, as the number of unique output-side word strings e˜ is large. There are a number of remedies to this, including noise contrastive estimation (Vaswani et al., 2013), but the binary approximation method has been reported to have better performance (Zhang et al., 2015). To learn this model, we use a feed-forward neural network with structure similar to neural network language models (Vaswani et al., 2013). The input of the neural rule selection model is a vector representation for t˜, another vector representation for e˜, and a set of ξ vector representations for both source-side and target-side context words of r: In our model, C (r) is calculated differently depending on the number of nonterminals included in the rule. Specifically, Equation 7 defines Cout (r, n) to be context words (n-grams) around r and Cin (r, n, Xk ) to be boundary words (n-grams) cov"
P16-1130,D15-1166,0,0.012445,"t al. (2011) proposed a rule Markov model using minimal rules for both training and decoding to achieve a slimmer model, a faster decoder and comparable performance with using non-minimal rules. Durrani et al. (2013) proposed a method to model with minimal translation units and decode with phrases for phrasebased SMT to improve translation performances. Both of these two methods do not use distributed representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these mode"
P16-1130,W06-0127,0,0.0362305,"o-German (ED), English-to-French (EF), English-to-Chinese (EC) and English-to-Japanese (EJ) translation tasks. For the ED and EF tasks, the translation systems are trained on Europarl v7 parallel corpus and tested on the WMT 2015 translation task.4 The test sets for the WMT 2014 translation task were used as development sets in our experiments. For the EC and EJ tasks, we used datasets provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011).5 The detailed statistics for training, development and test sets are given in Table 1. The word segmentation was done by BaseSeg (Zhao et al., 2006) for Chinese and Mecab6 for Japanese. For each translation task, we used Travatar (Neubig, 2013) to train a forest-to-string translation system. GIZA++ (Och and Ney, 2003) was used for word alignment. A 5-gram language model was trained on the target side of the training corpus using the IRST-LM Toolkit7 with modified Kneser-Ney smoothing. Rule extraction was 4 The WMT tasks provided other training corpora. We used only the Europarl corpus, because training a large-scale system on the whole data set requires large amounts of time and computational resources. 5 Note that NTCIR-9 only contained"
P16-1130,P15-1002,0,0.0492218,"Missing"
P16-1130,P15-1003,0,0.0142853,"ibuted representations as used in our model for better generalization. In addition, neural machine translation (NMT) has shown promising results recently (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b). NMT uses a recurrent neural network to encode the whole source sentence and then produce the target words one by one. These models can be trained on parallel corpora and do not need word alignments to be learned in advance. There are also neural translation models that are trained on word-aligned parallel corpus (Devlin et al., 2014; Meng et al., 2015; Zhang et al., 2015; Setiawan et al., 2015), which use the alignment information to decide which parts of the source sentence are more important for predicting one particular target word. All these models are trained on plain source and target sentences without considering any syntactic information while our neural model learns rule selection for tree-based translation rules and makes use of the tree structure of natural language for better translation. There is also a new syntactic NMT model (Eriguchi et al., 2016), which extends the original sequence-to-sequence NMT model with the source-si"
P17-2089,D11-1033,0,0.776056,"2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains a"
P17-2089,2014.iwslt-evaluation.1,0,0.0320202,"Missing"
P17-2089,P15-1001,0,0.0256648,"er, we exploit the NMT’s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver,"
P17-2089,2016.amta-researchers.8,0,0.537509,"hod can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able to improve translation performance. Recently Neural Machine Translation (NMT) has become prominent in the field. However, most of the existing domain adaptation methods only foc"
P17-2089,D15-1147,0,0.0659529,"Missing"
P17-2089,P07-2045,0,0.0260817,"Missing"
P17-2089,P13-2119,0,0.356115,"e training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able"
P17-2089,2015.mtsummit-papers.10,0,0.319092,"a few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and"
P17-2089,2015.iwslt-evaluation.11,0,0.719606,"Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelr"
P17-2089,P16-2021,0,0.0875788,"bedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver, Canada, July 30 - August 4, 2017."
P17-2089,D14-1062,0,0.0613509,"Missing"
P17-2089,P10-2041,0,0.40117,"od (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and out-of-domain data and select training data from the out-ofdomain data using a cut-off threshold on the resulting scores. A language model can be used to score sentences (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Wang et al., 2015), as well as joint models (Hoang and Sima’an, 2014a,b; Durrani et al., 2015), and more recently Convolutional Neural Network (CNN) models (Chen et al., 2016). These methods select useful sentences from the whole corpus, so they can be directly applied to NMT. However, these methods are specifically designed for PBSMT and nearly all of them use the models or criteria which do not have a direct relationship with the neural Although new corpora are becoming increasingly available for machine translation, only those that belong to the sam"
P17-2089,P02-1040,0,0.117011,"Missing"
P17-2089,E12-1055,0,0.0294002,"ma,eiichiro.sumita}@nict.go.jp Abstract a few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained"
P17-2089,P13-1082,0,0.0794448,"ta}@nict.go.jp Abstract a few works concerning NMT adaptation (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016). Most traditional adaptation methods focus on Phrase-Based Statistical Machine Translation (PBSMT) and they can be broken down broadly into two main categories namely model adaptation and data selection (Joty et al., 2015) as follows. For model adaptation, several PBSMT models, such as language models, translation models and reordering models, individually corresponding to each corpus, are trained. These models are then combined to achieve the best performance (Sennrich, 2012; Sennrich et al., 2013; Durrani et al., 2015). Since these methods focus on the internal models within a PBSMT system, they are not applicable to NMT adaptation. Recently, an NMT adaptation method (Luong and Manning, 2015) was proposed. The training is performed in two steps: first the NMT system is trained using out-of-domain data, and then further trained using in-domain data. Empirical results show their method can improve NMT performance, and this approach provides a natural baseline. For adaptation through data selection, the main idea is to score the out-domain data using models trained from the in-domain and"
P17-2089,P16-1008,0,0.0219846,"NMT’s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver, Canada, July 30"
P17-2089,C16-1295,1,0.865382,"ms apply a similar sentence representation. In comparison, we adopt a transition layer between the source and target layers and don’t use test data. 2 It is possible to use a sample of the out-of-domain data. In this paper, we use all of them. 561 vector center CFin as d(vf , CFin ) and out-ofdomain vector center CFout as d(vf , CFout ), respectively. We use the difference δ of these two distances to classify each sentence: δf = d(vf , CFin ) − d(vf , CFout ). blog texts. The statistics on data sets were shown in Table 1. These adaptation corpora settings were nearly the same as that used in (Wang et al., 2016). The differences were: (5) By using an English-to-French NMT system NEF , we can obtain a target sentence embedding ve , in-domain target vector center CEin and out-of-domain target vector center CEout . Corresponding distance difference δe is, δe = d(ve , CEin ) − d(ve , CEout ). • For IWSLT, they chose FR-EN translation task, which is popular in PBSMT. We chose EN-FR, which is more popular in NMT; • For NIST, they chose 02-05 as dev set, and we chose 02-04. Because we would report results on two test sets (MT05 and MT06) in comparison with only one (MT06). (6) δf , δe and δf e = δf + δe can"
P17-2089,P14-2122,1,0.851211,", the maximum sequence length were 50, and the beam size for decoding was 10. Default dropout were applied. We used a mini-batch Stochastic Gradient Descent (SGD) algorithm together with ADADELTA optimizer (Zeiler, 2012). Training was conducted on a single Tesla K80 GPU. Each NMT model was trained for 500K batches, taking 7-10 days. For sentence embedding and selection, it only took several hours to process all of sentences in the training data, because decoding was not necessary. • NIST 2006 Chinese (ZH) to English corpus5 was used as the in-domain training corpus, following the settings of (Wang et al., 2014). Chinese-to-English UN data set (LDC2013T06) and NTCIR-9 (Goto et al., 2011) patent data set were used as out-ofdomain data. NIST MT 2002-2004 and NIST MT 2005/2006 were used as the development and test data, respectively. We are aware of that there are additional NIST corpora in a similar domain, but because this task was for domain adaptation, we only selected a small subset, which is mainly focused on news and 3 https://wit3.fbk.eu/mt.php?release=2014-01 http://statmt.org/wmt15/translation-task.html 5 http://www.itl.nist.gov/iad/mig/tests/mt/2006/ 4 6 https://github.com/lisa-groundhog/ Gro"
P17-2089,D16-1050,0,0.0127489,"urce sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points. 1 Introduction Recently, Neural Machine Translation (NMT) has set new state-of-the-art benchmarks on many translation tasks (Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2015; Tu et al., 2016; Mi et al., 2016; Zhang et al., 2016). An ever increasing amount of data is becoming available for NMT training. However, only the in-domain or relateddomain corpora tend to have a positive impact on NMT performance. Unrelated additional corpora, known as out-of-domain corpora, have been shown not to benefit some domains and tasks for NMT, such as TED-talks and IWSLT tasks (Luong and Manning, 2015). To the best of our knowledge, there are only 560 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 560–566 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for"
P18-1116,P17-2021,0,0.394426,"forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-string NMT model). The BLEU score of the proposed method is higher than that of the string-to-string NMT, treebased NMT, and forest-based SMT systems. 1 Introduction NMT has witnessed promising improvements recently. Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems (Sutskever et al., 2014; Bahdanau et al., 2014); tree-to-string systems (Eriguchi et al., 2016, 2017); and string-totree systems (Aharoni and Goldberg, 2017; Nadejde et al., 2017). Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features. They can use more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017). ∗ Contribution during internship at National Institute of Information and Communications Technology. † Corresponding author Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays. Based on how to represent trees, there are two main categories of tree-ba"
P18-1116,P05-1022,0,0.250252,"Missing"
P18-1116,P17-1177,0,0.21255,"e used beam search, and fixed the beam size to 12. For the case of Forest (SoA), with 1 core of Tesla K80 GPU and LDC corpus as the training data, training spent about 10 days, and decoding speed is about 10 sentences per second. 5 https://nlp.stanford.edu/software/ stanford-segmenter-2017-06-09.zip 6 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2017/baseline/dataPreparationJE.html 7 LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06 8 https://github.com/EdinburghNLP/ nematus Types FS TN FN Systems & Configurations Mi et al. (2008) Eriguchi et al. (2016) Chen et al. (2017) Li et al. (2017) s2s 1-best (No score) 1-best (SoE) 1-best (SoA) Forest (No score) Forest (SoE) Forest (SoA) MT 03 FBIS LDC 27.10 28.21 29.00 29.71 28.34 29.64 28.40 29.60 27.44 29.18 28.61 29.38 28.78 30.65 29.39 30.80 28.06 29.63 29.58 31.07 29.63 31.35 MT 04 FBIS LDC 28.67 30.09 30.24 31.56 30.00 31.25 29.66 31.96 29.73 30.53 30.07 31.58 30.36 32.22 30.25 32.39 29.51 31.41 30.67 32.69 30.31 33.14 MT 05 FBIS LDC 26.57 28.36 28.38 30.33 28.14 29.59 27.74 29.84 27.32 28.80 28.59 30.01 29.31 30.16 29.30 30.61 28.48 29.75 29.26 30.41 29.87 31.23 p value < 0.01 < 0.05 < 0.005 < 0.01 < 0.001 < 0."
P18-1116,N16-1024,0,0.362868,"more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017). ∗ Contribution during internship at National Institute of Information and Communications Technology. † Corresponding author Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays. Based on how to represent trees, there are two main categories of tree-based NMT methods: representing trees by a tree-structured neural network (Eriguchi et al., 2016; Zaremoodi and Haffari, 2017), representing trees by linearization (Vinyals et al., 2015; Dyer et al., 2016; Ma et al., 2017). Compared with the former, the latter method has a relatively simple model structure, so that a larger corpus can be used for training and the model can be trained within reasonable time, hence is preferred from the viewpoint of computation. Therefore we focus on this kind of methods in this paper. In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). For SMT, forest"
P18-1116,P16-1078,0,0.358869,"f approach has not been attempted. This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-string NMT model). The BLEU score of the proposed method is higher than that of the string-to-string NMT, treebased NMT, and forest-based SMT systems. 1 Introduction NMT has witnessed promising improvements recently. Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems (Sutskever et al., 2014; Bahdanau et al., 2014); tree-to-string systems (Eriguchi et al., 2016, 2017); and string-totree systems (Aharoni and Goldberg, 2017; Nadejde et al., 2017). Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features. They can use more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017). ∗ Contribution during internship at National Institute of Information and Communications Technology. † Corresponding author Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays. Based on ho"
P18-1116,P17-2012,0,0.0416471,"ked forest with a forest-structured neural network. However, their method was evaluated in small-scale MT settings (each training dataset consists of under 10k parallel sentences). In contrast, our proposed method is effective in a largescale MT setting, and we present qualitative analysis regarding the effectiveness of using forests in NMT. Although these methods obtained good results, the tree-structured network used by the encoder made the training and decoding relatively slow, therefore restricts the scope of application. Other attempts at encoding syntactic trees have also been proposed. Eriguchi et al. (2017) combined the Recurrent Neural Network Grammar (Dyer et al., 2016) with NMT systems, while Li et al. (2017) linearized the constituent tree and encoded it using RNNs. The training of these methods is fast, because of the linear structures of RNNs. However, all these syntax-based NMT systems used only the 1-best parsing tree, making the systems sensitive to parsing errors. Instead of using trees to represent syntactic information, some studies use other data structures to represent the latent syntax of the input sentence. For example, Hashimoto and Tsuruoka (2017) proposed translating using a l"
P18-1116,D17-1012,0,0.034678,"Missing"
P18-1116,P08-1067,0,0.492245,"relatively simple model structure, so that a larger corpus can be used for training and the model can be trained within reasonable time, hence is preferred from the viewpoint of computation. Therefore we focus on this kind of methods in this paper. In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). For SMT, forest-based methods have employed a packed forest to address this problem (Huang, 2008), which represents exponentially many parse trees rather than just the 1-best one (Mi et al., 2008; Mi and Huang, 2008). But for NMT, (computationally efficient) forestbased methods are still being explored1 . Because of the structural complexity of forests, the inexistence of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest. This hinders the development of forest-based NMT to some extent. Inspired by the tree-based NMT methods based on linearization, we propose an efficient forestbased NMT a"
P18-1116,W04-3250,0,0.0752847,"29.63 31.35 MT 04 FBIS LDC 28.67 30.09 30.24 31.56 30.00 31.25 29.66 31.96 29.73 30.53 30.07 31.58 30.36 32.22 30.25 32.39 29.51 31.41 30.67 32.69 30.31 33.14 MT 05 FBIS LDC 26.57 28.36 28.38 30.33 28.14 29.59 27.74 29.84 27.32 28.80 28.59 30.01 29.31 30.16 29.30 30.61 28.48 29.75 29.26 30.41 29.87 31.23 p value < 0.01 < 0.05 < 0.005 < 0.01 < 0.001 < 0.001 Table 2: English-Chinese experimental results (character-level BLEU). “FS,” “TN,” and “FN” denote forest-based SMT, tree-based NMT, and forest-based NMT systems, respectively. We performed the paired bootstrap resampling significance test (Koehn, 2004) over the NIST MT 03 to 05 corpus, with respect to the s2s baseline, and list the p values in the table. Types FS TN FN Systems & Configurations Mi et al. (2008) Eriguchi et al. (2016) Chen et al. (2017) Li et al. (2017) s2s 1-best (No score) 1-best (SoE) 1-best (SoA) Forest (No score) Forest (SoE) Forest (SoA) BLEU (test) 34.13 37.52 36.94 36.21 37.10 38.01 38.53 39.42 37.92 41.35 42.17 p value < 0.05 < 0.01 < 0.001 < 0.1 < 0.01 < 0.005 Table 3: English-Japanese experimental results (character-level BLEU). 4.2 Experimental results Table 2 and 3 summarize the experimental results. To avoid the"
P18-1116,P17-1064,0,0.139505,"reebased NMT, and forest-based SMT systems. 1 Introduction NMT has witnessed promising improvements recently. Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems (Sutskever et al., 2014; Bahdanau et al., 2014); tree-to-string systems (Eriguchi et al., 2016, 2017); and string-totree systems (Aharoni and Goldberg, 2017; Nadejde et al., 2017). Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features. They can use more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017). ∗ Contribution during internship at National Institute of Information and Communications Technology. † Corresponding author Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays. Based on how to represent trees, there are two main categories of tree-based NMT methods: representing trees by a tree-structured neural network (Eriguchi et al., 2016; Zaremoodi and Haffari, 2017), representing trees by linearization (Vinyals et al., 2015; Dyer et al., 2016; Ma et al., 2017). Compare"
P18-1116,J93-2004,0,0.0609437,"endency labels and the sequence of words simultaneously, partially utilizing the syntax information, while Li et al. (2017) traversed the constituent tree of the source sentence and combined this with the word sequence, utilizing the syntax information completely. Regarding the linearization used for string-totree NMT (i.e., linearization of the target side), Nadejde et al. (2017) used a CCG supertag sequence as the target sequence, while Aharoni and Goldberg (2017) applied a linearization method in a top-down manner, generating a sequence ensemble for the annotated tree in the Penn Treebank (Marcus et al., 1993). Wu et al. (2017) used transition actions to linearize a dependency tree, and employed the sequence-to-sequence framework for NMT. It can be seen all current tree-based NMT systems use only one tree for encoding or decoding. In contrast, we hope to utilize multiple trees (i.e., a forest). This is not trivial, on account of the lack of a fixed traversal order and the need for a compact representation. 2.3 Packed forest The packed forest gives a representation of exponentially many parsing trees, and can compactly encode many more candidates than the n-best list S [11] [9] S0,5 NP 5.8665 VP NNP"
P18-1116,D08-1022,0,0.0348446,"within reasonable time, hence is preferred from the viewpoint of computation. Therefore we focus on this kind of methods in this paper. In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). For SMT, forest-based methods have employed a packed forest to address this problem (Huang, 2008), which represents exponentially many parse trees rather than just the 1-best one (Mi et al., 2008; Mi and Huang, 2008). But for NMT, (computationally efficient) forestbased methods are still being explored1 . Because of the structural complexity of forests, the inexistence of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest. This hinders the development of forest-based NMT to some extent. Inspired by the tree-based NMT methods based on linearization, we propose an efficient forestbased NMT approach (Section 3), which can en1 Zaremoodi and Haffari (2017) have proposed a forestbased NMT method based on a fores"
P18-1116,P08-1023,0,0.164271,"el can be trained within reasonable time, hence is preferred from the viewpoint of computation. Therefore we focus on this kind of methods in this paper. In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). For SMT, forest-based methods have employed a packed forest to address this problem (Huang, 2008), which represents exponentially many parse trees rather than just the 1-best one (Mi et al., 2008; Mi and Huang, 2008). But for NMT, (computationally efficient) forestbased methods are still being explored1 . Because of the structural complexity of forests, the inexistence of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest. This hinders the development of forest-based NMT to some extent. Inspired by the tree-based NMT methods based on linearization, we propose an efficient forestbased NMT approach (Section 3), which can en1 Zaremoodi and Haffari (2017) have proposed a forestbased NMT me"
P18-1116,P02-1040,0,0.100987,"es in the table. Types FS TN FN Systems & Configurations Mi et al. (2008) Eriguchi et al. (2016) Chen et al. (2017) Li et al. (2017) s2s 1-best (No score) 1-best (SoE) 1-best (SoA) Forest (No score) Forest (SoE) Forest (SoA) BLEU (test) 34.13 37.52 36.94 36.21 37.10 38.01 38.53 39.42 37.92 41.35 42.17 p value < 0.05 < 0.01 < 0.001 < 0.1 < 0.01 < 0.005 Table 3: English-Japanese experimental results (character-level BLEU). 4.2 Experimental results Table 2 and 3 summarize the experimental results. To avoid the affect of segmentation errors, the performance were evaluated by character-level BLEU (Papineni et al., 2002). We compare our proposed models (i.e., Forest (SoE) and Forest (SoA)) with three types of baseline: a string-to-string model (s2s), forest-based models that do not use score sequences (Forest (No score)), and tree-based models that use the 1-best parsing tree (1-best (No score, SoE, SoA)). For the 1-best models, we preserve the nodes and hyperedges that are used in the 1-best constituent tree in the packed forest, and remove all other nodes and hyperedges, yielding a pruned forest that contains only the 1-best constituent tree. For the “No score” configurations, we force the input score seque"
P18-1116,W06-1608,0,0.0520192,"ization (Vinyals et al., 2015; Dyer et al., 2016; Ma et al., 2017). Compared with the former, the latter method has a relatively simple model structure, so that a larger corpus can be used for training and the model can be trained within reasonable time, hence is preferred from the viewpoint of computation. Therefore we focus on this kind of methods in this paper. In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006). For SMT, forest-based methods have employed a packed forest to address this problem (Huang, 2008), which represents exponentially many parse trees rather than just the 1-best one (Mi et al., 2008; Mi and Huang, 2008). But for NMT, (computationally efficient) forestbased methods are still being explored1 . Because of the structural complexity of forests, the inexistence of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest. This hinders the development of forest-based NMT to some extent. Inspi"
P18-1116,E17-3017,0,0.054061,"Missing"
P18-1116,W16-2209,0,0.0382625,"n states of the source-side RNN and target-side RNN, respectively, c is the context vector, and et is the embedding of xt . Bahdanau et al. (2014) introduced an attention mechanism to deal with the issues related to long sequences (Cho et al., 2014). Instead of encoding the source sequence into a fixed vector c, the attention model uses different ci -s when calculating ci = T X αij hj , (6) j=0 exp(a(si−1 , hj )) αij = PT . k=0 exp(a(si−1 , hk )) 2.2 (7) Linear-structured tree-based NMT systems Regarding the linearization adopted for tree-tostring NMT (i.e., linearization of the source side), Sennrich and Haddow (2016) encoded the sequence of dependency labels and the sequence of words simultaneously, partially utilizing the syntax information, while Li et al. (2017) traversed the constituent tree of the source sentence and combined this with the word sequence, utilizing the syntax information completely. Regarding the linearization used for string-totree NMT (i.e., linearization of the target side), Nadejde et al. (2017) used a CCG supertag sequence as the target sequence, while Aharoni and Goldberg (2017) applied a linearization method in a top-down manner, generating a sequence ensemble for the annotated"
P18-1116,P15-1150,0,0.0449153,"Missing"
P18-1116,P17-1065,0,0.0199032,"sequence of words simultaneously, partially utilizing the syntax information, while Li et al. (2017) traversed the constituent tree of the source sentence and combined this with the word sequence, utilizing the syntax information completely. Regarding the linearization used for string-totree NMT (i.e., linearization of the target side), Nadejde et al. (2017) used a CCG supertag sequence as the target sequence, while Aharoni and Goldberg (2017) applied a linearization method in a top-down manner, generating a sequence ensemble for the annotated tree in the Penn Treebank (Marcus et al., 1993). Wu et al. (2017) used transition actions to linearize a dependency tree, and employed the sequence-to-sequence framework for NMT. It can be seen all current tree-based NMT systems use only one tree for encoding or decoding. In contrast, we hope to utilize multiple trees (i.e., a forest). This is not trivial, on account of the lack of a fixed traversal order and the need for a compact representation. 2.3 Packed forest The packed forest gives a representation of exponentially many parsing trees, and can compactly encode many more candidates than the n-best list S [11] [9] S0,5 NP 5.8665 VP NNP VBZ John has VP1,"
P18-1116,P17-1139,0,0.026196,"itnessed promising improvements recently. Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems (Sutskever et al., 2014; Bahdanau et al., 2014); tree-to-string systems (Eriguchi et al., 2016, 2017); and string-totree systems (Aharoni and Goldberg, 2017; Nadejde et al., 2017). Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features. They can use more syntactic information (Li et al., 2017), and can conveniently incorporate prior knowledge (Zhang et al., 2017). ∗ Contribution during internship at National Institute of Information and Communications Technology. † Corresponding author Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays. Based on how to represent trees, there are two main categories of tree-based NMT methods: representing trees by a tree-structured neural network (Eriguchi et al., 2016; Zaremoodi and Haffari, 2017), representing trees by linearization (Vinyals et al., 2015; Dyer et al., 2016; Ma et al., 2017). Compared with the former, the latter method has a relatively simple model stru"
P18-2048,2015.iwslt-evaluation.11,0,0.0402112,"r to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance. 1 Introduction Recently neural machine translation (NMT) has been prominently used to perform various translation tasks (Luong and Manning, 2015; Bojar et al., 2017). However, NMT is much more time-consuming than traditional phrasebased statistical machine translation (PBSMT) due to its deep neural network structure. To improve the efficiency of NMT training, most of the studies focus on reducing the number of parameters in the model (See et al., 2016; Crego et al., 2016; Hubara et al., 2016) and implementing parallelism 298 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 298–304 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics smal"
P18-2048,D15-1166,0,0.0664039,"erate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance. 1 Introduction Recently neural machine translation (NMT) has been prominently used to perform various translation tasks (Luong and Manning, 2015; Bojar et al., 2017). However, NMT is much more time-consuming than traditional phrasebased statistical machine translation (PBSMT) due to its deep neural network structure. To improve the efficiency of NMT training, most of the studies focus on reducing the number of parameters in the model (See et al., 2016; Crego et al., 2016; Hubara et al., 2016) and implementing parallelism 298 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 298–304 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics smal"
P18-2048,P02-1040,0,0.100028,"Missing"
P18-2048,K16-1029,0,0.0408088,"Missing"
P18-2048,E17-3017,0,0.0769889,"Missing"
P18-2048,P17-2089,1,0.867888,"Missing"
P18-2048,kocmi-bojar-2017-curriculum,0,0.279027,"n, Kyoto, Japan {wangrui, mutiyama, eiichiro.sumita}@nict.go.jp Abstract in the data or in the model (Wu et al., 2016; Kalchbrenner et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Although these technologies have been adopted, deep networks have to be improved to achieve state-of-the-art performance in order to handle very large datasets and several training iterations. Therefore, some researchers have proposed to accelerate the NMT training by resampling a smaller subset of the data that makes a relatively high contribution, to improve the training efficiency of NMT. Specifically, Kocmi and Bojar (2017) empirically investigated curriculum learning based on the sentence length and word rank. Wang et al. (2017a) proposed a static sentence-selection method for domain adaptation using the internal sentence embedding of NMT. They also proposed a sentence weighting method with dynamic weight adjustment (Wang et al., 2017b). Wees et al. (2017) used domain-based cross-entropy as a criterion to gradually fine-tune the NMT training in a dynamical manner. All of these criteria (Wang et al., 2017a,b; Wees et al., 2017) are calculated before performing the NMT training based on the domain information and"
P18-2048,W04-3250,0,0.397163,"Missing"
P18-2048,D17-1155,1,0.83141,"2016; Kalchbrenner et al., 2016; Gehring et al., 2017; Vaswani et al., 2017). Although these technologies have been adopted, deep networks have to be improved to achieve state-of-the-art performance in order to handle very large datasets and several training iterations. Therefore, some researchers have proposed to accelerate the NMT training by resampling a smaller subset of the data that makes a relatively high contribution, to improve the training efficiency of NMT. Specifically, Kocmi and Bojar (2017) empirically investigated curriculum learning based on the sentence length and word rank. Wang et al. (2017a) proposed a static sentence-selection method for domain adaptation using the internal sentence embedding of NMT. They also proposed a sentence weighting method with dynamic weight adjustment (Wang et al., 2017b). Wees et al. (2017) used domain-based cross-entropy as a criterion to gradually fine-tune the NMT training in a dynamical manner. All of these criteria (Wang et al., 2017a,b; Wees et al., 2017) are calculated before performing the NMT training based on the domain information and are fixed while performing the complete procedure. Zhang et al. (2017) adopted the sentence-level training"
P18-2048,P07-2045,0,0.0140916,"Missing"
P18-2048,I17-2046,0,0.302474,"on the sentence length and word rank. Wang et al. (2017a) proposed a static sentence-selection method for domain adaptation using the internal sentence embedding of NMT. They also proposed a sentence weighting method with dynamic weight adjustment (Wang et al., 2017b). Wees et al. (2017) used domain-based cross-entropy as a criterion to gradually fine-tune the NMT training in a dynamical manner. All of these criteria (Wang et al., 2017a,b; Wees et al., 2017) are calculated before performing the NMT training based on the domain information and are fixed while performing the complete procedure. Zhang et al. (2017) adopted the sentence-level training cost as a dynamic criterion to gradually fine-tune the NMT training. This approach was developed based on the idea that the training cost is a useful measure to determine the translation quality of a sentence. However, some of the sentences that can be potentially improved by training may be deleted using this method. In addition, all of the above works primarily focused on NMT translation performance, instead of training efficiency. In this study, we propose a method of dynamic sentence sampling (DSS) to improve the NMT training efficiency. First, the diff"
P18-2048,W17-4717,0,\N,Missing
P18-2078,W11-3502,0,0.0761458,"Missing"
P18-2078,P00-1031,0,0.124337,"anguage-specific optimization in a future work, via both data- and user-driven studies. The simplification scheme is shown in Fig. 3.1 Generally, the merges are based on the common distribution of consonant phonemes in most natural languages, as well as the etymology of the characters in each abugida. Specifically, three or four Related Work Some optimized keyboard layout have been proposed for specific abugidas (Ouk et al., 2008). Most studies on input methods have focused on Chinese and Japanese characters, where thousands of symbols need to be encoded and recovered. For Chinese characters, Chen and Lee (2000) made an early attempt to apply statistical methods to sentence-level processing, using a hidden Markov model. Others have examined max-entropy models, support vector machines (SVMs), conditional 1 Each script also includes native punctuation marks, digit notes, and standalone vowel characters that are not represented by diacritics. These characters were kept in the experimental texts but not evaluated in the final results, as the usage of these characters is trivial. In addition, white spaces, Latin letters, Arabic digits, and non-native punctuation marks were normalized into placeholders in"
P19-1119,D16-1250,0,0.0277561,"ods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internship research fellow at NICT when conducting this work. Based on these two findi"
P19-1119,P17-1042,0,0.0444313,"The supervised BWE (Mikolov et al., 2013), which exploits similarities between the source language and the target language through a linear transformation matrix, serves as the basis for many NLP tasks, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018b; Zhang and Zhao, 2019), dependency parsing (Zhang et al., 2016; Li et al., 2018), semantic role labeling (He et al., 2018; Li et al., 2019). However, the lack of a large wordpair dictionary poses a major practical problem for many language pairs. UBWE has attracted considerable attention. For example, Artetxe et al. (2017) proposed a self-learning framework to learn BWE with a 25-word dictionary, and Artetxe et al. (2018a) extended previous work without any word dictionary via fully unsupervised initialization. Zhang et al. (2017) and Conneau et al. (2018) proposed UBWE methods via generative adversarial network training. Recently, several UBWE methods (Conneau et al., 2018; Artetxe et al., 2018a) have been applied to UNMT (Artetxe et al., 2018c; Lample et al., 2018a). These rely solely on monolingual corpora in each language via UBWE initialization, denoising auto-encoder, and back-translation. A shared encode"
P19-1119,P18-1073,0,0.117216,"BWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internship research fellow at NICT when conducting this work. Based on these two findings, we hypothesize that the learning of UNMT with UBWE agreement would enhance UNMT performance. In detail, we propose two approaches, UBWE agreement regularization and UBWE adversarial training, to maintain the quality of UBWE during NMT"
P19-1119,D18-1399,0,0.0466684,"Missing"
P19-1119,P19-1019,0,0.0982651,"Missing"
P19-1119,J82-2005,0,0.756838,"Missing"
P19-1119,Q17-1010,0,0.039396,"UNMT Baseline + UBWE agreement regularization + UBWE adversarial training De-En n/a 13.33 14.62 21.0 21.23 22.38++ 22.67++ En-De n/a 9.64 10.86 17.2 17.06 18.04++ 18.29++ Fr-En 15.56 14.31 15.58 24.2 24.50 25.21++ 25.87++ En-Fr 15.13 15.05 16.97 25.1 25.37 27.86++ 28.38++ Ja-En n/a n/a n/a n/a 14.09 16.36++ 17.22++ En-Ja n/a n/a n/a n/a 21.63 23.01++ 23.64++ Table 2: Performance (BLEU score) of UNMT. “++” after a score indicates that the proposed method was significantly better than the UNMT baseline at significance level p <0.01. the embeddings for each language independently with fastText3 (Bojanowski et al., 2017) (default settings). The word embeddings were normalized by length and mean centered before bilingual projection. We then used VecMap4 (Artetxe et al., 2018a) (default settings) to project two monolingual word embeddings into one space. To evaluate the quality of UBWE, we selected the accuracy of word translation using the top-1 predicted candidate in the MUSE test set as the criterion. 5.3 UNMT Settings In the training process for UNMT, we used the transformer-based UNMT toolkit5 and the settings of Lample et al. (2018b). That is, we used four 3 https://github.com/facebookresearch/ fastText 4"
P19-1119,C16-1171,1,0.803699,"orm conventional UNMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internship research fellow at NICT when conducting this work. Based on these two findings, we hypothesize that the learning of UNMT with UBWE agreement would enhance UNMT performance. In detail, we propose two approache"
P19-1119,D17-1304,1,0.782183,"Missing"
P19-1119,C18-1271,0,0.0472765,"seline UBWE agreement regularization UBWE adversarial training Parameters 120,141K 120,141K 120,764K Speed 3784 3741 3733 Table 4: Analysis on parameters and training speed (number of processed words per second on one P100). 7 Related Work The supervised BWE (Mikolov et al., 2013), which exploits similarities between the source language and the target language through a linear transformation matrix, serves as the basis for many NLP tasks, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018b; Zhang and Zhao, 2019), dependency parsing (Zhang et al., 2016; Li et al., 2018), semantic role labeling (He et al., 2018; Li et al., 2019). However, the lack of a large wordpair dictionary poses a major practical problem for many language pairs. UBWE has attracted considerable attention. For example, Artetxe et al. (2017) proposed a self-learning framework to learn BWE with a 25-word dictionary, and Artetxe et al. (2018a) extended previous work without any word dictionary via fully unsupervised initialization. Zhang et al. (2017) and Conneau et al. (2018) proposed UBWE methods via generative adversarial network training. Recently, several UBWE methods (Conneau et al., 20"
P19-1119,E14-1049,0,0.0395704,"erformance of UNMT is significantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng S"
P19-1119,P18-1192,0,0.062598,"adversarial training Parameters 120,141K 120,141K 120,764K Speed 3784 3741 3733 Table 4: Analysis on parameters and training speed (number of processed words per second on one P100). 7 Related Work The supervised BWE (Mikolov et al., 2013), which exploits similarities between the source language and the target language through a linear transformation matrix, serves as the basis for many NLP tasks, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018b; Zhang and Zhao, 2019), dependency parsing (Zhang et al., 2016; Li et al., 2018), semantic role labeling (He et al., 2018; Li et al., 2019). However, the lack of a large wordpair dictionary poses a major practical problem for many language pairs. UBWE has attracted considerable attention. For example, Artetxe et al. (2017) proposed a self-learning framework to learn BWE with a 25-word dictionary, and Artetxe et al. (2018a) extended previous work without any word dictionary via fully unsupervised initialization. Zhang et al. (2017) and Conneau et al. (2018) proposed UBWE methods via generative adversarial network training. Recently, several UBWE methods (Conneau et al., 2018; Artetxe et al., 2018a) have been appl"
P19-1119,N16-1162,0,0.097674,"Missing"
P19-1119,N15-1028,0,0.0983804,"Missing"
P19-1119,W18-6419,1,0.821417,"ur method. In addition, an alternative unsupervised method based on statistical machine translation (SMT) was proposed (Lample et al., 2018b; Artetxe et al., 2018b). The unsupervised machine translation performance was improved through combining UNMT and unsupervised SMT (Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). More recently, Lample and Conneau (2019) achieved 1242 better UNMT performance through introducing the pretrained language model. Neural network based language model has been shown helpful in supervised machine translation (Wang et al., 2014; Wang et al., 2018; Marie et al., 2018). We think that the proposed agreement mechanism can work with the pretrained language model. 8 Conclusion UBWE is a fundamental component of UNMT. In previous methods, the pre-trained UBWE is only used to initialize the word embedding of UNMT. In this study, we found that the performance of UNMT is significantly affected by the quality of UBWE, not only in the initialization stage, but also during UNMT training. Based on this finding, we proposed two joint learning methods to train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods can mitigat"
P19-1119,P16-1009,0,0.124354,"et al., 2016). The denoising auto-encoder, which encodes a noisy version and reconstructs it with the decoder in the same language, is optimized by minimizing the Lauto = EX∼φL1 [−logPL1 →L1 (X|C(X)] + EY ∼φL2 [−logPL2 →L2 (Y |C(Y )], (2) where C(X) and C(Y ) are noisy versions of sentences X and Y , PL1 →L1 (PL2 →L2 ) denotes the reconstruction probability in the language L1 (L2 ). 2.3 Back-translation The denoising auto-encoder acts as a language model that has been trained in one language and does not consider the final goal of translating between two languages. Therefore, backtranslation (Sennrich et al., 2016) was adapted to train translation systems in a true translation setting based on monolingual corpora. Formally, given the sentences X and Y , the sentences YP (X) and XP (Y ) would be produced by the model at the previous iteration. The pseudo-parallel sentence pair (YP (X), X) and (XP (Y ), Y ) would be obtained to train the new translation model. Finally, the back-translation process is optimized by minimizing the following objective function: Lbt = EX∼φL1 [−logPL2 →L1 (X|YP (X)] + EY ∼φL2 [−logPL1 →L2 (Y |XP (Y )], (3) where PL1 →L2 (PL2 →L1 ) denotes the translation probability across two"
P19-1119,P18-1072,0,0.043009,"Missing"
P19-1119,P17-1179,0,0.327433,"NMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internship research fellow at NICT when conducting this work. Based on these two findings, we hypothesize that the learning of UNMT with UBWE agreement would enhance UNMT performance. In detail, we propose two approaches, UBWE agreement reg"
P19-1119,P16-1131,0,0.0253662,"eed of the model. Baseline UBWE agreement regularization UBWE adversarial training Parameters 120,141K 120,141K 120,764K Speed 3784 3741 3733 Table 4: Analysis on parameters and training speed (number of processed words per second on one P100). 7 Related Work The supervised BWE (Mikolov et al., 2013), which exploits similarities between the source language and the target language through a linear transformation matrix, serves as the basis for many NLP tasks, such as machine translation (Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018b; Zhang and Zhao, 2019), dependency parsing (Zhang et al., 2016; Li et al., 2018), semantic role labeling (He et al., 2018; Li et al., 2019). However, the lack of a large wordpair dictionary poses a major practical problem for many language pairs. UBWE has attracted considerable attention. For example, Artetxe et al. (2017) proposed a self-learning framework to learn BWE with a 25-word dictionary, and Artetxe et al. (2018a) extended previous work without any word dictionary via fully unsupervised initialization. Zhang et al. (2017) and Conneau et al. (2018) proposed UBWE methods via generative adversarial network training. Recently, several UBWE methods ("
P19-1119,C18-1269,0,0.0524389,"Missing"
P19-1119,D14-1023,1,0.81634,"s initialization process for UBWE in our method. In addition, an alternative unsupervised method based on statistical machine translation (SMT) was proposed (Lample et al., 2018b; Artetxe et al., 2018b). The unsupervised machine translation performance was improved through combining UNMT and unsupervised SMT (Marie and Fujita, 2018; Ren et al., 2019; Artetxe et al., 2019). More recently, Lample and Conneau (2019) achieved 1242 better UNMT performance through introducing the pretrained language model. Neural network based language model has been shown helpful in supervised machine translation (Wang et al., 2014; Wang et al., 2018; Marie et al., 2018). We think that the proposed agreement mechanism can work with the pretrained language model. 8 Conclusion UBWE is a fundamental component of UNMT. In previous methods, the pre-trained UBWE is only used to initialize the word embedding of UNMT. In this study, we found that the performance of UNMT is significantly affected by the quality of UBWE, not only in the initialization stage, but also during UNMT training. Based on this finding, we proposed two joint learning methods to train UNMT with UBWE agreement. Empirical results on several language pairs sh"
P19-1119,N15-1104,0,0.0375279,"gnificantly affected by the performance of UBWE. Thus, we propose two methods that train UNMT with UBWE agreement. Empirical results on several language pairs show that the proposed methods significantly outperform conventional UNMT. 1 • 1) There is a positive correlation between the quality of the pre-trained UBWE and the performance of UNMT. • 2) The UBWE quality significantly decreases during UNMT training. Introduction Since 2013, neural network based bilingual word embedding (BWE) has been applied to several natural language processing tasks (Mikolov et al., 2013; Faruqui and Dyer, 2014; Xing et al., 2015; Dinu et al., 2015; Lu et al., 2015; Wang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Wang et al., 2018). Recently, researchers have found that supervision is not always necessary (Cao et al., 2016; Zhang et al., 2017). Several unsupervised BWE (UBWE) methods (Conneau et al., 2018; Artetxe et al., 2018a) have been proposed and these have achieved impressive performance in wordtranslation tasks. The success of UBWE makes unsupervised neural machine translation (UNMT) possible. The combination of UBWE with denoising autoencoder and back-translation has ∗ Haipeng Sun was an internshi"
P19-1119,P18-1005,0,0.321075,"racy Base-Fr enc-En dec AR-Fr-En AR-Fr enc-En dec AT-Fr-En UBWEAT-Fr accuracy enc-En dec AR-Fr-En BLEU 20 40 60 80 100 120 140 Epoch Base-Ja-En UBWE accuracy UBWE accuracy Base-Ja enc-En dec AR-Ja-En AR-Ja enc-En dec Base-Fr-En BLEU AT-Ja-En UBWE AT-Ja accuracy enc-En dec AT-Fr-En BLEU AR-Ja-En BLEU (a) Fr-En Base-Ja-En BLEU AT-Ja-En BLEU (b) Ja-En Figure 4: The trends of UBWE quality and BLEU score for baseline (Base), UBWE agreement regularization (AR), and UBWE adversarial training (AT) during UNMT training on the Fr-En and Ja-En dataset Methods Artetxe et al. (2018c) Lample et al. (2018a) Yang et al. (2018) Lample et al. (2018b) UNMT Baseline + UBWE agreement regularization + UBWE adversarial training De-En n/a 13.33 14.62 21.0 21.23 22.38++ 22.67++ En-De n/a 9.64 10.86 17.2 17.06 18.04++ 18.29++ Fr-En 15.56 14.31 15.58 24.2 24.50 25.21++ 25.87++ En-Fr 15.13 15.05 16.97 25.1 25.37 27.86++ 28.38++ Ja-En n/a n/a n/a n/a 14.09 16.36++ 17.22++ En-Ja n/a n/a n/a n/a 21.63 23.01++ 23.64++ Table 2: Performance (BLEU score) of UNMT. “++” after a score indicates that the proposed method was significantly better than the UNMT baseline at significance level p <0.01. the embeddings for each language indepen"
P19-1174,J93-2003,0,0.124809,"lel bilingual sentence pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attention inside the fixed-size window, and reported a considerable improvement on the classical RNN-based NMT. This means that the word reordering information is also beneficial to the NMT. The Transformer (Vaswani et al., 2017) translation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transfor"
P19-1174,P17-1177,0,0.0856638,"utively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address this issue by directly modeling the reordered sequence of input words, as opposed to the reordering operations that generated it. Operation sequence models are n-gram models that include lexical translation operations and reordering operations in a single generative story, thereby combining elements from the previous three model families (Durrani et al., 2011, 2013, 2014). Their method were further extended by source syntax information (Chen et al., 2017c, 2018b) to improve the performance of SMT. Moreover, to address data sparsity (Guta et al., 2015) caused by a mass of reordering rules, Li et al. (2013, 2014) modeled ITG-based reordering rules in the translation by using neural networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural netwo"
P19-1174,D17-1304,1,0.836837,"Missing"
P19-1174,P05-1066,0,0.763417,"are sentence representation for machine translation. The proposed translation models outperform the state-of-the-art NMT baselines systems with a similar number of parameters and achieve comparable results compared to NMT systems with much more parameters. 2 2.1 Related Work Reordering Model for PBSMT In PBSMT, there has been a substantial amount of research works about reordering model, which was used as a key component to ensure the generation of fluent target translation. Bisazza and Federico (2016) divided these reordering models into four groups: Phrase orientation models (Tillman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address t"
P19-1174,P13-2071,0,0.149567,"Missing"
P19-1174,C14-1041,0,0.0608164,"Missing"
P19-1174,P11-1105,0,0.0991813,"Missing"
P19-1174,P16-1078,0,0.0203508,"g RNN-based NMT for improving the performance of translations. Du and Way (2017) 1788 and Kawara et al. (2018) reported that the prereordering method had an negative impact on the NMT for the ASPEC JA-EN translation task. In particular, Kawara et al. (2018) assumed that one reason is the isolation between pre-ordering and NMT models, where both models are trained using independent optimization functions. In addition, several research works have been proposed to explicitly introduce syntax structure into the RNN-based NMT for encoding syntax ordering dependencies into sentence representations (Eriguchi et al., 2016; Li et al., 2017; Chen et al., 2017a,b; Wang et al., 2017b; Chen et al., 2018a). Recently, the neural Transformer translation system (Vaswani et al., 2017), which relies solely on self-attention networks, used a fixed order sequence of positional embeddings to encode order dependencies between words in a sentence. 3 3.1 Background Positional Encoding Mechanism Transformer (Vaswani et al., 2017) typically uses a positional encoding mechanism to encode order dependencies between words in a sentence. Formally, given a embedding sequence of source sentence of length J, X={x1 , · · · , xJ }, the p"
P19-1174,2010.amta-papers.22,0,0.0465578,"illman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address this issue by directly modeling the reordered sequence of input words, as opposed to the reordering operations that generated it. Operation sequence models are n-gram models that include lexical translation operations and reordering operations in a single generative story, thereby combining elements from the previous three model families (Durrani et al., 2011, 2013, 2014). Their method were further extended by source syntax information (Chen et al., 2017c, 2018b) to improve the performance of SMT. Moreover, to address data sparsity (Guta et al., 2015) caused by a mass of reorde"
P19-1174,N13-1003,0,0.0494421,"Missing"
P19-1174,P13-1032,0,0.0308283,"Missing"
P19-1174,P05-1033,0,0.216411,"The reordering model plays an important role in phrase-based statistical machine translation (PBSMT), especially for translation between distant language pairs with large differences in word order, such as Chinese-to-English and Japaneseto-English translations (Galley and Manning, 2008; Goto et al., 2013). Typically, the traditional PBSMT learns large-scale reordering rules from parallel bilingual sentence pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional p"
P19-1174,D08-1089,0,0.301049,"into both the encoder and the decoder in the Transformer translation system. Experimental results on WMT’14 English-toGerman, NIST Chinese-to-English, and WAT ASPEC Japanese-to-English translation tasks demonstrate that the proposed methods can significantly improve the performance of the Transformer translation system. 1 Introduction The reordering model plays an important role in phrase-based statistical machine translation (PBSMT), especially for translation between distant language pairs with large differences in word order, such as Chinese-to-English and Japaneseto-English translations (Galley and Manning, 2008; Goto et al., 2013). Typically, the traditional PBSMT learns large-scale reordering rules from parallel bilingual sentence pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words"
P19-1174,P17-1012,0,0.0293336,"ration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attention inside the fixed-size window, and reported a considerable improvement on the classical RNN-based NMT. This means that the word reordering information is also beneficial to the NMT. The Transformer (Vaswani et al., 2017) translation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transformer generates an ordered sequence of positional embeddings by a positional encoding mechanism (Gehring et al., 2017a) to explicitly encode the order of dependencies between words in a sentence. The Transformer is adept at parallelizing of performing (multi-head) and stacking (multi-layer) SANs to learn the sentence representation to predict translation, and has delivered state-of-the-art performance on various translation tasks (Bojar et al., 2018; Marie et al., 2018). However, these positional embeddings focus on sequentially encoding order relations between words, and does not explicitly consider reordering information in a sentence, which may degrade the performance of Transformer translation systems. T"
P19-1174,N10-1129,0,0.03344,"Missing"
P19-1174,D15-1165,0,0.0408643,"Missing"
P19-1174,P18-1192,0,0.0268229,"reordering embedding was learned by considering the relationship between the positional embedding of a word and that of the entire sentence. The proposed reordering embedding can be easily introduced to the existing Transformer translation system to predict translations. Experiments showed that our method can significantly improve the performance of Transformer. In future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (Zhang et al., 2016; Li et al., 2018), and semantic role labeling (He et al., 2018; Li et al., 2019). We are grateful to the anonymous reviewers and the area chair for their insightful comments and suggestions. This work was partially conducted under the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of the Ministry of Internal Affairs and Communications (MIC), Japan. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenure-track researcher startup fund “Toward"
P19-1174,P17-4012,0,0.10494,"Missing"
P19-1174,N03-1017,0,0.379592,"ce pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attention inside the fixed-size window, and reported a considerable improvement on the classical RNN-based NMT. This means that the word reordering information is also beneficial to the NMT. The Transformer (Vaswani et al., 2017) translation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transformer generates an ord"
P19-1174,P17-1064,0,0.0207119,"roving the performance of translations. Du and Way (2017) 1788 and Kawara et al. (2018) reported that the prereordering method had an negative impact on the NMT for the ASPEC JA-EN translation task. In particular, Kawara et al. (2018) assumed that one reason is the isolation between pre-ordering and NMT models, where both models are trained using independent optimization functions. In addition, several research works have been proposed to explicitly introduce syntax structure into the RNN-based NMT for encoding syntax ordering dependencies into sentence representations (Eriguchi et al., 2016; Li et al., 2017; Chen et al., 2017a,b; Wang et al., 2017b; Chen et al., 2018a). Recently, the neural Transformer translation system (Vaswani et al., 2017), which relies solely on self-attention networks, used a fixed order sequence of positional embeddings to encode order dependencies between words in a sentence. 3 3.1 Background Positional Encoding Mechanism Transformer (Vaswani et al., 2017) typically uses a positional encoding mechanism to encode order dependencies between words in a sentence. Formally, given a embedding sequence of source sentence of length J, X={x1 , · · · , xJ }, the positional embeddi"
P19-1174,D13-1054,0,0.0422787,"Missing"
P19-1174,C14-1179,0,0.0358165,"Missing"
P19-1174,C18-1271,0,0.0358463,"echanism to capture knowledge of reordering. A reordering embedding was learned by considering the relationship between the positional embedding of a word and that of the entire sentence. The proposed reordering embedding can be easily introduced to the existing Transformer translation system to predict translations. Experiments showed that our method can significantly improve the performance of Transformer. In future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (Zhang et al., 2016; Li et al., 2018), and semantic role labeling (He et al., 2018; Li et al., 2019). We are grateful to the anonymous reviewers and the area chair for their insightful comments and suggestions. This work was partially conducted under the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of the Ministry of Internal Affairs and Communications (MIC), Japan. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT"
P19-1174,P18-3004,0,0.15555,"Missing"
P19-1174,W18-6419,1,0.792532,"anslation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transformer generates an ordered sequence of positional embeddings by a positional encoding mechanism (Gehring et al., 2017a) to explicitly encode the order of dependencies between words in a sentence. The Transformer is adept at parallelizing of performing (multi-head) and stacking (multi-layer) SANs to learn the sentence representation to predict translation, and has delivered state-of-the-art performance on various translation tasks (Bojar et al., 2018; Marie et al., 2018). However, these positional embeddings focus on sequentially encoding order relations between words, and does not explicitly consider reordering information in a sentence, which may degrade the performance of Transformer translation systems. Thus, the reordering problem in NMT has not been studied extensively, especially in Transformer. In this paper, we propose a reordering mechanism for the Transformer translation system. We dynamically penalize the given positional embedding of a word depending on its contextual information, thus generating a reordering embedding for each word. The reorderi"
P19-1174,D16-1096,0,0.0297414,"Missing"
P19-1174,P06-1090,0,0.185574,"ation for machine translation. The proposed translation models outperform the state-of-the-art NMT baselines systems with a similar number of parameters and achieve comparable results compared to NMT systems with much more parameters. 2 2.1 Related Work Reordering Model for PBSMT In PBSMT, there has been a substantial amount of research works about reordering model, which was used as a key component to ensure the generation of fluent target translation. Bisazza and Federico (2016) divided these reordering models into four groups: Phrase orientation models (Tillman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address this issue by directly"
P19-1174,W18-6319,0,0.0149356,"ng was set to 0.1, and the attention dropout and residual dropout were p = 0.1. The Adam optimizer (Kingma and Ba, 2014) was used to tune the parameters of the model. The learning rate was varied under a warm-up strategy with warmup steps of 8,000. For evaluation, we validated the model with an interval of 1,000 batches on the dev set. Following the training of 200,000 batches, the model with the highest BLEU score of the dev set was selected to evaluate on the test sets. During the decoding, the beam size was set to four. All models were trained and evaluated on a single P100 GPU. SacreBELU (Post, 2018) was used as the evaluation metric of EN-DE, and the multi-bleu.perl1 was used the evaluation metric of ZH-EN and JA-EN tasks. The signtest (Collins et al., 2005) was as statistical significance test. We re-implemented all methods (“this work” in the tables) on the OpenNMT toolkit (Klein et al., 1 https://github.com/mosessmt/mosesdecoder/tree/RELEASE-4.0/scripts/generic/multibleu.perl 2017). 6.4 Main Results To validate the effectiveness of our methods, the proposed models were first evaluated on the WMT14 EN-DE translation task as in the original Transformer translation system (Vaswani et al."
P19-1174,P16-1162,0,0.0678125,"ative positional embeddings into the selfattention mechanism of Transformer. Additional PE (control experiment): uses original absolute positional embeddings to enhance the position information of each SAN layer instead of the proposed reordering embeddings. Pre-reordering: a pre-ordering method (Goto et al., 2013) for JA-EN translation task was used to adjust the order of Japanese words in both the training, dev, and test datasets, and thus reordered each source sentence into the similar order as its target sentence. 6.3 System Setting For all models (base), the byte pair encoding algorithm (Sennrich et al., 2016) was adopted and the size of the vocabulary was set to 32,000. The number of dimensions of all input and output 1791 System Wu et al. (2016) Gehring et al. (2017b) Vaswani et al. (2017) Vaswani et al. (2017) this work Architecture newstest2014 Existing NMT systems GNMT 26.3 CONVS2S 26.36 Transformer (base) 27.3 Transformer (big) 28.4 Our NMT systems Transformer (base) 27.24 +Additional PEs 27.10 +Relative PEs 27.63 +Encoder REs 28.03++ +Decoder REs 27.61+ +Both REs 28.22++ Transformer (big) 28.34 +Both REs 29.11++ #Speed1 #Speed2 #Params N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 65.0M 213.0M 991"
P19-1174,N18-2074,0,0.0292439,"T07, LDC2004T08, and LDC2005T06. The MT06 and the MT02/MT03/MT04/MT05/MT08 datasets were used as the dev set and test set, respectively. 3) For the JA-EN translation task, the training dataset consisted of two million bilingual sentence pairs from the ASPEC corpus (Nakazawa et al., 2016). The dev set consisted of 1,790 sentence pairs and the test set of 1,812 sentence pairs. 6.2 Baseline Systems These baseline systems included: Transformer: a vanilla Transformer with absolute positional embedding (Vaswani et al., 2017), for example Transformer (base) and Transformer (big) models. Relative PE (Shaw et al., 2018): incorporates relative positional embeddings into the selfattention mechanism of Transformer. Additional PE (control experiment): uses original absolute positional embeddings to enhance the position information of each SAN layer instead of the proposed reordering embeddings. Pre-reordering: a pre-ordering method (Goto et al., 2013) for JA-EN translation task was used to adjust the order of Japanese words in both the training, dev, and test datasets, and thus reordered each source sentence into the similar order as its target sentence. 6.3 System Setting For all models (base), the byte pair en"
P19-1174,N04-4026,0,0.113282,"n reordering-aware sentence representation for machine translation. The proposed translation models outperform the state-of-the-art NMT baselines systems with a similar number of parameters and achieve comparable results compared to NMT systems with much more parameters. 2 2.1 Related Work Reordering Model for PBSMT In PBSMT, there has been a substantial amount of research works about reordering model, which was used as a key component to ensure the generation of fluent target translation. Bisazza and Federico (2016) divided these reordering models into four groups: Phrase orientation models (Tillman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al.,"
P19-1174,P16-1008,0,0.0276056,"networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural networks themselves to implicitly capture order dependencies between words (Sutskever et al., 2014; Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Zhang et al., 2018). Coverage model can partially model the word order information (Tu et al., 2016; Mi et al., 2016). Inspired by a distortion method (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpful for the SMT in improving translation quality. Moreover, neural networks were used to pre-reorder the sources"
P19-1174,P17-2089,1,0.579914,"Missing"
P19-1174,D17-1155,1,0.862726,"f reordering rules, Li et al. (2013, 2014) modeled ITG-based reordering rules in the translation by using neural networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural networks themselves to implicitly capture order dependencies between words (Sutskever et al., 2014; Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Zhang et al., 2018). Coverage model can partially model the word order information (Tu et al., 2016; Mi et al., 2016). Inspired by a distortion method (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpfu"
P19-1174,P18-2048,1,0.899817,"Missing"
P19-1174,P96-1021,0,0.289443,"eordering operations in a single generative story, thereby combining elements from the previous three model families (Durrani et al., 2011, 2013, 2014). Their method were further extended by source syntax information (Chen et al., 2017c, 2018b) to improve the performance of SMT. Moreover, to address data sparsity (Guta et al., 2015) caused by a mass of reordering rules, Li et al. (2013, 2014) modeled ITG-based reordering rules in the translation by using neural networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural networks themselves to implicitly capture order dependencies between words (Sutskever et al., 2014; Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Zhang et al., 2018). Coverage model can partially model the word order information (Tu et al., 2016; Mi et al., 2016). Inspired by a distortion method (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) orig"
P19-1174,J97-3002,0,0.637414,"Missing"
P19-1174,P06-1066,0,0.0442704,"model plays an important role in phrase-based statistical machine translation (PBSMT), especially for translation between distant language pairs with large differences in word order, such as Chinese-to-English and Japaneseto-English translations (Galley and Manning, 2008; Goto et al., 2013). Typically, the traditional PBSMT learns large-scale reordering rules from parallel bilingual sentence pairs in advance to form a reordering model. This reordering model is then integrated into the translation decoding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attent"
P19-1174,W06-3108,0,0.0419669,"nslation. The proposed translation models outperform the state-of-the-art NMT baselines systems with a similar number of parameters and achieve comparable results compared to NMT systems with much more parameters. 2 2.1 Related Work Reordering Model for PBSMT In PBSMT, there has been a substantial amount of research works about reordering model, which was used as a key component to ensure the generation of fluent target translation. Bisazza and Federico (2016) divided these reordering models into four groups: Phrase orientation models (Tillman, 2004; Collins et al., 2005; Nagata et al., 2006; Zens and Ney, 2006; Galley and Manning, 2008; Cherry, 2013), simply known as lexicalized reordering models, predict whether the next translated source span should be placed on the right (monotone), the left (swap), or anywhere else (discontinuous) of the last translated one. Jump models (Al-Onaizan and Papineni, 2006; Green et al., 2010) predict the direction and length of the jump that is performed between consecutively translated words or phrases, with the goal of better handling long-range reordering. Source decoding sequence models (Feng et al., 2010, 2013) address this issue by directly modeling the reorde"
P19-1174,P17-1140,0,0.233489,"ding process to ensure a reasonable order of translations of the source words (Chiang, 2005; Xiong et al., 2006; Galley and Manning, 2008). In contrast to the explicit reordering model for PBSMT, the RNN-based NMT (Sutskever et al., 2014; Bahdanau et al., 2015) depends on neural networks to implicitly encode order dependencies ∗ Corresponding author between words in a sentence to generate a fluent translation. Inspired by a distortion method originating in SMT (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006), there is a quite recent preliminary exploration work for NMT (Zhang et al., 2017). They distorted the existing content-based attention by an additional position-based attention inside the fixed-size window, and reported a considerable improvement on the classical RNN-based NMT. This means that the word reordering information is also beneficial to the NMT. The Transformer (Vaswani et al., 2017) translation system relies on self-attention networks (SANs), and has attracted growing interesting in the machine translation community. The Transformer generates an ordered sequence of positional embeddings by a positional encoding mechanism (Gehring et al., 2017a) to explicitly enc"
P19-1174,D18-1511,1,0.794361,". (2013, 2014) modeled ITG-based reordering rules in the translation by using neural networks. In particular, the NN-based reordering models can not only capture semantic similarity but also ITG reordering constraints (Wu, 1996, 1997) in the translation context. This neural network modeling method is further applied to capture reordering information and syntactic coherence. 2.2 Modeling Ordering for NMT The attention-based NMT focused on neural networks themselves to implicitly capture order dependencies between words (Sutskever et al., 2014; Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Zhang et al., 2018). Coverage model can partially model the word order information (Tu et al., 2016; Mi et al., 2016). Inspired by a distortion method (Brown et al., 1993; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpful for the SMT in improving tra"
P19-1174,P16-1131,0,0.0292113,"posed a reordering mechanism to capture knowledge of reordering. A reordering embedding was learned by considering the relationship between the positional embedding of a word and that of the entire sentence. The proposed reordering embedding can be easily introduced to the existing Transformer translation system to predict translations. Experiments showed that our method can significantly improve the performance of Transformer. In future work, we will further explore the effectiveness of the reordering mechanism and apply it to other natural language processing tasks, such dependency parsing (Zhang et al., 2016; Li et al., 2018), and semantic role labeling (He et al., 2018; Li et al., 2019). We are grateful to the anonymous reviewers and the area chair for their insightful comments and suggestions. This work was partially conducted under the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of the Ministry of Internal Affairs and Communications (MIC), Japan. Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Sc"
P19-1174,D18-1036,0,0.0254378,"93; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpful for the SMT in improving translation quality. Moreover, neural networks were used to pre-reorder the sourceside word orders close to those of the target side (Du and Way, 2017; Zhao et al., 2018b; Kawara et al., 2018), and thus were input to the existing RNN-based NMT for improving the performance of translations. Du and Way (2017) 1788 and Kawara et al. (2018) reported that the prereordering method had an negative impact on the NMT for the ASPEC JA-EN translation task. In particular, Kawara et al. (2018) assumed that one reason is the isolation between pre-ordering and NMT models, where both models are trained using independent optimization functions. In addition, several research works have been proposed to explicitly introduce syntax structure into the RNN-based NMT for encoding s"
P19-1174,L18-1143,0,0.0642441,"93; Koehn et al., 2003; Al-Onaizan and Papineni, 2006) originated from SMT, Zhang et al. (2017) proposed an additional position-based attention to enable the existing content-based attention to attend to the source words regarding both semantic requirement and the word reordering penalty. Pre-reordering, a pre-processing to make the source-side word orders close to those of the target side, has been proven very helpful for the SMT in improving translation quality. Moreover, neural networks were used to pre-reorder the sourceside word orders close to those of the target side (Du and Way, 2017; Zhao et al., 2018b; Kawara et al., 2018), and thus were input to the existing RNN-based NMT for improving the performance of translations. Du and Way (2017) 1788 and Kawara et al. (2018) reported that the prereordering method had an negative impact on the NMT for the ASPEC JA-EN translation task. In particular, Kawara et al. (2018) assumed that one reason is the isolation between pre-ordering and NMT models, where both models are trained using independent optimization functions. In addition, several research works have been proposed to explicitly introduce syntax structure into the RNN-based NMT for encoding s"
P19-1296,D17-1304,1,0.807793,"Missing"
P19-1296,I17-1002,1,0.860783,"Missing"
P19-1296,P18-1192,0,0.0281199,"mize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. 1 Introduction Neural network based methods have been applied to several natural language processing tasks (Zhang et al., 2016; Li et al., 2018; Chen et al., 2018; Li et al., 2019; He et al., 2018). In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. ∗ Mingming Yang was an internship research fellow at NICT when co"
P19-1296,P18-1164,0,0.250005,"conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. ∗ Mingming Yang was an internship research fellow at NICT when conducting this work. Based on this hypothesis, Kuang et al. (2018) proposed a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation. Tu et al. (2017) incorporated a reconstructor module into NMT, which reconstructs the input source sentence from the hidden layer of the output target sentence to enhance source representation. However, in previous studies, the training objective function was usually based on word-level and lacked explicit sentencelevel relationships (Zhang and Zhao, 2019). Although Transformer model (Vaswani et al., 2017) has archived state-of-the-art performance of NMT,"
P19-1296,C18-1271,0,0.0150768,"ose a sentencelevel agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. 1 Introduction Neural network based methods have been applied to several natural language processing tasks (Zhang et al., 2016; Li et al., 2018; Chen et al., 2018; Li et al., 2019; He et al., 2018). In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target words. ∗ Mingming"
P19-1296,D15-1166,0,0.122472,"Missing"
P19-1296,P02-1040,0,0.103188,"Missing"
P19-1296,W18-6319,0,0.0337367,"Missing"
P19-1296,W16-0533,0,0.0307462,"Although Transformer model (Vaswani et al., 2017) has archived state-of-the-art performance of NMT, more attention is paid to the words-level relationship via self-attention networks. Sentence-level agreement method has been applied to many natural language processing tasks. Aliguliyev (2009) used sentence similarity measure technique for automatic text summarization. Liang et al. (2010) have shown that the sentence similarity algorithm based on VSM is beneficial to address the FAQ problem. Su et al. (2016) presented a sentence similarity method for spoken dialogue system to improve accuracy. Rei and Cummins (2016) proposed sentence similarity measures to improve the estimation of topical relevance. Wang et al. (2017b; 2018) used sentence similarity to select sentences with the similar domains. The above methods only considered monolingual sentence-level agreement. In human translation, a translator’s primary concern is to translate a sentence through its entire meaning rather than word-by-word meaning. Therefore, in early machine translation studies, such as example-based machine translation (Nagao, 1984; Nio et al., 2013), use the sentence similarity matching between the sentences to be translated and"
P19-1296,W16-2323,0,0.0605544,"Missing"
P19-1296,P16-1162,0,0.158734,"Missing"
P19-1296,P16-1131,0,0.017386,"this paper, we propose a sentencelevel agreement module to directly minimize the difference between the representation of source and target sentence. The proposed agreement module can be integrated into NMT as an additional training objective function and can also be used to enhance the representation of the source sentences. Empirical results on the NIST Chinese-to-English and WMT English-to-German tasks show the proposed agreement module can significantly improve the NMT performance. 1 Introduction Neural network based methods have been applied to several natural language processing tasks (Zhang et al., 2016; Li et al., 2018; Chen et al., 2018; Li et al., 2019; He et al., 2018). In neural machine translation (NMT), unlike conventional phrase-based statistical machine translation, an attention mechanism is adopted to help align output with input words (Bahdanau et al., 2015). It is based on the estimation of a probability distribution over all input words for each target word. However, source and target words are in different representation space, and they still have to go through a long information processing procedure that may lead to the source words are incorrectly translated into the target w"
P19-1296,P16-1008,0,0.0212731,"nction in between: FFN(x) = max(0, xW1 + b1 )W2 + b2 , (2) where W1 and W2 are both linear transformation networks, b1 and b2 are both bias. We define Henc as the sentence representation of X via the self-attention layers in encoder, and Hdec as the sentence representation of words Y via embedding layers in decoder. The parameters of Transformer are trained to minimize the following objective function on a set of training examples {(X n , Y n )}N n=1 : Lmle = − 3 N Iy 1 XX n logP (yin |y<i , Henc , Hdec ). N n=1 i=1 (3) Agreement on Source and Target Sentence Some studies (Luong et al., 2015; Tu et al., 2016; Chen et al., 2017a,b; Kuang et al., 2018) showed that improving word alignment is beneficial to machine translation. Their idea is based on word-level agreement and make the embeddings of source words and corresponding target words similar. In this paper, we investigate the sentence-level relationship between the source and target sentences. We propose a sentence-level agreement method which can make the sentencelevel semantics of the source and target closer. The entire architecture of the proposed method is illustrated in Figure 1. 3.1 Sentence-Level Agreement First, we need to get the sen"
P19-1296,P17-2089,1,0.919928,"Missing"
P19-1296,C18-1269,0,0.0400623,"Missing"
P19-1296,D17-1155,1,0.822564,"ntion is paid to the words-level relationship via self-attention networks. Sentence-level agreement method has been applied to many natural language processing tasks. Aliguliyev (2009) used sentence similarity measure technique for automatic text summarization. Liang et al. (2010) have shown that the sentence similarity algorithm based on VSM is beneficial to address the FAQ problem. Su et al. (2016) presented a sentence similarity method for spoken dialogue system to improve accuracy. Rei and Cummins (2016) proposed sentence similarity measures to improve the estimation of topical relevance. Wang et al. (2017b; 2018) used sentence similarity to select sentences with the similar domains. The above methods only considered monolingual sentence-level agreement. In human translation, a translator’s primary concern is to translate a sentence through its entire meaning rather than word-by-word meaning. Therefore, in early machine translation studies, such as example-based machine translation (Nagao, 1984; Nio et al., 2013), use the sentence similarity matching between the sentences to be translated and the sentences in the 3076 Proceedings of the 57th Annual Meeting of the Association for Computational L"
S01-1033,W00-0730,0,0.0142784,"kernel functions. We have used the following polynomial function exclusively. K(x,y) =(x·y+l)d (9) C and d are constants set by experimentation. For all of the experiments reported in this paper, C was fixed as 1 and d was fixed as 2. A set of Xi that satisfies O:i &gt; 0 is called a support vector (SVs) 4 . The summation portion of Equation (4) was calculated using only the examples that were support vectors. Support vector machine methods are capable of handling data consisting of two categories. In general, data consisting of more than two categories is handled by using the pair-wise method (Kudoh and Matsumoto, 2000). In this method, for data consisting of N categories, pairs of two different categories (N (N1)/2 pairs) are constructed. The better cate1rn Figure 1, the circles in the broken lines indicate support vectors. gory is determined by using a 2-category classifier (in this paper, a support vector machine 5 was used as the 2-category classifier), and the correct category is finally determined by ""voting"" on the N(N-1)/2 pairs that result from analysis using the 2-category classifier. The support vector machine method is, in fact, performed by combining the support vector machine and pair-wise meth"
W03-1607,J96-1002,0,0.00571759,"Missing"
W06-1653,C00-1027,0,0.0317883,"he multinomial model. By comparing Eqs. 1 and 10, we can see why the Polya model is superior to the multinomial model for modeling the occurrences of words (and users). In the multinomial model, if a word with probability p occurs twice, its probability becomes p2 . In the Polya model, the word’s probability becomes ω = 1. Clearly, p1.5 , for example, if we set αw 2 1.5 p &lt; p ; therefore, the Polya model assigns higher probability. In this example, the Polya model assigns probability p to the first occurrence and p0.5 (&gt; p) to the second. Since words that occur once are likely to occur again (Church, 2000), the Polya model is better suited to model the occurrences of words and users. See Yamamoto and Sadamitsu (2005) for further discussion on applying the Polya distribution to text modeling. Zaragoza et al.(2003) applied the Polya distribution to ad hoc IR. They introduced the exact Polya distribution (see Eq. 9) as an extension to the Dirichlet prior method (Zhai and Lafferty, 2001). However, we have introduced a multinomial approximation of the Polya distribution. This approximation allows us to use the linear interpolation method to mix the approximated Polya distributions. Thus, our model i"
W06-1653,J93-1003,0,0.0338378,"e recommender systems for wikis is important. In our Wikipedia dataset, each item (article) x consisted of wx and ux . ux was the sequence of users who had edited x. If users had edited x multiple times, then those users occurred in ux multiple times. wx was the sequence of words that were typical in x. To make wx , we removed stop words and stemmed the remaining words with a Porter stemmer. Next, we identified 100 typical words in each article and extracted only those words (|wx |≥ 100 because some of them occurred multiple times). Typicality was measured using the log-likelihood ratio test (Dunning, 1993). We needed to reduce the number of words to speed up our recommender system. To make our dataset, we first extracted 302,606 articles, which had more than 100 tokens after the stop words were removed. We then selected typical words in each article. The implicit rating data were obtained from the histories of users editing these articles. Each rating consisted of {user, article, number of edits}. The size of this original rating data was 3,325,746. From this data, we extracted a dense subset that consisted of users and articles included in at least 25 units of the original data. We discarded t"
W10-3508,2009.mtsummit-posters.10,1,0.748465,"o QRedit, it loads the corresponding text into the left panel, as shown in Figure 2. Then, QRedit automatically looks up all words in the SL text. When a user clicks an SL word, its translation candidates are displayed in a pop-up window. 2 http://www.idiominc.com/en/ Figure 3: Screenshot of bilingual concordancer 3.2 Bilingual concordancer The translations published on MNH are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). MNH also has parallel texts from the Amnesty International Japan, Democracy Now! Japan, and open source software manuals (Ishisaka et al., 2009). These parallel texts are searched by using a simple bilingual concordancer as shown in Figure 3. 3.3 Bilingual term extraction tool MNH has a bilingual term extraction tool that is composed of a translation estimation tool (Tonoike et al., 2006) and a term extraction tool (Nakagawa and Mori, 2003). First, we apply the translation estimation tool to extract Japanese term candidates and their English translation candidates. Next, we apply the term extraction tool to extract English term candidates. If these English term candidates are found in the English translation candidates, then, we accep"
W10-3508,P09-4005,0,0.0320061,"ble translators to reference Wikipedia articles during the translation process as if they are looking up dictionaries. Third, MNH uses Creative Commons Licenses (CCLs) to help translators share their translations. CCLs are essential for sharing and opening translations. 63 Beijing, August 2010 Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 63–66, Figure 2: Screenshot of QRedit 2 Related work There are many translation support tools, such as Google Translator Toolkit, WikiBABEL (Kumaran et al., 2009), BEYtrans (Bey et al., 2008), Caitra (Koehn, 2009) and Idiom WorldServer system,2 an online multilingual document management system with translation memory functions. The functions that MNH provides are closer to those provided by Idiom WorldServer, but MNH provides a high-quality bilingual dictionaries and functions for seamless Wikipedia and web searches within the integrated translation aid editor QRedit. It also enables translators to share their translations, which are also used as language resources. 3 Helping Volunteer translators This section describes a set of translation aid tools installed in MNH. 3.1 QRedit QRedit is a translation"
W10-3508,P09-4008,0,0.0291935,"ictionary that was made from the English Wikipedia. This enable translators to reference Wikipedia articles during the translation process as if they are looking up dictionaries. Third, MNH uses Creative Commons Licenses (CCLs) to help translators share their translations. CCLs are essential for sharing and opening translations. 63 Beijing, August 2010 Proceedings of the 2nd Workshop on “Collaboratively Constructed Semantic Resources”, Coling 2010, pages 63–66, Figure 2: Screenshot of QRedit 2 Related work There are many translation support tools, such as Google Translator Toolkit, WikiBABEL (Kumaran et al., 2009), BEYtrans (Bey et al., 2008), Caitra (Koehn, 2009) and Idiom WorldServer system,2 an online multilingual document management system with translation memory functions. The functions that MNH provides are closer to those provided by Idiom WorldServer, but MNH provides a high-quality bilingual dictionaries and functions for seamless Wikipedia and web searches within the integrated translation aid editor QRedit. It also enables translators to share their translations, which are also used as language resources. 3 Helping Volunteer translators This section describes a set of translation aid tools i"
W10-3508,W06-1703,0,0.0239343,"www.idiominc.com/en/ Figure 3: Screenshot of bilingual concordancer 3.2 Bilingual concordancer The translations published on MNH are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). MNH also has parallel texts from the Amnesty International Japan, Democracy Now! Japan, and open source software manuals (Ishisaka et al., 2009). These parallel texts are searched by using a simple bilingual concordancer as shown in Figure 3. 3.3 Bilingual term extraction tool MNH has a bilingual term extraction tool that is composed of a translation estimation tool (Tonoike et al., 2006) and a term extraction tool (Nakagawa and Mori, 2003). First, we apply the translation estimation tool to extract Japanese term candidates and their English translation candidates. Next, we apply the term extraction tool to extract English term candidates. If these English term candidates are found in the English translation candidates, then, we accept these term candidates as the translations of those Japanese term candidates. 4 Fostering language resources Being a “one stop” translation aid tool for online translators, MNH incorporates mechanisms which enable users to naturally foster import"
W10-3508,P03-1010,1,0.7968,"em which is designed for volunteer translators working mainly online (Abekawa and Kageura, 2007). When a URL of a source language (SL) text is given to QRedit, it loads the corresponding text into the left panel, as shown in Figure 2. Then, QRedit automatically looks up all words in the SL text. When a user clicks an SL word, its translation candidates are displayed in a pop-up window. 2 http://www.idiominc.com/en/ Figure 3: Screenshot of bilingual concordancer 3.2 Bilingual concordancer The translations published on MNH are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). MNH also has parallel texts from the Amnesty International Japan, Democracy Now! Japan, and open source software manuals (Ishisaka et al., 2009). These parallel texts are searched by using a simple bilingual concordancer as shown in Figure 3. 3.3 Bilingual term extraction tool MNH has a bilingual term extraction tool that is composed of a translation estimation tool (Tonoike et al., 2006) and a term extraction tool (Nakagawa and Mori, 2003). First, we apply the translation estimation tool to extract Japanese term candidates and their English translation candidates. Next, we apply the term ex"
W13-4503,I11-1108,0,0.157615,"arthquake • ׆ಈ: ओʹඃࡂऀͷใͷཧɼ ɼΞυόΠεɼॿٹཁͷߦɽؒظ ࢿߦΓɼ෮ڵஈ֊ʹೖΔࠒ·Ͱ Λ༧ఆɽ In the early stages of the earthquake, many activities have been launched in the Net. Disaster information has been made public in portal sites such as Yahoo! and Google. In particular, “Google Person Finder” was wellknown safety as confirmation system, which was used from the Haiti earthquake of January 2010 (Google, 2011). “ANPI NLP” was a project launched by voluntary researchers to augment the safety information of the system from the Net in a method of natural language processing (Murakami et al., 2011; Neubig et al., 2011). As a system including rescue operations, “sinsai.info” was also well-known (Seki et al., 2011). The site was constructed using a crowdsourcing tool Ushahidi, which was used in many disasters since 2007 and was famous as a system built on the day of the earthquake. Also, a variety of systems were published such as sites displaying a time line of tweets related to the earthquake tags on Twitter, Activities: We report of rescue requests, verify the accuracy of request information and advice for survivors, until request supplies prevail in disaster-stricken areas and the areas enter the first st"
W14-7011,I13-1147,0,0.0240338,"Missing"
W14-7011,N03-1017,0,0.0207031,"ntences into Chinese-like word order, before a baseline phrase-based (PB) SMT system applied. Experimental results on the ASPEC-JC data show that the improvement of the proposed pre-reordering approach is slight on BLEU and mediocre on RIBES, compared with the organizer’s baseline PB SMT system. The approach also shows improvement in human evaluation. We observe the word order does not differ much in the two languages, though Japanese is a subject-object-verb (SOV) language and Chinese is an SVO language. 1 Introduction The state-of-the-art techniques of statistical machine translation (SMT) (Koehn et al., 2003; Koehn et al., 2007) demonstrate good performance on translation of languages with relatively similar word orders (Koehn, 2005). However, word reordering is a problematic issue for language pairs with significantly different word orders, such as the translation between a subject-verb-object (SVO) language and a subject-object-verb (SOV) language (Isozaki et al., 2012). To resolve the word reordering problem in SMT, a line of research handles the word reordering as a separate pre-process, which is referred as pre-reordering. In pre-reordering, the word order on source-side is arranged into the"
W14-7011,P07-2045,0,0.0576512,"-like word order, before a baseline phrase-based (PB) SMT system applied. Experimental results on the ASPEC-JC data show that the improvement of the proposed pre-reordering approach is slight on BLEU and mediocre on RIBES, compared with the organizer’s baseline PB SMT system. The approach also shows improvement in human evaluation. We observe the word order does not differ much in the two languages, though Japanese is a subject-object-verb (SOV) language and Chinese is an SVO language. 1 Introduction The state-of-the-art techniques of statistical machine translation (SMT) (Koehn et al., 2003; Koehn et al., 2007) demonstrate good performance on translation of languages with relatively similar word orders (Koehn, 2005). However, word reordering is a problematic issue for language pairs with significantly different word orders, such as the translation between a subject-verb-object (SVO) language and a subject-object-verb (SOV) language (Isozaki et al., 2012). To resolve the word reordering problem in SMT, a line of research handles the word reordering as a separate pre-process, which is referred as pre-reordering. In pre-reordering, the word order on source-side is arranged into the target-side word ord"
W14-7011,2005.mtsummit-papers.11,0,0.111268,"a show that the improvement of the proposed pre-reordering approach is slight on BLEU and mediocre on RIBES, compared with the organizer’s baseline PB SMT system. The approach also shows improvement in human evaluation. We observe the word order does not differ much in the two languages, though Japanese is a subject-object-verb (SOV) language and Chinese is an SVO language. 1 Introduction The state-of-the-art techniques of statistical machine translation (SMT) (Koehn et al., 2003; Koehn et al., 2007) demonstrate good performance on translation of languages with relatively similar word orders (Koehn, 2005). However, word reordering is a problematic issue for language pairs with significantly different word orders, such as the translation between a subject-verb-object (SVO) language and a subject-object-verb (SOV) language (Isozaki et al., 2012). To resolve the word reordering problem in SMT, a line of research handles the word reordering as a separate pre-process, which is referred as pre-reordering. In pre-reordering, the word order on source-side is arranged into the target-side word order, before a standard SMT system is applied, on both training and decoding phases. 77 Proceedings of the 1s"
W14-7011,W12-4207,0,0.0141584,"5-8573, Japan • {tei@mibel., myama@}cs.tsukuba.ac.jp {mutiyama, eiichiro.sumita}@nict.go.jp Abstract An effective rule-based approach, head finalization has been proposed for English-to-Japanese translation (Isozaki et al., 2012). The approach takes advantage of the head final property of Japanese on the target-side. It designs a head finalization rule to move the head word based on the parsing result by a head-driven phrase structure grammar (HPSG) parser. Generally, the idea can be applied to other SVO-to-Japanese translation tasks, such as its application in Chinese-toJapanese translation (Dan et al., 2012). However, the head finalization cannot be applied on the reverse translation task, i.e. Japaneseto-SVO translation, which becomes a more difficult task. Specifically, Japanese-to-English translation has been studied and several rule-based prereordering approaches have been proposed, taking advantage of the characters of Japanese and English (Komachi et al., 2006; Katz-Brown and Collins, 2008; Sudoh et al., 2011; Hoshino et al., 2013; Ding et al., 2014). A comparison of these approaches is reported in Ding et al. (2014). Because both Chinese and English are SVO languages, to transfer approache"
W14-7011,2006.iwslt-evaluation.11,0,0.0180473,"the head word based on the parsing result by a head-driven phrase structure grammar (HPSG) parser. Generally, the idea can be applied to other SVO-to-Japanese translation tasks, such as its application in Chinese-toJapanese translation (Dan et al., 2012). However, the head finalization cannot be applied on the reverse translation task, i.e. Japaneseto-SVO translation, which becomes a more difficult task. Specifically, Japanese-to-English translation has been studied and several rule-based prereordering approaches have been proposed, taking advantage of the characters of Japanese and English (Komachi et al., 2006; Katz-Brown and Collins, 2008; Sudoh et al., 2011; Hoshino et al., 2013; Ding et al., 2014). A comparison of these approaches is reported in Ding et al. (2014). Because both Chinese and English are SVO languages, to transfer approaches of Japanese-toEnglish to Japanese-to-Chinese translation is a natural idea. Based on the framework of Ding et al. (2014), we propose dependency-based prereordering rules for Japanese-to-Chinese translation in this paper. Contrary to our expectations, from the experimental results on ASPEC-JC data, we discover that the rule-based pre-reordering cannot improve th"
W14-7011,W02-2016,0,0.0690907,"rts are corresponding translation. The right example shows an extra-chunk move. A Japanese post-positioned case-marker is arranged to the left-most position of the range it governs. This move makes the Japanese postposition phrase have an identical order to the Chinese preposition phrase. 4 We further delete several Japanese functional morphemes which do not have exact Chinese translations. They are as follows. Experiment We tested our approach on the ASPEC-JC data (Nakazawa et al., 2014). For the source side Japanese sentences, we used MeCab (IPA dictionary)2 for morpheme analysis, CaboCha3 (Kudo and Matsumoto, 2002) for chunking and dependency parsing. We used the Stanford Chinese Word Segmenter4 (Tseng et al., 2005) with the Chinese Penn Treebank standard (CTB) to seg• topic marker wa • nominative case-marker ga • accusative case-marker wo • conjunctive particle te 2 http://mecab.googlecode.com/svn/ trunk/mecab/doc/index.html 3 https://code.google.com/p/cabocha/ 4 http://www-nlp.stanford.edu/software/ segmenter.shtml We illustrate examples of our pre-reordering approach in Figs. 3 and 4. 79 ment each Chinese sentence. We used the phrasebased (PB) translation system in Moses5 (Koehn et al., 2007) as a ba"
W14-7011,J03-1002,0,0.0069743,"gmenter4 (Tseng et al., 2005) with the Chinese Penn Treebank standard (CTB) to seg• topic marker wa • nominative case-marker ga • accusative case-marker wo • conjunctive particle te 2 http://mecab.googlecode.com/svn/ trunk/mecab/doc/index.html 3 https://code.google.com/p/cabocha/ 4 http://www-nlp.stanford.edu/software/ segmenter.shtml We illustrate examples of our pre-reordering approach in Figs. 3 and 4. 79 ment each Chinese sentence. We used the phrasebased (PB) translation system in Moses5 (Koehn et al., 2007) as a baseline SMT system. Word alignment was automatically generated by GIZA++6 (Och and Ney, 2003) with the default setting of Moses, and symmetrized by the grow-diagfinal-and heuristics (Koehn et al., 2003). In phrase extraction, the max-phrase-length was 7 with GoodTuring option in scoring. The language model used in decoding is an interpolated modified Kneser-Ney discounted 5-gram model, trained on the English side of the training corpus by SRILM7 (Stolcke, 2002). In decoding, the distortion-limit was 9. The MERT (Och, 2003) was used to tune the feature weights on the development set and the translation performance was evaluated on the test set with the tuned weights. We used identical"
W14-7011,P03-1021,0,0.0267577,"used the phrasebased (PB) translation system in Moses5 (Koehn et al., 2007) as a baseline SMT system. Word alignment was automatically generated by GIZA++6 (Och and Ney, 2003) with the default setting of Moses, and symmetrized by the grow-diagfinal-and heuristics (Koehn et al., 2003). In phrase extraction, the max-phrase-length was 7 with GoodTuring option in scoring. The language model used in decoding is an interpolated modified Kneser-Ney discounted 5-gram model, trained on the English side of the training corpus by SRILM7 (Stolcke, 2002). In decoding, the distortion-limit was 9. The MERT (Och, 2003) was used to tune the feature weights on the development set and the translation performance was evaluated on the test set with the tuned weights. We used identical decoding settings on development and test sets. Our approach reached a test set BLEU of 28.18 with CTB segmentation in the final evaluation, which had a slight improvement compared with the 28.01 of the organizer’s PB SMT baseline. As to the reordering measure RIBES, our approach reached a score of 0.8087, which had a mediocre improvement compared with the organizer’s 0.7926. In the human evaluation, our approach also had an improv"
W15-5004,N03-1017,0,0.0147896,"tactic analysis at all, while the DEP-REO makes a good use of the dependency structure of Japanese sentences. As both approaches have been described in detail in their original papers, We do not give repeated descriptions but just state several details in experiments. For DEP-REO, the processes were completely identical to the experiments in Ding et al. (2015), where the tool chain of MeCab2 and CaboCha3 Introduction Statistical machine translation (SMT) techniques have been well developed and widely applied in practice. Linguistic knowledge-free SMT frameworks, such as phrase-based (PB) SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (HIERO) (Chiang, 2007), handle many translation tasks efficiently as long as sufficient training data prepared. Further, sophisticated syntacticallydriven approaches (Neubig, 2013) give better performance than PB SMT and HIERO on difficult translation tasks (Neubig, 2014). At the 2nd Workshop on Asian Translation (WAT 2015) (Nakazawa et al., 2015), our intention is to test the efficiency of several simple techniques for Japanese-to-English (JE) and Korean-to-Japanese (KJ) translation, specifically, pre-reordering approaches for JE translation and character-ba"
W15-5004,P07-2045,0,0.00623357,"16.08 17.58 18.02 17.78 17.54 RIBES .6356 .6512 .6520 .6545 .6488 .6497 .6499 .6556 .6586 .6707 .6751 .6733 .6691 Table 1: Devtest set BLEU score and RIBES on JE translation. +Lex.-Reo. −Lex.-Reo. + Bracket Balanc. DL 0 3 6 0 0 BLEU 66.79 66.64 66.80 66.74 66.98 RIBES .9222 .9221 .9228 .9221 .9224 Table 2: Devtest set BLEU score and RIBES on KJ translation (morpheme level, by MeCab). guage model training (interpolated modified Kneser-Ney discounting; 5-gram on English for JE translation and 9-gram on Japanese for KJ translation). Experiment and Evaluation We used the PB SMT system in Moses12 (Koehn et al., 2007) for JE and KJ translation tasks. Basically, we used identical settings as the organizer used in the baseline. However, there were several differences as follows. • We used MeCab (IPA) and CaboCha to process Japanese sentences in JE translation. • We used no tools for Korean and Japanese morphological analysis in KJ translation, instead, the max-phrase-length were set to 9 in translation model training. • We used SRILM13 (Stolcke, 2002) for lan4 otherwise the reordering will become excessive. i.e., the ordinary comma. 6 “fullwidth comma”, the Chinese comma. 7 “ideographic comma”, the Japanese"
W15-5004,W02-2016,0,0.0984305,"fficiency of several simple techniques for Japanese-to-English (JE) and Korean-to-Japanese (KJ) translation, specifically, pre-reordering approaches for JE translation and character-based processing for KJ translation. On JE translation, we found the simple reverse preordering approach proposed by Katz-Brown and 1 A non-refereed version in Japanese is Ding et al. (2014a). http://taku910.github.io/mecab/ 3 http://taku910.github.io/cabocha/ 2 42 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 42‒47, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). (Kudo and Matsumoto, 2002) based on IPA system for Japanese morphemes was used. For REVREO, an important point is to avoid the reordering across punctuations4 . In the experiments, we used four marks to compose the punctuation set: U+002C5 , U+FF0C6 , U+30017 , and U+30028 . For the Japanese topic marker wa, which plays the key role of the approach, we did not judge it only by the surface form, but also referred to the specific tag joshi, kakarijoshi9 . 3 BASELINE +DEP-REO +REV-REO Character-based KJ Translation As Korean and Japanese share so many similar features, we tried a purely character-based approach in WAT 201"
W15-5004,P13-4016,0,0.0155952,"but just state several details in experiments. For DEP-REO, the processes were completely identical to the experiments in Ding et al. (2015), where the tool chain of MeCab2 and CaboCha3 Introduction Statistical machine translation (SMT) techniques have been well developed and widely applied in practice. Linguistic knowledge-free SMT frameworks, such as phrase-based (PB) SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (HIERO) (Chiang, 2007), handle many translation tasks efficiently as long as sufficient training data prepared. Further, sophisticated syntacticallydriven approaches (Neubig, 2013) give better performance than PB SMT and HIERO on difficult translation tasks (Neubig, 2014). At the 2nd Workshop on Asian Translation (WAT 2015) (Nakazawa et al., 2015), our intention is to test the efficiency of several simple techniques for Japanese-to-English (JE) and Korean-to-Japanese (KJ) translation, specifically, pre-reordering approaches for JE translation and character-based processing for KJ translation. On JE translation, we found the simple reverse preordering approach proposed by Katz-Brown and 1 A non-refereed version in Japanese is Ding et al. (2014a). http://taku910.github.io"
W15-5004,W14-7002,0,0.0118816,"dentical to the experiments in Ding et al. (2015), where the tool chain of MeCab2 and CaboCha3 Introduction Statistical machine translation (SMT) techniques have been well developed and widely applied in practice. Linguistic knowledge-free SMT frameworks, such as phrase-based (PB) SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (HIERO) (Chiang, 2007), handle many translation tasks efficiently as long as sufficient training data prepared. Further, sophisticated syntacticallydriven approaches (Neubig, 2013) give better performance than PB SMT and HIERO on difficult translation tasks (Neubig, 2014). At the 2nd Workshop on Asian Translation (WAT 2015) (Nakazawa et al., 2015), our intention is to test the efficiency of several simple techniques for Japanese-to-English (JE) and Korean-to-Japanese (KJ) translation, specifically, pre-reordering approaches for JE translation and character-based processing for KJ translation. On JE translation, we found the simple reverse preordering approach proposed by Katz-Brown and 1 A non-refereed version in Japanese is Ding et al. (2014a). http://taku910.github.io/mecab/ 3 http://taku910.github.io/cabocha/ 2 42 Proceedings of the 2nd Workshop on Asian Tr"
W16-4606,N12-1047,0,0.0247922,"in the previous subsection. Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation. In our experiments, the number of sentences actually used for evaluation is 300. Baseline SMT The baseline system for our experiment is Moses phrase-based SMT (Koehn et al., 2007) with the default distortion limit of six. We use KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szal, 2010) for word alignment. The weights of the models are tuned with the n-best batch MIRA (Cherry and Foster, 2012). As variants of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based (Chiang, 2005) SMT with the default maximum chart span of ten. Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berkeley Parser. The input sentences are parsed using the Berkeley Parser, and the binary nodes are swapped by the classifier (Goto et al., 2015). As a variant of conventional reordering, we also use a reordering model based on the top-down bracketing transducer gramma"
W16-4606,P05-1033,0,0.657321,"bles high quality images to be formed.]] Figure 1: Example of sublanguage-specific bilingual sentences requiring global reordering. A, B, C are the sentence segments constituting global sentence structures. was combined with syntactic pre-ordering. A statistically significant improvement was observed against the syntactic pre-ordering alone, and a substantial gain of more than 25 points in RIBES score against the baseline was observed for both Japanese-to-English and English-to-Japanese translations, and the BLEU scores remained comparable. 2 Related Work The hierarchical phrase-based method (Chiang, 2005) is one of the early attempts at reordering for SMT. In this method, reordering rules are automatically extracted from non-annotated text corpora during the training phase, and the reordering rules are applied in decoding. As the method does not require syntactic parsing and learns from raw text corpora, it is highly portable. However, this method does not specifically capture global sentence structures. The tree-to-string and string-to-tree SMTs are the methods which employ syntactic parsing, whenever it is available, either for the source or for the target language to improve the translation"
W16-4606,N15-1105,0,0.0599862,"Missing"
W16-4606,2015.mtsummit-papers.1,1,0.799688,"method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees and uses conventional SMT to train global reordering (Mellebeek et al., 2006; Xiao et al., 2014). However, the performance of this method is limited by syntactic parsing, therefore the global reordering has low accuracy where the accuracy of syntactic parsing is low. Another approach involves manually preparing synchronous context-free grammar rules for capturing the global sentence structure of the target sublanguage (Fuji et al., 2015). However, this method requires manual preparation of rules. Both methods are unsuitable for formal documents such as patent abstracts, because they fail to adapt to sentences with various expressions, for which manual preparation of rules is complex. This paper describes a novel global reordering method for capturing sublanguage-specific global sentence structure to supplement the performance of conventional syntactic reordering. The method learns a global pre-ordering model from non-annotated corpora without using syntactic parsing and uses this model to perform global pre-ordering on newly"
W16-4606,D08-1089,0,0.0313098,"hen reorder these detected segments globally. In step (ii), we experiment with a detection method based on heuristics, as well as a method based on machine learning. Steps (i) and (ii) are described in the following subsections. 3.1 Extraction of Sentence Pairs Containing Global Reordering We extract sentences containing global reordering from the training corpus and store them in the global reordering corpus; they can subsequently be used for training and prediction. We consider that a sentence pair contains global reordering if the segments in the target sentence appear in swap orientation (Galley and Manning, 2008) to the source segments, when the sentences are divided into two or three segments each. Figure 2 shows an example of a sentence pair involving global reordering with the sentence divided into three segments. We take the following steps: 1. We divide each source and target sentence into two or three segments. The candidate segments start at all possible word positions in the sentence. Here, a sentence pair consisting of K segments is represented as (ϕ1 , ϕ2 · · · ϕK ), where ϕk consists of the k th phrase of the source sentence and αk th phrase of the target sentence. These segments meet the s"
W16-4606,P13-2121,0,0.0130852,"sentence pairs for training, 1,000 for development and 1,000 for testing. This training data for the translation experiment are also used for training global reordering as described in the previous subsection. Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation. In our experiments, the number of sentences actually used for evaluation is 300. Baseline SMT The baseline system for our experiment is Moses phrase-based SMT (Koehn et al., 2007) with the default distortion limit of six. We use KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szal, 2010) for word alignment. The weights of the models are tuned with the n-best batch MIRA (Cherry and Foster, 2012). As variants of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based (Chiang, 2005) SMT with the default maximum chart span of ten. Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berkeley Parser. The input sentences are parsed using the Berkeley Parser, and the b"
W16-4606,P15-2023,0,0.0136127,"tactic parser to extract the global sentence structure, or the skeleton, from syntactic trees, and uses conventional SMT to train global reordering. Another related approach is the reordering method based on predicate-argument structure (Komachi et al., 2006). However, the performance of sentence structure extraction tends to be low when the accuracy of the syntactic parsing is low. The syntactic pre-ordering is the state-of-the-art method which has substantially improved reordering accuracy, and hence the translation quality (Isozaki et al., 2010b; Goto et al., 2015; de Gispert et al., 2015; Hoshino et al., 2015). However, the adaptation of this method to a new domain requires manually parsed corpora for the target domains. In addition, the method does not have a specific function for capturing global sentence structure. Thus, we apply here our proposed global reordering model as a preprocessor to this syntactic reordering method to ensure the capturing of global sentence structures. 3 Global Pre-ordering Method We propose a novel global reordering method for capturing sublanguage-specific global sentence structure. On the basis of the finding that sublanguage-specific global structures can be detecte"
W16-4606,W15-3058,0,0.0128803,"g global reordering. We evaluate both the heuristic and the machine learning-based methods for comparison. Evaluation metrics We use the RIBES (Isozaki et al., 2010a) and the BLEU (Papineni et al., 2002) scores as evaluation metrics. We use both metrics because n-gram-based metrics such as BLEU alone cannot fully illustrate the effects of global reordering. RIBES is an evaluation metric based on rank correlation which measures long-range relationships and is reported to show much higher correlation with human evaluation than BLEU for evaluating document translations between distant languages (Isozaki and Kouchi, 2015). 5 Results The evaluation results based on the present translation experiment are shown in Tables 1 and 2 for Japanese-to-English and English-to-Japanese translations respectively, listing the RIBES and BLEU scores computed for each of the four reordering configurations. The numbers in the brackets refer to the improvement over the baseline phrase-based SMT with a distortion limit of six. A substantial gain of more than 25 points in the RIBES scores compared to the baseline is observed for both Japanese-to-English and English-to-Japanese translations, when global pre-ordering is used in con89"
W16-4606,D10-1092,0,0.0355675,"Missing"
W16-4606,W10-1736,0,0.120586,"tructure (Mellebeek et al., 2006; Xiao et al., 2014). It uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees, and uses conventional SMT to train global reordering. Another related approach is the reordering method based on predicate-argument structure (Komachi et al., 2006). However, the performance of sentence structure extraction tends to be low when the accuracy of the syntactic parsing is low. The syntactic pre-ordering is the state-of-the-art method which has substantially improved reordering accuracy, and hence the translation quality (Isozaki et al., 2010b; Goto et al., 2015; de Gispert et al., 2015; Hoshino et al., 2015). However, the adaptation of this method to a new domain requires manually parsed corpora for the target domains. In addition, the method does not have a specific function for capturing global sentence structure. Thus, we apply here our proposed global reordering model as a preprocessor to this syntactic reordering method to ensure the capturing of global sentence structures. 3 Global Pre-ordering Method We propose a novel global reordering method for capturing sublanguage-specific global sentence structure. On the basis of th"
W16-4606,P07-2045,0,0.0042033,"ponding original Japanese abstracts, from which we randomly select 1,000,000 sentence pairs for training, 1,000 for development and 1,000 for testing. This training data for the translation experiment are also used for training global reordering as described in the previous subsection. Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation. In our experiments, the number of sentences actually used for evaluation is 300. Baseline SMT The baseline system for our experiment is Moses phrase-based SMT (Koehn et al., 2007) with the default distortion limit of six. We use KenLM (Heafield et al., 2013) for training language models and SyMGIZA++ (Junczys-Dowmunt and Szal, 2010) for word alignment. The weights of the models are tuned with the n-best batch MIRA (Cherry and Foster, 2012). As variants of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based (Chiang, 2005) SMT with the default maximum chart span of ten. Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berke"
W16-4606,W04-3250,0,0.0354322,"ed to the baseline is observed for both Japanese-to-English and English-to-Japanese translations, when global pre-ordering is used in con89 Table 1: Evaluation of Japanese-to-English translation where glob-pre denotes global pre-ordering and pre denotes conventional syntactic pre-ordering, dl denotes distortion limit, HPB denotes hierarchical phrase-based SMT and TDBTG denotes reordering based on top-down bracketing transduction grammar. The bold numbers indicate a statistically insignificant difference from the best system performance according to the bootstrap resampling method at p = 0.05 (Koehn, 2004). Reordering config Settings Results glob-pre pre SMT glob-pre pre RIBES BLEU PB dl=6 44.9 17.9 T1 PB dl=20 53.7 (+8.8) 21.3 (+3.4) HPB 54.9 (+10.0) 23.1 (+5.2) √ PB dl=6 heuristic 61.7 (+16.8) 19.6 (+1.7) T2 PB dl=6 SVM 61.0 (+16.1) 19.3 (+1.4) √ PB dl=6 TDBTG 64.6 (+19.7) 22.3 (+4.4) T3 PB dl=6 syntactic 64.9 (+20.0) 25.5 (+7.6) √ √ PB dl=6 heuristic syntactic 71.3 (+26.4) 25.3 (+7.4) T4 PB dl=6 SVM syntactic 72.1 (+27.2) 25.6 (+7.7) T1 T2 T3 T4 Table 2: Evaluation of English-to-Japanese translation Reordering config Settings Results glob-pre pre SMT glob-pre pre RIBES BLEU PB dl=6 43.2 27.9"
W16-4606,2006.iwslt-evaluation.11,1,0.718282,"ge to improve the translation of the language pair (Yamada and Knight, 2001; Ambati and Chen, 2007). However, these methods too are not specifically designed for capturing global sentence structures. The skeleton-based SMT is a method particularly focusing on the reordering of global sentence structure (Mellebeek et al., 2006; Xiao et al., 2014). It uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees, and uses conventional SMT to train global reordering. Another related approach is the reordering method based on predicate-argument structure (Komachi et al., 2006). However, the performance of sentence structure extraction tends to be low when the accuracy of the syntactic parsing is low. The syntactic pre-ordering is the state-of-the-art method which has substantially improved reordering accuracy, and hence the translation quality (Isozaki et al., 2010b; Goto et al., 2015; de Gispert et al., 2015; Hoshino et al., 2015). However, the adaptation of this method to a new domain requires manually parsed corpora for the target domains. In addition, the method does not have a specific function for capturing global sentence structure. Thus, we apply here our p"
W16-4606,E91-1054,0,0.534414,"and works in conjunction with conventional syntactic reordering. Experimental results on the patent abstract sublanguage show substantial gains of more than 25 points in the RIBES metric and comparable BLEU scores both for Japanese-to-English and English-to-Japanese translations. 1 Introduction Formal documents such as legal and technical documents often form sublanguages. Previous studies have highlighted that capturing the sentence structure specific to the sublanguage is extremely necessary for obtaining high-quality translations especially between distant languages (Buchmann et al., 1984; Luckhardt, 1991; Marcu et al., 2000). Figure 1 illustrates two pairs of bilingual sentences specific to the sublanguage of patent abstracts. In both sentence pairs, the global sentence structure ABC in the source sentences must be reordered to CBA in the target sentences to produce a structurally appropriate translation. Each of the components ABC must then be syntactically reordered to complete the reordering. Various attempts have been made along this line of research. One such method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence st"
W16-4606,A00-2002,0,0.0838799,"unction with conventional syntactic reordering. Experimental results on the patent abstract sublanguage show substantial gains of more than 25 points in the RIBES metric and comparable BLEU scores both for Japanese-to-English and English-to-Japanese translations. 1 Introduction Formal documents such as legal and technical documents often form sublanguages. Previous studies have highlighted that capturing the sentence structure specific to the sublanguage is extremely necessary for obtaining high-quality translations especially between distant languages (Buchmann et al., 1984; Luckhardt, 1991; Marcu et al., 2000). Figure 1 illustrates two pairs of bilingual sentences specific to the sublanguage of patent abstracts. In both sentence pairs, the global sentence structure ABC in the source sentences must be reordered to CBA in the target sentences to produce a structurally appropriate translation. Each of the components ABC must then be syntactically reordered to complete the reordering. Various attempts have been made along this line of research. One such method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence structure, or the skele"
W16-4606,2006.eamt-1.24,0,0.0750942,"Missing"
W16-4606,P15-1021,0,0.0168744,"ts of the baseline, we also evaluate the translation output of the Moses phrase-based SMT with a distortion limit of 20, as well as that of the Moses hierarchical phrase-based (Chiang, 2005) SMT with the default maximum chart span of ten. Conventional syntactic pre-ordering Syntactic pre-ordering is implemented on the Berkeley Parser. The input sentences are parsed using the Berkeley Parser, and the binary nodes are swapped by the classifier (Goto et al., 2015). As a variant of conventional reordering, we also use a reordering model based on the top-down bracketing transducer grammar (TDBTG) (Nakagawa, 2015). We use the output of mkcls and SyMGIZA++ obtained during the preparation of the baseline SMT for training TDBTG-based reordering. Global pre-ordering Global pre-ordering consists of the detection of segment boundaries and the reordering of the detected segments. Out of the 1,000,000 phrase-aligned sentence pairs in the training set for SMT, we use the first 100,000 sentence pairs for extracting the sentence pairs containing global reordering. We only use a portion of the SMT training data due to the slow execution speed of the current implementation of the software program for extracting sen"
W16-4606,P02-1040,0,0.101581,"egment boundaries and the reordering of the detected segments. Out of the 1,000,000 phrase-aligned sentence pairs in the training set for SMT, we use the first 100,000 sentence pairs for extracting the sentence pairs containing global reordering. We only use a portion of the SMT training data due to the slow execution speed of the current implementation of the software program for extracting sentence pairs containing global reordering. We evaluate both the heuristic and the machine learning-based methods for comparison. Evaluation metrics We use the RIBES (Isozaki et al., 2010a) and the BLEU (Papineni et al., 2002) scores as evaluation metrics. We use both metrics because n-gram-based metrics such as BLEU alone cannot fully illustrate the effects of global reordering. RIBES is an evaluation metric based on rank correlation which measures long-range relationships and is reported to show much higher correlation with human evaluation than BLEU for evaluating document translations between distant languages (Isozaki and Kouchi, 2015). 5 Results The evaluation results based on the present translation experiment are shown in Tables 1 and 2 for Japanese-to-English and English-to-Japanese translations respective"
W16-4606,2007.mtsummit-papers.63,1,0.694872,"am for Japanese input. Figure 5 shows the same for English input. The accuracy is the average accuracy of a ten-fold cross-validation for the global reordering corpus. From the calibration shown in the tables, we select the settings producing the highest prediction accuracy, namely, a value of f ive for the n of n-grams and a size of 100k for the global reordering corpus, for both Japanese and English inputs. 4.3 Translation Experiment Setup Data As our experimental data, we use the Patent Abstracts of Japan (PAJ), the English translations of Japanese patent abstracts. We automatically align (Utiyama and Isahara, 2007) PAJ with the corresponding original Japanese abstracts, from which we randomly select 1,000,000 sentence pairs for training, 1,000 for development and 1,000 for testing. This training data for the translation experiment are also used for training global reordering as described in the previous subsection. Out of the 1,000 sentences in the test set, we extract the sentences that show any matching with the n-grams and use these sentences for our evaluation. In our experiments, the number of sentences actually used for evaluation is 300. Baseline SMT The baseline system for our experiment is Mose"
W16-4606,P14-2092,0,0.076127,"In both sentence pairs, the global sentence structure ABC in the source sentences must be reordered to CBA in the target sentences to produce a structurally appropriate translation. Each of the components ABC must then be syntactically reordered to complete the reordering. Various attempts have been made along this line of research. One such method is the skeleton-based statistical machine translation (SMT) which uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees and uses conventional SMT to train global reordering (Mellebeek et al., 2006; Xiao et al., 2014). However, the performance of this method is limited by syntactic parsing, therefore the global reordering has low accuracy where the accuracy of syntactic parsing is low. Another approach involves manually preparing synchronous context-free grammar rules for capturing the global sentence structure of the target sublanguage (Fuji et al., 2015). However, this method requires manual preparation of rules. Both methods are unsuitable for formal documents such as patent abstracts, because they fail to adapt to sentences with various expressions, for which manual preparation of rules is complex. Thi"
W16-4606,P01-1067,0,0.249784,"tempts at reordering for SMT. In this method, reordering rules are automatically extracted from non-annotated text corpora during the training phase, and the reordering rules are applied in decoding. As the method does not require syntactic parsing and learns from raw text corpora, it is highly portable. However, this method does not specifically capture global sentence structures. The tree-to-string and string-to-tree SMTs are the methods which employ syntactic parsing, whenever it is available, either for the source or for the target language to improve the translation of the language pair (Yamada and Knight, 2001; Ambati and Chen, 2007). However, these methods too are not specifically designed for capturing global sentence structures. The skeleton-based SMT is a method particularly focusing on the reordering of global sentence structure (Mellebeek et al., 2006; Xiao et al., 2014). It uses a syntactic parser to extract the global sentence structure, or the skeleton, from syntactic trees, and uses conventional SMT to train global reordering. Another related approach is the reordering method based on predicate-argument structure (Komachi et al., 2006). However, the performance of sentence structure extra"
W16-4613,N12-1048,0,0.152918,"Missing"
W16-4613,2014.iwslt-papers.8,1,0.818639,"ines output steams of tokens which are split by long pauses that may contain a few sentences, a random number (from 1 to 10) of sentences were concatenated to form the input. After segmentation using the proposed methods, punctuation was inserted into the sentences with a hidden N-gram model model (Stolcke et al., 1998; Matusov et al., 2006) prior to translation. In (Anonymous, 2016), this method was shown to be the most effective strategy for the translation of unpunctuated text. The time efficiency of segmenters were measured by average latency per source word using the definition given in (Finch et al., 2014). The quality of segmenters were measured by the BLEU of end-to-end translation, and because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The parameters (all of the θ’s in the ‘Parameters’ column in Table 2) were set by grid search to maximize the BLEU score on the development set. 5-gram interpolated modified Kneser-Ney smoothed language models were used to calculate the confidence. These were trained on the training corpus using the SRILM (Sto"
W16-4613,2015.iwslt-evaluation.9,0,0.106224,"Missing"
W16-4613,2005.mtsummit-papers.11,0,0.0404732,"imental corpus was a union of corpora from multiple sources, including shared tasks such as the Basic Travel Expression Corpus (Takezawa et al., 2002), the NTCIR Patent Machine Translation Corpus (Goto et al., 2013), crawled web data and several in-house parallel resources. Table 1 shows the statistics of sentences and words in the training, development and test sets. The corpora were pre-processed using standard procedures for MT. The Japanese text was segmented into words using Mecab (Kudo, 2005). The English text was tokenized with the tokenization script released with the Europarl corpus (Koehn, 2005) and converted to lowercase. Two treatments were applied to the development and test sets in order to simulate the output from ASR engines. First, because ASR engines normally do not output punctuation, punctuation was removed. Second, because ASR engines output steams of tokens which are split by long pauses that may contain a few sentences, a random number (from 1 to 10) of sentences were concatenated to form the input. After segmentation using the proposed methods, punctuation was inserted into the sentences with a hidden N-gram model model (Stolcke et al., 1998; Matusov et al., 2006) prior"
W16-4613,D10-1018,0,0.31642,"Missing"
W16-4613,2005.iwslt-1.19,0,0.666287,"m model model (Stolcke et al., 1998; Matusov et al., 2006) prior to translation. In (Anonymous, 2016), this method was shown to be the most effective strategy for the translation of unpunctuated text. The time efficiency of segmenters were measured by average latency per source word using the definition given in (Finch et al., 2014). The quality of segmenters were measured by the BLEU of end-to-end translation, and because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The parameters (all of the θ’s in the ‘Parameters’ column in Table 2) were set by grid search to maximize the BLEU score on the development set. 5-gram interpolated modified Kneser-Ney smoothed language models were used to calculate the confidence. These were trained on the training corpus using the SRILM (Stolcke and others, 2002) tools. The machine translation system was an in-house phrasebased system that pre-ordered the input. 4.2 Experimental Results The performance of the interpretation systems using different sentence segmenters is presented in Table 2. The following observations can"
W16-4613,2006.iwslt-papers.1,0,0.207774,"roparl corpus (Koehn, 2005) and converted to lowercase. Two treatments were applied to the development and test sets in order to simulate the output from ASR engines. First, because ASR engines normally do not output punctuation, punctuation was removed. Second, because ASR engines output steams of tokens which are split by long pauses that may contain a few sentences, a random number (from 1 to 10) of sentences were concatenated to form the input. After segmentation using the proposed methods, punctuation was inserted into the sentences with a hidden N-gram model model (Stolcke et al., 1998; Matusov et al., 2006) prior to translation. In (Anonymous, 2016), this method was shown to be the most effective strategy for the translation of unpunctuated text. The time efficiency of segmenters were measured by average latency per source word using the definition given in (Finch et al., 2014). The quality of segmenters were measured by the BLEU of end-to-end translation, and because the segmented source sentences did not necessarily agree with the oracle, translations were aligned to reference sentences through edit distance in order to calculate BLEU (Matusov et al., 2005). The parameters (all of the θ’s in t"
W16-4613,P14-2090,0,0.0468592,"k based on confidence scores, denoted as bi . The final output is a segmented sentence, e.g. w0 , · · · , wi . The proposed segmenters work in an online manner as follows: words are input one by one. The sequence of input words and the derived confidence scores are maintained as states. Once an word is input, its confidence score is calculated and added into the sequence (which is labeled as a in Figure 2). Then a segmentation strategy is applied to the sequence (labeled as (b) in Figure 2). In case that the 1 Fujita et al. (2013)’s method may work on word streams without sentence boundaries; Oda et al. (2014)s’ segmentation model uses linear SVMs and local features extracted from just three word lookahead, so it might be adapted. 141 Algorithm 1 Online Sentence Segmenter Require: w0 , w1 , w2 , . . ., 1: W ← []; S ← [] 2: for wk in stream of words do 3: W ← W + [wk ] 4: sk−1 ← confidence of segmenting before wk 5: S ← S + [sk−1 ] 6: B ← apply segmentation strategy to S 7: if bi = 1 (0 ≤ i ≤ k − 1) then 8: output [w0 , w1 , . . . , wi ] as a segment 9: remove first i elements from W and S 10: end if 11: end for ⊲ assume W = [w0 , w1 , . . . , wk−1 , wk ] ⊲ assume S = [s0 , s1 , . . . , sk−1 ] ⊲ ass"
W16-4613,P02-1040,0,0.101875,"and MT, yet most of them require a long context of future words that follow sentence boundaries. In addition, they are often computationally expensive. These shortages make them unattractive for use in simultaneous interpretation. To the best of our knowledge, there are no published ready-to-use online sentence segmenters, and this motivated this paper. The proposed method is crafted in a way that requires little computation and minimum future words in order to achieve efficiency. Also the proposed method is directly optimized against the widely used measurement of translation quality – BLEU (Papineni et al., 2002) – in order to achieve effectiveness. We believe that this work can directly contribute to the development of real-world simultaneous interpretation systems. The main contributions of this paper are, • proposing a segment boundary confidence score; 139 Proceedings of the 3rd Workshop on Asian Translation, pages 139–148, Osaka, Japan, December 11-17 2016. Figure 1: Illustration of Online Sentence Segmenter in Simultaneous Interpretation System • proposing a hybrid online sentence segmenter; • an empirical study and analysis of the proposed method on two translation tasks. The rest of this paper"
W16-4613,2011.iwslt-papers.7,0,0.0928464,"tegy is not only efficient in terms of average latency per word, but also achieved end-to-end translation quality close to an offline baseline, and close to oracle segmentation. 1 Introduction Simultaneous interpretation performs spoken language translation in a online manner. A spoken language translation system automatically translates text from an automatic speech recognition (ASR) system into another language. Spoken language translation itself is an important application of machine translation (MT) because it takes one of the most natural forms of human communication – speech – as input (Peitz et al., 2011). Simultaneous interpretation is even more demanding than spoken language translation because the processing must occur online. Simultaneous interpretation can bridge the language gap in people’s daily lives transparently because of its ability to respond immediately to users’ speech input. Simultaneous interpretation systems recognize and translate speech at the same time the speakers are speaking, thus the audience can hear the translation and catch the meaning without delay. Potential applications of simultaneous interpretation include interpreting speeches and supporting cross-lingual conv"
W16-4613,takezawa-etal-2002-toward,1,0.387634,"95,054 103,638 95,176 Table 1: Experimental Corpora.† Including punctuations. 4 Experiments 4.1 Experimental Settings Experiments were performed on translation between Japanese and English in both directions. The word orders of these two languages are very different, thus long-distance reordering is often obligatory during translation. This makes simultaneous interpretation a very challenging task, and therefore we choose this language pair for experiments. The experimental corpus was a union of corpora from multiple sources, including shared tasks such as the Basic Travel Expression Corpus (Takezawa et al., 2002), the NTCIR Patent Machine Translation Corpus (Goto et al., 2013), crawled web data and several in-house parallel resources. Table 1 shows the statistics of sentences and words in the training, development and test sets. The corpora were pre-processed using standard procedures for MT. The Japanese text was segmented into words using Mecab (Kudo, 2005). The English text was tokenized with the tokenization script released with the Europarl corpus (Koehn, 2005) and converted to lowercase. Two treatments were applied to the development and test sets in order to simulate the output from ASR engines"
W16-4613,I13-1141,0,0.0397647,"Missing"
W16-4614,W07-0705,0,0.0139778,"e observation, we think the Thai-Laotian and Malay-Indonesian pairs can be handled simultaneously and harmoniously in further research, including corpus annotation, technique development, and NLP applications. The remaining of the paper is arranged as following. In section 2, we introduce the background of the languages discussed in this paper. In section 3, we describe the experiment settings used and discuss the numerical results obtained. Section 4 concludes the paper and provides our future work. 2 Background Specific approaches to process similar languages is an interesting topic in NLP (Vilar et al., 2007; Ding et al., 2015). In this research direction, a priori knowledge of the languages is required and specific approaches can thus be designed by taking advantage of the similarities to outperform a general approach. Here we provide outlines of linguistic features of the four languages mentioned in this paper. 1 http://www2.nict.go.jp/astrec-att/member/mutiyama/ALT/index.html 149 Proceedings of the 3rd Workshop on Asian Translation, pages 149–156, Osaka, Japan, December 11-17 2016. Thai Malay คุ ณ จะ ทำ อ ยำ ง ไร ? apa yang akan anda lakukan ? ທ າ ນ ຈະ ເຮັ ດ ແນ ວ ໃດ ? apa yang akan anda laku"
W16-4614,J93-2003,0,\N,Missing
W17-4753,W16-2358,0,0.056134,"Missing"
W17-4753,W16-2359,0,0.0171527,"Zhang1,2 , Masao Utiyama1 , Eiichro Sumita1 Graham Neubig2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, 3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp neubig/s-nakamura@is.naist.jp Abstract We also explored ways to improve the NMT model with image information. Compared to previous multimodal NMT (MNMT) models that integrate visual features directly (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Calixto et al., 2017), we first exploit image retrieval methods to obtain images that are similar to the image described by the source sentence, and then integrate the target language descriptions of these similar images into the NMT model to help the translation of the source sentence. This makes it possible to exploit a large corpus with only images and target language descriptions through an image retrieval step. This is similar to Hitschler et al. (2016)’s multimodal pivots method, which uses target descriptions of similar images for reranking MT outputs, while we use"
W17-4753,E17-2101,0,0.0123746,"a1 Graham Neubig2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, 3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp neubig/s-nakamura@is.naist.jp Abstract We also explored ways to improve the NMT model with image information. Compared to previous multimodal NMT (MNMT) models that integrate visual features directly (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Calixto et al., 2017), we first exploit image retrieval methods to obtain images that are similar to the image described by the source sentence, and then integrate the target language descriptions of these similar images into the NMT model to help the translation of the source sentence. This makes it possible to exploit a large corpus with only images and target language descriptions through an image retrieval step. This is similar to Hitschler et al. (2016)’s multimodal pivots method, which uses target descriptions of similar images for reranking MT outputs, while we use these target descriptions as additional in"
W17-4753,P05-1033,0,0.0896966,"ing alone. We also present a multimodal NMT model that integrates the target language descriptions of images that are similar to the image described by the source sentence as additional inputs of the neural networks to help the translation of the source sentence. We give detailed analysis for the results of the multimodal NMT model. Our system obtained the first place for the English-to-French task according to human evaluation. 1 2 Text-only MT We compared three text-only approaches for this translation task. 2.1 Introduction Hierarchical Phrase-based SMT The hierarchical phrase-based model (Chiang, 2005) extracts hierarchical phrase-based translation rules from parallel sentence pairs with word alignments. The word alignments can be learned by IBM models. Each translation rule contains several feature scores. The decoder of hierarchical phrase-based model implements a bottom-up CKY+ algorithm. The weights for different features can be tuned on the development set. We participated in the WMT 2017 shared multimodal machine translation task 1, which translates a source language description of an image into a target language description. We built systems for both English-to-German and English-toF"
W17-4753,W16-3210,0,0.06354,"Missing"
W17-4753,P16-1227,0,0.0232453,"ion. Compared to previous multimodal NMT (MNMT) models that integrate visual features directly (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Calixto et al., 2017), we first exploit image retrieval methods to obtain images that are similar to the image described by the source sentence, and then integrate the target language descriptions of these similar images into the NMT model to help the translation of the source sentence. This makes it possible to exploit a large corpus with only images and target language descriptions through an image retrieval step. This is similar to Hitschler et al. (2016)’s multimodal pivots method, which uses target descriptions of similar images for reranking MT outputs, while we use these target descriptions as additional inputs for the NMT model. This paper describes the NICT-NAIST system for the WMT 2017 shared multimodal machine translation task for both language pairs, English-to-German and English-to-French. We built a hierarchical phrase-based (Hiero) translation system and trained an attentional encoder-decoder neural machine translation (NMT) model to rerank the n-best output of the Hiero system, which obtained significant gains over both the Hiero"
W17-4753,W16-2360,0,0.0386153,"ama1 , Eiichro Sumita1 Graham Neubig2 , Satoshi Nakamura2 1 National Institute of Information and Communications Technology, 3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan 2 Graduate School of Information Science, Nara Institute of Science and Technology, Takayama, Ikoma, Nara 630-0192, Japan jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp neubig/s-nakamura@is.naist.jp Abstract We also explored ways to improve the NMT model with image information. Compared to previous multimodal NMT (MNMT) models that integrate visual features directly (Caglayan et al., 2016; Calixto et al., 2016; Huang et al., 2016; Calixto et al., 2017), we first exploit image retrieval methods to obtain images that are similar to the image described by the source sentence, and then integrate the target language descriptions of these similar images into the NMT model to help the translation of the source sentence. This makes it possible to exploit a large corpus with only images and target language descriptions through an image retrieval step. This is similar to Hitschler et al. (2016)’s multimodal pivots method, which uses target descriptions of similar images for reranking MT outputs, while we use these target descri"
W17-4753,P07-2045,0,0.0214797,"between image vectors. Method Hiero NMT Reranking Flickr en-de 27.86 30.52 31.98 en-fr 50.38 50.46 55.25 COCO en-de 24.57 24.27 28.05 en-fr 41.88 41.26 45.17 λ=0 λ = 0.2 en-de 52.17 52.93 en-fr 65.60 66.31 Table 3: 1-gram BLEU score of selected target words on the development set. Table 2: Results of text-only approaches (BLEU). coding, we find the most probable source word for each UNK and replace the UNK using a lexicon extracted from the word-aligned training set. et al., 2017). We lowercase, normalise punctuation and tokenise all sentences. The Hiero translation system was based on Moses (Koehn et al., 2007). We used GIZA++ (Och and Ney, 2003) and growdiag-final-and heuristic (Koehn et al., 2003) to obtain symmetric word alignments. For decoding, we used standard features: direct/inverse phrase translation probability, direct/inverse lexical translation probability and a 5-gram language model, which was trained on the target side of the training corpus by IRSTLM Toolkit3 with improved Kneser-Ney smoothing. Attentional encoder-decoder networks were trained with Lamtram4 . Word embedding size and hidden layer size are both 512. Training data was reshuffled between epochs. Validation was done after"
W17-4753,N03-1017,0,0.0127221,"38 50.46 55.25 COCO en-de 24.57 24.27 28.05 en-fr 41.88 41.26 45.17 λ=0 λ = 0.2 en-de 52.17 52.93 en-fr 65.60 66.31 Table 3: 1-gram BLEU score of selected target words on the development set. Table 2: Results of text-only approaches (BLEU). coding, we find the most probable source word for each UNK and replace the UNK using a lexicon extracted from the word-aligned training set. et al., 2017). We lowercase, normalise punctuation and tokenise all sentences. The Hiero translation system was based on Moses (Koehn et al., 2007). We used GIZA++ (Och and Ney, 2003) and growdiag-final-and heuristic (Koehn et al., 2003) to obtain symmetric word alignments. For decoding, we used standard features: direct/inverse phrase translation probability, direct/inverse lexical translation probability and a 5-gram language model, which was trained on the target side of the training corpus by IRSTLM Toolkit3 with improved Kneser-Ney smoothing. Attentional encoder-decoder networks were trained with Lamtram4 . Word embedding size and hidden layer size are both 512. Training data was reshuffled between epochs. Validation was done after each epoch. We used the Adam optimization algorithm (Kingma and Ba, 2014). Because the tra"
W17-4753,P03-1021,0,0.0340914,"Validation was done after each epoch. We used the Adam optimization algorithm (Kingma and Ba, 2014). Because the training set is only 29K sentence pairs, we used dropout (0.5) and a small learning rate (0.0001) to reduce overfitting, which yielded improvements of 3 − 4 BLEU on the development set. For training the NMT model, we replace words that occur less than twice in the training set as UNK. When de3 4 We used the NMT model to rerank the unique 10, 000-best output of the Hiero system. The NMT score was used as an additional feature for the Hiero system. Feature weights were tuned by MERT (Och, 2003). Table 2 shows results of the Hiero system, the NMT system and using the NMT model to rerank the Hiero outputs. The reranking system had the best performance on both language pairs. It is straightforward that using the NMT feature to rerank the Hiero outputs can achieve improvements over the pure Hiero system. The reason why the reranking method outperformed the NMT system should be that the training corpus is relatively small and the NMT system did not outperform the Hiero system largely. Therefore, the reranking method that takes advantages of both the Hiero and NMT systems worked the best"
W17-4753,J03-1002,0,0.0124739,"MT Reranking Flickr en-de 27.86 30.52 31.98 en-fr 50.38 50.46 55.25 COCO en-de 24.57 24.27 28.05 en-fr 41.88 41.26 45.17 λ=0 λ = 0.2 en-de 52.17 52.93 en-fr 65.60 66.31 Table 3: 1-gram BLEU score of selected target words on the development set. Table 2: Results of text-only approaches (BLEU). coding, we find the most probable source word for each UNK and replace the UNK using a lexicon extracted from the word-aligned training set. et al., 2017). We lowercase, normalise punctuation and tokenise all sentences. The Hiero translation system was based on Moses (Koehn et al., 2007). We used GIZA++ (Och and Ney, 2003) and growdiag-final-and heuristic (Koehn et al., 2003) to obtain symmetric word alignments. For decoding, we used standard features: direct/inverse phrase translation probability, direct/inverse lexical translation probability and a 5-gram language model, which was trained on the target side of the training corpus by IRSTLM Toolkit3 with improved Kneser-Ney smoothing. Attentional encoder-decoder networks were trained with Lamtram4 . Word embedding size and hidden layer size are both 512. Training data was reshuffled between epochs. Validation was done after each epoch. We used the Adam optimiz"
W17-5712,P11-2027,0,0.0225845,"https://github.com/google/sentencepiece 137 System (this-year) Adjusted (last-year) Table 2: JPO adequacy results. Scores Ensemble 1 2 3 4 8 models 0.25 1.75 8.25 36.50 Single 0.25 1.75 17.50 37.75 3 models 2.00 2.75 19.25 43.50 language pair. In addition, we also tried to use SentencePiece, an unsupervised tokenizer to avoid complicated tokenization problems, and also confirmed that the resulting translation systems can perform with no accuracy reduction. scribed in Section 2.4. Table 1 shows the official evaluation scores of our systems, including BLEU, RIBES (Isozaki et al., 2010), AM-FM (Banchs and Li, 2011), and the human evaluation. The rows labeled last-year shows the best system in all previous WAT campaigns. We can see that our one-best system already achieves higher translation accuracy in all automatic evaluation metrics than last-year systems. In addition, adjusted system achieves further better scores than one-best, which means applying better decoding strategy can improve translation accuracy even using the same model. Table 1 also shows the place of our systems in this year. Because official results do not separate scores of single (no-ensemble) models and ensemble models, we also calc"
W17-5712,D10-1092,1,0.7921,"W P = 0.75. 2 Model Ensembling https://github.com/google/sentencepiece 137 System (this-year) Adjusted (last-year) Table 2: JPO adequacy results. Scores Ensemble 1 2 3 4 8 models 0.25 1.75 8.25 36.50 Single 0.25 1.75 17.50 37.75 3 models 2.00 2.75 19.25 43.50 language pair. In addition, we also tried to use SentencePiece, an unsupervised tokenizer to avoid complicated tokenization problems, and also confirmed that the resulting translation systems can perform with no accuracy reduction. scribed in Section 2.4. Table 1 shows the official evaluation scores of our systems, including BLEU, RIBES (Isozaki et al., 2010), AM-FM (Banchs and Li, 2011), and the human evaluation. The rows labeled last-year shows the best system in all previous WAT campaigns. We can see that our one-best system already achieves higher translation accuracy in all automatic evaluation metrics than last-year systems. In addition, adjusted system achieves further better scores than one-best, which means applying better decoding strategy can improve translation accuracy even using the same model. Table 1 also shows the place of our systems in this year. Because official results do not separate scores of single (no-ensemble) models and"
W17-5712,W16-4601,0,0.192201,"The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model. According to the official results, our system achieves higher translation accuracy than any systems submitted previous campaigns despite simple model architecture. 1 Introduction Pr(e|f ) = 2.1 Pr(et |e<t , f ), (1) t=1 Neural machine translation (NMT) methods became one of the main-stream techniques in current machine translation studies. Previous WAT campaign showed that NMT methods can achieve higher translation accuracy in spite of simple model configurations (Nakazawa et al., 2016a). In this year, we chose the NMT architecture as our translation systems submitted for WAT2017 English-Japanese Scientific Paper Translation Task (Nakazawa et al., 2017). The main translation model is constructed by an encoder-decoder model (Sutskever et al., 2014) enforced by an attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). This paper describes the details of our system, including whole model architecture, training criteria, decoding strategy, and data preparation. Results show that our system achieves higher translation accuracy than any systems submitted in previous WAT"
W17-5712,L16-1350,1,0.888778,"Missing"
W17-5712,P02-1040,0,0.104299,"ystem penalizes shorter sentences, and tends to generate longer sentences. Note that if the beam width is 1, there is no effect from word penalty, because the translation system can generate only 1-best results. Results We trained all translation systems varied by model/training/tokenization hyper-parameters described in previous sections, and performed a grid search to find an optimal set of hyper-parameters for this task. For the training data, we used top 2M sentences in ASPEC corpus (Nakazawa et al., 2016b) provided by the organizer. We chose the optimal model that achieves the best BLEU (Papineni et al., 2002) score over the dev corpus. For the optimal model, we also performed a grid search about decoding-time hyper-parameters. All the optimal hyper-parameters described in previous sections are found as the results of these searches. We submitted two results generated from the same optimal model: one-best results, i.e., the results with fixing BW = 1, and adjusted results, i.e., the results with optimal BW and W P deHyper-parameters In our decoding strategy, We have 2 hyper-parameters: beam width BW and word penalty factor W P . We varied BW from 1 to 128, and W P from 0 to 1.5, and finally chose B"
W18-6419,N12-1047,0,0.0542077,"r training data. Consequently, for Tr-En and Zh-En we simply trained regular phrase-based models using MSLR (monotone, swap, discontinuous-left, discontinuous-right) lexicalized reordering models and used the default distortion limit of 6. We trained two 4-gram language models: one on the entire monolingual data concatenated to the target side of the parallel data, and another one on the in-domain “News Crawl” corpora only, using LMPLZ (Heafield et al., 2013). For English, all singletons were pruned due to the large size of the monolingual data. To tune the SMT model weights, we used KB-MIRA (Cherry and Foster, 2012) and selected the weights giving the best BLEU score on the development data after 15 decoding runs. 4 target monolingual data i=i+1 source to target NMT target to source NMT synthetic parallel data synthetic parallel data ki=rki-1 sentences ki=rki-1 sentences ki back-translated sentences ki back-translated sentences Figure 1: Our incremental training framework. systems are trained, from scratch, on their respective new training data comprising the mixture of the original parallel data and the synthetic parallel data whose source side is back-translated from the target side. At this stage, we"
W18-6419,P13-2121,0,0.05331,"Missing"
W18-6419,W18-2703,0,0.069259,"the mixture of the synthetic and original parallel data, we back-translate a larger number of monolingual sentences, including the same sentences backtranslated at the first iteration. Since we have better NMT systems than those at the first iteration, we can expect the back-translation to be of a better quality. We mix this new synthetic parallel data to the original one and train again from scratch a source-to-target and a target-to-source NMT systems to obtain further improved translation models. Note that this procedure is partially similar to the work proposed by Zhang et al. (2018) and Hoang et al. (2018), but differs in the sense that we increase incrementally our back-translated data. Given the number of sentences used in the first iteration, k1 , and an expansion factor, r, we determine ki , the number of monolingual sentences back-translated at iteration i, as follows: Back-translation of Monolingual Data 4.1 bilingual parallel data Incremental Back-Translation with Et-En, Fi-En, and Tr-En We introduced an incremental training framework for NMT aiming to iteratively increase the quality and quantity of the synthetic parallel data used for training. In this framework, we first simultaneousl"
W18-6419,P18-4020,0,0.0575771,"Missing"
W18-6419,I17-1016,1,0.84595,"the scores given by right-to-left NMT models that we trained for each translation direction with the same parameters as left-to-right NMT models. The two right-to-left NMT models, each achieving the best BLEU and the best perplexity scores on the development data, were selected, giving us two other features for each translation direction. Since the Tr-En training parallel data are much smaller, we were able to perform one more right-to-left train15 In practice, adding one more right-to-left model for reranking did not significantly improve the BLEU score on the development data. 453 7 score (Zhang et al., 2017) thanks to the small size of the phrase table learned for this language pair. Also only for this language pair, we computed the scores for each hypothesis given by the so-called minimum Bayes risk (MBR) decoding for n-best list using two metrics: sBLEU and chrF++ (Popovi´c, 2017). The reranking framework was trained on n-best lists produced by the decoding of the same development data that we used to validate NMT system’s training and to tune SMT’s model weights. 6 Conclusion We participated in eight translation directions and for all of them we did experiments to compare SMT and NMT performan"
W18-6419,P07-2045,0,0.0179829,"9M (Fi) 4.4M (Tr) 509.9M (Zh) 36.0M (En) 72.8M (En) 5.1M (En) 576.2M (En) Table 1: Statistics of our preprocessed parallel data. Language En Et Fi Tr Zh Table 2: data. #lines #tokens 338.7M 146.1M 177.1M 105.0M 130.5M 7.5B 3.6B 3.2B 1.8B 2.3B Statistics of our preprocessed monolingual used only 100 millions sentence pairs randomly extracted from “Common Crawl.” To tune/validate and evaluate our systems, we used Newstest2016 and Newstest2017 for Fi-En and Tr-En, Newsdev2017 and Newstest2017 for Zh-En, and Newsdev2018 for Et-En. 2.2 Tokenization, Truecasing and Cleaning We used Moses tokenizer (Koehn et al., 2007) and truecaser for English, Estonian, Finnish, and Turkish. The truecaser was trained on one million tokenized lines extracted randomly from the monolingual data. Truecasing was then performed on all the tokenized data. For Chinese, we used Jieba3 for tokenization but did not perform truecasing. For cleaning, we only applied the Moses script clean-n-corpus.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. Note that we did not perform any punctuation normalization. Tables 1 and 2 present the statistics of the parallel and monoli"
W18-6419,W17-3204,0,0.0263216,"rie and Fujita (2018), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even when SMT greatly underperforms NMT. Following Marie and Fujita (2018), our combination of NMT and SMT works as follows. 5.1 Generation of n-best Lists We first produced the 100-best translation hypotheses with our NMT and SMT systems, independently.11 Unlike Moses, Marian must use a beam of size k to produce a k-best list during decoding. However, using a larger beam size during decoding for NMT may worsen translation quality (Koehn and Knowles, 2017).12 Consequently, we also produced with Marian the 10-best lists, for Zh-En, and 12-best lists for the other language pairs, and merged them with Marian’s 100-best lists to obtain lists containing up to 110 or 112 hypotheses.13 In this way, we make sure that we still have hypotheses of good quality in the lists despite using a larger beam size.14 Then, we merged the lists produced by Marian and Moses. We rescored all the hypotheses in the resulting lists with a reranking framework using features to better model the fluency and the adequacy of each hySetting for Zh-En For the Zh-En language pai"
W18-6419,W18-1811,1,0.930099,"1M 1M 200k 2 2 2 2 2 4 #lines back-translated 10M 20M 40M Table 3: Parameters used for our incremental training. For each language pair, the same parameters were used for both translation directions. In our preliminary experiments, we found that setting r = 2 and k1 very close to, or smaller than, the size of the original parallel data consistently gives good results across language pairs. Fine-tuning r and k1 would result in a better translation quality but at a greater cost. our primary submissions for WMT18 are the results of a simple combination of NMT and SMT. Indeed, as demonstrated by Marie and Fujita (2018), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even when SMT greatly underperforms NMT. Following Marie and Fujita (2018), our combination of NMT and SMT works as follows. 5.1 Generation of n-best Lists We first produced the 100-best translation hypotheses with our NMT and SMT systems, independently.11 Unlike Moses, Marian must use a beam of size k to produce a k-best list during decoding. However, using a larger beam size during decoding for NMT may worsen translation quality (Koehn and Knowles, 20"
W18-6419,P02-1040,0,0.102926,"ords --valid-metrics ce-mean-words perplexity translation --keep-best --enc-depth 6 --dec-depth 6 --transformer-dropout 0.1 --learn-rate 0.0003 --dropout-src 0.1 --dropout-trg 0.1 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --label-smoothing 0.1 --devices 0 1 2 3 --dim-vocabs 50000 50000 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --sync-sgd --tied-embeddings --exponential-smoothing. For ZhEn, we did not use --dropout-src 0.1 --dropout-trg 0.1 since the training data is much larger. We performed NMT decoding with an ensemble of a total of six models according to the best BLEU (Papineni et al., 2002) and the best perplexity scores,7 produced by three independent training runs. #tokens 29.4M (Et) 52.9M (Fi) 4.4M (Tr) 509.9M (Zh) 36.0M (En) 72.8M (En) 5.1M (En) 576.2M (En) Table 1: Statistics of our preprocessed parallel data. Language En Et Fi Tr Zh Table 2: data. #lines #tokens 338.7M 146.1M 177.1M 105.0M 130.5M 7.5B 3.6B 3.2B 1.8B 2.3B Statistics of our preprocessed monolingual used only 100 millions sentence pairs randomly extracted from “Common Crawl.” To tune/validate and evaluate our systems, we used Newstest2016 and Newstest2017 for Fi-En and Tr-En, Newsdev2017 and Newstest2017 for"
W18-6419,W17-4770,0,0.0236645,"Missing"
W18-6419,P16-1009,0,0.397624,"(WMT), Volume 2: Shared Task Papers, pages 449–455 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64046 Language pair Et-En Fi-En Tr-En Zh-En #sent. pairs 1.9M 3.1M 207.4k 24.8M Marian4 (Junczys-Dowmunt et al., 2018) to train and evaluate our NMT systems since it supports state-of-the-art features and is one of the fastest NMT framework publicly available.5 In order to limit the size of the vocabulary of the NMT models, we segmented tokens in the parallel data into subword units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 50k operations. BPE segmentations were jointly learned on the training parallel data for source and target languages, except for Zh-En for which Chinese and English segmentations were trained separately. All our NMT systems for Et-En, Fi-En, and Tr-En were consistently trained on 4 GPUs,6 with the following parameters for Marian: --type transformer --max-length 80 --mini-batch-fit --valid-freq 5000 --save-freq 5000 --workspace 8000 --disp-freq 500 --beam-size 12 --normalize 1 --valid-mini-batch 16 --overwrite --early-stopping 5 --cost-type ce-mean-words --valid-metrics ce-mean-words p"
W18-6419,P16-1162,0,0.730129,"(WMT), Volume 2: Shared Task Papers, pages 449–455 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64046 Language pair Et-En Fi-En Tr-En Zh-En #sent. pairs 1.9M 3.1M 207.4k 24.8M Marian4 (Junczys-Dowmunt et al., 2018) to train and evaluate our NMT systems since it supports state-of-the-art features and is one of the fastest NMT framework publicly available.5 In order to limit the size of the vocabulary of the NMT models, we segmented tokens in the parallel data into subword units via byte pair encoding (BPE) (Sennrich et al., 2016b) using 50k operations. BPE segmentations were jointly learned on the training parallel data for source and target languages, except for Zh-En for which Chinese and English segmentations were trained separately. All our NMT systems for Et-En, Fi-En, and Tr-En were consistently trained on 4 GPUs,6 with the following parameters for Marian: --type transformer --max-length 80 --mini-batch-fit --valid-freq 5000 --save-freq 5000 --workspace 8000 --disp-freq 500 --beam-size 12 --normalize 1 --valid-mini-batch 16 --overwrite --early-stopping 5 --cost-type ce-mean-words --valid-metrics ce-mean-words p"
W18-6489,P18-4020,0,0.08313,"Missing"
W18-6489,W18-2709,0,0.0269454,"00 million and 10 million words and built corresponding NMT systems. Empirical results show that our NMT systems trained on sampled data achieve promising performance. 1 Introduction This paper describes the corpus filtering system built for the participation of the National Institute of Information and Communications Technology (NICT) to the WMT18 shared parallel corpus filtering task. NMT has shown large gains in quality over Statistical machine translation (SMT) and set several new benchmarks (Bojar et al., 2017). However, NMT is much more sensitive to domain (Wang et al., 2017) and noise (Khayrallah and Koehn, 2018). The reason is that NMT is a single neural network structure, which would be affected by each instance during the training procedure (Wang et al., 2017). In comparison, SMT is a combination of distributed models, such as a phrase-table and a language model. Even if some instances in the phrase-table or the language model are noisy, they can only affect part of the models and would not affect the entire system so much. To the best of our knowledge, there are only few works investigating the impact of the noise problem in NMT (Xu and Koehn, 2017; Belinkov and Bisk, 2017). ∗ 2 Task Description W"
W18-6489,W18-6453,0,0.0251529,"etwork structure, which would be affected by each instance during the training procedure (Wang et al., 2017). In comparison, SMT is a combination of distributed models, such as a phrase-table and a language model. Even if some instances in the phrase-table or the language model are noisy, they can only affect part of the models and would not affect the entire system so much. To the best of our knowledge, there are only few works investigating the impact of the noise problem in NMT (Xu and Koehn, 2017; Belinkov and Bisk, 2017). ∗ 2 Task Description WMT18 shared parallel corpus filtering task1 (Koehn et al., 2018) provides a very noisy 1 billion words (English word count) German-English (De-En) corpus crawled from the web as a part of the Paracrawl project. Participants are asked to provide a quality score for each sentence pair in the corpus. Computed scores are then evaluated given the performance of SMT and NMT systems trained on 100M and 10M words sampled from data using the quality scores computed by the participants. newstest2016 is used as the development data and the test data include newstest2018, iwslt2017, Acquis, EMEA, Global Voices, and KDE.2 The statistics of the noisy data to filter are"
W18-6489,P12-3005,0,0.0214345,"Tesla P100 GPUs. Our settings were the same for all of the NMT systems. For each method, we use their score to select the top 100M and 10M sentences to train the corresponding NMT systems. In Table 4, “Original” means the original corpus without any filtering. “Aggressive Filtering” is the method which we introduced in Section 3.1. “Hunalign” indicates the baseline corpus filtering method (Varga et al., 2007)8 given by the organizers. “Classifier” indicates the classifier that we proposed in Section 3.3. “Classifier + LangID” indicates that we also use a language identification tool, LangID (Lui and Baldwin, 2012)9 , to filter the sentence pairs containing sentences that are not German or English. The results were evaluated on the development data newstest2016. Classifier We chose a logistic regression classifier to compute a score for each sentence pair using the features presented in Section 3.2. We trained our classifier on Newstest2014, that we used as positive examples of good sentence pairs, and created the same number of negative examples using the following procedure. We created three-type of negative examples, each of which contains one third of the sentence number of Newstest2014: • Misaligne"
W18-6489,P17-2062,1,0.829142,"3.2 We scored each of the remaining sentence pairs with four NMT transformer models, trained with Marian (Junczys-Dowmunt et al., 2018)4 , on all the parallel data provided for the shared news translation task (excluding the “paracrawl” corpus). We trained left-to-right and right-to-left models for German-to-English and English-toGerman translation directions. We used these four model scores as features in our classifier. We also trained lexical translation probability with Moses and used them to compute a sentencelevel translation probability, for both translation directions, as proposed by Marie and Fujita (2017). To evaluate the semantic similarity between the source and target sentence, we compute a feature based on bilingual word embeddings as follows. First, we trained monolingual word embeddings with FastText (Bojanowski et al., 2017)5 on the monolingual English and German data provided by the WMT organizers. Then, we aligned English and German monolingual word embedding spaces in a bilingual space using the unsupervised method proposed by Artetxe et al. (2018).6 Given the bilingual word embeddings, we computed embeddings for the source and target sentence by doing the element-wise addition of th"
W18-6489,P18-1073,0,0.0238372,"robability with Moses and used them to compute a sentencelevel translation probability, for both translation directions, as proposed by Marie and Fujita (2017). To evaluate the semantic similarity between the source and target sentence, we compute a feature based on bilingual word embeddings as follows. First, we trained monolingual word embeddings with FastText (Bojanowski et al., 2017)5 on the monolingual English and German data provided by the WMT organizers. Then, we aligned English and German monolingual word embedding spaces in a bilingual space using the unsupervised method proposed by Artetxe et al. (2018).6 Given the bilingual word embeddings, we computed embeddings for the source and target sentence by doing the element-wise addition of the bilingual embedding of the words they contain. Finally, we computed the cosine similarity between the embeddings of source and target sentence for each sentence pair, and used it as a feature. Other features are computed to take into account the sentence length: the number of tokens in the source and target sentences, and the difference, and its absolute value, between them. We summarize the features that we used in Table 2. Sentence Pairs Scoring The task"
W18-6489,D17-1155,1,0.758833,"sy data. Finally, we sampled 100 million and 10 million words and built corresponding NMT systems. Empirical results show that our NMT systems trained on sampled data achieve promising performance. 1 Introduction This paper describes the corpus filtering system built for the participation of the National Institute of Information and Communications Technology (NICT) to the WMT18 shared parallel corpus filtering task. NMT has shown large gains in quality over Statistical machine translation (SMT) and set several new benchmarks (Bojar et al., 2017). However, NMT is much more sensitive to domain (Wang et al., 2017) and noise (Khayrallah and Koehn, 2018). The reason is that NMT is a single neural network structure, which would be affected by each instance during the training procedure (Wang et al., 2017). In comparison, SMT is a combination of distributed models, such as a phrase-table and a language model. Even if some instances in the phrase-table or the language model are noisy, they can only affect part of the models and would not affect the entire system so much. To the best of our knowledge, there are only few works investigating the impact of the noise problem in NMT (Xu and Koehn, 2017; Belinkov"
W18-6489,D17-1319,0,0.0246581,"to domain (Wang et al., 2017) and noise (Khayrallah and Koehn, 2018). The reason is that NMT is a single neural network structure, which would be affected by each instance during the training procedure (Wang et al., 2017). In comparison, SMT is a combination of distributed models, such as a phrase-table and a language model. Even if some instances in the phrase-table or the language model are noisy, they can only affect part of the models and would not affect the entire system so much. To the best of our knowledge, there are only few works investigating the impact of the noise problem in NMT (Xu and Koehn, 2017; Belinkov and Bisk, 2017). ∗ 2 Task Description WMT18 shared parallel corpus filtering task1 (Koehn et al., 2018) provides a very noisy 1 billion words (English word count) German-English (De-En) corpus crawled from the web as a part of the Paracrawl project. Participants are asked to provide a quality score for each sentence pair in the corpus. Computed scores are then evaluated given the performance of SMT and NMT systems trained on 100M and 10M words sampled from data using the quality scores computed by the participants. newstest2016 is used as the development data and the test data inclu"
W18-6489,Q17-1010,0,0.0149258,"acrawl” corpus). We trained left-to-right and right-to-left models for German-to-English and English-toGerman translation directions. We used these four model scores as features in our classifier. We also trained lexical translation probability with Moses and used them to compute a sentencelevel translation probability, for both translation directions, as proposed by Marie and Fujita (2017). To evaluate the semantic similarity between the source and target sentence, we compute a feature based on bilingual word embeddings as follows. First, we trained monolingual word embeddings with FastText (Bojanowski et al., 2017)5 on the monolingual English and German data provided by the WMT organizers. Then, we aligned English and German monolingual word embedding spaces in a bilingual space using the unsupervised method proposed by Artetxe et al. (2018).6 Given the bilingual word embeddings, we computed embeddings for the source and target sentence by doing the element-wise addition of the bilingual embedding of the words they contain. Finally, we computed the cosine similarity between the embeddings of source and target sentence for each sentence pair, and used it as a feature. Other features are computed to take"
W19-5313,P18-4020,0,0.0305142,"Missing"
W19-5313,P07-2045,0,0.0106799,"Missing"
W19-5313,W18-6419,1,0.860496,"algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2017) is the current state-of-the-art model for NMT. It is a equal contribution 168 Proceedings of the Fourth Conference on Machine Translation (WMT), Volu"
W19-5313,P02-1040,0,0.103864,"was set to 2048. The number of attention heads in each encoder and decoder layer was set to eight. During training, the value of label smoothing was set to 0.1, and the attention dropout and residual dropout were set to 0.1. The Adam optimizer (Kingma and Ba, 2014) was used to tune the parameters of the model. The learning rate was varied under a warm-up strategy with warm-up steps of 16,000. All NMT models for ZH↔EN tasks were consistently trained on four P100 GPUs. We validated the model with an interval of 5,000 batches on the development set and selected the best model according to BLEU (Papineni et al., 2002) score on the newsdev2018 data set. We performed the following training run independently for five times to obtain the models for ensembling. First, an initial model was trained on the provided parallel data and used to generate pseudo-parallel data through back-translation. A new model was then trained from scratch on the mixture of the original parallel data and the pseudo-parallel data. The new model was further 12 13 Results Table 2 shows the results of ZH↔EN tasks. It is obvious that the back-translation, fine-tuning, and ensemble methods are greatly effective for the ZH↔EN tasks. In part"
W19-5313,Y17-1038,1,0.807473,"ansfer Learning In addition to the approaches in Section 3.1, we also use fine-tuning for transfer learning. Zoph et al. (2016) proposed to train a robust L3→L1 parent model using a large L3–L1 parallel corpus and then fine-tune it on a small L2–L1 corpus to obtain a robust L2→L1 child model. The underlying assumption is that the pre-trained L3→L1 model contains prior probabilities for translation into L1. The prior information is divided into two parts: language modeling information (strong prior) and cross-lingual information (weak or strong depending on the relationship between L3 and L2). Dabre et al. (2017) have shown that linguistically similar L3 and L2 allow for better transfer learning. As such, we transliterate L3 to L2 before pre-training a parent model. This could help in faster convergence, ensure cognate overlap, and potentially lead to a better translation quality. In this participation, we used Hindi as the helping language, L3. Results Refer to rows 1 and 2 of Table 1 for the various automatic evaluation scores. For Kazakh→English our submitted system achieved a cased BLEU score of 26.2 placing our system at 3rd rank out of 9 primary systems. On the other hand, our English→Kazakh per"
W19-5313,P16-1009,0,0.456958,"al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2"
W19-5313,N16-1101,0,0.279052,", 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transfo"
W19-5313,P16-1162,0,0.806805,"al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which leads to poor translations. Kindly refer to the overview paper (Bojar et al., 2019) for additional details about the tasks, comparisons to other submissions, human analyses and insights. 2 The Transformer NMT Model The Transformer (Vaswani et al., 2"
W19-5313,D16-1163,0,0.143248,"English corpus. Chinese↔English translation can benefit from back-translation, model ensembling, and fine-tuning based on the development data. Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has enabled end-to-end training of a translation system without needing to deal with word alignments, translation rules, and complicated decoding algorithms, which are the characteristics of phrase-based statistical machine translation (PBSMT) (Koehn et al., 2007). NMT performs well in resource-rich scenarios but badly in resource-poor ones (Zoph et al., 2016). With the aid of multilingualism, transfer learning, and monolingual corpora, researchers have shown that the translation quality in a low-resource scenario can be significantly boosted (Zoph et al., 2016; Firat et al., 2016; Sennrich et al., 2016a). Furthermore, unsupervised NMT (Lample et al., 2018) has enabled ∗ English→Finnish translation generated by our WMT18’s NMT system (Marie et al., 2018) remains a strong baseline despite the availability of larger bilingual corpora for training this year. Noisy parallel corpora for back-translation leads to poor quality pseudo-parallel data which l"
W19-5330,P18-1073,0,0.354971,"sesdecoder 295 we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual word embeddings (BWE), and the 300 target phrases with the highest scores were kept in the phrase table for each source phrase. In total, the induced phrase table contains 90M phrase pairs. BWE of 512 dimensions were obtained using word embeddings trained with fastText9 and aligned in the same space using unsupervised Vecmap (Artetxe et al., 2018b)10 for this induction. In total four scores, to be used as features in the phrase table, for each of these phrase pairs were computed to mimic phrasebased SMT: forward and backward phrase and lexical translation probabilities. Then, the phrase table was plugged into a Moses system that was tuned on the development data using KB-MIRA. We performed four refinement steps to improve the system using at each step 3M synthetic parallel sentences generated by the forward and backward translation systems, instead of using only either forward (Marie and Fujita, 2018b) or backward translations (Artetx"
W19-5330,D18-1399,0,0.473732,"Missing"
W19-5330,P07-2045,0,0.00885169,"Missing"
W19-5330,W17-3204,0,0.021935,"work. To account for hypotheses length, we added the difference, and its absolute value, between the number of tokens in the translation hypothesis and the source sentence. The reranking framework was trained on n-best lists generated by decoding the first 3k sentence pairs of the development data that we also used to validate the training of UNMT and PNMT systems and to tune the weights of USMT models. Table 3: Parameters for training Marian. 4 Generation of n-best Lists 12 We generated n-best with different beam size for decoding since translation quality can decrease with larger beam size (Koehn and Knowles, 2017). https://marian-nmt.github.io/ 297 # Methods de-cs 1 2 Single UNMT system Single USMT system 15.5 11.1 3 4 5 6 7 Single NMT system pseudo-supervised by UNMT Single NMT system pseudo-supervised by USMT Single Pseudo-supervised MT system Ensemble Pseudo-supervised MT system Re-ranking Pseudo-supervised MT system 15.9 15.3 16.2 16.5 17.0 8 9 10 Fine-tuning Pseudo-supervised MT system Fine-tuning Pseudo-supervised MT system + fixed quotes Fine-tuning + re-ranking Pseudo-supervised MT system + fixed quotes 18.7 19.6 20.1 Table 4: BLEU scores of UMT. #10 is our primary system submitted to the organ"
W19-5330,N12-1047,0,0.0485966,"on 4 GPUs for 300,000 iterations, with the parameters listed by Table 3. Combination of PNMT and USMT Our primary submission for the task was the result of a simple combination of PNMT and USMT similarly to what we did last year in our participation to the supervised News Translation Task of WMT18 (Marie et al., 2018). As demonstrated by Marie and Fujita (2018a), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even though SMT greatly underperforms 11 Reranking Framework and Features We chose KB-MIRA (Cherry and Foster, 2012) as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018a). All the following features we used are described in details by Marie and Fujita (2018a). It includes the scores given by N PNMT models independently trained. We computed sentencelevel translation probabilities using the lexical translation probabilities learned by mgiza during the training of our USMT system. We also used two 4-gram language models to compute two features for each hypothesis. One is the same language model used by our USMT system while the other is a small model trained on all the"
W19-5330,P17-2061,0,0.0185166,"pus for Kazakh. Statistics of the data preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system according to BLEU. However, compared with pseudosupervised MT model trained only on pseudoparallel corpora generated by either UNMT ("
W19-5330,2015.iwslt-evaluation.11,0,0.0294481,"ge in-domain corpora. For Gujarati and Kazakh, we used Common Crawl and News Crawl corpora, in addition to the provided News Commentary corpus for Kazakh. Statistics of the data preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system a"
W19-5330,C18-1111,1,0.806407,"atistics of the data preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system according to BLEU. However, compared with pseudosupervised MT model trained only on pseudoparallel corpora generated by either UNMT (#3) or USMT (#4), me"
W19-5330,W18-1811,1,0.86005,"result of a simple combination of our unsupervised neural and statistical machine translation systems. Our system is ranked first for the German-to-Czech translation task, using only the data provided by the organizers (“constraint”), according to both BLEU-cased and human evaluation. We also performed contrastive experiments with other language pairs, namely, English-Gujarati and EnglishKazakh, to better assess the effectiveness of unsupervised machine translation in for distant language pairs and in truly low-resource conditions. reranking using different informative features as proposed by Marie and Fujita (2018a). This simple combination method performed the best among unsupervised MT systems at WMT19 by BLEU 1 and human evaluation (Bojar et al., 2019). In addition to the official track, we also present the unsupervised systems for English-Gujariti and English-Kazakh for contrastive experiments with much more distant language pairs. The remainder of this paper is organized as follows. In Section 2, we introduce the data preprocessing. In Section 3, we describe the details of our UNMT, USMT, and pseudosupervised MT systems. Then, the combination of pseudo-supervised NMT and USMT is described in Secti"
W19-5330,N16-1162,0,0.0471887,"The other parameters for training the language model were set as listed in Table 1. Then we trained a Transformer-based UNMT model with the pre-trained cross-lingual language model using XLM toolkit. The auto-encoder of UNMT architecture cannot learn useful knowledge without some constraints; it would merely become a copying task that learns to copy the input words one by one (Lample et al., 2018). To alleviate this issue, we utilized a denoising auto-encoder (Vincent et al., 2010), and added noise in the form of random token swapping in input sentences to improve the model learning ability (Hill et al., 2016; He et al., 2016). The denoising auto-encoder acts as a language model that has been trained in one language and Systems Our entire system is illustrated in Figure 1. 3.1 Unsupervised NMT To build competitive UNMT systems, we chose to rely on the Transformer-based UNMT initialized by a pre-trained cross-lingual language model (Lample and Conneau, 2019) since it had been shown to outperform UNMT initialized with word embeddings, in quality and efficiency. In order to limit the size of the vocabulary of the UNMT model, we segmented tokens in the training data into sub-word units via byte pair e"
W19-5330,W18-6419,1,0.803583,"pairs generated by UNMT. To train this pseudo-supervised NMT (PNMT) system, we chose Marian (Junczys-Dowmunt et al., 2018)11 since it supports state-of-the-art features and is one of the fastest NMT frameworks publicly available. Specifically, the pseudo-supervised NMT system for de-cs was trained on 4 GPUs for 300,000 iterations, with the parameters listed by Table 3. Combination of PNMT and USMT Our primary submission for the task was the result of a simple combination of PNMT and USMT similarly to what we did last year in our participation to the supervised News Translation Task of WMT18 (Marie et al., 2018). As demonstrated by Marie and Fujita (2018a), and despite the simplicity of the method used, combining NMT and SMT makes MT more robust and can significantly improve translation quality, even though SMT greatly underperforms 11 Reranking Framework and Features We chose KB-MIRA (Cherry and Foster, 2012) as a rescoring framework and used a subset of the features proposed in Marie and Fujita (2018a). All the following features we used are described in details by Marie and Fujita (2018a). It includes the scores given by N PNMT models independently trained. We computed sentencelevel translation pr"
W19-5330,P18-4020,0,0.0370204,"Missing"
W19-5330,P16-1009,0,0.474495,"16). The denoising auto-encoder acts as a language model that has been trained in one language and Systems Our entire system is illustrated in Figure 1. 3.1 Unsupervised NMT To build competitive UNMT systems, we chose to rely on the Transformer-based UNMT initialized by a pre-trained cross-lingual language model (Lample and Conneau, 2019) since it had been shown to outperform UNMT initialized with word embeddings, in quality and efficiency. In order to limit the size of the vocabulary of the UNMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training monolingual data for German and Czech, and used a shared vocabulary for both languages with 60k tokens based on BPE. We used 50M monolingual corpora to train a 6 https://github.com/facebookresearch/ XLM 7 NVIDIA @ Tesla @ P100 16Gb. 5 https://github.com/moses-smt/ mosesdecoder 295 we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual"
W19-5330,P16-1162,0,0.857696,"16). The denoising auto-encoder acts as a language model that has been trained in one language and Systems Our entire system is illustrated in Figure 1. 3.1 Unsupervised NMT To build competitive UNMT systems, we chose to rely on the Transformer-based UNMT initialized by a pre-trained cross-lingual language model (Lample and Conneau, 2019) since it had been shown to outperform UNMT initialized with word embeddings, in quality and efficiency. In order to limit the size of the vocabulary of the UNMT model, we segmented tokens in the training data into sub-word units via byte pair encoding (BPE) (Sennrich et al., 2016b). We determined 60k BPE operations jointly on the training monolingual data for German and Czech, and used a shared vocabulary for both languages with 60k tokens based on BPE. We used 50M monolingual corpora to train a 6 https://github.com/facebookresearch/ XLM 7 NVIDIA @ Tesla @ P100 16Gb. 5 https://github.com/moses-smt/ mosesdecoder 295 we also considered all the token types in the corpora. Then, we selected the 300k most frequent phrases in the monolingual corpora to be used for inducing a phrase table. All possible phrase pairs are scored, as in Marie and Fujita (2018b), using bilingual"
W19-5330,P18-1072,0,0.0953411,"Missing"
W19-5330,P17-2089,1,0.888859,"Missing"
W19-5330,D17-1155,1,0.833261,"preprocessed with Moses are presented in Table 5. Our results are presented in Table 6. In contrast to what we observed for de-cs, unsupervised BWE are too noisy to be used in phrase table induction for USMT. For both en-gu and en-kk, we obtained unexploitable results confirming the conclusions of Søgaard et al. (2018). Switching to supervised BWE improved significantly the translation quality of USMT but Fine-tuning (Luong and Manning, 2015; Sennrich et al., 2016a) is a conventional method for NMT on low-resource language pairs and domainspecific tasks (Chu et al., 2017; Chu and Wang, 2018; Wang et al., 2017a,b). The PNMT model only relying on monolingual corpora was further trained on the parallel development data to improve translation performance. Finally, fixed quotes method was applied to the final Czech translation. 6 Results on the German-to-Czech Task The results of our systems computed for the Newstest2019 test set are presented in Table 4. As Table 4 shows, UNMT systems significantly outperformed our best USMT system according to BLEU. However, compared with pseudosupervised MT model trained only on pseudoparallel corpora generated by either UNMT (#3) or USMT (#4), merging pseudo-parall"
W19-6601,N12-1048,0,0.042591,"Missing"
W19-6601,P07-2045,0,0.00614914,"Missing"
W19-6601,2015.iwslt-papers.8,0,0.628516,"Missing"
W19-6601,2005.mtsummit-papers.11,0,0.0833525,"decrease θhki by µ 17: if θ′hk−1i &gt; θ′hki &gt; θ′hk+1i and 0.0 6 θ′hki 6 1.0 then 18: Θ ← Θ ∪ [θ′ : s] 19: end if 20: end for 21: end if 22: end if 23: end for return θ∗ English TED talks into Chinese. Table 3 presents the statistics of the corpora. The news commentary corpora (Tiedemann, 2012)4 and a subset of the OpenSubtitles corpora (Lison and Tiedemann, 2016)5 are used to scale up the in-domain training set in order to achieve higher performance. The corpora are pre-processed using standard procedures for MT. The English text is tokenized using the toolkit released with the Europarl corpus (Koehn, 2005) and converted to lower case. The Chinese text is tokenized into Chinese characters and English words using the tool of splitUTF8Characters.pl from the NIST Open Machine Translation 2008 Evaluation 6 4 http://opus.nlpl.eu/News-Commentary.php http://opus.nlpl.eu/OpenSubtitles2016.php 6 ftp://jaguar.ncsl.nist.gov/mt/resources/ 5 Proceedings of MT Summit XVII, volume 1 Two operations are applied in order to simulate the transcripts generated by ASR following the setting in (Wang et al., 2016) and (Cho et al., 2017). First, because ASR engines normally do not produce punctuation, punctuation is re"
W19-6601,2015.iwslt-evaluation.9,0,0.0925523,"Missing"
W19-6601,L16-1147,0,0.0201567,"d vectors 5: for θ in the beginning of Θ do 6: remove θ from Θ 7: if θ′ not in dict then 8: dict ← dict ∪ {θ} 9: s ← decode D using θ and evaluate 10: if s &gt; s∗ − ν then 11: if s &gt; s∗ then 12: s∗ ← s 13: θ∗ ← θ 14: end if 15: for k in 1 to m do 16: θ′ ← increase/decrease θhki by µ 17: if θ′hk−1i &gt; θ′hki &gt; θ′hk+1i and 0.0 6 θ′hki 6 1.0 then 18: Θ ← Θ ∪ [θ′ : s] 19: end if 20: end for 21: end if 22: end if 23: end for return θ∗ English TED talks into Chinese. Table 3 presents the statistics of the corpora. The news commentary corpora (Tiedemann, 2012)4 and a subset of the OpenSubtitles corpora (Lison and Tiedemann, 2016)5 are used to scale up the in-domain training set in order to achieve higher performance. The corpora are pre-processed using standard procedures for MT. The English text is tokenized using the toolkit released with the Europarl corpus (Koehn, 2005) and converted to lower case. The Chinese text is tokenized into Chinese characters and English words using the tool of splitUTF8Characters.pl from the NIST Open Machine Translation 2008 Evaluation 6 4 http://opus.nlpl.eu/News-Commentary.php http://opus.nlpl.eu/OpenSubtitles2016.php 6 ftp://jaguar.ncsl.nist.gov/mt/resources/ 5 Proceedings of MT Summ"
W19-6601,2015.iwslt-evaluation.11,0,0.0631315,"Missing"
W19-6601,D15-1166,0,0.0445983,"Missing"
W19-6601,2005.iwslt-1.19,0,0.863852,"Missing"
W19-6601,tiedemann-2012-parallel,0,0.0211808,"eshold vector 4: dict ← {} ⊲ a dictionary of visited threshold vectors 5: for θ in the beginning of Θ do 6: remove θ from Θ 7: if θ′ not in dict then 8: dict ← dict ∪ {θ} 9: s ← decode D using θ and evaluate 10: if s &gt; s∗ − ν then 11: if s &gt; s∗ then 12: s∗ ← s 13: θ∗ ← θ 14: end if 15: for k in 1 to m do 16: θ′ ← increase/decrease θhki by µ 17: if θ′hk−1i &gt; θ′hki &gt; θ′hk+1i and 0.0 6 θ′hki 6 1.0 then 18: Θ ← Θ ∪ [θ′ : s] 19: end if 20: end for 21: end if 22: end if 23: end for return θ∗ English TED talks into Chinese. Table 3 presents the statistics of the corpora. The news commentary corpora (Tiedemann, 2012)4 and a subset of the OpenSubtitles corpora (Lison and Tiedemann, 2016)5 are used to scale up the in-domain training set in order to achieve higher performance. The corpora are pre-processed using standard procedures for MT. The English text is tokenized using the toolkit released with the Europarl corpus (Koehn, 2005) and converted to lower case. The Chinese text is tokenized into Chinese characters and English words using the tool of splitUTF8Characters.pl from the NIST Open Machine Translation 2008 Evaluation 6 4 http://opus.nlpl.eu/News-Commentary.php http://opus.nlpl.eu/OpenSubtitles2016."
W19-6601,I13-1141,0,0.0451441,"Missing"
W19-7201,D17-1151,0,0.0296082,"Missing"
W19-7201,P16-1160,0,0.0311178,"Missing"
W19-7201,P15-1001,0,0.115725,"Missing"
W19-7201,P17-4012,0,0.406549,"its high accuracy. A downside of NMT is it requires a long training time. For instance, training a Seq2Seq RNN machine translation (MT) with attention (Luong et al., 2015) could take over 10 days using 10 million sentence pairs. A natural solution to this is to use multiple GPUs. There are currently two common approaches for reducing the training time of NMT models. One approach is by using data parallel approach, while the other approach is through the use of the model parallel approach. 1 The data parallel approach is common in many neural network (NN) frameworks. For instance, OpenNMT-lua (Klein et al., 2017) 1 , an NMT toolkit, uses multiple GPUs in training NN models using the data parallel approach. In this approach, the same model is distributed to different GPUs as replicas, and each replica is updated using different data. Afterward, the gradients obtained from each replica are accumulated, and parameters are updated. The model parallel approach has been used for training a Seq2Seq RNN MT with attention (Wu et al., 2016). In this approach, the model is distributed across multiple GPUs, that is, each GPU has only a part of the model. Subsequently, the same data are processed by all GPUs so th"
W19-7201,D15-1166,0,0.141649,"Missing"
W19-7201,W18-6301,0,0.119803,"fficult and can worsen accuracy of the tasks (Krizhevsky, 2014; Keskar et al., 2017). Another important factor to be considered is the ratio of processing time needed for synchronization and forward-backward process on each GPU. If synchronization takes much longer than the forward-backward process, the advantage of using multiple GPUs diminishes. In summary, depending on models, data parallelism may not work effectively. In such a case, there are methods that can be used to achieve synchronization after several mini-batches or to overlap backward and synchronization process at the same time (Ott et al., 2018). However, these advanced synchronization methods are out of the scope of this study. Proceedings of The 8th Workshop on Patent and Scientific Literature Translation 2.2 Model parallelism In this approach, each GPU has different parameters (and computation) of different parts of a model. Most of the communication occurs when passing intermediate results between GPUs. In other words, multiple GPUs do not need to synchronize the values of the parameters. In contrast to data parallelism, most DNN frameworks do not implement automatic model parallelism. Programmers have to implement it depending o"
W19-7201,P16-1009,0,0.0253533,", the conditional probabilities of the target sentence words can be computed as (5) Softmax ,⋯, ,⋯, | ,⋯, , 4.1 We used datasets of WMT14 (Bojar et al., 2014)2 and WMT17 (Bojar et al., 2017)3 English-German shared news translation tasks in the experiments. Both datasets were pre-processed using the scripts of the Marian toolkit (Junczys-Dowmunt et al., 2018)4. Table 1 shows the number of sentences in these datasets. For the WMT17 dataset, first, we duplicated the provided parallel corpus, and then we augmented the parallel corpus with the pseudo-parallel corpus obtained using backtranslation (Sennrich et al., 2016a) of the provided German monolingual data of 10 million (M) sentences. Overall, we used 19 M sentence pairs in the training. We also used the word vocabulary of 32 thousand (K) types from joint source and target byte pair encoding (BPE; Sennrich et al., 2016b). 2 http://www.statmt.org/wmt14/translation-task.html http://www.statmt.org/wmt17/translation-task.html 4 https://github.com/marian-nmt/marian3 Proceedings of The 8th Workshop on Patent and Scientific Literature Translation 4492K ― 4492K 3000 3003 4561K 10000K 19122K 2999 3004 Parameter Experiments Data statistics WMT17 Table 1. Datasets"
W19-7201,Q16-1027,0,0.0498595,"Missing"
W19-7201,P16-1162,0,0.0272778,", the conditional probabilities of the target sentence words can be computed as (5) Softmax ,⋯, ,⋯, | ,⋯, , 4.1 We used datasets of WMT14 (Bojar et al., 2014)2 and WMT17 (Bojar et al., 2017)3 English-German shared news translation tasks in the experiments. Both datasets were pre-processed using the scripts of the Marian toolkit (Junczys-Dowmunt et al., 2018)4. Table 1 shows the number of sentences in these datasets. For the WMT17 dataset, first, we duplicated the provided parallel corpus, and then we augmented the parallel corpus with the pseudo-parallel corpus obtained using backtranslation (Sennrich et al., 2016a) of the provided German monolingual data of 10 million (M) sentences. Overall, we used 19 M sentence pairs in the training. We also used the word vocabulary of 32 thousand (K) types from joint source and target byte pair encoding (BPE; Sennrich et al., 2016b). 2 http://www.statmt.org/wmt14/translation-task.html http://www.statmt.org/wmt17/translation-task.html 4 https://github.com/marian-nmt/marian3 Proceedings of The 8th Workshop on Patent and Scientific Literature Translation 4492K ― 4492K 3000 3003 4561K 10000K 19122K 2999 3004 Parameter Experiments Data statistics WMT17 Table 1. Datasets"
W19-7201,W17-4717,0,\N,Missing
W19-7201,P18-4020,0,\N,Missing
W19-7201,W17-4739,0,\N,Missing
W99-0204,J95-2003,0,0.015696,"Missing"
W99-0204,J93-2004,0,0.0315962,"Missing"
W99-0204,P98-2151,1,0.753269,"Missing"
W99-0204,C98-2146,1,\N,Missing
Y04-1017,P03-1010,1,0.875737,"Missing"
Y15-1030,2012.eamt-1.42,0,0.201113,"SMT approach (Chiang, 2007) is a model based on synchronous context-free grammar. The models are able to be learned from a corpus of unannotated parallel text. The advantage this technique offers over the phrase-based approach is that the hierarchical structure is able to represent the word reordering process. The re-ordering is represented explicitly rather than encoded into a lexicalized re-ordering model (commonly used in purely phrase-based approaches). This makes the approach particularly applicable to languages pairs that require long-distance re-ordering during the translation process (Braune et al., 2012). For PACLIC 29 Source-Target Syllable Word PBSMT HPBSMT OSM PBSMT HPBSMT OSM km-ar 29.87 23.33 30.08 42.74 42.46 42.60 km-da 41.53 23.68 40.88 52.22 52.05 52.66 km-de 35.03 19.44 35.03 48.79 47.58 48.99 km-en 49.07 36.79 49.20 59.51 57.83 60.02 km-es 42.17 30.82 41.14 52.97 52.45 53.53 km-fr 40.85 34.00 40.96 50.79 49.76 51.63 km-hi 26.30 8.82 26.22 40.53 42.05 40.87 km-id 43.26 32.18 43.78 53.26 52.14 53.65 km-it 37.60 29.15 37.03 47.27 46.87 47.79 km-ja 23.46 16.06 23.43 34.27 36.42 33.78 km-ko 21.37 22.57 21.53 32.21 33.61 32.13 km-ms 42.90 33.55 43.03 53.85 52.52 53.56 km-my 27.43 24.40 2"
Y15-1030,P96-1041,0,0.398816,"ed SMT system provided by the Moses toolkit (Koehn and Haddow, 2009) for training the phrase-based machine statistical translation system. The Khmer was aligned with the word segmented target languages (except for the Myanmar language that was syllable segmented) using GIZA++ (Och and Ney, 2000). The alignment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 2003) was used to tune the decoder parameters and the decoding was done using the Moses decoder (version 2.1) (Koehn and Haddow, 2009). 3.3 Hierarchical Phrase-based Machine Translation (HPBSMT) The hierarchical phrase-based SMT approach (Chiang, 2007) is a model based on synchronous context-free grammar. The models are able to be learned from a corpus of unannotated parallel text. The advantage this technique offers over the phrase-based approach is that the hierarchical structure is able to represent the word reordering process. The re-ordering is r"
Y15-1030,J07-2003,0,0.12215,"gnment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 2003) was used to tune the decoder parameters and the decoding was done using the Moses decoder (version 2.1) (Koehn and Haddow, 2009). 3.3 Hierarchical Phrase-based Machine Translation (HPBSMT) The hierarchical phrase-based SMT approach (Chiang, 2007) is a model based on synchronous context-free grammar. The models are able to be learned from a corpus of unannotated parallel text. The advantage this technique offers over the phrase-based approach is that the hierarchical structure is able to represent the word reordering process. The re-ordering is represented explicitly rather than encoded into a lexicalized re-ordering model (commonly used in purely phrase-based approaches). This makes the approach particularly applicable to languages pairs that require long-distance re-ordering during the translation process (Braune et al., 2012). For P"
Y15-1030,J15-2001,0,0.188134,".95 km-vi 45.67 27.20 46.91 53.39 52.57 53.86 km-zh 23.72 8.14 23.87 32.09 32.99 32.22 Table 2: BLEU scores for translating from Khmer. the experiments in this paper we used the implementation of hierarchical model provided by the Moses machine translation toolkit (both the hierarchical decoder and training procedure provided by the experiment management system), using the default settings. 3.4 Operation Sequence Model (OSM) The operation sequence model is a model for statistical MT that combines the benefits of two state-of-the-art SMT frameworks, namely ngram-based SMT and phrase-based SMT (Durrani et al., 2015). It is a generative model that performs the translation process as a linear sequence of operations that jointly generate the source and target sentences. The operation 263 types are (i) generation of a sequence of source and/or target words (ii) insertion of gaps as explicit target positions for reordering operations, and (iii) forward and backward jump operations which perform the actual reordering. The probability of a sequence of operations is given by an n-gram model. The OSM integrates translation and reordering into a single model which provides a natural reordering mechanism that is ab"
Y15-1030,D10-1092,0,0.0465491,"5 53.78 53.78 54.39 ru-km 39.22 38.28 40.00 50.30 50.02 51.34 th-km 46.19 46.46 47.59 53.16 52.40 53.27 tl-km 43.93 42.66 44.06 53.34 53.39 52.76 vi-km 47.93 47.80 48.60 54.26 54.45 55.07 zh-km 32.21 31.16 32.66 39.20 39.49 39.05 Table 3: BLEU scores for translating into Khmer. 4 4.1 Results Evaluation Criteria We used two automatic criteria for the evaluation of the machine translation output. One was the de facto standard automatic evaluation metric Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2001) and the other was the Rank-based Intuitive Bilingual Evaluation Measure (RIBES) (Isozaki et al., 2010). The BLEU score measures the precision of n-grams (over all n ≤ 4 in our case) with respect to a reference translation with a penalty for short translations (Papineni et al., 2001). Intuitively, the BLEU score measures the adequacy of the translations and large BLEU scores are better. RIBES is 264 an automatic evaluation metric based on rank correlation coefficients modified with precision and special care is paid to word order of the translation results. The RIBES score is suitable for distant language pairs such as Khmer and English, Khmer and Korean, Khmer and Myanmar (Isozaki et al., 2010"
Y15-1030,W09-0429,0,0.131977,"others domains. The CRF segmenter achieved 99.15 Precision, 95.72 Recall and 97.31 F-Score. This CRF word segmenter was used to segment the Khmer BTEC data for the experiments in the next section. 3 Experimental Methodology 3.1 Corpus Statistics We used twenty one languages from the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of travel-related expressions (Kikui et al., 2003). The languages were Arabic (ar), Chinese (zh), Danish (da), Dutch (nl), English (en), French (fr), German (de), Hindi 262 We used the phrase based SMT system provided by the Moses toolkit (Koehn and Haddow, 2009) for training the phrase-based machine statistical translation system. The Khmer was aligned with the word segmented target languages (except for the Myanmar language that was syllable segmented) using GIZA++ (Och and Ney, 2000). The alignment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 20"
Y15-1030,N03-1017,0,0.0900442,"Missing"
Y15-1030,W04-3250,0,0.114954,"is the target language. It is clear from the results in the experiments, that syllable segmentation is a far worse segmentation strategy for SMT than word segmentation. This is not always the case, and for 265 languages such as Myanmar it has been shown (Thu et al., 2013) that syllable segmentation can give rise to machine translation scores that are competitive with other approaches. However, for Khmer the proposed word segmentation strategy gave rise to considerable gains in performance and is therefore to be preferred in all cases. Statistical significance tests using bootstrap resampling (Koehn, 2004) were run for all experiments involving the two segmentation schemes. For all experiments the differences were significant (p &lt; 0.01). For most languages combinations the OSM approach gave the highest scores. It is not surprising that is was able to exceed the performance of the phrase-based approach which it extends. However, in all-but-one of the evaluations involving Japanese and Korean the HPBSMT approach gave rise to the highest scores. Looking at the Kendall’s tau distances in Figure 2 it can be seen that Japanese and Korean are the two most distant languages from Khmer in terms of this"
Y15-1030,P00-1056,0,0.278169,"us Statistics We used twenty one languages from the multilingual Basic Travel Expressions Corpus (BTEC), which is a collection of travel-related expressions (Kikui et al., 2003). The languages were Arabic (ar), Chinese (zh), Danish (da), Dutch (nl), English (en), French (fr), German (de), Hindi 262 We used the phrase based SMT system provided by the Moses toolkit (Koehn and Haddow, 2009) for training the phrase-based machine statistical translation system. The Khmer was aligned with the word segmented target languages (except for the Myanmar language that was syllable segmented) using GIZA++ (Och and Ney, 2000). The alignment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 2003) was used to tune the decoder parameters and the decoding was done using the Moses decoder (version 2.1) (Koehn and Haddow, 2009). 3.3 Hierarchical Phrase-based Machine Translation (HPBSMT) The hierarchical phrase-based SMT a"
Y15-1030,P03-1021,0,0.0861983,", 2009) for training the phrase-based machine statistical translation system. The Khmer was aligned with the word segmented target languages (except for the Myanmar language that was syllable segmented) using GIZA++ (Och and Ney, 2000). The alignment was symmetrized by grow-diag-final-and heuristic (Koehn et al., 2003). The lexicalized reordering model was trained with the msd-bidirectionalfe option (Tillmann, 2004). We use SRILM for training the 5-gram language model with interpolated modified Kneser-Ney discounting (Stolcke, 2002; Chen and Goodman, 1996). Minimum error rate training (MERT) (Och, 2003) was used to tune the decoder parameters and the decoding was done using the Moses decoder (version 2.1) (Koehn and Haddow, 2009). 3.3 Hierarchical Phrase-based Machine Translation (HPBSMT) The hierarchical phrase-based SMT approach (Chiang, 2007) is a model based on synchronous context-free grammar. The models are able to be learned from a corpus of unannotated parallel text. The advantage this technique offers over the phrase-based approach is that the hierarchical structure is able to represent the word reordering process. The re-ordering is represented explicitly rather than encoded into a"
Y15-1030,2001.mtsummit-papers.68,0,0.0744377,"-km 33.82 25.84 33.94 38.25 31.83 38.15 nl-km 44.85 43.05 45.22 53.51 53.98 53.96 pt-km 44.89 44.13 45.55 53.78 53.78 54.39 ru-km 39.22 38.28 40.00 50.30 50.02 51.34 th-km 46.19 46.46 47.59 53.16 52.40 53.27 tl-km 43.93 42.66 44.06 53.34 53.39 52.76 vi-km 47.93 47.80 48.60 54.26 54.45 55.07 zh-km 32.21 31.16 32.66 39.20 39.49 39.05 Table 3: BLEU scores for translating into Khmer. 4 4.1 Results Evaluation Criteria We used two automatic criteria for the evaluation of the machine translation output. One was the de facto standard automatic evaluation metric Bilingual Evaluation Understudy (BLEU) (Papineni et al., 2001) and the other was the Rank-based Intuitive Bilingual Evaluation Measure (RIBES) (Isozaki et al., 2010). The BLEU score measures the precision of n-grams (over all n ≤ 4 in our case) with respect to a reference translation with a penalty for short translations (Papineni et al., 2001). Intuitively, the BLEU score measures the adequacy of the translations and large BLEU scores are better. RIBES is 264 an automatic evaluation metric based on rank correlation coefficients modified with precision and special care is paid to word order of the translation results. The RIBES score is suitable for dist"
Y15-1030,N04-4026,0,\N,Missing
Y15-1030,P02-1040,0,\N,Missing
Y15-1030,D08-1076,0,\N,Missing
Y18-3006,P03-1021,0,0.0360105,"neni et al., 2002) and the perplexity scores, produced by 4 independent training runs. 3.2 SMT We also trained phrase-based SMT systems using Moses. Word alignments and phrase tables were trained on the tokenized parallel data using mgiza. Source-to-target and target-to-source word alignments were symmetrized with the grow-diag -final-and heuristic. We simply trained regular phrase-based models and used the default distortion limit of 6. We trained two 5gram language models on the entire target side of the parallel data, with SRILM (Stolcke, 2002). To tune the SMT model weights, we used MERT (Och, 2003) and selected the weights giving the best BLEU score on the development data. 3.3 Pre-ordering We also tried a classic pre-ordering method for English-to-Myanmar translation task. Specifically, the dependency-based head finalization in Ding et al. (2014) is exactly reproduced in our experiment. The source English part is pre-ordered before being input into NMT and SMT systems. 973 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 4 Results Our systems are ev"
Y18-3006,P02-1040,0,0.109271,"ni-batch 16 --valid-freq 5000 --learn-rate 0.0003 --lr-decay-inv-sqrt 16000 --lr-warmup 16000 --lr-report --sync-sgd --devices 0 1 2 3 --dim-vocabs 50000 50000 --exponential-smoothing 2 https://marian-nmt.github.io/, version 1.4.0 It is fully implemented in pure C++ and supports multiGPU training. 4 NVIDIA® Tesla® P100 16Gb. 3 --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --tied-embeddings --mini-batch-fit --early-stopping 5 --label-smoothing 0.1 --valid-metrics ce-mean-words perplexity translation We performed NMT decoding with an ensemble of a total of 4 models according to the best BLEU (Papineni et al., 2002) and the perplexity scores, produced by 4 independent training runs. 3.2 SMT We also trained phrase-based SMT systems using Moses. Word alignments and phrase tables were trained on the tokenized parallel data using mgiza. Source-to-target and target-to-source word alignments were symmetrized with the grow-diag -final-and heuristic. We simply trained regular phrase-based models and used the default distortion limit of 6. We trained two 5gram language models on the entire target side of the parallel data, with SRILM (Stolcke, 2002). To tune the SMT model weights, we used MERT (Och, 2003) and sel"
Y18-3006,2014.iwslt-papers.5,1,0.84444,"-to-target and target-to-source word alignments were symmetrized with the grow-diag -final-and heuristic. We simply trained regular phrase-based models and used the default distortion limit of 6. We trained two 5gram language models on the entire target side of the parallel data, with SRILM (Stolcke, 2002). To tune the SMT model weights, we used MERT (Och, 2003) and selected the weights giving the best BLEU score on the development data. 3.3 Pre-ordering We also tried a classic pre-ordering method for English-to-Myanmar translation task. Specifically, the dependency-based head finalization in Ding et al. (2014) is exactly reproduced in our experiment. The source English part is pre-ordered before being input into NMT and SMT systems. 973 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 4 Results Our systems are evaluated on the ALT test set and the results5 are shown in Table 2. Our observations from are as follows: 1) Our NMT (Marian) system performed much better than SMT (Moses) system in My-to-En. That is, nearly 7 BLEU scores. However, there is no significant"
Y18-3006,W18-6419,1,0.806536,"slation task (Nakazawa et al., 2018), specifically Myanmar (My) - English (En) for both translation directions. All of our systems are constrained, i.e., we used only the parallel adata provided by the organizers to train and tune our systems. The remainder of this paper is organized as follows. In Section 2, we present the data preprocessing. In Section 3, we introduce the details 1 of our NMT and SMT systems with pre-ordering technology. Empirical results obtained with our systems are analyzed in Section 4 and we conclude this paper in Section 5. This system is based on our WMT-2018 system (Marie et al., 2018). Corpus train(ALT) train(UCSY) dev(ALT) test(ALT) #lines #tokens (My/En) 17.9K 208.6K 0.9K 1.0K 1.0M / 410.2K 5.8M / 2.6M 57.4K / 22.1K 58.3K / 22.7K We used Moses tokenizer and truecaser for English. The truecaser was trained on the English data, after tokenization. For Myanmar, we used the original tokens. For cleaning, we only applied the Moses script clean-n-corpus.perl to remove lines in the parallel data containing more than 80 tokens and replaced characters forbidden by Moses. 972 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translatio"
