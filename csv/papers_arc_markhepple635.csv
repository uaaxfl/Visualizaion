W18-4004,"Transferred Embeddings for {I}gbo Similarity, Analogy, and Diacritic Restoration Tasks",2018,0,2,3,1,18304,ignatius ezeani,Proceedings of the Third Workshop on Semantic Deep Learning,0,"Existing NLP models are mostly trained with data from well-resourced languages. Most minority languages face the challenge of lack of resources - data and technologies - for NLP research. Building these resources from scratch for each minority language will be very expensive, time-consuming and amount largely to unnecessarily re-inventing the wheel. In this paper, we applied transfer learning techniques to create Igbo word embeddings from a variety of existing English trained embeddings. Transfer learning methods were also used to build standard datasets for Igbo word similarity and analogy tasks for intrinsic evaluation of embeddings. These projected embeddings were also applied to diacritic restoration task. Our results indicate that the projected models not only outperform the trained ones on the semantic-based tasks of analogy, word-similarity, and odd-word identifying, but they also achieve enhanced performance on the diacritic restoration with learned diacritic embeddings."
N18-4008,{I}gbo Diacritic Restoration using Embedding Models,2018,0,0,2,1,18304,ignatius ezeani,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"Igbo is a low-resource language spoken by approximately 30 million people worldwide. It is the native language of the Igbo people of south-eastern Nigeria. In Igbo language, diacritics - orthographic and tonal - play a huge role in the distinguishing the meaning and pronunciation of words. Omitting diacritics in texts often leads to lexical ambiguity. Diacritic restoration is a pre-processing task that replaces missing diacritics on words from which they have been removed. In this work, we applied embedding models to the diacritic restoration task and compared their performances to those of n-gram models. Although word embedding models have been successfully applied to various NLP tasks, it has not been used, to our knowledge, for diacritic restoration. Two classes of word embeddings models were used: those projected from the English embedding space; and those trained with Igbo bible corpus ({\mbox{$\approx$}} 1m). Our best result, 82.49{\%}, is an improvement on the baseline n-gram models."
W17-1907,Lexical Disambiguation of {I}gbo using Diacritic Restoration,2017,4,2,2,1,18304,ignatius ezeani,"Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications",0,"Properly written texts in Igbo, a low-resource African language, are rich in both orthographic and tonal diacritics. Diacritics are essential in capturing the distinctions in pronunciation and meaning of words, as well as in lexical disambiguation. Unfortunately, most electronic texts in diacritic languages are written without diacritics. This makes diacritic restoration a necessary step in corpus building and language processing tasks for languages with diacritics. In our previous work, we built some n-gram models with simple smoothing techniques based on a closed-world assumption. However, as a classification task, diacritic restoration is well suited for and will be more generalisable with machine learning. This paper, therefore, presents a more standard approach to dealing with the task which involves the application of machine learning algorithms."
W16-6610,Automatic label generation for news comment clusters,2016,15,7,6,0.589376,25116,ahmet aker,Proceedings of the 9th International Natural Language Generation conference,0,"We present a supervised approach to automat-n ically labelling topic clusters of reader com-n ments to online news. We use a feature setn that includes both features capturing proper-n ties local to the cluster and features that cap-n ture aspects from the news article and fromn comments outside the cluster. We evaluaten the approach in an automatic and a manual,n task-based setting. Both evaluations show then approach to outperform a baseline method,n which uses tf*idf to select comment-internaln terms for use as topic labels. We illustrate hown cluster labels can be used to generate clustern summaries and present two alternative sum-n mary formats: a pie chart summary and an ab-n stractive summary."
W16-3605,The {SENSEI} Annotated Corpus: Human Summaries of Reader Comment Conversations in On-line News,2016,15,9,5,0,33318,emma barker,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Researchers are beginning to explore hown to generate summaries of extended argumentativen conversations in social media,n such as those found in reader comments inn on-line news. To date, however, there hasn been little discussion of what these summariesn should be like and a lack of humanauthoredn exemplars, quite likely becausen writing summaries of this kind of interchangen is so difficult. In this paper wen propose one type of reader comment summaryn xe2x80x93 the conversation overview summaryn xe2x80x93 that aims to capture the key argumentativen content of a reader commentn conversation. We describe a method wen have developed to support humans in authoringn conversation overview summariesn and present a publicly available corpus xe2x80x93n the first of its kind xe2x80x93 of news articles plusn comment sets, each multiply annotated,n according to our method, with conversationn overview summaries."
L16-1494,What{'}s the Issue Here?: Task-based Evaluation of Reader Comment Summarization Systems,2016,9,4,7,0,33318,emma barker,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Automatic summarization of reader comments in on-line news is an extremely challenging task and a capability for which there is a clear need. Work to date has focussed on producing extractive summaries using well-known techniques imported from other areas of language processing. But are extractive summaries of comments what users really want? Do they support users in performing the sorts of tasks they are likely to want to perform with reader comments? In this paper we address these questions by doing three things. First, we offer a specification of one possible summary type for reader comment, based on an analysis of reader comment in terms of issues and viewpoints. Second, we define a task-based evaluation framework for reader comment summarization that allows summarization systems to be assessed in terms of how well they support users in a time-limited task of identifying issues and characterising opinion on issues in comments. Third, we describe a pilot evaluation in which we used the task-based evaluation framework to evaluate a prototype reader comment clustering and summarization system, demonstrating the viability of the evaluation framework and illustrating the sorts of insight such an evaluation affords."
L16-1694,Studying the Temporal Dynamics of Word Co-occurrences: An Application to Event Detection,2016,35,4,3,0,25678,daniel preoctiucpietro,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Streaming media provides a number of unique challenges for computational linguistics. This paper studies the temporal variation in word co-occurrence statistics, with application to event detection. We develop a spectral clustering approach to find groups of mutually informative terms occurring in discrete time frames. Experiments on large datasets of tweets show that these groups identify key real world events as they occur in time, despite no explicit supervision. The performance of our method rivals state-of-the-art methods for event detection on F-score, obtaining higher recall at the expense of precision."
W15-5405,"Use of Transformation-Based Learning in Annotation Pipeline of {I}gbo, an {A}frican Language",2015,7,3,2,1,28212,ikechukwu onyenwe,"Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects",0,"The accuracy of an annotated corpus can be increased through evaluation and re- vision of the annotation scheme, and through adjudication of the disagreements found. In this paper, we describe a novel process that has been applied to improve a part-of-speech (POS) tagged corpus for the African language Igbo. An inter-annotation agreement (IAA) exercise was undertaken to iteratively revise the tagset used in the creation of the initial tagged corpus, with the aim of refining the tagset and maximizing annotator performance. The tagset revisions and other corrections were efficiently propagated to the overall corpus in a semi-automated manner using transformation-based learning (TBL) to identify candidates for cor- rection and to propose possible tag corrections. The affected word-tag pairs in the corpus were inspected to ensure a high quality end-product with an accuracy that would not be achieved through a purely automated process. The results show that the tagging accuracy increases from 88% to 94%. The tagged corpus is potentially re-usable for other dialects of the language."
W15-4635,Comment-to-Article Linking in the Online News Domain,2015,15,5,3,0.589376,25116,ahmet aker,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Online commenting to news articles provides a communication channel between media professionals and readers offering a crucial tool for opinion exchange and freedom of expression. Currently, comments are detached from the news article and thus removed from the context that they were written for. In this work, we propose a method to connect readersxe2x80x99 comments to the news article segments they refer to. We use similarity features to link comments to relevant article segments and evaluate both word-based and term-based vector spaces. Our results are comparable to state-of-theart topic modeling techniques when used for linking tasks. We demonstrate that article segments and comments representation are relevant to linking accuracy since we achieve better performances when similarity features are computed using similarity between terms rather than words."
W14-4914,"Part-of-speech Tagset and Corpus Development for {I}gbo, an {A}frican Language",2014,0,2,3,1,28212,ikechukwu onyenwe,Proceedings of {LAW} {VIII} - The 8th Linguistic Annotation Workshop,0,"This project aims to develop linguistic resources to support computational NLP research on then Igbo language. The starting point for this project is the development of a new part-of-speech taggingn scheme based on the EAGLES tagset guidelines, adapted to incorporate additional languagen internal features. The tags are currently being used in a part-of-speech annotation task for then development of POS tagged Igbo corpus. The proposed tagset has 59 tags."
N10-1037,Evaluation Metrics for the Lexical Substitution Task,2010,3,4,2,0,45798,sanaz jabbari,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We identify some problems of the evaluation metrics used for the English Lexical Substitution Task of SemEval-2007, and propose alternative metrics that avoid these problems, which we hope will better guide the future development of lexical substitution systems."
jabbari-etal-2010-evaluating,Evaluating Lexical Substitution: Analysis and New Measures,2010,2,1,2,0,45798,sanaz jabbari,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Lexical substitution is the task of finding a replacement for a target word in a sentence so as to preserve, as closely as possible, the meaning of the original sentence. It has been proposed that lexical substitution be used as a basis for assessing the performance of word sense disambiguation systems, an idea realised in the English Lexical Substitution Task of SemEval-2007. In this paper, we examine the evaluation metrics used for the English Lexical Substitution Task and identify some problems that arise for them. We go on to propose some alternative measures for this purpose, that avoid these problems, and which in turn can be seen as redefining the key tasks that lexical substitution systems should be expected to perform. We hope that these new metrics will better serve to guide the development of lexical substitution systems in future work. One of the new metrics addresses how effective systems are in ranking substitution candidates, a key ability for lexical substitution systems, and we report some results concerning the assessment of systems produced by this measure as compared to the relevant measure from SemEval-2007."
guthrie-etal-2010-efficient,Efficient Minimal Perfect Hash Language Models,2010,19,10,2,0,46329,david guthrie,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,The availability of large collections of text have made it possible to build language models that incorporate counts of billions of n-grams. This paper proposes two new methods of efficiently storing large language models that allow O(1) random access and use significantly less space than all known approaches. We introduce two novel data structures that take advantage of the distribution of n-grams in corpora and make use of various numbers of minimal perfect hashes to compactly store language models containing full frequency counts of billions of n-grams using 2.5 Bytes per n-gram and language models of quantized probabilities using 2.26 Bytes per n-gram. These methods allow language processing applications to take advantage of much larger language models than previously was possible using the same hardware and we additionally describe how they can be used in a distributed environment to store even larger models. We show that our approaches are simple to implement and can easily be combined with pruning and quantization to achieve additional reductions in the size of the language model.
D10-1026,Storing the Web in Memory: Space Efficient Language Models with Constant Time Retrieval,2010,23,23,2,0,46329,david guthrie,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We present three novel methods of compactly storing very large n-gram language models. These methods use substantially less space than all known approaches and allow n-gram probabilities or counts to be retrieved in constant time, at speeds comparable to modern language modeling toolkits. Our basic approach generates an explicit minimal perfect hash function, that maps all n-grams in a model to distinct integers to enable storage of associated values. Extensions of this approach exploit distributional characteristics of n-gram data to reduce storage costs, including variable length coding of values and the use of tiered structures that partition the data for more efficient storage. We apply our approach to storing the full Google Web1T n-gram set and all 1-to-5 grams of the Gigaword newswire corpus. For the 1.5 billion n-grams of Gigaword, for example, we can store full count information at a cost of 1.66 bytes per n-gram (around 30% of the cost when using the current state-of-the-art approach), or quantized counts for 1.41 bytes per n-gram. For applications that are tolerant of a certain class of relatively innocuous errors (where unseen n-grams may be accepted as rare n-grams), we can reduce the latter cost to below 1 byte per n-gram."
W08-0602,Extracting Clinical Relationships from Patient Narratives,2008,17,37,3,0,16937,angus roberts,Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,0,"The Clinical E-Science Framework (CLEF) project has built a system to extract clinically significant information from the textual component of medical records, for clinical research, evidence-based healthcare and genotype-meets-phenotype informatics. One part of this system is the identification of relationships between clinically important entities in the text. Typical approaches to relationship extraction in this domain have used full parses, domain-specific grammars, and large knowledge bases encoding domain knowledge. In other areas of biomedical NLP, statistical machine learning approaches are now routinely applied to relationship extraction. We report on the novel application of these statistical techniques to clinical relationships.n n We describe a supervised machine learning system, trained with a corpus of oncology narratives hand-annotated with clinically important relationships. Various shallow features are extracted from these texts, and used to train statistical classifiers. We compare the suitability of these features for clinical relationship extraction, how extraction varies between inter- and intra-sentential relationships, and examine the amount of training data needed to learn various relationships."
roberts-etal-2008-combining,Combining Terminology Resources and Statistical Methods for Entity Recognition: an Evaluation,2008,20,38,3,0,16937,angus roberts,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Terminologies and other knowledge resources are widely used to aid entity recognition in specialist domain texts. As well as providing lexicons of specialist terms, linkage from the text back to a resource can make additional knowledge available to applications. Use of such resources is especially pertinent in the biomedical domain, where large numbers of these resources are available, and where they are widely used in informatics applications. Terminology resources can be most readily used by simple lexical lookup of terms in the text. A major drawback with such lexical lookup, however, is poor precision caused by ambiguity between domain terms and general language words. We combine lexical lookup with simple filtering of ambiguous terms, to improve precision. We compare this lexical lookup with a statistical method of entity recognition, and to a method which combines the two approaches. We show that the combined method boosts precision with little loss of recall, and that linkage from recognised entities back to the domain knowledge resources can be maintained."
webb-etal-2008-cross,Cross-Domain Dialogue Act Tagging,2008,22,7,3,1,27626,nick webb,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We present recent work in the area of Cross-Domain Dialogue Act (DA) tagging. We have previously reported on the use of a simple dialogue act classifier based on purely intra-utterance features - principally involving word n-gram cue phrases automatically generated from a training corpus. Such a classifier performs surprisingly well, rivalling scores obtained using far more sophisticated language modelling techniques. In this paper, we apply these automatically extracted cues to a new annotated corpus, to determine the portability and generality of the cues we learn."
S07-1014,{S}em{E}val-2007 Task 15: {T}emp{E}val Temporal Relation Identification,2007,4,212,4,0,3193,marc verhagen,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"The TempEval task proposes a simple way to evaluate automatic extraction of temporal relations. It avoids the pitfalls of evaluating a graph of inter-related labels by defining three sub tasks that allow pairwise evaluation of temporal relations. The task not only allows straightforward evaluation, it also avoids the complexities of full temporal parsing."
S07-1098,{USFD}: Preliminary Exploration of Features and Classifiers for the {T}emp{E}val-2007 Task,2007,5,24,1,1,28213,mark hepple,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"We describe the Sheffield system used in TempEval-2007. Our system takes a machine-learning (ML) based approach, treating temporal relation assignment as a simple classification task and using features easily derived from the TempEval data, i.e. which do not require 'deeper' NLP analysis. We aimed to explore three questions: (1) How well would a 'lite' approach of this kind perform? (2) Which features contribute positively to system performance? (3) Which ML algorithm is better suited for the TempEval tasks? We used the Weka ML workbench to facilitate experimenting with different ML algorithms. The paper describes our system and supplies preliminary answers to the above questions."
W05-1527,{SUPPLE}: A Practical Parser for Natural Language Engineering Applications,2005,8,25,2,0.427072,33330,robert gaizauskas,Proceedings of the Ninth International Workshop on Parsing Technology,0,"We describe SUPPLE, a freely-available, open source natural language parsing system, implemented in Prolog, and designed for practical use in language engineering (LE) applications. SUPPLE can be run as a stand-alone application, or as a component within the GATE General Architecture for Text Engineering. SUPPLE is distributed with an example grammar that has been developed over a number of years across several LE projects. This paper describes the key characteristics of the parser and the distributed grammar."
W04-3110,A Large Scale Terminology Resource for Biomedical Text Processing,2004,15,17,3,0,42340,henk harkema,"{HLT}-{NAACL} 2004 Workshop: Linking Biological Literature, Ontologies and Databases",0,"In this paper we discuss the design, implementation, and use of Termino, a large scale terminological resource for text processing. Dealing with terminology is a difficult but unavoidable task for language processing applications, such as Information Extraction in technical domains. Complex, heterogeneous information must be stored about large numbers of terms. At the same time term recognition must be performed in realistic times. Termino attempts to reconcile this tension by maintaining a flexible, extensible relational database for storing terminological information and compiling finite state machines from this database to do term lookup. While Termino has been developed for biomedical applications, its general design allows it to be used for term processing in any domain."
wilks-etal-2004-human,Human Dialogue Modelling Using Annotated Corpora,2004,7,0,4,0,37311,yorick wilks,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We describe two major dialogue system segments: first we describe a Dialogue Manager which uses a representation of stereotypical dialogue patterns that we call Dialogue Action Frames and which, we believe, generate strong and novel constraints on later access to incomplete dialogue topics. Secondly, an analysis module that learns to assign dialogue acts from corpora, but on the basis of limited quantities of data, and up to what seems to be some kind of limit on this task, a fact we also discuss."
harkema-etal-2004-large,A Large-Scale Resource for Storing and Recognizing Technical Terminology,2004,8,5,3,0,42340,henk harkema,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
hepple-etal-2004-nlp,{NLP}-enhanced Content Filtering Within the {POESIA} Project,2004,7,7,1,1,28213,mark hepple,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper introduces the POESIA internet filtering system, which is open-source, and which combines standard filtering methods, such as positive/negative URL lists, with more advanced techniques, such as image processing and NLP-enhanced text filtering. The description here focusses on components providing textual content filtering for three European languages (English, Italian and Spanish), employing NLP methods to enhance performance. We address also the acquisition of language data needed to develop these filters, and the evaluation of the system and its components."
P00-1036,Independence and Commitment: Assumptions for Rapid Training and Execution of Rule-based {POS} Taggers,2000,14,117,1,1,28213,mark hepple,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method."
P99-1060,An {E}arley-style Predictive Chart Parsing Method for {L}ambek Grammars,1999,12,3,1,1,28213,mark hepple,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"We present a new chart parsing method for Lambek grammars, inspired by a method for D-Tree grammar parsing. The formulae of a Lambek sequent are firstly converted into rules of an indexed grammar formalism, which are used in an Earley-style predictive chart algorithm. The method is non-polynomial, but performs well for practical purposes---much better than previous chart methods for Lambek grammars."
W98-0117,On some similarities between {D}-tree grammars and type-logical grammars,1998,0,3,1,1,28213,mark hepple,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,None
P98-1088,Memoisation for Glue Language Deduction and Categorial Parsing,1998,12,6,1,1,28213,mark hepple,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"The multiplicative fragment of linear logic has found a number of applications in computational linguistics: in the glue language approach to LFG semantics, and in the formulation and parsing of various categorial grammars. These applications call for efficient deduction methods. Although a number of deduction methods for multiplicative linear logic are known, none of them are tabular methods, which bring a substantial efficiency gain by avoiding redundant computation (c.f. chart methods in CFG parsing): this paper presents such a method, and discusses its use in relation to the above applications."
P98-1115,Compacting the {P}enn {T}reebank Grammar,1998,5,38,2,0,55364,alexander krotov,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"Treebanks, such as the Penn Treebank (PTB), offer a simple approach to obtaining a broad coverage grammar: one can simply read the grammar off the parse trees in the treebank. While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more treebanked text would be required to obtain a complete grammar, if one exists at some limit. However, we offer an alternative explanation in terms of the underspecification of structures within the treebank. This hypothesis is explored by applying an algorithm to compact the derived grammar by eliminating redundant rules - rules whose right hand sides can be parsed by other rules. The size of the resulting compacted grammar, which is significantly less than that of the full treebank grammar, is shown to approach a limit. However, such a compacted grammar does not yield very good performance figures. A version of the compaction algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated. Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall, but a loss in precision."
C98-1085,Memoisation for Glue Language Deduction and Categorial Parsing,1998,12,6,1,1,28213,mark hepple,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"The multiplicative fragment of linear logic has found a number of applications in computational linguistics: in the glue language approach to LFG semantics, and in the formulation and parsing of various categorial grammars. These applications call for efficient deduction methods. Although a number of deduction methods for multiplicative linear logic are known, none of them are tabular methods, which bring a substantial efficiency gain by avoiding redundant computation (c.f. chart methods in CFG parsing): this paper presents such a method, and discusses its use in relation to the above applications."
C98-1111,Compacting the {P}enn {T}reebank Grammar,1998,5,38,2,0,55364,alexander krotov,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"Treebanks, such as the Penn Treebank (PTB), offer a simple approach to obtaining a broad coverage grammar: one can simply read the grammar off the parse trees in the treebank. While such a grammar is easy to obtain, a square-root rate of growth of the rule set with corpus size suggests that the derived grammar is far from complete and that much more treebanked text would be required to obtain a complete grammar, if one exists at some limit. However, we offer an alternative explanation in terms of the underspecification of structures within the treebank. This hypothesis is explored by applying an algorithm to compact the derived grammar by eliminating redundant rules - rules whose right hand sides can be parsed by other rules. The size of the resulting compacted grammar, which is significantly less than that of the full treebank grammar, is shown to approach a limit. However, such a compacted grammar does not yield very good performance figures. A version of the compaction algorithm taking rule probabilities into account is proposed, which is argued to be more linguistically motivated. Combined with simple thresholding, this method can be used to give a 58% reduction in grammar size without significant change in parsing performance, and can produce a 69% reduction with some gain in recall, but a loss in precision."
P97-1044,Maximal Incrementality in Linear Categorial Deduction,1997,16,1,1,1,28213,mark hepple,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"Recent work has seen the emergence of a common framework for parsing categorial grammar (CG) formalisms that fall within the 'type-logical' tradition (such as the Lambek calculus and related systems), whereby some method of linear logic theorem proving is used in combination with a system of labelling that ensures only deductions appropriate to the relevant grammatical logic are allowed. The approaches realising this framework, however, have not so far addressed the task of incremental parsing --- a key issue in earlier work with 'flexible' categorial grammars. In this paper, the approach of (Hepple, 1996) is modified to yield a linear deduction system that does allow flexible deduction and hence incremental processing, but that hence also suffers the problem of 'spurious ambiguity'. This problem is avoided via normalisation."
C96-1091,A Compilation-Chart Method for Linear Categorial Deduction,1996,13,12,1,1,28213,mark hepple,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"Recent work in categorial grammar has seen proposals for a wide range of systems, differing in their 'resource sensitivity' and hence, implicitly, their underlying notion of 'linguistic structure'. A common framework for parsing such systems is emerging, whereby some method of linear logic theorem proving is used in combination with a system of labelling that ensures that only deductions appropriate to the relevant categorial formalism are allowed. This paper presents a deduction method for implicational linear logic that brings with it the benefit that chart parsing provides for CFG parsing, namely avoiding the need to recompute intermediate results when searching exhaustively for all possible analyses. The method involves compiling possibly higher-order linear formulae to indexed first-order formulae, over which deduction is made using just a single inference rule."
E95-1018,Mixing Modes of Linguistic Description in Categorial Grammar,1995,12,8,1,1,28213,mark hepple,Seventh Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Recent work within the field of Categorial Grammar has seen the development of approaches that allow different modes of logical behaviour to be displayed within a single system, something corresponding to making available differing modes of linguistic description. Earlier attempts to achieve this goal have employed modal operators called structural modalities, whose use presents a number of problems. I propose an alternative approach, involving co-existence and interrelation of different sublogics, that eliminates the need for structural modalities, whilst maintaining the descriptive power they provide."
C94-2201,Discontinuity and the {L}ambek Calculus,1994,10,12,1,1,28213,mark hepple,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
C92-1024,Chart Parsing {L}ambek Grammars: Modal Extensions and Incrementality,1992,11,13,1,1,28213,mark hepple,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper describes a method for chart parsing Lambek grammars. The method is of particular interest in two regards. Firstly, it allows efficient processing of grammars which use necessity operators, an extension proposed for handling locality phenomena. Secondly, the method is easily adapted to allow incremental processing of Lambek grammars, a possibility that has hitherto been unavailable."
P91-1011,Efficient Incremental Processing With Categorial Grammar,1991,15,11,1,1,28213,mark hepple,29th Annual Meeting of the Association for Computational Linguistics,1,"Some problems are discussed that arise for incremental processing using certain flexible categorial grammars, which involve either undesirable parsing properties or failure to allow combinations useful to incrementality. We suggest a new calculus which, though 'designed' in relation to categorial interpretations of some notions of dependency grammar, seems to provide a degree of flexibility that is highly appropriate for incremental interpretation. We demonstrate how this grammar may be used for efficient incremental parsing, by employing normalisation techniques."
E91-1035,Proof Figures and Structural Operators for Categorial Grammar,1991,14,20,2,0,57388,guy barry,Fifth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Use of Lambek's (1958) categorial grammar for linguistic work has generally been rather limited. There appear to be two main reasons for this: the notations most commonly used can sometimes obscure the structure of proofs and fail to clearly convey linguistic structure, and the calculus as it stands is apparently not powerful enough to describe many phenomena encountered in natural language.In this paper we suggest ways of dealing with both these deficiencies. Firstly, we reformulate Lambek's system using proof figures based on the 'natural deduction' notation commonly used for derivations in logic, and discuss some of the related proof-theory. Natural deduction is generally regarded as the most economical and comprehensible system for working on proofs by hand, and we suggest that the same advantages hold for a similar presentation of categorial derivations. Secondly, we introduce devices called structural modalities, based on the structural rules found in logic, for the characterization of commutation, iteration and optionality. This permits the description of linguistic phenomena which Lambek's system does not capture with the desired sensitivity and generality."
C90-2030,Normal Form Theorem Proving for the {L}ambek Calculus,1990,4,18,1,1,28213,mark hepple,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"The possibility of multiple equivalent proofs presents a problem for efficient parsing of a number of flexible categorial grammar (CG) frameworks. In this paper I outline a normal form system for a sequent formulation of the product-free associative Lambek Calculus. This leads to a simple parsing approach that yields only normal form proofs. This approach is both safe in that all distinct readings for a sentence will be returned, and optimal in that there is only one normal form proof yielding each distinct meaning."
E89-1002,Parsing and Derivational Equivalence,1989,13,39,1,1,28213,mark hepple,Fourth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"It is a tacit assumption of much linguistic inquiry that all distinct derivations of a string should assign distinct meanings. But despite the tidiness of such derivational uniqueness, there seems to be no a priori reason to assume that a grammar must have this property. If a grammar exhibits derivational equivalence, whereby distinct derivations of a string assign the same meanings, naive exhaustive search for all derivations will be redundant, and quite possibly intractable. In this paper we show how notions of derivation-reduction and normal form can be used to avoid unnecessary work while parsing with grammars exhibiting derivational equivalence. With grammar regarded as analogous to logic, derivations are proofs; what we are advocating is proof-reduction, and normal form proof; the invocation of these logical techniques adds a further paragraph to the story of parsing-as-deduction."
