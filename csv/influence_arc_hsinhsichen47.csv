1995.iwpt-1.10,1991.iwpt-1.24,0,0.0454779,"Missing"
1995.iwpt-1.10,1991.iwpt-1.18,0,0.0605978,"Missing"
1995.iwpt-1.10,P92-1006,0,0.0615049,"Missing"
1995.iwpt-1.10,E93-1006,0,0.0653521,"Missing"
1995.iwpt-1.10,E93-1040,0,0.0663324,"Missing"
1995.iwpt-1.10,P94-1016,0,0.063555,"Missing"
1995.iwpt-1.10,E95-1038,0,0.0561659,"Missing"
1995.iwpt-1.10,A92-1027,0,0.0657183,"Missing"
1995.iwpt-1.10,H92-1022,0,0.0471104,"Missing"
1995.iwpt-1.10,A94-1009,0,0.020146,"Missing"
1995.iwpt-1.10,A94-1008,0,0.0336708,"Missing"
1995.iwpt-1.10,W94-0103,0,0.0228694,"Missing"
1995.tmi-1.21,J85-2002,0,\N,Missing
1995.tmi-1.21,J90-1003,0,\N,Missing
1995.tmi-1.21,C94-1012,0,\N,Missing
1995.tmi-1.21,J90-2002,0,\N,Missing
1995.tmi-1.21,1991.mtsummit-papers.9,0,\N,Missing
1995.tmi-1.21,O93-1003,1,\N,Missing
2011.mtsummit-papers.31,W09-0432,0,0.376054,"he target domain. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). In their work, domain related parameters were added in standard HMM training to derive an alignment model sensitive to the topic of each sentence. In some applications, bilingual in-domain corpus is unavailable while monolingual one (either source or target side) is relatively easy to acquire. Zhao et al. (2004) combined the baseline language model (LM) with the in-domain LM, which was trained by retrieving documents from large text collections using query models. Besides LM, Bertoldi and Federico (2009) generated a synthetic bilingual corpus from a monolingual one to train a domain-specific TM. Our work is close to the monolingual scenario. Provided with a monolingual in-domain corpus, we adapt our background MT into the one suitable for translating medical summaries. There are some major differences among our work and those proposed previously. First, we identify and translate significant patterns from in-domain corpus and introduce them into our SMT system. Instead, the related works exploited the entire in-domain training data to adapt the existing LM or TM by model mixture and parameter"
2011.mtsummit-papers.31,P05-1033,0,0.0448033,"Missing"
2011.mtsummit-papers.31,W07-0722,0,0.141891,"been proposed. Foster and Kuhn (2007) proposed a mixture-model approach to deal with the case where bilingual in-domain text is available but in a relatively small size. A training corpus was divided into several components to train several models. Each model was weighted to estimate the similarity between components and in-domain development data. Based on this work, Foster et al. (2010) incorporated instance weighting that learned the weights of phrase pairs to capture the degree of relevance to the target domain. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). In their work, domain related parameters were added in standard HMM training to derive an alignment model sensitive to the topic of each sentence. In some applications, bilingual in-domain corpus is unavailable while monolingual one (either source or target side) is relatively easy to acquire. Zhao et al. (2004) combined the baseline language model (LM) with the in-domain LM, which was trained by retrieving documents from large text collections using query models. Besides LM, Bertoldi and Federico (2009) generated a synthetic bilingual corpus from a monolingual one to train a domain-specific"
2011.mtsummit-papers.31,W07-0717,0,0.0317757,"nguistic patterns. In recent years, training SMT system from large bilingual data has been a common practice. The parallel corpus used to train an MT system mainly comes from fixed domains such as parliamentary and news articles. The bilingual resources for a specific language pair or a specific domain usually come in small size, even unavailable. One of the challenging issues in a cross-domain MT applica278 tion is to realize an in-domain MT model in such a resource-poor environment. Depending on what kinds of in-domain resources are at hand, various adaptation techniques have been proposed. Foster and Kuhn (2007) proposed a mixture-model approach to deal with the case where bilingual in-domain text is available but in a relatively small size. A training corpus was divided into several components to train several models. Each model was weighted to estimate the similarity between components and in-domain development data. Based on this work, Foster et al. (2010) incorporated instance weighting that learned the weights of phrase pairs to capture the degree of relevance to the target domain. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). In their work,"
2011.mtsummit-papers.31,D10-1044,0,0.0601468,"ng issues in a cross-domain MT applica278 tion is to realize an in-domain MT model in such a resource-poor environment. Depending on what kinds of in-domain resources are at hand, various adaptation techniques have been proposed. Foster and Kuhn (2007) proposed a mixture-model approach to deal with the case where bilingual in-domain text is available but in a relatively small size. A training corpus was divided into several components to train several models. Each model was weighted to estimate the similarity between components and in-domain development data. Based on this work, Foster et al. (2010) incorporated instance weighting that learned the weights of phrase pairs to capture the degree of relevance to the target domain. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). In their work, domain related parameters were added in standard HMM training to derive an alignment model sensitive to the topic of each sentence. In some applications, bilingual in-domain corpus is unavailable while monolingual one (either source or target side) is relatively easy to acquire. Zhao et al. (2004) combined the baseline language model (LM) with the in-"
2011.mtsummit-papers.31,1985.tmi-1.10,0,0.676381,"Missing"
2011.mtsummit-papers.31,P03-1054,0,0.00713775,"rd in favor of our general purpose parser. For instance, we replace a complicated diagnosis ""primary biliary cirrhosis"" with the simpler one ""disease"". In this way, we reduce not only the OOV words, but also the length of sentences, and thereby facilitate the parsing procedure. For each extracted pattern, we select m distinct sentences in which it occurs. These sentences are then analyzed by a parser and m parsing trees are produced. The pattern is considered as a significant candidate, if it is a syntactic constituent in any one of these parsing trees. In this paper we apply Stanford Parser (Klein and Manning, 2003) and set m to 10 in consideration of the parsing speed. 4.4 Pattern Coverage Finding and Filtering Coverage Relation 1+4 2+3 3+2 4+1 Examples she is admitted for SURGERY she is admitted for SURGERY Lab data showed no DIAGNOSIS Lab data showed no DIAGNOSIS TEST on DATE showed DIAGNOSIS TEST on DATE showed DIAGNOSIS Past Surgical History : SURGERY Past Surgical History : SURGERY Table 1. Four kinds of coverage relations for 5-grams. most representative pattern may imply the translations of the others in the same cluster. An example of a cluster of similar patterns is illustrated below: The invol"
2011.mtsummit-papers.31,N03-1017,0,0.0165064,"e examine each sentence from left to right and adopt a longest-first strategy to replace medical terms with classes. In this way, a set of medical summaries are transformed into a new corpus. 4.2 Highly Frequent Pattern Identification To address the domain adaptation problem in MT, we extract patterns from an in-domain corpus to capture domain specific writing styles. These patterns are translated and will be applied in the runtime translation. Accordingly, we prefer the 280 format of patterns that is easy to be integrated into an SMT system for our target application. The phrase-based model (Koehn et al., 2003; Koehn, 2004) is one of the state of the art TMs, in terms of both accuracy and speed. The phrasebased system translates source phrases into target ones with phrase table, which consists of bilingual phrases and feature functions. Since a phrase (i.e., a string of consecutive words) is served as the basic unit of translation, integrating n-gram based patterns into the background phrase-based SMT system is a natural choice. We enumerate all n-grams from sentences of our in-domain corpus that contains words and medical classes. In this way, two kinds of patterns are extracted: (1) class pattern"
2011.mtsummit-papers.31,koen-2004-pharaoh,0,0.148043,"nce from left to right and adopt a longest-first strategy to replace medical terms with classes. In this way, a set of medical summaries are transformed into a new corpus. 4.2 Highly Frequent Pattern Identification To address the domain adaptation problem in MT, we extract patterns from an in-domain corpus to capture domain specific writing styles. These patterns are translated and will be applied in the runtime translation. Accordingly, we prefer the 280 format of patterns that is easy to be integrated into an SMT system for our target application. The phrase-based model (Koehn et al., 2003; Koehn, 2004) is one of the state of the art TMs, in terms of both accuracy and speed. The phrasebased system translates source phrases into target ones with phrase table, which consists of bilingual phrases and feature functions. Since a phrase (i.e., a string of consecutive words) is served as the basic unit of translation, integrating n-gram based patterns into the background phrase-based SMT system is a natural choice. We enumerate all n-grams from sentences of our in-domain corpus that contains words and medical classes. In this way, two kinds of patterns are extracted: (1) class patterns that contain"
2011.mtsummit-papers.31,P07-2045,0,0.00788532,"cal summaries. Google Translate reports its Chinese translation as ""ཽՑ ਢԫଡཬԵՂച۩ 2009  ڣ10 ִ 9 ֲ"". Compared to the Chinese reference translation "" ڇ2009  ڣ10 ִ 9 ֲച۩Գՠۨጥᆜ"", there are several translation and reordering errors. In this paper, we develop an English-Chinese medical summary translation system to tackle this problem. In an SMT model, an English-Chinese parallel corpus is indispensable for training the translation model (TM). However, only English medical summary corpus is available in this domain. Here, we first develop a general EnglishChinese SMT system with Moses toolkit (Koehn et al., 2007) and a general English-Chinese parallel corpus. Next, we adapt this SMT system with patterns learned from an English medical summary corpus. The problem is that these patterns are still monolingual. It is necessary to have domain expert involved in setting up bilingual patterns. The cost of domain experts is the major concerns. Therefore, to identify significant patterns from a monolingual in-domain corpus, to find their coverage relationships, to decide which patterns should be translated by experts, and to introduce these patterns to the general MT system are research issues in this paper. S"
2011.mtsummit-papers.31,P06-1066,0,0.248102,"Missing"
2011.mtsummit-papers.31,C04-1059,0,0.055875,"in development data. Based on this work, Foster et al. (2010) incorporated instance weighting that learned the weights of phrase pairs to capture the degree of relevance to the target domain. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). In their work, domain related parameters were added in standard HMM training to derive an alignment model sensitive to the topic of each sentence. In some applications, bilingual in-domain corpus is unavailable while monolingual one (either source or target side) is relatively easy to acquire. Zhao et al. (2004) combined the baseline language model (LM) with the in-domain LM, which was trained by retrieving documents from large text collections using query models. Besides LM, Bertoldi and Federico (2009) generated a synthetic bilingual corpus from a monolingual one to train a domain-specific TM. Our work is close to the monolingual scenario. Provided with a monolingual in-domain corpus, we adapt our background MT into the one suitable for translating medical summaries. There are some major differences among our work and those proposed previously. First, we identify and translate significant patterns"
2020.acl-main.13,P16-1139,0,0.0275195,"Chinese Discourse Treebank (Zhou and Xue, 2012), which is annotated with the PDTB-style for shallow discourse parsing, we use the term CDTB-14 to refer to the RST-style one and the term CDTB-12 to refer to the PDTB-style one. Kong and Zhou (2017) propose a pipeline framework and generate the discourse parsing tree in a bottom-up way. Lin et al. (2018) propose an end-to-end system based on a recursive neural network (RvNN) to construct the parsing tree with a CKY-like algorithm. Sun and Kong (2018) use transition-based method with the stack augmented parser-interpreter neural network (SPINN) (Bowman et al., 2016) as the backbone model, helping their model make a better prediction with the previous information. In this work, we attempt to construct a complete Chinese discourse parser, which supports all the four sub-tasks in hierarchical discourse parsing, including EDU segmentation, tree structure construction, nuclearity labeling, and rhetorical relation recognition. Given a paragraph, our parser extracts all EDUs, builds the tree structure, identifies the nucleuses, and recognizes the rhetorical relations of all internal nodes. We propose a revised dynamic-oracle procedure (Yu et al., 2018) for trai"
2020.acl-main.13,W01-1605,0,0.740536,"ion Discourse parsing is one of the fundamental tasks in natural language processing (NLP). Typical types of discourse parsing include hierarchical discourse parsing and shallow discourse parsing. The former is aimed at finding the relationships among a series of neighboring elementary discourse units (EDUs) and further building up a hierarchical tree structure (Mann and Thompson, 1988). Instead of establishing a tree structure, the latter finds the across-paragraph relations between all text units in a paragraph or a document. Based on Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001a), hierarchical discourse parsing in English has been well-studied. This paper focuses on hierarchical discourse parsing in Chinese. Previous work on hierarchical Chinese discourse parsing is mostly based on the RST-style Chinese Discourse Treebank (Li et al., 2014). To distinguish from the other Chinese Discourse Treebank (Zhou and Xue, 2012), which is annotated with the PDTB-style for shallow discourse parsing, we use the term CDTB-14 to refer to the RST-style one and the term CDTB-12 to refer to the PDTB-style one. Kong and Zhou (2017) propose a pipeline framework and generate the discours"
2020.acl-main.13,D14-1224,0,0.0188203,"ring elementary discourse units (EDUs) and further building up a hierarchical tree structure (Mann and Thompson, 1988). Instead of establishing a tree structure, the latter finds the across-paragraph relations between all text units in a paragraph or a document. Based on Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001a), hierarchical discourse parsing in English has been well-studied. This paper focuses on hierarchical discourse parsing in Chinese. Previous work on hierarchical Chinese discourse parsing is mostly based on the RST-style Chinese Discourse Treebank (Li et al., 2014). To distinguish from the other Chinese Discourse Treebank (Zhou and Xue, 2012), which is annotated with the PDTB-style for shallow discourse parsing, we use the term CDTB-14 to refer to the RST-style one and the term CDTB-12 to refer to the PDTB-style one. Kong and Zhou (2017) propose a pipeline framework and generate the discourse parsing tree in a bottom-up way. Lin et al. (2018) propose an end-to-end system based on a recursive neural network (RvNN) to construct the parsing tree with a CKY-like algorithm. Sun and Kong (2018) use transition-based method with the stack augmented parser-inter"
2020.acl-main.13,C18-2016,1,0.917828,"in English has been well-studied. This paper focuses on hierarchical discourse parsing in Chinese. Previous work on hierarchical Chinese discourse parsing is mostly based on the RST-style Chinese Discourse Treebank (Li et al., 2014). To distinguish from the other Chinese Discourse Treebank (Zhou and Xue, 2012), which is annotated with the PDTB-style for shallow discourse parsing, we use the term CDTB-14 to refer to the RST-style one and the term CDTB-12 to refer to the PDTB-style one. Kong and Zhou (2017) propose a pipeline framework and generate the discourse parsing tree in a bottom-up way. Lin et al. (2018) propose an end-to-end system based on a recursive neural network (RvNN) to construct the parsing tree with a CKY-like algorithm. Sun and Kong (2018) use transition-based method with the stack augmented parser-interpreter neural network (SPINN) (Bowman et al., 2016) as the backbone model, helping their model make a better prediction with the previous information. In this work, we attempt to construct a complete Chinese discourse parser, which supports all the four sub-tasks in hierarchical discourse parsing, including EDU segmentation, tree structure construction, nuclearity labeling, and rhet"
2020.acl-main.13,J18-2001,0,0.0310017,"Missing"
2020.acl-main.13,N18-1202,0,0.0148654,"2020. 2020 Association for Computational Linguistics 3. We release the pre-trained, standalone, readyto-use parser as a resource for the research community.1 2 Converter Methodology Sense , Center Figure 1 gives an overview of our parser. Five stages are performed to transform a raw document into a parse tree: EDU segmentation, tree structure construction, rhetorical relation and nuclearity classification, binary tree conversion, and beam search. Reduce Classifier queue stack 2.1 Elementary Discourse Unit Segmentation Typically, EDU segmentation is a sequence labeling task (Wang et al., 2018; Peters et al., 2018). We propose a model for labeling each Chinese character in a raw document. The Begin-Inside scheme is employed that the word beginning with a new EDU will be labeled as B, and the rest of the words will be labeled as I. Our model is based on the pretrained text encoder BERT (Devlin et al., 2018). More specifically, we adopt the version BERT-base, Chinese since this is the only pre-trained BERT dedicated to Chinese so far. As the BERT for Chinese is character-based, we feed each Chinese character into a BERT layer to obtain its contextual embedding. Then, we fine tune the representation with a"
2020.acl-main.13,P16-1009,0,0.0247011,"Given a span of text x, our main model P (·) predicts the rhetorical relation yc . Eq. 2 shows the additional consistency loss to enforce the smoothness of our main model, and x ˆ stands for the augmented unlabeled sentence pair. L and U stand for labeled data and unlabeled data, respectively. As shown in Eq. 3, we train both objectives at the same time with a weight λ to adjust the effect of UDA. N M 1 XX H=− yc log (P (yc |x)) (1) N x∈L c=1   N 1 X P (y|x) DKL = − P (y|x) log (2) N P (y|ˆ x) proaches to paraphrasing can be employed. In this work, we utilize the back-translation strategy (Sennrich et al., 2016), where we translate the Chinese sentence pair to English and then translate back to Chinese. This is equivalent to add noises to the original inputs. As the original and the backtranslated sentence pairs express the same meaning, our model is expected to predict the same label for both pairs. By minimizing the consistency loss, our model can behave consistently no matter whether an original instance or its paraphrases are given. In this way, the model can be more generalized and robust. Besides, when our model is able to predict the same label for both sentence pairs, it means that our model"
2020.acl-main.13,D18-1116,0,0.0346168,"Missing"
2020.acl-main.13,D16-1137,0,0.032053,"Missing"
2020.acl-main.13,C18-1047,0,0.368036,"NN) (Bowman et al., 2016) as the backbone model, helping their model make a better prediction with the previous information. In this work, we attempt to construct a complete Chinese discourse parser, which supports all the four sub-tasks in hierarchical discourse parsing, including EDU segmentation, tree structure construction, nuclearity labeling, and rhetorical relation recognition. Given a paragraph, our parser extracts all EDUs, builds the tree structure, identifies the nucleuses, and recognizes the rhetorical relations of all internal nodes. We propose a revised dynamic-oracle procedure (Yu et al., 2018) for training the shift-reduce parser. Because of the limited training instances in CDTB-14, we also address the data sparsity issue by introducing unsupervised data augmentation (Xie et al., 2019). Experimental results show that our methodology is effective, and our model outperforms all the previous models. The contributions of this work are three-fold shown as follows. 1. We explore the task of Chinese discourse parsing with a variety of strategies, and our parser achieves the state-of-the-art performance. Our robust dynamic-oracle procedure can be applied to other shift-reduce parsers. 2."
2020.acl-main.13,P12-1008,0,0.0270232,"l tree structure (Mann and Thompson, 1988). Instead of establishing a tree structure, the latter finds the across-paragraph relations between all text units in a paragraph or a document. Based on Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001a), hierarchical discourse parsing in English has been well-studied. This paper focuses on hierarchical discourse parsing in Chinese. Previous work on hierarchical Chinese discourse parsing is mostly based on the RST-style Chinese Discourse Treebank (Li et al., 2014). To distinguish from the other Chinese Discourse Treebank (Zhou and Xue, 2012), which is annotated with the PDTB-style for shallow discourse parsing, we use the term CDTB-14 to refer to the RST-style one and the term CDTB-12 to refer to the PDTB-style one. Kong and Zhou (2017) propose a pipeline framework and generate the discourse parsing tree in a bottom-up way. Lin et al. (2018) propose an end-to-end system based on a recursive neural network (RvNN) to construct the parsing tree with a CKY-like algorithm. Sun and Kong (2018) use transition-based method with the stack augmented parser-interpreter neural network (SPINN) (Bowman et al., 2016) as the backbone model, help"
2020.coling-main.199,P17-1074,0,0.158633,"ng in a missing punctuation mark. Each family of model is adept at correcting different kinds of errors. By integrating these two kinds of models using recycle generation, a wider range of errors can be effectively corrected for each round of correction. In this work, we also discuss the performance metrics of Chinese GEC. The Maxmatch (M2) scorer (Dahlmeier and Ng, 2012) that has been extensively used for English GEC, and also for the NLPCC 2018 Chinese GEC task can only report overall model performance. This problem has been solved in English GEC, with the introduction of the ERRANT scorer (Bryant et al., 2017). The ERRANT scorer can provide model performance in terms of edit-level operation as well as specific English grammatical error types. We extend the idea of the ERRANT scorer to deal with Chinese sentences. This will allow Chinese GEC researchers to be able to get more detailed analysis of model performance. In summary, our contributions are threefold as follows. 1. We use a heterogeneous system composed of multiple kinds of models for Chinese GEC, beating the previous state-of-the-art results on the NLPCC 2018 task dataset. Combining multiple models that are designed to correct different kin"
2020.coling-main.199,W19-4406,0,0.0106568,"ntences. 1 Introduction Grammatical error correction (GEC) is the task of correcting grammatical and spelling errors that appear in a sentence. An example of Chinese GEC is correcting the word-choice error in the following sentence: 本人是在貴公司的一名實習。 (I am an internship at your company.) by changing the word 實習 (internship) to 實習生 (intern), resulting in the corrected sentence: 本人是在貴公司的一名實習生。 (I am an intern at your company.) In recent years, there has been a great deal of GEC related research for English, most notably with the CoNLL 2014 shared task (Ng et al., 2014) and the BEA-2019 shared task (Bryant et al., 2019). Chinese GEC has a much shorter history, with the NLPCC 2018 shared task (Zhao et al., 2018) being the first to focus on this research topic. Most work prior to the NLPCC 2018 shared task focused on correcting only one type of error, such as preposition errors (Huang et al., 2016) or Chinese spelling error correction (Wu et al., 2013). Most recent work in GEC formulate correction as a translation task, and use neural machine translation (NMT) based models. That is, models are trained to translate an erroneous source sentence into a corrected target sentence. A considerable disadvantage of thi"
2020.coling-main.199,I05-2023,0,0.0677592,"th edits. We then adapt the original sentence alignment code from ERRANT 8 to be able to align Chinese sentences. Edit Extraction The per-sentence edits (annotations) are extracted in three steps: tokenization, alignment, and merging. For tokenization, we use character-level tokenization. Alignment works by computing an alignment score for each pair of characters in the source and target sentence. Matching characters 8 https://github.com/chrisjbryant/errant 2198 give a score of zero. Insertion and deletion is each given a score of one. Substitution score is computed using a method similar to (Che et al., 2005), in which the Cilin (Mei et al., 1996) thesaurus is leveraged to give a similarity score between characters. Once the alignment score matrix is computed, the sequence with the lowest total score is returned. We use a simple merging strategy that merges consecutive sequences of edits of the same type. 5.2 Error-type Specific Performance The results of re-scoring our systems with our scorer is presented in Table 6. We can see some clear differences in the strengths and weaknesses of our models. The NMT-based Transformer is best at correcting general substitution and word-ordering errors. The se"
2020.coling-main.199,N12-1067,0,0.235333,"re capable of rewriting the entire sentence, making large scale corrections such as re-ordering or performing multi-word substitutions possible. In contrast, sequence editing models focus on smaller scale corrections, such as removing a word or adding in a missing punctuation mark. Each family of model is adept at correcting different kinds of errors. By integrating these two kinds of models using recycle generation, a wider range of errors can be effectively corrected for each round of correction. In this work, we also discuss the performance metrics of Chinese GEC. The Maxmatch (M2) scorer (Dahlmeier and Ng, 2012) that has been extensively used for English GEC, and also for the NLPCC 2018 Chinese GEC task can only report overall model performance. This problem has been solved in English GEC, with the introduction of the ERRANT scorer (Bryant et al., 2017). The ERRANT scorer can provide model performance in terms of edit-level operation as well as specific English grammatical error types. We extend the idea of the ERRANT scorer to deal with Chinese sentences. This will allow Chinese GEC researchers to be able to get more detailed analysis of model performance. In summary, our contributions are threefold"
2020.coling-main.199,P19-1331,0,0.0287622,"h applying a fixed set of operations to the input. This can be formulated in the following way: given a fixed vocabulary of edit operations E and an input sequence x1:n = (x1 , · · · , xn ), a model learns to predict an edit operation ei ∈ E for each xi in our input sequence. A set of rules can then be applied to the output sequence e1:n to obtain the target output sequence y. 2192 Several sequence editing models have been proposed for text simplification tasks. The Levenshtein Transformer (Gu et al., 2019) performs text simplification by using a sequence of insertion and deletion operations. Dong et al. (2019) perform sequence editing through three primary edit operations, KEEP, ADD, and DELETE. Currently, the only sequence editing model to be applied to GEC is LaserTagger (Malmi et al., 2019). Similarly to the two previously cited works, LaserTagger learns to edit sentences by two different edit operations: KEEP and DELETE, along with pairing these operations with a limited phrase vocabulary consisting of tokens that are frequently changed between the source and target sequences. While LaserTagger performed well for English GEC considering the small number of training samples that it used, it was"
2020.coling-main.199,N15-1060,0,0.0175025,"ypes, we have to be able to calculate model performance specific to each of our four error types: redundant (R), missing (M), word selection (S), and word ordering (W). Unfortunately, the official Maxmatch (M2) scorer (Dahlmeier and Ng, 2012) does not have this capability. It can only provide the overall score of the system. In order to rectify this situation, we develop a scorer that can report the precision, recall, and F0.5 score of our models with respect to our four error types. Another issue with the MaxMatch scorer worth mentioning is that it is known to overestimate model performance (Felice and Briscoe, 2015; Napoles et al., 2015). The problem of error-type specific performance evaluation has already been solved for English GEC. The ERRANT scorer (Bryant et al., 2017) provides very detailed performance results for English GEC systems, by reporting the precision, recall, and F0.5 scores with respect to over twenty-five different error categories. The ERRANT scorer works in two main steps, (1) edit extraction (annotation) and (2) scoring. The edit extraction step works in the following way. First, the source and target sentences are tokenized and the part-of-speech for each token is computed. Next,"
2020.coling-main.199,N18-2046,0,0.0151312,"the small number of training samples that it used, it was still very far from reaching state-of-the-art performance. In this work, we apply LaserTagger to Chinese GEC, and also explore combining it with NMT-based models. 2.3 Recycle Generation Recycle generation refers to the method of performing multiple rounds of correction on an input sentence. Recycle generation is also known as iterative decoding or multi-pass decoding. This has been attempted in English GEC in which one NMT-based model is used repeatedly (Lichtarge et al., 2018), or with a combination of a SMT-based and NMT-based model (Grundkiewicz and Junczys-Dowmunt, 2018). In Chinese GEC, only NMT-based recycle generation has been used (Qiu and Qu, 2019). In previous works, recycle generation has always been performed with models trained to do translation. In this work, we attempt to perform recycle generation with one model trained to do translation and another model trained to do sequence editing. 3 Methodology Our GEC system is composed of three separate components: a neural machine translation system, a sequence editing system, and a spell-checker. Each model performs one or several rounds of correction on the input sentence to produce the final corrected"
2020.coling-main.199,W19-4427,0,0.164942,"ion (Wu et al., 2013). Most recent work in GEC formulate correction as a translation task, and use neural machine translation (NMT) based models. That is, models are trained to translate an erroneous source sentence into a corrected target sentence. A considerable disadvantage of this approach is that NMT-based systems require an enormous amount of training data to achieve good results, while the availability of parallel correction data is limited in many languages. The current leading methods for English GEC both rely on pre-training models with a large amount of artificially generated data (Grundkiewicz et al., 2019; Kiyono et al., 2019). In this work, we aim to avoid this issue by combining several different models that perform corrections in different ways. Another challenge of GEC is that sentences can have multiple errors. Sometimes a model is not able to correct all of the errors present in a sentence in one pass, resulting in only a partial correction. One of the methods used to resolve this issue is recycle generation, also known as iterative decoding (Lichtarge et al., 2018). In this method, a system performs multiple iterations of correction on an erroneous sentence. This work is licensed under"
2020.coling-main.199,C16-1085,1,0.82935,") by changing the word 實習 (internship) to 實習生 (intern), resulting in the corrected sentence: 本人是在貴公司的一名實習生。 (I am an intern at your company.) In recent years, there has been a great deal of GEC related research for English, most notably with the CoNLL 2014 shared task (Ng et al., 2014) and the BEA-2019 shared task (Bryant et al., 2019). Chinese GEC has a much shorter history, with the NLPCC 2018 shared task (Zhao et al., 2018) being the first to focus on this research topic. Most work prior to the NLPCC 2018 shared task focused on correcting only one type of error, such as preposition errors (Huang et al., 2016) or Chinese spelling error correction (Wu et al., 2013). Most recent work in GEC formulate correction as a translation task, and use neural machine translation (NMT) based models. That is, models are trained to translate an erroneous source sentence into a corrected target sentence. A considerable disadvantage of this approach is that NMT-based systems require an enormous amount of training data to achieve good results, while the availability of parallel correction data is limited in many languages. The current leading methods for English GEC both rely on pre-training models with a large amoun"
2020.coling-main.199,D19-1119,0,0.121187,"recent work in GEC formulate correction as a translation task, and use neural machine translation (NMT) based models. That is, models are trained to translate an erroneous source sentence into a corrected target sentence. A considerable disadvantage of this approach is that NMT-based systems require an enormous amount of training data to achieve good results, while the availability of parallel correction data is limited in many languages. The current leading methods for English GEC both rely on pre-training models with a large amount of artificially generated data (Grundkiewicz et al., 2019; Kiyono et al., 2019). In this work, we aim to avoid this issue by combining several different models that perform corrections in different ways. Another challenge of GEC is that sentences can have multiple errors. Sometimes a model is not able to correct all of the errors present in a sentence in one pass, resulting in only a partial correction. One of the methods used to resolve this issue is recycle generation, also known as iterative decoding (Lichtarge et al., 2018). In this method, a system performs multiple iterations of correction on an erroneous sentence. This work is licensed under a Creative Commons Att"
2020.coling-main.199,D19-1510,0,0.061409,"n ), a model learns to predict an edit operation ei ∈ E for each xi in our input sequence. A set of rules can then be applied to the output sequence e1:n to obtain the target output sequence y. 2192 Several sequence editing models have been proposed for text simplification tasks. The Levenshtein Transformer (Gu et al., 2019) performs text simplification by using a sequence of insertion and deletion operations. Dong et al. (2019) perform sequence editing through three primary edit operations, KEEP, ADD, and DELETE. Currently, the only sequence editing model to be applied to GEC is LaserTagger (Malmi et al., 2019). Similarly to the two previously cited works, LaserTagger learns to edit sentences by two different edit operations: KEEP and DELETE, along with pairing these operations with a limited phrase vocabulary consisting of tokens that are frequently changed between the source and target sequences. While LaserTagger performed well for English GEC considering the small number of training samples that it used, it was still very far from reaching state-of-the-art performance. In this work, we apply LaserTagger to Chinese GEC, and also explore combining it with NMT-based models. 2.3 Recycle Generation R"
2020.coling-main.199,P15-2097,0,0.0181601,"o calculate model performance specific to each of our four error types: redundant (R), missing (M), word selection (S), and word ordering (W). Unfortunately, the official Maxmatch (M2) scorer (Dahlmeier and Ng, 2012) does not have this capability. It can only provide the overall score of the system. In order to rectify this situation, we develop a scorer that can report the precision, recall, and F0.5 score of our models with respect to our four error types. Another issue with the MaxMatch scorer worth mentioning is that it is known to overestimate model performance (Felice and Briscoe, 2015; Napoles et al., 2015). The problem of error-type specific performance evaluation has already been solved for English GEC. The ERRANT scorer (Bryant et al., 2017) provides very detailed performance results for English GEC systems, by reporting the precision, recall, and F0.5 scores with respect to over twenty-five different error categories. The ERRANT scorer works in two main steps, (1) edit extraction (annotation) and (2) scoring. The edit extraction step works in the following way. First, the source and target sentences are tokenized and the part-of-speech for each token is computed. Next, the resulting tokenize"
2020.coling-main.199,W14-1701,0,0.0269174,"he ERRANT scorer to be able to score Chinese sentences. 1 Introduction Grammatical error correction (GEC) is the task of correcting grammatical and spelling errors that appear in a sentence. An example of Chinese GEC is correcting the word-choice error in the following sentence: 本人是在貴公司的一名實習。 (I am an internship at your company.) by changing the word 實習 (internship) to 實習生 (intern), resulting in the corrected sentence: 本人是在貴公司的一名實習生。 (I am an intern at your company.) In recent years, there has been a great deal of GEC related research for English, most notably with the CoNLL 2014 shared task (Ng et al., 2014) and the BEA-2019 shared task (Bryant et al., 2019). Chinese GEC has a much shorter history, with the NLPCC 2018 shared task (Zhao et al., 2018) being the first to focus on this research topic. Most work prior to the NLPCC 2018 shared task focused on correcting only one type of error, such as preposition errors (Huang et al., 2016) or Chinese spelling error correction (Wu et al., 2013). Most recent work in GEC formulate correction as a translation task, and use neural machine translation (NMT) based models. That is, models are trained to translate an erroneous source sentence into a corrected"
2020.coling-main.199,N19-4009,0,0.115516,"， 一@@ 男@@ 一@@ 女 Table 3: Example of preprocessing steps. 3.2 Neural Machine Translation Model Our NMT model, based on the Transformer architecture (Vaswani et al., 2017), is an encoder-decoder sequence to sequence model, where both the encoder and decoder are composed of six layers of selfattention modules. We use the “Transformer (big)” settings described in Vaswani et al. (2017). In general, we follow similar training steps as described in English state-of-the-art models (Kiyono et al., 2019; Grundkiewicz et al., 2019). Training Settings Our model is implemented using the Fairseq5 toolkit (Ott et al., 2019). Optimization is performed using the Adam (Kingma and Ba, 2014) optimizer, with criterion set to label-smoothed cross entropy (Szegedy et al., 2016). We use beta values of 0.9 and 0.98 for Adam, and a smoothing value of 0.1 for the criterion. We first set the learning rate to 10−7 and perform 4,000 warm-up updates. After the warm-up period, the learning rate is increased to 0.001. Thereafter we use an inverse square 2 https://github.com/fxsjy/jieba https://github.com/rsennrich/subword-nmt 4 https://github.com/BYVoid/OpenCC 5 https://github.com/pytorch/fairseq 3 2194 Figure 1: An example of La"
2020.coling-main.199,P16-1162,0,0.0310895,"n pairs. Since an official validation set is not provided, we randomly select 5,000 pairs from the training set to serve as a validation set. In addition to 1 https://lang-8.com/ 2193 samples from lang8, we also use monolingual WMT News data (Barrault et al., 2019) to form a training set for the language model that we use in our spell checker. All three models that we use require different preprocessing steps, due to several different reasons. The NMT-based model uses standard preprocessing steps: word-level tokenization followed by subword segmentation of rare words using byte pair encoding (Sennrich et al., 2016) to handle out of vocabulary words. Word-level segmentation is performed using Jieba.2 BPE is performed using subword-nmt,3 with the number of merge operations set to 35k and the vocabulary threshold set to 50. The language model cannot use subword units because the spelling check algorithm that we use requires looking up words in a dictionary. The sequence editing model has better results with character-level segmentation because the algorithm it uses to build its vocabulary is sensitive to any noise introduced by incorrect segmentation that often occurs in the erroneous source sentences. For"
2020.coling-main.199,W13-4406,0,0.0243482,"sulting in the corrected sentence: 本人是在貴公司的一名實習生。 (I am an intern at your company.) In recent years, there has been a great deal of GEC related research for English, most notably with the CoNLL 2014 shared task (Ng et al., 2014) and the BEA-2019 shared task (Bryant et al., 2019). Chinese GEC has a much shorter history, with the NLPCC 2018 shared task (Zhao et al., 2018) being the first to focus on this research topic. Most work prior to the NLPCC 2018 shared task focused on correcting only one type of error, such as preposition errors (Huang et al., 2016) or Chinese spelling error correction (Wu et al., 2013). Most recent work in GEC formulate correction as a translation task, and use neural machine translation (NMT) based models. That is, models are trained to translate an erroneous source sentence into a corrected target sentence. A considerable disadvantage of this approach is that NMT-based systems require an enormous amount of training data to achieve good results, while the availability of parallel correction data is limited in many languages. The current leading methods for English GEC both rely on pre-training models with a large amount of artificially generated data (Grundkiewicz et al.,"
2020.coling-main.199,N19-1014,0,0.0190484,"gger’s small phrase vocabulary will not be able to be corrected by it. Currently, most word-order errors (W) are also not able to be corrected by LaserTagger. This is because LaserTagger lacks a method to re-arrange arbitrary subsequences of tokens inside of a sequence. The main advantage LaserTagger has over NMT models is that it is very easy for LaserTagger to copy or delete a token from the source sentence to the output sentence. This is very useful in GEC, as errors are usually limited to just a few words in the source sentence, so most words can be copied directly to the target sentence (Zhao et al., 2019). 6 Conclusion In this work, we propose a system for Chinese GEC that uses three different models: a NMT-based model, a sequence editing model, and a spell checker. We showed how these models can be composed using heterogeneous recycle generation into a system that achieves state of the art performance for Chinese GEC. Furthermore, we extended an automatic annotator and scorer for parallel error corpora to be able to handle Chinese sentences. We use this scorer to evaluate each models performance in terms of the four error types, and show how each model is adept at correcting different types o"
2020.fnp-1.11,D19-1238,0,0.046511,"Missing"
2020.fnp-1.11,W19-5510,0,0.0135367,"in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our code for academic usage. 1 Introduction Adopting causality information as features can benefit lots of applications such as question answering (Sharp et al., 2016), event prediction (Balashankar et al., 2019), and medical text mining (Khoo et al., 2000). In the financial domain, causality detection can be applied to stock movement prediction (Balashankar et al., 2019) and supporting financial services (Izumi and Sakaji, 2019). To better explain the causality that occurs between financial events, cause-effect detection is a fundamental research issue. Taking a close look to financial documents, we find that there may exist multiple causal events and multiple causal chains in a paragraph. In such a case, traditional extraction methods like discourse parser are not feasible. In order to deal with this issue, we formulate the cause-effect detection task as a sequence labeling problem and propose an approach using BIO scheme and Viterbi decoder. We find that the proposed approach has the ability to identify multiple ca"
2020.fnp-1.11,P00-1043,0,0.322526,"etection in financial news and propose an approach, which combines the BIO scheme with the Viterbi decoder for addressing this challenge. Our approach is ranked the first in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our code for academic usage. 1 Introduction Adopting causality information as features can benefit lots of applications such as question answering (Sharp et al., 2016), event prediction (Balashankar et al., 2019), and medical text mining (Khoo et al., 2000). In the financial domain, causality detection can be applied to stock movement prediction (Balashankar et al., 2019) and supporting financial services (Izumi and Sakaji, 2019). To better explain the causality that occurs between financial events, cause-effect detection is a fundamental research issue. Taking a close look to financial documents, we find that there may exist multiple causal events and multiple causal chains in a paragraph. In such a case, traditional extraction methods like discourse parser are not feasible. In order to deal with this issue, we formulate the cause-effect detect"
2020.fnp-1.11,P14-5010,0,0.00244555,"ns are two-fold as follows. 1. We propose an approach to cause-effect detection for financial news that could better identify multiple causal events and event spans. 2. We release the code of the best-performing model for future research.1 2 Pre-processing We experiment on the FinCausal-2020 dataset (Mariko et al., 2020), which consists of two subtasks, including causal meanings detection (Task 1) and cause-effect detection (Task 2). The numbers of training instances are 22,058 and 1,750 for Task 1 and Task 2, respectively. This work only focuses on Task 2. We use the Stanford CoreNLP Stanza (Manning et al., 2014; Qi et al., 2020) toolkit2 to tokenize each sentence and generate the part-of-speech (POS) tag for each token. For the examples with multiple causal events, we recognize them by their indices and add a special number token before each example to treat them as different model inputs. As for causal relations tagging, we use “B, I, O” (Begin, Inside, and Outside) and “C, E” (Cause and Effect) labels to represent the positional information of the words and the semantic roles of the causal events. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons"
2020.fnp-1.11,2020.fnp-1.3,0,0.21222,"Missing"
2020.fnp-1.11,2020.acl-demos.14,0,0.0118515,"lows. 1. We propose an approach to cause-effect detection for financial news that could better identify multiple causal events and event spans. 2. We release the code of the best-performing model for future research.1 2 Pre-processing We experiment on the FinCausal-2020 dataset (Mariko et al., 2020), which consists of two subtasks, including causal meanings detection (Task 1) and cause-effect detection (Task 2). The numbers of training instances are 22,058 and 1,750 for Task 1 and Task 2, respectively. This work only focuses on Task 2. We use the Stanford CoreNLP Stanza (Manning et al., 2014; Qi et al., 2020) toolkit2 to tokenize each sentence and generate the part-of-speech (POS) tag for each token. For the examples with multiple causal events, we recognize them by their indices and add a special number token before each example to treat them as different model inputs. As for causal relations tagging, we use “B, I, O” (Begin, Inside, and Outside) and “C, E” (Cause and Effect) labels to represent the positional information of the words and the semantic roles of the causal events. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4"
2020.fnp-1.11,D16-1014,0,0.0570101,"the artificial intelligence research community. In this paper, we explore the causeeffect detection in financial news and propose an approach, which combines the BIO scheme with the Viterbi decoder for addressing this challenge. Our approach is ranked the first in the official run of cause-effect detection (Task 2) of the FinCausal-2020 shared task. We not only report the implementation details and ablation analysis in this paper, but also publish our code for academic usage. 1 Introduction Adopting causality information as features can benefit lots of applications such as question answering (Sharp et al., 2016), event prediction (Balashankar et al., 2019), and medical text mining (Khoo et al., 2000). In the financial domain, causality detection can be applied to stock movement prediction (Balashankar et al., 2019) and supporting financial services (Izumi and Sakaji, 2019). To better explain the causality that occurs between financial events, cause-effect detection is a fundamental research issue. Taking a close look to financial documents, we find that there may exist multiple causal events and multiple causal chains in a paragraph. In such a case, traditional extraction methods like discourse parse"
2020.lrec-1.128,H91-1060,0,0.0230007,"ation sense labeling, and relation center labeling. Figure 1 illustrates an example of Chinese discourse parsing tree. Note that a coordination relation may have more than two arguments while the other kinds of relations are always binary. Details of Chinese discourse parsing can be found in (Li et al., 2014b). The CDT annotates the hierarchical discourse structure of a given article, which is different from the PDTB-style scheme proposed by (Zhou and Xue, 2012). Although the annotation of the Chinese Discourse TreeBank (CDTB) is well-defined, the evaluation is divergent. Generally, PARSEVAL (Black et al., 1991) is used to evaluate the quality of a predicted parsing tree. For a golden standard discourse tree, we have a set of non-leaf nodes N = {n1 , n2 , ..., nk }. We also have a set of text spans T = {t1 , t2 , ..., tk } where ni dominates ti for all i (e.g., the node B in Figure 1 dominates the text spanning from EDU 1 to EDU 3, so if we use nj to represent node B, then tj should represent the text span from EDU 1 to 3). Similarly, for a predicted discourse parsing tree, we 0 0 0 0 have non-leaf nodes N = {n1 , n2 , ..., nh } and text spans E Coordination Equal B Causation Latter A C Coordination"
2020.lrec-1.128,N19-1423,0,0.0260207,"Missing"
2020.lrec-1.128,P14-1002,0,0.0235016,"y. For English, the most commonly used one is the Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001). RST-DT follows the Rhetorical Structure Theory (RST) and is annotated from 385 Wall Street Journal articles. For Chinese, the Chinese discourse treebank (CDTB) (Li et al., 2014b) is a hierarchically annotated corpus. We will use this corpus to conduct our experiments. CDTB follows the CDT scheme, where 500 Xinhua newswire documents selected from Chinese Treebank are annotated. Although many works have been done on RST-DT (Yu et al., 2018) (Heilman and Sagae, 2015) (Ji and Eisenstein, 2014) (Li et al., 2016) (Li et al., 2014a) (Joty et al., 2013), related researches focusing on Chinese are relatively fewer. Sun and Kong (2018) propose a transition based neural network model to construct a discourse parsing tree based on the given EDUs and their POS features. For the end-toend system development, Kong and Zhou (2017) build a pipeline framework, with each stage utilizing sparse features. They use a greedy bottom-up approach to construct a parsing tree. Lin et al. (2018) propose a unified framework based on recursive neural network to jointly parse the EDUs and the discourse struct"
2020.lrec-1.128,P13-1048,0,0.0316548,"ructure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001). RST-DT follows the Rhetorical Structure Theory (RST) and is annotated from 385 Wall Street Journal articles. For Chinese, the Chinese discourse treebank (CDTB) (Li et al., 2014b) is a hierarchically annotated corpus. We will use this corpus to conduct our experiments. CDTB follows the CDT scheme, where 500 Xinhua newswire documents selected from Chinese Treebank are annotated. Although many works have been done on RST-DT (Yu et al., 2018) (Heilman and Sagae, 2015) (Ji and Eisenstein, 2014) (Li et al., 2016) (Li et al., 2014a) (Joty et al., 2013), related researches focusing on Chinese are relatively fewer. Sun and Kong (2018) propose a transition based neural network model to construct a discourse parsing tree based on the given EDUs and their POS features. For the end-toend system development, Kong and Zhou (2017) build a pipeline framework, with each stage utilizing sparse features. They use a greedy bottom-up approach to construct a parsing tree. Lin et al. (2018) propose a unified framework based on recursive neural network to jointly parse the EDUs and the discourse structure with a probabilistic CKYlike algorithm. All the three"
2020.lrec-1.128,D14-1220,0,0.404605,"pproaches lead to significant differences in performance. Keywords: Discourse parsing, CKY, Shift-reduce, PARSEVAL 1. Introduction As mentioned by Rhetorical Structure Theory (Mann and Thompson, 1988), a discourse is composed of elementary discourse units (EDUs), which can be formed into a hierarchical structure by relating each other with discourse relations. This kind of discourse parsing tree provides a deep understanding of an article and benefits downstream NLP tasks. The majority of Chinese discourse relation is “implicit”, lacking obvious lexical cues to discriminate the relation type (Li et al., 2014b). It makes the task of Chinese discourse parsing more challenging as the parser has to catch the implicit meaning from the text instead of relying on lexical information. According to the Connective-Driven Dependency Tree (CDT) scheme (Li et al., 2014b), there are four subtasks in Chinese discourse parsing, including EDU segmentation, tree structure construction, relation sense labeling, and relation center labeling. Figure 1 illustrates an example of Chinese discourse parsing tree. Note that a coordination relation may have more than two arguments while the other kinds of relations are alwa"
2020.lrec-1.128,D14-1224,0,0.042013,"Missing"
2020.lrec-1.128,D16-1035,0,0.01257,"ommonly used one is the Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001). RST-DT follows the Rhetorical Structure Theory (RST) and is annotated from 385 Wall Street Journal articles. For Chinese, the Chinese discourse treebank (CDTB) (Li et al., 2014b) is a hierarchically annotated corpus. We will use this corpus to conduct our experiments. CDTB follows the CDT scheme, where 500 Xinhua newswire documents selected from Chinese Treebank are annotated. Although many works have been done on RST-DT (Yu et al., 2018) (Heilman and Sagae, 2015) (Ji and Eisenstein, 2014) (Li et al., 2016) (Li et al., 2014a) (Joty et al., 2013), related researches focusing on Chinese are relatively fewer. Sun and Kong (2018) propose a transition based neural network model to construct a discourse parsing tree based on the given EDUs and their POS features. For the end-toend system development, Kong and Zhou (2017) build a pipeline framework, with each stage utilizing sparse features. They use a greedy bottom-up approach to construct a parsing tree. Lin et al. (2018) propose a unified framework based on recursive neural network to jointly parse the EDUs and the discourse structure with a probabi"
2020.lrec-1.128,C18-2016,1,0.935447,"are annotated. Although many works have been done on RST-DT (Yu et al., 2018) (Heilman and Sagae, 2015) (Ji and Eisenstein, 2014) (Li et al., 2016) (Li et al., 2014a) (Joty et al., 2013), related researches focusing on Chinese are relatively fewer. Sun and Kong (2018) propose a transition based neural network model to construct a discourse parsing tree based on the given EDUs and their POS features. For the end-toend system development, Kong and Zhou (2017) build a pipeline framework, with each stage utilizing sparse features. They use a greedy bottom-up approach to construct a parsing tree. Lin et al. (2018) propose a unified framework based on recursive neural network to jointly parse the EDUs and the discourse structure with a probabilistic CKYlike algorithm. All the three proposed models construct binary parsing trees and thus require either a de-binarization step or binarizing ground truth for comparison. Morey et al. (2017) note that there is a discrepancy in evaluation among different works on RST-DT even though these works are also based on PARSEVAL. They thus reproduce these methods to make direct comparisons. For CDTB, there are two main branches of evaluation scenarios. Kong and Zhou (2"
2020.lrec-1.128,D17-1136,0,0.0124553,"truct a discourse parsing tree based on the given EDUs and their POS features. For the end-toend system development, Kong and Zhou (2017) build a pipeline framework, with each stage utilizing sparse features. They use a greedy bottom-up approach to construct a parsing tree. Lin et al. (2018) propose a unified framework based on recursive neural network to jointly parse the EDUs and the discourse structure with a probabilistic CKYlike algorithm. All the three proposed models construct binary parsing trees and thus require either a de-binarization step or binarizing ground truth for comparison. Morey et al. (2017) note that there is a discrepancy in evaluation among different works on RST-DT even though these works are also based on PARSEVAL. They thus reproduce these methods to make direct comparisons. For CDTB, there are two main branches of evaluation scenarios. Kong and Zhou (2017) and Lin et al. (2018) adopt micro F1 score, multiway gold parsing tree, and left-heavy binarization. In contrast, Sun and Kong (2018) use macro F1, binary gold tree, and right-heavy binarization. Recently, Devlin et al. (2018) introduce a neural language representation model called Bidirectional Encoder Representations f"
2020.lrec-1.128,C18-1047,0,0.0132771,"parsers for English and Chinese respectively. For English, the most commonly used one is the Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001). RST-DT follows the Rhetorical Structure Theory (RST) and is annotated from 385 Wall Street Journal articles. For Chinese, the Chinese discourse treebank (CDTB) (Li et al., 2014b) is a hierarchically annotated corpus. We will use this corpus to conduct our experiments. CDTB follows the CDT scheme, where 500 Xinhua newswire documents selected from Chinese Treebank are annotated. Although many works have been done on RST-DT (Yu et al., 2018) (Heilman and Sagae, 2015) (Ji and Eisenstein, 2014) (Li et al., 2016) (Li et al., 2014a) (Joty et al., 2013), related researches focusing on Chinese are relatively fewer. Sun and Kong (2018) propose a transition based neural network model to construct a discourse parsing tree based on the given EDUs and their POS features. For the end-toend system development, Kong and Zhou (2017) build a pipeline framework, with each stage utilizing sparse features. They use a greedy bottom-up approach to construct a parsing tree. Lin et al. (2018) propose a unified framework based on recursive neural networ"
2020.lrec-1.128,P12-1008,0,0.0328783,"Tree (CDT) scheme (Li et al., 2014b), there are four subtasks in Chinese discourse parsing, including EDU segmentation, tree structure construction, relation sense labeling, and relation center labeling. Figure 1 illustrates an example of Chinese discourse parsing tree. Note that a coordination relation may have more than two arguments while the other kinds of relations are always binary. Details of Chinese discourse parsing can be found in (Li et al., 2014b). The CDT annotates the hierarchical discourse structure of a given article, which is different from the PDTB-style scheme proposed by (Zhou and Xue, 2012). Although the annotation of the Chinese Discourse TreeBank (CDTB) is well-defined, the evaluation is divergent. Generally, PARSEVAL (Black et al., 1991) is used to evaluate the quality of a predicted parsing tree. For a golden standard discourse tree, we have a set of non-leaf nodes N = {n1 , n2 , ..., nk }. We also have a set of text spans T = {t1 , t2 , ..., tk } where ni dominates ti for all i (e.g., the node B in Figure 1 dominates the text spanning from EDU 1 to EDU 3, so if we use nj to represent node B, then tj should represent the text span from EDU 1 to 3). Similarly, for a predicted"
2020.lrec-1.711,S17-2002,0,0.0611061,"Missing"
2020.lrec-1.711,N19-1423,0,0.0401024,"Missing"
2020.lrec-1.711,P12-1092,0,0.77437,"w that word embedding models are capable of learning semantic and syntactic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). However, one essential issue in word embeddings is that each word form is represented by only one vector. That is, multiple senses of a word form are indistinguishable, which is problematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above. Camacho-Collados and Pilehvar (2018) provide an extensive review of previous studies in sense embeddings. More recently, pre-trained contextualized word representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) handle polysemy by assigning a vector representation conditioned on the specific context to every word in a sentence. Different from those approaches, sense embeddings can be grounded in an ontology such as WordNet"
2020.lrec-1.711,N15-1070,0,0.0820792,"g models are capable of learning semantic and syntactic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). However, one essential issue in word embeddings is that each word form is represented by only one vector. That is, multiple senses of a word form are indistinguishable, which is problematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above. Camacho-Collados and Pilehvar (2018) provide an extensive review of previous studies in sense embeddings. More recently, pre-trained contextualized word representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) handle polysemy by assigning a vector representation conditioned on the specific context to every word in a sentence. Different from those approaches, sense embeddings can be grounded in an ontology such as WordNet (Miller, 1995) or Ba"
2020.lrec-1.711,D17-1034,0,0.0783968,"d syntactic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). However, one essential issue in word embeddings is that each word form is represented by only one vector. That is, multiple senses of a word form are indistinguishable, which is problematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above. Camacho-Collados and Pilehvar (2018) provide an extensive review of previous studies in sense embeddings. More recently, pre-trained contextualized word representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) handle polysemy by assigning a vector representation conditioned on the specific context to every word in a sentence. Different from those approaches, sense embeddings can be grounded in an ontology such as WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2012), which"
2020.lrec-1.711,C18-1141,1,0.887704,"ion from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). However, one essential issue in word embeddings is that each word form is represented by only one vector. That is, multiple senses of a word form are indistinguishable, which is problematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above. Camacho-Collados and Pilehvar (2018) provide an extensive review of previous studies in sense embeddings. More recently, pre-trained contextualized word representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) handle polysemy by assigning a vector representation conditioned on the specific context to every word in a sentence. Different from those approaches, sense embeddings can be grounded in an ontology such as WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2012), which can support operati"
2020.lrec-1.711,W13-3512,0,0.0437674,"how useful a model would be in practical applications, this kind of evaluation is usually time-consuming. Furthermore, unlike word embeddings, sense embeddings can not be directly adopted in an application without some sense selection process, so it would be hard to decouple the quality of sense embeddings and the performance of WSD. Thus, intrinsic evaluation benchmarks are still important for efficient model and parameter selection. Researchers commonly use word embedding benchmarks, including semantic similarity datasets (Bruni et al., 2014; Radinsky et al., 2011; Finkelstein et al., 2002; Luong et al., 2013), the contextual word similarity dataset (Huang et al., 2012), and synonym selection datasets (Turney, 2001; Landauer and Dumais, 1997; Jarmasz and Szpakowicz, 2004), to evaluate sense embeddings. In this work, we argue that these evaluation benchmarks do not provide a solid base for testing sense embeddings in a polysemy scenario. We examined eight datasets and found that every dataset contains a great number of single-sense words, where there is no ambiguity to resolve. Moreover, most of these benchmarks have a biased distribution of human-annotated scores, leading to concerns about reliabil"
2020.lrec-1.711,W16-1620,0,0.019761,"rformance gap also widens, showing that models that emphasize multisense information from the ontology are stronger. 6. 6.1 (pat, perfectly) (register, join) (descent, falling) MSD 78 72 47 MSD 78 767 118 MSD 72 89 79 |D| 837 814 812 |D| 785 752 744 |D| 903 860 854 Table 7: Word pairs with the largest differences |D| between the similarity rank of MSD-1030 and that given by the embedding models. Evaluating Sense Embedding Models on MSD-1030 Performance of Sense Embeddings We evaluate two knowledge-based sense embedding models: GenSense and SenseRetro, and three unsupervised models: sensegram (Pelevina et al., 2016), AdaGram (Bartunov et al., 2016), and MUSE (Lee and Chen, 2017). When training the sense embeddings, knowledge-based models take advantage of knowledge bases such as WordNet, Wikipedia, and Roget, while unsupervised models learn sense representations directly from text corpora. For all models, we downloaded the off-the-shelf 300-dimensional pre-trained vectors and reported their best results. Table 6 shows the Spearman and Pearson correlation coefficients of various embedding models on four datasets. While these embedding models yielded high performance on previous semantic similarity dataset"
2020.lrec-1.711,D14-1162,0,0.0785946,"(MSD-1030), which contains a high ratio of multi-sense word pairs. A series of analyses and experiments show that MSD-1030 serves as a more reliable benchmark for sense embeddings. The dataset is available at http://nlg.csie.ntu.edu.tw/nlpresource/MSD-1030/. Keywords: semantics, evaluation methodologies, crowdsourcing 1. Introduction Word embeddings, or distributed word representations, have attracted much attention in recent years. Previous studies show that word embedding models are capable of learning semantic and syntactic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). However, one essential issue in word embeddings is that each word form is represented by only one vector. That is, multiple senses of a word form are indistinguishable, which is problematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above"
2020.lrec-1.711,N18-1202,0,0.0458027,"ematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above. Camacho-Collados and Pilehvar (2018) provide an extensive review of previous studies in sense embeddings. More recently, pre-trained contextualized word representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) handle polysemy by assigning a vector representation conditioned on the specific context to every word in a sentence. Different from those approaches, sense embeddings can be grounded in an ontology such as WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2012), which can support operations such as queries and making inference over the ontology or any connected knowledge base. On the other hand, sense embeddings can be trained with smaller amounts of data and further enhanced by external ontologies, making them extremely useful in low-resource scenarios"
2020.lrec-1.711,N19-1128,0,0.035966,"Missing"
2020.lrec-1.711,D18-1169,0,0.0174815,"drops significantly on every dataset as the weight of the original word vectors is decreased. That is, these existing benchmarks may favor dominating senses in the training corpus, making it unnecessary for the sense embedding models to deal with different senses separately. As a result, we cannot properly evaluate a sense embedding model’s ability of handling multiple diverse senses using these datasets. 3.4 Skewed Score Distribution To understand the distribution of the similarity score in the semantic similarity datasets and the SCWS dataset, we perform an analysis similar to that used by Pilehvar et al. (2018). We divide each dataset’s score scale into four equal bins on the interval [1, 10] and assign the similarity scores and human annotators to select the closest pair of senses, making the two judgments more comparable. 5804 Figure 2: Annotation guidelines. to their corresponding bins. The results in Figure 1 show that except for MEN and the proposed MSD (to be introduced later), the score distributions are significantly imbalanced. The skewed distributions suggest that the datasets cannot support evaluations on word pairs with similarity across a variety of levels. 3.5 High Proportion of Multi-"
2020.lrec-1.711,N10-1013,0,0.0906338,"years. Previous studies show that word embedding models are capable of learning semantic and syntactic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). However, one essential issue in word embeddings is that each word form is represented by only one vector. That is, multiple senses of a word form are indistinguishable, which is problematic for applications involving word ambiguity such as word sense disambiguation (WSD) and semantic relation identification. Sense embedding models, in which each sense of a word form is represented by its own vector (Reisinger and Mooney, 2010; Huang et al., 2012; Jauhar et al., 2015; Bartunov et al., 2016; Lee and Chen, 2017; Lee et al., 2018), have been proposed to address the polysemy issue mentioned above. Camacho-Collados and Pilehvar (2018) provide an extensive review of previous studies in sense embeddings. More recently, pre-trained contextualized word representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) handle polysemy by assigning a vector representation conditioned on the specific context to every word in a sentence. Different from those approaches, sense embeddings can be grounded in an onto"
2020.lrec-1.749,baccianella-etal-2010-sentiwordnet,0,0.0493137,"indicates that we cannot evaluate investor performance directly by extracting gain/loss information from the content, and must therefore infer performance from past tweets. For example, capturing the buying price and selling price of the same writer (Chen et al., 2019) can be used to evaluate the performance of this writer. 5. Comparison of Market Sentiment and General Sentiment In order to show the difference between market sentiment and general sentiment, we experiment on the expertannotated dataset (Cortis et al., 2017). We compare the performance of using general dictionary, SentiWordNet (Baccianella et al., 2010) with that of using market sentiment dictionary, NTUSD-Fin (Chen et al., 2018a). The experimental results echo the finding of our annotation results of writers’ sentiment and the market sentiment. 5.1. Dataset Semeval-2017 task 5 dataset (Cortis et al., 2017) is adopted in this paper. There are 2,030 tweets collected from Twitter and Stocktwits. This dataset was annotated by three independent experts. The details of the toolkit for annotation and the annotating process are described in Daudert et al. (2019). They selected piece(s) of tweets containing opinions for a certain cashtag as the key"
2020.lrec-1.749,W14-4012,0,0.0110919,"Missing"
2020.lrec-1.749,S17-2089,0,0.0583157,"Missing"
2020.lrec-1.749,W19-5506,0,0.0202141,"t al., 2017). We compare the performance of using general dictionary, SentiWordNet (Baccianella et al., 2010) with that of using market sentiment dictionary, NTUSD-Fin (Chen et al., 2018a). The experimental results echo the finding of our annotation results of writers’ sentiment and the market sentiment. 5.1. Dataset Semeval-2017 task 5 dataset (Cortis et al., 2017) is adopted in this paper. There are 2,030 tweets collected from Twitter and Stocktwits. This dataset was annotated by three independent experts. The details of the toolkit for annotation and the annotating process are described in Daudert et al. (2019). They selected piece(s) of tweets containing opinions for a certain cashtag as the key snippet. In our experiment, we test the performance of the models in extracting the key snippet as the experts with different sentiment dictionaries. (T6) is an example of the key snippets of the target cashtag, $VXX (iPath S&P 500 VIX Short-Term Futures ETN) and $SPX (S&P 500 Index). (T6) $VXX on the move up again should bring $SPX down into the close. we see. Annotated result of (T6): • $SPX: should bring $SPX down into the close. • $VXX: on the move up again 5.2. Approaches to Key Snippet Extraction 5.2."
2020.lrec-1.749,de-marneffe-etal-2006-generating,0,0.160531,"Missing"
2020.lrec-1.749,P18-2122,1,0.830294,"xample, which provides a forum for investors to discuss their ideas, trading strategies, and analyses of financial instruments. On StockTwits, a leading social trading platform, investors use cashtags to indicate financial instruments. For example, Apple Inc. and Microsoft Corporation are tagged as $AAPL and $MSFT, respectively. Furthermore, investors can also annotate their tweets with a bullish or bearish label. Financial tweets with the writer-labeled annotations have been used as training and test data (Li and Shah, 2017). However, the reliability of writer-labeled data is sometimes poor (Huang et al., 2018). For instance, tweet (T1) is labeled as bullish but lacks enough content. (T2), assigned a bullish label, is a description of the trading volume of $DPW. Note that both soaring and collapsing stocks are characterized by huge trading volumes. When the content of such tweets is associated with bullish or bearish labels and is used as training data, the performance of the resulting model may be greatly diminished. (T1) $AAPL today... (T2) $DPW 50 MILLION VOLUME!!! Specifically, although an investor’s label provides information about the mentioned cashtag, it does not always represent the content"
2020.lrec-1.749,K17-1031,0,0.138755,"social media platforms have been developed. The social trading platform is a typical example, which provides a forum for investors to discuss their ideas, trading strategies, and analyses of financial instruments. On StockTwits, a leading social trading platform, investors use cashtags to indicate financial instruments. For example, Apple Inc. and Microsoft Corporation are tagged as $AAPL and $MSFT, respectively. Furthermore, investors can also annotate their tweets with a bullish or bearish label. Financial tweets with the writer-labeled annotations have been used as training and test data (Li and Shah, 2017). However, the reliability of writer-labeled data is sometimes poor (Huang et al., 2018). For instance, tweet (T1) is labeled as bullish but lacks enough content. (T2), assigned a bullish label, is a description of the trading volume of $DPW. Note that both soaring and collapsing stocks are characterized by huge trading volumes. When the content of such tweets is associated with bullish or bearish labels and is used as training data, the performance of the resulting model may be greatly diminished. (T1) $AAPL today... (T2) $DPW 50 MILLION VOLUME!!! Specifically, although an investor’s label pr"
2020.lrec-1.749,R13-1054,0,0.0686687,"Missing"
2020.lrec-1.749,P13-2005,0,0.0268916,"r, we address critical problems when using financial social media data, and discuss directions for future research. The contributions of our work are three-fold as follows. 1. We conduct a careful study of financial social media data with an adequate annotated dataset. 2. We publish Fin-SoMe, a dataset for academic use with 10,000 tweets labeled from four aspects. 3. We present several perspectives toward research on financial social media data. 2. Related Work Sentiment analysis in financial social media data has been a research focus in the recent decade. Several works (Bollen et al., 2011; Si et al., 2013; Sul et al., 2017; Vanstone et al., 2018) show that financial social media data provides trading clues. However, to the best of our knowledge, little work takes a close look at the contents of individual investors’ tweets. Maks and Vossen (2013) show that evaluations may differ between the writer and readers of a given product review; 6106 they recommend to use product reviews based on readers’ ratings rather than the writer’s rating. However, most works on financial social media data use writer-labeled information to construct lexicons (Li and Shah, 2017) or train sentiment classifiers (Deng"
2020.lrec-1.749,P18-1183,0,0.0209054,"Missing"
2020.lrec-1.76,D18-1547,0,0.0675599,"Missing"
2020.lrec-1.76,L18-1252,0,0.0746388,"e former is built in the open domain. The social dialogue system aims to understand both users’ intents and their feelings, and creates an appropriate response to the users. To design a dialogue system that understands the users’ feelings, a dataset with the labels of the users’ mental state is indispensable. However, those researches about the emotion detection are usually based on the datasets crawled from comment (Feng et al, 2010) and social media (Sun et al., 2010). There is few publicly available Chinese dataset containing conversation content and emotion labels similar to EmotionLines (Chen et al., 2018). In addition to the users’ emotion, we aim to investigate other factors that may affect the conversation. Interpersonal relationship between a speaker and a listener plays an important role. When people talk to friends, the wording would be much different from that talking to a stranger. If a dialogue system can predict which relation type the wording of a user belongs to, the system will be able to generate a better response. In this paper, we release a dataset, Multi-Party Dialogue Dataset (MPDD),1 with both emotion and relation labels on each utterance. All the dialogues are collected from"
2020.lrec-1.76,D14-1181,0,0.00471923,": responded utterance selector, contextualized embedding encoder, feature connector, and the output layer. 4.1 Responded Utterance Selector In MPDD, every utterance has the labels that are the listeners of the utterance. We postulate that the closest utterance spoken by one of the listeners is the responded utterance that the current utterance responds to. For the utterances without responded utterances, we select the previous utterances as the alternatives. 4.2 Contextualized Embedding Encoder To convert an utterance to the contextualized embedding, we adopt two kinds of encoders, i.e., CNN (Kim, 2014) and BERT (Devlin et al., 2018). CNN: If there is a responded utterance, we concatenate the responded utterance and the current utterance first. Then we represent the concatenation with the word embedding, which is a 300-dimensional vector pre-trained on the Wikipedia dataset with word2vec. The word embedding matrix is fed into an 1D-Convolution layer that has 64 filters with the window size in the range of 1 to 5. After that, we fed the result into an 1D-max pooling layer, and flatten 612 Encoder CNN BERT Baseline .7169 .7259 w/ previous utterance .7207 .7494 w/ responded utterance .7207 .763"
2020.lrec-1.76,P18-2033,0,0.0608422,"Missing"
2020.lrec-1.76,I17-1099,0,0.0232711,"oriented dialogue systems. In contrast, the social dialogue systems usually operate in the open domain, and need to capture users’ feelings. To train the open domain conversation model, researchers usually use the movie subtitles datasets like OpenSubtitle (Tiedemann, 2009), or use the dataset crawled from social media platforms likes Weibo (Wang et al., 2013) and Twitter (Ritter et al., 2011). However, the former has no emotion labels, which are important for training, and the latter extracts dialogues from the post-reply pairs, which are quite different from human conversation. DailyDialog (Li et al., 2017) is a dataset in which the daily dialogues are crawled from various English learning websites and annotated with the emotion and intension type on every utterance. EmotionLines (Chen et al., 2018) is a dialogue dataset whose materials are crawled from Friends TV scripts and private Facebook messenger dialogues, and has the emotion labels on each utterance. Both DailyDialog and EmotionLines are in English. We cannot directly translate these datasets to Chinese for system development because of translation errors and the culture difference. In this paper, we aim to build a Chinese dialogue datas"
2020.lrec-1.76,petukhova-etal-2014-dbox,0,0.0268277,"this paper, we release a dataset, Multi-Party Dialogue Dataset (MPDD),1 with both emotion and relation labels on each utterance. All the dialogues are collected from TV series scripts. To the best of our knowledge, this dataset is very rare Chinese multi-party dialogue dataset with both emotion and relation labels. We also make some experiments to observe the effects of the context on the emotion and relation classification, and the correlation between emotion and relation types. 2. Related Work Several dialogue datasets have been released in recent years. Some focus on the specific domains (Petukhova et al., 2014; Feng et al., 2016; Wei et al., 2018), and some on 1 the multiple domains (Budzianowski et al., 2018). These datasets are annotated with the dialogue act information, and appropriate for the slot filling task when building tasked-oriented dialogue systems. In contrast, the social dialogue systems usually operate in the open domain, and need to capture users’ feelings. To train the open domain conversation model, researchers usually use the movie subtitles datasets like OpenSubtitle (Tiedemann, 2009), or use the dataset crawled from social media platforms likes Weibo (Wang et al., 2013) and Tw"
2020.lrec-1.76,D11-1054,0,0.0254358,"et al., 2016; Wei et al., 2018), and some on 1 the multiple domains (Budzianowski et al., 2018). These datasets are annotated with the dialogue act information, and appropriate for the slot filling task when building tasked-oriented dialogue systems. In contrast, the social dialogue systems usually operate in the open domain, and need to capture users’ feelings. To train the open domain conversation model, researchers usually use the movie subtitles datasets like OpenSubtitle (Tiedemann, 2009), or use the dataset crawled from social media platforms likes Weibo (Wang et al., 2013) and Twitter (Ritter et al., 2011). However, the former has no emotion labels, which are important for training, and the latter extracts dialogues from the post-reply pairs, which are quite different from human conversation. DailyDialog (Li et al., 2017) is a dataset in which the daily dialogues are crawled from various English learning websites and annotated with the emotion and intension type on every utterance. EmotionLines (Chen et al., 2018) is a dialogue dataset whose materials are crawled from Friends TV scripts and private Facebook messenger dialogues, and has the emotion labels on each utterance. Both DailyDialog and"
2020.lrec-1.76,O10-1013,0,0.0282181,"major types of dialogue systems. Unlike the latter, which needs to focus on confirming user’s purpose and usually targets a specific domain, the former is built in the open domain. The social dialogue system aims to understand both users’ intents and their feelings, and creates an appropriate response to the users. To design a dialogue system that understands the users’ feelings, a dataset with the labels of the users’ mental state is indispensable. However, those researches about the emotion detection are usually based on the datasets crawled from comment (Feng et al, 2010) and social media (Sun et al., 2010). There is few publicly available Chinese dataset containing conversation content and emotion labels similar to EmotionLines (Chen et al., 2018). In addition to the users’ emotion, we aim to investigate other factors that may affect the conversation. Interpersonal relationship between a speaker and a listener plays an important role. When people talk to friends, the wording would be much different from that talking to a stranger. If a dialogue system can predict which relation type the wording of a user belongs to, the system will be able to generate a better response. In this paper, we releas"
2020.semeval-1.279,N19-1423,0,0.0412291,"Missing"
2020.semeval-1.279,S19-2011,0,0.0218706,"Missing"
2020.semeval-1.279,S19-2123,0,0.0210275,"Missing"
2020.semeval-1.279,N19-1144,0,0.0410524,"Missing"
2020.semeval-1.279,S19-2010,0,0.0203437,"/licenses/by/4.0/. 2105 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 2105–2110 Barcelona, Spain (Online), December 12, 2020. IU@LING team (Zhu et al., 2019) wins the third place in Sub-task A. They use the PyTorch framework to build the BERT model in Sub-task A, and use the SVM model in Sub-tasks B and C. Multi-task learning (MTL) is one of widely-used strategies for improving neural network models. Ruder (2017) introduces the two most commonly used methods of MTL in deep learning, namely hard parameter sharing and soft parameter sharing approaches. Sanh et al. (2019) propose a hierarchical multi-task (HMTL) model, by combining low-level simple tasks with high-level complex tasks to train. The model achieves the state-of-the-art results in named entity recognition, entity mention detection, and relation extraction. In this paper, we consider the hierarchical nature of the three tasks and introduces the HMTL framework to deal with the problems. 3 3.1 Data and Methodology Data The OLID 1.0 dataset used in the competition last year is available as a part of the OffensEval 2019 Shared Task Zampieri et al. (2019a). It is collected from Twitter API by searching"
2020.semeval-1.279,S19-2138,0,0.011562,"shows that the pre-trained BERT model has a good competition result in the task. The vradivchev_anikolov team (Radivchev and Nikolov, 2019) wins the first place and the second place in Sub-tasks C and A, respectively. They explore several models, among which BERT has the highest performance. The UMThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 2105 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 2105–2110 Barcelona, Spain (Online), December 12, 2020. IU@LING team (Zhu et al., 2019) wins the third place in Sub-task A. They use the PyTorch framework to build the BERT model in Sub-task A, and use the SVM model in Sub-tasks B and C. Multi-task learning (MTL) is one of widely-used strategies for improving neural network models. Ruder (2017) introduces the two most commonly used methods of MTL in deep learning, namely hard parameter sharing and soft parameter sharing approaches. Sanh et al. (2019) propose a hierarchical multi-task (HMTL) model, by combining low-level simple tasks with high-level complex tasks to train. The model achieves the state-of-the-art results in named"
2021.eacl-main.122,C16-1284,0,0.0699899,"Missing"
2021.eacl-main.122,D19-1490,0,0.0560409,"Missing"
2021.eacl-main.122,D19-1495,0,0.0157294,"ph in1426 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1426–1431 April 19 - 23, 2021. ©2021 Association for Computational Linguistics formation. Our approach outperforms recent works in implicit tag recognition. • Our pre-training task, masked entity prediction, is helpful for predicting the implicit entities. The pre-trained model can be also applied in other information extraction tasks. 2 Related work Extracting and using the information in articles is one of the focuses in the NLP community. Some works (Hu et al., 2018; Ding et al., 2019; Ma et al., 2019) adopt the extracted information for stock market prediction. Some of them (Baker et al., 2016; Min and Zhao, 2019) use the information in news articles to construct socio-economic indicators. Most of previous works only focus on explicit information in the articles. In this way, the implicit entities in the articles may be under looked. In this paper, we aim to extract the non-mentioned but related entities from a document. Recently, GNN has become popular for modeling relationships among multiple entities. Kipf and Welling (2016) use the convolution neural network to learn"
2021.eacl-main.122,P19-1136,0,0.062595,"Missing"
2021.eacl-main.122,D19-1510,0,0.0252661,", two companies, Sprint and T-Mobile, are explicitly mentioned companies in the news article in Figure 1. However, the stock price of a non-mentioned company, SoftBank, may be influenced since Softbank owns shares of Sprint. Although this kind of inference is intuitive for professional analysts, few previous works take such implicit information into consideration. In this paper, we aim to increase the sense of machines toward this kind of implicit information. Transformer-based (Vaswani et al., 2017) neural networks achieve state-of-the-art performances in many NLP tasks (Devlin et al., 2018; Malmi et al., 2019). To model the relationships between entities, graph neural network (GNN) is a well-known architecture for representing the knowledge and additional information (Fu et al., 2019; You et al., 2020). Furthermore, the models blending these two architectures show their effectiveness (Lu et al., 2020). In this paper, we propose dynamic graph transformer (DGT), a novel blend of Transformer and GNN. In previous work, the weights of the GNN are pre-determined in the training stage and not affected by the given input. Our DGT adjusts the weights depending on the input on the fly. In this way, the repre"
2021.eacl-main.122,D19-1121,0,0.0140288,"431 April 19 - 23, 2021. ©2021 Association for Computational Linguistics formation. Our approach outperforms recent works in implicit tag recognition. • Our pre-training task, masked entity prediction, is helpful for predicting the implicit entities. The pre-trained model can be also applied in other information extraction tasks. 2 Related work Extracting and using the information in articles is one of the focuses in the NLP community. Some works (Hu et al., 2018; Ding et al., 2019; Ma et al., 2019) adopt the extracted information for stock market prediction. Some of them (Baker et al., 2016; Min and Zhao, 2019) use the information in news articles to construct socio-economic indicators. Most of previous works only focus on explicit information in the articles. In this way, the implicit entities in the articles may be under looked. In this paper, we aim to extract the non-mentioned but related entities from a document. Recently, GNN has become popular for modeling relationships among multiple entities. Kipf and Welling (2016) use the convolution neural network to learn the node representation by aggregating the features of neighboring nodes. Veliˇckovi´c et al. (2017) employ the attention mechanism t"
2021.eacl-main.122,N16-1041,0,0.0326594,"Missing"
2021.eacl-main.122,D19-1471,0,0.0217301,"Missing"
2021.eacl-main.122,P18-1183,0,0.0666733,"Missing"
2021.emnlp-main.362,N19-1423,0,0.0965982,", we propose a method that not only provides more data, but also retains the meanings of the original instances. Our experimental results show that the proposed method performs well in all datasets. 2.2 Aspect-Based Sentiment Analysis Wang et al. (2016) propose an attention-based LSTM, which can concentrate on distinct parts of a sentence by calculating the corresponding attention weights, to learn aspect embedding. Ma et al. (2017) find that interactive attention networks can learn the representations of target and context separately, which is helpful to sentiment classification. BERT-based (Devlin et al., 2019) methods have shown to be effective on ABSA (Hoang et al., 2019; Xu et al., 2019). Because BERT-based methods achieve the best performance in most ABSA tasks, we adopt BERT as the base architecture for test performance of data augmentation methods. 3 review. For each review, we concatenate on the auxiliary sentence and the review with a special token [SEP]. For example, the auxiliary sentence “the polarity of the service is positive” and the review “the staff was so kind to us” are concatenated to “the polarity of the service is positive [SEP] the staff was so kind to us”. An input sentence (S"
2021.emnlp-main.362,D18-1045,0,0.0144991,"(Pontiki et al., 2014), Rest15 (Pontiki et al., 2015), and Rest16 (Pontiki et al., 2016). Furthermore, we evaluate our model on two sentiment classification (SC) benchmark datasets including Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) and Movie Review (MR) (Pang and Lee, 2005). The statistics of these datasets are reported in Table 1. In our experiments, we use the BERT-baseuncased model to show the performances with and without the proposed augmentation methods. Additionally, we compare with commonly used data augmentation methods, including Back Translation 5 Discussion (BT) (Edunov et al., 2018), Easy Data Augmenta5.1 Multilingual Experiment tion (EDA) (Wei and Zou, 2019), and C-BERT (Wu et al., 2019). In ACSC, ATSC, and SC tasks, we In this section, we utilize Google Translate to transdouble the original training set in size. In ATE task, late corresponding auxiliary sentences, and experwe augment the reviews according to the number of iment on ABSA datasets (Pontiki et al., 2016) in 4419 Model Bertbase + BT + C-BERT + AS-SPM & AE + Senti-SPM & AE + AS-SPM & Seq2Seq + Senti-SPM & Seq2Seq AR 88.480.78 88.240.87 87.882.24 89.201.01 87.410.88 90.281.34 88.481.76 CH 93.790.87 94.581.40"
2021.emnlp-main.362,D19-1654,0,0.0201007,"ts on ACSC-Rest14 under different multiples of the training set size. The performances of models become better than that only using the original training set when the multiple is between 1 and 2. It also shows that using too much training data generated by the proposed method harms the performance of ABSA because it may contain too much noise. 5.4 Case Study Table 6 presents the augmented results of different approaches. It shows that previous approaches 5.2 Multi-Aspect Multi-Sentiment may change the aspect or sentiment of an instance. Experiment For example, EDA generates an unmeaningful senJiang et al. (2019) propose a multi-aspect multi- tence. Although “ugly” could be a hint for negative sentiment dataset, MAMS. In MAMS, each in- sentiment, the aspect of the generated instance is stance is annotated with different sentiments from changed. C-BERT shows the other worse case— at least two aspects. They claim that this is a more the sentiment of the generated review is changed. challenging dataset than that in the previous works. Both cases show the importance of the proposed We further experiment on this dataset with the pro- idea, i.e., controlling the aspect or sentiment word posed method, and re"
2021.emnlp-main.362,2020.findings-emnlp.372,0,0.0958615,"idered as real-world sentiment analysis application scenarios (Xu and Cohen, 2018). The experimental results on all kinds of datasets support the usefulness of the proposed data augmentation method, and also indicate that the improvement of using the proposed method is larger than that of using other augmentation methods. Our main contributions are summarized as follows: Data annotation, the first step of most artificial intelligence researches, often takes lots of time and is very expensive. Many studies propose methodologies for augmenting data based on a few annotations (Wei and Zou, 2019; Jiao et al., 2020; Xie et al., 2020; Wu et al., 2019; Kobayashi, 2018) to reduce the cost of annotation. However, most of 1. We propose a semantics-preserved augmenthem encounter a problem—hardly ensuring readtation method,1 which provides more trainability and semantic coherence. To overcome these ing data without changing the meaning of the problems, we introduce a novel method, selective given instances in augmentation. perturbed masking (SPM), to measure the importance of each word in a textual sentence, and further 2. Our experimental results show the proposed replace the unimportant word with pre-trained"
2021.emnlp-main.362,N18-2072,0,0.115871,"enarios (Xu and Cohen, 2018). The experimental results on all kinds of datasets support the usefulness of the proposed data augmentation method, and also indicate that the improvement of using the proposed method is larger than that of using other augmentation methods. Our main contributions are summarized as follows: Data annotation, the first step of most artificial intelligence researches, often takes lots of time and is very expensive. Many studies propose methodologies for augmenting data based on a few annotations (Wei and Zou, 2019; Jiao et al., 2020; Xie et al., 2020; Wu et al., 2019; Kobayashi, 2018) to reduce the cost of annotation. However, most of 1. We propose a semantics-preserved augmenthem encounter a problem—hardly ensuring readtation method,1 which provides more trainability and semantic coherence. To overcome these ing data without changing the meaning of the problems, we introduce a novel method, selective given instances in augmentation. perturbed masking (SPM), to measure the importance of each word in a textual sentence, and further 2. Our experimental results show the proposed replace the unimportant word with pre-trained lanmethod can achieve better performances in guage m"
2021.emnlp-main.362,2020.acl-main.703,0,0.0178045,"61 80.662.27 70.341.65 81.001.68 69.211.14 82.861.50 70.681.15 81.511.07 69.270.87 - Rest16 72.422.38 74.230.64 75.190.57 75.620.64 75.240.58 - Table 2: Experimental results. Auto Encoding (AE): The AE model is initialized with the pre-trained weights of BERT (Devlin et al., 2019), which will predict the masked word. We use the predicted word to replace the unimportant term. aspect terms. Accuracy is adopted as the evaluation metric for ACSC, ATSC, and SC tasks. F1-score is used in ATE task. Sequence-to-Sequence (Seq2Seq): The Seq2Seq model is initialized with the pre-trained weights of BART (Lewis et al., 2020) which includes encoder and decoder. The predicted word from the decoder is adopted for replacing the unimportant term. We report the averaged results across five random seeds in Table 2, and the standard deviations are also shown in subscripts. We do not adopt EDA to the ATE task because the insertion and the deletion operations are not suitable for token-level tasks. Firstly, we find that the combinations of the SPM settings (AS-SPM and Senti-SPM) and token replacement strategies (AE and Seq2Seq) achieve better performances on all settings with stable results (lower standard deviations). Tha"
2021.emnlp-main.362,D13-1170,0,0.00673025,"the proposed SPM consistently outperforms the random masking strategy (C-BERT). Thirdly, the proposed token replacement strategy Seq2Seq performs well in ACSC and ATSC, and AE achieves the best results in ATE. 4 4.1 Experiments Experimental Setup 4.2 Experimental Results We experiment on four widely-used datasets in ABSA, including Lap14 (Pontiki et al., 2014), Rest14 (Pontiki et al., 2014), Rest15 (Pontiki et al., 2015), and Rest16 (Pontiki et al., 2016). Furthermore, we evaluate our model on two sentiment classification (SC) benchmark datasets including Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) and Movie Review (MR) (Pang and Lee, 2005). The statistics of these datasets are reported in Table 1. In our experiments, we use the BERT-baseuncased model to show the performances with and without the proposed augmentation methods. Additionally, we compare with commonly used data augmentation methods, including Back Translation 5 Discussion (BT) (Edunov et al., 2018), Easy Data Augmenta5.1 Multilingual Experiment tion (EDA) (Wei and Zou, 2019), and C-BERT (Wu et al., 2019). In ACSC, ATSC, and SC tasks, we In this section, we utilize Google Translate to transdouble the original training set i"
2021.emnlp-main.362,N19-1035,0,0.0415176,"Missing"
2021.emnlp-main.362,D16-1058,0,0.027143,"ormation and use the language model to randomly replacing single-word with more diverse substitutions. Xie et al. (2020) replace the uninformative words based on TF-IDF scores. Although previous works show that their methods can improve the performance of some NLP tasks, their data augmentation methods may change the meanings of the given instances. In this paper, we propose a method that not only provides more data, but also retains the meanings of the original instances. Our experimental results show that the proposed method performs well in all datasets. 2.2 Aspect-Based Sentiment Analysis Wang et al. (2016) propose an attention-based LSTM, which can concentrate on distinct parts of a sentence by calculating the corresponding attention weights, to learn aspect embedding. Ma et al. (2017) find that interactive attention networks can learn the representations of target and context separately, which is helpful to sentiment classification. BERT-based (Devlin et al., 2019) methods have shown to be effective on ABSA (Hoang et al., 2019; Xu et al., 2019). Because BERT-based methods achieve the best performance in most ABSA tasks, we adopt BERT as the base architecture for test performance of data augmen"
2021.emnlp-main.362,D19-1670,0,0.269046,"ve always been considered as real-world sentiment analysis application scenarios (Xu and Cohen, 2018). The experimental results on all kinds of datasets support the usefulness of the proposed data augmentation method, and also indicate that the improvement of using the proposed method is larger than that of using other augmentation methods. Our main contributions are summarized as follows: Data annotation, the first step of most artificial intelligence researches, often takes lots of time and is very expensive. Many studies propose methodologies for augmenting data based on a few annotations (Wei and Zou, 2019; Jiao et al., 2020; Xie et al., 2020; Wu et al., 2019; Kobayashi, 2018) to reduce the cost of annotation. However, most of 1. We propose a semantics-preserved augmenthem encounter a problem—hardly ensuring readtation method,1 which provides more trainability and semantic coherence. To overcome these ing data without changing the meaning of the problems, we introduce a novel method, selective given instances in augmentation. perturbed masking (SPM), to measure the importance of each word in a textual sentence, and further 2. Our experimental results show the proposed replace the unimportant wo"
2021.emnlp-main.362,2020.acl-main.383,0,0.0147395,"ecial token [SEP]. For example, the auxiliary sentence “the polarity of the service is positive” and the review “the staff was so kind to us” are concatenated to “the polarity of the service is positive [SEP] the staff was so kind to us”. An input sentence (S) is thus formulated as follows: S = a a r ], [w1a , ..., waspect , ..., wsentiment , w[SEP] , w1r ..., wN a r where w and w denote words in auxiliary sentence and review, respectively. waspect and wsentiment are the words denoting aspect and sentiment, respectively, and N is the length of the review. 3.2 Selective Perturbed Masking (SPM) Wu et al. (2020) introduce perturbed masking (PM) to analyze syntactic information, and verify its effectiveness on syntactic parsing and discourse dependency parsing. In this work, we propose selective perturbed masking (SPM) to estimate the correlation between the tokens in reviews and sentiment words (aspect terms) in the auxiliary sentence. The following procedure is proposed to measure the impact wir (1 ≤ i ≤ N ) on predicting a a waspect and wsentiment , respectively. First, we replace waspect (wsentiment ) with the special token [MASK], and use this word sequence as BERT’s input for predicting the mask"
2021.emnlp-main.362,N19-1242,0,0.0266341,"of the original instances. Our experimental results show that the proposed method performs well in all datasets. 2.2 Aspect-Based Sentiment Analysis Wang et al. (2016) propose an attention-based LSTM, which can concentrate on distinct parts of a sentence by calculating the corresponding attention weights, to learn aspect embedding. Ma et al. (2017) find that interactive attention networks can learn the representations of target and context separately, which is helpful to sentiment classification. BERT-based (Devlin et al., 2019) methods have shown to be effective on ABSA (Hoang et al., 2019; Xu et al., 2019). Because BERT-based methods achieve the best performance in most ABSA tasks, we adopt BERT as the base architecture for test performance of data augmentation methods. 3 review. For each review, we concatenate on the auxiliary sentence and the review with a special token [SEP]. For example, the auxiliary sentence “the polarity of the service is positive” and the review “the staff was so kind to us” are concatenated to “the polarity of the service is positive [SEP] the staff was so kind to us”. An input sentence (S) is thus formulated as follows: S = a a r ], [w1a , ..., waspect , ..., wsentime"
2021.emnlp-main.362,P18-1183,0,0.112237,"asks to the given sentence “the staff was so kind” are (service, positive), (staff, positive), and “staff”, respectively. These tasks are commonly taken as examples for evaluating the performance of augmentation methods. In this paper, we explore both sentiment analysis and all subtasks in ABSA to show the usefulness of the proposed method in semantics preservation. In addition to probe on sentiment analysis and ABSA tasks, we also experiment on stock price and risk movement prediction tasks. These experiments have always been considered as real-world sentiment analysis application scenarios (Xu and Cohen, 2018). The experimental results on all kinds of datasets support the usefulness of the proposed data augmentation method, and also indicate that the improvement of using the proposed method is larger than that of using other augmentation methods. Our main contributions are summarized as follows: Data annotation, the first step of most artificial intelligence researches, often takes lots of time and is very expensive. Many studies propose methodologies for augmenting data based on a few annotations (Wei and Zou, 2019; Jiao et al., 2020; Xie et al., 2020; Wu et al., 2019; Kobayashi, 2018) to reduce t"
2021.emnlp-main.362,P05-1015,0,0.208777,"he random masking strategy (C-BERT). Thirdly, the proposed token replacement strategy Seq2Seq performs well in ACSC and ATSC, and AE achieves the best results in ATE. 4 4.1 Experiments Experimental Setup 4.2 Experimental Results We experiment on four widely-used datasets in ABSA, including Lap14 (Pontiki et al., 2014), Rest14 (Pontiki et al., 2014), Rest15 (Pontiki et al., 2015), and Rest16 (Pontiki et al., 2016). Furthermore, we evaluate our model on two sentiment classification (SC) benchmark datasets including Stanford Sentiment Treebank (SST-2) (Socher et al., 2013) and Movie Review (MR) (Pang and Lee, 2005). The statistics of these datasets are reported in Table 1. In our experiments, we use the BERT-baseuncased model to show the performances with and without the proposed augmentation methods. Additionally, we compare with commonly used data augmentation methods, including Back Translation 5 Discussion (BT) (Edunov et al., 2018), Easy Data Augmenta5.1 Multilingual Experiment tion (EDA) (Wei and Zou, 2019), and C-BERT (Wu et al., 2019). In ACSC, ATSC, and SC tasks, we In this section, we utilize Google Translate to transdouble the original training set in size. In ATE task, late corresponding aux"
2021.emnlp-main.362,S15-2082,0,0.067438,"Missing"
2021.emnlp-main.362,2021.naacl-main.185,0,0.0202721,"Missing"
2021.emnlp-tutorials.2,2020.lrec-1.749,1,0.850016,"nical details and the application scenarios will be introduced. The contrast of financial opinion mining with general opinion mining will also be discussed. The characteristics of different kinds of financial documents will be listed. 3.1 Coarse-grained Financial Opinion Mining The topic of the first session gives the overview of financial opinion mining, including the investor’s opinion and the customer’s opinion. We start with sentiment analysis in the financial domain. The comparison between the general sentiment and the market sentiment will also be discussed (Loughran and McDonald, 2011; Chen et al., 2020b). The lexicons for the sentiment analysis (Bodnaruk et al., 2015; Li and Shah, 2017; Sedinkina et al., 2019) in financial documents and the applications of adopting sentiment analysis results (Bollen et al., 2011; Du et al., 2019; Lin et al., 2020) will be included. This session also covers the sentiment analysis of financial narratives from different resources, including formal documents such as financial statements 7 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 7–10 Punta Cana and Online, November 10–11, 2021. ©20201Assoc"
2021.emnlp-tutorials.2,P19-1047,0,0.0939837,"issed if we only focus on the market sentiment analysis of financial documents. Thanks to the recent “CS + X” trend, more interdisciplinary cooperation exists between computer science and other domains. In the “NLP + Finance” community, lots of recent works pay their attention to in-depth analysis of different kinds of financial documents rather than market sentiment prediction. For example, our previous works (Chen et al., 2018, 2019a) find that the numeral information extracted from financial social media data is comparable to the price targets extracted from professional analysts’ reports. Keith and Stent (2019) analyze the pragmatic and semantic features in the earnings conference calls and discuss how these features influence the investor’s decision-making process. Zong et al. (2020) point out the difference between the textual factors and cognitive factors by comparing the accurate and inaccurate professional analysts’ reports. The abovementioned works conclude the necessity of capturing fine-grained opinions in the financial narratives. As the increasing interest of our community on this topic, recently, more and more related workshops spring up in the leading conferences, including FinWeb-2021 i"
2021.emnlp-tutorials.2,J19-4006,0,0.0308938,"ch competition (2018), and the 2nd prize in both the Jih Sun FinTech Possible Research Directions In the last session, we will discuss four possible research directions for future works (Chen et al., 2020a), including (1) argument mining in finance, (2) opinion quality evaluation, (3) implicit influence inference, and (4) opinion tracking in time series. We will link the proposed directions with the latest progress of NLP. For example, when introducing the ideas of argument mining in finance, we will provide a brief overview of current development on argument mining (Cabrio and Villata, 2018; Lawrence and Reed, 2019), and further present some instances for discussing the relation between current works and the proposed directions in financial opinion mining (Chen et al., 2020c). When discussing opinion quality evaluation, we will cover the studies of online review quality evaluation (Eirinaki et al., 2012; Wei et al., 2016; Ocampo Diaz and Ng, 2018), and show the difference between dealing with online reviews and 1 8 http://cjchen.nlpfin.com/ Hackathon (2018) and the E.SUN FHC FinTech Hackathon (2017). Hen-Hsen Huang2 is an assistant research fellow at the Institue of Information Science, Academia Sinica,"
2021.emnlp-tutorials.2,K17-1031,0,0.0240112,"cial opinion mining with general opinion mining will also be discussed. The characteristics of different kinds of financial documents will be listed. 3.1 Coarse-grained Financial Opinion Mining The topic of the first session gives the overview of financial opinion mining, including the investor’s opinion and the customer’s opinion. We start with sentiment analysis in the financial domain. The comparison between the general sentiment and the market sentiment will also be discussed (Loughran and McDonald, 2011; Chen et al., 2020b). The lexicons for the sentiment analysis (Bodnaruk et al., 2015; Li and Shah, 2017; Sedinkina et al., 2019) in financial documents and the applications of adopting sentiment analysis results (Bollen et al., 2011; Du et al., 2019; Lin et al., 2020) will be included. This session also covers the sentiment analysis of financial narratives from different resources, including formal documents such as financial statements 7 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 7–10 Punta Cana and Online, November 10–11, 2021. ©20201Association for Computational Linguistics and professional analyst’s reports and informal"
2021.emnlp-tutorials.2,P16-2032,0,0.0291716,"(4) opinion tracking in time series. We will link the proposed directions with the latest progress of NLP. For example, when introducing the ideas of argument mining in finance, we will provide a brief overview of current development on argument mining (Cabrio and Villata, 2018; Lawrence and Reed, 2019), and further present some instances for discussing the relation between current works and the proposed directions in financial opinion mining (Chen et al., 2020c). When discussing opinion quality evaluation, we will cover the studies of online review quality evaluation (Eirinaki et al., 2012; Wei et al., 2016; Ocampo Diaz and Ng, 2018), and show the difference between dealing with online reviews and 1 8 http://cjchen.nlpfin.com/ Hackathon (2018) and the E.SUN FHC FinTech Hackathon (2017). Hen-Hsen Huang2 is an assistant research fellow at the Institue of Information Science, Academia Sinica, Taiwan. His research interests include natural language processing and information retrieval. His work has been published in ACL, SIGIR, WWW, IJCAI, CIKM, COLING, and so on. Dr. Huang received the Honorable Mention of Doctoral Dissertation Award of ACLCLP in 2014 and the Honorable Mention of Master Thesis Awar"
2021.emnlp-tutorials.2,2020.acl-main.473,0,0.209943,"ce and other domains. In the “NLP + Finance” community, lots of recent works pay their attention to in-depth analysis of different kinds of financial documents rather than market sentiment prediction. For example, our previous works (Chen et al., 2018, 2019a) find that the numeral information extracted from financial social media data is comparable to the price targets extracted from professional analysts’ reports. Keith and Stent (2019) analyze the pragmatic and semantic features in the earnings conference calls and discuss how these features influence the investor’s decision-making process. Zong et al. (2020) point out the difference between the textual factors and cognitive factors by comparing the accurate and inaccurate professional analysts’ reports. The abovementioned works conclude the necessity of capturing fine-grained opinions in the financial narratives. As the increasing interest of our community on this topic, recently, more and more related workshops spring up in the leading conferences, including FinWeb-2021 in the Web Conference, FinNLP-2021 in IJCAI, FinIR-2020 in SIGIR, and FNP-2020 in COLING. In this tutorial, we will show where we are and where we will be to those researchers in"
2021.emnlp-tutorials.2,P19-1329,0,0.0161227,"pinions. The audience will be inspired by this tutorial and find an interesting research direction for their work. With the discussion on the possible research directions, many novel ideas will be figured out during this tutorial. Fine-grained Financial Opinion Mining 4 The second session will focus on the fine-grained financial opinion mining, which is the recent trend in this field and also the research interest of the presenters. This session will start by the discussion of the aspect analysis of financial narratives (Maia et al., 2018; Chen et al., 2019a). The numeral in the textual data (Naik et al., 2019; Wallace et al., 2019; Chen et al., 2018, 2019a, 2020c) and the numeracy of the neural network models (Spithourakis and Riedel, 2018; Chen et al., 2019b) attract lots of attentions recently. In the financial narrative, the proportion of numerals are higher than that of other domains’ documents. Without numerals, more important information will be missed. Thus, we summarize the related works for understanding the numerals in financial documents and provide a systematic analysis on these studies. The linguistic features of different kinds of financial documents will also be discussed (Keith and"
2021.emnlp-tutorials.2,P18-1065,0,0.0214254,"time series. We will link the proposed directions with the latest progress of NLP. For example, when introducing the ideas of argument mining in finance, we will provide a brief overview of current development on argument mining (Cabrio and Villata, 2018; Lawrence and Reed, 2019), and further present some instances for discussing the relation between current works and the proposed directions in financial opinion mining (Chen et al., 2020c). When discussing opinion quality evaluation, we will cover the studies of online review quality evaluation (Eirinaki et al., 2012; Wei et al., 2016; Ocampo Diaz and Ng, 2018), and show the difference between dealing with online reviews and 1 8 http://cjchen.nlpfin.com/ Hackathon (2018) and the E.SUN FHC FinTech Hackathon (2017). Hen-Hsen Huang2 is an assistant research fellow at the Institue of Information Science, Academia Sinica, Taiwan. His research interests include natural language processing and information retrieval. His work has been published in ACL, SIGIR, WWW, IJCAI, CIKM, COLING, and so on. Dr. Huang received the Honorable Mention of Doctoral Dissertation Award of ACLCLP in 2014 and the Honorable Mention of Master Thesis Award of ACLCLP in 2008. He ser"
2021.emnlp-tutorials.2,P19-1034,0,0.0548762,"Missing"
2021.emnlp-tutorials.2,P18-1196,0,0.0136317,"he discussion on the possible research directions, many novel ideas will be figured out during this tutorial. Fine-grained Financial Opinion Mining 4 The second session will focus on the fine-grained financial opinion mining, which is the recent trend in this field and also the research interest of the presenters. This session will start by the discussion of the aspect analysis of financial narratives (Maia et al., 2018; Chen et al., 2019a). The numeral in the textual data (Naik et al., 2019; Wallace et al., 2019; Chen et al., 2018, 2019a, 2020c) and the numeracy of the neural network models (Spithourakis and Riedel, 2018; Chen et al., 2019b) attract lots of attentions recently. In the financial narrative, the proportion of numerals are higher than that of other domains’ documents. Without numerals, more important information will be missed. Thus, we summarize the related works for understanding the numerals in financial documents and provide a systematic analysis on these studies. The linguistic features of different kinds of financial documents will also be discussed (Keith and Stent, 2019; Zong et al., 2020), which can provide insights for the future works on feature engineering. The results of cross-docume"
A97-1010,P94-1041,0,0.013059,"ct object, during a prepositional phrase, during subordination, and so on. The variety makes the identification of Chinese speech repairs more troublesome. Because the repairs introduce much noise in language processing, we cannot defer the task of repair processing to the latter stages. This paper employs acoustic and prosodic information to correct Chinese repetition repairs. The results are applied to Chinese homophone disambiguation. Section 2 defines four major types of speech repairs. Section 3 introduces the spoken corpus. Sections 4 and 5 describe the 2 Types of Chinese Speech Repairs Heeman and Allen (1994) divide English speech repairs into three types: fresh start, modification repair and abridged repair. For Chinese speech repairs, Chui (1995) classify them into eleven patterns. In this section, we map these eleven patterns into four major types according to their surface forms. Let A, B, C, D, X and Y be character strings and # be interruption point~. The four major types of speech repairs are described below. The repair, the repaired segment and the repairing segment are in unde[lin_e, italic and boldface, respectively. They appear within an utterance or between two consecutive utterances."
A97-1010,P83-1019,0,0.0524102,"Missing"
A97-1010,P93-1007,0,0.0901921,"enotesthe glottal stop. 2 The editing terms can either be filled pauses (e.g., urn, un, er) or cue phrases(e.g., I mean, 1guess, well). 57 baseline and the advanced models for repetition repairs, respectively. Section 6 demonstrates the effects of repair processing in Chinese homophone disambiguation. Section 7 concludes the remarks. and the repairing segment 3. The resulting potential repairs are then passed to a statistical filter that judges the proposal as either fluent speech or an actual repair. The speech-first approach tries to identify speech repairs using acoustic and prosodic cues. Nakatani and Hirschberg (1993a; 1993b) investigate the detection of the interruption point of speech repairs based on this line. The cues that they found are the occurrence of a filled pause, the presence of a word fragment, the energy peaks in each word and other features such as accent. However, they do not address the problem of correcting speech repairs. In other words, they do not determine which words are undesired. These approaches cannot be adopted to deal with Chinese speech repairs for the following reasons. First, a Chinese sentence is composed of a string of characters without any word boundaries. In other wor"
A97-1010,P92-1008,0,0.0493358,"Missing"
A97-1010,C96-1039,1,0.810882,"found are the occurrence of a filled pause, the presence of a word fragment, the energy peaks in each word and other features such as accent. However, they do not address the problem of correcting speech repairs. In other words, they do not determine which words are undesired. These approaches cannot be adopted to deal with Chinese speech repairs for the following reasons. First, a Chinese sentence is composed of a string of characters without any word boundaries. In other words, it is necessary to segment Chinese sentence before tagging and parsing (Chen and Liu, 1992; Sproat, et al., 1994; Chen and Lee, 1996). Repairs make segmentation and text-first approach more difficult. Second, Chinese repairs may not always have an editing terms between a repaired segment and a repairing segment. In other words, editing terms do not have much effect in Chinese repair processing. Third, duplicate constructions (e.g., ~'~['I~ (bangl bangl mang2, help), ]i~:~]i~:,~ (yan2 jiu4 yah2 jiu4, study)) generated by repeating words or phrases in Chinese utterances are used very often, but they do not always initiate a repair. That is, a simple pattern matching mechanism cannot be workable. Forth, the Chinese speech repa"
A97-1010,P94-1010,0,0.0562088,"Missing"
A97-1010,J96-3004,0,\N,Missing
A97-1010,C92-1019,0,\N,Missing
A97-1010,O90-1010,0,\N,Missing
bian-chen-1998-integrating,J85-2002,0,\N,Missing
bian-chen-1998-integrating,J90-1003,0,\N,Missing
bian-chen-1998-integrating,C96-1038,1,\N,Missing
bian-chen-1998-integrating,C94-1012,0,\N,Missing
bian-chen-1998-integrating,J90-2002,0,\N,Missing
bian-chen-1998-integrating,1995.tmi-1.21,1,\N,Missing
bian-chen-1998-integrating,1991.mtsummit-papers.9,0,\N,Missing
bian-chen-1998-integrating,O97-1010,1,\N,Missing
bian-chen-1998-integrating,A92-1045,0,\N,Missing
C00-1024,P98-1036,1,0.897728,"Missing"
C00-1024,P99-1028,1,0.872804,"Missing"
C00-1024,C96-1039,1,0.773162,"hese entities form important cues during clustering. Systems for named entity extraction in a famous message understanding competition (MUC, 1998) demonstrate promising performances for English, Japanese and Chinese. In our multilingual summarization system, we focus on English and Chinese. Gazetteer approach is adopted to deal with English news articles. Comparatively, Chinese news articles are segmented at first. Then, several types of information from character, sentence and text levels are employed to extract Chinese named entities. These tasks are similar to the approaches in the papers (Chen and Lee, 1996; Chen, et al., 1998a). 1.2 Multilingual Clustering The multilingual clusterer takes input from the monolingual clusterers, and determines which news clusters in which languages talk about the same story. Recall that a news cluster consists of several news articles reporting the same event, and one news cluster exists for one event after monolingual clustering. In this way, there is at most one corresponding news cluster in another language. Therefore, the main task of the multilingual news clusterer is to find the matchings among the clusters in different languages. Figure 2 shows an example."
C00-1024,J98-3005,0,0.0377245,"ll present a personal news secretariat that helps on-line readers absorb news information from multiple sources in different languages. Such a news secretariat eliminates the redundant information in the news articles, reorganizes the news for readers, and helps them resolve the language barriers. Reorganization of news is some sort of document summarization, which creates a short version of original document. Recently, many papers touch on single document summarization (Hovy and Marcu, 1998a). Only a few touch on multiple document summarization (Chen and Huang, 1999; Mani and Bloedorn, 1997; Radev and McKeown, 1998) and multilingual document Chuan-Jie Lin Department of Computer Science and Information Engineering National Taiwan University Taipei, TAIWAN, R.O.C. cjlin@nlg2.csie.ntu.edu.tw summarization (Hovy and Marcu, 1998b). For multilingual multiple news summarization, several issues have to be addressed: (1) Translation among news stories in different languages The basic idea in multiple document summarizations is to identify which parts of news articles present similar reports. Because the news stories are in different languages, some kind of translation is required, e.g., term translation. Besides"
C00-1025,A97-1032,0,0.611172,"n presentation scheme for writers to describe schedules, organize statistical data, summarize experimental results, and so on, in texts of different domains. Because tables provide rich information, table acquisition is useful for many applications such as document understanding, question-and-answering, text retrieval, etc. However, most of previous approaches on text data mining focus on text parts, and only few touch on tabular ones (Appelt and Israel, 1997; Gaizauskas and Wilks, 1998; Hurst, 1999a). Of the papers on table extractions (Douglas, Hurst and Quinn, 1995; Douglas and Hurst 1996; Hurst and Douglas, 1997; Ng, Lim and Koo, 1999), plain texts are their targets. In plain text, writers often use special symbols, e.g., tabs, blanks, dashes, etc., to make tables. The following shows an example. It depicts book titles, authors, and prices. title Statistical Language Learning Cross-Language Information Retrieval Natural Language Information Retrieval author E.Charniak G. Grefenstette T.Strzalkowski price $30 $115 $144 When detecting if there is a table in free text, we should disambiguate the uses of the special symbols. That is, the special symbol may be a separator or content of cells. Previous pap"
C00-1025,P99-1057,0,0.073154,"Missing"
C00-1025,E95-1027,0,\N,Missing
C00-1025,O98-4002,0,\N,Missing
C02-1006,P00-1039,0,0.053068,"Missing"
C02-1006,P01-1064,0,\N,Missing
C12-1034,W11-0207,0,0.0526315,"Missing"
C12-1034,2010.eamt-1.31,0,0.0131036,"h as machine translation, natural language generation and computer assisted language learning. For SMT task, paraphrasing is often used to alleviate the data sparseness problem in translation model. For example, we paraphrase a source language text so that the paraphrased parts are easier for the background SMT system to translate. Callison-Burch et al. (2006) pioneered a pivoting approach through parallel corpus to improve phrase-based SMT model. Marton et al. (2009) proposed a monolingual framework to select paraphrases of a term by comparing its context with those of candidate paraphrases. Aziz et al. (2010) proposed a semi-automatic approach to mine paraphrases from hypernyms and hyponyms in ontology. Resnik et al. (2010) 548 (a) (b) (c) 2 – The idea behind our STR framework applied to the bold phrases. We (a) simplify the source diagnosis term before translation, (b) translate the simplified sentence with an SMT system, and (c) restore the original diagnosis term and produce the translation result. FIGURE conducted a pilot study of targeted paraphrasing in which monolingual speakers on both sides collaborate to improve SMT output by paraphrasing the critical segments of source text. Different f"
C12-1034,W09-0432,0,0.033412,"ty between a model and the in-domain development data. Matsoukas et al. (2009) devised sentence-level features and weighted the domain relevance to each sentence in the bilingual training corpus by optimizing an objective function. Foster et al. (2010) further raised the granularity by weighting at the level of phrase pairs. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). Zhao et al. (2004) applied information retrieval techniques to select in-domain documents from large monolingual text collections and enhanced the baseline language model. Bertoldi and Federico (2009) exploited an in-domain monolingual corpus to synthesize a pseudo bilingual corpus and trained an indomain translation model from the synthesized corpus. While previous works concentrated on model and parameter estimation to achieve domain adaptation, they worked on the data sets with similar lexicons. Few studies dealt with large domain gap, which is a practical issue for a cross-domain SMT system. In the medical domain, for example, a term may not even appear in training corpus and therefore SMT system gives no translation to it. This OOV problem is common when translating domain specific te"
C12-1034,N06-1003,0,0.0218042,"as a pre-processing module of the out-domain SMT model which has poor in-domain knowledge. The simplification approach can be viewed as a variant of paraphrasing, which expresses the same meaning in different ways. Paraphrasing is employed for various NLP tasks, such as machine translation, natural language generation and computer assisted language learning. For SMT task, paraphrasing is often used to alleviate the data sparseness problem in translation model. For example, we paraphrase a source language text so that the paraphrased parts are easier for the background SMT system to translate. Callison-Burch et al. (2006) pioneered a pivoting approach through parallel corpus to improve phrase-based SMT model. Marton et al. (2009) proposed a monolingual framework to select paraphrases of a term by comparing its context with those of candidate paraphrases. Aziz et al. (2010) proposed a semi-automatic approach to mine paraphrases from hypernyms and hyponyms in ontology. Resnik et al. (2010) 548 (a) (b) (c) 2 – The idea behind our STR framework applied to the bold phrases. We (a) simplify the source diagnosis term before translation, (b) translate the simplified sentence with an SMT system, and (c) restore the ori"
C12-1034,D07-1007,0,0.0769833,"Missing"
C12-1034,P07-1005,0,0.0186598,"ws the conclusion, indicates the potentials of our method, and shows some future work. 2 Related Work Building an SMT system from large scale bilingual data for a specific application has become a practical option today. On the other hand, SMT model heavily relies on the statistical evidences in the training corpus. As a result, it may learn a biased SMT model, and suffer from the data sparseness problem of the training corpus when dealing with the ambiguity nature of human language. This drawback results in some typical issues such as translation disambiguation problem (Carpuat et al., 2007; Chan et al., 2007), in which a word has several senses, but the corpus is biased towards a particular subset of the senses. The SMT model trained from such a corpus is therefore prone to give the wrong translation due to the wrong choice of sense. 547 When a cross-domain SMT application is concerned, data sparseness problem is worsened by limited in-domain bilingual corpus. Domain adaptation techniques therefore play a key role in building an in-domain SMT system under a resource poor environment. A number of adaptation approaches have been proposed by leveraging either bilingual or monolingual in-domain resour"
C12-1034,2011.mtsummit-papers.31,1,0.810228,"nowledge. For example, bilingual dictionaries can be found in the specific areas with long histories, such as medicine, physics and economics. They provide indomain terminology with high quality and less noise. Not limited to the hand-crafted dictionaries, the bilingual in-domain knowledge may include phrase pairs or synchronous grammar rules, depending on the translation model and the decoding style of our background SMT system. Such bilingual knowledge can be collected by using automatic approaches (Wu and Chang, 2007; Haghighi et al., 2008) or semi-automatic approaches (Morin et al., 2007; Chen et al., 2011). 3.2 Simplification In the simplification step, the identified in-domain segments of a text are transformed into the more general expressions. The modified text is then ready to be translated by the background SMT system. The simplification step serves as a pre-processing step before translation. We simplify an in-domain segment according to its type – say, terminological unit and syntactic unit, in this study. Terminological units refer to terms that appear in domain specific dictionaries and glossaries without specifiers and modifiers. For these in-domain terms, we simplify them by finding"
C12-1034,P05-1033,0,0.0309072,"al in-domain segments are available. It is possible to be combined with other domain adaptation approaches that exploit monolingual or bilingual in-domain corpus to help further improve the translation quality. For example, if a parallel in-domain corpus is available, we can perform learning-based domain adaptation approaches described in Section 2, and tune the background translation model toward the specific domain. In this way, we may receive better translation results from the background SMT system and facilitate the next restoration step. For a phrase-based SMT system and its variations (Chiang, 2005; Xiong et al., 2006; Huang and Chiang, 2007), we can further customize the decoder to produce the translation with higher quality and facilitate the next restoration step of our framework. For example, the in-domain segments in a text are either terminological or syntactic units in our experiments, and therefore their translations are continuous without other interleaving translations. However, an out-domain SMT system may give wrong ordering without in-domain knowledge. For instance, the translation of the medical term ""bone lesion"" is separated as illustrated in FIGURE 4. In our work, we se"
C12-1034,W07-0722,0,0.0586024,"n resources. Foster and Kuhn (2007) proposed a mixture-model approach that divides and trains a bilingual corpus into several models. Different models were then weighted by estimating the similarity between a model and the in-domain development data. Matsoukas et al. (2009) devised sentence-level features and weighted the domain relevance to each sentence in the bilingual training corpus by optimizing an objective function. Foster et al. (2010) further raised the granularity by weighting at the level of phrase pairs. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). Zhao et al. (2004) applied information retrieval techniques to select in-domain documents from large monolingual text collections and enhanced the baseline language model. Bertoldi and Federico (2009) exploited an in-domain monolingual corpus to synthesize a pseudo bilingual corpus and trained an indomain translation model from the synthesized corpus. While previous works concentrated on model and parameter estimation to achieve domain adaptation, they worked on the data sets with similar lexicons. Few studies dealt with large domain gap, which is a practical issue for a cross-domain SMT sys"
C12-1034,D10-1044,0,0.026738,"lding an in-domain SMT system under a resource poor environment. A number of adaptation approaches have been proposed by leveraging either bilingual or monolingual in-domain resources. Foster and Kuhn (2007) proposed a mixture-model approach that divides and trains a bilingual corpus into several models. Different models were then weighted by estimating the similarity between a model and the in-domain development data. Matsoukas et al. (2009) devised sentence-level features and weighted the domain relevance to each sentence in the bilingual training corpus by optimizing an objective function. Foster et al. (2010) further raised the granularity by weighting at the level of phrase pairs. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). Zhao et al. (2004) applied information retrieval techniques to select in-domain documents from large monolingual text collections and enhanced the baseline language model. Bertoldi and Federico (2009) exploited an in-domain monolingual corpus to synthesize a pseudo bilingual corpus and trained an indomain translation model from the synthesized corpus. While previous works concentrated on model and parameter estimation to"
C12-1034,P08-1088,0,0.011748,"a special domain, there are various ways to collect bilingual in-domain knowledge. For example, bilingual dictionaries can be found in the specific areas with long histories, such as medicine, physics and economics. They provide indomain terminology with high quality and less noise. Not limited to the hand-crafted dictionaries, the bilingual in-domain knowledge may include phrase pairs or synchronous grammar rules, depending on the translation model and the decoding style of our background SMT system. Such bilingual knowledge can be collected by using automatic approaches (Wu and Chang, 2007; Haghighi et al., 2008) or semi-automatic approaches (Morin et al., 2007; Chen et al., 2011). 3.2 Simplification In the simplification step, the identified in-domain segments of a text are transformed into the more general expressions. The modified text is then ready to be translated by the background SMT system. The simplification step serves as a pre-processing step before translation. We simplify an in-domain segment according to its type – say, terminological unit and syntactic unit, in this study. Terminological units refer to terms that appear in domain specific dictionaries and glossaries without specifiers a"
C12-1034,P07-1019,0,0.0272359,"e. It is possible to be combined with other domain adaptation approaches that exploit monolingual or bilingual in-domain corpus to help further improve the translation quality. For example, if a parallel in-domain corpus is available, we can perform learning-based domain adaptation approaches described in Section 2, and tune the background translation model toward the specific domain. In this way, we may receive better translation results from the background SMT system and facilitate the next restoration step. For a phrase-based SMT system and its variations (Chiang, 2005; Xiong et al., 2006; Huang and Chiang, 2007), we can further customize the decoder to produce the translation with higher quality and facilitate the next restoration step of our framework. For example, the in-domain segments in a text are either terminological or syntactic units in our experiments, and therefore their translations are continuous without other interleaving translations. However, an out-domain SMT system may give wrong ordering without in-domain knowledge. For instance, the translation of the medical term ""bone lesion"" is separated as illustrated in FIGURE 4. In our work, we set up a Moses (Koehn et al., 2007) SMT system"
C12-1034,P03-1054,0,0.00948156,"Missing"
C12-1034,N03-1017,0,0.00518571,"統的資料稀疏問題。我們取得翻譯結果後，根據提供的領域 內知識將這些重要片段還原。最後我們進行了醫療領域的英中翻譯的實驗，並且評估STR 架構內的每一步驟。翻譯結果顯示我們的方法顯著地優於領域外的SMT系統，以及簡易型 的領域內SMT系統。 KEYWORDS : Cross-Domain SMT, Domain Adaptation, Statistical Machine Translation 關鍵詞 : 跨領域統計式機器翻譯, 領域調適, 統計式機器翻譯 Proceedings of COLING 2012: Technical Papers, pages 545–560, COLING 2012, Mumbai, December 2012. 545 1 Introduction Over the past decades, the rapid growth of available parallel corpus makes SMT development feasible, and SMT system has gradually moved toward practical use because of its relatively acceptable translation speed and quality. A phrase-based SMT system (Koehn et al., 2003; Koehn, 2004), for example, trains a phrase table from a large bilingual corpus as its translation model, and decodes source language input in polynomial time with greedy algorithms such as beam search. It translates phrases as basic units, and thus captures short-range reordering phenomena between source and target languages. Generally phrase-based SMT models outperform word-based ones (Koehn et al., 2003). However, an SMT system fails to capture long-range contextual knowledge due to the limited horizon and the sparseness nature of lexical n-grams. These drawbacks reduce the translation qua"
C12-1034,koen-2004-pharaoh,0,0.0310985,"據提供的領域 內知識將這些重要片段還原。最後我們進行了醫療領域的英中翻譯的實驗，並且評估STR 架構內的每一步驟。翻譯結果顯示我們的方法顯著地優於領域外的SMT系統，以及簡易型 的領域內SMT系統。 KEYWORDS : Cross-Domain SMT, Domain Adaptation, Statistical Machine Translation 關鍵詞 : 跨領域統計式機器翻譯, 領域調適, 統計式機器翻譯 Proceedings of COLING 2012: Technical Papers, pages 545–560, COLING 2012, Mumbai, December 2012. 545 1 Introduction Over the past decades, the rapid growth of available parallel corpus makes SMT development feasible, and SMT system has gradually moved toward practical use because of its relatively acceptable translation speed and quality. A phrase-based SMT system (Koehn et al., 2003; Koehn, 2004), for example, trains a phrase table from a large bilingual corpus as its translation model, and decodes source language input in polynomial time with greedy algorithms such as beam search. It translates phrases as basic units, and thus captures short-range reordering phenomena between source and target languages. Generally phrase-based SMT models outperform word-based ones (Koehn et al., 2003). However, an SMT system fails to capture long-range contextual knowledge due to the limited horizon and the sparseness nature of lexical n-grams. These drawbacks reduce the translation quality in terms"
C12-1034,P07-2045,0,0.00491577,", 2006; Huang and Chiang, 2007), we can further customize the decoder to produce the translation with higher quality and facilitate the next restoration step of our framework. For example, the in-domain segments in a text are either terminological or syntactic units in our experiments, and therefore their translations are continuous without other interleaving translations. However, an out-domain SMT system may give wrong ordering without in-domain knowledge. For instance, the translation of the medical term ""bone lesion"" is separated as illustrated in FIGURE 4. In our work, we set up a Moses (Koehn et al., 2007) SMT system and apply its advanced feature of specifying reordering constraint to each of the simplified phrases. Under the constraint, a simplified phrase is translated as a block and its translation is continuous on the target side. FIGURE 3.4 4 – The incorrect reordering of the in-domain segments ""bone lesion"". Restoration In the restoration step, we receive the translation result from the background SMT system and perform post-processing steps. We locate the simplified phrase pairs and replace them with their corresponding bilingual in-domain segments as illustrated in FIGURE 2(c). The res"
C12-1034,D09-1040,0,0.0142004,"can be viewed as a variant of paraphrasing, which expresses the same meaning in different ways. Paraphrasing is employed for various NLP tasks, such as machine translation, natural language generation and computer assisted language learning. For SMT task, paraphrasing is often used to alleviate the data sparseness problem in translation model. For example, we paraphrase a source language text so that the paraphrased parts are easier for the background SMT system to translate. Callison-Burch et al. (2006) pioneered a pivoting approach through parallel corpus to improve phrase-based SMT model. Marton et al. (2009) proposed a monolingual framework to select paraphrases of a term by comparing its context with those of candidate paraphrases. Aziz et al. (2010) proposed a semi-automatic approach to mine paraphrases from hypernyms and hyponyms in ontology. Resnik et al. (2010) 548 (a) (b) (c) 2 – The idea behind our STR framework applied to the bold phrases. We (a) simplify the source diagnosis term before translation, (b) translate the simplified sentence with an SMT system, and (c) restore the original diagnosis term and produce the translation result. FIGURE conducted a pilot study of targeted paraphrasi"
C12-1034,D09-1074,0,0.0350734,"oss-domain SMT application is concerned, data sparseness problem is worsened by limited in-domain bilingual corpus. Domain adaptation techniques therefore play a key role in building an in-domain SMT system under a resource poor environment. A number of adaptation approaches have been proposed by leveraging either bilingual or monolingual in-domain resources. Foster and Kuhn (2007) proposed a mixture-model approach that divides and trains a bilingual corpus into several models. Different models were then weighted by estimating the similarity between a model and the in-domain development data. Matsoukas et al. (2009) devised sentence-level features and weighted the domain relevance to each sentence in the bilingual training corpus by optimizing an objective function. Foster et al. (2010) further raised the granularity by weighting at the level of phrase pairs. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). Zhao et al. (2004) applied information retrieval techniques to select in-domain documents from large monolingual text collections and enhanced the baseline language model. Bertoldi and Federico (2009) exploited an in-domain monolingual corpus to synt"
C12-1034,P07-1084,0,0.0134218,"ilingual in-domain knowledge. For example, bilingual dictionaries can be found in the specific areas with long histories, such as medicine, physics and economics. They provide indomain terminology with high quality and less noise. Not limited to the hand-crafted dictionaries, the bilingual in-domain knowledge may include phrase pairs or synchronous grammar rules, depending on the translation model and the decoding style of our background SMT system. Such bilingual knowledge can be collected by using automatic approaches (Wu and Chang, 2007; Haghighi et al., 2008) or semi-automatic approaches (Morin et al., 2007; Chen et al., 2011). 3.2 Simplification In the simplification step, the identified in-domain segments of a text are transformed into the more general expressions. The modified text is then ready to be translated by the background SMT system. The simplification step serves as a pre-processing step before translation. We simplify an in-domain segment according to its type – say, terminological unit and syntactic unit, in this study. Terminological units refer to terms that appear in domain specific dictionaries and glossaries without specifiers and modifiers. For these in-domain terms, we simpl"
C12-1034,W99-0604,0,0.104326,"ments and the thin lines are word alignments. FIGURE In some cases, a simplified phrase is translated together with its nearby context, and therefore we need to determine the translation of the simplified phrase before we can restore its original form, such as the term ""hypertension"" in FIGURE 2(b). Phrase level alignments are insufficient now and higher granularity is needed, such as word-level alignments, to separate a simplified phrase from 553 its contexts. With word alignment information, we may extract a simplified phrase pair without violating the consistency judgment of a phrase pair (Och et al., 1999). We then replace the simplified phrase pair with its original bilingual in-domain segment. Our word alignment method is illustrated in FIGURE 5(b). The hollow blocks denote the context f2f3, which is translated together with its nearby simplified term f1. The thin lines and the thick lines are word alignments and phrase alignments, respectively. Compliant with the consistency of phrase extraction, we separate the phrase pair f1-e1 from the phrase pair f1f2f3-e1e2e3, and perform the restoration. The method is exemplified in FIGURE 6(b). Based on the word alignments, the simplified term ""hypert"
C12-1034,P02-1040,0,0.0851785,"Missing"
C12-1034,D10-1013,0,0.100809,"and report the experimental results of the translation tasks on medical summaries in a hospital. The central issues in this framework include (1) collecting bilingual in-domain knowledge and identifying indomain segments in a source text, (2) replacing the in-domain segments with the proper simplified forms, (3) translating the modified text with a background SMT system and (4) restoring the original in-domain segments after receiving the translation results from the background SMT system. We are not the first to rephrase source language text in order to improve SMT output. As a pilot study, Resnik et al. (2010) proposed a targeted paraphrasing approach which identifies the critical source segments difficult for the background SMT system to translate. These segments are then manually paraphrased in many ways in order to provide the SMT system with more choices of decoding paths. Different from their work, we automatically identify the critical segments with in-domain knowledge, simplify them with linguistic information, and restore these critical segments after receiving the SMT results. This paper is organized as follows. In Section 2, we review the previous works on domain adaptation and related wo"
C12-1034,2006.amta-papers.25,0,0.0448089,"Missing"
C12-1034,D07-1077,0,0.0211107,"cular tachycardia and dyslipidemia"" to ""with diseases"". (d) S (Clause) We simplify a clause by simplifying its children recursively according to the above simplification rules. FIGURE 3 – A parse tree of a syntactic unit. A bracketed string at each non-terminal node is the head of the corresponding syntactic category. The rule-based simplification approach is straightforward, but can be effectively applied to most of the syntactic units as discussed in Section 4. Applying transformation or rewriting rules on source sentences based on their syntactic structures has been adopted in other works. Wang et al. (2007) listed a set of prominent syntactic reordering rules that systematically describe the word order difference between the source and target languages. Based on these rules, they parsed a source language input and reordered its structure to match the target language grammar for training a better translation model and improving a phrase-based SMT system. In their work, source side syntactic reordering also serves as a pre-processing module of the SMT system. Different from their work, we simplify a source language input in favour of the background SMT system instead of changing the order of its s"
C12-1034,D11-1038,0,0.148502,"en appear in training corpus and therefore SMT system gives no translation to it. This OOV problem is common when translating domain specific terms such as diagnosis and surgical names in biomedical literature or medical records using a general purpose SMT system. Different from the previous works, we address cross-domain issues in SMT across two largely distinct domains by simplifying in-domain segments to the ones that can be recognized by the out-domain or the background SMT system, and restoring the in-domain segments after receiving the SMT results. Text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) itself has some straightforward NLP applications. For example, we produce a simpler version of a text by modifying the lexical contents and shortening the grammatical structures without changing the original text at the semantic level. Such simplified contents are beneficial for language learners and people with lower levels of literacy. One of the real world applications is Simple English Wikipedia (http://simple.wikipedia.org), which uses simple English words and grammar, and thus English language learners can benefit from it. In this paper we apply sentence simplifica"
C12-1034,P12-1107,0,0.189295,"Missing"
C12-1034,P06-1066,0,0.0222127,"egments are available. It is possible to be combined with other domain adaptation approaches that exploit monolingual or bilingual in-domain corpus to help further improve the translation quality. For example, if a parallel in-domain corpus is available, we can perform learning-based domain adaptation approaches described in Section 2, and tune the background translation model toward the specific domain. In this way, we may receive better translation results from the background SMT system and facilitate the next restoration step. For a phrase-based SMT system and its variations (Chiang, 2005; Xiong et al., 2006; Huang and Chiang, 2007), we can further customize the decoder to produce the translation with higher quality and facilitate the next restoration step of our framework. For example, the in-domain segments in a text are either terminological or syntactic units in our experiments, and therefore their translations are continuous without other interleaving translations. However, an out-domain SMT system may give wrong ordering without in-domain knowledge. For instance, the translation of the medical term ""bone lesion"" is separated as illustrated in FIGURE 4. In our work, we set up a Moses (Koehn"
C12-1034,C04-1059,0,0.0184736,"Kuhn (2007) proposed a mixture-model approach that divides and trains a bilingual corpus into several models. Different models were then weighted by estimating the similarity between a model and the in-domain development data. Matsoukas et al. (2009) devised sentence-level features and weighted the domain relevance to each sentence in the bilingual training corpus by optimizing an objective function. Foster et al. (2010) further raised the granularity by weighting at the level of phrase pairs. Similarly, a mixture-model approach was also applied in word-alignment task (Civera and Juan, 2007). Zhao et al. (2004) applied information retrieval techniques to select in-domain documents from large monolingual text collections and enhanced the baseline language model. Bertoldi and Federico (2009) exploited an in-domain monolingual corpus to synthesize a pseudo bilingual corpus and trained an indomain translation model from the synthesized corpus. While previous works concentrated on model and parameter estimation to achieve domain adaptation, they worked on the data sets with similar lexicons. Few studies dealt with large domain gap, which is a practical issue for a cross-domain SMT system. In the medical"
C12-1034,C10-1152,0,0.135047,"a term may not even appear in training corpus and therefore SMT system gives no translation to it. This OOV problem is common when translating domain specific terms such as diagnosis and surgical names in biomedical literature or medical records using a general purpose SMT system. Different from the previous works, we address cross-domain issues in SMT across two largely distinct domains by simplifying in-domain segments to the ones that can be recognized by the out-domain or the background SMT system, and restoring the in-domain segments after receiving the SMT results. Text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) itself has some straightforward NLP applications. For example, we produce a simpler version of a text by modifying the lexical contents and shortening the grammatical structures without changing the original text at the semantic level. Such simplified contents are beneficial for language learners and people with lower levels of literacy. One of the real world applications is Simple English Wikipedia (http://simple.wikipedia.org), which uses simple English words and grammar, and thus English language learners can benefit from it. In this paper w"
C12-1034,2010.amta-workshop.3,0,\N,Missing
C12-1184,A00-2019,0,0.135509,"Missing"
C12-1184,P03-2026,0,0.0290259,"Missing"
C12-1184,N07-2024,0,0.0354777,"Missing"
C12-1184,lin-etal-2010-new,0,0.041876,"Missing"
C12-1184,P00-1012,0,0.0505173,"ductory element, while the most frequent error for ESL (English as a Second Language) students is comma splice, which joins two sentences by using comma instead of a conjunction (Leacock et al., 2010). In addition, learners may generate sentences with multiple types of errors. This complicates the error detection. To simplify the problem and evaluate the performance of the proposed methods, researchers usually detect one type of error at a time, such as common grammatical errors (Islam & Inkpen, 2011; Wagner, Foster & Genabith, 2007) and prenominal adjective ordering errors (Lin et al., 2009; Malouf, 2000). A training dataset is indispensable for learning an error detection system. In English, there are many well-established corpora for this purpose such as Cambridge Learner Corpus (CLS). In Chinese, Beijing Language and Culture University built an HSK dynamic composition corpus ( /dynamic /composition /corpus) and announced a publicly accessible online system to query this corpus1. The HSK corpus contains Chinese compositions written by Chinese language learners in the university. The students’ compositions were collected and annotated with different error types. In this paper, we will deal wi"
C12-1184,N03-1033,0,0.00507176,"lueWeb09. Although most of the Chinese records are crawled from mainland China, there are still some pages in other languages, such as Japanese. In addition, a Chinese web page may be in different encodings, e.g., Big5, CNS 11643, GBK, GB2312, and Unicode. Thus, the encoding detection and language identification problems have to be dealt with in the development of the CP5g dataset. After identifying the encoding scheme and the written language, they use the Stanford Chinese word segmenter (Tseng, Chang, Andrew, Jurafsky, & Manning, 2005) to segment the text, and adopt the Stanford POS tagger (Toutanova, Klein, Manning, & Singer, 2003) to tag Chinese sentences. After all n-gram (n=1, 2, 3, 4, 5) patterns are extracted, those patterns that occur less than 40 times are filtered out. The output format is similar to Google N-gram V2. 3007 We also use the Google Chinese Web 5-grams (GC5g) (Liu et al., 2010) corpus for comparison. The size of GC5g is larger than that of CP5g, but GC5g does not contain linguistic information. In addition, GC5g and CP5g are developed by different segmentation systems, i.e., a maximum probability segmenter and a CRF-based segmenter, respectively. That may result in different word patterns and intro"
C12-1184,D07-1012,0,0.108854,"Missing"
C12-1184,yu-etal-2012-development,1,0.899859,"Missing"
C12-1184,J90-1003,0,\N,Missing
C12-2119,I08-2083,1,0.90093,"Missing"
C12-2119,R11-1093,0,0.0279425,"d IJCAI 2009. Previous Internet advertising focuses on bidding (selecting) advertisements and placing them in the best (right) positions. Ghosh et al. (2009) proposed bidding strategies for the allocations of advertisements. Edelman, Ostrovsky and Schwarz (2007) investigated generalized second-price (GSP) auction for online advertising. Huang, Lin, and Chen (2008) classified instant messaging dialogues into the Yahoo categories, and applied the method to advertisement recommendation. Cheng and Cantú-Paz (2010) proposed a framework to predict the probability that individual users click on ads. Scaiano and Inkpen (2011) used Wikipedia as an annotated corpus to find negative key phrases to avoid displaying advertisements to non-target audience. Unlike advertisement bidding, matching and recommendation in computational advertising, this paper deals with illegal advertisement recognition. Illegal advertising is similar to ad spam1 in financial gain, but the former exploits false, overstated or misleading statements to defraud customers, and the latter creates artificial ad traffic, inflates click/impression, and so on, to defraud online advertising systems like AdWords. To the best of our knowledge, advertising"
C12-2119,tang-chen-2012-mining,1,0.823066,"ommon words are included as incorrect expansion. The DOH feature set includes lists that are edited by professionals in the 1223 government, so it captures illegal advertising statements that are in actual use. However, the coverage is an issue. Section 4 discusses how to expand this list. 3.3 Feature Set 3: Log Relative Frequency Ratio Relative frequency ratio between two datasets has been shown to be useful to discover collocations that are characteristic of a dataset when compared to the other dataset (Damerau, 1993). It is also used to model emotion transition between writers and readers (Tang and Chen, 2012). We extend this idea to select the critical features that capture the legality transition. The log relative frequency ratio lr of words in two datasets A and B are defined as follows. For each wi∈A∪B, we compute f A ( wi ) |A| lrAB ( w ) = log f B ( wi ) |B| i where lrAB(wi) is a log ratio of relative frequencies of word wi in A and B, fA(wi) and fB(wi) are frequencies of wi in A and in B, and |A |and |B |are total words in A and in B, respectively. The log relative frequency ratios are used to estimate the distribution of the words in datasets A and B. The interpretations of lrAB(wi) are sho"
C12-2119,P08-5001,0,\N,Missing
C12-3028,I11-1170,1,0.646173,"Missing"
C12-3028,miltsakaki-etal-2004-penn,0,0.0650152,"Missing"
C12-3028,W12-1636,1,\N,Missing
C12-3028,P04-1087,0,\N,Missing
C12-3028,prasad-etal-2008-penn,0,\N,Missing
C12-3028,P12-1008,0,\N,Missing
C12-3028,W00-1205,0,\N,Missing
C12-3029,R09-1010,0,0.017741,"uded their ratio as 7:3. Similar supporting evidences were also found in various other languages, e.g., English (Augustine et al., 2011; Garcia et al, 2012; Kloumann et al, 2012), Italian (Suitner and Maass, 2008), German and Spanish (Garcia et al, 2012), and even across 20 different languages (Rozin et al., 2010). In contrast, only a few works addressed this particular issue in opinion mining and sentiment analysis. These papers (Bolasco and della Ratta-Rinaldi, 2004; Brooke, 2009; Mohammad et al., 2009) demonstrated supporting evidences of the Pollyanna Hypothesis. Taboada et al. (2009) and Brooke et al. (2009) claimed the positivity bias could affect lexicon-based sentiment analysis systems like those of Kennedy and Diana (2006), and proposed an adjusting strategy (Taboada et al., 2011). The contribution of this paper is three-fold: (1) To the best of our knowledge, we conduct the first detailed survey of the Pollyanna phenomena in various modern Chinese corpora. (2) Through quantitative and qualitative analyses, we discover that for the documents with relatively fewer positive words, the intra-polarity document similarity, of either positive or negative opinion polarity, significantly increases. ("
C12-3029,D09-1063,0,0.0203045,"the positive and negative Chinese words based on the Modern Chinese Frequency Dictionary (Wang 1986), and concluded their ratio as 7:3. Similar supporting evidences were also found in various other languages, e.g., English (Augustine et al., 2011; Garcia et al, 2012; Kloumann et al, 2012), Italian (Suitner and Maass, 2008), German and Spanish (Garcia et al, 2012), and even across 20 different languages (Rozin et al., 2010). In contrast, only a few works addressed this particular issue in opinion mining and sentiment analysis. These papers (Bolasco and della Ratta-Rinaldi, 2004; Brooke, 2009; Mohammad et al., 2009) demonstrated supporting evidences of the Pollyanna Hypothesis. Taboada et al. (2009) and Brooke et al. (2009) claimed the positivity bias could affect lexicon-based sentiment analysis systems like those of Kennedy and Diana (2006), and proposed an adjusting strategy (Taboada et al., 2011). The contribution of this paper is three-fold: (1) To the best of our knowledge, we conduct the first detailed survey of the Pollyanna phenomena in various modern Chinese corpora. (2) Through quantitative and qualitative analyses, we discover that for the documents with relatively fewer positive words, the i"
C12-3029,W09-3909,0,0.0156673,"ary (Wang 1986), and concluded their ratio as 7:3. Similar supporting evidences were also found in various other languages, e.g., English (Augustine et al., 2011; Garcia et al, 2012; Kloumann et al, 2012), Italian (Suitner and Maass, 2008), German and Spanish (Garcia et al, 2012), and even across 20 different languages (Rozin et al., 2010). In contrast, only a few works addressed this particular issue in opinion mining and sentiment analysis. These papers (Bolasco and della Ratta-Rinaldi, 2004; Brooke, 2009; Mohammad et al., 2009) demonstrated supporting evidences of the Pollyanna Hypothesis. Taboada et al. (2009) and Brooke et al. (2009) claimed the positivity bias could affect lexicon-based sentiment analysis systems like those of Kennedy and Diana (2006), and proposed an adjusting strategy (Taboada et al., 2011). The contribution of this paper is three-fold: (1) To the best of our knowledge, we conduct the first detailed survey of the Pollyanna phenomena in various modern Chinese corpora. (2) Through quantitative and qualitative analyses, we discover that for the documents with relatively fewer positive words, the intra-polarity document similarity, of either positive or negative opinion polarity, s"
C12-3029,J11-2001,0,0.00830488,"Italian (Suitner and Maass, 2008), German and Spanish (Garcia et al, 2012), and even across 20 different languages (Rozin et al., 2010). In contrast, only a few works addressed this particular issue in opinion mining and sentiment analysis. These papers (Bolasco and della Ratta-Rinaldi, 2004; Brooke, 2009; Mohammad et al., 2009) demonstrated supporting evidences of the Pollyanna Hypothesis. Taboada et al. (2009) and Brooke et al. (2009) claimed the positivity bias could affect lexicon-based sentiment analysis systems like those of Kennedy and Diana (2006), and proposed an adjusting strategy (Taboada et al., 2011). The contribution of this paper is three-fold: (1) To the best of our knowledge, we conduct the first detailed survey of the Pollyanna phenomena in various modern Chinese corpora. (2) Through quantitative and qualitative analyses, we discover that for the documents with relatively fewer positive words, the intra-polarity document similarity, of either positive or negative opinion polarity, significantly increases. (3) Based on our findings, we propose a strategy for sentiment classification and improve performance significantly. 2 Word-Level Linguistic Positivity Bias Sentimental Information"
C12-3063,D11-1142,0,0.0610883,"Missing"
C12-3063,lin-etal-2010-new,0,0.0978925,"tration Papers, pages 501–508, COLING 2012, Mumbai, December 2012. 501 1 Introduction While using a large volume of data becomes a new paradigm in many NLP applications, preparing a web scale data collection are time consuming and need much cost. Recently, various versions of Gigawords which are comprehensive archive of newswire text data in Arabic, Chinese, English, French, and Spanish are distributed through LDC 1 to boost the researches. Besides newswire text, web n-grams in Chinese, English, Japanese, and 10 European languages created by Google researchers also released via LDC. Moreover, Lin et al. (2010) extend an English word n-gram corpus by adding parts of speech information and develop tools to accelerate the query speed. In contrast to the web n-gram corpora, the ClueWeb 2 dataset developed by Carnegie Mellon University (CMU) contains a huge collection of raw web pages for researchers. It provides an alternative to construct corpora with fruitful context information rather than n-grams only. In this paper we extract the Chinese materials from the ClueWeb dataset, and develop two Chinese datasets, including a Chinese segmented and POS-tagged dataset called PText, and a Chinese POS n-gram"
C12-3063,N03-1033,0,0.00716052,"Missing"
C12-3063,I05-3027,0,0.10205,"Missing"
C12-3063,J04-3002,0,0.0111489,"ons with the PText dataset The PText dataset is beneficial for many potential researches. We outline some interesting applications below. (1) Knowledge mining. The PText dataset can be used as the source of OpenIE (Etzioni et al., 2008). In ReVerb, Fader, Soderland, and Etzioni (2011) use simple POS patterns to mine facts from the web texts in English. Similarly, researchers can extract facts from Chinese texts by specifying Chinese POS patterns. With the PText dataset, considerable preprocessing time can be saved for Chinese OpenIE researchers. (2) Sentiment analysis. As shown by the papers (Wiebe et al., 2004; Abbasi, Chen, and Salem, 2008), parts-of-speech information is useful for many sentiment analysis tasks such as opinion classification and subjectivity analysis of web texts. With the large-scale PText dataset, researchers can investigate more rich phenomena in the web texts. 504 (3) Basic NLP tasks. Besides the new and interesting researches, the fundamental NLP tasks can also benefit from the PText dataset, e.g., the encoding detection and language identification in pre-processing Chinese web pages, the performance of Chinese word segmenters and POS taggers in large scale web texts, and so"
C12-3063,yu-etal-2012-development,1,0.794253,"Missing"
C12-3063,C12-1184,1,0.790739,"E 1 –A user interface to query the Chinese POS-NGram corpus 唸书 In Figure 1, users search bigram patterns. The first word is (read), and it must be Noun, Verb/Adjective, or tags NN, NT, VV, which are selected in Figure 2. The second word can be any words with Noun, or NN, NT and NT tags. The word frequency is limited between 40 to 2,000 times. The returned POS n-gram results are similar to the examples shown in Section 2.2. 505 FIGURE 2 –Select specific POS tags for a word 4.2 Applications of the PNgram Corpus The PNgram corpus along with the user interface is useful for many NLP applications. Yu and Chen (2012) employ it to detect Chinese word order errors. We outline some others as follows. (1) Language Learning. In Chinese learning, we may be interested in what linguistic context some specific reduplication forms like “ ” (happy happy) appear. Through the interface, it is easy to collect their usages from the web data. Similarly, the uses of measure words in Chinese sentences are very common. The PNgram corpus along with the tool provides a flexible way to analyze the measure words for a specific word in web texts. (2) Pattern Identification for Information Extraction (IE). In IE, a named entity u"
C14-1028,W12-2006,0,0.0335672,"matical errors made by nonnative Chinese learners, (2) word ordering errors in Chinese language, (3) computer processing of grammatical errors in Chinese language, and (4) grammatical error correction in other languages. Leacock et al. (2014) give thorough surveys in automated grammatical error detection for language learners. Error types, available corpora, evaluation methods, and approaches for different types of errors are specified. Several shared tasks on grammatical error correction in English have been organized in recent years, including HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012) and CoNLL 2013 (Ng et al., 2013). Different types of grammatical errors are focused: (1) HOO 2011: article and preposition errors, (2) HOO 2012: determiner and preposition errors, and (3) CoNLL 2013: article or determiner errors, preposition errors, noun number errors, verb form errors, and subject-verb agreement errors. In Chinese, spelling check evaluation was held at SIGHAN Bake-off 2013 (Wu et al., 2013). However, none of the above evaluations deals with word ordering errors. Wang (2011) focuses on the Chinese teaching for native English-speaking students. He shows the most frequent gramm"
C14-1028,W11-2838,0,0.0549594,"d work from the four aspects: (1) grammatical errors made by nonnative Chinese learners, (2) word ordering errors in Chinese language, (3) computer processing of grammatical errors in Chinese language, and (4) grammatical error correction in other languages. Leacock et al. (2014) give thorough surveys in automated grammatical error detection for language learners. Error types, available corpora, evaluation methods, and approaches for different types of errors are specified. Several shared tasks on grammatical error correction in English have been organized in recent years, including HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012) and CoNLL 2013 (Ng et al., 2013). Different types of grammatical errors are focused: (1) HOO 2011: article and preposition errors, (2) HOO 2012: determiner and preposition errors, and (3) CoNLL 2013: article or determiner errors, preposition errors, noun number errors, verb form errors, and subject-verb agreement errors. In Chinese, spelling check evaluation was held at SIGHAN Bake-off 2013 (Wu et al., 2013). However, none of the above evaluations deals with word ordering errors. Wang (2011) focuses on the Chinese teaching for native English-speaking students. He"
C14-1028,D11-1018,0,0.0264078,"ncies of POS n-grams and the outputs of parsers as features. Gamon et al. (2009) identify and correct errors made by non-native English writers. They first detect article and preposition errors, and then apply different techniques to correct each type of errors. Huang et al. (2010) propose a correction rule extraction model trained from 310,956 sets of erroneous and corrected pairwise sentences. Some studies related to word orderings are specific to the topic of pre-processing or post-processing of statistical machine translation, such as Galley and Manning (2008), Setiawan et al. (2009), and DeNero and Uszkoreit (2011). The major contributions of this paper cover the following aspects: (1) application aspect: detecting and correcting a common type of Chinese written errors of foreign learners with HSK corpus; (2) language aspect: considering the effects of words and segments in Chinese sentences; and (3) resource aspect: exploring the feasibility of using a Chinese web n-gram corpus in WOE detection/correction. 3 Overview of a Chinese Word Ordering Detection and Correction System Figure 1 sketches an overview of our Chinese WOE detection and correction system. It is composed of three major parts, including"
C14-1028,D08-1089,0,0.0329214,"mon grammatical errors in English. They consider frequencies of POS n-grams and the outputs of parsers as features. Gamon et al. (2009) identify and correct errors made by non-native English writers. They first detect article and preposition errors, and then apply different techniques to correct each type of errors. Huang et al. (2010) propose a correction rule extraction model trained from 310,956 sets of erroneous and corrected pairwise sentences. Some studies related to word orderings are specific to the topic of pre-processing or post-processing of statistical machine translation, such as Galley and Manning (2008), Setiawan et al. (2009), and DeNero and Uszkoreit (2011). The major contributions of this paper cover the following aspects: (1) application aspect: detecting and correcting a common type of Chinese written errors of foreign learners with HSK corpus; (2) language aspect: considering the effects of words and segments in Chinese sentences; and (3) resource aspect: exploring the feasibility of using a Chinese web n-gram corpus in WOE detection/correction. 3 Overview of a Chinese Word Ordering Detection and Correction System Figure 1 sketches an overview of our Chinese WOE detection and correctio"
C14-1028,W95-0107,0,0.0892206,"Missing"
C14-1028,P09-1037,0,0.0246582,"nglish. They consider frequencies of POS n-grams and the outputs of parsers as features. Gamon et al. (2009) identify and correct errors made by non-native English writers. They first detect article and preposition errors, and then apply different techniques to correct each type of errors. Huang et al. (2010) propose a correction rule extraction model trained from 310,956 sets of erroneous and corrected pairwise sentences. Some studies related to word orderings are specific to the topic of pre-processing or post-processing of statistical machine translation, such as Galley and Manning (2008), Setiawan et al. (2009), and DeNero and Uszkoreit (2011). The major contributions of this paper cover the following aspects: (1) application aspect: detecting and correcting a common type of Chinese written errors of foreign learners with HSK corpus; (2) language aspect: considering the effects of words and segments in Chinese sentences; and (3) resource aspect: exploring the feasibility of using a Chinese web n-gram corpus in WOE detection/correction. 3 Overview of a Chinese Word Ordering Detection and Correction System Figure 1 sketches an overview of our Chinese WOE detection and correction system. It is composed"
C14-1028,D07-1012,0,0.0655446,"Missing"
C14-1028,W13-4406,0,0.0576877,"s of errors are specified. Several shared tasks on grammatical error correction in English have been organized in recent years, including HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012) and CoNLL 2013 (Ng et al., 2013). Different types of grammatical errors are focused: (1) HOO 2011: article and preposition errors, (2) HOO 2012: determiner and preposition errors, and (3) CoNLL 2013: article or determiner errors, preposition errors, noun number errors, verb form errors, and subject-verb agreement errors. In Chinese, spelling check evaluation was held at SIGHAN Bake-off 2013 (Wu et al., 2013). However, none of the above evaluations deals with word ordering errors. Wang (2011) focuses on the Chinese teaching for native English-speaking students. He shows the most frequent grammatical errors made by foreigners are missing components, word orderings and sentence structures. One major learning problem of foreign learners is the influence of negative transfer of mother tongue. Lin (2011) studies the biased errors of word order in Chinese written by foreign students in the HSK corpus. Sun (2011) compares the word orderings between English and Chinese to figure out the differences in sen"
C14-1028,C12-1184,1,0.479076,"lp.blcu.edu.cn/online-systems/hsk-language-lib-indexing-system.html), there are 35,884 errors at sentence level. The top 10 error types and their occurrences are listed below: Word Ordering Errors (WOE) (8,515), Missing Component (Adverb) (3,244), Missing Component (Predicate) (3,018), Grammatical Error (“Is … DE”) (2,629), Missing Component (Subject) (2,405), Missing Component (Head Noun) (2364), Grammatical Error (“Is” sentence) (1,427), Redundant Component (Predicate) (1,130), Uncompleted Sentence (1,052), and Redundant Component (Adverb) (1,051). WOEs are the most frequent type of errors (Yu and Chen, 2012). The types of WOEs in Chinese are different from those in English. A Chinese character has its own meaning in text, while individual characters are meaningless in English. Learners taking Chinese as a foreign language often place character(s) in the wrong places in sentences, and that results in wrong word(s) or ungrammatical sentences. Besides, there are no clear word boundaries in Chinese sentences. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org"
C14-1028,O10-5004,0,\N,Missing
C14-1028,W13-3601,0,\N,Missing
C14-1060,O06-1001,0,0.0234419,"ink to the previous or the next discourse units. S5 and S6 have the similar behaviors. The singleword “但是” (dàn shì, but) in (S5) shows a backward-linking. In (S6), it is shifted to the first position and becomes ambiguous. It may be linked to the previous, or to the next discourse units. The correct interpretation depends on the context. These phenomena show a single-word connective may have different senses when it is not at its original position. Figure 1: Examples for forward linkging and backward linking. In this study, we collect 808 discourse connectives based on Cheng and Tian (1989), Cheng (2006), and Lu (2007). The discourse connective lexicon contains 319 single-word and 489 word-pair connectives. Initially, each connective is associated with only one discourse function manually by linguists. 634 For example, the word-pair connective, “雖然...但是” (suī rán…dàn shì, although…but), is assigned a Comparison function. The assignment is one-to-one mapping, thus it cannot capture the complete discourse functions of Chinese connectives. Table 1 shows an overview of the discourse connective lexicon. In this lexicon, Expansion is the majority, and Comparison is the minority. The percentages of"
C14-1060,P13-2114,0,0.0349182,"Missing"
C14-1060,I11-1170,1,0.868821,"e dataset. 3.1 A Semi-Supervised Learning Algorithm Given a pair of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expansion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1…c2, where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. Each discourse unit is mapped into a representation. Various features from different linguistic levels have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particular on the probability distributions of the discourse functions and the positions of connectives. Length. This feature includes the word counts of ds1 and ds2. Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is dropped from the features because it is always a comma. Words. The bags of words in ds1 and ds2 are consi"
C14-1060,W12-1636,1,0.859959,"Supervised Learning Algorithm Given a pair of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expansion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1…c2, where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. Each discourse unit is mapped into a representation. Various features from different linguistic levels have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particular on the probability distributions of the discourse functions and the positions of connectives. Length. This feature includes the word counts of ds1 and ds2. Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is dropped from the features because it is always a comma. Words. The bags of words in ds1 and ds2 are considered. Hypernym. The b"
C14-1060,C12-3028,1,0.796366,"Supervised Learning Algorithm Given a pair of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expansion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1…c2, where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. Each discourse unit is mapped into a representation. Various features from different linguistic levels have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particular on the probability distributions of the discourse functions and the positions of connectives. Length. This feature includes the word counts of ds1 and ds2. Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is dropped from the features because it is always a comma. Words. The bags of words in ds1 and ds2 are considered. Hypernym. The b"
C14-1060,W13-2309,1,0.906501,"rse relation labeling determines how two discourse units cohere to each other. A discourse unit may be a clause, a sentence, or a group of sentences. The labeled relation has many potential applications. Coherence is considered as a metric to evaluate the essay writing by essay scorer (Lin et al., 2011). Discourse relations are used to order sentences in an event in a summarization system (Derczynski and Gaizauskas, 2013). Sentiment transition of two clausal arguments is identified based on their discourse relation in sentiment analysis (Hutchinson, 2004; Zhou et al., 2011; Wang et al., 2012; Huang et al., 2013). The pioneer research of discourse has been established by Hobbs (1985), Polanyi (1988), Hovy and Maier (1992), and Asher and Lascarides (1995). Various discourse relation types have been defined in the frameworks such as Sanders et al. (1992), Hovy and Maier (1992), RST-DT (Carlson et al., 2002), Wolf and Gibson (2005), and PDTB (Prasad et al., 2008). Temporal, Contingency, Comparison, and Expansion, the four classes on the top level of PDTB sense hierarchy, are common used in the discourse relation labeling tasks. When two arguments are temporally related, they form a Temporal relation. The"
C14-1060,P04-1087,0,0.0967729,"Missing"
C14-1060,P11-1100,0,0.0746483,"Missing"
C14-1060,P09-2004,0,0.106921,"Missing"
C14-1060,prasad-etal-2008-penn,0,0.238062,"in an event in a summarization system (Derczynski and Gaizauskas, 2013). Sentiment transition of two clausal arguments is identified based on their discourse relation in sentiment analysis (Hutchinson, 2004; Zhou et al., 2011; Wang et al., 2012; Huang et al., 2013). The pioneer research of discourse has been established by Hobbs (1985), Polanyi (1988), Hovy and Maier (1992), and Asher and Lascarides (1995). Various discourse relation types have been defined in the frameworks such as Sanders et al. (1992), Hovy and Maier (1992), RST-DT (Carlson et al., 2002), Wolf and Gibson (2005), and PDTB (Prasad et al., 2008). Temporal, Contingency, Comparison, and Expansion, the four classes on the top level of PDTB sense hierarchy, are common used in the discourse relation labeling tasks. When two arguments are temporally related, they form a Temporal relation. The Contingency relation talks about the situation that the event in one argument casually affects the event in the other argument. Comparison is used to show the difference between two arguments. The last one relation, Expansion, is the most common. An Expansion relation either expands the information for one argument in the other one or continues the na"
C14-1060,C12-2128,0,0.0451216,"Missing"
C14-1060,J05-2005,0,0.225533,"Missing"
C14-1060,yu-etal-2012-development,1,0.576379,"595 0.6879 0.7334 0.7344 0.6600 0.6694 0.6731 0.7276 0.7299 0.6606 0.6644 0.6805 0.7305 0.7322 Table 3: Performance comparisons among models. (a) F-Score (b) Precision/Recall Figure 2: Effects of the number of test instances for each connective on relation labeling. 638 4 Further Analyses on a Big Dataset We further apply the best model (M4) to predict the probability distributions of discourse functions of connectives on a big dataset. For each discourse connective c, up to 500 sentences composed of two discourse units linked by c are randomly selected from the Chinese Web POS tagged corpus (Yu et al., 2012). The limitation of 500 is set to reduce the imbalance among the discourse connectives. Some connectives appear quite often in the dataset, e.g., the connective “也” (yě, also). Some connectives appear less than 500 times, e.g., “千萬…不然” (qiān wàn…bù rán, must...otherwise) occurs only 212 times. Finally, total 302,293 sentences are extracted and predicted. Because the dataset is very large, it is not easy to evaluate each pair of discourse units. We examine the linguistic phenomena instead. A lexicon of the probability distributions of connectives estimated by M4 is available at http://nlg.csie."
C14-1060,D11-1015,0,0.03034,"orithm Given a pair of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expansion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1…c2, where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. Each discourse unit is mapped into a representation. Various features from different linguistic levels have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particular on the probability distributions of the discourse functions and the positions of connectives. Length. This feature includes the word counts of ds1 and ds2. Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is dropped from the features because it is always a comma. Words. The bags of words in ds1 and ds2 are considered. Hypernym. The bags of hypernyms of"
C14-1060,C12-2138,0,0.338029,"r of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expansion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1…c2, where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. Each discourse unit is mapped into a representation. Various features from different linguistic levels have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particular on the probability distributions of the discourse functions and the positions of connectives. Length. This feature includes the word counts of ds1 and ds2. Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is dropped from the features because it is always a comma. Words. The bags of words in ds1 and ds2 are considered. Hypernym. The bags of hypernyms of the words in ds1 an"
C14-1060,P12-1008,0,0.178731,"Missing"
C14-1060,afantenos-etal-2012-empirical,0,\N,Missing
C14-1120,W10-2914,0,0.103012,"d irony recently, but it is still not clear whether sarcasm and irony differ significantly or represent the same concept. The research of non-literal expression identification has drawn attention in recent years. Katz and Giesbrecht (2006) use meaning vectors for literal and non-literal expression classification. Li and Sporleder (2010) focus on distinguishing literal and non-literal usages of idioms. Filatova (2012) uses crowdsourcing to generate an irony and sarcasm corpus. Veale and Hao (2010) construct a corpus of ironic similes using the wildcarded query “as * as a *” on a search engine. Davidov et al. (2010) collect messages from Twitter and product reviews from Amazon.com using the Mechanical Turk service. The #sarcasm hashtag is used as ground truth, and a k-nearest neighbor strategy is used for classification. González-Ibáñez et al. (2011) also make use of hashtags in Twitter as labels to build a sarcasm corpus. In their study, both human classification and automatic classification achieve low accuracy in sarcasm detection. Reyes et al. (2012) analyze humor and irony based on the user-generated tags, such as “#humor” and “#irony”, in twitter. Lukin and Walker (2013) use a bootstrapping method"
C14-1120,filatova-2012-irony,0,0.109869,"a and Fein, 1999; Gibbs and Colston, 2007) for years, but there has been no concrete claim on the linguistic structure of irony. Some studies have started focusing on the processing of sarcasm and irony recently, but it is still not clear whether sarcasm and irony differ significantly or represent the same concept. The research of non-literal expression identification has drawn attention in recent years. Katz and Giesbrecht (2006) use meaning vectors for literal and non-literal expression classification. Li and Sporleder (2010) focus on distinguishing literal and non-literal usages of idioms. Filatova (2012) uses crowdsourcing to generate an irony and sarcasm corpus. Veale and Hao (2010) construct a corpus of ironic similes using the wildcarded query “as * as a *” on a search engine. Davidov et al. (2010) collect messages from Twitter and product reviews from Amazon.com using the Mechanical Turk service. The #sarcasm hashtag is used as ground truth, and a k-nearest neighbor strategy is used for classification. González-Ibáñez et al. (2011) also make use of hashtags in Twitter as labels to build a sarcasm corpus. In their study, both human classification and automatic classification achieve low ac"
C14-1120,P11-2102,0,0.163182,"t (2006) use meaning vectors for literal and non-literal expression classification. Li and Sporleder (2010) focus on distinguishing literal and non-literal usages of idioms. Filatova (2012) uses crowdsourcing to generate an irony and sarcasm corpus. Veale and Hao (2010) construct a corpus of ironic similes using the wildcarded query “as * as a *” on a search engine. Davidov et al. (2010) collect messages from Twitter and product reviews from Amazon.com using the Mechanical Turk service. The #sarcasm hashtag is used as ground truth, and a k-nearest neighbor strategy is used for classification. González-Ibáñez et al. (2011) also make use of hashtags in Twitter as labels to build a sarcasm corpus. In their study, both human classification and automatic classification achieve low accuracy in sarcasm detection. Reyes et al. (2012) analyze humor and irony based on the user-generated tags, such as “#humor” and “#irony”, in twitter. Lukin and Walker (2013) use a bootstrapping method to improve the performance of the classifiers for identifying sarcastic and nasty utterances in online dialogues. The hashtag-based approaches are not always suitable for irony corpus construction for all the languages. As of March 9, 2014"
C14-1120,W13-2309,1,0.834491,"social function of irony is expressing negative meaning with positive words, as mentioned in Gibbs and Colston (2007), focus was directed on those messages with negative emoticons and positive words. A total of 3,178,372 messages was found containing at least one negative emoticon. Among them, 304,754 messages with at least one positive word are found and form an irony candidate dataset. Discourse relation determines how two discourse units cohere to each other. Sentiment transition of two clausal arguments is identified based on their discourse relation (Zhou et al., 2011; Wang et al., 2012; Huang et al., 2013). In the sentence “he is nice but not attractive,” positive opinion in the beginning is transformed to a negative one by the discourse connective “but.” Both the positive word “nice” and the negative phrase “not attractive” are used literally. Thus, it was necessary to filter out messages containing such connectives. Messages are removed only when the positive word occurs earlier than the discourse connectives with a comparison function, due to Chinese grammatical structure. The Chinese discourse connectives used here include “但”, “但是”, “可是”, “只是”, “不過” (all the above are equivalent to the Eng"
C14-1120,W06-1203,0,0.0120221,"mputational Linguistics: Technical Papers, pages 1269–1278, Dublin, Ireland, August 23-29 2014. 2 Related Work Sarcasm and irony have been studied by linguistics and cognitive scientists (Giora and Fein, 1999; Gibbs and Colston, 2007) for years, but there has been no concrete claim on the linguistic structure of irony. Some studies have started focusing on the processing of sarcasm and irony recently, but it is still not clear whether sarcasm and irony differ significantly or represent the same concept. The research of non-literal expression identification has drawn attention in recent years. Katz and Giesbrecht (2006) use meaning vectors for literal and non-literal expression classification. Li and Sporleder (2010) focus on distinguishing literal and non-literal usages of idioms. Filatova (2012) uses crowdsourcing to generate an irony and sarcasm corpus. Veale and Hao (2010) construct a corpus of ironic similes using the wildcarded query “as * as a *” on a search engine. Davidov et al. (2010) collect messages from Twitter and product reviews from Amazon.com using the Mechanical Turk service. The #sarcasm hashtag is used as ground truth, and a k-nearest neighbor strategy is used for classification. González"
C14-1120,C10-2078,0,0.0232103,"ted Work Sarcasm and irony have been studied by linguistics and cognitive scientists (Giora and Fein, 1999; Gibbs and Colston, 2007) for years, but there has been no concrete claim on the linguistic structure of irony. Some studies have started focusing on the processing of sarcasm and irony recently, but it is still not clear whether sarcasm and irony differ significantly or represent the same concept. The research of non-literal expression identification has drawn attention in recent years. Katz and Giesbrecht (2006) use meaning vectors for literal and non-literal expression classification. Li and Sporleder (2010) focus on distinguishing literal and non-literal usages of idioms. Filatova (2012) uses crowdsourcing to generate an irony and sarcasm corpus. Veale and Hao (2010) construct a corpus of ironic similes using the wildcarded query “as * as a *” on a search engine. Davidov et al. (2010) collect messages from Twitter and product reviews from Amazon.com using the Mechanical Turk service. The #sarcasm hashtag is used as ground truth, and a k-nearest neighbor strategy is used for classification. González-Ibáñez et al. (2011) also make use of hashtags in Twitter as labels to build a sarcasm corpus. In"
C14-1120,W13-1605,0,0.152108,"Missing"
C14-1120,W13-1104,0,0.0126398,"* as a *” on a search engine. Davidov et al. (2010) collect messages from Twitter and product reviews from Amazon.com using the Mechanical Turk service. The #sarcasm hashtag is used as ground truth, and a k-nearest neighbor strategy is used for classification. González-Ibáñez et al. (2011) also make use of hashtags in Twitter as labels to build a sarcasm corpus. In their study, both human classification and automatic classification achieve low accuracy in sarcasm detection. Reyes et al. (2012) analyze humor and irony based on the user-generated tags, such as “#humor” and “#irony”, in twitter. Lukin and Walker (2013) use a bootstrapping method to improve the performance of the classifiers for identifying sarcastic and nasty utterances in online dialogues. The hashtag-based approaches are not always suitable for irony corpus construction for all the languages. As of March 9, 2014, only 113 messages are found to contain the hashtag #反諷 (#irony) in Weibo, the largest Chinese language microblogging platform. This paper differs from the previous work in that we employ negative emoticons and positive words as clues to capture the irony. The linguistic patterns mined from the irony corpus can be used to detect i"
C14-1120,C12-2128,0,0.0196212,"Since the typical social function of irony is expressing negative meaning with positive words, as mentioned in Gibbs and Colston (2007), focus was directed on those messages with negative emoticons and positive words. A total of 3,178,372 messages was found containing at least one negative emoticon. Among them, 304,754 messages with at least one positive word are found and form an irony candidate dataset. Discourse relation determines how two discourse units cohere to each other. Sentiment transition of two clausal arguments is identified based on their discourse relation (Zhou et al., 2011; Wang et al., 2012; Huang et al., 2013). In the sentence “he is nice but not attractive,” positive opinion in the beginning is transformed to a negative one by the discourse connective “but.” Both the positive word “nice” and the negative phrase “not attractive” are used literally. Thus, it was necessary to filter out messages containing such connectives. Messages are removed only when the positive word occurs earlier than the discourse connectives with a comparison function, due to Chinese grammatical structure. The Chinese discourse connectives used here include “但”, “但是”, “可是”, “只是”, “不過” (all the above are"
C14-1120,D11-1015,0,0.0199135,"pus by using NTUSD. Since the typical social function of irony is expressing negative meaning with positive words, as mentioned in Gibbs and Colston (2007), focus was directed on those messages with negative emoticons and positive words. A total of 3,178,372 messages was found containing at least one negative emoticon. Among them, 304,754 messages with at least one positive word are found and form an irony candidate dataset. Discourse relation determines how two discourse units cohere to each other. Sentiment transition of two clausal arguments is identified based on their discourse relation (Zhou et al., 2011; Wang et al., 2012; Huang et al., 2013). In the sentence “he is nice but not attractive,” positive opinion in the beginning is transformed to a negative one by the discourse connective “but.” Both the positive word “nice” and the negative phrase “not attractive” are used literally. Thus, it was necessary to filter out messages containing such connectives. Messages are removed only when the positive word occurs earlier than the discourse connectives with a comparison function, due to Chinese grammatical structure. The Chinese discourse connectives used here include “但”, “但是”, “可是”, “只是”, “不過”"
C14-2015,C12-1184,1,0.41187,"ign Language (EFL), support for CFL learners is relatively sparse, especially in terms of tools designed to automatically detect and correct Chinese grammatical errors. For example, while Microsoft Word has integrated robust English spelling and grammar checking functions for years, such tools for Chinese are still quite primitive. In contrast to the plethora of research related to EFL learning, relatively few studies have focused on grammar checking for CFL learners. Wu et al. (2010) proposed relative position and parse template language models to detect Chinese errors written by US learner. Yu and Chen (2012) proposed a classifier to detect word-ordering errors in Chinese sentences from the HSK dynamic composition corpus. Chang et al. (2012) proposed a penalized probabilistic First-Order Inductive Learning (pFOIL) algorithm for error diagnosis. In summary, although there are many approaches and tools to help EFL learners, the research problem described above for CFL learning is still under-explored. In addition, no common platform is available to compare different approaches and to promote the study of this important issue. This study develops a sentence judgment system using both rule-based and n"
C14-2015,C02-1049,0,0.0362478,"le-baased method, and n-gram statistical method. m 2.1 Prre-processin ng Chinese is written without w word boundaries.. As a result,, prior to thee implementaation of mosst Natural Languagge Processingg (NLP) task ks, texts musst undergo au utomatic wo ord segmentaation. Autom matic Chinese word segmenteers are generrally trained by an input lexicon and d probability models. Ho owever, it usually ssuffers from the unknown word (i.e., the out-of-v vocabulary, or o OOV) prooblem. In thiss study, a corpus-bbased learninng method is used to merg rge unknown n words to tacckle the OOV V problem (C Chen and Ma, 2002). This is foollowed by a reliable andd cost-effectiive POS-tagg ging method to label the segmented words with parts--of-speech (T Tsai and Cheen, 2004). For example, take the Chhinese senten nce “歐巴 馬是美國 國總統” (Obbama is the president p off the USA). It was segm mented and taagged in thee form of “POS:W Word” sequennce shown as follows: Nb:歐巴馬 馬 SHI:是 Nc:美國 N Naa:總統. Amo ong these words, tthe translatioon of a foreign proper naame “歐巴馬 馬” (Obama) is not likelyy to be inclu uded in a lexicon aand thereforee is extracted d by the unkknown word detection meechanism. Inn this case, th he special PO"
C14-2015,O04-2005,0,0.0729102,"Missing"
C16-1085,J96-1002,0,0.0290129,"the performance gaps among the three models are more apparent. The information of Chinese word segmentation and POS tags not only speeds up training, but also improves the generalization. (a) L1 Testing (b) L2 Testing Figure 5: Accuracy versus order of n-gram in L1 and L2 testing. 895 Previous work suggests that the ensemble of RNN and traditional language model may improve the performance, especially for the RNN models with small hidden layer (Mikolov, 2012; Mikolov et al., 2011). We build an ensemble model GRU-ME by joint training the GRU model with a 4-gram maximum entropy language model (Berger et al., 1996). The input unit is word with POS tag (w/p), which has best performance in the experiments. The feature size of maximum entropy model is 1 billion. Figure 6 compares the performances between GRU and GRU-ME in L1 and L2. In L1 testing, ensembling the maximum entropy model with GRU significantly increases the performances of three GRU models, especially the ones with smaller hidden layer. In L2 testing, ensembling the maximum entropy model increases the performances of the GRU model with hidden sizes of 128. The results confirm that the RNN models with smaller hidden size gain from the combinati"
C16-1085,P15-1068,0,0.0461291,"Missing"
C16-1085,N13-1055,0,0.0200958,"ted perceptron classifier for disambiguating the uses of five common prepositions including “in”, “of”, “on”, “to”, and “with”. In the work of Felice and Pulman (2008), error detection of nine common prepositions is tackled with the maximum entropy classifier. Chodorow et al. (2007) deal with the detection of preposition errors of non-native learners. In addition to error detection, some studies address the task of English preposition selection. Bergsma et al. (2009) propose a supervised language model for preposition correction. Tetreault et al. (2010) introduce parse features for this task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) introduce a framework for English grammatical error correction using the maximum entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), H"
C16-1085,P15-1168,0,0.0177291,"lity of a preposition in a given sentence. Instead of the single sigmoid or tanh function used by the simple RNN, GRU model, which is simplified from LSTM, uses a structure namely gated recurrent unit in the hidden layer. ?? = (1 − ?)ℎ + ???−1 ℎ = tanh(?ℎ ?? + ?? (?ℎ ??−1 )) ? = σ(?? ?? + ?? ??−1 ) ? = σ(?? ?? + ?? ??−1 ) As shown in Figure 3, the update gate z decides how much the hidden state s is updated with the candidate hidden state ? , while the reset gate r decides how much the memory to be forgotten. GRU is reported to be better for long-term dependency modeling (Chung et al., 2014; Chen et al., 2015b) than the simple RNN. Compared to LSTM, GRU requires few parameters for the same size of hidden layer. Figure 3: Gated recurrent unit (GRU). In this work, we use the noise contrastive estimation (NCE) (Chen et al., 2015a) as the output layer for both simple RNN and GRU language models. The training performance of NCE is comparable to the class-based softmax, but NCE is faster in testing stage and can speed up together with GPU in training stage. The implementation of RNN models is based on faster-rnnlm, a toolkit for RNN language modelling5. 6 Experiments Three language models, n-gram, simpl"
C16-1085,C14-1028,1,0.858158,"l for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error detection in Chinese has been investigated recently. Wang (2011) shows common Chinese grammatical errors like missing components and error word orderings. Lin (2011), Yu and Chen (2012), and Cheng et al. (2014) focus on the detection and correction of word ordering errors in Chinese written by foreign students in the HSK corpus. Shiue and Chen (2016) determine if a Chinese sentence contains word usage errors. In the NLP-TEA shared tasks (Yu et al., 2014; Lee et al., 2015a), detection of four grammatical errors are targeted. Lin and Chan (2014) train SVM classifiers with various bigram features. Zampiperi and Tan (2014) propose a frequency-based approach based on a large general corpus. Zhao et al. (2014; 2015) model the task of correction as machine translation in such a way that the wrong sentences"
C16-1085,W07-1604,0,0.110389,"d in this study. Section 5 presents our approach to Chinese preposition selection. Section 6 shows the experimental results. We further analyze the results in Section 7. Finally, Section 8 concludes this work. 2 Related Work English preposition error detection has attracted much attention for years. Felice and Pulman (2007) proposed a voted perceptron classifier for disambiguating the uses of five common prepositions including “in”, “of”, “on”, “to”, and “with”. In the work of Felice and Pulman (2008), error detection of nine common prepositions is tackled with the maximum entropy classifier. Chodorow et al. (2007) deal with the detection of preposition errors of non-native learners. In addition to error detection, some studies address the task of English preposition selection. Bergsma et al. (2009) propose a supervised language model for preposition correction. Tetreault et al. (2010) introduce parse features for this task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) in"
C16-1085,W11-2838,0,0.0865719,"is task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) introduce a framework for English grammatical error correction using the maximum entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error detection in Chinese has been investigated recently. Wang (2011) shows common Chinese grammatical errors like missing components and error word orderings. Lin (2011), Yu and Chen (2012), and Cheng et al. (2014) focus on the detection and correction of word ordering errors in Chinese written by foreign students in the HSK corpus. Shiue and Chen (2016) determine if a Chinese sentence contains word usage errors. In the NLP-T"
C16-1085,W07-1607,0,0.0921105,"Missing"
C16-1085,C08-1022,0,0.195871,"Missing"
C16-1085,huang-etal-2008-quality,0,0.0781653,"Missing"
C16-1085,D15-1022,0,0.031023,"sk of English preposition selection. Bergsma et al. (2009) propose a supervised language model for preposition correction. Tetreault et al. (2010) introduce parse features for this task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) introduce a framework for English grammatical error correction using the maximum entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error detection in Chinese has been investigated recently. Wang (2011) shows common Chinese grammatical errors like missing components and error word orderings. Lin (2011), Yu and Chen (2012), and Cheng et al. (2014) focus on the detection and cor"
C16-1085,L16-1033,1,0.590434,"s. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error detection in Chinese has been investigated recently. Wang (2011) shows common Chinese grammatical errors like missing components and error word orderings. Lin (2011), Yu and Chen (2012), and Cheng et al. (2014) focus on the detection and correction of word ordering errors in Chinese written by foreign students in the HSK corpus. Shiue and Chen (2016) determine if a Chinese sentence contains word usage errors. In the NLP-TEA shared tasks (Yu et al., 2014; Lee et al., 2015a), detection of four grammatical errors are targeted. Lin and Chan (2014) train SVM classifiers with various bigram features. Zampiperi and Tan (2014) propose a frequency-based approach based on a large general corpus. Zhao et al. (2014; 2015) model the task of correction as machine translation in such a way that the wrong sentences are translated to correct ones. The preposition error detection is one of error cases in NLP-TEA shared tasks. However, the preposition corre"
C16-1085,P10-2065,0,0.0243199,"much attention for years. Felice and Pulman (2007) proposed a voted perceptron classifier for disambiguating the uses of five common prepositions including “in”, “of”, “on”, “to”, and “with”. In the work of Felice and Pulman (2008), error detection of nine common prepositions is tackled with the maximum entropy classifier. Chodorow et al. (2007) deal with the detection of preposition errors of non-native learners. In addition to error detection, some studies address the task of English preposition selection. Bergsma et al. (2009) propose a supervised language model for preposition correction. Tetreault et al. (2010) introduce parse features for this task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) introduce a framework for English grammatical error correction using the maximum entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered i"
C16-1085,W13-3616,0,0.0207347,"tection of nine common prepositions is tackled with the maximum entropy classifier. Chodorow et al. (2007) deal with the detection of preposition errors of non-native learners. In addition to error detection, some studies address the task of English preposition selection. Bergsma et al. (2009) propose a supervised language model for preposition correction. Tetreault et al. (2010) introduce parse features for this task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) introduce a framework for English grammatical error correction using the maximum entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error dete"
C16-1085,C12-1184,1,0.868922,"um entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error detection in Chinese has been investigated recently. Wang (2011) shows common Chinese grammatical errors like missing components and error word orderings. Lin (2011), Yu and Chen (2012), and Cheng et al. (2014) focus on the detection and correction of word ordering errors in Chinese written by foreign students in the HSK corpus. Shiue and Chen (2016) determine if a Chinese sentence contains word usage errors. In the NLP-TEA shared tasks (Yu et al., 2014; Lee et al., 2015a), detection of four grammatical errors are targeted. Lin and Chan (2014) train SVM classifiers with various bigram features. Zampiperi and Tan (2014) propose a frequency-based approach based on a large general corpus. Zhao et al. (2014; 2015) model the task of correction as machine translation in such a way"
C16-1085,W14-1713,0,0.0278781,". Chodorow et al. (2007) deal with the detection of preposition errors of non-native learners. In addition to error detection, some studies address the task of English preposition selection. Bergsma et al. (2009) propose a supervised language model for preposition correction. Tetreault et al. (2010) introduce parse features for this task. Cahill et al. (2013) propose a preposition error correction model trained on error-annotated data, and treated the revision logs of Wikipedia as a large error-annotated corpus. Xiang et al. (2013) propose a hybrid approach to deal with preposition selection. Zhang and Wang (2014) introduce a framework for English grammatical error correction using the maximum entropy language model for the replacement errors. Ramisa et al. (2015) address the task of preposition prediction for image descriptions with multimodal features. Related evaluations are covered in the shared tasks of HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). In addition to Chinese spelling checking (Lee et al., 2015b), grammatical error detection in Chinese has been investigated recently. Wang (2011) shows common Chinese gr"
C16-1085,W15-4417,0,0.0306622,"Missing"
C16-1178,W00-0402,0,0.114041,"discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al., 2014). Due to limited resource, early studies often used self-constructed corpora that made it difficult to compare between different works. T&apos;sou et al. (1999), T&apos;sou et al. (2000) and Chan et al. (2000) investigated connective detection in Chinese texts as a part of a tagging system. Hu et al. (2009) developed an automatic system to extract connective components from sentences. They used a rule-based method and found that the performance was sensitive to the connective lexicon. They improved their performance by removing words commonly used in non-discourse contexts. Zhou et al. (2012) and Li, Carpuat et al. (2014) employed cross-lingual information to deal with discourse usage ambiguity. Li, Carpuat et al. (2014) used 5-way classification to classify a connective between four relation types"
C16-1178,W08-0336,0,0.0120619,"lined system. Each paragraph in CDTB is processed by the following modules: (1) identify connective candidates, (2) eliminate non-discourse usages from connective candidates, (3) resolve linking ambiguities, (4) disambiguate relation types, and (5) extract arguments. 4.1 Connective Candidate Extraction We use string matching with the connective lexicon collected from CDTB to extract all possible instances. Directly matching with raw text would yield 12,498 candidate components2 because many characters used for connectives appear in other unrelated words. Therefore, Stanford Chinese segmenter (Chang et al., 2008) is employed to segment paragraphs into tokens. Only the components composed of complete tokens are extracted. Total 7,649 component candidates which recover 2,068 of 2,131 annotated components are extracted. These candidates form 7,976 connective candidates which recover 1,755 of 1,813 annotated connectives. While some correct instances are not extracted due to segmentation errors, it reduces the number of spurious candidates substantially while maintaining high recall. 4.2 Discourse Usage Disambiguation A logistic regression classifier is trained to eliminate spurious connective candidates."
C16-1178,L16-1164,1,0.84217,"d found that the performance was sensitive to the connective lexicon. They improved their performance by removing words commonly used in non-discourse contexts. Zhou et al. (2012) and Li, Carpuat et al. (2014) employed cross-lingual information to deal with discourse usage ambiguity. Li, Carpuat et al. (2014) used 5-way classification to classify a connective between four relation types and non-discourse usage. Li et al. (2015) used CDTB to investigate detection and classification of connective components. They used maximum entropy and decision tree algorithms with various syntactic features. Chen et al. (2016) investigated fine-grained Chinese discourse relation labelling. Hu et al. (2011) dealt with linking ambiguity. However, they only focused on intra-sentential relations in sentences that have multiple clauses and assumed all connective components in the sentence have already been correctly identified. As researchers start to focus on higher level problems for linguistic analysis, interest in discourse parsing also grows. The CoNLL-2015 Shared Task (Xue et al., 2015) and the CoNLL-2016 Shared Task (Xue et al., 2016) both focus on shallow discourse parsing. In particular, the CoNLL-2016 Shared T"
C16-1178,P12-1007,0,0.0716625,"scourse connective identification, relation type disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al., 2014). Due to limited resource, early studies often used self-constructed corpora that made it difficult to compare between different works. T&apos;sou et al. (1999), T&apos;sou et al. (2000) and Chan et al. (2000) investigated connective detection in Chinese texts as a part of a tagging system. Hu et al. (2009) developed an automatic system to extract connective components"
C16-1178,P07-1062,0,0.0304766,"vestigated different subtasks of English discourse parsing on PDTB2, including discourse connective identification, relation type disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al., 2014). Due to limited resource, early studies often used self-constructed corpora that made it difficult to compare between different works. T&apos;sou et al. (1999), T&apos;sou et al. (2000) and Chan et al. (2000) investigated connective detection in Chinese texts as a part of a tagging system."
C16-1178,I11-1120,0,0.0183878,"the experimental results. Section 6 concludes the remarks. 2 Related Work Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001) and the Penn Discourse Treebank 2.0 (PDTB2) (Prasad et al., 2008) are two popular English discourse corpora for discourse analysis. Many groups have investigated different subtasks of English discourse parsing on PDTB2, including discourse connective identification, relation type disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2"
C16-1178,W12-1622,0,0.0167319,"sults. Section 6 concludes the remarks. 2 Related Work Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001) and the Penn Discourse Treebank 2.0 (PDTB2) (Prasad et al., 2008) are two popular English discourse corpora for discourse analysis. Many groups have investigated different subtasks of English discourse parsing on PDTB2, including discourse connective identification, relation type disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al."
C16-1178,C14-1060,1,0.925468,"discourse usage disambiguation, linking disambiguation, relation type disambiguation, and argument boundary identification, respectively, in a pipelined Chinese discourse parser. 1 Introduction Discourse relations represent how discourse units logically connect with each other. A discourse connective explicitly signals the presence of a discourse relation, and therefore it is an important clue for discourse analysis. There are several challenges in Chinese discourse parsing. Firstly, there are more discourse connectives in Chinese than in English and their parts of speech have more varieties (Huang et al., 2014). Therefore, it is likely to encounter words that have the same surface forms as real connectives but do not function as discourse connectives. Secondly, many Chinese connectives are parallel connectives that have multiple discontinuous components (Zhou and Xue, 2012). Each connective can be composed of one or more connective components. For example, &apos;&apos;雖然-但&apos;&apos; (although-but) consists of two connective components: &apos;&apos;雖然&apos;&apos; (although) and &apos;&apos;但&apos;&apos; (but). When multiple connectives are present in a paragraph, their components often link with each other in multiple possible ways. (S1) is an example. Ther"
C16-1178,P14-1002,0,0.024761,"n (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al., 2014). Due to limited resource, early studies often used self-constructed corpora that made it difficult to compare between different works. T&apos;sou et al. (1999), T&apos;sou et al. (2000) and Chan et al. (2000) investigated connective detection in Chinese texts as a part of a tagging system. Hu et al. (2009) developed an automatic system to extract connective components from sentences. They used a rule-based method and found that the"
C16-1178,D12-1083,0,0.0145499,"tasks of English discourse parsing on PDTB2, including discourse connective identification, relation type disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al., 2014). Due to limited resource, early studies often used self-constructed corpora that made it difficult to compare between different works. T&apos;sou et al. (1999), T&apos;sou et al. (2000) and Chan et al. (2000) investigated connective detection in Chinese texts as a part of a tagging system. Hu et al. (2009) dev"
C16-1178,P13-1048,0,0.0417167,"ntification, relation type disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al., 2014). Due to limited resource, early studies often used self-constructed corpora that made it difficult to compare between different works. T&apos;sou et al. (1999), T&apos;sou et al. (2000) and Chan et al. (2000) investigated connective detection in Chinese texts as a part of a tagging system. Hu et al. (2009) developed an automatic system to extract connective components from sentences. Th"
C16-1178,D14-1008,0,0.0837524,"cludes the remarks. 2 Related Work Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001) and the Penn Discourse Treebank 2.0 (PDTB2) (Prasad et al., 2008) are two popular English discourse corpora for discourse analysis. Many groups have investigated different subtasks of English discourse parsing on PDTB2, including discourse connective identification, relation type disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al., 2014). Due to limi"
C16-1178,D14-1220,0,0.014056,"ype disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al., 2014). Due to limited resource, early studies often used self-constructed corpora that made it difficult to compare between different works. T&apos;sou et al. (1999), T&apos;sou et al. (2000) and Chan et al. (2000) investigated connective detection in Chinese texts as a part of a tagging system. Hu et al. (2009) developed an automatic system to extract connective components from sentences. They used a rule-based"
C16-1178,C14-1055,0,0.0189828,"ype disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al., 2014). Due to limited resource, early studies often used self-constructed corpora that made it difficult to compare between different works. T&apos;sou et al. (1999), T&apos;sou et al. (2000) and Chan et al. (2000) investigated connective detection in Chinese texts as a part of a tagging system. Hu et al. (2009) developed an automatic system to extract connective components from sentences. They used a rule-based"
C16-1178,D14-1224,0,0.0215713,"ype disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al., 2014). Due to limited resource, early studies often used self-constructed corpora that made it difficult to compare between different works. T&apos;sou et al. (1999), T&apos;sou et al. (2000) and Chan et al. (2000) investigated connective detection in Chinese texts as a part of a tagging system. Hu et al. (2009) developed an automatic system to extract connective components from sentences. They used a rule-based"
C16-1178,D14-1162,0,0.0825944,"rage, each argument is composed of 1.6 EDUs, and the longest argument is composed of 20 EDUs. 3.2 NTU PN-Gram Corpus Recently, efficient methods to learn word embeddings have been developed. In this paper, we investigate whether such word vectors are useful for dealing with discourse issues. NTU PN-Gram Corpus released by Yu et al., (2012), which was constructed by POS-tagging the Chinese texts extracted from the ClueWeb09 dataset (Callan et al., 2009). It has 21,217,147 unique sentences, containing 326,996,602 tokens. We used this corpus to create 400-dimensional embeddings using GloVe tool (Pennington et al., 2014) and word2vec tool (Mikolov et al., 2013a; Mikolov et al., 2013b) with skipgram and continuous bag-of-words models. 3.3 Connective Component Dictionary Discourse connectives serve as linking elements that connect discourse units. There are three kinds of linking directions (Li and Thompson, 1989): (1) forward-linking, (2) backward-linking, and (3) couple-linking. Such linking directions could be useful for identifying the positions of arguments for a given connective. We used the connective linking dictionary collected by Huang et al. (2014) as features for argument boundary identification. To"
C16-1178,P09-2004,0,0.135196,"the related work. Section 3 describes the datasets used in this study. Section 4 presents our Chinese discourse parser. Section 5 shows and discusses the experimental results. Section 6 concludes the remarks. 2 Related Work Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001) and the Penn Discourse Treebank 2.0 (PDTB2) (Prasad et al., 2008) are two popular English discourse corpora for discourse analysis. Many groups have investigated different subtasks of English discourse parsing on PDTB2, including discourse connective identification, relation type disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). C"
C16-1178,prasad-etal-2008-penn,0,0.0600036,"tives, (2) linking resolution between the component candidates, (3) classification of the relation type for each discourse connective, and (4) extraction of the discourse arguments of a connective. This paper is organized as follows. Section 2 surveys the related work. Section 3 describes the datasets used in this study. Section 4 presents our Chinese discourse parser. Section 5 shows and discusses the experimental results. Section 6 concludes the remarks. 2 Related Work Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001) and the Penn Discourse Treebank 2.0 (PDTB2) (Prasad et al., 2008) are two popular English discourse corpora for discourse analysis. Many groups have investigated different subtasks of English discourse parsing on PDTB2, including discourse connective identification, relation type disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to constr"
C16-1178,H05-1033,0,0.0311719,"nalysis. Many groups have investigated different subtasks of English discourse parsing on PDTB2, including discourse connective identification, relation type disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al., 2014). Due to limited resource, early studies often used self-constructed corpora that made it difficult to compare between different works. T&apos;sou et al. (1999), T&apos;sou et al. (2000) and Chan et al. (2000) investigated connective detection in Chinese texts as a pa"
C16-1178,W00-1206,0,0.0536218,"ttempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou and Xue, 2012; Zhou and Xue, 2015; Zhang et al., 2014; Li, Feng et al., 2014). Due to limited resource, early studies often used self-constructed corpora that made it difficult to compare between different works. T&apos;sou et al. (1999), T&apos;sou et al. (2000) and Chan et al. (2000) investigated connective detection in Chinese texts as a part of a tagging system. Hu et al. (2009) developed an automatic system to extract connective components from sentences. They used a rule-based method and found that the performance was sensitive to the connective lexicon. They improved their performance by removing words commonly used in non-discourse contexts. Zhou et al. (2012) and Li, Carpuat et al. (2014) employed cross-lingual information to deal with discourse usage ambiguity. Li, Carpuat et al. (2014) used 5-way classification to classify a connective betw"
C16-1178,D07-1010,0,0.0395805,"our Chinese discourse parser. Section 5 shows and discusses the experimental results. Section 6 concludes the remarks. 2 Related Work Rhetorical Structure Theory Discourse Treebank (RST-DT) (Carlson et al., 2001) and the Penn Discourse Treebank 2.0 (PDTB2) (Prasad et al., 2008) are two popular English discourse corpora for discourse analysis. Many groups have investigated different subtasks of English discourse parsing on PDTB2, including discourse connective identification, relation type disambiguation (Pitler and Nenkova, 2009; Wellner, 2009; Faiz and Mercer, 2013), and argument extraction (Wellner and Pustejovsky, 2007; Elwell and Baldridge, 2008; Ghosh et al., 2011; Ghosh et al., 2012; Kong et al., 2014). Lin et al. (2014) build an end-to-end discourse parser. As RST-DT provides hierarchical discourse structure annotations, there are also many attempts to construct the discourse structures automatically for sentences (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Joty et al., 2012) and documents (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li, Li et al., 2014; Ji and Eisenstein, 2014). Comparatively, there have been few large-scale Chinese discourse corpora until recently (Zhou an"
C16-1178,yu-etal-2012-development,1,0.81846,"e tree, the length of an argument could vary greatly, but most of the arguments for 1 These numbers are computed by their surface forms, i.e., instances of the same connective class may have different relation types. 1893 explicit relation are composed of only one EDU. On average, each argument is composed of 1.6 EDUs, and the longest argument is composed of 20 EDUs. 3.2 NTU PN-Gram Corpus Recently, efficient methods to learn word embeddings have been developed. In this paper, we investigate whether such word vectors are useful for dealing with discourse issues. NTU PN-Gram Corpus released by Yu et al., (2012), which was constructed by POS-tagging the Chinese texts extracted from the ClueWeb09 dataset (Callan et al., 2009). It has 21,217,147 unique sentences, containing 326,996,602 tokens. We used this corpus to create 400-dimensional embeddings using GloVe tool (Pennington et al., 2014) and word2vec tool (Mikolov et al., 2013a; Mikolov et al., 2013b) with skipgram and continuous bag-of-words models. 3.3 Connective Component Dictionary Discourse connectives serve as linking elements that connect discourse units. There are three kinds of linking directions (Li and Thompson, 1989): (1) forward-linkin"
C16-1178,P12-1008,0,0.1466,"ach other. A discourse connective explicitly signals the presence of a discourse relation, and therefore it is an important clue for discourse analysis. There are several challenges in Chinese discourse parsing. Firstly, there are more discourse connectives in Chinese than in English and their parts of speech have more varieties (Huang et al., 2014). Therefore, it is likely to encounter words that have the same surface forms as real connectives but do not function as discourse connectives. Secondly, many Chinese connectives are parallel connectives that have multiple discontinuous components (Zhou and Xue, 2012). Each connective can be composed of one or more connective components. For example, &apos;&apos;雖然-但&apos;&apos; (although-but) consists of two connective components: &apos;&apos;雖然&apos;&apos; (although) and &apos;&apos;但&apos;&apos; (but). When multiple connectives are present in a paragraph, their components often link with each other in multiple possible ways. (S1) is an example. There are five possible connective candidates, including (1) &apos;&apos;除了...還&apos;&apos; (in addition to ... also), (2) &apos;&apos;還...也&apos;&apos; (also ... also), (3) &apos;&apos;除 了&apos;&apos; (in addition to), (4) &apos;&apos;還&apos;&apos; (also), and (5) &apos;&apos;也&apos;&apos; (also). Figure 1 illustrates the ambiguous linking. Only candidates (1) and (5)"
C16-1178,C12-2138,0,0.0299702,"2014; Li, Feng et al., 2014). Due to limited resource, early studies often used self-constructed corpora that made it difficult to compare between different works. T&apos;sou et al. (1999), T&apos;sou et al. (2000) and Chan et al. (2000) investigated connective detection in Chinese texts as a part of a tagging system. Hu et al. (2009) developed an automatic system to extract connective components from sentences. They used a rule-based method and found that the performance was sensitive to the connective lexicon. They improved their performance by removing words commonly used in non-discourse contexts. Zhou et al. (2012) and Li, Carpuat et al. (2014) employed cross-lingual information to deal with discourse usage ambiguity. Li, Carpuat et al. (2014) used 5-way classification to classify a connective between four relation types and non-discourse usage. Li et al. (2015) used CDTB to investigate detection and classification of connective components. They used maximum entropy and decision tree algorithms with various syntactic features. Chen et al. (2016) investigated fine-grained Chinese discourse relation labelling. Hu et al. (2011) dealt with linking ambiguity. However, they only focused on intra-sentential re"
C16-1210,P15-2110,0,0.0172882,"th International Conference on Computational Linguistics: Technical Papers, pages 2227–2237, Osaka, Japan, December 11-17 2016. lation from CDTB, and manually label the tense for each argument as ground-truth for the two tasks. To automatically extract the tense features, a Chinese tense predictor is required. The grammatical tense in English explicitly denotes the temporal information for a given text. In Chinese, however, the temporal information is communicated with aspect particles such as 了 (le) and 着 (zhe) and temporal adverbials such as 現在 (“now”) and 明天 (“tomorrow”) (Xue et al., 2008; Ge et al., 2015). In other words, it is more challenging to determine the tense in Chinese text. Thus, we propose a semisupervised algorithm that learns to label tense information in Chinese text. With UM-Corpus, a large English-Chinese parallel corpus aligned at sentence-level (Tian et al., 2014), we generate a pseudolabelled Chinese tense corpus by deriving the tense information from their English counterpart. Dependency-based convolutional neural network (DCNN) is trained to predict Chinese tense. We incorporate the semi-supervised Chinese tense predictor in the tasks of causal type classification and caus"
C16-1210,P14-1093,0,0.0146087,"tention in AI community for years. A variety of issues have been explored. One of the hottest topic is event analysis, where causal information plays a crucial role (Do et al., 2011; Riaz and Girju, 2013; 2014; Mirza and Tonelli, 2014; Kives et al., 2015). Other applications include generation of event causality hypotheses (Hashimoto et al., 2015), motivation identification (Nguyen et al., 2015), causality detection and extraction (Hashimoto et al., 2012; Mihaila and Ananiadou, 2013), causal inference (Tanaka et al., 2012), question answering (Oh et al., 2013), and future scenario generation (Hashimoto et al., 2014). The correlation between temporality and causality is studied by Mirza (2014) and Mirza and Tonelli (2014). Unlike English, no grammatical tense is available in Chinese. Various approaches are explored to address the topic of Chinese tense prediction. Liu et al. (2011) propose an unsupervised method for Chinese tense labelling by learning from a Chinese-English parallel corpus. Zhang and Xue (2014) deal with Chinese tense inference by training a supervised model with various linguistic features on a Chinese tense corpus (Xue and Zhang, 2014). Following the unsupervised method by Liu et al. (2"
C16-1210,D12-1057,0,0.0735927,"Missing"
C16-1210,D14-1181,0,0.00996357,"l. (2011) propose an unsupervised method for Chinese tense labelling by learning from a Chinese-English parallel corpus. Zhang and Xue (2014) deal with Chinese tense inference by training a supervised model with various linguistic features on a Chinese tense corpus (Xue and Zhang, 2014). Following the unsupervised method by Liu et al. (2011), we develop a semi-supervised model that benefits from a large amount of data labelled by an accurate English tense predictor. Neural networks such as recurrent neural network (RNN) and convolutional neural network (CNN) are very popular in NLP community. Kim (2014) releases a sentence classifier with convolutional neural network (CNN), where a sentence is represented as a sequence of word vectors (Mikolov et al., 2013). Based on Kim’s work, Ma et al. (2015) propose the dependency-based CNN (DCNN) by adding the structure information features to the sentence representation. In this work, we employ DCNN for Chinese tense classification under supervised, unsupervised, and semi-supervised learning. 3 Linguistic Resources Three types of corpora are used in this work. Section 3.1 describes the corpus for Chinese causal analysis. Section 3.2 and Section 3.3 int"
C16-1210,D14-1224,0,0.165688,"ish, the topic of causal analysis in Chinese is rarely touched. In this work, we explore the role of tense information in Chinese causal analysis. As pointed by Mirza (2014), the causal relation and temporal information is correlated. In a causal relation, the cause intuitively precedes its effect. In other words, the tense information could be useful features in the tasks of causal analysis. Two tasks of causal analysis are investigated in this study: causal type classification and causal directionality identification. The Chinese discourse relation corpus, Chinese Discourse Treebank (CDTB) (Li et al., 2014), is adopted as our dataset. In CDTB, six types of causality relations, Purpose, Background, Hypothetical, Inference, Condition, and Cause-Result, are defined. A discourse relation connects two arguments. In the case of causality, one of the two arguments (e.g., arg1) presents a situation, and it is causally affected by the other argument (e.g., arg2). For example, the first part of the sentence (S1) shows a reason, and the second part, which is underlined, is its effect. (S1) 由於產能不足，國內自給率不到四成，大部分要仰賴進口。 (Because of insufficient capacity, the domestic self-sufficiency rate is less than 40, most"
C16-1210,I11-1125,0,0.0171324,"include generation of event causality hypotheses (Hashimoto et al., 2015), motivation identification (Nguyen et al., 2015), causality detection and extraction (Hashimoto et al., 2012; Mihaila and Ananiadou, 2013), causal inference (Tanaka et al., 2012), question answering (Oh et al., 2013), and future scenario generation (Hashimoto et al., 2014). The correlation between temporality and causality is studied by Mirza (2014) and Mirza and Tonelli (2014). Unlike English, no grammatical tense is available in Chinese. Various approaches are explored to address the topic of Chinese tense prediction. Liu et al. (2011) propose an unsupervised method for Chinese tense labelling by learning from a Chinese-English parallel corpus. Zhang and Xue (2014) deal with Chinese tense inference by training a supervised model with various linguistic features on a Chinese tense corpus (Xue and Zhang, 2014). Following the unsupervised method by Liu et al. (2011), we develop a semi-supervised model that benefits from a large amount of data labelled by an accurate English tense predictor. Neural networks such as recurrent neural network (RNN) and convolutional neural network (CNN) are very popular in NLP community. Kim (2014"
C16-1210,P15-2029,0,0.0150605,"supervised model with various linguistic features on a Chinese tense corpus (Xue and Zhang, 2014). Following the unsupervised method by Liu et al. (2011), we develop a semi-supervised model that benefits from a large amount of data labelled by an accurate English tense predictor. Neural networks such as recurrent neural network (RNN) and convolutional neural network (CNN) are very popular in NLP community. Kim (2014) releases a sentence classifier with convolutional neural network (CNN), where a sentence is represented as a sequence of word vectors (Mikolov et al., 2013). Based on Kim’s work, Ma et al. (2015) propose the dependency-based CNN (DCNN) by adding the structure information features to the sentence representation. In this work, we employ DCNN for Chinese tense classification under supervised, unsupervised, and semi-supervised learning. 3 Linguistic Resources Three types of corpora are used in this work. Section 3.1 describes the corpus for Chinese causal analysis. Section 3.2 and Section 3.3 introduce the corpora for developing our Chinese tense predictor. 3.1 Chinese Causality Corpus There are few resources for Chinese causal analysis. In this work, we extract instances labelled with ca"
C16-1210,P14-5010,0,0.00503366,"Missing"
C16-1210,P13-3006,0,0.0291371,"ion in causal type and causal directionality identification. Section 6 concludes this paper. 2 Related Work Causal analysis attracts much attention in AI community for years. A variety of issues have been explored. One of the hottest topic is event analysis, where causal information plays a crucial role (Do et al., 2011; Riaz and Girju, 2013; 2014; Mirza and Tonelli, 2014; Kives et al., 2015). Other applications include generation of event causality hypotheses (Hashimoto et al., 2015), motivation identification (Nguyen et al., 2015), causality detection and extraction (Hashimoto et al., 2012; Mihaila and Ananiadou, 2013), causal inference (Tanaka et al., 2012), question answering (Oh et al., 2013), and future scenario generation (Hashimoto et al., 2014). The correlation between temporality and causality is studied by Mirza (2014) and Mirza and Tonelli (2014). Unlike English, no grammatical tense is available in Chinese. Various approaches are explored to address the topic of Chinese tense prediction. Liu et al. (2011) propose an unsupervised method for Chinese tense labelling by learning from a Chinese-English parallel corpus. Zhang and Xue (2014) deal with Chinese tense inference by training a supervised mod"
C16-1210,P14-3002,0,0.136386,"our semisupervised approach improves the dependency-based convolutional neural network (DCNN) models for Chinese tense labelling and thus the causal analysis. 1 Introduction Causal analysis plays a crucial role in the applications such as event extraction (Hashimoto et al., 2012; 2014), causality inference (Tanaka et al., 2012), question-answering (Oh et al., 2013), and motivation identification (Nguyen et al., 2015). Compared to English, the topic of causal analysis in Chinese is rarely touched. In this work, we explore the role of tense information in Chinese causal analysis. As pointed by Mirza (2014), the causal relation and temporal information is correlated. In a causal relation, the cause intuitively precedes its effect. In other words, the tense information could be useful features in the tasks of causal analysis. Two tasks of causal analysis are investigated in this study: causal type classification and causal directionality identification. The Chinese discourse relation corpus, Chinese Discourse Treebank (CDTB) (Li et al., 2014), is adopted as our dataset. In CDTB, six types of causality relations, Purpose, Background, Hypothetical, Inference, Condition, and Cause-Result, are define"
C16-1210,C14-1198,0,0.0306565,"a sentence. The rest of this paper is organized as follows. Section 2 surveys the related work. Section 3 describes the experimental materials. Section 4 shows our approach to Chinese tense labelling. Section 5 illustrates the use of tense information in causal type and causal directionality identification. Section 6 concludes this paper. 2 Related Work Causal analysis attracts much attention in AI community for years. A variety of issues have been explored. One of the hottest topic is event analysis, where causal information plays a crucial role (Do et al., 2011; Riaz and Girju, 2013; 2014; Mirza and Tonelli, 2014; Kives et al., 2015). Other applications include generation of event causality hypotheses (Hashimoto et al., 2015), motivation identification (Nguyen et al., 2015), causality detection and extraction (Hashimoto et al., 2012; Mihaila and Ananiadou, 2013), causal inference (Tanaka et al., 2012), question answering (Oh et al., 2013), and future scenario generation (Hashimoto et al., 2014). The correlation between temporality and causality is studied by Mirza (2014) and Mirza and Tonelli (2014). Unlike English, no grammatical tense is available in Chinese. Various approaches are explored to addre"
C16-1210,D15-1308,0,0.0437231,"Missing"
C16-1210,J03-1002,0,0.0114412,"Missing"
C16-1210,P13-1170,0,0.0619385,"Missing"
C16-1210,W14-4322,0,0.0683198,"Missing"
C16-1210,tian-etal-2014-um,0,0.123759,"se tense predictor is required. The grammatical tense in English explicitly denotes the temporal information for a given text. In Chinese, however, the temporal information is communicated with aspect particles such as 了 (le) and 着 (zhe) and temporal adverbials such as 現在 (“now”) and 明天 (“tomorrow”) (Xue et al., 2008; Ge et al., 2015). In other words, it is more challenging to determine the tense in Chinese text. Thus, we propose a semisupervised algorithm that learns to label tense information in Chinese text. With UM-Corpus, a large English-Chinese parallel corpus aligned at sentence-level (Tian et al., 2014), we generate a pseudolabelled Chinese tense corpus by deriving the tense information from their English counterpart. Dependency-based convolutional neural network (DCNN) is trained to predict Chinese tense. We incorporate the semi-supervised Chinese tense predictor in the tasks of causal type classification and causal directionality identification. The experimental results are compared with the supervised approach and the ideal situation where human-labelled information is available. The contribution of this paper is three-fold: (1) we transfer the tense information from English sentence to i"
C16-1210,xue-etal-2008-annotating,0,0.0199489,"OLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2227–2237, Osaka, Japan, December 11-17 2016. lation from CDTB, and manually label the tense for each argument as ground-truth for the two tasks. To automatically extract the tense features, a Chinese tense predictor is required. The grammatical tense in English explicitly denotes the temporal information for a given text. In Chinese, however, the temporal information is communicated with aspect particles such as 了 (le) and 着 (zhe) and temporal adverbials such as 現在 (“now”) and 明天 (“tomorrow”) (Xue et al., 2008; Ge et al., 2015). In other words, it is more challenging to determine the tense in Chinese text. Thus, we propose a semisupervised algorithm that learns to label tense information in Chinese text. With UM-Corpus, a large English-Chinese parallel corpus aligned at sentence-level (Tian et al., 2014), we generate a pseudolabelled Chinese tense corpus by deriving the tense information from their English counterpart. Dependency-based convolutional neural network (DCNN) is trained to predict Chinese tense. We incorporate the semi-supervised Chinese tense predictor in the tasks of causal type class"
C16-1210,xue-zhang-2014-buy,0,0.0204678,"h et al., 2013), and future scenario generation (Hashimoto et al., 2014). The correlation between temporality and causality is studied by Mirza (2014) and Mirza and Tonelli (2014). Unlike English, no grammatical tense is available in Chinese. Various approaches are explored to address the topic of Chinese tense prediction. Liu et al. (2011) propose an unsupervised method for Chinese tense labelling by learning from a Chinese-English parallel corpus. Zhang and Xue (2014) deal with Chinese tense inference by training a supervised model with various linguistic features on a Chinese tense corpus (Xue and Zhang, 2014). Following the unsupervised method by Liu et al. (2011), we develop a semi-supervised model that benefits from a large amount of data labelled by an accurate English tense predictor. Neural networks such as recurrent neural network (RNN) and convolutional neural network (CNN) are very popular in NLP community. Kim (2014) releases a sentence classifier with convolutional neural network (CNN), where a sentence is represented as a sequence of word vectors (Mikolov et al., 2013). Based on Kim’s work, Ma et al. (2015) propose the dependency-based CNN (DCNN) by adding the structure information feat"
C16-1210,D14-1204,0,0.0190837,"lity detection and extraction (Hashimoto et al., 2012; Mihaila and Ananiadou, 2013), causal inference (Tanaka et al., 2012), question answering (Oh et al., 2013), and future scenario generation (Hashimoto et al., 2014). The correlation between temporality and causality is studied by Mirza (2014) and Mirza and Tonelli (2014). Unlike English, no grammatical tense is available in Chinese. Various approaches are explored to address the topic of Chinese tense prediction. Liu et al. (2011) propose an unsupervised method for Chinese tense labelling by learning from a Chinese-English parallel corpus. Zhang and Xue (2014) deal with Chinese tense inference by training a supervised model with various linguistic features on a Chinese tense corpus (Xue and Zhang, 2014). Following the unsupervised method by Liu et al. (2011), we develop a semi-supervised model that benefits from a large amount of data labelled by an accurate English tense predictor. Neural networks such as recurrent neural network (RNN) and convolutional neural network (CNN) are very popular in NLP community. Kim (2014) releases a sentence classifier with convolutional neural network (CNN), where a sentence is represented as a sequence of word vect"
C16-2059,D11-1142,0,0.0605895,"ional patterns like PATTY (Nakashole et al., 2012) show efficacy on related applications (Dutta et al., 2015). In this work, we present a system for Chinese relation extraction and release a collection of human-verified Chinese relational patterns as a resource. We also demonstrate the applications of relational patterns on the demo website. This paper is organized as follows. Section 2 surveys the related work. Section 3 describes the methodology. Section 4 shows and discusses the results. Section 5 demonstrates the NL2KB system. 2 Related Work Information extraction (IE) models like ReVerb (Fader et al., 2011) automatically extract information from unstructured or semi-structured documents. Given an English sentence, ReVerb identifies two arguments and their relation in the form of (argument1, relation, argument2). PATTY (Nakashole et al., 2012) is a taxonomy system of relational patterns in English. From Wikipedia and the New York Times, 127,811 relational patterns are mined to describe 225 DBpedia properties, and 43,124 relational patterns are mined to describe 25 YAGO’s properties. However, the coverage is still an issue. Most open IE systems are developed for English, and few are for other lang"
C16-2059,D12-1104,0,0.0275646,"becomes a trend. In the sentence “蜜雪兒歐巴馬嫁給巴拉克奧巴馬” (Michelle Obama is married to Barack Obama), there are the two entities, i.e., 蜜雪兒歐巴馬 (Michelle Obama) and 巴拉克奧巴馬 (Barack Obama), and a relation 嫁給 (is married to) between them. In DBpedia, the relation 嫁給 (is married to) is represented as the property &lt;spouse&gt;. In other words, 嫁給 (is married to) in NL side is an NL relational pattern of the property &lt;spouse&gt; in KB side. The vocabulary gap not only affects knowledge base construction, but also knowledge retrieval applications such as question answering. English relational patterns like PATTY (Nakashole et al., 2012) show efficacy on related applications (Dutta et al., 2015). In this work, we present a system for Chinese relation extraction and release a collection of human-verified Chinese relational patterns as a resource. We also demonstrate the applications of relational patterns on the demo website. This paper is organized as follows. Section 2 surveys the related work. Section 3 describes the methodology. Section 4 shows and discusses the results. Section 5 demonstrates the NL2KB system. 2 Related Work Information extraction (IE) models like ReVerb (Fader et al., 2011) automatically extract informat"
C16-2059,D14-1201,0,0.0388192,"Missing"
C16-2059,P15-1128,0,0.02107,"lled NL2KB in this paper, users can browse which properties in KB side may be mapped to for a given relational pattern in NL side. Besides, they can retrieve the sets of relational patterns in NL side for a given property in KB side. We describe how the mapping is established in detail. Although the mined patterns are used for Chinese knowledge base applications, the methodology can be extended to other languages. 1 Introduction Knowledge bases (KBs) such as YAGO (Suchanek et al., 2007) and DBpedia (Lehmann et al., 2014) are useful resources in various applications such as question answering (Yih et al., 2015). KBs contain rich information of entities and their properties. A fact in a KB is usually represented as the form (entity1, property, entity2). Most KBs rely on manpower for editing and maintenance, so it is challenging to keep them up-to-date. Frank et al. (2012) point out the latency issue in knowledge base update. How to construct and update the knowledge base automatically is indispensable. Mining facts from natural language (NL) statements and introducing them to knowledge base becomes a trend. In the sentence “蜜雪兒歐巴馬嫁給巴拉克奧巴馬” (Michelle Obama is married to Barack Obama), there are the tw"
C18-1141,N16-1163,0,0.0172016,"014; Liu et al., 2016; Yu and Dredze, 2014) or a post-processing (Faruqui et al., 2015) fashion. When the need for sense embedding is getting higher, some researches are inspired from the word level embedding learning model and propose sense level embedding (Iacobacci et al., 2015; Jauhar et al., 2015; Lee and Chen, 2017). Although some evidence shows that the sense embedding cannot improve every natural language processing task (Li and Jurafsky, 2015), the benefit of having a sense embedding for improving tasks that need sense level representation is still in great need (Azzini et al., 2012; Ettinger et al., 2016; Qiu et al., 2016). 3 Generalized Sense Retrofitting Model Let ? = {?1 , … , ?? } be a vocabulary of a trained word embedding and |? |be its size. The matrix ? will be the pre-trained collection of vector representations ?? ∈ ℝ? , where ? is the dimensionality of a word vector. Each ?? ∈ ? is learned using a standard word embedding technique (e.g., GloVe (Pennington et al., 2014) or Word2Vec (Mikolov et al., 2013)). Let Ω = (?, ?) be an ontology that contains the semantic relationship, where ? = {?1 , … , ?? } is a set of senses and |? |is total number of senses. The edge (?, ?) ∈ ? indicat"
C18-1141,N15-1184,0,0.0654751,"Missing"
C18-1141,P12-1092,0,0.2912,"gies. In Table 1, row 3 shows the number of words that are both listed in the datasets and the ontology. The word count in Roget is 63,942. MEN 3,000 751 707 Pair count Word count Roget MTurk 287 499 416 RW 2,034 2,951 2,371 WS353 353 437 412 Table 1. A summary of the semantic relatedness benchmark datasets. 4.3 Contextual Word Similarity Although the semantic relatedness datasets are used in many researches, one major disadvantage is that the words in those word pairs do not have contexts. Therefore, we also conduct experiments with the Stanford's Contextual Word Similarities (SCWS) dataset (Huang et al., 2012). SCWS consists of 2,003 word pairs together with human rated scores. A higher score value indicates higher semantic similarity. Different from the semantic relatedness datasets, the words in the SCWS have their contexts and partof-speech tags. That is, the human subjects can know the usage of the word when they rate the similarity. For each word pair, we compute its AvgSimC/MaxSimC scores from the learned sense embedding (Reisinger and Mooney, 2010): ? AvgSimC(?, ? ′) K 1 ≝ 2 ∑ ∑ ??,?,? ?? ′ ,?′ ,? ? (?? (?), ?? (? ′ )) ? (7) ?=1 ?=1 MaxSimC(?, ? ′ ) ≝ ?(?(?), ?(? ′ )) (8) where ??,?,? ≝ ?("
C18-1141,P15-1010,0,0.171877,"pre-trained word embedding, some researches propose post-processing models that incorporate with the existing semantic knowledge into the word embedding model (Faruqui et al., 2015; Yu and Dredze, 2014). However, word embedding models use only one vector to represent a word, and are problematic in some natural language processing applications that require sense level representation (e.g., word sense disambiguation, semantic relation identification, etc.). As a result, some researches try to resolve the polysemy and homonymy issue and introduce sense level embedding, either act as pre-process (Iacobacci et al., 2015) or post-process (Jauhar et al., 2015) fashion. In this research, we focus on the post-processing sense retrofitting approach and propose GenSense, a generalized sense embedding learning framework that retrofits a pre-trained word embedding via incorporating with the semantic relations between the senses, the relation strength and the semantic strength. Although some parts of the idea are not new, it is the first time of putting all the parts into a generalized framework. Our proposed GenSense for generating low-dimensional sense embedding is inspired from sense retro (Jauhar et al., 2015), bu"
C18-1141,N15-1070,0,0.669654,"es propose post-processing models that incorporate with the existing semantic knowledge into the word embedding model (Faruqui et al., 2015; Yu and Dredze, 2014). However, word embedding models use only one vector to represent a word, and are problematic in some natural language processing applications that require sense level representation (e.g., word sense disambiguation, semantic relation identification, etc.). As a result, some researches try to resolve the polysemy and homonymy issue and introduce sense level embedding, either act as pre-process (Iacobacci et al., 2015) or post-process (Jauhar et al., 2015) fashion. In this research, we focus on the post-processing sense retrofitting approach and propose GenSense, a generalized sense embedding learning framework that retrofits a pre-trained word embedding via incorporating with the semantic relations between the senses, the relation strength and the semantic strength. Although some parts of the idea are not new, it is the first time of putting all the parts into a generalized framework. Our proposed GenSense for generating low-dimensional sense embedding is inspired from sense retro (Jauhar et al., 2015), but has three major differences. First,"
C18-1141,W16-2509,0,0.0302928,"′ )) (8) where ??,?,? ≝ ?(?(?), ?? (?)) is the likelihood of context ? belonging to cluster ?? , and ?(?) ≝ ?arg max ??,?,? (?), the maximum likelihood cluster for ? in context ?. We use a window size of 5 for 1≤?≤? the words in the word pairs (i.e., 5 words prior to ?/? ′ and 5 words after ?/? ′ ). Stop words are removed from the context. For measuring the performance, we compute the spearman correlation between the human rated scores and the AvgSimC/MaxSimC scores. 4.4 Semantic Difference This task is defined to answer if a word has a closer semantic feature to a concept than another word (Krebs and Paperno, 2016). In this dataset, there are 528 concepts, 24,963 word pairs, and 128,515 items. Each word pair comes with a feature. For example, in the test (????????, ℎ?????????): ?????, choosing the first word if and only if cos(????????, ?????) > ???(ℎ?????????, ?????), otherwise, choose the second word. As this dataset does not provide context for disambiguation, we use the similar strategies from the semantic relatedness task: ?? ??′ 1 AvgSimD(?, ? ′ ) ≝ ∑ ∑ cos (??? , ?? ′ ) ? ?? ??′ (9) ?=1 ?=1 MaxSimD(?, ? ′ ) ≝ max 1≤?≤?? ,1≤?≤??′ 1666 cos (??? , ?? ′ ) ? (10) In AvgSimD, we choose the first word i"
C18-1141,D17-1034,0,0.144594,"ntain lexical knowledge, such as WordNet (Fellbaum, 1998), Roget’s 21st Century Thesaurus (Kipfer and Institute, 1993) or the paraphrase database (Pavlick et al., 2015). As a result, many researches combine the word embedding with ontological resources, either in a joint training (Bian et al., 2014; Liu et al., 2016; Yu and Dredze, 2014) or a post-processing (Faruqui et al., 2015) fashion. When the need for sense embedding is getting higher, some researches are inspired from the word level embedding learning model and propose sense level embedding (Iacobacci et al., 2015; Jauhar et al., 2015; Lee and Chen, 2017). Although some evidence shows that the sense embedding cannot improve every natural language processing task (Li and Jurafsky, 2015), the benefit of having a sense embedding for improving tasks that need sense level representation is still in great need (Azzini et al., 2012; Ettinger et al., 2016; Qiu et al., 2016). 3 Generalized Sense Retrofitting Model Let ? = {?1 , … , ?? } be a vocabulary of a trained word embedding and |? |be its size. The matrix ? will be the pre-trained collection of vector representations ?? ∈ ℝ? , where ? is the dimensionality of a word vector. Each ?? ∈ ? is learn"
C18-1141,D15-1200,0,0.0409704,"phrase database (Pavlick et al., 2015). As a result, many researches combine the word embedding with ontological resources, either in a joint training (Bian et al., 2014; Liu et al., 2016; Yu and Dredze, 2014) or a post-processing (Faruqui et al., 2015) fashion. When the need for sense embedding is getting higher, some researches are inspired from the word level embedding learning model and propose sense level embedding (Iacobacci et al., 2015; Jauhar et al., 2015; Lee and Chen, 2017). Although some evidence shows that the sense embedding cannot improve every natural language processing task (Li and Jurafsky, 2015), the benefit of having a sense embedding for improving tasks that need sense level representation is still in great need (Azzini et al., 2012; Ettinger et al., 2016; Qiu et al., 2016). 3 Generalized Sense Retrofitting Model Let ? = {?1 , … , ?? } be a vocabulary of a trained word embedding and |? |be its size. The matrix ? will be the pre-trained collection of vector representations ?? ∈ ℝ? , where ? is the dimensionality of a word vector. Each ?? ∈ ? is learned using a standard word embedding technique (e.g., GloVe (Pennington et al., 2014) or Word2Vec (Mikolov et al., 2013)). Let Ω = (?,"
C18-1141,W13-3512,0,0.202468,"ess specifically address, αs are set to 1 in the experiments. We set the convergence criteria for sense vectors to ϵ = 0.1 with the number of iterations of 10. With the capability of generalization, we run three types of the model: GenSense-syn (only considers the synonyms and positive contextual neighbors), GenSense-ant (only considers the antonyms and negative contextual neighbors) and GenSense-all (considers everything). 4.2 Semantic Relatedness We downloaded four semantic relatedness benchmark datasets from the web: MEN (Bruni et al., 2014), MTurk (Radinsky et al., 2011), Rare Words (RW) (Luong et al., 2013) and WordSim353 (WS353) (Finkelstein et al., 2001). In MEN dataset, there are two versions of the word pairs: lemma and natural form. We show the natural form in the experimental result, but the performances on two datasets are similar. In each dataset, there is a list of word pairs together with their corresponding human rated scores. A higher score value indicates higher semantic similarity. For example, the score of (journey, voyage) is 9.29 and the score of (king, cabbage) is 0.23 in WS353. For measuring the semantic similarity between a word pair (?, ? ′ ) in the datasets, we adopt the se"
C18-1141,P15-2070,0,0.0440337,"Missing"
C18-1141,D14-1162,0,0.103966,"om the existing sense retrofitting model. The generalization takes three major components: semantic relations between the senses, the relation strength and the semantic strength. In the experiments, we show that the generalized model outperforms the previous approaches in three aspects: semantic relatedness, contextual word similarity and semantic difference. 1 Introduction The distributed representation of word model (word embedding) has drawn great interest in recent years due to its ability to acquire syntactic and semantic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). With the pre-trained word embedding, some researches propose post-processing models that incorporate with the existing semantic knowledge into the word embedding model (Faruqui et al., 2015; Yu and Dredze, 2014). However, word embedding models use only one vector to represent a word, and are problematic in some natural language processing applications that require sense level representation (e.g., word sense disambiguation, semantic relation identification, etc.). As a result, some researches try to resolve the polysemy and homonymy issue and introduce sense level embedding, either act as pr"
C18-1141,D16-1018,0,0.0210197,"Yu and Dredze, 2014) or a post-processing (Faruqui et al., 2015) fashion. When the need for sense embedding is getting higher, some researches are inspired from the word level embedding learning model and propose sense level embedding (Iacobacci et al., 2015; Jauhar et al., 2015; Lee and Chen, 2017). Although some evidence shows that the sense embedding cannot improve every natural language processing task (Li and Jurafsky, 2015), the benefit of having a sense embedding for improving tasks that need sense level representation is still in great need (Azzini et al., 2012; Ettinger et al., 2016; Qiu et al., 2016). 3 Generalized Sense Retrofitting Model Let ? = {?1 , … , ?? } be a vocabulary of a trained word embedding and |? |be its size. The matrix ? will be the pre-trained collection of vector representations ?? ∈ ℝ? , where ? is the dimensionality of a word vector. Each ?? ∈ ? is learned using a standard word embedding technique (e.g., GloVe (Pennington et al., 2014) or Word2Vec (Mikolov et al., 2013)). Let Ω = (?, ?) be an ontology that contains the semantic relationship, where ? = {?1 , … , ?? } is a set of senses and |? |is total number of senses. The edge (?, ?) ∈ ? indicates a semantic relat"
C18-1141,N10-1013,0,0.424163,"., 2001). In MEN dataset, there are two versions of the word pairs: lemma and natural form. We show the natural form in the experimental result, but the performances on two datasets are similar. In each dataset, there is a list of word pairs together with their corresponding human rated scores. A higher score value indicates higher semantic similarity. For example, the score of (journey, voyage) is 9.29 and the score of (king, cabbage) is 0.23 in WS353. For measuring the semantic similarity between a word pair (?, ? ′ ) in the datasets, we adopt the sense evaluation metrics AvgSim and MaxSim (Reisinger and Mooney, 2010): ?? ??′ 1 AvgSim(?, ? ′ ) ≝ ∑ ∑ cos (??? , ?? ′ ) ? ?? ?? ′ ?=1 ?=1 1665 (5) MaxSim(?, ? ′ ) ≝ max 1≤?≤?? ,1≤?≤??′ cos (??? , ?? ′ ) ? (6) where ?? and ?? ′ denote the number of senses of ? and ? ′ , respectively. The AvgSim can be seen as a soft metric as it averages all the similarity scores. Whereas the MaxSim can be seen as a hard metric as it only selects the senses with maximum similarity score. For measuring the performance of the sense embedding, we compute the spearman correlation between the human rated scores and the AvgSim/MaxSim scores. Table 1 shows a summary of the benchmark da"
C18-1141,P14-2089,0,0.115809,"eneralized model outperforms the previous approaches in three aspects: semantic relatedness, contextual word similarity and semantic difference. 1 Introduction The distributed representation of word model (word embedding) has drawn great interest in recent years due to its ability to acquire syntactic and semantic information from a large unannotated corpus (Mikolov et al., 2013; Pennington et al., 2014). With the pre-trained word embedding, some researches propose post-processing models that incorporate with the existing semantic knowledge into the word embedding model (Faruqui et al., 2015; Yu and Dredze, 2014). However, word embedding models use only one vector to represent a word, and are problematic in some natural language processing applications that require sense level representation (e.g., word sense disambiguation, semantic relation identification, etc.). As a result, some researches try to resolve the polysemy and homonymy issue and introduce sense level embedding, either act as pre-process (Iacobacci et al., 2015) or post-process (Jauhar et al., 2015) fashion. In this research, we focus on the post-processing sense retrofitting approach and propose GenSense, a generalized sense embedding l"
C18-1204,C14-1028,1,0.844615,"check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection (Shiue et al., 2017). Huang et al. (2016) study the Chinese preposition selection problem, which is subsumed by WUE correction. Gated recurrent unit (GRU)-based models are trained to select the most suitable one from a closed set of 43 Chinese prepositions in a context. Nevertheless, it is still worth investigating how to treat WUEs involving other types of words such as verbs a"
C18-1204,W17-5037,0,0.0123316,"rather mature field of study in NLP. Several shared tasks have been conducted for English GEC (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014). Language models, machine learning classifiers, rule-based classifiers, and machine translation models are used. The machine translation approach has the advantage that there is no need to explicitly formulate the types of the errors. A series of English GEC studies are based on the phrase-based statistical machine translation (SMT) framework (Dahlmeier and Ng, 2011; Chollampatt et al., 2016b; Chollampatt et al., 2016a; Chollampatt and Ng, 2017). Nevertheless, the satisfactory performance of the SMT approach cannot be reached without sufficient training data. In fact, Chollampatt et al. (2016a) have shown that the model trained with smaller training data from writers with the same first language (L1) as writers of the test data performs even worse than the model trained with larger training data from writers whose L1 differs from that of the test data. The amount of available Chinese learner data are even less sufficient than that of English ones. Therefore, as a preliminary research in Chinese WUE correction, we impose restrictions"
C18-1204,D16-1195,0,0.0134594,"nal human annotations. 2 Related Work English GEC is a rather mature field of study in NLP. Several shared tasks have been conducted for English GEC (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014). Language models, machine learning classifiers, rule-based classifiers, and machine translation models are used. The machine translation approach has the advantage that there is no need to explicitly formulate the types of the errors. A series of English GEC studies are based on the phrase-based statistical machine translation (SMT) framework (Dahlmeier and Ng, 2011; Chollampatt et al., 2016b; Chollampatt et al., 2016a; Chollampatt and Ng, 2017). Nevertheless, the satisfactory performance of the SMT approach cannot be reached without sufficient training data. In fact, Chollampatt et al. (2016a) have shown that the model trained with smaller training data from writers with the same first language (L1) as writers of the test data performs even worse than the model trained with larger training data from writers whose L1 differs from that of the test data. The amount of available Chinese learner data are even less sufficient than that of English ones. Therefore, as a preliminary rese"
C18-1204,D11-1010,0,0.0247116,"WUE dataset with additional human annotations. 2 Related Work English GEC is a rather mature field of study in NLP. Several shared tasks have been conducted for English GEC (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014). Language models, machine learning classifiers, rule-based classifiers, and machine translation models are used. The machine translation approach has the advantage that there is no need to explicitly formulate the types of the errors. A series of English GEC studies are based on the phrase-based statistical machine translation (SMT) framework (Dahlmeier and Ng, 2011; Chollampatt et al., 2016b; Chollampatt et al., 2016a; Chollampatt and Ng, 2017). Nevertheless, the satisfactory performance of the SMT approach cannot be reached without sufficient training data. In fact, Chollampatt et al. (2016a) have shown that the model trained with smaller training data from writers with the same first language (L1) as writers of the test data performs even worse than the model trained with larger training data from writers whose L1 differs from that of the test data. The amount of available Chinese learner data are even less sufficient than that of English ones. Theref"
C18-1204,W11-2838,0,0.0564404,"Missing"
C18-1204,W12-2006,0,0.0686296,"Missing"
C18-1204,P13-2121,0,0.0327239,"Missing"
C18-1204,C16-1085,1,0.859745,"et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection (Shiue et al., 2017). Huang et al. (2016) study the Chinese preposition selection problem, which is subsumed by WUE correction. Gated recurrent unit (GRU)-based models are trained to select the most suitable one from a closed set of 43 Chinese prepositions in a context. Nevertheless, it is still worth investigating how to treat WUEs involving other types of words such as verbs and nouns. Correcting errors of such open-set types could be much more difficult since the set of candidates can be huge. To the best of our knowledge, this is the first research dealing with general-type Chinese WUE correction. 3 Neural Network-based Correctio"
C18-1204,W15-4401,0,0.0132183,"is the most frequent lexical-level error. In most cases, a wrong usage of a word does not lead to violation of syntactic rules. Instead, its incorrectness cannot be determined without understanding the meaning of the whole sentence. As a result, besides directly adopting the techniques used for English GEC, many aspects of Chinese GEC are worth studying. The Chinese spelling check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection"
C18-1204,W16-4906,0,0.0153392,"ent lexical-level error. In most cases, a wrong usage of a word does not lead to violation of syntactic rules. Instead, its incorrectness cannot be determined without understanding the meaning of the whole sentence. As a result, besides directly adopting the techniques used for English GEC, many aspects of Chinese GEC are worth studying. The Chinese spelling check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection (Shiue et al., 20"
C18-1204,P14-5010,0,0.00249225,"llowing weighted harmonic mean to obtain a new “rank” for the candidate. 1 rcom = α (4) 1−α rLM + rM LP 2415 where α is a parameter that can be tuned with the validation set (actual values will be given in Section 8.2). Preliminary experiments show that harmonic mean performs better than arithmetic and geometric mean. Though rcom may not be an integer, it can be interpreted as a rank. The correction with smaller rcom is considered better. 7 Experimental Settings We adopt the HSK WUE dataset released by Shiue et al. (2017)2 and follow their train/validation/test split. We use Stanford CoreNLP (Manning et al., 2014) for Chinese word segmentation and POS tagging. For each split, we filter the instances where the correction is not within the top 50,000 frequent words in the Chinese part of the ClueWeb corpus3 (Yu et al., 2012). This decision is made based on the fact that the vocabulary used by non-native language learners is limited. After filtering, the number of instances in the train, validation, and test sets are 8,205, 1,026, and 1,025, respectively. With punctuation marks and English words eliminated, the candidate vocabulary size |D |is 48,394. Our MLP model has two hidden layers of size 1,024. The"
C18-1204,K16-1006,0,0.199464,"cope with this problem, we experimented with the CWE variant that only keeps one vector for each Chinese character regardless of its position, but the performance is not as good as the model with 2413 CWE+P features. Alternatively, we design a separate set of character embedding features CWEc which is the sum of the character embedding of all positions divided by the number of characters in the word. 1P For instance, CWEc (決解) = 2 p=s,m,e [CE(決, p) + CE(解, p)]. As can be seen, CWEc (決解) will contain CE(解, s) and CE(決, e), which are the terms of CWEw (解決). 5.3 Context2vec Features Context2vec (Melamud et al., 2016) is a bidirectional LSTM-based model that can encode a “context” into a real-valued vector. A context is a sequence of words with a certain position blanked out. For instance, (E3-1) is a context: (E3-1) 可是 每 個 人 的 [ ] 都 千差萬別 ( but everyone’s [ ] is different ) The representation of a context is a combination of the sequence of words before and after the blank: C2Vctx (w1 ...wp−1 [ ]wp+1 ...wL ) = LSTM(w1 ...wp−1 ) ⊕ LSTM(wp+1 ...wL ) (2) where each wi is a token, L is the number of tokens, p is the index of the blank, and ⊕ is the vector concatenation operation. Context2vec also keeps the emb"
C18-1204,W13-3601,0,0.0213176,"he model trained with larger training data from writers whose L1 differs from that of the test data. The amount of available Chinese learner data are even less sufficient than that of English ones. Therefore, as a preliminary research in Chinese WUE correction, we impose restrictions on our setting that there is exactly one error in a sentence segment, the error position is known, and the error can be corrected by replacing the erroneous token with an appropriate word. The distribution of errors of non-native Chinese differ a lot from that of non-native English. In the CoNLL 2013 shared task (Ng et al., 2013), the most frequent error types are article, preposition, noun number, verb form, and subject-verb agreement. These error types are mostly in violation of English 2411 Input Sentence 生活方式已經 猛烈 地改變了 The way of living has been fiercely Character-enhanced Word Embedding Target Feature changed Candidate Vocabulary Candidate Embedding Context2vec LM Re-Ranking Candidate Vector Context Feature Multilayer Perceptron Correction Candidate Correction Vector Cosine Similarity Candidate Score Figure 1: Overview of our correction generation model. The score of a correction candidate d is predicted for repl"
C18-1204,W14-1701,0,0.0551719,"Missing"
C18-1204,I17-4001,0,0.0188285,"error. In most cases, a wrong usage of a word does not lead to violation of syntactic rules. Instead, its incorrectness cannot be determined without understanding the meaning of the whole sentence. As a result, besides directly adopting the techniques used for English GEC, many aspects of Chinese GEC are worth studying. The Chinese spelling check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection (Shiue et al., 2017). Huang et al. ("
C18-1204,L16-1033,1,0.877351,"tools can help language learners revise their writing. Chinese GEC tools are in high demand since Chinese has become an increasingly popular second language worldwide. Despite the increasing need, most of the existing studies on GEC are based on English learner data. The method of correcting sentences in Chinese, a language which differs substantially from English in major aspects such as the morphological structure and the distribution of learner errors, has not yet been fully developed. This paper focuses on the correction of Chinese word usage errors (WUEs). According to the definition of Shiue and Chen (2016), a WUE refers to an incorrect token that involves morphological, syntactical, or semantical problems. The token is either an incorrect word form, or a correct existent word that is improper for its context. Given a token in a sentence segment that is known to be erroneous, we aim to generate a suitable correction for it. The criteria for a suitable correction are: • Correctness: After substituting the erroneous token with the correction token, the result is a syntactically and semantically correct Chinese sentence segment. This work is licensed under a Creative Commons Attribution 4.0 Interna"
C18-1204,P17-2064,1,0.798642,"Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection (Shiue et al., 2017). Huang et al. (2016) study the Chinese preposition selection problem, which is subsumed by WUE correction. Gated recurrent unit (GRU)-based models are trained to select the most suitable one from a closed set of 43 Chinese prepositions in a context. Nevertheless, it is still worth investigating how to treat WUEs involving other types of words such as verbs and nouns. Correcting errors of such open-set types could be much more difficult since the set of candidates can be huge. To the best of our knowledge, this is the first research dealing with general-type Chinese WUE correction. 3 Neural Ne"
C18-1204,W15-3106,1,0.863638,", in the HSK dynamic composition corpus built by Beijing Language and Culture University, which is the largest available Chinese learner corpus at the time of this study, WUE is the most frequent lexical-level error. In most cases, a wrong usage of a word does not lead to violation of syntactic rules. Instead, its incorrectness cannot be determined without understanding the meaning of the whole sentence. As a result, besides directly adopting the techniques used for English GEC, many aspects of Chinese GEC are worth studying. The Chinese spelling check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction ca"
C18-1204,W13-4406,0,0.0323545,"patterns of correction. In contrast, in the HSK dynamic composition corpus built by Beijing Language and Culture University, which is the largest available Chinese learner corpus at the time of this study, WUE is the most frequent lexical-level error. In most cases, a wrong usage of a word does not lead to violation of syntactic rules. Instead, its incorrectness cannot be determined without understanding the meaning of the whole sentence. As a result, besides directly adopting the techniques used for English GEC, many aspects of Chinese GEC are worth studying. The Chinese spelling check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further"
C18-1204,C12-1184,1,0.756229,"ects of Chinese GEC are worth studying. The Chinese spelling check task (Wu et al., 2013; Yu et al., 2014b; Tseng et al., 2015; Fung et al., 2017) evaluates the detection and correction of character errors. The Shared Task for Chinese Grammatical Error Diagnosis (Yu et al., 2014a; Lee et al., 2015; Lee et al., 2016; Rao et al., 2017) extends the above task to word errors, including redundant word, missing word, word disorder and word selection. Nevertheless, these tasks only deal with detection but not correction. Some researchers focus on certain types of Chinese writing errors. For example, Yu and Chen (2012) identify word ordering errors (WOEs), and Cheng et al. (2014) further recommend word ordering correction candidates with the use of ranking support vector machine (RankSVM). Previous researches on Chinese WUE include segment-level (Shiue and Chen, 2016) and token-level detection (Shiue et al., 2017). Huang et al. (2016) study the Chinese preposition selection problem, which is subsumed by WUE correction. Gated recurrent unit (GRU)-based models are trained to select the most suitable one from a closed set of 43 Chinese prepositions in a context. Nevertheless, it is still worth investigating ho"
C18-1204,yu-etal-2012-development,1,0.824212,". Preliminary experiments show that harmonic mean performs better than arithmetic and geometric mean. Though rcom may not be an integer, it can be interpreted as a rank. The correction with smaller rcom is considered better. 7 Experimental Settings We adopt the HSK WUE dataset released by Shiue et al. (2017)2 and follow their train/validation/test split. We use Stanford CoreNLP (Manning et al., 2014) for Chinese word segmentation and POS tagging. For each split, we filter the instances where the correction is not within the top 50,000 frequent words in the Chinese part of the ClueWeb corpus3 (Yu et al., 2012). This decision is made based on the fact that the vocabulary used by non-native language learners is limited. After filtering, the number of instances in the train, validation, and test sets are 8,205, 1,026, and 1,025, respectively. With punctuation marks and English words eliminated, the candidate vocabulary size |D |is 48,394. Our MLP model has two hidden layers of size 1,024. The activation function is Rectified Linear Unit (ReLU) and the dropout rate is set to 0.2. The parameters are optimized with Adagrad (Duchi et al., 2011) under a cosine proximity objective function. CWE (embedding s"
C18-1204,W14-6820,1,0.909066,"Missing"
C18-2016,P16-1139,0,0.0455085,"eft input right input Figure 2: Tree-LSTM unit used in the recursive neural network. 2.1 Recursive Neural Network Figure 2 illustrates the unit in our RvNN based on the Tree-LSTM unit (Tai et al., 2015). Given the left and the right inputs (i.e. two text segments or two DUs), the Tree-LSTM composition function produces a representation for the new tree node. The Tree-LSTM unit generalizes the LSTM unit to the treebased input. Similar to LSTM, Tree-LSTM makes use of intermediate states as a pair of an active state representation ~h and a memory representation ~c. We use the version similar to (Bowman et al., 2016) as the formula:     ~i σ  ~  ! "" #  fl   σ  ~h1  ~   s (1) fr  =  σ  Wcomp ~ 2 + ~bcomp     hs  ~o   σ  ~g tanh (2) ~c = f~l ~cs2 + f~r ~cs1 + ~i ~g (3) ~h = ~o tanh(~c) where σ is the sigmoid activation function, is the element-wise product, and the pairs h~h1s , ~cs1 i and h~h2s , ~cs2 i are input from its two children tree nodes. The output of Tree-LSTM is the pair h~h, ~ci. Note that the Tree-LSTM unit is designed for binary tree. We handle the multinuclear with the same scheme as Kang et al. (2016). The representations ~h and ~c produced by Tree-LSTM are taken"
C18-2016,W01-1605,0,0.626667,"urse parse tree based on the predictions made by TreeLSTM. The Cocke–Younger–Kasami (CKY) algorithm (Younger, 1967) is employed to maximize the probability of the whole parse tree. The dynamic programing algorithm simulates the recursive parsing procedure, considering local and global information jointly. 3 Experiments We compare our model LSTM-RvNN with the baseline model proposed by Kang et al. (2016). To the best of our knowledge, it is the only existing Chinese discourse parser at the paragraph level. We also evaluate our model given the golden EDUs. The standard evaluation tool PARSEVAL (Carlson et al., 2001) is performed to measure the F-score of the tree structure prediction. Table 1 shows the experimental results. The F-scores of EDU segmentation, parse tree construction (Structure), parse tree construction with sense labeling (+Sense), parse tree construction with center labeling (+Center), and parse tree construction with both sense and center labeling (Overall) are reported. In general, our model outperforms the baseline model in every aspect except EDU segmentation. Even so, the final discourse parse trees constructed and labeled by our model are more accurate. Model Baseline with golden ED"
C18-2016,W12-1636,1,0.826807,"ing is aimed at identifying how the discourse units are related with each other, forming the hierarchical structure of an article. As pointed out by Mann and Thompson (1988), no part in an article is completely isolated. The discourse structure provides critical information to understand an article. NLP tasks such as summarization (Louis et al., 2010), information retrieval (Lioma et al., 2012), and text categorization (Ji and Smith, 2017) have been shown benefited from the information extracted by discourse parsing. Prior work of Chinese discourse parsing focuses on intra-sentential parsing (Huang and Chen, 2012). The CoNLL 2016 Shared Task deals with shallow parsing (Xue et al., 2016). So far, there is quite less work on complete hierarchical Chinese discourse parsing at paragraph or article level (Kang et al., 2016). The subtasks in Chinese discourse parsing depend on each other. In a pipelined system, there may be a severe issue of error propagation among elementary discourse unit (EDU) segmentation, connective recognition, parse tree construction, and relation labeling (Kang et al., 2016). The other problem is that prior Chinese discourse parser relies on linguistic features extracted by external"
C18-2016,P17-1092,0,0.0252636,"迴類神經網路為基 礎，同時對四項子任務進行學習，在中文語篇樹庫（CDTB）資料集上，達到最先進的 效能。我們釋出了這個剖析器的原始碼與預先訓練完成的模型，立即可用。據我們所 知，這是第一個開放原始碼的中文剖析工具集，而且這套獨立的工具集不須依賴外部資 源（如句法剖析器），便於下游應用的整合。 1 Introduction Discourse parsing is aimed at identifying how the discourse units are related with each other, forming the hierarchical structure of an article. As pointed out by Mann and Thompson (1988), no part in an article is completely isolated. The discourse structure provides critical information to understand an article. NLP tasks such as summarization (Louis et al., 2010), information retrieval (Lioma et al., 2012), and text categorization (Ji and Smith, 2017) have been shown benefited from the information extracted by discourse parsing. Prior work of Chinese discourse parsing focuses on intra-sentential parsing (Huang and Chen, 2012). The CoNLL 2016 Shared Task deals with shallow parsing (Xue et al., 2016). So far, there is quite less work on complete hierarchical Chinese discourse parsing at paragraph or article level (Kang et al., 2016). The subtasks in Chinese discourse parsing depend on each other. In a pipelined system, there may be a severe issue of error propagation among elementary discourse unit (EDU) segmentation, connective recognition,"
C18-2016,K16-2003,0,0.321901,"solated. The discourse structure provides critical information to understand an article. NLP tasks such as summarization (Louis et al., 2010), information retrieval (Lioma et al., 2012), and text categorization (Ji and Smith, 2017) have been shown benefited from the information extracted by discourse parsing. Prior work of Chinese discourse parsing focuses on intra-sentential parsing (Huang and Chen, 2012). The CoNLL 2016 Shared Task deals with shallow parsing (Xue et al., 2016). So far, there is quite less work on complete hierarchical Chinese discourse parsing at paragraph or article level (Kang et al., 2016). The subtasks in Chinese discourse parsing depend on each other. In a pipelined system, there may be a severe issue of error propagation among elementary discourse unit (EDU) segmentation, connective recognition, parse tree construction, and relation labeling (Kang et al., 2016). The other problem is that prior Chinese discourse parser relies on linguistic features extracted by external third party packages. This is an important issue especially for a pipeline system. Extracting feature from free text is also an issue, while most systems rely on external syntactic parser for providing informa"
C18-2016,D14-1220,0,0.335996,"ipeline system. Extracting feature from free text is also an issue, while most systems rely on external syntactic parser for providing informations to do the above tasks. For a toolkit targeting real-world applications, a standalone system is more robust and easy to This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 73 Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, pages 73–77 Santa Fe, New Mexico, USA, August 20-26, 2018. deploy. Inspired by Li et al. (2014a), in this work we propose an end-to-end Chinese discourse parser that performs EDU segmentation, discourse tree construction, and discourse relation labeling in a unified framework based on recursive neural network (RvNN) proposed by Goller and Kuchler (1996). The RvNN model learns to construct the structured output through merging children nodes to parent nodes in the bottom-up fashion. Within the RvNN paradigm, recurrent neural network (RNN) is employed to model the representations from word segments, discourse units, to the whole paragraph. RNN like long short-term memory (LSTM) neural ne"
C18-2016,D14-1224,0,0.318033,"ipeline system. Extracting feature from free text is also an issue, while most systems rely on external syntactic parser for providing informations to do the above tasks. For a toolkit targeting real-world applications, a standalone system is more robust and easy to This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 73 Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, pages 73–77 Santa Fe, New Mexico, USA, August 20-26, 2018. deploy. Inspired by Li et al. (2014a), in this work we propose an end-to-end Chinese discourse parser that performs EDU segmentation, discourse tree construction, and discourse relation labeling in a unified framework based on recursive neural network (RvNN) proposed by Goller and Kuchler (1996). The RvNN model learns to construct the structured output through merging children nodes to parent nodes in the bottom-up fashion. Within the RvNN paradigm, recurrent neural network (RNN) is employed to model the representations from word segments, discourse units, to the whole paragraph. RNN like long short-term memory (LSTM) neural ne"
C18-2016,W10-4327,0,0.021974,"篇單元分割、剖析樹建立、主次關係識別、語 篇關係辨識等。本文展示一個點對點中文語篇剖析器，並提出一套統一架構，可以對輸 入之中文篇章直接產生完整的中文語篇剖析結果。我們的剖析器以遞迴類神經網路為基 礎，同時對四項子任務進行學習，在中文語篇樹庫（CDTB）資料集上，達到最先進的 效能。我們釋出了這個剖析器的原始碼與預先訓練完成的模型，立即可用。據我們所 知，這是第一個開放原始碼的中文剖析工具集，而且這套獨立的工具集不須依賴外部資 源（如句法剖析器），便於下游應用的整合。 1 Introduction Discourse parsing is aimed at identifying how the discourse units are related with each other, forming the hierarchical structure of an article. As pointed out by Mann and Thompson (1988), no part in an article is completely isolated. The discourse structure provides critical information to understand an article. NLP tasks such as summarization (Louis et al., 2010), information retrieval (Lioma et al., 2012), and text categorization (Ji and Smith, 2017) have been shown benefited from the information extracted by discourse parsing. Prior work of Chinese discourse parsing focuses on intra-sentential parsing (Huang and Chen, 2012). The CoNLL 2016 Shared Task deals with shallow parsing (Xue et al., 2016). So far, there is quite less work on complete hierarchical Chinese discourse parsing at paragraph or article level (Kang et al., 2016). The subtasks in Chinese discourse parsing depend on each other. In a pipelined system, there may be a severe issue of err"
C18-2016,P15-1150,0,0.0479636,"e11 s3 s2 s1 w13 …… w21 w22 w23 …… e31 e32 e33 …… =Embedding Layer w31 w32 w33 …… Figure 1: Architecture of our unified RvNN discourse parser. 1 Merge Score Sense Classifier Center Classifier http://nlg.csie.ntu.edu.tw/nlpresource/cdp/ to parent unit 74 Tree-LSTM Unit left input right input w11 w12 w13 …… Merge Score w21 w22 Sense Classifier w23 …… w31 Center Classifier to parent unit w32 w33 …… Tree-LSTM Unit left input right input Figure 2: Tree-LSTM unit used in the recursive neural network. 2.1 Recursive Neural Network Figure 2 illustrates the unit in our RvNN based on the Tree-LSTM unit (Tai et al., 2015). Given the left and the right inputs (i.e. two text segments or two DUs), the Tree-LSTM composition function produces a representation for the new tree node. The Tree-LSTM unit generalizes the LSTM unit to the treebased input. Similar to LSTM, Tree-LSTM makes use of intermediate states as a pair of an active state representation ~h and a memory representation ~c. We use the version similar to (Bowman et al., 2016) as the formula:     ~i σ  ~  ! "" #  fl   σ  ~h1  ~   s (1) fr  =  σ  Wcomp ~ 2 + ~bcomp     hs  ~o   σ  ~g tanh (2) ~c = f~l ~cs2 + f~r ~cs1 + ~i ~g (3)"
C18-2016,K16-2001,0,0.0256856,"r, forming the hierarchical structure of an article. As pointed out by Mann and Thompson (1988), no part in an article is completely isolated. The discourse structure provides critical information to understand an article. NLP tasks such as summarization (Louis et al., 2010), information retrieval (Lioma et al., 2012), and text categorization (Ji and Smith, 2017) have been shown benefited from the information extracted by discourse parsing. Prior work of Chinese discourse parsing focuses on intra-sentential parsing (Huang and Chen, 2012). The CoNLL 2016 Shared Task deals with shallow parsing (Xue et al., 2016). So far, there is quite less work on complete hierarchical Chinese discourse parsing at paragraph or article level (Kang et al., 2016). The subtasks in Chinese discourse parsing depend on each other. In a pipelined system, there may be a severe issue of error propagation among elementary discourse unit (EDU) segmentation, connective recognition, parse tree construction, and relation labeling (Kang et al., 2016). The other problem is that prior Chinese discourse parser relies on linguistic features extracted by external third party packages. This is an important issue especially for a pipeline"
C18-2030,C14-1028,1,0.818261,"istical machine translation (SMT)-based English GEC system is released by Chollampatt and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al. (2016) correct preposition errors. Nevertheless, there has not yet been a general model that handles all types of Chinese writing errors. Given the promising results of translation approaches in English, it is worth investigating their effectiveness in Chinese. Because the machine translation models need to be trained with parallel corpus of wrong-corrected sentences and there is limited amount of Chinese learner data with annotated corrections, we use NMT models and facilitate them with word embeddings pre-trained"
C18-2030,W17-5037,0,0.0159644,"trast, there is far fewer readily usable writing correction tools for Chinese. Chinese has become a popular foreign language to learn worldwide, motivating the development of Chinese writing correction system targeting second language (L2) learners. Unlike the classification approach, the translation approach to English GEC does not require exact recognition of error types. With many-to-many mappings handled, it is possible to deal with multiple errors of various types with a single translation model. An open-source statistical machine translation (SMT)-based English GEC system is released by Chollampatt and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al."
C18-2030,C16-1085,1,0.861265,"and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al. (2016) correct preposition errors. Nevertheless, there has not yet been a general model that handles all types of Chinese writing errors. Given the promising results of translation approaches in English, it is worth investigating their effectiveness in Chinese. Because the machine translation models need to be trained with parallel corpus of wrong-corrected sentences and there is limited amount of Chinese learner data with annotated corrections, we use NMT models and facilitate them with word embeddings pre-trained on large amount of well-formed Chinese text. To our knowledge, we are the first to ap"
C18-2030,P17-4012,0,0.0426378,"it to the correction of Chinese. A typical NMT model is composed of an encoder and a decoder. The encoder transforms the input sequence into a sequence of hidden states, each of which is calculated with the hidden state of the previous time step and the input of the current time step. The decoder predicts the distribution of words for each time step conditioned on the encoder hidden states and the output of all previous time steps. The encoder-decoder network is trained to maximize the likelihood of the ground-truth translations in the training data. Our system is built on the top of OpenNMT (Klein et al., 2017). We adopt a bidirectional Long-short Term Memory (LSTM) encoder and a two-layer LSTM decoder. Global attention over the sequence of hidden states at the source side is applied. The model generates one to five corrections according to the n-best decoding result. Several design choices will be discussed in Section 2.2. 2.1 Datasets and Evaluation To train the NMT correction model, we utilize the publicly available datasets of the NLPTEA 14-17 CGED shared tasks1 . As a whole, there are more simplified Chinese sentences than traditional Chinese ones, so we convert all sentences to simplified Chin"
C18-2030,N15-1142,0,0.0159923,"ce “* 我 覺得 他 是 一個 很 好人” (* I think he is a very good-person) is corrected to “我 覺得 他 是 一個 很 好 的 人” (I think he is a very good person). Based on these results, we decide to use the word-based NMT model in our system. Pre-trained Word Embeddings Initializing word representations in NMT models with pre-trained word vectors can be useful when the training data is insufficient. In addition to the standard Word2vec continuous bag-of-words (CBOW) and Skip-gram (SG) embeddings (Mikolov et al., 2013), we also experiment with the continuous window (CWIN) and structured skip-gram (Struct-SG) embeddings (Ling et al., 2015), which consider the relative order of context words during training and are shown to be useful for Chinese error detection (Shiue et al., 2017). We segment the Chinese part of ClueWeb4 with the THULAC toolkit and train the embeddings with it. The embedding size is fixed to 500 and the context window size is 5 for all kinds of embeddings. The results are summarized in Table 1. All pre-trained word embeddings bring improvement over random embeddings. Generally, the NMT correction model with pre-trained Struct-SG embeddings achieves the best performance. Thus, we use Struct-SG embeddings in our"
C18-2030,P15-2097,0,0.0273702,"use the test data of NLPTEA 16 and 17 since there are only error type labels but no correction in the datasets. The correction performance can be evaluated by judging whether a correction is exactly the same as the ground-truth. We report the accuracy as well as hit rates of top candidates. However, hit rates can still be 1 https://sites.google.com/view/nlptea2018/shared-task 138 somehow strict since a model will not get any scores even if the top candidate it proposes is only slightly different from the answer. Thus, we also report the General Language Evaluation Understanding (GLEU) metric (Napoles et al., 2015), which is a modification of BLEU that rewards correct modifications while penalizing unnecessary changes. We use the publicly released toolkit2 to calculate GLEU of n-gram order 4. GLEU is calculated only for the top candidate. 2.2 Design Choices There are several design choices for building the NMT-based correction system. We discuss the reasons for each decision and show experimental results when necessary. In the experiments, we choose the model with the highest validation GLEU and report the performance on the test set. The GLEU of an output that is completely the same as the source can b"
C18-2030,I17-4001,0,0.0247161,"gnition of error types. With many-to-many mappings handled, it is possible to deal with multiple errors of various types with a single translation model. An open-source statistical machine translation (SMT)-based English GEC system is released by Chollampatt and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al. (2016) correct preposition errors. Nevertheless, there has not yet been a general model that handles all types of Chinese writing errors. Given the promising results of translation approaches in English, it is worth investigating their effectiveness in Chinese. Because the machine translation models need to be trained with parallel corpus of w"
C18-2030,L16-1033,1,0.749204,"lish GEC system is released by Chollampatt and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al. (2016) correct preposition errors. Nevertheless, there has not yet been a general model that handles all types of Chinese writing errors. Given the promising results of translation approaches in English, it is worth investigating their effectiveness in Chinese. Because the machine translation models need to be trained with parallel corpus of wrong-corrected sentences and there is limited amount of Chinese learner data with annotated corrections, we use NMT models and facilitate them with word embeddings pre-trained on large amount of well-formed Chinese text"
C18-2030,P17-2064,1,0.916497,"eased by Chollampatt and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al. (2016) correct preposition errors. Nevertheless, there has not yet been a general model that handles all types of Chinese writing errors. Given the promising results of translation approaches in English, it is worth investigating their effectiveness in Chinese. Because the machine translation models need to be trained with parallel corpus of wrong-corrected sentences and there is limited amount of Chinese learner data with annotated corrections, we use NMT models and facilitate them with word embeddings pre-trained on large amount of well-formed Chinese text. To our knowledge, w"
C18-2030,tian-etal-2014-um,0,0.0232397,"0.418 0.414 0.433 0.431 char. GLEU 0.552 0.625 0.650 0.655 0.657 0.658 0.668 Table 1: Performance of NMT-based correction models 2 https://github.com/cnap/gec-ranking http://thulac.thunlp.org/ 4 http://lemurproject.org/clueweb09.php 3 139 word GLEU 0.411 0.558 0.564 0.564 0.566 0.580 3 Example Sentence Retrieval Besides giving correction suggestions, our system also shows example sentences to demonstrate how to correctly use the words and grammar patterns in the user input. These example sentences also serve as additional evidence of the correctness of some usage patterns.We adopt UM-Corpus (Tian et al., 2014), a sentence-aligned English-Chinese corpus, as the database of example sentences. We only use sentences in the “Education” domain, which are extracted from online teaching materials. There are 450,000 English-Chinese sentence pairs. We exclude example sentence pairs in which the Chinese sentence is longer than 30 Chinese characters since they usually have complex syntactic structures. Upon user input, ten example sentences are retrieved. They are ranked by the overlaps of Chinese character bigrams. The more character bigrams an example sentence has in common with the input sentence, the highe"
C18-2030,N16-1042,0,0.0278049,"ting the development of Chinese writing correction system targeting second language (L2) learners. Unlike the classification approach, the translation approach to English GEC does not require exact recognition of error types. With many-to-many mappings handled, it is possible to deal with multiple errors of various types with a single translation model. An open-source statistical machine translation (SMT)-based English GEC system is released by Chollampatt and Ng (2017). More recently, neural machine translation (NMT) is applied to English GEC and improvements over the SMT baseline are shown (Yuan and Briscoe, 2016). With the use of distributional word representations, NMT has better ability to generalize to unseen corrections. The Shared Task for Chinese Grammatical Error Diagnosis (CGED) (Rao et al., 2017) only evaluates detection but not correction performance until 2017. Some studies focus on certain error types of L2 Chinese, such as word ordering errors (Cheng et al., 2014) and word usage errors (Shiue and Chen, 2016; Shiue et al., 2017). Huang et al. (2016) correct preposition errors. Nevertheless, there has not yet been a general model that handles all types of Chinese writing errors. Given the p"
C88-1024,J87-1001,0,0.731406,"Missing"
C88-1024,J81-4003,0,\N,Missing
C90-2009,J81-4003,0,0.157122,"r is a restrictive context sensitive grammar. This is because the truth value of a movement non-terminal depends on the appearance of a virtual non-temainal preceding or following it. /Chen 1988/ proposes a bottom-up parsing system for GBLGs. Figure 2 shows the execution of our sample grammar for the sentence &quot;The man who he met is a teacher&quot;. The label on the are indicates the step number during parsing. The empty constituent trace is generated in phrase vp, then passed to phrase s, and finally cut in phrase rel. Comp,&apos;tred with other logic programming approaches/Matsumoto 1983, McCord 1987, Pereira 1981, Stabler 1987/, especially R L G s / S t a b l e r 1987/, GBLGs have the following features: (1) the uniform treatments of leftward movement and the rightward movement, (2) the arbitrary number of movement non-terminals in 50 the rule body, (3) automatic detection of grammar errors befi)re parsing. The former two features are useful to express the highly flexible languages like Chinese. 4. A Chinese Parser 4.1 Topic-comment Structures Topic-comment structure is one of the specific features in Mandarin Chinese. There are several interesting linguistic phenomena concerning these structures: (1)"
C90-2009,J87-1001,0,0.0199932,"tive context sensitive grammar. This is because the truth value of a movement non-terminal depends on the appearance of a virtual non-temainal preceding or following it. /Chen 1988/ proposes a bottom-up parsing system for GBLGs. Figure 2 shows the execution of our sample grammar for the sentence &quot;The man who he met is a teacher&quot;. The label on the are indicates the step number during parsing. The empty constituent trace is generated in phrase vp, then passed to phrase s, and finally cut in phrase rel. Comp,&apos;tred with other logic programming approaches/Matsumoto 1983, McCord 1987, Pereira 1981, Stabler 1987/, especially R L G s / S t a b l e r 1987/, GBLGs have the following features: (1) the uniform treatments of leftward movement and the rightward movement, (2) the arbitrary number of movement non-terminals in 50 the rule body, (3) automatic detection of grammar errors befi)re parsing. The former two features are useful to express the highly flexible languages like Chinese. 4. A Chinese Parser 4.1 Topic-comment Structures Topic-comment structure is one of the specific features in Mandarin Chinese. There are several interesting linguistic phenomena concerning these structures: (1) Topic may be"
C90-2009,C88-1024,1,\N,Missing
C94-1026,P91-1022,0,0.120268,"Missing"
C94-1026,P91-1017,0,0.0258946,"Missing"
C94-1026,E93-1015,0,0.0141451,"rtantly, the experimental objects are Chinese-English texts, which are selected from different language families. 1. I n t r o d u c t i o n Real texts provide the alive phenomena, usages, and tendency of langnage in a parlictflar space and time. This recommends us to do the researches on the corpora. Recently, many rese~{rchers timber claim that ""two languages art more informative than one"" (Dagan, 1991). They show that two languages coukl disambigna.te each other (Gale e¢ al., 1992); bilingual corpus could form a bilingual dictionary (Brown et al., 1988) and terminology correspondence bank (Eijk, 1993); a refined bilingual corpus could be formed the examples for machine translation systems (Sumita et al., 1990). To do such kinds of researches, the most impmlant task is to align the bilingual texts. Many length-based alignment algorithms have been proposed (Brown et al., 1991; Gale and Church, 1991a). The correct rates are good. However, the languages they processed belong to occidental family. When these algorithms are applied to other rtmning texts from different families, will the performance keep on tile same level? Other translation-based alignments (Kay, 199l; Chen, 1993) show the diff"
C94-1026,H91-1026,0,0.0254337,"he corpora. Recently, many rese~{rchers timber claim that ""two languages art more informative than one"" (Dagan, 1991). They show that two languages coukl disambigna.te each other (Gale e¢ al., 1992); bilingual corpus could form a bilingual dictionary (Brown et al., 1988) and terminology correspondence bank (Eijk, 1993); a refined bilingual corpus could be formed the examples for machine translation systems (Sumita et al., 1990). To do such kinds of researches, the most impmlant task is to align the bilingual texts. Many length-based alignment algorithms have been proposed (Brown et al., 1991; Gale and Church, 1991a). The correct rates are good. However, the languages they processed belong to occidental family. When these algorithms are applied to other rtmning texts from different families, will the performance keep on tile same level? Other translation-based alignments (Kay, 199l; Chen, 1993) show the difficulty in determining the word correspondence and are very complex. In tiffs paper, we will introduce a part-of-speech (POS)-based alignment algorithm. Section 2 will touch on the level of alignment and define the sentence terminators. In Section 3, we will propose tile criterion of critical POSes an"
C94-1026,P91-1023,0,0.0318718,"he corpora. Recently, many rese~{rchers timber claim that ""two languages art more informative than one"" (Dagan, 1991). They show that two languages coukl disambigna.te each other (Gale e¢ al., 1992); bilingual corpus could form a bilingual dictionary (Brown et al., 1988) and terminology correspondence bank (Eijk, 1993); a refined bilingual corpus could be formed the examples for machine translation systems (Sumita et al., 1990). To do such kinds of researches, the most impmlant task is to align the bilingual texts. Many length-based alignment algorithms have been proposed (Brown et al., 1991; Gale and Church, 1991a). The correct rates are good. However, the languages they processed belong to occidental family. When these algorithms are applied to other rtmning texts from different families, will the performance keep on tile same level? Other translation-based alignments (Kay, 199l; Chen, 1993) show the difficulty in determining the word correspondence and are very complex. In tiffs paper, we will introduce a part-of-speech (POS)-based alignment algorithm. Section 2 will touch on the level of alignment and define the sentence terminators. In Section 3, we will propose tile criterion of critical POSes an"
C94-1026,1992.tmi-1.9,0,0.0654282,"Missing"
C94-1026,J93-1004,0,\N,Missing
C94-1026,C88-1016,0,\N,Missing
C94-1026,P91-1024,0,\N,Missing
C94-1026,J93-1006,0,\N,Missing
C94-1026,P93-1002,0,\N,Missing
C96-1038,J93-2004,0,\N,Missing
C96-1038,O90-1004,0,\N,Missing
C96-1038,J93-1005,0,\N,Missing
C96-1038,C94-2195,0,\N,Missing
C96-1038,1995.tmi-1.21,1,\N,Missing
C96-1038,P83-1017,0,\N,Missing
C96-1039,C94-1026,1,0.86399,"Missing"
C96-1039,C92-1019,0,0.204083,"Missing"
C96-1039,O92-1003,0,0.0495103,"Missing"
C96-1039,O93-1004,0,0.112768,"Missing"
C96-1039,J93-1007,0,0.124504,"Missing"
C96-1039,P94-1010,0,0.233736,"Missing"
C96-1039,C92-4199,0,0.292146,"Missing"
C96-1039,J96-3004,0,\N,Missing
C96-1039,C90-2009,1,\N,Missing
C98-1036,C96-1030,0,0.0367397,"Missing"
C98-1036,C96-1039,1,0.864457,"Missing"
C98-1036,M95-1018,0,0.0222634,"Missing"
C98-1036,W93-0105,0,0.108786,"Missing"
C98-1036,W93-0104,0,0.0890788,"Missing"
C98-1036,W93-0114,0,0.0427972,"Missing"
C98-1036,W97-0315,0,0.0405711,"Missing"
C98-1036,M95-1006,0,0.0931239,"Missing"
C98-1036,M92-1024,0,\N,Missing
chen-chu-2004-pattern,W03-1501,1,\N,Missing
chen-chu-2004-pattern,C96-1039,1,\N,Missing
chen-chu-2004-pattern,M98-1017,1,\N,Missing
D08-1015,I05-1001,0,0.0117171,"and results. Section 7 concludes the paper. 2 Related Work Only a few studies in the past deal with the reader aspect of emotion analysis. For example, Lin et al. (2007; 2008) classify documents into readeremotion categories. Most previous work focuses on the writer’s perspective. Pang et al. (2002) design an algorithm to determine whether a document’s author expresses a positive or negative sentiment. They discover that using Support Vector Machines (SVM) with word unigram features results in the best performance. Since then, more work has been done to find features better than unigrams. In (Hu et al., 2005), word sentiment information is exploited to achieve better classification accuracy. Experiments have been done to extract emotional information from texts at granularities finer than documents. Wiebe (2000) investigates the subjectivity of words, whereas Aman and Szpakowicz (2007) manually label phrases with emotional categories. In 2007, the SemEval-2007 workshop organized a task on the unsupervised annotation of news headlines with emotions (Strapparava and Mihalcea, 2007). As for the task of ranking, many machinelearning algorithms have been proposed in information retrieval. These techniq"
D08-1015,W02-1011,0,0.0164527,"with the correct lists. The rest of this paper is organized as follows. Section 2 describes related work. In Section 3, details about the two proposed approaches are provided. Section 4 introduces the corpus and Section 5 presents how features are extracted from the corpus. Section 6 shows the experiment procedures and results. Section 7 concludes the paper. 2 Related Work Only a few studies in the past deal with the reader aspect of emotion analysis. For example, Lin et al. (2007; 2008) classify documents into readeremotion categories. Most previous work focuses on the writer’s perspective. Pang et al. (2002) design an algorithm to determine whether a document’s author expresses a positive or negative sentiment. They discover that using Support Vector Machines (SVM) with word unigram features results in the best performance. Since then, more work has been done to find features better than unigrams. In (Hu et al., 2005), word sentiment information is exploited to achieve better classification accuracy. Experiments have been done to extract emotional information from texts at granularities finer than documents. Wiebe (2000) investigates the subjectivity of words, whereas Aman and Szpakowicz (2007) m"
D08-1015,S07-1013,0,0.0128581,"word unigram features results in the best performance. Since then, more work has been done to find features better than unigrams. In (Hu et al., 2005), word sentiment information is exploited to achieve better classification accuracy. Experiments have been done to extract emotional information from texts at granularities finer than documents. Wiebe (2000) investigates the subjectivity of words, whereas Aman and Szpakowicz (2007) manually label phrases with emotional categories. In 2007, the SemEval-2007 workshop organized a task on the unsupervised annotation of news headlines with emotions (Strapparava and Mihalcea, 2007). As for the task of ranking, many machinelearning algorithms have been proposed in information retrieval. These techniques generate ranking functions which predict the relevance of a 137 document. One class of algorithms minimizes the errors resulting from ordering document pairs incorrectly. Examples include (Joachims, 2002), (Freund et al., 2003) and (Qin et al., 2007). In particular, the training phase of the Joachims’ Ranking SVM (Joachims, 2002) is formulated as the following SVM optimization problem: min w, ξ i , j , k 1 wTw 2 +C ∑ξ i , j ,k subject to: ∀( qk , d i ), ( qk , d j ) ∈ V |"
D09-1131,D08-1014,0,0.0248716,"entences, we identify these relations and utilize them for opinion analysis on sentences. As the experimental corpus, some researchers managed to generate annotated materials and gold standards under many constraints. Ku set a standard for generating final answers from annotations of multiple annotators (Ku et al., 2007), and Somasundaran annotated discourse information from meeting dialogs to train a sentiment model (Somasundaran et al., 2007). For multilingual issues, researchers concerned mainly about the applicability of corpus and algorithms from the native language to foreign languages (Banea et al., 2008; Bautin et al., 2008). Several opinion analysis systems have been developed so far. OASYS (Cesarano et al., 2007) and CopeOpi (Ku et al., 2007) allow users input their queries and select preferred data sources, 1260 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1260–1269, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP and then track opinions in a time zone. For both systems, extracting opinions is the main focus, while holders and targets are identified implicitly when retrieving relevant documents. Carenini’s team proposed a graphical user in"
D09-1131,C04-1200,0,0.307965,"lling, and so on. Opinion dictionaries are important resources for identifying subjective information. Several approaches were proposed to collect such resources. Wiebe (2000) learned subjective adjectives from corpora. Takamura et al. (2005) extracted semantic orientations of words. Ku et al. (2007) measured sentiment degrees of Chinese words by averaging the sentiment scores of the composing characters. When the opinion words are available, the polarities of sentences and documents can be determined by them. Riloff and Wiebe (2003) learned the extraction patterns for subjective expressions. Kim and Hovy (2004) found the polarity of subjective expressions. Pang et al. (2002) and Dave et al. (2003) explored various techniques at document level. Morphological information has been widely used in classifying words, telling the meanings, and doing other in-depth analysis (Tzeng and Chen, 2002). However, morphological information was seldom applied either in Chinese opinion extraction, or in solving the coverage problem of opinion dictionary. Instead of bag-ofcharacters approach (Ku et al., 2007), this paper employs morphological structures of words to extract opinion words. Relations between sentence seg"
D09-1131,W02-1011,0,0.0124422,"identifying subjective information. Several approaches were proposed to collect such resources. Wiebe (2000) learned subjective adjectives from corpora. Takamura et al. (2005) extracted semantic orientations of words. Ku et al. (2007) measured sentiment degrees of Chinese words by averaging the sentiment scores of the composing characters. When the opinion words are available, the polarities of sentences and documents can be determined by them. Riloff and Wiebe (2003) learned the extraction patterns for subjective expressions. Kim and Hovy (2004) found the polarity of subjective expressions. Pang et al. (2002) and Dave et al. (2003) explored various techniques at document level. Morphological information has been widely used in classifying words, telling the meanings, and doing other in-depth analysis (Tzeng and Chen, 2002). However, morphological information was seldom applied either in Chinese opinion extraction, or in solving the coverage problem of opinion dictionary. Instead of bag-ofcharacters approach (Ku et al., 2007), this paper employs morphological structures of words to extract opinion words. Relations between sentence segments are also defined by linguistics in the Chinese language. Th"
D09-1131,W03-1014,0,0.0262565,"variety of fields, including product recommendation, review summarization, public polling, and so on. Opinion dictionaries are important resources for identifying subjective information. Several approaches were proposed to collect such resources. Wiebe (2000) learned subjective adjectives from corpora. Takamura et al. (2005) extracted semantic orientations of words. Ku et al. (2007) measured sentiment degrees of Chinese words by averaging the sentiment scores of the composing characters. When the opinion words are available, the polarities of sentences and documents can be determined by them. Riloff and Wiebe (2003) learned the extraction patterns for subjective expressions. Kim and Hovy (2004) found the polarity of subjective expressions. Pang et al. (2002) and Dave et al. (2003) explored various techniques at document level. Morphological information has been widely used in classifying words, telling the meanings, and doing other in-depth analysis (Tzeng and Chen, 2002). However, morphological information was seldom applied either in Chinese opinion extraction, or in solving the coverage problem of opinion dictionary. Instead of bag-ofcharacters approach (Ku et al., 2007), this paper employs morphologi"
D09-1131,2007.sigdial-1.5,0,0.0302449,"n sentence segments are also defined by linguistics in the Chinese language. These are similar to morphological structures between Chinese characters. Based on parsing trees of sentences, we identify these relations and utilize them for opinion analysis on sentences. As the experimental corpus, some researchers managed to generate annotated materials and gold standards under many constraints. Ku set a standard for generating final answers from annotations of multiple annotators (Ku et al., 2007), and Somasundaran annotated discourse information from meeting dialogs to train a sentiment model (Somasundaran et al., 2007). For multilingual issues, researchers concerned mainly about the applicability of corpus and algorithms from the native language to foreign languages (Banea et al., 2008; Bautin et al., 2008). Several opinion analysis systems have been developed so far. OASYS (Cesarano et al., 2007) and CopeOpi (Ku et al., 2007) allow users input their queries and select preferred data sources, 1260 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1260–1269, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP and then track opinions in a time zone. For both systems, e"
D09-1131,P05-1017,0,0.0274812,"tence extraction, and 0.54 for opinion sentence polarity detection. 1 Introduction Sentiment analysis has attracted much attention in recent years because a large scale of subjective information is disseminated through various platforms on the web. Sentiment information can be applied to a wide variety of fields, including product recommendation, review summarization, public polling, and so on. Opinion dictionaries are important resources for identifying subjective information. Several approaches were proposed to collect such resources. Wiebe (2000) learned subjective adjectives from corpora. Takamura et al. (2005) extracted semantic orientations of words. Ku et al. (2007) measured sentiment degrees of Chinese words by averaging the sentiment scores of the composing characters. When the opinion words are available, the polarities of sentences and documents can be determined by them. Riloff and Wiebe (2003) learned the extraction patterns for subjective expressions. Kim and Hovy (2004) found the polarity of subjective expressions. Pang et al. (2002) and Dave et al. (2003) explored various techniques at document level. Morphological information has been widely used in classifying words, telling the meanin"
D09-1131,W02-1811,0,0.174401,"ds. Ku et al. (2007) measured sentiment degrees of Chinese words by averaging the sentiment scores of the composing characters. When the opinion words are available, the polarities of sentences and documents can be determined by them. Riloff and Wiebe (2003) learned the extraction patterns for subjective expressions. Kim and Hovy (2004) found the polarity of subjective expressions. Pang et al. (2002) and Dave et al. (2003) explored various techniques at document level. Morphological information has been widely used in classifying words, telling the meanings, and doing other in-depth analysis (Tzeng and Chen, 2002). However, morphological information was seldom applied either in Chinese opinion extraction, or in solving the coverage problem of opinion dictionary. Instead of bag-ofcharacters approach (Ku et al., 2007), this paper employs morphological structures of words to extract opinion words. Relations between sentence segments are also defined by linguistics in the Chinese language. These are similar to morphological structures between Chinese characters. Based on parsing trees of sentences, we identify these relations and utilize them for opinion analysis on sentences. As the experimental corpus, s"
D14-1156,H91-1057,0,0.152474,"Missing"
D14-1156,P07-1085,0,0.0519061,"Missing"
D14-1156,P11-5003,0,0.0325849,"aneous (or in-domain) corpus. Finally, the enhanced query model (that is P(w|H) in speech recognition) can be estimated by RM, SMM, RSMM or QMM, and further combined with the background n-gram (e.g., trigram) language model to form an adaptive language model to guide the speech recognition process. 4.2 Speech Summarization On the other hand, extractive speech summarization aims at producing a concise summary by selecting salient sentences or paragraphs from the original spoken document according to a predefined target summarization ratio (Carbonell and Goldstein, 1998; Mani and Maybury, 1999; Nenkova and McKeown, 2011; Liu and Hakkani-Tur, 2011). Intuitively, this task could be framed as an ad-hoc IR problem, where the spoken document is treated as an information need and each sentence of the document is regarded as a candidate information unit to be retrieved according to its relevance to the information need. Therefore, KLM can be used to quantify how close the document D and one of its sentences S are: the closer the sentence model P(w|S) to the document model P(w|D), the more PRM ( w |Q )   D r DTop P( w |Dr )  DrDTop P(Q |Dr ) P( Dr )   L    P ( w |D r ) P ( Dr |Q )    wV  Dr D"
D14-1156,W01-0100,0,\N,Missing
E14-4003,P08-1004,1,0.944569,"lves human intervention of handcrafted rules or tagged examples as the input for machine learning to recognize the assertion of a particular relationship between two entities in texts (Riloff, 1996; Soderland, 1999). Although machine learning helps enumerate potential relation patterns for extraction, this approach is often limited to extracting the relation sets that are predefined. In addition, traditional IE has focused on satisfying pre-specified requests from small homogeneous corpora, leaving the question open whether it can scale up to massive and heterogeneous corpora such as the Web (Banko and Etzioni, 2008; Etzioni et al., 2008, 2011). The relatively rich morpho-syntactic marking system of English (e.g., verbal inflection, nominal case, clausal markers) makes the syntactic roles of many words detectable from their surface forms. A tensed verb in English, for example, generally indicates its main verb status of a clause. The pinning down of the main verb in a Chinese clause, on the other hand, must rely on other linguistic cues such as word context due to the lack of tense markers. In contrast to the syntax-oriented English language, Chinese is discourse-oriented and rich in ellipsis – meaning i"
E14-4003,D11-1142,1,0.917387,"n2, Oren Etzioni3, Anthony Fader4 1 2 Information Technology Center, National Taiwan Normal University Dept. of Computer Science and Information Engineering, National Taiwan University 3 Allen Institute for Artificial Intelligence, Seattle, WA 4 Dept. of Computer Science and Engineering, University of Washington {samtseng, lhlee, sylin, skylock, meijun}@ntnu.edu.tw, hhchen@ntu.edu.tw, OrenE@allenai.org, afader@cs.washington.edu massive text corpora, where target relations are unknown in advance. Several Open IE systems, such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), ReVerb (Fader et al., 2011), and OLLIE (Mausam et al., 2012) achieve promising performance in open relation extraction on English sentences. However, application of these systems poses challenges to those languages that are very different from English, such as Chinese, as grammatical functions in English and Chinese are realized in markedly different ways. It is not sure whether those techniques for English still work for Chinese. This issue motivates us to extend the state-of-the-art Open IE systems to extract relations from Chinese texts. Abstract This study presents the Chinese Open Relation Extraction (CORE) system"
E14-4003,W00-1205,0,0.017299,"Head”-labeled words only, can be applied to strain out from each component of this triple the most prominent word: “民主黨 / 發佈 / 報告” (‘Democrats / released / report’). ‘ 燈 泡 /Na’ were annotated as two nominal phrases (i.e., ‘NP’), and ‘發明/VC 了/Di’ was annotated as a verbal phrase (i.e., ‘VP’). CKIP parser also adopts dependency decisionmaking and example-based approaches to label the semantic role “Head”, showing the status of a word or a phrase as the pivotal constituent of a sentence (You and Chen, 2004). CORE adopts the head-driven principle to identify the main relation in a given sentence (Huang et al., 2000). Firstly, a relation is defined by both the “Head”labeled verb and the other words in the syntactic chunk headed by the verb. Secondly, the noun phrases preceding/preceded by the relational chunk are regarded as the candidates of the head’s arguments. Finally, the entity-relation triple is identified in the form of (entity1, relation, entity2). Regarding the example sentence described above, the triple (愛迪生/Edison, 發明 Figure 1: The parsed tree of a Chinese sentence. 4 Experiments and Evaluation We adopted the same test set released by ReVerb for performance evaluation. The test set consists o"
E14-4003,W12-0702,0,0.0486469,"(‘Apples nutritious’), “ 蘋 果 是 營 養 豐 富 的 ” (‘Apples are nutritious’), and “蘋果富含營養” 12 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 12–16, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics (‘Apples are rich in nutrition’) are semantically synonymous sentences, but the first one, which lacks an overt verb, is used far more often than the other two. Presumably, an adequate multilingual IE system must take into account those intrinsic differences between languages. For multilingual open IE, Gamallo et al. (2012) adopts a rule-based dependency parser to extract relations represented in English, Spanish, Portuguese, and Galician. For each parsed sentence, they separate each verbal clause and then identify each one’s verb participants, including their functions: subject, direct object, attribute, and prepositional complements. A set of rules is then applied on the clause constituents to extract the target triples. For Chinese open IE, we adopt a similar general approach. The main differences are the processing steps specific to Chinese language. This paper introduces the Chinese Open Relation Extraction"
E14-4003,P10-1013,0,0.482828,", Mei-Jun Liu1, Hsin-Hsi Chen2, Oren Etzioni3, Anthony Fader4 1 2 Information Technology Center, National Taiwan Normal University Dept. of Computer Science and Information Engineering, National Taiwan University 3 Allen Institute for Artificial Intelligence, Seattle, WA 4 Dept. of Computer Science and Engineering, University of Washington {samtseng, lhlee, sylin, skylock, meijun}@ntnu.edu.tw, hhchen@ntu.edu.tw, OrenE@allenai.org, afader@cs.washington.edu massive text corpora, where target relations are unknown in advance. Several Open IE systems, such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), ReVerb (Fader et al., 2011), and OLLIE (Mausam et al., 2012) achieve promising performance in open relation extraction on English sentences. However, application of these systems poses challenges to those languages that are very different from English, such as Chinese, as grammatical functions in English and Chinese are realized in markedly different ways. It is not sure whether those techniques for English still work for Chinese. This issue motivates us to extend the state-of-the-art Open IE systems to extract relations from Chinese texts. Abstract This study presents the Chinese Open Relat"
E14-4003,W04-1116,0,0.0178753,"riple is chunked from its original sentence fully automatically. Finally, a filtering process, which retains “Head”-labeled words only, can be applied to strain out from each component of this triple the most prominent word: “民主黨 / 發佈 / 報告” (‘Democrats / released / report’). ‘ 燈 泡 /Na’ were annotated as two nominal phrases (i.e., ‘NP’), and ‘發明/VC 了/Di’ was annotated as a verbal phrase (i.e., ‘VP’). CKIP parser also adopts dependency decisionmaking and example-based approaches to label the semantic role “Head”, showing the status of a word or a phrase as the pivotal constituent of a sentence (You and Chen, 2004). CORE adopts the head-driven principle to identify the main relation in a given sentence (Huang et al., 2000). Firstly, a relation is defined by both the “Head”labeled verb and the other words in the syntactic chunk headed by the verb. Secondly, the noun phrases preceding/preceded by the relational chunk are regarded as the candidates of the head’s arguments. Finally, the entity-relation triple is identified in the form of (entity1, relation, entity2). Regarding the example sentence described above, the triple (愛迪生/Edison, 發明 Figure 1: The parsed tree of a Chinese sentence. 4 Experiments and"
E14-4003,W12-6338,0,0.0116822,"correctly once “ 了 ” is associated with its following identified as the verbal phrase that heads the sentence. This verbal phrase is regarded as the center of a potential relation. The two noun phrases before and after the verbal phrase, i.e., the NP “白宮 預算 委員會 的 民主黨” and NP character, instead of its precedent word. We adopt CKIP, the best-performing parser in the bakeoff of SIGHAN 2012 (Tseng et al., 2012), to do syntactic structure analysis. The CKIP solution re-estimates the contextdependent probability for Chinese parsing and improves the performance of probabilistic context-free grammar (Hsieh et al., 2012). For the example sentence above, ‘愛迪生/Nb’ and “報告” are regarded as the entities that complete the relation. A potential entity-relation-entity triple (i.e., 白宮預算委員會的民主黨 / 星期一 發佈 / 報告, ‘Democrats on the House Budget Committee / on Monday released / a report’) is extracted accordingly. This triple is chunked from its original sentence fully automatically. Finally, a filtering process, which retains “Head”-labeled words only, can be applied to strain out from each component of this triple the most prominent word: “民主黨 / 發佈 / 報告” (‘Democrats / released / report’). ‘ 燈 泡 /Na’ were annotated as two"
E14-4003,C02-1049,0,0.0327284,"source for their extractor. Experimental results indicated that parsed dependency features lead to further improvements over TextRunner. Chinese is generally written without word boundaries. As a result, prior to the implementation of most NLP tasks, texts must undergo automatic word segmentation. Automatic Chinese word segmenters are generally trained by an input lexicon and probability models. However, it usually suffers from the unknown word (i.e., the out-ofvocabulary, or OOV) problem. In CORE, a corpus-based learning method to merge the unknown words is adopted to tackle the OOV problem (Chen and Ma, 2002). This is followed by a reliable and cost-effective POS-tagging method to label the segmented words with partof-speeches (Tsai and Chen, 2004). Take the Chinese sentence “愛迪生發明了燈泡” (‘Edison ReVerb (Fader et al., 2011) introduced another approach by identifying first a verb-centered relational phrase that satisfies their pre-defined syntactic and lexical constraints, and then split the input sentence into an Argument-VerbArgument triple. This approach involves only POS tagging for English and “regular expression”-like matching. As such, it is suitable for large corpora, and likely to be applica"
E14-4003,O04-2005,0,0.0104221,"nese is generally written without word boundaries. As a result, prior to the implementation of most NLP tasks, texts must undergo automatic word segmentation. Automatic Chinese word segmenters are generally trained by an input lexicon and probability models. However, it usually suffers from the unknown word (i.e., the out-ofvocabulary, or OOV) problem. In CORE, a corpus-based learning method to merge the unknown words is adopted to tackle the OOV problem (Chen and Ma, 2002). This is followed by a reliable and cost-effective POS-tagging method to label the segmented words with partof-speeches (Tsai and Chen, 2004). Take the Chinese sentence “愛迪生發明了燈泡” (‘Edison ReVerb (Fader et al., 2011) introduced another approach by identifying first a verb-centered relational phrase that satisfies their pre-defined syntactic and lexical constraints, and then split the input sentence into an Argument-VerbArgument triple. This approach involves only POS tagging for English and “regular expression”-like matching. As such, it is suitable for large corpora, and likely to be applicable to Chinese. invented the light bulb’) for instance. It was segmented and tagged as follows: 愛迪生/Nb 發 明/VC 了/Di 燈泡/Na. Among these words, t"
E14-4003,W12-6335,1,0.892043,"ith other character, such as “了解” meaning “understand”. 會/Nc, 的/DE, 民主黨/Nb, 星期一/Nd, 發怖/VE, Therefore, “ 愛 迪 生 發 明 了 解 藥 ” (‘Edison 報 告 /Na. Next, “ 星 期 一 /Nd 發 佈 /VE” is invented a cure’) would be segmented incorrectly once “ 了 ” is associated with its following identified as the verbal phrase that heads the sentence. This verbal phrase is regarded as the center of a potential relation. The two noun phrases before and after the verbal phrase, i.e., the NP “白宮 預算 委員會 的 民主黨” and NP character, instead of its precedent word. We adopt CKIP, the best-performing parser in the bakeoff of SIGHAN 2012 (Tseng et al., 2012), to do syntactic structure analysis. The CKIP solution re-estimates the contextdependent probability for Chinese parsing and improves the performance of probabilistic context-free grammar (Hsieh et al., 2012). For the example sentence above, ‘愛迪生/Nb’ and “報告” are regarded as the entities that complete the relation. A potential entity-relation-entity triple (i.e., 白宮預算委員會的民主黨 / 星期一 發佈 / 報告, ‘Democrats on the House Budget Committee / on Monday released / a report’) is extracted accordingly. This triple is chunked from its original sentence fully automatically. Finally, a filtering process, whic"
huang-etal-2010-predicting,D09-1131,1,\N,Missing
huang-etal-2010-predicting,W02-1811,0,\N,Missing
huang-etal-2010-predicting,I08-4008,0,\N,Missing
huang-etal-2010-predicting,I05-3005,0,\N,Missing
huang-etal-2014-sentence,O98-3002,0,\N,Missing
huang-etal-2014-sentence,lin-chen-2006-constructing,1,\N,Missing
huang-etal-2014-sentence,nilsson-nivre-2008-malteval,0,\N,Missing
huang-etal-2014-sentence,D09-1060,0,\N,Missing
huang-etal-2014-sentence,W13-1101,0,\N,Missing
huang-etal-2014-sentence,N10-4004,0,\N,Missing
huang-etal-2014-sentence,C12-2088,0,\N,Missing
huang-etal-2014-sentence,C12-1034,1,\N,Missing
huang-etal-2014-sentence,sekine-2008-extended,0,\N,Missing
I05-1073,O04-1005,1,0.858064,"Missing"
I05-1073,sekine-nobata-2004-definition,0,0.0264981,"Missing"
I05-1073,C02-1012,0,0.0599764,"Missing"
I11-1039,P08-2037,0,0.0162281,"e action, but the polarity is the multiplication of the signs of opinion scores of oleft and oright. if ( S (oleft )  0 and S (oright )  0) then S (oleft oright )  S (oleft )  SIGN ( S (oleft ))  SIGN ( S (oright )) (3) else S (oleft oright )  S (oleft )  S (oright )  Verb-Complement Type: The scoring function for trios of this type is defined the same as that of a Subjective-Predicate type in Formula (2). The complement node is the deciding factor of the opinion score. 3.3.2 Using opinion dependency relations The usages of opinion dependency relations were seen in several researches (Bikel and Castelli, 2008). In these researches, rules for a small number of major dependency relations were proposed in different papers but they were not listed together for a better utilization. Some rules were not ever mentioned in pervious researches. Instead, all relations are analyzed in this paper. For each relation r of which gop(r) equals true (when gold opinion relations are used for opinion analysis) or op(r) equals true (when predicted opinion relations are used for opinion analysis), we calculate its opinion score ops(r). Let RM(w) be a function to return the dependency relations of word w’s modifiers one"
I11-1039,W09-2307,0,0.100632,"corresponding dependency trees generated by the same parser from the same sentence as the gold standard for training the automatic annotator of the opinion dependency relations. We conduct experiments on the annotated opinion syntactic structures in parsing trees, and on the opinion dependency relations corresponding to them. The proposed process demonstrates a feasible direction toward the development of an opinion dependency parser. 2 Problem Definition Given a set of non-collapsed dependencies parsed from a specific sentence by the Stanford dependency parser (de Marneffe and Manning, 2008; Chang et al., 2009), each associated with a dependency relation between two words in this sentence, our goal is to identify which of them are with sentiment, i.e., those which reveal a part of opinions or the aroused emotions. For example, in the sentence “活动 取得 了 圆满 成功 (Activities scored le perfect success)”, the Stanford dependency parser gives three relations: nmod(成功 &lt;success&gt;, 圆 满 &lt;perfect&gt;), nsubj( 取 得 &lt;scored&gt;, 活 动 &lt;activities&gt;), dobj(取得 &lt;scored&gt;, 成功&lt;success&gt;), and asp(取得 &lt;scored&gt;, 了&lt;le&gt;). The goal is to identify the former three may bear sentiment or opinions. The corresponding dependency tree is shown i"
I11-1039,P08-1019,0,0.0215935,"could find the opinion passages if they can understand the whole sentence, i.e. from parsing trees. However, when the linguistic background is needed, it could be difficult for most people to reconstruct the whole sentence from the dependency trees in order to find the opinion passage. Therefore, if we want to find annotators to build a corpus which could be used to train an opinion relation recognizer, parsing trees are the better materials compared to dependency trees. However, compared to relations between words, complicated tree structures are more challenge to be utilized by algorithms (Doan et al., 2008). 345 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 345–353, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP This paper focuses on extracting opinionated dependency relations from relations generated by the Stanford parser. We design an annotation mechanism on the syntactic structures on the sentence from Chinese Treebank to create an annotation environment with a lower entry barrier so that sufficient annotations can be labeled. Then these annotations are aligned to the relations in the corresponding dependency trees generated by the sam"
I11-1039,D09-1131,1,0.850598,"Missing"
I11-1039,W02-1011,0,0.0116262,"Missing"
I11-1039,J11-1002,0,0.190669,"edu.tw formation (Qiu et al., 2008). Their researches showed that linguistic knowledge is helpful in determining opinions. For various applications related to opinions, syntactic structures have become powerful tools for extracting useful clues. To find opinions in product reviews, modification relations were used to identify the product and their features (Lu et al., 2009), e.g., a good price (feature) of this camera (product). To find opinion holders and targets, templates and linguistic rules were adopted (Breck et al., 2007). To find more opinion words, dependency relations were utilized (Qiu et al., 2011). Even when applying the basic negation rule that flips opinion polarity over, we need to find its modified word first by syntactic clues. However, we will show that syntactic relations do not directly suggest opinions. Syntactic relations are obtained usually from all kinds of syntax trees. Parsing trees (phrase structured) and dependency trees (grammatical) are the most commonly seen ones. Parsing trees are in-order trees which keep the order of words in sentences, so they are more readable for people. Instead, nodes in dependency trees are displayed by the head-modifier relations, in which"
I11-1039,2007.sigdial-1.5,0,0.0649538,"Missing"
I11-1039,I08-2079,0,0.0220489,"ns. These results clearly indicate that the syntactic information benefit opinion analysis. Because of 351 the possible information loss in the automatic alignment process, that the performance of using trios is a little better than using dependency relations matches our expectation. Setting C+W+N C+W+N+goldTrio C+W+N+Trio C+W+N+goldDep C+W+N+Dep f-Score 0.7162 0.7922 0.7993 0.7784 0.7782 Table 7. Performance of using syntactic information for opinion analysis. 5 tic structures, and it caused the lack of analysis of opinion syntactic structures. Researchers have acquired syntactic structures (Zhou, 2008), but few of them have tried to associate syntactic structures with opinions. The most similar previous work to ours was proposed by Ku et al. (2009). Compared to it, the proposed process made the development of opinion dependency parser feasible. As dependency relations and the predicted opinion dependency relations are of the same form, no extra knowledge or integration is needed for the use of them. Related Work For all we know, no previous work has annotated opinion information on all dependency relations, or mapped annotated opinionated structures to dependency relations on a large quanti"
I11-1039,P07-2023,1,0.784744,"es of opinion sentences when bearing opinions (gop(r) equals true), i.e., the value in F column, is taken as the support value. The support value indicates that in what degree this relation bears opinions. If the support value is high, it is confident to say that the relation is opinionated; otherwise, considering the content words is necessary. This idea conforms to the previous observation in Section 4.2: some of the opinions are structural, but not all of them. According to the support value, dependency relations were divided into four categories. The Chinese opinion word dictionary NTUSD (Ku et al., 2007) is involved to help identify opinion dependency relations when the support value is not high. The selecting criteria are listed as follows.  Very supportive: with the support value above 0.8, e.g., dvpmod. Relations in this category are viewed as opinionated and their gop(r) are automatically set to true.  Supportive: with the support value above 0.35 but lower than 0.8, e.g., pass, dobj, npsubj, ba, top, nsubj, neg, amod, rcmod. RH(w) is a function to return the word w’s 350 head in other relations of the same sentences, and RM(w) returns w’s modifier. For each r  {rel , wh ,wm } in this"
I11-1170,W01-1605,0,0.194435,"the relationship between arguments is often difficult to decide and inherently subjective. Thus, the annotation and the evaluation are problematic and laborintensive. In recent years, the study of discourse relation recognition is growing in the English domain rapidly. One of the reasons is the availability of English corpora with discourse annotations. The two most popular discourse corpora are the RheHsin-Hsi Chen Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan hhchen@csie.ntu.edu.tw torical Structure Theory Discourse Treebank (RSTDT) (Carlson et al., 2001) and PDTB-2.0. Both of them are based on the Wall Street Journal corpus with human-annotated discourse information. The PDTB-2.0 consists of 36,592 pairs of successive arguments and is tagged with three classes, including Implicit, Explicit, and AltLex, and with the relation types at three levels. Based on these corpora, a number of aspects on discourse relation are explored in these years. Compared to the English corpora, there is still no Chinese discourse corpus worldwide available. For this reason, the dataset is the first challenge encountered in the study of Chinese discourse relation re"
I11-1170,D10-1039,0,0.0972035,"Missing"
I11-1170,D09-1036,0,0.140058,"Missing"
I11-1170,P02-1047,0,0.142817,"Missing"
I11-1170,P09-1077,0,0.262286,"Missing"
I11-1170,P09-2004,0,0.114215,"Missing"
I11-1170,prasad-etal-2008-penn,0,0.536791,"Missing"
I17-1098,D13-1160,0,0.111093,"Section 2 introduces related works of QA system. Section 3 presents our three-step paradigm. Section 4 shows the experiments on the SimpleQuestions dataset (Bordes et al., 2015) and compares ours with previous works. We also discuss the importance of each component of our system. Section 5 analyzes the errors in the experiments. Section 6 concludes the remarks. 2 Related Works Previous simple QA models can be divided into two categories. The first one is based on semantic parsing, which maps a question to its logical form. Then, the logical form can be transformed to SPARQL for KB retrieval. Berant et al. (2013) present a semantic parser that does not need to be trained through the annotated logical form. They construct a lexicon that maps natural language phrases to KB relations by aligning large text corpus with Freebase. Candidate logical forms can be obtained by this lexicon and the other bridging operations. Berant and Liang (2014) propose a semantic parser via paraphrasing. They use the intermediate question to deal with the problem of the mismatch between input question and its logical forms. Yih et al. (2015) treat a question as a query graph, which can be directly mapped to its logical form."
I17-1098,P14-1133,0,0.019421,"ts. Section 6 concludes the remarks. 2 Related Works Previous simple QA models can be divided into two categories. The first one is based on semantic parsing, which maps a question to its logical form. Then, the logical form can be transformed to SPARQL for KB retrieval. Berant et al. (2013) present a semantic parser that does not need to be trained through the annotated logical form. They construct a lexicon that maps natural language phrases to KB relations by aligning large text corpus with Freebase. Candidate logical forms can be obtained by this lexicon and the other bridging operations. Berant and Liang (2014) propose a semantic parser via paraphrasing. They use the intermediate question to deal with the problem of the mismatch between input question and its logical forms. Yih et al. (2015) treat a question as a query graph, which can be directly mapped to its logical form. Semantic parsing is then equivalent to finding a sub-graph of the KB which can represent the question. The semantic parsing approach which often requires the human annotated logical form may increase the cost of obtaining training data. Although the use of rule-based method to generate logical forms can reduce the use of annotat"
I17-1098,P16-1076,0,0.633441,"which can access KB to get the answer. 976 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 976–985, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Previous simple QA model often adopts a twostep paradigm. Entity-linking step identifies the subject entities in questions, and forms the candidate entity set and relation set. The candidate relation set is formed by all the relations which have connections with any entity in candidate entity set. Relation-finding step further identifies a proper relation from the candidate relation set. Dai et al. (2016) propose a neural-network based two-step approach to simple QA over Freebase, and formulate the task into a probabilistic form. Given a question q, the first step is to find the candidate relation r with high probability ?(?|?). The second step is to find the subject s with high ?(?|?, ?). As a result, the object in the KB triple which contains the subject s and the relation r with the highest ?(?, ?|?) is the answer. The relation in KB triple has hierarchical structure: domain-type-property. For example, in “people.person.place_of_birth”, “place_of_birth” is the property used to present the b"
I17-1098,P15-1026,0,0.0760089,"the QA problem into a probabilistic form. Given a question, the answer is the triple in KB containing the subject and relation with the highest conditional probability. They first use a focused pruning method to tag the span in question which is most probable to be the subject entity and gets the candidate answer triples. Then they use a relation network and a subject network, both are a two-layer bidirectional-GRU model, to get the similarity scores between candidate triples and question. We modify their probabilistic form with the splitting of the relation part ? into type ? and property ?. Dong et al. (2015) introduce a multi-column CNNs (MCCNNs) to analyze the question in three different aspects: answer path, answer context and answer type. The system represents the question in three low-dimensional vectors, each of them then matches to one of the answer aspect to derive the scores of candidates. Yin et al. (2016) also use the CNN-based approach to implement the QA system. They use a word-level CNN with attentive max-pooling to model the relationship between KB relations and question pattern. They also add an active linker, which is similar to the focused pruning method in Dai et al. (2016), to"
I17-1098,C16-1164,0,0.678121,"triples. Then they use a relation network and a subject network, both are a two-layer bidirectional-GRU model, to get the similarity scores between candidate triples and question. We modify their probabilistic form with the splitting of the relation part ? into type ? and property ?. Dong et al. (2015) introduce a multi-column CNNs (MCCNNs) to analyze the question in three different aspects: answer path, answer context and answer type. The system represents the question in three low-dimensional vectors, each of them then matches to one of the answer aspect to derive the scores of candidates. Yin et al. (2016) also use the CNN-based approach to implement the QA system. They use a word-level CNN with attentive max-pooling to model the relationship between KB relations and question pattern. They also add an active linker, which is similar to the focused pruning method in Dai et al. (2016), to reduce the number of candidates and improve the performance significantly. 3 Figure 1: System Overview. the relations “wine.wine.color” and “roses.roses.color” are used to describe the color of things, but they are in type “wine” and “roses”, respectively. We separate a relation into type and property parts to u"
I17-1098,P13-1158,0,0.120291,"e “(Harry Potter, author, J.K. Rowling)”. Although this category is “simple” question, retrieving a triple from the KB is not a trivial task due to the billions of facts in the KB. Complex questions contain more restrictions. These questions may involve two or more triples in the KB, or have other semantic constraints to restrict the answers to a smaller set. For example, “the first” in the question “What is the name of the first Harry Potter novel?” restricts that there is only one answer. Previous researches showed that simple questions are the more common category in community QA websites (Fader et al., 2013). This paper focuses on factoid simple question-answering over Freebase. Simple question can be answered with the object of one KB triple. Thus, the systems only need to find the subject and relation of the triple which can describe the question properly. The issues of simple QA are the identification of the subject entity in a question, and the resolution of the gap between the natural language expression in the question and the relation description in the KB. After a QA system receives users’ questions, it needs to transform a question into a KB query, e.g. SPARQL. The question can then be t"
I17-1098,D16-1166,0,0.0123487,"with the pre-trained GloVe (Pennington et al., 2014) with the dimension of 300. All the networks are optimized by mini-batch and Adam (Kingma et al., 2014) with the learning rate 0.001. The TYPE_LSTM in entity identification step has a drop rate of 0.2. The hidden size of LSTM is 2 Overall Results Table 1 shows the performances of our model compared with the other four methods. Bordes et al. (2015) use a memory network. Dai et al. (2016) employ a conditional focused neural-network based approach. Yin et al. (2016) apply attentive convolutional neural network with the passive or active linker. Golub and He (2016) use the character-level encoder-decoder framework. In our approach, “probability” means the outcome is the triple with the highest probability computed by Equation (1). We find that the subject entity and the property are more important than the type. The approach “sum” combines the scores ?1 (?, ?) , ?2 (?, ?) , and ?3 (?, ?) by weighted summation, and selects the triple with the highest weighted sum. The weights are entity:type:property = 4:1:3, which are tuned on the validation data. Our “probability” approach outperforms all previous models on FB5M, including the previous best model by Yi"
I17-1098,N16-1030,0,0.0385724,"verbs exist, this feature is set to 0 Out-degree: The number of out-going links of the entity in the KB is taken as Out-degree feature. The more the number of links the entity has, the higher the feature value is and the entity in the KB is more informative. The direction of links from subject to object represents the impact of the entity. IDF: We take each question in SimpleQuesions training set as a document, and compute the inverse document frequency of the entity. The higher the value is, the more specific the entity is. NER_LCS: We use the LSTM-CRF named entity recognition (NER) tagger1 (Lample et al., 2016) to find the span from the question that is most 1 Figure 2: Structure of Type_LSTM in entity identification. likely to be a subject entity, and compute the length of the longest common subsequence (in characters) between the candidate entity and the words in the tagged span. The tagger consists of an embedding layer, a bidirectional-LSTM layer, and a conditional random field layer to predict the label for each word. The higher the NER_LCS value is, the more possible the candidate is a subject entity. Type_LSTM: Each entity has entity types in Freebase to describe its characteristics, e.g., en"
I17-1098,D14-1162,0,0.080656,"7 test data, is adopted in the experiments. The evaluation is the same as in Bordes et al. (2015). The predicted answer is correct when the subject-relation pair is the same as the correct answer. We train our model on the training set. The validation set is used for early stop and parameter tuning. The test set is used for evaluation. 4.1 Experimental Setup The number of negative samples used in SVMrank is set to 5. Other parameters for SVMrank are C = 0.1, epsilon = 0.01, and loss function option = 2. The word embeddings used in each neural network is initialized with the pre-trained GloVe (Pennington et al., 2014) with the dimension of 300. All the networks are optimized by mini-batch and Adam (Kingma et al., 2014) with the learning rate 0.001. The TYPE_LSTM in entity identification step has a drop rate of 0.2. The hidden size of LSTM is 2 Overall Results Table 1 shows the performances of our model compared with the other four methods. Bordes et al. (2015) use a memory network. Dai et al. (2016) employ a conditional focused neural-network based approach. Yin et al. (2016) apply attentive convolutional neural network with the passive or active linker. Golub and He (2016) use the character-level encoder-"
I17-1098,P15-1128,0,0.0709812,"gical form. Then, the logical form can be transformed to SPARQL for KB retrieval. Berant et al. (2013) present a semantic parser that does not need to be trained through the annotated logical form. They construct a lexicon that maps natural language phrases to KB relations by aligning large text corpus with Freebase. Candidate logical forms can be obtained by this lexicon and the other bridging operations. Berant and Liang (2014) propose a semantic parser via paraphrasing. They use the intermediate question to deal with the problem of the mismatch between input question and its logical forms. Yih et al. (2015) treat a question as a query graph, which can be directly mapped to its logical form. Semantic parsing is then equivalent to finding a sub-graph of the KB which can represent the question. The semantic parsing approach which often requires the human annotated logical form may increase the cost of obtaining training data. Although the use of rule-based method to generate logical forms can reduce the use of annotated data, it limits the application domain. The second approach is information extraction, which needs only questionanswer pairs for training. This method retrieves some candidate answe"
ku-etal-2006-tagging,W03-1014,0,\N,Missing
ku-etal-2006-tagging,C04-1200,0,\N,Missing
ku-etal-2006-tagging,W02-1011,0,\N,Missing
ku-etal-2006-tagging,P05-1017,0,\N,Missing
ku-etal-2010-construction,D09-1131,1,\N,Missing
ku-etal-2010-construction,W03-0404,0,\N,Missing
ku-etal-2010-construction,P07-2023,1,\N,Missing
L16-1033,W09-2307,0,0.0206159,"the frequency of the word sequence in the Google Web 5-gram corpus. We combine the sum of n-gram probabilities with segment length (s_len). All n-gram features are concatenated into a feature vector G = (g2, g3, g4, g5), where g n = ∑Li=n p(wi |wi−n+1 , … , wi−1 ) (2) 3.3. Dependency count feature Errors in a sentence affect the result of segmentation and parsing. We postulate that there is a certain distribution of dependency counts in normal sentences, and the counts of error sentences deviate from the distribution. Therefore, we take the count of each type of dependency of Stanford Parser (Chang et al., 2009) output as a set of features. For each dependency, there are two types of “count”: (1) internal count, which counts the occurrence if the two words are both in the target segment, and (2) external count, which counts as long as one of the words is in the target segment. There are 45 types of dependency in our dataset, and we also include total internal and external counts. The result feature vector D has 92 dimensions. We also combine them with segment length (s_len). U Error (2) & (4) 13,314 3.4. Dependency bigram feature Long distance dependency is common in Chinese sentences. In the example"
L16-1033,W12-2006,0,0.156534,"Missing"
L16-1033,W11-2838,0,0.166389,"ates with other words in the given sentence and the former does not. CC (1) and (3) are similar in that the misused forms are 1 2 2. Related Work Leacock et al. (2014) give a comprehensive study of grammatical error correction (GEC). They pointed out the errors made by non-native language learners are quite different from those by native language learners. Training data should come from non-native language learners to capture the phenomena of grammatical errors. To measure the performance of GEC systems, several shared tasks have been organized in recent years for English, including HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). Different types of grammatical errors were investigated. Language models, machine learning-based classifiers, rule-based classifiers, and machine translation models have been explored. In Chinese, spelling check evaluations were held at SIGHAN 2013 Bake-off (Wu et al., 2013) and SIGHAN 2014 Bake-off (Yu et al., 2014). Yu, Lee and Chang (2014) extended the evaluation to Chinese grammatical error diagnosis. Four kinds of grammatical errors, i.e., redundant word, missing word, word disorder, and word ht"
L16-1033,W14-1701,0,0.0619828,"Missing"
L16-1033,W13-4406,0,0.0594966,"come from non-native language learners to capture the phenomena of grammatical errors. To measure the performance of GEC systems, several shared tasks have been organized in recent years for English, including HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). Different types of grammatical errors were investigated. Language models, machine learning-based classifiers, rule-based classifiers, and machine translation models have been explored. In Chinese, spelling check evaluations were held at SIGHAN 2013 Bake-off (Wu et al., 2013) and SIGHAN 2014 Bake-off (Yu et al., 2014). Yu, Lee and Chang (2014) extended the evaluation to Chinese grammatical error diagnosis. Four kinds of grammatical errors, i.e., redundant word, missing word, word disorder, and word http://202.112.195.192:8060/hsk/tongji2.asp http://202.112.195.192:8060/hsk/help2.asp 220 selection, were defined. Yu and Chen (2012) adopted the HSK corpus to study word ordering errors (WOEs) in Chinese, and proposed syntactic features, web corpus features and perturbation features for WOE detection. Cheng, Yu and Chen (2014) identified sentence segments containing WO"
L16-1033,C12-1184,1,0.569123,"grammatical errors were investigated. Language models, machine learning-based classifiers, rule-based classifiers, and machine translation models have been explored. In Chinese, spelling check evaluations were held at SIGHAN 2013 Bake-off (Wu et al., 2013) and SIGHAN 2014 Bake-off (Yu et al., 2014). Yu, Lee and Chang (2014) extended the evaluation to Chinese grammatical error diagnosis. Four kinds of grammatical errors, i.e., redundant word, missing word, word disorder, and word http://202.112.195.192:8060/hsk/tongji2.asp http://202.112.195.192:8060/hsk/help2.asp 220 selection, were defined. Yu and Chen (2012) adopted the HSK corpus to study word ordering errors (WOEs) in Chinese, and proposed syntactic features, web corpus features and perturbation features for WOE detection. Cheng, Yu and Chen (2014) identified sentence segments containing WOEs, and further recommended the candidates with correct word orderings by using ranking SVM. Different from the above researches, this paper focuses on Chinese word usage error detection. WUE appears at lexical level rather than character level in spelling checking. Moreover, this task is also different from Chinese diagnosis task defined in Yu, Lee and Chang"
L16-1033,W14-6820,1,0.878277,"apture the phenomena of grammatical errors. To measure the performance of GEC systems, several shared tasks have been organized in recent years for English, including HOO 2011 (Dale and Kilgarriff, 2011), HOO 2012 (Dale et al., 2012), CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 (Ng et al., 2014). Different types of grammatical errors were investigated. Language models, machine learning-based classifiers, rule-based classifiers, and machine translation models have been explored. In Chinese, spelling check evaluations were held at SIGHAN 2013 Bake-off (Wu et al., 2013) and SIGHAN 2014 Bake-off (Yu et al., 2014). Yu, Lee and Chang (2014) extended the evaluation to Chinese grammatical error diagnosis. Four kinds of grammatical errors, i.e., redundant word, missing word, word disorder, and word http://202.112.195.192:8060/hsk/tongji2.asp http://202.112.195.192:8060/hsk/help2.asp 220 selection, were defined. Yu and Chen (2012) adopted the HSK corpus to study word ordering errors (WOEs) in Chinese, and proposed syntactic features, web corpus features and perturbation features for WOE detection. Cheng, Yu and Chen (2014) identified sentence segments containing WOEs, and further recommended the candidates"
L16-1033,C14-1028,1,\N,Missing
L16-1164,D10-1039,0,0.020122,"bank (RST-DT) (Carlson et al., 2002) and the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) are two important resources to facilitate the researches of discourse relation recognition. Because most Chinese discourse corpora follow the PDTB scheme, this section surveys the related work from this direction. Pitler and Nenkova (2009) deal with two types of ambiguity of discourse connectives in English: non-discourse vs. discourse usage and unique vs. more discourse functions. They achieve 0.9419 F-score for the first issue and 0.9415 accuracy for the 4-way sense classification of explicits. Hernault et al. (2010, 2011) improve classifi-cation accuracy for the infrequent English discourse relation types. Because the 4-way sense classification accuracy for explicits using just the connective is very high, i.e., 0.9367 (Pitler and Nenkova, 2009), the subsequent works focus on resolution of implicits. Different features are explored. Pilter et al. (2009) use polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. Lin et al. (2009) employ the context of two arguments, word pair information, arguments’ internal constituent and dependency parses. Louis et al. (201"
L16-1164,I11-1170,1,0.82219,"composed of 500 news documents. Xue (2005), Zhou and Xue (2012), Li, Feng, et al. (2014) describe the related issues in constructing a Chinese discourse corpus Chinese connectives are useful, but ambiguous. Zhou et al. (2012) and Li, Carpuat, et al. (2014) use cross-lingual information to disambiguate Chinese discourse connectives. Huang et al. (2014) propose a semi-supervised method to learn probability distribution of discourse functions of connectives, and apply the result to enhance relation labelling. Most previous Chinese discourse relation labelling was done with coarse-grained senses. Huang and Chen (2011) report F-score of 0.6288 on the 4-way inter-sentential relation classification. Zhou et al. (2012) report the F-score of 0.7481 on 4-way classification at the intra-sentential level. Li, Carpuat et al. (2014) show F-scores for binary classification for Temporal, Contingency, Comparison and Expansion are 0.4865, 0.4194, 0.5970 and 0.6920, respectively. 3. HIT-CDTB In HIT-CDTB 1.0 (http://ir.hit.edu.cn/hit-cdtb/), discourse relations at intra-sentential, inter-sentential, and passage levels are annotated. It follows the PDTB relation scheme with some modification. The top level consists of 4 di"
L16-1164,C14-1060,1,0.799083,"ation on 11 relation types is 0.40. Recently, five Chinese discourse corpora (Huang et al., 2013; Zhang et al., 2014; Zhou, Li, et al., 2014; Zhou, Lu, et al., 2014; Li, Feng, et al., 2014) based on PDTB-like 1034 senses have been built. In the CUHK Discourse TreeBank (Zhou, Li, et al., 2014), explicit discourse connectives, their corresponding arguments and senses are annotated. In the HIT-CIR Chinese Discourse Relation Treebank 1.0 (Zhang et al., 2013), coarse-grained and fine-grained relations are labelled at intra- and inter-sentential levels. In NTU discourse corpora (Huang et al., 2013; Huang et al., 2014), 7,601 and 300,000 sentences selected from Chinese part of Clue-Web09 dataset (Yu et al., 2012) are annotated manually and automatically at the intra-sentential level. In the Chinese Discourse Treebank 0.5 released by LDC (Zhou, Lu, et al., 2014), there are approximately 5,500 annotation instances. CDTB 1.0 (Li, Feng, et al., 2014) is composed of 500 news documents. Xue (2005), Zhou and Xue (2012), Li, Feng, et al. (2014) describe the related issues in constructing a Chinese discourse corpus Chinese connectives are useful, but ambiguous. Zhou et al. (2012) and Li, Carpuat, et al. (2014) use c"
L16-1164,W13-2309,1,0.824976,"the above works are done for the four top-level classes in PDTB except the paper (Lin et al., 2009). One-vs-the-rest strategy is usually adopted to evaluate the classification performance because the numbers of instances in the classes are unbalanced. Park and Cardie (2012) report the best F-scores for Temporal vs. Rest, Contingency vs. Rest, Comparison vs. Rest, and Expansion vs. Rest are 0.2657, 0.4982, 0.3132, and 0.7922, respectively. Lin et al. (2009) report the micro-averaged F-score of a fine-grained classification on 11 relation types is 0.40. Recently, five Chinese discourse corpora (Huang et al., 2013; Zhang et al., 2014; Zhou, Li, et al., 2014; Zhou, Lu, et al., 2014; Li, Feng, et al., 2014) based on PDTB-like 1034 senses have been built. In the CUHK Discourse TreeBank (Zhou, Li, et al., 2014), explicit discourse connectives, their corresponding arguments and senses are annotated. In the HIT-CIR Chinese Discourse Relation Treebank 1.0 (Zhang et al., 2013), coarse-grained and fine-grained relations are labelled at intra- and inter-sentential levels. In NTU discourse corpora (Huang et al., 2013; Huang et al., 2014), 7,601 and 300,000 sentences selected from Chinese part of Clue-Web09 datase"
L16-1164,C14-1055,0,0.193887,"Missing"
L16-1164,D09-1036,0,0.025327,"functions. They achieve 0.9419 F-score for the first issue and 0.9415 accuracy for the 4-way sense classification of explicits. Hernault et al. (2010, 2011) improve classifi-cation accuracy for the infrequent English discourse relation types. Because the 4-way sense classification accuracy for explicits using just the connective is very high, i.e., 0.9367 (Pitler and Nenkova, 2009), the subsequent works focus on resolution of implicits. Different features are explored. Pilter et al. (2009) use polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. Lin et al. (2009) employ the context of two arguments, word pair information, arguments’ internal constituent and dependency parses. Louis et al. (2010) predict implicit discourse relations between adjacent sentences using entity features. Zhou et al. (2010) predict the implicit discourse connective between two augments with language model. Park and Cardie (2012) optimize feature combinations of the proposed features. Lin et al. (2014) develop an end-to-end discourse parser based on PDTB. Most of the above works are done for the four top-level classes in PDTB except the paper (Lin et al., 2009). One-vs-the-res"
L16-1164,W10-4310,0,0.0172922,"arlson et al., 2002) and the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) are two important resources to facilitate the researches of discourse relation recognition. Because most Chinese discourse corpora follow the PDTB scheme, this section surveys the related work from this direction. Pitler and Nenkova (2009) deal with two types of ambiguity of discourse connectives in English: non-discourse vs. discourse usage and unique vs. more discourse functions. They achieve 0.9419 F-score for the first issue and 0.9415 accuracy for the 4-way sense classification of explicits. Hernault et al. (2010, 2011) improve classifi-cation accuracy for the infrequent English discourse relation types. Because the 4-way sense classification accuracy for explicits using just the connective is very high, i.e., 0.9367 (Pitler and Nenkova, 2009), the subsequent works focus on resolution of implicits. Different features are explored. Pilter et al. (2009) use polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. Lin et al. (2009) employ the context of two arguments, word pair information, arguments’ internal constituent and dependency parses. Louis et al. (2010) pred"
L16-1164,W12-1614,0,0.0172378,"h, i.e., 0.9367 (Pitler and Nenkova, 2009), the subsequent works focus on resolution of implicits. Different features are explored. Pilter et al. (2009) use polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. Lin et al. (2009) employ the context of two arguments, word pair information, arguments’ internal constituent and dependency parses. Louis et al. (2010) predict implicit discourse relations between adjacent sentences using entity features. Zhou et al. (2010) predict the implicit discourse connective between two augments with language model. Park and Cardie (2012) optimize feature combinations of the proposed features. Lin et al. (2014) develop an end-to-end discourse parser based on PDTB. Most of the above works are done for the four top-level classes in PDTB except the paper (Lin et al., 2009). One-vs-the-rest strategy is usually adopted to evaluate the classification performance because the numbers of instances in the classes are unbalanced. Park and Cardie (2012) report the best F-scores for Temporal vs. Rest, Contingency vs. Rest, Comparison vs. Rest, and Expansion vs. Rest are 0.2657, 0.4982, 0.3132, and 0.7922, respectively. Lin et al. (2009) re"
L16-1164,P09-1077,0,0.0760211,"Missing"
L16-1164,prasad-etal-2008-penn,0,0.658426,"d to evaluate the proposed models. The 25-way classifier achieves 0.57 micro-averaged F-score. Keywords: Discourse analysis; Discourse relation labelling; HIT Chinese discourse relation treebank. 1. Introduction Discourse relation labelling aims at predicting the most proper discourse relation between two discourse units such as clauses, sentences, and groups of sentences. The labelling task can be done at the intra-sentential and the inter-sentential levels depending on the analysis units. Several schemes have been proposed to define discourse relations to be analyzed. In the PDTB framework (Prasad et al., 2008), three levels of sense hierarchy are utilized. The four classes on the top level are Temporal, Contingency, Comparison, and Expansion. In this paper, we present Chinese discourse analysis on fine-grained discourse relations at intra-sentential level. HIT Chinese discourse relation treebank (Zhang et al., 2014) is used to investigate some specific issues. A discourse marker in Chinese may belong to various lexical categories including conjunction, adverb, noun, preposition, and verb. It may be a single word such as “但” (but) or a word pair such as “雖然...但” (although … but). A discourse marker"
L16-1164,W05-0312,0,0.0266318,"HIT-CIR Chinese Discourse Relation Treebank 1.0 (Zhang et al., 2013), coarse-grained and fine-grained relations are labelled at intra- and inter-sentential levels. In NTU discourse corpora (Huang et al., 2013; Huang et al., 2014), 7,601 and 300,000 sentences selected from Chinese part of Clue-Web09 dataset (Yu et al., 2012) are annotated manually and automatically at the intra-sentential level. In the Chinese Discourse Treebank 0.5 released by LDC (Zhou, Lu, et al., 2014), there are approximately 5,500 annotation instances. CDTB 1.0 (Li, Feng, et al., 2014) is composed of 500 news documents. Xue (2005), Zhou and Xue (2012), Li, Feng, et al. (2014) describe the related issues in constructing a Chinese discourse corpus Chinese connectives are useful, but ambiguous. Zhou et al. (2012) and Li, Carpuat, et al. (2014) use cross-lingual information to disambiguate Chinese discourse connectives. Huang et al. (2014) propose a semi-supervised method to learn probability distribution of discourse functions of connectives, and apply the result to enhance relation labelling. Most previous Chinese discourse relation labelling was done with coarse-grained senses. Huang and Chen (2011) report F-score of 0."
L16-1164,yu-etal-2012-development,1,0.584019,"hang et al., 2014; Zhou, Li, et al., 2014; Zhou, Lu, et al., 2014; Li, Feng, et al., 2014) based on PDTB-like 1034 senses have been built. In the CUHK Discourse TreeBank (Zhou, Li, et al., 2014), explicit discourse connectives, their corresponding arguments and senses are annotated. In the HIT-CIR Chinese Discourse Relation Treebank 1.0 (Zhang et al., 2013), coarse-grained and fine-grained relations are labelled at intra- and inter-sentential levels. In NTU discourse corpora (Huang et al., 2013; Huang et al., 2014), 7,601 and 300,000 sentences selected from Chinese part of Clue-Web09 dataset (Yu et al., 2012) are annotated manually and automatically at the intra-sentential level. In the Chinese Discourse Treebank 0.5 released by LDC (Zhou, Lu, et al., 2014), there are approximately 5,500 annotation instances. CDTB 1.0 (Li, Feng, et al., 2014) is composed of 500 news documents. Xue (2005), Zhou and Xue (2012), Li, Feng, et al. (2014) describe the related issues in constructing a Chinese discourse corpus Chinese connectives are useful, but ambiguous. Zhou et al. (2012) and Li, Carpuat, et al. (2014) use cross-lingual information to disambiguate Chinese discourse connectives. Huang et al. (2014) prop"
L16-1164,zhou-etal-2014-cuhk,0,0.0315231,"Missing"
L16-1164,C12-2138,0,0.243937,"Missing"
L16-1164,C10-2172,0,0.0216632,"es. Because the 4-way sense classification accuracy for explicits using just the connective is very high, i.e., 0.9367 (Pitler and Nenkova, 2009), the subsequent works focus on resolution of implicits. Different features are explored. Pilter et al. (2009) use polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features. Lin et al. (2009) employ the context of two arguments, word pair information, arguments’ internal constituent and dependency parses. Louis et al. (2010) predict implicit discourse relations between adjacent sentences using entity features. Zhou et al. (2010) predict the implicit discourse connective between two augments with language model. Park and Cardie (2012) optimize feature combinations of the proposed features. Lin et al. (2014) develop an end-to-end discourse parser based on PDTB. Most of the above works are done for the four top-level classes in PDTB except the paper (Lin et al., 2009). One-vs-the-rest strategy is usually adopted to evaluate the classification performance because the numbers of instances in the classes are unbalanced. Park and Cardie (2012) report the best F-scores for Temporal vs. Rest, Contingency vs. Rest, Comparison"
L16-1164,P12-1008,0,0.165896,"nese Discourse Relation Treebank 1.0 (Zhang et al., 2013), coarse-grained and fine-grained relations are labelled at intra- and inter-sentential levels. In NTU discourse corpora (Huang et al., 2013; Huang et al., 2014), 7,601 and 300,000 sentences selected from Chinese part of Clue-Web09 dataset (Yu et al., 2012) are annotated manually and automatically at the intra-sentential level. In the Chinese Discourse Treebank 0.5 released by LDC (Zhou, Lu, et al., 2014), there are approximately 5,500 annotation instances. CDTB 1.0 (Li, Feng, et al., 2014) is composed of 500 news documents. Xue (2005), Zhou and Xue (2012), Li, Feng, et al. (2014) describe the related issues in constructing a Chinese discourse corpus Chinese connectives are useful, but ambiguous. Zhou et al. (2012) and Li, Carpuat, et al. (2014) use cross-lingual information to disambiguate Chinese discourse connectives. Huang et al. (2014) propose a semi-supervised method to learn probability distribution of discourse functions of connectives, and apply the result to enhance relation labelling. Most previous Chinese discourse relation labelling was done with coarse-grained senses. Huang and Chen (2011) report F-score of 0.6288 on the 4-way int"
L18-1139,fillmore-etal-2002-framenet,0,0.0612943,"Missing"
L18-1139,P14-1136,0,0.0356,"Missing"
L18-1139,C16-2037,0,0.278861,"LUs, 1,221 semantic frames, and 200,000 annotated sentences are provided, Chinese FrameNet is considerably smaller. In other words, Chinese FrameNet has a much low coverage of frames and lexical units, and results in limited applications. Efforts have been made to construct FrameNet resources in other languages. Most of which construct their resources with human annotation one by one laboriously, such as Japanese FrameNet (Ohara et al., 2003). Park et al. (2014) conduct the construction of Korean FrameNet by hiring trained translators to import 4,025 sentences selected from English FrameNet. Kim et al. (2016) further import additional 1,795 sentences to Korean FrameNet from Japanese FrameNet based on the similarities between these two languages. However, the high construction cost of such resources sometimes hinders it from growing to the proper scale that is applicable for real NLP tasks. Tonelli et al. (2008) propose an algorithm that projects English frames onto Italian ones, so that FrameNet in Italian could be constructed more easily. In this work, we propose a novel approach to automate FrameNet construction. Based on a large-scale bilingual corpus, we transfer the machine-annotated FEs from"
L18-1139,kingsbury-palmer-2002-treebank,0,0.364983,"Missing"
L18-1139,P15-1122,0,0.0606553,"Missing"
L18-1139,J03-1002,0,0.00770966,"osed to augment LUs based on the seeds. 3.1 3.2 Projection of Bilingual Frame Elements Word alignment tools align words in a sentence in the source language to the corresponding words in the sentence in the target language. In our case, we regard English as the source language and Chinese as the target language since we will utilize the alignment information as our basis of finding the projection of semantic relations from English to Chinese between bilingual sentences. In this paper, we employ TsinghuaAligner (Liu and Sun, 2015), which takes the translation probabilities derived from GIZA++ (Och and Ney, 2003) as the central feature in the word alignment process. The alignment procedure consists of three major steps. Figures 2, 3, and 4 show examples for the tasks respectively. First, we label 2.2M English sentences in the bilingual UMCorpus with frame semantics using SEMAFOR. As shown in Figure 2, SEMAFOR generates 14M predicate-argument structures from these 2.2M sentences. Figure 3 shows the second step, where TsinghuaAligner is performed to derive English-Chinese word pairs as our basis for the step. Finally, as illustrated in Figure 4, we utilize both the parsed result and the alignment inform"
L18-1139,tian-etal-2014-um,0,0.0185606,"for other languages. In the rest of this paper, we first introduce the linguistic resources that support our construction in Section 2. Section 3 presents our methodology and discusses the filtering strategies that are used for quality improvement. Section 4 demonstrates an application of our resource that would improve the annotation process in terms of annotation time consumption. Finally, we conclude our contributions and discuss future work in Section 5. 868 Figure 1: Overview of our approach to Chinese FrameNet construction. 2. Resources We construct Chinese FrameNet based on UM-Corpus (Tian et al., 2014), which is a large-scale, balanced EnglishChinese corpus consisting of 2.2 million parallel sentences from eight genres in a reasonable proportion, including News, Spoken, Laws, Thesis, Education, Science, Subtitle, and Microblog. They are parsed and extracted from online journals (national and international), official websites, online language learning resources (e.g. online dictionary and translation portals), TED, and Microblogs. Tian et al. (2014) apply some well-designed algorithms and tools to speed up the building process, such as document alignment, sentence boundary detection, and sen"
L18-1139,tonelli-pianta-2008-frame,0,0.0203702,"Missing"
L18-1541,D12-1104,0,0.104317,"not be mapped by string matching directly. In addition, a KB predicate may be described in multiple NL statements. A number of ReVerb patterns such as “is the hometown of”, “was raised in”, and “grew up in” are related to the predicate “hometown” in DBpedia. That makes the mapping between KB and NL even more challenging. Recently, more and more works show their interests in the issue of KB construction. Knowledge graph embedding models (Bordes et al., 2013; Yang et al., 2015; Xie et al., 2016a; Xie et al., 2016b) focus on learning the vector representation on the KB side only. Previous works (Nakashole et al., 2012; Riedel et al., 2013; Dutta et al., 2015) aim to solve similar problems as ours. Nakashole et al. (2012) in their PATTY approach try to learn paraphrases to define the predicates of DBpedia, and Dutta et al. (2015) propose clustering-based approaches to transform the knowledge extracted by an open information extraction system into DBpedia paradigm. Riedel et al. (2013) propose universal schemas, which are the mapping between NL surface forms to the KB predicates, by using matrix factorization. However, all of them suffer from low coverage on relational phrases. In this work, we aim to propos"
L18-1541,D11-1142,0,0.0986968,"Missing"
lin-chen-2006-constructing,W97-0315,0,\N,Missing
lin-chen-2006-constructing,J03-3005,0,\N,Missing
lin-chen-2006-constructing,W03-0405,0,\N,Missing
lin-chen-2006-constructing,O06-4001,1,\N,Missing
lin-chen-2006-constructing,W04-0701,0,\N,Missing
M98-1017,O94-1010,1,0.825289,"Missing"
M98-1017,C96-1039,1,0.839203,"Missing"
M98-1017,O97-1010,1,0.876935,"Missing"
M98-1017,P98-1036,1,0.884486,"Missing"
M98-1017,C98-1036,1,\N,Missing
O00-1005,W97-0315,0,0.0738167,"Missing"
O00-1005,P98-2220,0,0.275007,"Missing"
O01-3002,voorhees-tice-2000-trec,0,0.0414834,"Missing"
O03-1013,lin-2002-web,0,0.071309,"Missing"
O03-1013,J93-2004,0,0.0249419,"Missing"
O03-1013,P01-1052,0,0.064128,"Missing"
O03-1013,W00-0603,0,0.0630438,"Missing"
O04-1010,W03-1501,1,0.883818,"Missing"
O04-1031,C96-1039,1,0.778377,"Missing"
O05-1024,A00-1044,0,0.0214014,"named entity recognition more challenging on spoken level than on written level. This paper emphasizes on a special kind of named entities, called transliteration names. people, places, etc. They denote foreign Spoken transliteration name recognition is useful for many applications. For example, cross language image retrieval via spoken query aims to employ spoken queries in one language to retrieve images with captions in another language [2]. In the past, Appelt and Martin [3] adapted TextPro system for processing text to processing transcripts generated by a speech recognizer. Miller et al [4] analyzed the effects of out-of-vocabulary errors and loss of punctuation in name finding of automatic speech recognition. Huang and Waibel [5] proposed an adaptive Figure 1. Flow of transliteration name recognition. method of named entity extraction for meeting understanding. access to image collection. access. Chen [6] dealt with spoken cross-language The coverage of a lexicon is one of the major issues in spoken transliteration name Recently, researchers are interested in exploring the Web, which provides huge-collection of up-to-date data, as a corpus. Keller and Lapata [7] employed the We"
O05-1024,J03-3005,0,0.105127,"r. Miller et al [4] analyzed the effects of out-of-vocabulary errors and loss of punctuation in name finding of automatic speech recognition. Huang and Waibel [5] proposed an adaptive Figure 1. Flow of transliteration name recognition. method of named entity extraction for meeting understanding. access to image collection. access. Chen [6] dealt with spoken cross-language The coverage of a lexicon is one of the major issues in spoken transliteration name Recently, researchers are interested in exploring the Web, which provides huge-collection of up-to-date data, as a corpus. Keller and Lapata [7] employed the Web to obtain frequencies for bigrams that are unseen in a given corpus. In this paper, we consider the Web as a live dictionary for recognizing spoken transliteration names, and employ fuzzy search capability of Google to retrieve relevant web page summaries. overall flow of our method. Section 2 sketches the Section 3 employs PAT trees to learn patterns from the Web dynamically and correct the recognition errors. Section 4 shows the experiments with/without the uses of the Web. Section 5 concludes the remarks. 2. Flow of transliteration name recognition A spoken transliteration"
O05-1024,W03-1501,1,0.832631,"e length of string alignment minus the number of edit operations. Finally, the ranking score of a PAT string relative to an ASR string is defined as follows. Score(ASR, PAT) = × Freq(PAT) + × Sim(ASR, PAT) It is computed by weighted merging of the frequency of the PAT string, and the similarity of the ASR string and PAT string. This value determines if the ASR string will be replaced by the PAT string. example, Freq(湯姆克魯斯)=43 and Sim(塔莫克魯斯, 湯姆克魯斯)=3. In the above 4. Experimental results The speech input to the transliteration name recognition system is Chinese utterance. transliteration names [9] to train the bi-character model specified in Section 2. include 50 American state names, 29 movie star names from 31 st We employed 51,114 In the experiments, the test data annual people’s choice awards (http://www.pcavote.com), and 21 NBA star names from NBA 2005 all star (http://www.nba.com/allstar2005/). The test set is different from the training set and it is open test. Because there may be more than one transliteration for a foreign named entity, the answer keys are manually prepared and checked with respect to the Web. For example, “Arizona” has four possible transliterations in Chines"
O06-1017,O04-3004,0,0.0663704,"Missing"
O06-1017,O05-1013,0,0.0631166,"Missing"
O06-4001,W03-1501,1,0.900484,"Missing"
O06-4001,W04-0701,0,0.199969,"Missing"
O06-4001,J03-3005,0,0.362297,"Missing"
O06-4001,W03-0405,0,0.0674234,"Missing"
O06-4001,A00-1044,0,0.06869,"Missing"
O06-4001,W97-0315,0,0.591663,"Missing"
O06-4001,niessen-etal-2000-evaluation,0,\N,Missing
O06-4001,2001.mtsummit-papers.35,0,\N,Missing
O06-4001,J00-4006,0,\N,Missing
O06-4001,W04-1116,0,\N,Missing
O06-4001,H94-1019,0,\N,Missing
O06-4001,C96-2141,0,\N,Missing
O06-4001,N03-2021,0,\N,Missing
O06-4001,O03-5003,0,\N,Missing
O06-4001,P02-1033,0,\N,Missing
O06-4001,P04-1037,0,\N,Missing
O06-4001,2001.mtsummit-papers.67,0,\N,Missing
O06-4001,O91-1003,0,\N,Missing
O06-4001,2001.mtsummit-papers.3,0,\N,Missing
O06-4001,P00-1056,0,\N,Missing
O06-4001,C02-1057,0,\N,Missing
O06-4001,P06-1009,0,\N,Missing
O06-4001,O05-2001,0,\N,Missing
O07-1013,W02-1011,0,0.0113611,"Missing"
O07-1013,W03-0404,0,0.0259786,"nions have long as well as complex answers which tend to scatter across different documents. Traditional QA approaches are not effective enough to retrieve answers for opinion questions as they have been for factual questions (Stoyanov et al., 2005). Hence, an opinion QA system is essential and urgent. Most of the research on QA systems has been developed for factual questions, and the association of subjective information with question answering has not yet been much studied. As for subjective information, Wiebe (2000) proposed a method to identify strong clues of subjectivity on adjectives. Riloff et al. (2003) presented a subjectivity classifier using lists of subjective nouns learned by bootstrapping algorithms. Riloff and Wiebe (2003) proposed a bootstrapping process to learn linguistically rich extraction patterns for subjective expressions. Kim and Hovy (2004) presented a system to determine word sentiments and combined sentiments within a sentence. Pang, Lee, and Vaithyanathan (2002) classified documents not by the topic, but by the overall sentiment, and then determined the polarity of a review. Wiebe et al. (2002) proposed a method for opinion summarization. Wilson et al. (2005) presented a"
O07-1013,H05-1116,0,0.290007,"ames Dean born?” and “Who won the Nobel Peace Prize in 1991?”. In addition to facts, people would also like to know about others’ opinions, thoughts, and feelings toward some specific topics, groups, and events. Opinion questions (e.g. “How do Americans consider the US-Iraq war?” and “What are the public’s opinions on human cloning?”) revealing answers about people’s opinions have long as well as complex answers which tend to scatter across different documents. Traditional QA approaches are not effective enough to retrieve answers for opinion questions as they have been for factual questions (Stoyanov et al., 2005). Hence, an opinion QA system is essential and urgent. Most of the research on QA systems has been developed for factual questions, and the association of subjective information with question answering has not yet been much studied. As for subjective information, Wiebe (2000) proposed a method to identify strong clues of subjectivity on adjectives. Riloff et al. (2003) presented a subjectivity classifier using lists of subjective nouns learned by bootstrapping algorithms. Riloff and Wiebe (2003) proposed a bootstrapping process to learn linguistically rich extraction patterns for subjective ex"
O07-1013,H05-1044,0,0.0403167,"adjectives. Riloff et al. (2003) presented a subjectivity classifier using lists of subjective nouns learned by bootstrapping algorithms. Riloff and Wiebe (2003) proposed a bootstrapping process to learn linguistically rich extraction patterns for subjective expressions. Kim and Hovy (2004) presented a system to determine word sentiments and combined sentiments within a sentence. Pang, Lee, and Vaithyanathan (2002) classified documents not by the topic, but by the overall sentiment, and then determined the polarity of a review. Wiebe et al. (2002) proposed a method for opinion summarization. Wilson et al. (2005) presented a phrase-level sentiment analysis to automatically identify the contextual polarity. Ku et al. (2006) proposed a method to automatically mine and organize opinions from heterogeneous information sources. Some research has gone from opinion analysis in texts toward that in QA systems. Cardie et al. (2003) took advantage of opinion summarization to support Multi-Perspective Question Answering (MPQA) system which aims to extract opinion-oriented information of a question. Yu and Hatzivassiloglou (2003) separated opinions from facts, at both the document and sentence levels. They intend"
O07-1013,W03-1017,0,0.291137,"ned the polarity of a review. Wiebe et al. (2002) proposed a method for opinion summarization. Wilson et al. (2005) presented a phrase-level sentiment analysis to automatically identify the contextual polarity. Ku et al. (2006) proposed a method to automatically mine and organize opinions from heterogeneous information sources. Some research has gone from opinion analysis in texts toward that in QA systems. Cardie et al. (2003) took advantage of opinion summarization to support Multi-Perspective Question Answering (MPQA) system which aims to extract opinion-oriented information of a question. Yu and Hatzivassiloglou (2003) separated opinions from facts, at both the document and sentence levels. They intended to cluster opinion sentences from the same perspective together and summarize them as answers to opinion questions. Kim and Hovy (2005) identified opinion holders, which are frequently asked in opinion questions. This paper deals with two major problems in opinion QA systems: question analysis and answer passage retrieval. Several issues, including how to separate opinion questions from factual ones, how to define question types for opinion questions, how to correctly classify opinion questions into corresp"
O07-1015,P07-2034,1,0.653634,"Missing"
O08-5003,C04-1200,0,0.109551,"Missing"
O08-5003,W02-1011,0,0.0107734,"Missing"
O08-5003,W03-1014,0,0.0736729,"Missing"
O08-5003,W03-0404,0,0.0451141,"Missing"
O08-5003,H05-1116,0,0.0355929,"Missing"
O08-5003,W03-1017,0,0.147762,"Missing"
O08-5003,H05-1044,0,\N,Missing
O09-1008,C04-1200,0,0.155952,"Missing"
O09-1008,W06-0301,0,0.0471135,"Missing"
O09-1008,H05-1045,0,0.0476198,"Missing"
O09-6003,H05-1045,0,0.0951735,"Missing"
O09-6003,C04-1200,0,0.0682816,"Missing"
O09-6003,W06-0301,0,0.0450974,"Missing"
O11-1010,esuli-sebastiani-2006-sentiwordnet,0,0.177475,"Missing"
O11-1010,P97-1023,0,0.344997,"Missing"
O11-1010,kamps-etal-2004-using,0,0.129535,"Missing"
O11-1010,E06-1025,0,0.0537812,"Missing"
O11-1010,C04-1145,0,0.0494121,"Missing"
O11-1010,P10-1089,0,0.0296933,"Missing"
O11-4004,P09-1097,0,0.0711107,"Missing"
O12-3002,esuli-sebastiani-2006-sentiwordnet,0,0.203302,"Missing"
O12-3002,E06-1025,0,0.077896,"Missing"
O12-3002,P97-1023,0,0.296671,"Missing"
O12-3002,C04-1145,0,0.0434779,"Missing"
O12-5003,D09-1131,1,0.887392,"Missing"
O12-5003,W11-3703,1,0.887507,"Missing"
O12-5003,P02-1053,0,0.0130726,"Missing"
O14-4003,P06-4018,0,0.0467152,"Missing"
O14-4003,W08-0336,0,0.0291509,"Missing"
O14-4003,D09-1060,0,0.0765189,"Missing"
O14-4003,de-marneffe-etal-2006-generating,0,0.0073215,"Missing"
O14-4003,P03-1056,0,0.056901,"Missing"
O14-4003,P10-1122,0,0.0552529,"Missing"
O14-4003,D09-1082,0,0.0475942,"Missing"
O93-1003,W89-0222,0,\N,Missing
O93-1003,W89-0211,0,\N,Missing
O93-1003,W89-0240,0,\N,Missing
O93-1003,H89-2012,0,\N,Missing
O93-1003,E85-1024,0,\N,Missing
O93-1003,A88-1019,0,\N,Missing
O93-1003,P84-1005,0,\N,Missing
O93-1003,P90-1034,0,\N,Missing
O93-1003,W89-0215,0,\N,Missing
O93-1003,W89-0213,0,\N,Missing
O93-1003,1991.iwpt-1.22,0,\N,Missing
O93-1003,E91-1004,0,\N,Missing
O96-1005,J96-3004,0,\N,Missing
O96-1005,P94-1041,0,\N,Missing
O96-1005,P94-1010,0,\N,Missing
O96-1005,P92-1008,0,\N,Missing
O96-1005,P83-1019,0,\N,Missing
O96-1005,C92-1019,0,\N,Missing
O96-2005,J85-2002,0,\N,Missing
O96-2005,J90-1003,0,\N,Missing
O96-2005,O90-1004,0,\N,Missing
O96-2005,J93-1005,0,\N,Missing
O96-2005,C94-1012,0,\N,Missing
O96-2005,C94-2195,0,\N,Missing
O96-2005,1991.mtsummit-papers.9,0,\N,Missing
O96-2005,P83-1017,0,\N,Missing
O96-2005,P94-1032,1,\N,Missing
O96-2005,O93-1003,1,\N,Missing
O97-4001,W95-0113,1,\N,Missing
O97-4001,E93-1006,0,\N,Missing
O97-4001,E93-1040,0,\N,Missing
O97-4001,P90-1034,0,\N,Missing
O97-4001,H91-1026,0,\N,Missing
O98-3005,C96-1039,1,0.902013,"s needed to associate URLs and E-mail addresses with suitable proper nouns. Different kinds of clues, such as the spelling method, adjacency principle and HTML tags (e.g., title, headings, address, and font style elements), are employed. 3. System Overview We periodically collect web pages from the Internet/Intranet using a spider. The white page constructor first analyzes these HTML files. Basic processing units (sentences or quasi-sentences) and HTML meta-information are gathered. Because a Chinese sentence (or quasi sentence) is composed of a sequence of characters without word boundaries [Chen and Lee, 1996], a Chinese segmentation system identifies the word tokens. Then, a proper noun identification system (see Section 4) extracts personal names and organization names. During processing, the information in the anchor parts is placed in the anchor set (AS). Other information, i.e., that appearing in non-anchor parts, is placed in one of the content sets (CSes) which correspond to different types of information. In the current implementation, there are three content sets: CS_Proper-Noun, CS_E-Mail and CS_HTTP. They record proper nouns, E-mail addresses and URLs, respectively. For the anchor set,"
O98-3005,J90-1003,0,0.0156773,"Missing"
O98-3005,W93-0105,0,0.0847293,"Missing"
O98-3005,W93-0104,0,0.0488829,"CS_Proper-Noun with the following attributes: the position information of token (token_no) and the associated HTML meta information (<TITLE>, <Hn>, <Address>, <Bold>, <Font> and <Italic>) } 5. Extract different types of information with the position information of token (token_no), and add to the corresponding Content Sets (CS_E-Mail and CS_HTTP) 6. End White Page Construction from Web Pages 83 4. Identification of Proper Nouns Proper nouns that are not collected in lexicons are major unknown words in natural language texts. Several methods [Boguraev and Pustejovsky, 1996; Mani, et al., 1993; McDonald, 1993; Paik, et al., 1993] have been proposed to identify English proper nouns. For research related to Chinese, Chang et al. [1992] and Wang et al. [1992] touched on Chinese personal names; Sproat et al. [1994] considered Chinese personal names and transliterations of foreign words; Chen and Lee [1996] identified Chinese personal names, Chinese transliterated personal names and organization names. The name identification module is based on our previous design. The methods are described below. 4.1 Identification of Personal names A Chinese personal name is composed of surname and given name parts."
O98-3005,W93-0114,0,0.121657,"Missing"
O98-3005,P94-1010,0,0.0298922,"Missing"
O98-3005,C92-4199,0,0.0766174,"Missing"
O98-3005,J96-3004,0,\N,Missing
O99-3003,J85-2002,0,\N,Missing
O99-3003,C94-1012,0,\N,Missing
O99-3003,C96-1039,1,\N,Missing
O99-3003,J90-2002,0,\N,Missing
O99-3003,1995.tmi-1.21,1,\N,Missing
O99-3003,1991.mtsummit-papers.9,0,\N,Missing
O99-3003,O96-2005,1,\N,Missing
O99-3003,O93-1004,0,\N,Missing
O99-4002,W89-0240,0,\N,Missing
O99-4002,H89-2012,0,\N,Missing
O99-4002,bian-chen-1998-integrating,1,\N,Missing
P06-1127,J03-3005,0,0.0694717,"if corpora adopted can reflect the up-to-date usage. As we know, languages are live. New terms and phrases are used in daily life. How to capture the new usages is an important research topic. The Web is a heterogeneous document collection. Huge-scale and dynamic nature are characteristics of the Web. Regarding the Web as a live corpus becomes an active research topic recently. How to utilize the huge volume of web data to measure association of information is an important issue. Resnik and Smith (2003) employ the Web as parallel corpora to provide bilingual sentences for translation models. Keller and Lapata (2003) show that bigram statistics for English language is correlated between corpus and web counts. Besides, how to get the word counts and the word association counts from the web pages without scanning over the whole collections is indispensable. Directly managing the web pages is not an easy task when the Web grows very fast. Search engine provides some way to return useful information. Page counts for a query denote how many web pages containing a specific word or a word pair roughly. Page count is different from word frequency, which denotes how many occurrences a word appear. Lin and Chen (20"
P06-1127,O05-1024,1,0.626565,"y denote how many web pages containing a specific word or a word pair roughly. Page count is different from word frequency, which denotes how many occurrences a word appear. Lin and Chen (2004) explore the use of the page counts provided by different search engines to compute the statistics for Chinese segmentation. In addition to the page counts, snippets returned by web search, are another web data for training. A snippet consists of a title, a short summary of a web page and a hyperlink to the web page. Because of the cost to retrieve the full web pages, short summaries are always adopted (Lin, Chen, and Chen, 2005). Various measures have been proposed to compute the association of objects of different granularity like terms and documents. Rodríguez and Egenhofer (2003) compute the semantic 1009 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1009–1016, c Sydney, July 2006. 2006 Association for Computational Linguistics similarity from WordNet and SDTS ontology by word matching, feature matching and semantic neighborhood matching. Li et al. (2003) investigate how information sources could be used effectively, and propose a new simil"
P06-1127,lin-chen-2006-constructing,1,0.722174,"o be disambiguated. A named entity NEj has m accompanying names, called cue names later, CNj1, CNj2, …, CNjm. We have two alternatives to use the cue names. One is using them directly, i.e., NEj is represented as a community of cue names Community(NEj)={CNj1, CNj2, …, CNjm}. The other is to expand the cue names CNj1, CNj2, …, CNjm for NEj using the web data as follows. Let CNj1 be an initial seed. Figure 3 sketches the concept of community expansion. 1014 (1) Collection: We submit a seed to Google, and select the top N returned snippets. Then, we use suffix trees to extract possible patterns (Lin and Chen, 2006). (2) Validation: We calculate CODC score of each extracted pattern (denoted Bi) with the seed A. If CODC(A,Bi) is strong enough, i.e., larger than a threshold θ, we employ Bi as a new seed and repeat steps (1) and (2). This procedure stops either expected number of nodes is collected or maximum number of layers is reached. (3) Union: The community initiated by the seed CNji is denoted by Community(CNji)={Bji , Bji , …, Bji }, where Bji is a new seed. The Cscore score, community score, of Bji is the CODC score of Bji with its parent divided by the layer it is located. We repeat Collection and"
P06-1127,J03-3002,0,0.0420261,"available and many language models have been experimented. One major issue behind the corpus-based approaches is: if corpora adopted can reflect the up-to-date usage. As we know, languages are live. New terms and phrases are used in daily life. How to capture the new usages is an important research topic. The Web is a heterogeneous document collection. Huge-scale and dynamic nature are characteristics of the Web. Regarding the Web as a live corpus becomes an active research topic recently. How to utilize the huge volume of web data to measure association of information is an important issue. Resnik and Smith (2003) employ the Web as parallel corpora to provide bilingual sentences for translation models. Keller and Lapata (2003) show that bigram statistics for English language is correlated between corpus and web counts. Besides, how to get the word counts and the word association counts from the web pages without scanning over the whole collections is indispensable. Directly managing the web pages is not an easy task when the Web grows very fast. Search engine provides some way to return useful information. Page counts for a query denote how many web pages containing a specific word or a word pair rough"
P06-1127,P98-1012,0,0.108083,"Missing"
P06-1127,C98-1012,0,\N,Missing
P06-1127,O06-4001,1,\N,Missing
P06-2011,W04-3248,0,0.0987761,"s area until present. 1 Introduction Translation of named entities (NE) attracts much attention due to its practical applications in World Wide Web. The most challenging issue behind is: the genres of NEs are various, NEs are open vocabulary and their translations are very flexible. Some previous approaches use phonetic similarity to identify corresponding transliterations, i.e., translation by phonetic values (Lin and Chen, 2002; Lee and Chang, 2003). Some approaches combine lexical (phonetic and meaning) and semantic information to find corresponding translation of NEs in bilingual corpora (Feng et al., 2004; Huang et al., 2004; Lam et al., 2004). These studies focus on the alignment of NEs in parallel or comparable corpora. That is called “close-ended” NE translation. In “open-ended” NE translation, an arbitrary NE is given, and we want to find its corresponding translations. Most previous approaches exploit web search engine to help find translating candidates on the Internet. Al-Onaizan and Knight (2003) adopt language models to generate 2 Background Translating NEs, which is different from translating common words, is an “asymmetric” translation. Translations of an NE in various languages can"
P06-2011,N04-1036,0,0.0568102,"Missing"
P06-2011,W02-2017,1,0.888793,"Missing"
P06-2011,W03-0317,0,\N,Missing
P06-2011,P02-1051,0,\N,Missing
P07-2023,W02-1011,0,0.014459,"Missing"
P07-2034,O06-1017,1,0.498571,"icons, or emoticons. Different kinds of emoticons are introduced into text expressions to convey bloggers’ emotions. Since thousands of blog articles are created everyday, emotional expressions can be collected to form a large-scale corpus which guides us to build vocabularies that are more emotionally expressive. Our approach can create an emotion lexicon free of laborious efforts of the experts who must be familiar with both linguistic and psychological knowledge. 2 Related Works Some previous works considered emoticons from weblogs as categories for text classification. Mishne (2005), and Yang and Chen (2006) used emoticons as tags to train SVM (Cortes and Vapnik, 1995) classifiers at document or sentence level. In their studies, emoticons were taken as moods or emotion tags, and textual keywords were taken as features. Wu et al. (2006) proposed a sentencelevel emotion recognition method using dialogs as their corpus. “Happy, “Unhappy”, or “Neutral” was assigned to each sentence as its emotion category. Yang et al. (2006) adopted Thayer’s model (1989) to classify music emotions. Each music segment can be classified into four classes of moods. In sentiment analysis research, Read (2005) used emotic"
P07-2034,P05-2008,0,\N,Missing
P13-2079,P10-1122,0,0.31449,"nd hypothesis (H). If human would agree that the meaning of H can be inferred from the meaning of T, we say that T entails H (Dagan et al., 2006). The researches on textual entailment have attracted much attention in recent years due to its potential applications (Androutsopoulos and Malakasiotis, 2010). Recognizing Textual Entailment (RTE) (Bentivogli, et al., 2011), a series of evaluations on the developments of English TE recognition technologies, have been held seven times up to 2011. In the meanwhile, TE recognition technologies in other languages are also underway (Shima, et al., 2011). Sammons, et al., (2010) propose an evaluation metric to examine the characteristics of a TE recognition system. They annotate texthypothesis pairs selected from the RTE-5 test set with a series of linguistic phenomena required in the human inference process. The RTE systems are evaluated by the new indicators, such as how many T-H pairs annotated with a particular phenomenon can be correctly recognized. The indicators can tell developers which systems are better to deal with T-H pairs with the appearance of which phenomenon. That would give developers a direction to enhance their RTE systems. Such linguistic phenome"
P13-2079,D09-1082,0,0.0206814,"n (cnn) in H}. Then, we extract 2 sets of relations, including {relation in H} and {relation in T}, where each relation in the sets is in a form of Predicate(Argument1, Argument2). Some typical examples of relations are verb(subject, object) for verb phrases, neg(A, B) for negations, num(Noun, number) for numeric modifier, and tmod(C, temporal argument) for temporal modifier. A predicate has only 2 arguments in this representation. Thus, a di-transitive verb is in terms of two relations. Instead of measuring the relatedness of T-H pairs by comparing T and H on the predicateargument structure (Wang and Zhang, 2009), our method tries to find the five negative entailment phenomena based on the similar representation. Each of the five negative entailment phenomena is extracted as follows according to their definitions. To reduce the error propagation which may be arisen from the parsing errors, we directly match those nouns and named entities appearing in H to the text T. Furthermore, we introduce WordNet to align arguments in H to T. (a) Disconnected Relation. If (1) for each a  {noun in H}{nnp in H}{cnn in H}, we can find a  T too, and (2) for each r1=h(a1,a2)  {relation in H}, we can find a relatio"
P13-2079,W07-1401,0,0.150159,"Missing"
P13-2079,P06-4018,0,0.00528895,"5, 7, 8) setting does not have much performance difference. In the above experiments, we do all the analyses on the corpus annotated with linguistic phenomena by human. We aim at knowing the ulti447 mate performance of TE recognition systems embodying human knowledge in the inference. The human knowledge in the inference cannot be captured by TE recognition systems fully correctly. In the later experiments, we explore the five critical features, (3, 4, 5, 7, 8), and examine how the performance is affected if they are extracted automatically. 3 Levy and Manning, 2003), and stemming with NLTK (Bird, 2006). 3.1 Negative Entailment Phenomena Extraction The experimental results in Section 2.2 show that disconnected relation, exclusive argument, exclusive relation, missing argument, and missing relation are significant. We follow the definitions of Sammons et al. (2010) and show them as follows. (a) Disconnected Relation. The arguments and the relations in Hypothesis (H) are all matched by counterparts in Text (T). None of the arguments in T is connected to the matching relation. (b) Exclusive Argument. There is a relation common to both the hypothesis and the text, but one argument is matched in"
P13-2079,P03-1056,0,0.0137712,"n the one hand, adding more phenomena into (3, 4, 5, 7, 8) setting does not have much performance difference. In the above experiments, we do all the analyses on the corpus annotated with linguistic phenomena by human. We aim at knowing the ulti447 mate performance of TE recognition systems embodying human knowledge in the inference. The human knowledge in the inference cannot be captured by TE recognition systems fully correctly. In the later experiments, we explore the five critical features, (3, 4, 5, 7, 8), and examine how the performance is affected if they are extracted automatically. 3 Levy and Manning, 2003), and stemming with NLTK (Bird, 2006). 3.1 Negative Entailment Phenomena Extraction The experimental results in Section 2.2 show that disconnected relation, exclusive argument, exclusive relation, missing argument, and missing relation are significant. We follow the definitions of Sammons et al. (2010) and show them as follows. (a) Disconnected Relation. The arguments and the relations in Hypothesis (H) are all matched by counterparts in Text (T). None of the arguments in T is connected to the matching relation. (b) Exclusive Argument. There is a relation common to both the hypothesis and the"
P13-2079,de-marneffe-etal-2006-generating,0,0.0124486,"Missing"
P13-2079,W08-0336,0,\N,Missing
P13-2079,D09-1060,0,\N,Missing
P14-5018,W12-0404,0,0.0143942,"on for Computational Linguistics: System Demonstrations, pages 103–108, c Baltimore, Maryland USA, June 23-24, 2014. 2014 Association for Computational Linguistics major sources of illegal advertisements on the web, i.e., food and cosmetic advertising, as examples. Section 2 surveys the related work. Section 3 introduces the datasets used in the experiments. Section 4 presents classification models and shows their performance. Section 5 mines the overstated phrases. Section 6 demonstrates the uses of FAdR system with screenshot. Both sentence and document levels are considered. 2 Related Work Gokhman et al. (2012) collected data from the Internet and explored methods to construct a gold standard corpus for “deception” studies. Ott et al. (2011) studied methods to detect “disruptive opinion spams.” Unlike conventional advertising spams, these fake opinions look authentic and are used to mislead users. Mukherjee et al. (2013) used reviewer’s behavioral footprints to detect spammer. As they pointed out, one of the largest problems to solve this issue is that there is no appropriate datasets for fake and non-fake reviews. Previous online advertising research mostly focuses on bidding, matching or recommend"
P14-5018,I08-2083,1,0.813374,"Ott et al. (2011) studied methods to detect “disruptive opinion spams.” Unlike conventional advertising spams, these fake opinions look authentic and are used to mislead users. Mukherjee et al. (2013) used reviewer’s behavioral footprints to detect spammer. As they pointed out, one of the largest problems to solve this issue is that there is no appropriate datasets for fake and non-fake reviews. Previous online advertising research mostly focuses on bidding, matching or recommendation of advertisements on websites. Ghosh et al. (2009) studied bidding strategies for advertisement allocations. Huang et al. (2008) proposed an advertisement recommendation method by classifying instant messages into the Yahoo categories. Scaiano and Inkpen (2011) used Wikipedia for negative keyphrase generation to hide advertisements that users are not interested in. This paper, in contrast, focuses on identifying false statements in online advertisements with classification models. 3 Datasets We use the illegal advertising lists and statements made public by the Taipei City Government1 as the illegal advertising datasets. The contents of the government data are split into sentences by colon, period, question mark and ex"
P14-5018,P11-1032,0,0.0348574,"Computational Linguistics major sources of illegal advertisements on the web, i.e., food and cosmetic advertising, as examples. Section 2 surveys the related work. Section 3 introduces the datasets used in the experiments. Section 4 presents classification models and shows their performance. Section 5 mines the overstated phrases. Section 6 demonstrates the uses of FAdR system with screenshot. Both sentence and document levels are considered. 2 Related Work Gokhman et al. (2012) collected data from the Internet and explored methods to construct a gold standard corpus for “deception” studies. Ott et al. (2011) studied methods to detect “disruptive opinion spams.” Unlike conventional advertising spams, these fake opinions look authentic and are used to mislead users. Mukherjee et al. (2013) used reviewer’s behavioral footprints to detect spammer. As they pointed out, one of the largest problems to solve this issue is that there is no appropriate datasets for fake and non-fake reviews. Previous online advertising research mostly focuses on bidding, matching or recommendation of advertisements on websites. Ghosh et al. (2009) studied bidding strategies for advertisement allocations. Huang et al. (2008"
P14-5018,R11-1093,0,0.0276479,"ions look authentic and are used to mislead users. Mukherjee et al. (2013) used reviewer’s behavioral footprints to detect spammer. As they pointed out, one of the largest problems to solve this issue is that there is no appropriate datasets for fake and non-fake reviews. Previous online advertising research mostly focuses on bidding, matching or recommendation of advertisements on websites. Ghosh et al. (2009) studied bidding strategies for advertisement allocations. Huang et al. (2008) proposed an advertisement recommendation method by classifying instant messages into the Yahoo categories. Scaiano and Inkpen (2011) used Wikipedia for negative keyphrase generation to hide advertisements that users are not interested in. This paper, in contrast, focuses on identifying false statements in online advertisements with classification models. 3 Datasets We use the illegal advertising lists and statements made public by the Taipei City Government1 as the illegal advertising datasets. The contents of the government data are split into sentences by colon, period, question mark and exclamation mark. Two types of datasets are built for illegal food and cosmetic advertising, named FOOD_ILLEGAL and COS_ILLEGAL, respec"
P14-5018,W11-3703,1,0.821226,"g schemes are considered. In the binary weighting, each sentence is represented by a word vector (w1, w2, …, w1000), where wi is a binary value indicating whether a word occurs in the sentence or not. In the weighting of log relative frequency ratio, we follow the idea of collocation mining (Damerau, 1993). Relative frequency ratio between two datasets has been shown to be useful to discover collocations that are characteristic of a dataset when compared to the other dataset. It has been successfully applied to mine sentiment words from microblog and to model reader/writer emotion transition (Tang and Chen, 2011, 2012). The log relative frequency ratio (logRF) is defined formally as follows. Given two datasets A and B, the log relative frequency ratio for each wi∈A∪B is computed with the following formula. f A (w i ) logRFAB (w i ) = log |A |i fB (w ) |B| logRFAB(wi) is a log ratio of relative frequencies of word wi in A and B, fA(wi) and fB(wi) are frequencies of wi in A and in B, respectively, and |A |and |B |are total words in A and in B, respectively. logRF values are used to estimate the distribution of the words in datasets A and B. If wi has higher relative frequency in A than in B, then logRF"
P14-5018,tang-chen-2012-mining,1,0.895958,"Missing"
P16-2004,W14-5905,0,0.0798391,"vector for each aspect node in the review hierarchy. Zeng and Li (2013) regard identification of implicit features as a classification problem, and consider reviews for each clustered opinion-pair as training set. Wang et al. (2013) employ five collocation methods including frequency, PMI, frequency⁄PMI, t-test and chi-square test to measure the association between opinion words and aspect terms. Cruz et al. (2014) manually annotate implicit aspects and implicit aspect indicators (IAI) on the customer review datasets in Hu and Liu (2004), and employ Conditional Random Fields to recognize IAI. Poria et al. (2014) identify implicit aspect clues (IACs) in a document. Both approaches establish IAI (IAC) and aspect mapping. Mukherjee and Liu (2012) propose two statistical models to deal with aspect categorization problem. They use hotel reviews from tripadvisor.com, and point out categorizing aspects is a subjective task. Total 9 major aspects based on commonsense knowledge, including Dining, Staff, Maintenance, Check In, Cleanliness, Comfort, Amenities, Location and Value for Money, are considered. Kim et al. (2013) further analyze general aspects and specific aspects, and discuss how aspect structure is"
P16-2004,D11-1013,0,0.0271076,"feature which does not occur explicitly, but can be inferred from the surrounding opinion word. They propose a mutual reinforcement approach to cluster product features and opinion words simultaneously, and extract implicit features based on opinion words. In the subsequent work, different methodologies are proposed to identify the association between opinion words and aspect terms (called also product features), thus implicit aspects are inferred from opinion word-aspect term mapping (Bagheri et al., 2013). Zhen et al. (2011) propose a two-phase cooccurrence association rule mining approach. Yu et al. (2011) generate a review hierarchy based on aspects. Implicit aspect of a review can be deter3 Constructing Implicit Opinions Corpus This section first defines the implicit opinions, collects a Chinese hotel dataset, identifies opinion and aspect clusters from the dataset, and constructs implicit opinion corpus labelled with aspect class and polarity. 3.1 Definitions of Implicit Opinions A sentence in a review can be partitioned into several segments separated by punctuation marks. The 21 following show four possible types of segments polarity and aspect by polarity and aspect classifibased on the o"
P16-2004,yu-etal-2012-development,1,0.858728,"Missing"
P16-2004,P12-1036,0,0.0308768,"ation problem, and consider reviews for each clustered opinion-pair as training set. Wang et al. (2013) employ five collocation methods including frequency, PMI, frequency⁄PMI, t-test and chi-square test to measure the association between opinion words and aspect terms. Cruz et al. (2014) manually annotate implicit aspects and implicit aspect indicators (IAI) on the customer review datasets in Hu and Liu (2004), and employ Conditional Random Fields to recognize IAI. Poria et al. (2014) identify implicit aspect clues (IACs) in a document. Both approaches establish IAI (IAC) and aspect mapping. Mukherjee and Liu (2012) propose two statistical models to deal with aspect categorization problem. They use hotel reviews from tripadvisor.com, and point out categorizing aspects is a subjective task. Total 9 major aspects based on commonsense knowledge, including Dining, Staff, Maintenance, Check In, Cleanliness, Comfort, Amenities, Location and Value for Money, are considered. Kim et al. (2013) further analyze general aspects and specific aspects, and discuss how aspect structure is helpful. Zhao et al. (2015) present a fine-grained corpus for sentiment analysis. Our work is different from the previous ones in two"
P16-2004,D14-1181,0,0.00503592,"Missing"
P16-2004,H05-1043,0,0.0849854,"can be found. The direct opinion word and aspect mapping is not feasible in implicit polarity and implicit aspect recognition. We focus on the construction of an implicit opinions corpus for double-implicit recognition. The aspect categorization is not the major concern. Related Work Hu and Liu (2004) present the first feature-based opinion summarization system. They point out explicit and implicit product features, and extract explicit features by using association miner and pruning strategies. The opinionated sentences along with their polarity are listed under individual product features. Popescu and Etzioni (2005) introduce an opinion extraction system OPINE. OPINE extracts explicit product features based on Point-wise Mutual Information. This work does not discuss the implicit feature generation. Liu et al. (2005) present an association mining approach to extract both explicit and implicit features in their opinion observer, but the implicit features discussed occur explicitly in an overt form, e.g., [MB] indicates a product feature <memory>. Su et al. (2008) define an implicit feature as the product feature which does not occur explicitly, but can be inferred from the surrounding opinion word. They p"
P17-2064,N15-1142,0,0.0272701,". (2012). CBOW/Skip-gram Word Embeddings We trained word vectors with the two architectures included in the word2vec software (Mikolov et al., 2013a). The continuous bag-of-words model (CBOW) uses the words in a context window to predict the target word, while the skipgram model (SG) uses the target word to predict every word in the context window. CWINDOW/Structured Skip-gram Word Embeddings Taking the order of the context words into consideration, we also employ the continuous window model (CWIN) and the structured skip-gram 1 405 http://lemurproject.org/clueweb09.php 5.2 model (Struct-SG) (Ling et al., 2015). The former replaces the summation of context word vectors in CBOW with a concatenation operation, and the latter applies different projection matrices for predicting context words in different relative position with the target word. 3.2 Accuracy We use the detection accuracy as our main evaluation metric. A test instance is regarded as correct only if our system gives the highest score of incorrectness for the ground-truth position. This metric is relatively strict as the average length of the sentence segments in our dataset is 9.24. The McNemar’s test is adopted to perform statistical sign"
P17-2064,W12-2006,0,0.0862204,"Missing"
P17-2064,P14-5010,0,0.00572964,"Missing"
P17-2064,W11-2838,0,0.0851315,"Missing"
P17-2064,C16-1085,1,0.711149,"Missing"
P17-2064,N13-1090,0,0.185316,"on, their model only determines whether a sentence segment contains WUEs. Huang et al. (2016) used the HSK corpus to study the preposition selection problem. They proposed gated recurrent unit (GRU)-based models to select the most suitable one from a closed set of Chinese prepositions given the sentential context. Although their approach can be utilized to detect and correct preposition errors, it is still worth investigating how to recognize WUEs involving other types of words such as verbs and nouns. In the past few years, distributed word representations derived from neural network models (Mikolov et al., 2013a; Pennington et al., 2014) have become popular among various studies in natural language processing. Beyond surface forms, these low-dimensional vector representations can encode syntactic and semantic information implicitly (Mikolov et al., 2013b). Because WUEs involve syntactic or semantic problems, vector representations could be promising for finding the erroneous tokens. One challenging aspect of dealing with grammatical errors is that the errors usually do not stand on their own, but are dependent on the context (Chollampatt et al., 2016). Therefore, we need a model that considers the s"
P17-2064,W16-4919,0,0.0626137,"ational Linguistics https://doi.org/10.18653/v1/P17-2064 our system can output a ranked list of candidate error positions. The positions with the highest incorrectness scores will be marked as incorrect. In (E2) we show an example labeling result of our system. The tokens 差 (bad) and 知識 (knowledge), with the highest scores, are most likely to be incorrect. LSTM, to detect errors in English learner writing. However, they mainly focused on comparing different composition architectures under the same word representation, so it remained unclear to what extent pre-trained word embeddings can help. Huang and Wang (2016) used LSTM for Chinese grammatical error diagnosis, but their models are trained only on learner data, without external well-formed text. That means the performance might be limited by the relatively small amount of annotated sentences written by foreign learners. This paper utilizes LSTM and its extension (Bidirectional LSTM) along with the information derived from external resources to deal with Chinese WUE detection. Several types of pre-trained word embeddings and additional token-level features are considered. Each token in a sentence will be labeled correct or incorrect. Experimental res"
P17-2064,W14-1701,0,0.0950208,"Missing"
P17-2064,W13-3601,0,0.0867299,"Missing"
P17-2064,W16-4906,0,0.47638,"Missing"
P17-2064,W15-4401,0,0.191677,"Missing"
P17-2064,P16-1112,0,0.0378883,"Missing"
P17-2064,L16-1033,1,0.717535,"Missing"
P17-2064,yu-etal-2012-development,1,0.907131,"Missing"
P18-2122,S17-2089,0,0.123494,"Missing"
P18-2122,P14-5010,0,0.00468192,"Missing"
P18-2122,W10-2914,0,0.202584,"1 Chiao-Chen Chen,1 and Hsin-Hsi Chen12 1 Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan 2 MOST Joint Research Center for AI Technology and All Vista Healthcare, Taipei, Taiwan hhhuang@nlg.csie.ntu.edu.tw, {b04902055,hhchen}@ntu.edu.tw Abstract nancial opinion mining (Cortis et al., 2017), and irony detection (Ghosh et al., 2015; Peled and Reichart, 2017; Hee et al., 2018). In the case of irony detection, it is impractical to manually annotate the ironic sentences from randomly sampled data due to the relatively low occurrences of irony (Davidov et al., 2010). Collecting the tweets with the hashtags like #sarcasm, #irony, and #not becomes the mainstream approach to dataset construction (Sulis et al., 2016). As shown in (S1), the tweet with the hashtag #not is treated as a positive (ironic) instance by removing #not from the text. The reliability of self-labeled data is an important issue when the data are regarded as ground-truth for training and testing learning-based models. This paper addresses the issue of false-alarm hashtags in the self-labeled data for irony detection. We analyze the ambiguity of hashtag usages and propose a novel neural ne"
P18-2122,S16-1003,0,0.0691604,"Missing"
P18-2122,C04-1125,0,0.076951,"edia to check my social media. When the hashtag plays as a content word in a tweet, the tweet is not a good candidate of selflabeled ironic instances because the sentence will be incomplete once the hashtag is removed. In this work, both kinds of unreliable data, the tweets with a misused hashtag and the tweets in which the hashtag serves as a content word, are our targets to remove from the training data. Manual data cleaning is labor-intensive and inefficient (Van Hee et al., 2016a). Compared to general training data cleaning approaches (Malik and Bhardwaj, 2011; Esuli and Sebastiani, 2013; Fukumoto and Suzuki, 2004) such as boostingbased learning, this work leverages the characteristics of hashtag usages in tweets. With small amount of golden labeled data, we propose a neural network classifier for pruning the self-labeled tweets, and train an ironic detector on the less but cleaner instances. This approach is easily to apply to other NLP tasks that rely on self-labeled data. The contributions of this work are three-fold: (1) We make an empirically study on an issue that is potentially inherited in a number of research topics based on self-labeled data. (2) We propose a model for hashtag disambiguation."
P18-2122,D15-1116,0,0.0503357,"Missing"
P18-2122,P17-1155,0,0.0260925,"Missing"
P18-2122,D14-1127,0,0.174493,"are considered as labels for training and testing learning-based models, which usually benefit from large amount of data. One of the sources of self-labeled data widely used in the research community is Twitter, where the short-text messages tweets written by the crowd are publicly shared. In a tweet, the author can tag the short text with some hashtags such as #excited, #happy, #UnbornLivesMatter, and #Hillary4President to express their emotion or opinion. The tweets with a certain types of hashtags are collected as self-label data in a variety of research works including sentiment analysis (Qadir and Riloff, 2014), stance detection (Mohammad et al., 2016; Sobhani et al., 2017), fi(S2) BestProAdvice @Anonymous More clean OR cleaner, never more cleaner. #irony When the false-alarm instances like (S2) are collected and mixed in the training and test data, the models that learn from the unreliable data may be misled, and the evaluation is also suspicious. The other kind of unreliable data comes from the hashtags not only functioning as metadata. That is, a hashtag in a tweet may also function as a content word in its word form. For example, the hashtag #irony in (S3) is a part of the sentence “the irony of"
P18-2122,S18-1005,0,0.0449867,"Missing"
P18-2122,E17-2088,0,0.0137974,"odels, which usually benefit from large amount of data. One of the sources of self-labeled data widely used in the research community is Twitter, where the short-text messages tweets written by the crowd are publicly shared. In a tweet, the author can tag the short text with some hashtags such as #excited, #happy, #UnbornLivesMatter, and #Hillary4President to express their emotion or opinion. The tweets with a certain types of hashtags are collected as self-label data in a variety of research works including sentiment analysis (Qadir and Riloff, 2014), stance detection (Mohammad et al., 2016; Sobhani et al., 2017), fi(S2) BestProAdvice @Anonymous More clean OR cleaner, never more cleaner. #irony When the false-alarm instances like (S2) are collected and mixed in the training and test data, the models that learn from the unreliable data may be misled, and the evaluation is also suspicious. The other kind of unreliable data comes from the hashtags not only functioning as metadata. That is, a hashtag in a tweet may also function as a content word in its word form. For example, the hashtag #irony in (S3) is a part of the sentence “the irony of taking a break...”, in contrast to the hashtag #not in (S1), wh"
P18-2122,D14-1181,0,0.00245713,"tag sequences. Finally, the sigmoid activation function decides whether the instance is real ironic or false-alarm. The details of each component will be presented in the rest of this section. Word Sequences: The word sequences of the context preceding and following the targeting hashtag are separately encoded by neural network sentence encoders. The Penn Treebank Tokenizer provided by NLTK (Bird et al., 2009) is used for tokenization. As a result, each of the left and the right word sequences is encoded as a embedding with a length of 50. We experiments with convolution neural network (CNN) (Kim, 2014), gated recurrent unit (GRU) (Cho et al., 2014), and attentive-GRU for sentence encoding. CNN for sentence classification has been shown effective in NLP applications such as sentiment analysis (Kim, 2014). Classifiers based on recurrent neural network (RNN) 2 773 https://code.google.com/archive/p/word2vec/ Model LR CNN GRU Att.GRU Our Method Our Method Our Method w/o LM ble of modeling the longer dependencies among the sequential tokens (Mikolov et al., 2011). Two millions of English tweets that are entirely different from those in the training and test data described in Section 2 are collect"
P18-2122,L16-1283,0,0.0531197,"Missing"
P18-2122,C16-1257,0,0.0528051,"Missing"
P19-1635,W14-4012,0,0.0568898,"Missing"
P19-1635,D14-1181,0,0.00509517,"onsidered the magnitude be6308 Magnitude Decimal 1 2 3 4 5 6 &gt;6 Range 0≤m&lt;1 1 ≤ m &lt; 10 10 ≤ m &lt; 102 102 ≤ m &lt; 103 103 ≤ m &lt; 104 104 ≤ m &lt; 105 105 ≤ m &lt; 106 106 ≤ m Ratio 23.24 37.53 25.36 12.21 1.12 0.29 0.23 0.01 Table 3: Distribution of numerals in the dataset. Model LR CNN GRU BiGRU CRNN CNN-capsule GRU-capsule BiGRU-capsule fore the decimal point, i.e., 10.08 was classified as a 2nd magnitude. Finally, we separate the dataset into training set and test set of sizes 500k and 100k, respectively. 4 4.1 Empirical Study Models We adopt seven different architectures for our task, including CNN (Kim, 2014), GRU (Cho et al., 2014), BiGRU, CRNN (Choi et al., 2017), CNNcapsule (Sabour et al., 2017), GRU-capsule, and BiGRU-capsule (Wang et al., 2018b). In our models, each word in the input sentence is represented as a d-dimensional vector with word embeddings, and all the words are concatenated in as a d × l matrix, where l denotes the sentence length. Some preprocessing was performed on the data. We transformed all characters to lowercase. The sentence representation was padded to the maximum length of an instance. The target numeral to be inferred is replaced with a special token &lt;TRT&gt;. Appendice"
P19-1635,P18-1196,0,0.112945,"Missing"
P94-1032,A88-1019,0,0.507466,"the attachment relations among the constituents. However, parsing the text completely is very difficult, since various ambiguities cannot be resolved solely by syntactic or semantic information. Do we really need to fully parse the texts in every application? Some researchers apply shallow or partial parsers (Smadja, 1991; Hindle, 1990) to acquiring specific patterns from texts. These tell us that it is not necessary to completely parse the texts for some applications. This paper will propose a probabilistic partial parser and incorporate linguistic knowledge to extract noun 2. Previous Works Church (1988) proposes a part of speech tagger and a simple noun phrase extractor. His noun phrase extractor brackets the noun phrases of input tagged texts according to two probability matrices: one is starting noun phrase matrix; the other is ending noun phrase matrix. The methodology is a simple version of Garside and Leech&apos;s probabilistic parser (1985). Church lists a sample text in the Appendix of his paper to show the performance of his work. It demonstrates only 5 out of 248 noun phrases are omitted. Because the tested text is too small to assess the results, the experiment for large volume of texts"
P94-1032,E85-1024,0,0.0744427,"Missing"
P94-1032,P90-1034,0,0.0200321,"the texts, we will understand the texts to some extent. This consideration is also captured by theories of discourse analysis, such as Discourse Representation Theory (Kamp, 1981). Traditionally, to make out the noun phrases in a text means to parse the text and to resolve the attachment relations among the constituents. However, parsing the text completely is very difficult, since various ambiguities cannot be resolved solely by syntactic or semantic information. Do we really need to fully parse the texts in every application? Some researchers apply shallow or partial parsers (Smadja, 1991; Hindle, 1990) to acquiring specific patterns from texts. These tell us that it is not necessary to completely parse the texts for some applications. This paper will propose a probabilistic partial parser and incorporate linguistic knowledge to extract noun 2. Previous Works Church (1988) proposes a part of speech tagger and a simple noun phrase extractor. His noun phrase extractor brackets the noun phrases of input tagged texts according to two probability matrices: one is starting noun phrase matrix; the other is ending noun phrase matrix. The methodology is a simple version of Garside and Leech&apos;s probabi"
P94-1032,J93-1005,0,0.0208937,"nks,&quot; in Principle-Based Parsing, Berwick, Abney and Tenny (Eds.), Khiwer Academic Publishers, pp. 257-278. Bourigault, Didier (1992), &quot;Surface Grammatical Analysis for the Extraction of Terminological Noun Phrases,&quot; Proceedings of the 15th International (18) to pull the wool over a husband eyes to sell the books of my uncle Conference on Computational Linguistics, COLING-92, Vol. III, Nantes, France, pp. 977-98 I. In contrast, the noun phrase &quot;the books of my uncle&quot; is so called maximal noun phrase in current context. As the result, we conclude that if we do not resolve PPattachment problem (Hindle and Rooth, 1993), to the expected extent, we will not extract the maximal noun phrases. In our work, the probabilistic chunker decides the implicit boundaries between words and the NPTRACTOR connects the adjacent noun chunks. When a noun chunk is followed by a preposition chunk, we do not connect the two chunks except the preposition chunk is led by &quot;of&apos; preposition. Comparing with other works, our results are evaluated by a parsed corpus automatically and show the high precision. Although we do not point out the exact recall, we provide estimated values. The testing scale is large enough (about 150,000 words"
P94-1032,W93-0306,0,0.402168,"Missing"
P94-1032,C92-3150,0,\N,Missing
P98-1036,C96-1030,0,0.0417991,"Missing"
P98-1036,C96-1039,1,0.846757,"Missing"
P98-1036,M95-1018,0,0.0222721,"Missing"
P98-1036,W93-0105,0,0.110103,"Missing"
P98-1036,W93-0104,0,0.093885,"Missing"
P98-1036,W93-0114,0,0.0542058,"Missing"
P98-1036,W97-0315,0,0.0429417,"Missing"
P98-1036,M95-1006,0,0.0827757,"Missing"
P98-1036,M92-1024,0,\N,Missing
P99-1028,bian-chen-1998-integrating,1,0.927898,"enstette, 1996; Davis, 1997), randomly select N (Ballesteros and Croft, 1996; Kwok 1997) and select best N (Hayashi, Kikui and Susaki, 1997; Davis 1997). Corpus-based approaches exploit sentence-aligned corpora (Davis and Dunning, 1996) and document-aligned corpora (Sheridan and Ballerini, 1996). These two approaches are complementary. Dictionary provides translation candidates, and corpus provides context to fit user intention. Coverage of dictionaries, alignment performance and domain shift of corpus are major problems of these two approaches. Hybrid approaches (Ballesteros and Croft, 1998; Bian and Chen, 1998; Davis 1997) integrate both lexical and corpus knowledge. All the above approaches deal with the translation ambiguity problem in query translation. Few touch on translation ambiguity and target polysemy together. This paper will study the multiplication effects of translation ambiguity and target polysemy in cross-language information retrieval systems, and propose a new translation method to resolve these problems. Section 2 shows the effects of translation ambiguity and target polysemy in Chinese-English and EnglishChinese information retrievals. Section 3 presents several models to revolv"
P99-1028,W89-0240,0,0.0151168,"Missing"
P99-1028,H89-2012,0,\N,Missing
R11-1021,M95-1012,0,0.0176606,"Missing"
R11-1021,W08-0336,0,0.0454333,"Missing"
R11-1021,N09-2061,0,0.0365927,"Missing"
R11-1021,W10-4103,1,0.826054,"heme for text segmentation is 2-tag set in which two types of labels, “start” and “non-start”, are used. As shown in Table 1, the ratios of the pauses to the stops are 2.40 in Sinica dataset and 2.25 in Master dataset. In other words, the classification between the class “start” and the class “non-start” is unbalanced. On average, a stop-ending clause appears after two to three pause-ending clauses. Rather than the 2-tag set scheme, a longer tagging schemes, k-tag sets, are reported better in Chinese word segmentation (Xue, 2003; Zhao et al., 2006) and Classical Chinese sentence segmentation (Huang et al, 2010). We experiment different k-tag set schemes in pause and stop labeling. A fragment could be labeled with one of the following tags: L1, L2, …, Lk-3, R, M, and S. L means Left boundary. The tag Li (1ik-3) labeled on fragment f denotes f is the i-th fragment of a sentence. The tag R, which means Right boundary, marks the last fragment of a sentence. The fragments between Lk-3 and R are labeled with the tag M (Middle). A single fragment forming a sentence is labeled with the tag S (Single). The Markov Chain of the k-tag set tagging scheme is shown in Figure 1. For example, the fragments in the"
R11-1021,W04-1101,0,0.391748,"Missing"
R11-1021,P03-1056,0,0.0270029,"e two datasets is 2.49 characters. For this reason, all the characters in most fragments are able to be captured within the leftmost and the rightmost trigrams. Part-of-Speech Level (POS): The features include the leftmost and the rightmost POS unigrams, bigrams, and trigrams in a fragment. Besides, the presences or absences of certain POS tags in a fragment are also checked. These tags include noun, pronoun, verb, conjunction, particle, adverb, adjective, and their combinations. Figure 2. Extracting the top-level structure from the syntax tree We perform POS tagging with the Stanford parser (Levy and Manning, 2003). Syntactic Level (S): We get the syntactic tree of a fragment by the Stanford parser, and extract the structure of the upper three levels, which forms the fundamental composition of the fragment. In addition, the leftmost path and the rightmost path of the tree are also extracted. Figure 2 shows the upper three levels of the parsing tree, the leftmost path, and the rightmost path of the sample fragment in the bold edges. For instance, the structure of the upper three levels in Figure 2 formed in preorder format is IP(IP(ADVP NP VP) VP(VV AS NP)), the leftmost path is IP(IP(ADVP(AD))), and the"
R11-1021,W03-1703,0,0.0486509,"Missing"
R11-1021,2005.eamt-1.37,0,0.0750046,"Missing"
R11-1021,O03-4002,0,0.0261339,"s larger than that in Labeling Method Tagging Scheme The typical tagging scheme for text segmentation is 2-tag set in which two types of labels, “start” and “non-start”, are used. As shown in Table 1, the ratios of the pauses to the stops are 2.40 in Sinica dataset and 2.25 in Master dataset. In other words, the classification between the class “start” and the class “non-start” is unbalanced. On average, a stop-ending clause appears after two to three pause-ending clauses. Rather than the 2-tag set scheme, a longer tagging schemes, k-tag sets, are reported better in Chinese word segmentation (Xue, 2003; Zhao et al., 2006) and Classical Chinese sentence segmentation (Huang et al, 2010). We experiment different k-tag set schemes in pause and stop labeling. A fragment could be labeled with one of the following tags: L1, L2, …, Lk-3, R, M, and S. L means Left boundary. The tag Li (1ik-3) labeled on fragment f denotes f is the i-th fragment of a sentence. The tag R, which means Right boundary, marks the last fragment of a sentence. The fragments between Lk-3 and R are labeled with the tag M (Middle). A single fragment forming a sentence is labeled with the tag S (Single). The Markov Chain of t"
R11-1021,Y06-1012,0,0.0252148,"an that in Labeling Method Tagging Scheme The typical tagging scheme for text segmentation is 2-tag set in which two types of labels, “start” and “non-start”, are used. As shown in Table 1, the ratios of the pauses to the stops are 2.40 in Sinica dataset and 2.25 in Master dataset. In other words, the classification between the class “start” and the class “non-start” is unbalanced. On average, a stop-ending clause appears after two to three pause-ending clauses. Rather than the 2-tag set scheme, a longer tagging schemes, k-tag sets, are reported better in Chinese word segmentation (Xue, 2003; Zhao et al., 2006) and Classical Chinese sentence segmentation (Huang et al, 2010). We experiment different k-tag set schemes in pause and stop labeling. A fragment could be labeled with one of the following tags: L1, L2, …, Lk-3, R, M, and S. L means Left boundary. The tag Li (1ik-3) labeled on fragment f denotes f is the i-th fragment of a sentence. The tag R, which means Right boundary, marks the last fragment of a sentence. The fragments between Lk-3 and R are labeled with the tag M (Middle). A single fragment forming a sentence is labeled with the tag S (Single). The Markov Chain of the k-tag set tagging"
R11-1021,J02-3002,0,\N,Missing
R11-1021,Y96-1018,0,\N,Missing
S17-2144,D14-1108,0,0.031547,"ependency-based approach (2.1) Stanford Parser (ED-S) A tweet is parsed by Stanford dependency parser (Marneffe, 2006). To reduce the effects of out of vocabulary (OOV) words in parsing, cashtags and company names are replaced by common names like “Bob”. A dependency tree for n-word tweet is composed of n triples in the form of dep(wordi,wordj), where wordi and wordj has a dependency dep, wordi is a parent of wordj, and wordj is a child of wordi. We take the ancestors and the decedents of a target as its span. (2.2) TweeboParser (ED-T) TweeboParser is a dependency parser, designed for tweets (Kong et al., 2014). It tries to deal with the following challenges: token selection, multiword expressions, multiple roots, and structure within noun phrases. The multiple roots property Figure 1. Structure of a financial tweet 848 tends to provide shorter span than ED-S with the same extracted algorithm. The average length of the tweets, manual spans, EP spans, ED-S and ED-T spans are 17.61, 6.26, 12.17, 10.27, and 7.78 words, respectively. Compared with 140-character word limit in twitter, a financial tweet is very short. In particular, manual spans are much shorter. Besides text span in tweets, tweets may co"
S17-2144,C16-1251,0,0.0310049,"Missing"
S17-2144,S17-2089,0,0.0517844,"Missing"
S18-1171,Q17-1010,0,0.0213967,"” word embedding features do not work, we turn to more abstract features. Let sim1 and sim2 be the cosine similarity of the vector of a 1028 1 https://code.google.com/archive/p/word2vec/ to the vector of w1 and w2 respectively. We compare the values sim1 and sim2 . The rationale is that if a word w has an attribute a, then it tends to, though not necessarily, be more similar to a than other words without a. The following six embedding models are experimented with. The embedding size is fixed to 300. 1. W2V(GNews): The standard Word2vec model as described in Section 2.1. 2. fastText: fastText (Bojanowski et al., 2017) is a modification of Word2vec that takes subword information into account. We adopt the pretrained vectors trained on 6B tokens2 . 3. Numberbatch: Numberbatch embeddings are built upon several corpus-based word embeddings and improved by retrofitting on ConceptNet, a large semantic network containing an abundance of general knowledge (Speer et al., 2017). We use the pre-trained embeddings of English concepts3 . 4. GloVe(Common Crawl): The GloVe model (Pennington et al., 2014) obtains word representation according to global co-occurrence statistics. We use the pre-trained vectors trained on 84"
S18-1171,S18-1117,0,0.0470901,"mpetence. Our knowledge about what is a “subway”, for example, may contain “it is a kind of train that runs underground”. Also, discriminating things is an important mechanism for teaching and learning. For example, if we would like to explain how a “plate” is different from a “bowl”, we may use expressions like “a plate is flatter” or “a bowl is deeper”. All these examples show that one form of semantic difference is a discriminative attribute which applies to one of the two concepts being compared but does not apply to the other. In the SemEval-2018 Capturing Discriminative Attributes task (Krebs et al., 2018), participants need to put forward semantic models that are aware of semantic differences. A data instance consists of a triple and a label. In this paper, we denote a triple with < w1 , w2 , a &gt;, in which w1 and w2 are the two words (concepts) to be compared, and a is an attribute. The label is either positive (1) or negative (0). In a positive example, a is an attribute of w1 but not an attribute of w2 . For negative examples, there are two cases: 1) both w1 and w2 have attribute a ; 2) neither w1 nor w2 has attribute a. In this task, a is limited to visual ones such as color and shape. The"
S18-1171,P16-2035,0,0.0188441,"ot an attribute of w2 . For negative examples, there are two cases: 1) both w1 and w2 have attribute a ; 2) neither w1 nor w2 has attribute a. In this task, a is limited to visual ones such as color and shape. The evaluation metric is the macro-averaged F1 score of the positive and the negative classes. Visual attribute learning has been investigated by past researchers. Silberer et al. (2013) build a dataset of concept-level attribute annotations based on images in ImageNet (Deng et al., 2009). For each attribute, they train a classifier to predict its presence or absence in the input image. Lazaridou et al. (2016) propose a model that does not learn visual attributes explicitly, but learns discriminativeness. Their model predicts whether an attribute can be used to discriminate a referent from a context. Both the referent and the context are represented by visual instances sampled from ImageNet. This setting is similar to that of this SemEval task. However, one critical difference is that in this task, the set of attributes is open. The dataset is partitioned so that all the attributes in the test set are unseen in the training set, which makes this task more challenging. The use of word embeddings for"
S18-1171,N15-1098,0,0.030501,"However, according to the results shown in Table 1, the models considering solely a part of every triple can still “learn” some information from the training set (majority-class baseline accuracy on the training set: 0.6383). Some models with partial information even achieve better validation scores than that with complete information. This indicates that the models overfit to the vocabulary of the training set. At the test time, all the attributes are unknown, so the model cannot make effective predictions. In fact, these results are similar to the lexical memorization phenomenon reported by Levy et al. (2015) on the hypernym detection task. 2.2 Embeddings Similarity Difference Because “raw” word embedding features do not work, we turn to more abstract features. Let sim1 and sim2 be the cosine similarity of the vector of a 1028 1 https://code.google.com/archive/p/word2vec/ to the vector of w1 and w2 respectively. We compare the values sim1 and sim2 . The rationale is that if a word w has an attribute a, then it tends to, though not necessarily, be more similar to a than other words without a. The following six embedding models are experimented with. The embedding size is fixed to 300. 1. W2V(GNews)"
S18-1171,D14-1162,0,0.0813083,"g size is fixed to 300. 1. W2V(GNews): The standard Word2vec model as described in Section 2.1. 2. fastText: fastText (Bojanowski et al., 2017) is a modification of Word2vec that takes subword information into account. We adopt the pretrained vectors trained on 6B tokens2 . 3. Numberbatch: Numberbatch embeddings are built upon several corpus-based word embeddings and improved by retrofitting on ConceptNet, a large semantic network containing an abundance of general knowledge (Speer et al., 2017). We use the pre-trained embeddings of English concepts3 . 4. GloVe(Common Crawl): The GloVe model (Pennington et al., 2014) obtains word representation according to global co-occurrence statistics. We use the pre-trained vectors trained on 840B tokens of Common Crawl4 . 5. Sense(enwiki)-c: Sense vectors may encode more fine-grained semantic information than word vectors do, so we also experimented with sense vectors. We perform word sense disambiguation (WSD) on the English Wikipedia corpus to get a sense-annotated corpus, using the Adapted Lesk algorithm implemented in pywsd5 . The sense inventory is based on synsets in WordNet. We train a Word2vec Skip-gram (SG) model with this corpus to obtain sense vectors. To"
S18-1171,P15-2119,0,0.0196331,"ttributes explicitly, but learns discriminativeness. Their model predicts whether an attribute can be used to discriminate a referent from a context. Both the referent and the context are represented by visual instances sampled from ImageNet. This setting is similar to that of this SemEval task. However, one critical difference is that in this task, the set of attributes is open. The dataset is partitioned so that all the attributes in the test set are unseen in the training set, which makes this task more challenging. The use of word embeddings for detecting semantic properties is studied by Rubinstein et al. (2015). They focus on a fixed set of properties and train a binary classifier for each property. Their 1027 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 1027–1033 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics results indicate that word embeddings capture taxonomic properties (e.g. “an animal”) better than attributive properties (e.g. “is fast”), possibly because attributive signal is weak in text. In this task, most visual attributes are attributive properties. The signal of “visual” attributes can be even weake"
S18-1171,P13-1056,0,0.0237999,"enote a triple with < w1 , w2 , a &gt;, in which w1 and w2 are the two words (concepts) to be compared, and a is an attribute. The label is either positive (1) or negative (0). In a positive example, a is an attribute of w1 but not an attribute of w2 . For negative examples, there are two cases: 1) both w1 and w2 have attribute a ; 2) neither w1 nor w2 has attribute a. In this task, a is limited to visual ones such as color and shape. The evaluation metric is the macro-averaged F1 score of the positive and the negative classes. Visual attribute learning has been investigated by past researchers. Silberer et al. (2013) build a dataset of concept-level attribute annotations based on images in ImageNet (Deng et al., 2009). For each attribute, they train a classifier to predict its presence or absence in the input image. Lazaridou et al. (2016) propose a model that does not learn visual attributes explicitly, but learns discriminativeness. Their model predicts whether an attribute can be used to discriminate a referent from a context. Both the referent and the context are represented by visual instances sampled from ImageNet. This setting is similar to that of this SemEval task. However, one critical differenc"
tang-chen-2012-mining,D08-1015,1,\N,Missing
tang-chen-2012-mining,W02-1011,0,\N,Missing
W00-1202,kilgarriff-rosenzweig-2000-english,0,0.0600266,"Missing"
W00-1202,P96-1006,0,0.181179,"Missing"
W00-1202,W89-0240,0,0.0152665,"Missing"
W00-1202,J98-1001,0,\N,Missing
W00-1202,H89-2012,0,\N,Missing
W02-2017,P98-1069,0,\N,Missing
W02-2017,C98-1066,0,\N,Missing
W02-2017,C96-1039,1,\N,Missing
W02-2017,W98-1005,0,\N,Missing
W02-2017,P94-1010,0,\N,Missing
W02-2017,P98-1036,1,\N,Missing
W02-2017,C98-1036,1,\N,Missing
W02-2017,P98-2220,0,\N,Missing
W02-2017,C98-2215,0,\N,Missing
W02-2017,P97-1017,0,\N,Missing
W03-1304,C00-1030,0,0.316976,"In the past, named entities extraction mainly focuses on general domains. Recently, large amount of scientific documents has been published, in particular for biomedical domains. Several attempts have been made to mine knowledge from biomedical documents (Hirschman, et al., 2002). One of their goals is to construct a knowledge base automatically and to find new information embedded in documents (Craven and Kumlien, 1999). Similar information extraction works have been explored on this domain. Named entities like protein names, gene names, drug names, disease names, and so on, were recognized (Collier, et al., 2000; Fukuda, et al., 1998; Olsson, et al., 2002; Rindflesch, et al., 2000). Besides, the relationships among these entities, e.g., protein-protein, protein-gene, drug-gene, drug-disease, etc., were extracted (Blaschke, et al., 1999; Frideman, et al., 2001; Hou and Chen, 2002; Marcotte, et al., 2001; Ng and Wong, 1999; Park, et al., 2001; Rindflesch, et al., 2000; Thomas, et al., 2000; Wong, 2001). Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). The related technologies have been applied to terminological extraction, natural language generation, parsi"
W03-1304,C02-1110,0,0.815506,"focuses on general domains. Recently, large amount of scientific documents has been published, in particular for biomedical domains. Several attempts have been made to mine knowledge from biomedical documents (Hirschman, et al., 2002). One of their goals is to construct a knowledge base automatically and to find new information embedded in documents (Craven and Kumlien, 1999). Similar information extraction works have been explored on this domain. Named entities like protein names, gene names, drug names, disease names, and so on, were recognized (Collier, et al., 2000; Fukuda, et al., 1998; Olsson, et al., 2002; Rindflesch, et al., 2000). Besides, the relationships among these entities, e.g., protein-protein, protein-gene, drug-gene, drug-disease, etc., were extracted (Blaschke, et al., 1999; Frideman, et al., 2001; Hou and Chen, 2002; Marcotte, et al., 2001; Ng and Wong, 1999; Park, et al., 2001; Rindflesch, et al., 2000; Thomas, et al., 2000; Wong, 2001). Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). The related technologies have been applied to terminological extraction, natural language generation, parsing, and so on. This paper deals with a speci"
W03-1304,C96-1039,1,\N,Missing
W03-1304,E99-1043,0,\N,Missing
W03-1501,P02-1051,0,0.16836,"Missing"
W03-1501,C96-1039,1,0.786948,"a named entity should be meaning-translated and which part should be phoneme-transliterated. An application of the results on cross language information retrieval is also shown. 1 Introduction Named entities are major components of a document. Capturing named entities is a fundamental task to understanding documents (MUC, 1998). Several approaches have been proposed to recognize these types of terms. For example, corpus-based methods are employed to extract Chinese personal names, and rule-based methods are used to extract Chinese date/time expressions and monetary and percentage expressions (Chen and Lee, 1996; Chen, Ding and Tsai, 1998). In the past, named entity extraction mainly focuses on general domains and is employed to various applications such as information retrieval (Chen, Ding and Tsai, 1998), question-answering (Lin, et al., 2001), and so on. Recently, several attempts have been extended to mine knowledge from biomedical documents (Hirschman, et al., 2002). Most of the previous approaches dealt with monolingual named entity extraction. Chen et al. (1998) extended it to cross-language information retrieval. A grapheme-based model was proposed to compute the similarity between Chinese tr"
W03-1501,O00-1005,1,0.930882,"past, named entity extraction mainly focuses on general domains and is employed to various applications such as information retrieval (Chen, Ding and Tsai, 1998), question-answering (Lin, et al., 2001), and so on. Recently, several attempts have been extended to mine knowledge from biomedical documents (Hirschman, et al., 2002). Most of the previous approaches dealt with monolingual named entity extraction. Chen et al. (1998) extended it to cross-language information retrieval. A grapheme-based model was proposed to compute the similarity between Chinese transliteration name and English name. Lin and Chen (2000) further classified the works into two directions – say, forward transliteration (Wan and Verspoor, 1998) and backward transliteration (Chen et al., 1998; Knight and Graehl, 1998), and proposed a phoneme-based model. Lin and Chen (2002) employed a machine learning approach to determine phonetic similarity scores for machine transliteration. AI-Onaizan and Knight (2002) investigated the translation of Arabic named entities to English using monolingual and bilingual resources. The past works on multilingual named entities emphasizes on the transliteration issues. However, the transformation betw"
W03-1501,W02-2017,1,0.795216,"mpts have been extended to mine knowledge from biomedical documents (Hirschman, et al., 2002). Most of the previous approaches dealt with monolingual named entity extraction. Chen et al. (1998) extended it to cross-language information retrieval. A grapheme-based model was proposed to compute the similarity between Chinese transliteration name and English name. Lin and Chen (2000) further classified the works into two directions – say, forward transliteration (Wan and Verspoor, 1998) and backward transliteration (Chen et al., 1998; Knight and Graehl, 1998), and proposed a phoneme-based model. Lin and Chen (2002) employed a machine learning approach to determine phonetic similarity scores for machine transliteration. AI-Onaizan and Knight (2002) investigated the translation of Arabic named entities to English using monolingual and bilingual resources. The past works on multilingual named entities emphasizes on the transliteration issues. However, the transformation between named entities in different languages is not transliteration only. The mapping may be a combination of meaning translation and/or phoneme transliteration. The following five English-Chinese examples show this issue. The symbol A ⇔ B"
W03-1501,P98-2220,0,0.0543715,"ns such as information retrieval (Chen, Ding and Tsai, 1998), question-answering (Lin, et al., 2001), and so on. Recently, several attempts have been extended to mine knowledge from biomedical documents (Hirschman, et al., 2002). Most of the previous approaches dealt with monolingual named entity extraction. Chen et al. (1998) extended it to cross-language information retrieval. A grapheme-based model was proposed to compute the similarity between Chinese transliteration name and English name. Lin and Chen (2000) further classified the works into two directions – say, forward transliteration (Wan and Verspoor, 1998) and backward transliteration (Chen et al., 1998; Knight and Graehl, 1998), and proposed a phoneme-based model. Lin and Chen (2002) employed a machine learning approach to determine phonetic similarity scores for machine transliteration. AI-Onaizan and Knight (2002) investigated the translation of Arabic named entities to English using monolingual and bilingual resources. The past works on multilingual named entities emphasizes on the transliteration issues. However, the transformation between named entities in different languages is not transliteration only. The mapping may be a combination o"
W03-1501,P98-1036,1,\N,Missing
W03-1501,C98-1036,1,\N,Missing
W03-1501,C98-2215,0,\N,Missing
W03-1501,J98-4003,0,\N,Missing
W04-0703,W99-0211,0,0.0353213,"Missing"
W04-0703,P98-1012,0,0.0352179,"hat translation after clustering is better than translation before clustering, and translation deferred to sentence clustering, which reduces the propagation of translation errors, is most promising. Fukumoto and Suzuki (2000) proposed concepts of topic words and event words for event tracking. They introduced more semantic approach for feature selection than the approach of parts of speech. Wong, Kuo and Chen (2001) employed these concepts to select informative words for headline generation, and to rank the extracted sentences in multi-document summarization (Kuo, Wong, Lin, and Chen, 2002). Bagga and Baldwin (1998) proposed entitybased cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document. Azzam, Humphreys, and Gaizauskas (1999) proposed a primitive model for text summarization using co-reference chains as well. Silber and McCoy (2002) proposed a text summarization model using lexical chains and showed that proper nouns and anaphora resolution is indispensable. The two semantics-based feature selection approaches, i.e., co-reference chains"
W04-0703,J02-4004,0,0.0247656,"h. Wong, Kuo and Chen (2001) employed these concepts to select informative words for headline generation, and to rank the extracted sentences in multi-document summarization (Kuo, Wong, Lin, and Chen, 2002). Bagga and Baldwin (1998) proposed entitybased cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document. Azzam, Humphreys, and Gaizauskas (1999) proposed a primitive model for text summarization using co-reference chains as well. Silber and McCoy (2002) proposed a text summarization model using lexical chains and showed that proper nouns and anaphora resolution is indispensable. The two semantics-based feature selection approaches, i.e., co-reference chains and event words, are complementary in some sense. The former denotes equivalence classes of noun phrases, and the latter considers both nominal and verbal features, which appear across paragraphs. This paper will employ both co-reference chains and event words for temporal event clustering. An event clustering system using co-reference chains is described in Section 2. The evaluation meth"
W04-0703,C98-1012,0,\N,Missing
W04-1209,C02-1110,0,0.0225388,"rds at hand, we also computed the occurrences of these words in each sentence, given an abstract. Each abstract is then represented by the number of occurrences of these words ni the 9 sentences respectively, i.e., the feature vector is 94,554 in length. Classification based on this type of features is denoted the sentence-wise bag of words model in the rest of this paper. Combining these two models, we got totally 94,563 features. Since we are extracting sentences discussing gene functions, it’s reasonable to expect gene or protein names in the GeneRIF sentence. Therefore, we employed Yapex (Olsson et al., 2002) and GAPSCORE (Chang et al., 2004) protein/gene name detectors to count the number of protein/gene names in each of the 9 sentences, resulting in 94,581 features. 4 Results and Discussions The performance measures are based on Dice coefficient, which calculates the overlap between the candidate GeneRIF and actual GeneRIF. Classic Dice (CD) is the classic Dice formula using a common stop word list and the Porter stemming algorithm. Due to lack of space, we referred you to the Genomics track overview for the other three modifications of CD (Hersh and Bhupatiraju, 2003). The evaluation results ar"
W04-1215,W03-1305,0,0.0139799,"iwan University 1 Roosevelt Road, Section 4, Taipei, Taiwan, 106 {clee, wjhou}@nlg.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). Hou and Chen (2003) showed that protein/gene collocates are capable of assisting existing protein/gene taggers. In this paper, we addressed this task as a multi-class classification problem with SVMs and extended the idea of collocation to generate features at word and pattern level in our method. Existing protein/gene recognizers were used to perform feature extraction as well. The rest of"
W04-1215,W03-1308,0,0.0284748,"Roosevelt Road, Section 4, Taipei, Taiwan, 106 {clee, wjhou}@nlg.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). Hou and Chen (2003) showed that protein/gene collocates are capable of assisting existing protein/gene taggers. In this paper, we addressed this task as a multi-class classification problem with SVMs and extended the idea of collocation to generate features at word and pattern level in our method. Existing protein/gene recognizers were used to perform feature extraction as well. The rest of this paper is organized as f"
W04-1215,W03-1306,0,0.0210852,"Missing"
W04-1215,W03-1309,0,0.0225138,"Missing"
W04-1215,C00-1030,0,0.0124498,"ion The volumn of on-line material in the biomedical field has been growing steadily for more than 20 years. Several attempts have been made to mine knowledge from biomedical documents, such as identifying gene/protein names, recognizing protein interactions, and capturing specific relations in databases. Among these, named entity recognition is a fundamental step to mine knowledge from biological articles. Previous approaches on biological named entity extraction can be classified into two types – rulebased (Fukuda et al., 1998; Olsson et al., 2002; Tanabe and Wilbur, 2002) and corpus-based (Collier et al., 2000; Chang et al., 2004). Yapex (Olsson et al., 2002) implemented some heuristic steps described by Fukuda, et al., and applied filters and knowledge bases to remove false alarms. Syntactic information obtained from the parser was incorporated as well. GAPSCORE (Chang et al., 2004) scored words on the basis of statistical models that quantifie d their appearance, morphology and context. The models includes Naive Bayes (Manning and Schutze, 1999), Maximum Entropy (Ratnaparkhi, 1998) and 2 Methods Most of the works in the past on recognizing named entities in the biomedical domain focused on identi"
W04-1215,W03-1304,1,0.853391,"POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). Hou and Chen (2003) showed that protein/gene collocates are capable of assisting existing protein/gene taggers. In this paper, we addressed this task as a multi-class classification problem with SVMs and extended the idea of collocation to generate features at word and pattern level in our method. Existing protein/gene recognizers were used to perform feature extraction as well. The rest of this paper is organized as follows. The methods used in this study are introduced in Section 2. The experimental results are shown and discussed in Section 3. Finally, Section 4 concludes the remarks and lists some future wor"
W04-1215,W02-0301,0,0.0229396,"gineering National Taiwan University 1 Roosevelt Road, Section 4, Taipei, Taiwan, 106 {clee, wjhou}@nlg.csie.ntu.edu.tw, hh_chen@csie.ntu.edu.tw Support Vector Machines (Burges, 1998). GAPSCORE also used Brill’s tagger (Brill, 1994) to get the POS tag to filter out some words that are clearly not gene or protein names. Efforts have been made (Hou and Chen, 2002, 2003; Tsuruoka and Tsujii, 2003) to improve the performance. The nature of classification makes it possible to integrate existing approaches by extracting good features from them. Several works employing SVM classifier have been done (Kazama et al., 2002; Lee et al., 2003; Takeuchi and Collier, 2003; Yamamoto et al., 2003), and will be discussed further in the rest of this paper. Collocation denotes two or more words having strong relationships (Manning and Schutze, 1999). Hou and Chen (2003) showed that protein/gene collocates are capable of assisting existing protein/gene taggers. In this paper, we addressed this task as a multi-class classification problem with SVMs and extended the idea of collocation to generate features at word and pattern level in our method. Existing protein/gene recognizers were used to perform feature extraction as"
W04-1215,C02-1110,0,\N,Missing
W10-4103,P03-1035,0,0.064771,"Missing"
W10-4103,W04-3209,0,0.0693666,"Missing"
W10-4103,P05-1056,0,0.045505,"Missing"
W10-4103,C04-1081,0,0.179925,"ndled with a dictionary predefined by Chinese language experts or extracted from the corpus automatically. However, it is impossible to maintain a dictionary of the infinite number of sentences and clauses. For these reasons, the Classical Chinese sentence segmentation problem is more challenging. Methods of Chinese word segmentation can be mainly classified into heuristic rule-based approaches, statistical machine learning approaches, and hybrid approaches. Hybrid approaches combine the advantages of heuristic and statistical approaches to achieve better results (Gao et al., 2003; Xue, 2003; Peng et al., 2004). Xue (2003) transformed the Chinese word segmentation problem into a tagging problem. For a given sequence of Chinese characters, the author applies a Maxent tagger to assign each character one of four positions-of-character (POC) tags, and then coverts the tagged sequence into a segmented sequence. The four POC tags used in Xue (2003) denote the positions of characters within a word. For example, the first character of a word is tagged “left boundary”, the last character of a word is tagged “right boundary”, the middle character of a word is tagged “middle”, and a single character that forms"
W10-4103,O03-4002,0,0.496095,"s can be handled with a dictionary predefined by Chinese language experts or extracted from the corpus automatically. However, it is impossible to maintain a dictionary of the infinite number of sentences and clauses. For these reasons, the Classical Chinese sentence segmentation problem is more challenging. Methods of Chinese word segmentation can be mainly classified into heuristic rule-based approaches, statistical machine learning approaches, and hybrid approaches. Hybrid approaches combine the advantages of heuristic and statistical approaches to achieve better results (Gao et al., 2003; Xue, 2003; Peng et al., 2004). Xue (2003) transformed the Chinese word segmentation problem into a tagging problem. For a given sequence of Chinese characters, the author applies a Maxent tagger to assign each character one of four positions-of-character (POC) tags, and then coverts the tagged sequence into a segmented sequence. The four POC tags used in Xue (2003) denote the positions of characters within a word. For example, the first character of a word is tagged “left boundary”, the last character of a word is tagged “right boundary”, the middle character of a word is tagged “middle”, and a single"
W11-3703,D08-1015,1,0.753326,"r responses forms writer and reader emotiontagged corpora, respectively, facilitating writer emotion and reader emotion mining. Previous studies (e.g., Yang, Lin, and Chen 2007a; Yang, Lin, and Chen 2007b; Yang, Lin, and Chen 2008) have used an emotion-tagged weblog corpus to investigate the ways in which people express their emotions, trying to detect writers’ affective status with textual contents they have written. While these studies aimed to perform emotion analysis and detection from the writer’s perspective, a few papers have studied reader emotion generation (Lin, Yang, and Chen 2007; Lin and Chen 2008; Lin, Yang, and Chen, 2008) using an emotion-tagged news corpus, modeling how readers react to articles on news websites. To study how writer emotion affects readers’ feelings, Yang, Lin and Chen (2009) used the Yahoo! Kimo Blog and Yahoo! Kimo News to produce a dataset annotated with both writer and reader emotions. They constructed a documentlevel reader-emotion classifier using the Yahoo! Kimo News corpus, and applied the resulting classifier on the Yahoo! Kimo Blog corpus. In this way, a new blog corpus labeled with both writer and reader emotions was obtained. The major problem with the"
W11-3703,O10-1013,0,0.0607958,"Missing"
W11-3703,P07-2034,1,\N,Missing
W11-3703,W10-0202,0,\N,Missing
W12-1636,W00-1205,0,0.167476,"e tree of the sentence. Shown in Figure 2 is the structure of sample (S3) based on the syntactic tree generated by the Stanford parser. However, it is clear that the 262 (S3) 目前雖然還只能在圖片上讓女性露露臉 (“Although women only appear in the pictures”)， 但 未 來 女 性 的 貢 獻 (“The contribution of women”)，將是教科書另一個著墨的重點 (“Will be another major focus in textbooks in the future”)。 Figure 1: Relation structure of sample (S1). Figure 2: Structure of sample (S3) based on the syntactic tree generated by the Stanford parser. Figure 3: Correct structure of sample (S3) 2 Dataset The corpus is based on the Sinica Treebank (Huang et al., 2000). A Total of 81 articles are randomly selected from the Sino and Travel sets. All the sentences that consist of two, three, and four clauses are extracted for relation and structure labeling by native Chinese speakers. A web-based system is developed for annotation. The annotation scheme is designed as follows. An annotator first signs in to the annotation system, and a list of sentences that are assigned to the annotator are given. The annotator labels the sentences one by one in the system. A sentence is split into clauses along commas, and all of its feasible binary tree structures are show"
W12-1636,I11-1170,1,0.883314,"Missing"
W12-1636,miltsakaki-etal-2004-penn,0,0.0408712,"Missing"
W12-1636,D09-1036,0,0.0323717,"ency pairs from the sentence. A dependency pair consists of two arguments, i.e., the governor and the dependent, and their types. We are interested in those dependency pairs that are across two clauses. That is, the two arguments of a pair are from different clauses. In our assumption, the clauses have a closer connection if some dependencies occur between them. All such dependency pairs and their types are extracted and counted. Structure: Recent research work reported improved performance using syntactic information for English discourse relation detection. In the work of Pilter and Nenkova (2009), the categories of a tree node, its parent, its left sibling, and its right sibling are taken as features. In the work of Wang et al. (2010), the entire paragraph is parsed Relation Temporal Contingency Comparison Expansion Type Single Phrase # 41 Samples 目前 “now” 之後 “after” Intra-Sent Phrase Pair 80 接著...再 “Then...again” 當初...曾 “At first...ever” Inter-Sent Phrase Pair 30 當初...後來 “Initially...Later” 最早...緊接著 “At first...Then” Single Phrase 62 如此一來 “As a result” 假設 “If” Intra-Sent Phrase Pair 180 如果...則 “If ... then” 無論...都 “Whether ...” Inter-Sent Phrase Pair 14 既然...看來 “Since... It seems” 幸而"
W12-1636,P11-1100,0,0.111357,"Missing"
W12-1636,P09-2004,0,0.0838576,"Missing"
W12-1636,P09-1077,0,0.0637267,"ency pairs from the sentence. A dependency pair consists of two arguments, i.e., the governor and the dependent, and their types. We are interested in those dependency pairs that are across two clauses. That is, the two arguments of a pair are from different clauses. In our assumption, the clauses have a closer connection if some dependencies occur between them. All such dependency pairs and their types are extracted and counted. Structure: Recent research work reported improved performance using syntactic information for English discourse relation detection. In the work of Pilter and Nenkova (2009), the categories of a tree node, its parent, its left sibling, and its right sibling are taken as features. In the work of Wang et al. (2010), the entire paragraph is parsed Relation Temporal Contingency Comparison Expansion Type Single Phrase # 41 Samples 目前 “now” 之後 “after” Intra-Sent Phrase Pair 80 接著...再 “Then...again” 當初...曾 “At first...ever” Inter-Sent Phrase Pair 30 當初...後來 “Initially...Later” 最早...緊接著 “At first...Then” Single Phrase 62 如此一來 “As a result” 假設 “If” Intra-Sent Phrase Pair 180 如果...則 “If ... then” 無論...都 “Whether ...” Inter-Sent Phrase Pair 14 既然...看來 “Since... It seems” 幸而"
W12-1636,prasad-etal-2008-penn,0,0.435317,"Missing"
W12-1636,P10-1073,0,0.028022,"Missing"
W12-1636,W05-0312,0,\N,Missing
W13-2309,C04-1200,0,0.0862213,"s positive than that of the first clause. Equal stands for the cases in which the polarities of both clauses are identical. The last aspect is Negativity, which regards the polarity of an argument as binary values, i.e., Negative and NonNegative. In this way, we re-classify the nine-way sentiment polarity transitions into four transitions. In other words, both the polarity states Neutral and Positive are merged into one state NonNegative in this aspect. Such a binary scheme is also used in some related work, in which the negative polarity is distinguished and the rest are considered Positive (Kim and Hovy, 2004; Devitt and Ahmad, 2007). For each type of each aspect, five discourse markers that occur more than 10 times in the dataset and have the highest ratio of the corresponding type are listed in the fifth column of Table 8 as significant discourse markers. We analyze the annotations according to the four aspects, and the results are shown in Table 9. The chi-squared test is used to test the dependency between the PDTB classes of discourse markers and each aspect of sentiment transitions. The results show that no matter whether the sentiment polarity transitions are categorized into Polarity Tende"
W13-2309,O06-1001,0,0.163071,"blicly available at present. To construct a Chinese discourse corpus, we sample instances from a huge Chinese corpus (Yu et al., 2012). This corpus was developed based on the ClueWeb09 dataset, where Chinese material is the second largest. It contains a total of 9,598,430,559 POS-tagged sentences in 172,298,866 documents. In this paper, only the explicit discourse relations are concerned. A dictionary of discourse markers is consulted to extract the instances of explicit discourse relations from the ClueWeb. This Chinese discourse marker dictionary is developed based on Cheng and Tian (1989), Cheng (2006) and Lu (2007). Table 1 shows an overview of the discourse marker dictionary. It contains 808 words and word pairs mapped into the PDTB four top-level classes (Cheng and Tian, 1989; Wolf and Gibson, 2005). Besides the types of discourse relations, we further classify the markers into three groups of scopes shown in the second column, including Single word, Intrasentential, and Inter-sentential, according to their grammatical usages. The Single word group contains those individual words used as discourse markers. The Intra-sentential group contains pairs of words that occur inside the same sent"
W13-2309,prasad-etal-2008-penn,0,0.679222,"annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annotation, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Finally, we highlight some language phenomena and give some remarks. 1 Introduction A discourse relation indicates how two arguments (i.e., elementary discourse units) cohere to each other. Various discourse relations were defined according to different taxonomy (Carlson and Marcu, 2001; Carlson et al., 2002; Prasad et al., 2008). In the work of the Penn Discourse Treebank 2.0 annotation, Prasad et al. (2008) labeled four grammatical classes of connectives in English, including subordinating conjunctions, coordinating conjunctions, adverbial connectives, and implicit connectives. Besides, the sense of each connective was also tagged. They defined three levels of sense hierarchy for the connectives. The four classes on the top level are Temporal, Contingency, Comparison, and Expansion. There are explicit and implicit uses of discourse relations. An explicit discourse relation indicates the arguments are connected with"
W13-2309,P07-1124,0,0.0315168,"of the first clause. Equal stands for the cases in which the polarities of both clauses are identical. The last aspect is Negativity, which regards the polarity of an argument as binary values, i.e., Negative and NonNegative. In this way, we re-classify the nine-way sentiment polarity transitions into four transitions. In other words, both the polarity states Neutral and Positive are merged into one state NonNegative in this aspect. Such a binary scheme is also used in some related work, in which the negative polarity is distinguished and the rest are considered Positive (Kim and Hovy, 2004; Devitt and Ahmad, 2007). For each type of each aspect, five discourse markers that occur more than 10 times in the dataset and have the highest ratio of the corresponding type are listed in the fifth column of Table 8 as significant discourse markers. We analyze the annotations according to the four aspects, and the results are shown in Table 9. The chi-squared test is used to test the dependency between the PDTB classes of discourse markers and each aspect of sentiment transitions. The results show that no matter whether the sentiment polarity transitions are categorized into Polarity Tendency, Polarity Change, Dir"
W13-2309,N03-1030,0,0.0409575,"a Positive-Negative Comparison relation. As the PDTB 2.0 annotation manual suggests (Prasad, et al., 2007), a Comparison relation is established to emphasize the differences between two arguments. Therefore, it is expected that the two arguments of a Comparison relation are relatively likely to have the opposing polarity states (i.e., Positive-Negative or NegativePositive). On the other hand, the two arguments of an Expansion relation are relatively likely to belong to the same polarity states (e.g., PositivePositive or Neutral-Neutral). Discourse relation recognition (Hernault et al., 2010; Soricut and Marcu, 2003) and sentiment analysis (Pang and Lee, 2008) have attracted much attention recently. Due to the limitation of the resources, the research on Chinese discourse relation analysis is relatively rare. In our previous work, we annotated a collection of Chinese discourse corpora, namely NTU Chinese Discourse Resources (http://nlg.csie.ntu.edu.tw/ntudiscourse/), for inter-sentential and intrasentential discourse relation recognition (Huang and Chen, 2011; Huang and Chen, 2012a). However, no sentiment information is labeled in these corpora. In another work (Huang and Chen, 2012b), we proposed an anno"
W13-2309,I11-1170,1,0.772965,"likely to belong to the same polarity states (e.g., PositivePositive or Neutral-Neutral). Discourse relation recognition (Hernault et al., 2010; Soricut and Marcu, 2003) and sentiment analysis (Pang and Lee, 2008) have attracted much attention recently. Due to the limitation of the resources, the research on Chinese discourse relation analysis is relatively rare. In our previous work, we annotated a collection of Chinese discourse corpora, namely NTU Chinese Discourse Resources (http://nlg.csie.ntu.edu.tw/ntudiscourse/), for inter-sentential and intrasentential discourse relation recognition (Huang and Chen, 2011; Huang and Chen, 2012a). However, no sentiment information is labeled in these corpora. In another work (Huang and Chen, 2012b), we proposed an annotation scheme to construct a Chinese discourse corpus with rich information including sentiment polarities, but the corpus is still under construction due to its complexity. Zhou and Xue (2012) did PDTBstyle Chinese discourse corpus annotation, but the corpus is also not available yet. In this paper, we annotate a moderate-sized Chinese corpus with the information of discourse relations and sentiment polarities. Total 7,638 sentences are sampled f"
W13-2309,J05-2005,0,0.191128,"where Chinese material is the second largest. It contains a total of 9,598,430,559 POS-tagged sentences in 172,298,866 documents. In this paper, only the explicit discourse relations are concerned. A dictionary of discourse markers is consulted to extract the instances of explicit discourse relations from the ClueWeb. This Chinese discourse marker dictionary is developed based on Cheng and Tian (1989), Cheng (2006) and Lu (2007). Table 1 shows an overview of the discourse marker dictionary. It contains 808 words and word pairs mapped into the PDTB four top-level classes (Cheng and Tian, 1989; Wolf and Gibson, 2005). Besides the types of discourse relations, we further classify the markers into three groups of scopes shown in the second column, including Single word, Intrasentential, and Inter-sentential, according to their grammatical usages. The Single word group contains those individual words used as discourse markers. The Intra-sentential group contains pairs of words that occur inside the same sentence and denote a discourse relation. Here, a Chinese sentence is defined as a sequence of successive words that is ended by a period, a question mark, or an exclamation mark. The clauses of a sentence ar"
W13-2309,W12-1636,1,0.758787,"discourse relations, i.e., without the information from discourse markers, is more challenging (Lin et al., 2009; Zhou et al., 2010). Hutchinson (2004) pointed out the properties of a discourse marker from three dimensions, including polarity, veridicality, and type. The polarity of a discourse marker indicates the sentiment transition of its two arguments. Veridicality, the second dimension of a discourse marker, specifies whether both the two arguments are true or not. Type, similar to the sense which is annotated in the PDTB, is the third dimension of a discourse marker. Our previous work (Huang and Chen, 2012a; Huang and Chen, 2012b) addressed the interaction between the sentiment polarity and the discourse structure in Chinese. Consider (S1), which consists of three clauses and forms a nested discourse structure shown in Figure 1. Abstract Discourse relation may entail sentiment information. In this work, we annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annotation, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Fina"
W13-2309,yu-etal-2012-development,1,0.546942,"Missing"
W13-2309,C12-3028,1,0.918716,"discourse relations, i.e., without the information from discourse markers, is more challenging (Lin et al., 2009; Zhou et al., 2010). Hutchinson (2004) pointed out the properties of a discourse marker from three dimensions, including polarity, veridicality, and type. The polarity of a discourse marker indicates the sentiment transition of its two arguments. Veridicality, the second dimension of a discourse marker, specifies whether both the two arguments are true or not. Type, similar to the sense which is annotated in the PDTB, is the third dimension of a discourse marker. Our previous work (Huang and Chen, 2012a; Huang and Chen, 2012b) addressed the interaction between the sentiment polarity and the discourse structure in Chinese. Consider (S1), which consists of three clauses and forms a nested discourse structure shown in Figure 1. Abstract Discourse relation may entail sentiment information. In this work, we annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annotation, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Fina"
W13-2309,C10-2172,0,0.11225,"Human-Annotated Corpus Hen-Hsen Huang Chi-Hsin Yu Tai-Wei Chang Cong-Kai Lin Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University, Taipei, Taiwan {hhhuang, jsyu, twchang, cklin}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw discourse marker presents the relation of its two arguments. In other cases, discourse marker is absent from an implicit relation. However, readers can still infer the relation from its argument pair. To resolve implicit discourse relations, i.e., without the information from discourse markers, is more challenging (Lin et al., 2009; Zhou et al., 2010). Hutchinson (2004) pointed out the properties of a discourse marker from three dimensions, including polarity, veridicality, and type. The polarity of a discourse marker indicates the sentiment transition of its two arguments. Veridicality, the second dimension of a discourse marker, specifies whether both the two arguments are true or not. Type, similar to the sense which is annotated in the PDTB, is the third dimension of a discourse marker. Our previous work (Huang and Chen, 2012a; Huang and Chen, 2012b) addressed the interaction between the sentiment polarity and the discourse structure i"
W13-2309,P04-1087,0,0.341908,"pus Hen-Hsen Huang Chi-Hsin Yu Tai-Wei Chang Cong-Kai Lin Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University, Taipei, Taiwan {hhhuang, jsyu, twchang, cklin}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw discourse marker presents the relation of its two arguments. In other cases, discourse marker is absent from an implicit relation. However, readers can still infer the relation from its argument pair. To resolve implicit discourse relations, i.e., without the information from discourse markers, is more challenging (Lin et al., 2009; Zhou et al., 2010). Hutchinson (2004) pointed out the properties of a discourse marker from three dimensions, including polarity, veridicality, and type. The polarity of a discourse marker indicates the sentiment transition of its two arguments. Veridicality, the second dimension of a discourse marker, specifies whether both the two arguments are true or not. Type, similar to the sense which is annotated in the PDTB, is the third dimension of a discourse marker. Our previous work (Huang and Chen, 2012a; Huang and Chen, 2012b) addressed the interaction between the sentiment polarity and the discourse structure in Chinese. Consider"
W13-2309,P12-1008,0,0.139091,"is is relatively rare. In our previous work, we annotated a collection of Chinese discourse corpora, namely NTU Chinese Discourse Resources (http://nlg.csie.ntu.edu.tw/ntudiscourse/), for inter-sentential and intrasentential discourse relation recognition (Huang and Chen, 2011; Huang and Chen, 2012a). However, no sentiment information is labeled in these corpora. In another work (Huang and Chen, 2012b), we proposed an annotation scheme to construct a Chinese discourse corpus with rich information including sentiment polarities, but the corpus is still under construction due to its complexity. Zhou and Xue (2012) did PDTBstyle Chinese discourse corpus annotation, but the corpus is also not available yet. In this paper, we annotate a moderate-sized Chinese corpus with the information of discourse relations and sentiment polarities. Total 7,638 sentences are sampled from the ClueWeb09. We review the results of annotation and analyze some language phenomena found in the corpus. The rest of this paper is organized as follows. In Section 2, we introduce the ClueWeb corpus 71 PDTB Class Expansion Temporal Comparison Contingency Scope Single word Intrasentential Intersentential Single word Intrasentential In"
W13-2309,C04-1020,0,\N,Missing
W13-2309,D09-1036,0,\N,Missing
W13-2817,D11-1038,0,0.018242,"effing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results of monolingual data. To deal with the resource-poor issue, Bertoldi and Federico (2009) generate a pseudo bilingual corpus from the monolingual in-domain corpus, and then train a translation model from the pseudo bilingual corpus. Besides counting similarities and generating pseudo bilingual in-domain corpus, text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) is another direction. Simplifying a source language text makes the translation easier in a background MT system. Chen et al. (2012a) propose a method to simplify a sentence before MT and to restore the translation of the simplified part after MT. They focus on the treatments of input text only, but do not consider how to adapt the background MT to the specific domain. The translation performance depends on the coverage of the simplification rules and the quality of the background system. This paper adopts the simplificationtranslation-restoration methodology (Chen et al."
W13-2817,P12-1107,0,0.0532218,"Missing"
W13-2817,C04-1059,0,0.0260446,"do bilingual corpus is significant. 1 Introduction Bilingual dictionary and corpus are important resources for MT applications. They are used for lexical choice and model construction. However, not all resources are available in bilingual forms in each domain. For example, medical records are in English only in some countries. In such a case, only bilingual dictionary and monolingual corpus is available. Lack of bilingual corpus makes domain adaptation more challenging. A number of adaptation approaches (Civera and Juan, 2007; Foster and Kuhn 2007; Foster et al., 2010, Matsoukas et al., 2009; Zhao et al., 2004) have been proposed. They address the reliability of a model in a new domain and count the domain similarities between a model and the indomain development data. The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results of monolingual data. To deal with the resource-poor issue, Berto"
W13-2817,C10-1152,0,0.0259508,"are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results of monolingual data. To deal with the resource-poor issue, Bertoldi and Federico (2009) generate a pseudo bilingual corpus from the monolingual in-domain corpus, and then train a translation model from the pseudo bilingual corpus. Besides counting similarities and generating pseudo bilingual in-domain corpus, text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) is another direction. Simplifying a source language text makes the translation easier in a background MT system. Chen et al. (2012a) propose a method to simplify a sentence before MT and to restore the translation of the simplified part after MT. They focus on the treatments of input text only, but do not consider how to adapt the background MT to the specific domain. The translation performance depends on the coverage of the simplification rules and the quality of the background system. This paper adopts the simplificationtranslation-restorati"
W13-2817,W09-0432,0,0.0608974,"2004) have been proposed. They address the reliability of a model in a new domain and count the domain similarities between a model and the indomain development data. The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results of monolingual data. To deal with the resource-poor issue, Bertoldi and Federico (2009) generate a pseudo bilingual corpus from the monolingual in-domain corpus, and then train a translation model from the pseudo bilingual corpus. Besides counting similarities and generating pseudo bilingual in-domain corpus, text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) is another direction. Simplifying a source language text makes the translation easier in a background MT system. Chen et al. (2012a) propose a method to simplify a sentence before MT and to restore the translation of the simplified part after MT. They focus on the treatments of input text"
W13-2817,C12-1034,1,0.882217,"tlysupervised training to generate additional training data from the translation results of monolingual data. To deal with the resource-poor issue, Bertoldi and Federico (2009) generate a pseudo bilingual corpus from the monolingual in-domain corpus, and then train a translation model from the pseudo bilingual corpus. Besides counting similarities and generating pseudo bilingual in-domain corpus, text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012) is another direction. Simplifying a source language text makes the translation easier in a background MT system. Chen et al. (2012a) propose a method to simplify a sentence before MT and to restore the translation of the simplified part after MT. They focus on the treatments of input text only, but do not consider how to adapt the background MT to the specific domain. The translation performance depends on the coverage of the simplification rules and the quality of the background system. This paper adopts the simplificationtranslation-restoration methodology (Chen et al., 2012a), but emphasizes on how to update bilingual translation rules, translation model and language model, which are two kernels of rulebased and stati"
W13-2817,W07-0722,0,0.0249594,"rom the monolingual in-domain corpus are useful, and the effect of using the selected pseudo bilingual corpus is significant. 1 Introduction Bilingual dictionary and corpus are important resources for MT applications. They are used for lexical choice and model construction. However, not all resources are available in bilingual forms in each domain. For example, medical records are in English only in some countries. In such a case, only bilingual dictionary and monolingual corpus is available. Lack of bilingual corpus makes domain adaptation more challenging. A number of adaptation approaches (Civera and Juan, 2007; Foster and Kuhn 2007; Foster et al., 2010, Matsoukas et al., 2009; Zhao et al., 2004) have been proposed. They address the reliability of a model in a new domain and count the domain similarities between a model and the indomain development data. The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from t"
W13-2817,W07-0717,0,0.0337879,"domain corpus are useful, and the effect of using the selected pseudo bilingual corpus is significant. 1 Introduction Bilingual dictionary and corpus are important resources for MT applications. They are used for lexical choice and model construction. However, not all resources are available in bilingual forms in each domain. For example, medical records are in English only in some countries. In such a case, only bilingual dictionary and monolingual corpus is available. Lack of bilingual corpus makes domain adaptation more challenging. A number of adaptation approaches (Civera and Juan, 2007; Foster and Kuhn 2007; Foster et al., 2010, Matsoukas et al., 2009; Zhao et al., 2004) have been proposed. They address the reliability of a model in a new domain and count the domain similarities between a model and the indomain development data. The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results"
W13-2817,D10-1044,0,0.0218196,"ul, and the effect of using the selected pseudo bilingual corpus is significant. 1 Introduction Bilingual dictionary and corpus are important resources for MT applications. They are used for lexical choice and model construction. However, not all resources are available in bilingual forms in each domain. For example, medical records are in English only in some countries. In such a case, only bilingual dictionary and monolingual corpus is available. Lack of bilingual corpus makes domain adaptation more challenging. A number of adaptation approaches (Civera and Juan, 2007; Foster and Kuhn 2007; Foster et al., 2010, Matsoukas et al., 2009; Zhao et al., 2004) have been proposed. They address the reliability of a model in a new domain and count the domain similarities between a model and the indomain development data. The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results of monolingual data."
W13-2817,D09-1074,0,0.034019,"using the selected pseudo bilingual corpus is significant. 1 Introduction Bilingual dictionary and corpus are important resources for MT applications. They are used for lexical choice and model construction. However, not all resources are available in bilingual forms in each domain. For example, medical records are in English only in some countries. In such a case, only bilingual dictionary and monolingual corpus is available. Lack of bilingual corpus makes domain adaptation more challenging. A number of adaptation approaches (Civera and Juan, 2007; Foster and Kuhn 2007; Foster et al., 2010, Matsoukas et al., 2009; Zhao et al., 2004) have been proposed. They address the reliability of a model in a new domain and count the domain similarities between a model and the indomain development data. The domain relevance in different granularities including words, phrases, sentences, documents and corpora are considered. Ueffing et al. (2007) propose semisupervised methods which use monolingual data in source language to improve translation performance. Schwenk (2008) present lightlysupervised training to generate additional training data from the translation results of monolingual data. To deal with the resour"
W13-2817,2008.iwslt-papers.6,0,\N,Missing
W13-2817,P07-1004,0,\N,Missing
W13-4414,O11-1001,1,0.842514,"n such a document are assumed to be independent of each other (the so-called “bag-of-words” assumption). When we calculate the conditional probability P( wL |W1L 1 ) , we can linearly combine the associated WTM models of the words occurring in W1L 1 to form a composite WTM model for predicting wL : PWTM( wL |W1L 1 ) In addition to topic models, many other language modeling techniques have been proposed to complement the n-gram model in different ways, such as recurrent neural network language modeling (RNNLM) (Tomáš et al., 2010), discriminative language modeling (DLM) (Roark et al., 2007; Lai et al., 2011; Chen et al., 2012), and relevance modeling (RM) (Lavrenko and Croft, 2001; Chen and Chen, 2011; Chen and Chen, 2013). RNNLM tries to project W1L 1 and wL into a continuous space, and estimate the conditional probability in a recursive way by incorporating the full information about W1L 1 . DLM takes an objective function corresponding to minimizing the word error rate for speech recognition or maximizing the ROUGE score for summarization as a holy grail and updates the language model parameters to achieve the goal. RM assumes that each word sequence W1L is associated with a relevance class"
W13-4414,P10-1009,0,0.0130967,"fly review the n-gram and topic language models. Section 3 details our proposed CSC system. A series of experiments are presented in Section 4. Finally, conclusions and future work are given in Section 5. 2 2.1 Language Modeling N-gram Language Modeling From the early 20th century, statistical language modeling has been successfully applied to various applications related to natural language processing (NLP), such as speech recognition (Chen and Goodman, 1999; Chen and Chen, 2011), information retrieval (Ponte and Croft, 1998; Lavrenko and Croft, 2001; Lavrenko, 2009), document summarization (Lin and Chen, 2010), and spelling correction (Chen et al., 2009; Liu et al., 2011; Wu et al., 2010). The most widely-used and well-practiced language model, by far, is the n-gram language model (Jelinek, 1999), because of its simplicity and fair predictive power. Quantifying the quality of a word string in a natural language is the most commonly executed task. Take the tri-gram model for example, when given a word string W1L  w1 , w2 ,, wL , the probability of the word string is approximated by the Since most Chinese characters have other characters similar to them in either shape or pronunciation, an intuitiv"
W13-4414,W10-4107,0,0.69511,"ing the long-span semantic information for language modeling for CSC. Moreover, we make a step forward to incorporate a search engine to provide extra information from the Web resources to make a more robust system. Introduction Chinese is a tonal syllabic and character (symbol) language, in which each character is pronounced as a tonal syllable. A Chinese “word” usually comprises two or more characters. The difficulty of Chinese processing is that many Chinese characters have similar shapes or similar (or same) pronunciations. Some characters are even similar in both shape and pronunciation (Wu et al., 2010; Liu et al., 2011). However, the meanings of these characters (or words composed of the characters) may be widely divergent. Due to this reason, all the students in elementary school in Taiwan or the foreign Chinese learners need to practice to identify and correct “erroneous words” in a Chinese sentence, which is called the Incorrect Character Correction (ICC) test. In fact, the ICC test is not a simple task even for some adult native speakers in Taiwan. The rest of this paper is organized as follows. In Section 2, we briefly review the n-gram and topic language models. Section 3 details our"
W13-4414,W03-1726,0,0.102448,"Missing"
W13-4414,O09-2007,0,0.0175181,"s. Section 3 details our proposed CSC system. A series of experiments are presented in Section 4. Finally, conclusions and future work are given in Section 5. 2 2.1 Language Modeling N-gram Language Modeling From the early 20th century, statistical language modeling has been successfully applied to various applications related to natural language processing (NLP), such as speech recognition (Chen and Goodman, 1999; Chen and Chen, 2011), information retrieval (Ponte and Croft, 1998; Lavrenko and Croft, 2001; Lavrenko, 2009), document summarization (Lin and Chen, 2010), and spelling correction (Chen et al., 2009; Liu et al., 2011; Wu et al., 2010). The most widely-used and well-practiced language model, by far, is the n-gram language model (Jelinek, 1999), because of its simplicity and fair predictive power. Quantifying the quality of a word string in a natural language is the most commonly executed task. Take the tri-gram model for example, when given a word string W1L  w1 , w2 ,, wL , the probability of the word string is approximated by the Since most Chinese characters have other characters similar to them in either shape or pronunciation, an intuitive idea for CSC is to construct a confusion s"
W14-6820,W13-4406,1,0.29649,"data preparation, performance metrics, and evaluation results based on essays written by Chinese as a foreign language learners. The hope is that such evaluations can produce more advanced Chinese spelling check techniques. 1 Introduction Chinese spelling errors frequently arise from confusion between multiple Chinese characters which are phonologically and visually similar, but semantically distinct (Liu et al., 2011). The SIGHAN 2013 Chinese Spelling Check Bakeoff was the first campaign to provide data sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to"
W14-6820,W13-4418,1,0.817147,"licly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers. Given a passage composed of several sentences, the checker should identify all possible spelling errors, highlight their locations and suggest possible correction"
W14-6820,W13-4414,1,0.830631,"lly similar, but semantically distinct (Liu et al., 2011). The SIGHAN 2013 Chinese Spelling Check Bakeoff was the first campaign to provide data sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check"
W14-6820,W13-4409,0,0.0469075,"ta sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers. Given a passage compo"
W14-6820,W13-4420,1,\N,Missing
W15-3106,W10-4107,0,0.0316855,"ith spelling errors for which no errors are detected. The criteria for judging correctness are determined at two levels as follows. (1) Detection level: all locations of incorrect characters in a given passage should be completely identical with the gold standard. (2) Correction level: all locations and corresponding corrections of incorrect characters should be completely identical with the gold standard. 33 In addition to achieve satisfactory detection/correction performance, reducing the false positive rate, that is the mistaken identification of errors where none exist, is also important (Wu et al., 2010). The following metrics are measured at both levels with the help of the confusion matrix. • F1=0.57 (=2*0.5*0.67/(0.5+0.67)) • Correction-level • Accuracy =0.4 (=2/5) • False Positive Rate (FPR) = FP / (FP+TN) • Accuracy = (TP+TN) / (TP+FP+TN+FN) • Precision = TP / (TP+FP) Notes: {“B2-1923-2, 8, 誤, 41, 情”, “B22731-1, 0”} / {“A2-0092-2, 5, 玩”, “A20243-1, 3, 件, 4, 康”, “B2-1923-2, 8, 誤, 41, 情”, “B2-2731-1, 0”, “B2-3754-3, 11, 觀”} • Recall = TP / (TP+FN) • Precision = 0.25 (=1/4) • F1= 2 *Precision*Recall/(Precision+Recall) Notes: {“B2-1923-2, 8, 誤 , 41, 情 ”} / {“A2-0092-2, 5, 玩”, “A2-0243-1, 3,"
W15-3106,W13-4406,1,0.421764,"these make Chinese spell checking a challengeable task. An empirical analysis indicated that Chinese spelling errors frequently arise from confusion among multiple-character words, which are phonologically and visually similar, but semantically distinct (Liu et al., 2011). The automatic spelling checker should have both capabilities of identifying the spelling errors and suggesting the correct characters of erroneous usages. The SIGHAN 2013 Bake-off for Chinese Spelling Check was the first campaign to provide data sets as benchmarks for the performance evaluation of Chinese spelling checkers (Wu et al., 2013). The data in SIGHAN 2013 originated from the essays written by native Chinese speakers. Following the experience of the first evaluation, the second bake-off was held in CIPS-SIGHAN Joint CLP2 Task Description The goal of this task is to evaluate the capability of a Chinese spelling checker. A passage consisting of several sentences with/without spelling errors is given as the input. The checker should return the locations of incorrect characters and suggest the correct characters. Each character or punctuation mark occupies 1 spot for counting location. The input instance is given a unique p"
W15-3106,W14-6820,1,\N,Missing
W19-4508,N16-1165,0,0.0280976,"Missing"
W19-4508,C16-1260,0,0.013467,"dia. However, only 2,500 of them are labeled. It may not be sufficient for training a model, especially for neural network models. The definition of argumentative components differs from dataset to dataset. In the dataset used in this work, an argumentative component is a span of text with reasoning or evidence, which is able to either support or oppose a topic (Stab et al., 2018b). Related Works Neural networks have been used in varieties of AM tasks. To improve the vanilla LSTM model, Stab et al. (2018a) use attention mechanism to fuse topic and sentence information together. In the work of Laha and Raykar (2016), they present several bi-sequence classification models on different datasets. However, rather than using some sophisticated architecture such as attention, it considers only different concatenation or condition method on the output of LSTM. Eger et al. (2017) propose an end-to-end training model to mining argument structure, identifying argument components. Besides syntactic and positional information, lexical information is also reported as one of the most used features in argument mining task (Cabrio and Villata, 2018). In some similar research fields such as sentiment analysis and emotion"
W19-4508,W17-5104,0,0.0227656,"yntactic and positional information, lexical information is also reported as one of the most used features in argument mining task (Cabrio and Villata, 2018). In some similar research fields such as sentiment analysis and emotion mining, a number of works have been proposed to combine lexical information with the NN models. Teng et al. (2016) use lexical scores as the weights and do the weighted sum over the outputs of the LSTM model, in order to derive the sentence scores. Zou et al. (2018) determines attention weights using lexicon labels, which lead the model to focus on the lexicon words. Bar-Haim et al. (2017) proposes an idea of expanding lexicons to improve stance classifying task. However, in AM, seldom works directly combine lexicon with models. By using discourse feature, Levy et al. (2018) generates weak labels and use weak supervision. Shnarch et al. (2018) also present a methodology to blend such weak labeled data with high quality but scarce labeled data for AM. Al-Khatib et al. (2016) consider the distant supervision method. Most of these works use the end-to-end training paradigm with the outside resources only for generating the weak label, which may not fully leverage the information o"
W19-4508,C18-1176,0,0.656208,"mental results and concludes this work. 2 3.1 We conduct the experiments on the dataset released by Stab et al. (2018b).1 The dataset includes 25,492 sentences over eight topics that are randomly selected from an online list of controversial topics.2 The selected topics, which are considered as queries, are used to retrieve documents from heterogeneous sources via the Google search engine. Among these sentences, 4,944 of them are supporting arguments, 6,195 are opposing arguments, and 14,353 are non-argument sentences. This dataset is commonly used for sentential argument identification task. Levy et al. (2018) collect a dataset with around 1.5 million sentences over 150 topics from Wikipedia. However, only 2,500 of them are labeled. It may not be sufficient for training a model, especially for neural network models. The definition of argumentative components differs from dataset to dataset. In the dataset used in this work, an argumentative component is a span of text with reasoning or evidence, which is able to either support or oppose a topic (Stab et al., 2018b). Related Works Neural networks have been used in varieties of AM tasks. To improve the vanilla LSTM model, Stab et al. (2018a) use atte"
W19-4508,W17-5110,0,0.800857,"m into roles of support/opposition. Our model is based on the recurrent neural network (RNN) , which has been widely used in natural language processing tasks (Cho et al., 2014). With the help of the attention mechanism (Bahdanau et al., 2015), RNN can further attend on the key information. We propose a novel attention mechanism that is guided by argumentative lexicon information. Lexicon information is reported as one kind of the most frequently used features in argument mining (Cabrio and Villata, 2018). Previous works on AM have tried to integrate lexical features into the learning models (Levy et al., 2017; Nguyen and Litman, 2015; Rinott et al., 2015). These lexicons are mostly composed by human beings or derived by hand-crafted rules, and result in domainspecificity. That is, it may fail to be used for other domains. In the contrast of scarcity of general lexicon for AM, lexical resources are abundant in other fields like sentiment analysis, opinion mining, and emotion detection (Hu and Liu, 2004; Mohammad and Turney, 2013; Kiritchenko and Mohammad, 2016). As a more general domain, AM may get the benefits of not only in-domain lexicon, but also out-domain lexicons. The contribution of this wo"
W19-4508,D14-1179,0,0.00758891,"Missing"
W19-4508,P17-1002,0,0.0195477,"nt is a span of text with reasoning or evidence, which is able to either support or oppose a topic (Stab et al., 2018b). Related Works Neural networks have been used in varieties of AM tasks. To improve the vanilla LSTM model, Stab et al. (2018a) use attention mechanism to fuse topic and sentence information together. In the work of Laha and Raykar (2016), they present several bi-sequence classification models on different datasets. However, rather than using some sophisticated architecture such as attention, it considers only different concatenation or condition method on the output of LSTM. Eger et al. (2017) propose an end-to-end training model to mining argument structure, identifying argument components. Besides syntactic and positional information, lexical information is also reported as one of the most used features in argument mining task (Cabrio and Villata, 2018). In some similar research fields such as sentiment analysis and emotion mining, a number of works have been proposed to combine lexical information with the NN models. Teng et al. (2016) use lexical scores as the weights and do the weighted sum over the outputs of the LSTM model, in order to derive the sentence scores. Zou et al."
W19-4508,W15-0503,0,0.022469,"port/opposition. Our model is based on the recurrent neural network (RNN) , which has been widely used in natural language processing tasks (Cho et al., 2014). With the help of the attention mechanism (Bahdanau et al., 2015), RNN can further attend on the key information. We propose a novel attention mechanism that is guided by argumentative lexicon information. Lexicon information is reported as one kind of the most frequently used features in argument mining (Cabrio and Villata, 2018). Previous works on AM have tried to integrate lexical features into the learning models (Levy et al., 2017; Nguyen and Litman, 2015; Rinott et al., 2015). These lexicons are mostly composed by human beings or derived by hand-crafted rules, and result in domainspecificity. That is, it may fail to be used for other domains. In the contrast of scarcity of general lexicon for AM, lexical resources are abundant in other fields like sentiment analysis, opinion mining, and emotion detection (Hu and Liu, 2004; Mohammad and Turney, 2013; Kiritchenko and Mohammad, 2016). As a more general domain, AM may get the benefits of not only in-domain lexicon, but also out-domain lexicons. The contribution of this work is two-fold: (1) We pr"
W19-4508,W13-2707,0,0.0695041,"Missing"
W19-4508,D14-1162,0,0.0817048,"Missing"
W19-4508,W16-0410,0,0.0145063,"ntly used features in argument mining (Cabrio and Villata, 2018). Previous works on AM have tried to integrate lexical features into the learning models (Levy et al., 2017; Nguyen and Litman, 2015; Rinott et al., 2015). These lexicons are mostly composed by human beings or derived by hand-crafted rules, and result in domainspecificity. That is, it may fail to be used for other domains. In the contrast of scarcity of general lexicon for AM, lexical resources are abundant in other fields like sentiment analysis, opinion mining, and emotion detection (Hu and Liu, 2004; Mohammad and Turney, 2013; Kiritchenko and Mohammad, 2016). As a more general domain, AM may get the benefits of not only in-domain lexicon, but also out-domain lexicons. The contribution of this work is two-fold: (1) We propose an attention mechanism to leverage lexicon information. (2) In the face of the scarcity of argument lexicon, we explore several different types of lexicons to verify whether outside resources are useful for AM tasks. The rest of this paper is organized as follows. Section 2 summarizes related works about AM. The dataset and linguistic resources used for experiments are shown in Section 3. We introduce Identification of argume"
W19-4508,P18-2095,0,0.0145672,"been proposed to combine lexical information with the NN models. Teng et al. (2016) use lexical scores as the weights and do the weighted sum over the outputs of the LSTM model, in order to derive the sentence scores. Zou et al. (2018) determines attention weights using lexicon labels, which lead the model to focus on the lexicon words. Bar-Haim et al. (2017) proposes an idea of expanding lexicons to improve stance classifying task. However, in AM, seldom works directly combine lexicon with models. By using discourse feature, Levy et al. (2018) generates weak labels and use weak supervision. Shnarch et al. (2018) also present a methodology to blend such weak labeled data with high quality but scarce labeled data for AM. Al-Khatib et al. (2016) consider the distant supervision method. Most of these works use the end-to-end training paradigm with the outside resources only for generating the weak label, which may not fully leverage the information of the lexicons. 3 Data 3.2 Lexicon resource To improve the baseline model, we consider several existing lexicons across different domains. We first explore the claim lexicon that is built for argument mining task (Levy et al., 2017). We also include the lexic"
W19-4508,C14-1142,0,0.0679932,"Missing"
W19-4508,D18-1402,0,0.197443,"Missing"
W19-4508,D16-1169,0,0.0576156,"Missing"
W19-4508,C18-1074,0,0.0256188,"al. (2017) propose an end-to-end training model to mining argument structure, identifying argument components. Besides syntactic and positional information, lexical information is also reported as one of the most used features in argument mining task (Cabrio and Villata, 2018). In some similar research fields such as sentiment analysis and emotion mining, a number of works have been proposed to combine lexical information with the NN models. Teng et al. (2016) use lexical scores as the weights and do the weighted sum over the outputs of the LSTM model, in order to derive the sentence scores. Zou et al. (2018) determines attention weights using lexicon labels, which lead the model to focus on the lexicon words. Bar-Haim et al. (2017) proposes an idea of expanding lexicons to improve stance classifying task. However, in AM, seldom works directly combine lexicon with models. By using discourse feature, Levy et al. (2018) generates weak labels and use weak supervision. Shnarch et al. (2018) also present a methodology to blend such weak labeled data with high quality but scarce labeled data for AM. Al-Khatib et al. (2016) consider the distant supervision method. Most of these works use the end-to-end t"
W95-0113,C94-2123,0,0.0457119,"Missing"
W95-0113,P94-1032,1,0.883412,"Missing"
W95-0113,P92-1017,0,0.129457,"Missing"
W95-0113,H91-1037,0,0.0381428,"Missing"
W95-0113,H91-1026,0,0.0325375,"rules are shown as follows. Table 5. Experimental Results After Applying Two Heuristic Rules Mapping Types Subtypes Numbe r of Mapping 232 Unique Tag Correct Wrong 22 18 Multiple Tags Include Exclude 10 No Match Correct 26 Wrong Three tags - say, FA, FB and GG, must be treated in particular. For example, Susanne Corpus tags genitive case noun as [John NP &apos;s_GG], but LOB Corpus tags it as [John&apos;s_PN$]. Two Susanne tags may be mapped into One LOB tag. Ignoring these three special tags, only nineteen Susanne tags have wrong mapping in Uniq0e-Tag case. 4. A Probabilistic Chunker Gale and Church, [8] propose d~2, a X2-1ike statistic, to measure the association between two words. Table 6 illustrates a twr-by-two contingency table for words w I and w 2. 165 Table 6. A Contingency Table Word w 1 Word w 2 a b c d Cell a counts the number of sentences that contain both w I and w 2. Cell b (c) counts the number of sentences that contain w 2 (Wl) but not w I (w2). Cell d counts the number o f sentences that does not contain both w 1 and w 2. contingency table, That is, if N is the total number of sentences, d=N-a-b-c. Based on this (~2 is defined as follows: (a*d +b&apos;c) 2 42 = ( a + b ) * ( a + c"
W95-0113,A88-1019,0,0.183518,"Missing"
W95-0113,W89-0240,0,0.0367572,"Missing"
W95-0113,P90-1034,0,0.177662,"Missing"
W95-0113,J93-1007,0,0.0261163,"Missing"
X98-1022,P94-1032,1,0.688248,"Missing"
X98-1022,E95-1037,0,0.031184,"Missing"
X98-1022,J90-1003,0,0.037154,"Missing"
X98-1022,M98-1017,1,\N,Missing
yu-etal-2012-development,N03-1033,0,\N,Missing
yu-etal-2012-development,N10-2012,0,\N,Missing
yu-etal-2012-development,P10-1089,0,\N,Missing
yu-etal-2012-development,lin-etal-2010-new,0,\N,Missing
yu-etal-2012-development,I05-3027,0,\N,Missing
