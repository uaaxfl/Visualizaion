2001.mtsummit-papers.5,C00-2131,1,0.878601,"sh translation dictionary consultation. Then, the system finds correspondences in remaining phrases by using sentences dependency structures and the balance of all correspondences. The method is based on an assumption that in parallel corpus most fragments in a source sentence have corresponding fragments in a target sentence. Keywords Example-based Translation, Finding Phrasal Correspondence, Phrasal Alignment, Parallel Corpus, Dependency Structure Figure 1: System Image and an Example of System Output Introduction Example-based translation system requires a large set of translation patterns [1]. Over the last decade, the sentence alignment and word alignment have been explored and achieved numerous successes by using statistical approach. In contrast, a fewer results are reported in phrasal alignment. In statistical phrasal alignment acquires the bilingual-correspondences appear with high frequency. However the coverage is low [2] [3]. In parallel corpus, we think that fragments in a source sentence usually have corresponding fragments in a target sentence. So, this paper proposes a system finds correspondences by using dependency structures and a balance of correspondences. This pa"
2001.mtsummit-papers.5,P95-1033,0,0.0752679,"le-based Translation, Finding Phrasal Correspondence, Phrasal Alignment, Parallel Corpus, Dependency Structure Figure 1: System Image and an Example of System Output Introduction Example-based translation system requires a large set of translation patterns [1]. Over the last decade, the sentence alignment and word alignment have been explored and achieved numerous successes by using statistical approach. In contrast, a fewer results are reported in phrasal alignment. In statistical phrasal alignment acquires the bilingual-correspondences appear with high frequency. However the coverage is low [2] [3]. In parallel corpus, we think that fragments in a source sentence usually have corresponding fragments in a target sentence. So, this paper proposes a system finds correspondences by using dependency structures and a balance of correspondences. This paper is organized as follows: In the next section, we present the overview of our approach. In section 3, we describe our methods in detail. In section 4, experiments and results are given. In section 5, we describe a conclusion. Overview of our approach We have developed a system finds phrasal correspondences in parallel parsed corpus that a"
2001.mtsummit-papers.5,C00-2135,0,0.0566837,"ased Translation, Finding Phrasal Correspondence, Phrasal Alignment, Parallel Corpus, Dependency Structure Figure 1: System Image and an Example of System Output Introduction Example-based translation system requires a large set of translation patterns [1]. Over the last decade, the sentence alignment and word alignment have been explored and achieved numerous successes by using statistical approach. In contrast, a fewer results are reported in phrasal alignment. In statistical phrasal alignment acquires the bilingual-correspondences appear with high frequency. However the coverage is low [2] [3]. In parallel corpus, we think that fragments in a source sentence usually have corresponding fragments in a target sentence. So, this paper proposes a system finds correspondences by using dependency structures and a balance of correspondences. This paper is organized as follows: In the next section, we present the overview of our approach. In section 3, we describe our methods in detail. In section 4, experiments and results are given. In section 5, we describe a conclusion. Overview of our approach We have developed a system finds phrasal correspondences in parallel parsed corpus that are c"
2001.mtsummit-papers.5,J94-4001,1,0.914151,"es in a dictionary (2817 sentences) Figure 9: E(k) and J(K) E(k) and J(k) are distances as shown in figure 9. E(k) is the distance between an English remaining phrase and an Short sentences We acquired test-set 200 sentences by extracting 100 sentences form each corpora under follow 3 conditions. Condition 1: A pair of sentences has one-to-one sentence correspondence. Condition 2: The number of both English and Japanese phrases differed by less than 2:1 ratio. Condition 3: The number of both English and Japanese phrases is less than 20 phrases We made a parsed bilingual corpus by using the KNP[4] Japanese parser (developed by Kyoto University) for Japanese Sentences and ESG[5] English parser (developed by IBM Watson Research Center) for English sentences. We evaluated the system output as follows. We tagged on correct target phrases every phrase in 200 testset sentences. If a system output correspondence exactly equal with a pre-aligned correspondence, we regard it as correct. If a correspondence which system output partly matches with a pre-aligned correspondence, we regard it as near-correct. Else we regard it as wrong. In extend-correspondences selection, the system has the thresho"
2001.mtsummit-papers.5,C90-3044,1,0.895338,"Missing"
2001.mtsummit-papers.5,C92-2115,1,0.863046,"Missing"
2001.mtsummit-papers.5,J80-1003,0,\N,Missing
2004.iwslt-evaluation.15,2001.mtsummit-papers.5,1,0.918544,"Missing"
2004.iwslt-evaluation.15,A00-2018,0,0.111904,"Missing"
2004.iwslt-evaluation.15,W03-0312,1,0.846103,"Missing"
2004.iwslt-evaluation.15,P02-1040,0,0.0768205,"0.49 per 0.45 0.42 gtm 0.66 0.67 3. Experiments 3.1. Experimental Condition We built translation examples from a training-set for the IWSLT04. The training-set consists of 20,000 Japanese and English sentence pairs. The evaluation was conducted using a dev-set and a test-set for the IWSLT04, which consist of about 500 Japanese sentences with 16 references. 3.2. Result The following five automatic evaluation results are shown in Table 1 and some translation samples are shown in Table 2. BLEU: The geometric mean of n-gram precision by the system output with respect to the reference translations[6]. NIST: A variant of BLEU using the arithmetic mean of weighted n-gram precision values. WER (word error rate): The edit distance between the system output and the closest reference translation. PER (position-independent WER): A variant of mWER which disregards word ordering. Figure 6: Corpus Size and Performance (BLEU). * The system without a corpus can generate translations using only the translation dictionaries. GTM (general text matcher): Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid. 93 A target language model may be helpful for th"
2004.iwslt-evaluation.15,J94-4001,1,\N,Missing
2005.mtsummit-papers.29,2004.iwslt-evaluation.1,0,0.0123601,"ure. ᶛ VTCPUNCVKQPGZCORNG UQWTEG U K 6&apos; K K VCTIGV U 6&apos; U QWVRWV V V V V V V Figure 5: Output Sentence Generation. 4 Experiments 2. The relation between translation examples is equal to the relation between their corresponding input phrases. For example, TE2 has a corresponding input phrase i2 , and it has a relation to i3 in the input structure. In this case, TE2 has a relation to an example phrase t( 3), which corresponds to i3 ( as shown in a dotted line). 4.1 Experimental setting For evaluation, we used corpora (trainingset and test-set) which are provided in the IWSLT04(Akiba et al., 2004). The training-set consists of 20K English-Japanese sentence pairs in a travel conversation domain. We built translation examples from the training-set by using the proposed alignment method mentioned in Section 3. The test-set consists of 500 Japanese sentences and their English references (500 × 16). The experiments are conducted using the following ﬁve systems: Finally, the output word-order is decided based on the n-gram language model (n = 3). proposed: The system which selects translation examples based on the proposed 223 method . Table 1: Evaluation Metrics. basic: The system (Aramaki"
2005.mtsummit-papers.29,2004.iwslt-evaluation.15,1,0.799506,"., 2004). The training-set consists of 20K English-Japanese sentence pairs in a travel conversation domain. We built translation examples from the training-set by using the proposed alignment method mentioned in Section 3. The test-set consists of 500 Japanese sentences and their English references (500 × 16). The experiments are conducted using the following ﬁve systems: Finally, the output word-order is decided based on the n-gram language model (n = 3). proposed: The system which selects translation examples based on the proposed 223 method . Table 1: Evaluation Metrics. basic: The system (Aramaki and Kurohashi, 2004) which selects translation examples based on the heuristic criterion. This system submitted translation evaluation workshop on IWSLT04(Akiba et al., 2004), and showed its basic feasibility. Note that basic uses the same alignment result as proposed. BLEU NIST baseline: EBMT baseline, which searches the most similar translation examples by using a character-based DP matching method, and outputs its target parts as is. WER C1, C2: Commercial machine translation systems under rule base approach. PER 4.2 Evaluation Evaluation is conducted based on the following conditions and by using the ﬁve eval"
2005.mtsummit-papers.29,2001.mtsummit-papers.5,1,0.784603,"rs are parsed by the Japanese parser KNP (Kurohashi and Nagao, 1994) and the English nl-parser (Charniak, 2000). The Japanese parser outputs a dependency structure, and we use it as is. The English parser outputs a phrase structure. Then, it is converted into a dependency structure by rules which decide on a head word in a phrase. A Japanese phrase unit consists of sequential content words and their following function words. An English phrase unit is a base-NP or a base-VP. Step 2: Alignment based on translation dictionary Then, correspondences are estimated by using translation dictionaries (Aramaki et al., 2001). We used four dictionaries: EDR, EDICT, ENAMDICT, and EIJIRO. These dictionaries have about two million entries in total. Step 3: Building Translation Example Database The system generate possible combinations of correspondences from aligned sentence pairs as shown in Figure 4. We regard these combinations of correspondences as translation examples, and store them in the database. In this operation, the system stores also their surrounding phrases, which are used for calculating context sim. Translation module Step 1: Input sentence analysis First, an input sentence is analyzed by the Japanes"
2005.mtsummit-papers.29,W03-0312,1,0.781548,"ith larger corpora. 㪇㪅㪋㪌 㪇㪅㪋 5 Related Work 㪇㪅㪊㪌 㪹㫃㪼㫌 㪇㪅㪊 㪇㪅㪉㪌 㪇㪅㪉 㪇㪅㪈㪌 㪇㪅㪈 㪇㪅㪇㪌 㪇 㪇 㪌㪇㪇㪇 㪈㪇㪇㪇㪇 㪈㪌㪇㪇㪇 㪉㪇㪇㪇㪇 㪺㫆㫉㫇㫌㫊㩷㫊㫀㫑㪼 㪧㪩㪦㪧㪦㪪㪜㪛 Figure 6: (BLEU). 㪙㪘㪪㪜㪣㪠㪥㪜 Corpus Size and Performance To our knowledge, there has been no work realizing EBMT based on the translation probability, and previous EBMT systems handle their translation examples using heuristic measures/criterion. For instance, MSR-MT (Richardson et al., 2001) retrieves translation examples by using only the example size. HPAT(Imamura, 2002) and TDMT(Furuse and Iida, 1994) are EBMT systems based on size and context similarity. UTOKYOMT(Aramaki et al., 2003) used alignment conﬁdence in addition to these metrics. Such a combination of multiple metrics leads to a problem of how to estimate the weight of each metric. 6 OTHERS: OTHERS is a case that multiple errors occur ,and we could not classify it into the above error types. As shown in Table 3, DATA-SPARSENESS is the most outstanding problem. Therefore, we can believe that the system will achieve a higher performance if we obtain more corpora. 4.6 Corpus size and accuracy Finally, we investigated the relation between the corpus size (the number of training sentence pairs) and its performance (BLE"
2005.mtsummit-papers.29,J93-2003,0,0.00677215,"odel, the system searches the translation example combination which has the highest probability. The proposed model clearly formalizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental results demonstrate that the proposed model has a slightly better translation quality than stateof-the-art EBMT systems. 1 Introduction Nowadays, much attention has been given to data-driven (or corpus-based) machine translation, such as example-based machine translation or EBMT(Nagao, 1984) and statistical machine translation or SMT (Brown et al., 1993). This paper focuses on EBMT approach. The idea of EBMT is that translation examples similar to a part of an input sentence are retrieved and combined to produce a translation. EBMT basically prefers larger translation examples, because the larger the translation is, the wider context is taken into account. So, most EBMT systems retrieve large examples as possible as they can, and the retrieving is based on some heuristic criterion/measures which prefer larger examples. On the other hand, SMT approach basically breaks down translation examples into small word/phrases in order to calculate tran"
2005.mtsummit-papers.29,A00-2018,0,0.013935,"able, i.e., P (play |kakeru) = 23 , P (set |kakeru) = 1 3. Thus, a translation example with the similar context can naturally get a higher translation probability. 3 Algorithm The Algorithm of proposed method consists of the following two modules: (1) an alignment module, which builds translation example from corpus, and (2) a translation module, which generates a translation. Alignment module Step 1: Conversion into phrasal dependency structures saurus(Ikehara et al., 1997). 222 First, sentence pairs are parsed by the Japanese parser KNP (Kurohashi and Nagao, 1994) and the English nl-parser (Charniak, 2000). The Japanese parser outputs a dependency structure, and we use it as is. The English parser outputs a phrase structure. Then, it is converted into a dependency structure by rules which decide on a head word in a phrase. A Japanese phrase unit consists of sequential content words and their following function words. An English phrase unit is a base-NP or a base-VP. Step 2: Alignment based on translation dictionary Then, correspondences are estimated by using translation dictionaries (Aramaki et al., 2001). We used four dictionaries: EDR, EDICT, ENAMDICT, and EIJIRO. These dictionaries have abo"
2005.mtsummit-papers.29,C94-1015,0,0.0476541,"HERS the fact that, as mentioned before, the system will achieve a higher performance with larger corpora. 㪇㪅㪋㪌 㪇㪅㪋 5 Related Work 㪇㪅㪊㪌 㪹㫃㪼㫌 㪇㪅㪊 㪇㪅㪉㪌 㪇㪅㪉 㪇㪅㪈㪌 㪇㪅㪈 㪇㪅㪇㪌 㪇 㪇 㪌㪇㪇㪇 㪈㪇㪇㪇㪇 㪈㪌㪇㪇㪇 㪉㪇㪇㪇㪇 㪺㫆㫉㫇㫌㫊㩷㫊㫀㫑㪼 㪧㪩㪦㪧㪦㪪㪜㪛 Figure 6: (BLEU). 㪙㪘㪪㪜㪣㪠㪥㪜 Corpus Size and Performance To our knowledge, there has been no work realizing EBMT based on the translation probability, and previous EBMT systems handle their translation examples using heuristic measures/criterion. For instance, MSR-MT (Richardson et al., 2001) retrieves translation examples by using only the example size. HPAT(Imamura, 2002) and TDMT(Furuse and Iida, 1994) are EBMT systems based on size and context similarity. UTOKYOMT(Aramaki et al., 2003) used alignment conﬁdence in addition to these metrics. Such a combination of multiple metrics leads to a problem of how to estimate the weight of each metric. 6 OTHERS: OTHERS is a case that multiple errors occur ,and we could not classify it into the above error types. As shown in Table 3, DATA-SPARSENESS is the most outstanding problem. Therefore, we can believe that the system will achieve a higher performance if we obtain more corpora. 4.6 Corpus size and accuracy Finally, we investigated the relation be"
2005.mtsummit-papers.29,2002.tmi-papers.9,0,0.0180469,"D-ORDER SELECTION-ERR OTHERS the fact that, as mentioned before, the system will achieve a higher performance with larger corpora. 㪇㪅㪋㪌 㪇㪅㪋 5 Related Work 㪇㪅㪊㪌 㪹㫃㪼㫌 㪇㪅㪊 㪇㪅㪉㪌 㪇㪅㪉 㪇㪅㪈㪌 㪇㪅㪈 㪇㪅㪇㪌 㪇 㪇 㪌㪇㪇㪇 㪈㪇㪇㪇㪇 㪈㪌㪇㪇㪇 㪉㪇㪇㪇㪇 㪺㫆㫉㫇㫌㫊㩷㫊㫀㫑㪼 㪧㪩㪦㪧㪦㪪㪜㪛 Figure 6: (BLEU). 㪙㪘㪪㪜㪣㪠㪥㪜 Corpus Size and Performance To our knowledge, there has been no work realizing EBMT based on the translation probability, and previous EBMT systems handle their translation examples using heuristic measures/criterion. For instance, MSR-MT (Richardson et al., 2001) retrieves translation examples by using only the example size. HPAT(Imamura, 2002) and TDMT(Furuse and Iida, 1994) are EBMT systems based on size and context similarity. UTOKYOMT(Aramaki et al., 2003) used alignment conﬁdence in addition to these metrics. Such a combination of multiple metrics leads to a problem of how to estimate the weight of each metric. 6 OTHERS: OTHERS is a case that multiple errors occur ,and we could not classify it into the above error types. As shown in Table 3, DATA-SPARSENESS is the most outstanding problem. Therefore, we can believe that the system will achieve a higher performance if we obtain more corpora. 4.6 Corpus size and accuracy Finally,"
2005.mtsummit-papers.29,J94-4001,1,0.691537,"only three, but its target translation becomes more stable, i.e., P (play |kakeru) = 23 , P (set |kakeru) = 1 3. Thus, a translation example with the similar context can naturally get a higher translation probability. 3 Algorithm The Algorithm of proposed method consists of the following two modules: (1) an alignment module, which builds translation example from corpus, and (2) a translation module, which generates a translation. Alignment module Step 1: Conversion into phrasal dependency structures saurus(Ikehara et al., 1997). 222 First, sentence pairs are parsed by the Japanese parser KNP (Kurohashi and Nagao, 1994) and the English nl-parser (Charniak, 2000). The Japanese parser outputs a dependency structure, and we use it as is. The English parser outputs a phrase structure. Then, it is converted into a dependency structure by rules which decide on a head word in a phrase. A Japanese phrase unit consists of sequential content words and their following function words. An English phrase unit is a base-NP or a base-VP. Step 2: Alignment based on translation dictionary Then, correspondences are estimated by using translation dictionaries (Aramaki et al., 2001). We used four dictionaries: EDR, EDICT, ENAMDI"
2005.mtsummit-papers.29,niessen-etal-2000-evaluation,0,0.0324007,"ems under rule base approach. PER 4.2 Evaluation Evaluation is conducted based on the following conditions and by using the ﬁve evaluation metrics in Table 1. (1) (2) (3) (4) case insensitive (lower case only) no punctuation marks (.,?!”) no hyphen spelling out numerals GTM The geometric mean of n-gram precision by the system output with respect to the reference translations(Papineni et al., 2002). A variant of BLEU using the arithmetic mean of weighted n-gram precision values(Doddington, 2002). word error rate; The edit distance between the system output and the closest reference translation(Niessen et al., 2000). Position-independent WER; A variant of mWER which disregards word ordering(Och et al., 2001). general text matcher; Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid.(Turian et al., 2003) * Large scores are better in BLEU, NIST and GTM. Small scores are better in WER and PER. are incorrect. Their errors are classiﬁed in Table 3. 4.3 Results The result is shown in Table 2. Because proposed accuracy is slightly higher than basic, the result demonstrates validity of the proposed translation model. 4.4 Contribution of context similarity We inv"
2005.mtsummit-papers.29,W99-0604,0,0.105445,"input sentence are retrieved and combined to produce a translation. EBMT basically prefers larger translation examples, because the larger the translation is, the wider context is taken into account. So, most EBMT systems retrieve large examples as possible as they can, and the retrieving is based on some heuristic criterion/measures which prefer larger examples. On the other hand, SMT approach basically breaks down translation examples into small word/phrases in order to calculate translation probability reliably. Of course, recent SMT studies incorporate larger phrase unit, for example, Och(Och et al., 1999) used alignment template to handle phrase chunks. However, SMT translation unit is smaller than EBMT, which has no limitation in its unit size. 219 Simply speaking, EBMT and SMT have two diﬀerences: 1. EBMT pays more attention to the size; SMT to the frequency. 2. EBMT relies rion/measures; formalized. on heuristic criteSMT is statistically For the formalization of EBMT, this paper proposes a probabilistic translation model, which deals not only with the example size but with the context similarity. In the experiments, the proposed model has a slightly better translation quality than stateof-t"
2005.mtsummit-papers.29,W01-1408,0,0.0234891,"nditions and by using the ﬁve evaluation metrics in Table 1. (1) (2) (3) (4) case insensitive (lower case only) no punctuation marks (.,?!”) no hyphen spelling out numerals GTM The geometric mean of n-gram precision by the system output with respect to the reference translations(Papineni et al., 2002). A variant of BLEU using the arithmetic mean of weighted n-gram precision values(Doddington, 2002). word error rate; The edit distance between the system output and the closest reference translation(Niessen et al., 2000). Position-independent WER; A variant of mWER which disregards word ordering(Och et al., 2001). general text matcher; Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid.(Turian et al., 2003) * Large scores are better in BLEU, NIST and GTM. Small scores are better in WER and PER. are incorrect. Their errors are classiﬁed in Table 3. 4.3 Results The result is shown in Table 2. Because proposed accuracy is slightly higher than basic, the result demonstrates validity of the proposed translation model. 4.4 Contribution of context similarity We investigated the contribution of context similarity to the translation performance. This is condu"
2005.mtsummit-papers.29,P02-1040,0,0.0788812,"ed. BLEU NIST baseline: EBMT baseline, which searches the most similar translation examples by using a character-based DP matching method, and outputs its target parts as is. WER C1, C2: Commercial machine translation systems under rule base approach. PER 4.2 Evaluation Evaluation is conducted based on the following conditions and by using the ﬁve evaluation metrics in Table 1. (1) (2) (3) (4) case insensitive (lower case only) no punctuation marks (.,?!”) no hyphen spelling out numerals GTM The geometric mean of n-gram precision by the system output with respect to the reference translations(Papineni et al., 2002). A variant of BLEU using the arithmetic mean of weighted n-gram precision values(Doddington, 2002). word error rate; The edit distance between the system output and the closest reference translation(Niessen et al., 2000). Position-independent WER; A variant of mWER which disregards word ordering(Och et al., 2001). general text matcher; Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid.(Turian et al., 2003) * Large scores are better in BLEU, NIST and GTM. Small scores are better in WER and PER. are incorrect. Their errors are classiﬁed in Ta"
2005.mtsummit-papers.29,W01-1402,0,0.174752,"Missing"
2005.mtsummit-papers.29,2003.mtsummit-papers.51,0,0.0610725,"yphen spelling out numerals GTM The geometric mean of n-gram precision by the system output with respect to the reference translations(Papineni et al., 2002). A variant of BLEU using the arithmetic mean of weighted n-gram precision values(Doddington, 2002). word error rate; The edit distance between the system output and the closest reference translation(Niessen et al., 2000). Position-independent WER; A variant of mWER which disregards word ordering(Och et al., 2001). general text matcher; Harmonic mean of precision and recall measures for maximum matchings of aligned words in a bitext grid.(Turian et al., 2003) * Large scores are better in BLEU, NIST and GTM. Small scores are better in WER and PER. are incorrect. Their errors are classiﬁed in Table 3. 4.3 Results The result is shown in Table 2. Because proposed accuracy is slightly higher than basic, the result demonstrates validity of the proposed translation model. 4.4 Contribution of context similarity We investigated the contribution of context similarity to the translation performance. This is conducted by performance comparison between the proposed system and without sim, which does not use the thesaurus (Table 2). As shown in Table 2, propose"
2007.tmi-papers.3,C04-1119,0,0.399601,"Missing"
2007.tmi-papers.3,P07-1082,0,0.122253,"Missing"
2007.tmi-papers.3,P07-1016,0,0.188611,"Missing"
2007.tmi-papers.3,C04-1176,0,0.222131,"Missing"
2007.tmi-papers.3,2004.iwslt-evaluation.15,1,0.866642,"Missing"
2007.tmi-papers.3,W02-2017,0,0.0602639,"Missing"
2007.tmi-papers.3,C04-1086,0,0.332628,"Missing"
2007.tmi-papers.3,J93-2003,0,0.00691332,"onstrating the feasibility of the proposed approach. 1 Introduction Orthographic variation can be a serious problem for many natural language-processing (NLP) applications, such as information extraction (IE), question answering (QA), and machine translation (MT). For example, many example-based machine translation (EBMT) (Nagao, 1984) methods, such as (Somers, 1999; Richardson et al., 2001; Sumita, 2001; Carl and Way, 2003; Aramaki and Kurohashi, 2004; Nakazawa et al., 2006), utilize a translation dictionary during bilingual text alignment. Also, several statistical machine translation (SMT)(Brown et al., 1993) methods set initial translation parameters using a translation dictionary. When consulting a dictionary, a system must disambiguate orthographic variation. The following terms are an example of Japanese orthographic variation, corresponding to the term “Avogadro’s number”: 1. アヴォガド ロ数 (A VO GA DO RO SU), 2. アボガド ロ数 (A BO GA DO RO SU). Although both terms are frequently used (term (1) resulted in 25,700 Google hits and Term (2) resulted in 25,000 Google hits1 ), translation dictionaries contain only one of the terms, resulting in low levels of accuracy with dictionary-based bilingual text alig"
2007.tmi-papers.3,2006.iwslt-evaluation.9,0,0.0226517,"Missing"
2007.tmi-papers.3,W01-1402,0,0.0382686,"Missing"
2007.tmi-papers.3,P07-1119,0,0.116381,"Missing"
2007.tmi-papers.3,1999.mtsummit-1.37,0,0.0537099,"Missing"
2007.tmi-papers.3,W98-1005,0,0.546714,"Missing"
2007.tmi-papers.3,W01-1401,0,0.0460231,"Missing"
2007.tmi-papers.3,P07-1015,0,0.131357,"Missing"
2020.coling-main.175,N19-1149,0,0.0864201,"fied characteristics for research on offensive language detection, it is also crucial to focus on video live streaming platforms that share similar features to Twitter, such as Twitch2 , which is a popular live streaming platform mainly for gamers. This paper introduces a chat room that detects offensive expressions in real time and filters them out, as illustrated in Figure 1. To detect offensive expressions, we apply a deep learning approach. Modern neural architectures for natural language processing are generally highly effective when provided with a large amount of labeled training data (Dai et al., 2019). However, large volumes of labeled data are difficult to obtain, especially for video live streaming, as it is an emerging platform. Therefore, we apply a transfer learning approach to solve this problem. Domain adaptation, a sub-field of transfer learning, aims to learn a transferable representation from a source domain and apply it to a target domain (Dai et al., 2019). We propose the use of (1) an existing large labeled social media dataset from Twitter This work is licensed under a Creative Commons Attribution 4.0 International License. //creativecommons.org/licenses/by/4.0/. 1 https://cn"
2020.coling-main.175,N19-1423,0,0.0116496,"8 0.660 0.657 0.664 0.660 0.664 0.672 Table 1: Results of RoBERTa model using live chat datasets. The left column shows the test set. In the training set columns, the training set used in the current test set is checked. We used “Twitter + X” to denote our training sets, where Twitter is the baseline, and “X” refers to its combinations with LCFN , LCGT , LCHS , and LCMC . The TVcC column is the textual similarity of chat posts, and the overlap coefficient column is the game-genre similarity. 2.2 Model and Similarity Measures We adopted the following transformer-based pre-trained models: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019), which have performed well on many natural language processing tasks. We trained each model with 5 epochs and a batch size of 64. After training on the social media dataset, RoBERTa achieved the best performance (0.795 in F1-score). Therefore, we selected RoBERTa as our model. For the similarity measures, we adopted the following approaches: Textual similarity: Because similar expressions in chat posts contribute to transfer learning, we employed Target Vocabulary Covered (TVcC) (Dai et al., 2019) to measure the textual similarity amo"
2020.coling-main.175,N19-1144,0,0.12478,"aming. In our preliminary study using crowdsourcing, seven out of 100 users of live streaming platforms answered that they had sent offensive expressions, while ten users answered that they had been the target of offensive behaviors. Tackling this problem on social media can provide more protection to users and more inspection methods for service providers. Offensive language detection on Twitter has already attracted much attention due to Twitter’s large user base and linguistic idiosyncrasies. For example, SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (Zampieri et al., 2019b) was organized as a shared task for offensive language detection on Twitter. The task consisted of three sub-tasks: (a) offensive language identification, (b) categorization of offense types, and (c) offense target identification. To provide more diversified characteristics for research on offensive language detection, it is also crucial to focus on video live streaming platforms that share similar features to Twitter, such as Twitch2 , which is a popular live streaming platform mainly for gamers. This paper introduces a chat room that detects offensive expressions in real time and filters t"
2020.coling-main.175,S19-2010,0,0.180243,"aming. In our preliminary study using crowdsourcing, seven out of 100 users of live streaming platforms answered that they had sent offensive expressions, while ten users answered that they had been the target of offensive behaviors. Tackling this problem on social media can provide more protection to users and more inspection methods for service providers. Offensive language detection on Twitter has already attracted much attention due to Twitter’s large user base and linguistic idiosyncrasies. For example, SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (Zampieri et al., 2019b) was organized as a shared task for offensive language detection on Twitter. The task consisted of three sub-tasks: (a) offensive language identification, (b) categorization of offense types, and (c) offense target identification. To provide more diversified characteristics for research on offensive language detection, it is also crucial to focus on video live streaming platforms that share similar features to Twitter, such as Twitch2 , which is a popular live streaming platform mainly for gamers. This paper introduces a chat room that detects offensive expressions in real time and filters t"
2020.lrec-1.561,W09-1324,1,0.780113,"numerous variations. In medical NE recognition (NER) tasks, for instance, disease names are a popular NE category. The definition of ‘diseases,’ however, varies substantially along with downstream applications. On the one hand, disease names refer strictly to diagnoses of patients (Wakamiya et al., 2019; Patel et al., 2018). On the other hand, disease names encompass not only diagnoses, but also symptoms and even complaints (Morita et al., 2013; Aramaki et al., 2014). Several corpora have been designed for general purposes covering multiple categories. An example is a Japanese medical corpus (Aramaki et al., 2009) consisting of datetime information (TIMEX), diseases, medication (drug names & dosages), test names, and test values, as well as the relation between drug names and their adverse effects. Because this complex corpus covers multiple NE categories, the annotation cost for 435 discharge summaries was extremely high (over 10,000 USD; during two years). In addition, a French medical corpus, QUAERO (Suominen et al., 2013) uses UMLS3 semantic categories consisting of anatomy, chemical and drugs, devices, disorders, geographic areas, living organisms, objects, phenomena, physiology, and procedures. S"
2020.lrec-1.561,N19-1423,0,0.0234069,"Missing"
2020.lrec-1.561,L18-1375,1,0.71619,"by one if they find an entity that matches our definitions presented above. The following guidelines also support annotators to label entities without extensive medical knowledge, while maintaining coherent annotation: 1. nested structures in which another tag is labeled inside one tag are not allowed; in other words, technical medical terms should be annotated as a single entity 2. most informative entity types such as “diseases and symptoms <D&gt;” and “time expressions <TIMEX3&gt;” are assigned priority over others 3. an easy-to-use reference dictionary for diseases and symptoms such as J-MeDic (Ito et al., 2018) is used when annotators are not confident about the exact span of the entity 4. annotators can take a longer span of a single entity if unsure, especially under the case of complex compound words Although these guidelines might reduce the granularity of annotated entities, it can be controlled after the annotation is completed. It is noteworthy that the granularity, range, and definition of entities depends on the downstream application. We instead assign importance to the ease of labeling for non-medical professionals. Overly finer-grained annotation might impose burdens, especially for such"
2020.lrec-1.561,N16-1030,0,0.0341966,"Missing"
2020.lrec-1.561,D18-1228,0,0.312735,"portrays a summary of existing corpora. As the table shows, the corpora are designed for an idiosyncratic task. Only task-specific annotation was done for each task. For example, the i2b2 Deid corpus includes only personal health information (PHI). The definition (range) of tags also presents numerous variations. In medical NE recognition (NER) tasks, for instance, disease names are a popular NE category. The definition of ‘diseases,’ however, varies substantially along with downstream applications. On the one hand, disease names refer strictly to diagnoses of patients (Wakamiya et al., 2019; Patel et al., 2018). On the other hand, disease names encompass not only diagnoses, but also symptoms and even complaints (Morita et al., 2013; Aramaki et al., 2014). Several corpora have been designed for general purposes covering multiple categories. An example is a Japanese medical corpus (Aramaki et al., 2009) consisting of datetime information (TIMEX), diseases, medication (drug names & dosages), test names, and test values, as well as the relation between drug names and their adverse effects. Because this complex corpus covers multiple NE categories, the annotation cost for 435 discharge summaries was extr"
2020.lrec-1.561,N18-1202,0,0.0685803,"Missing"
2020.lrec-1.561,C73-2019,0,0.510227,"Missing"
2021.bionlp-1.18,2020.acl-main.335,0,0.0199345,"ize of span s. These representations gs are then fed into a simple feed-forward NN, FFNN, and a nonlinear function, GELU (Hendrycks and Gimpel, 2016). Given a particular span representation and a candidate concept as the inputs, we formulate the context score as follows: Experiment Baseline Models We compared several baselines to evaluate our model. DNorm (Leaman et al., 2013) and NormCo (Wright et al., 2019) were used as pipeline models due to their high performance. In addition, we used the pipeline systems consisting of stateof-the-art models: BioBERT (Lee et al., 2020) for NER and BioSyn (Sung et al., 2020) for NEN. TaggerOne (Leaman and Lu, 2016) and Transition-based model (Lou et al., 2017) are used as joint-learning models. These models outperformed the pipeline models in NCBID and BC5CDR. For the model introduced by Zhao et al. (2019), we cannot reproduce the performance reported by them. Instead, we report the performance of the simple token-level joint learning model based on the BioBERT, which referred as “joint (token)”. 3.3 Implementation Dictionary-matching score We used the cosine similarity of the TF-IDF vectors as the dictionary- We performed several preprocessing steps: splitmatchi"
2021.bionlp-1.18,2020.acl-main.748,0,0.0995668,"Missing"
2021.wnut-1.21,N19-1078,0,0.0137472,"trump,” were highly correlated with the fake labels. These results revealed a bias in the relationship between specific person names and labels. The detection model trained on one of these datasets is not adaptable to instances such as the change of president, and thus does not work well on other datasets. 4 4.1 Diachronic bias mitigation Masking methods We examine multiple masking methods, starting from word deletion to word replacement for input text data to mitigate the diachronic bias and to build a robust detection model for out-of-domain data. We utilize Named Entity Recognition (NER) (Akbik et al., 2019) in Flair (Akbik et al., 2018) to search for words to be used as masks. Examples of each masking method are listed in Appendix D. Named Entity (NE) Del: Words tagged with NEs are removed. This masking method aims to build a detection model independent of NE, same as (Suntwal et al., 2019). Basic NER: Words tagged with NEs are replaced with the corresponding labels, such as PER (person label), LOC (location label), etc. WikiD: Words tagged with PER labels are replaced with Wikidata (Wikidata) label, specifically, position held (P39), or alternatively occupation (P106) depending on availability."
2021.wnut-1.21,C18-1139,0,0.0300789,"d with the fake labels. These results revealed a bias in the relationship between specific person names and labels. The detection model trained on one of these datasets is not adaptable to instances such as the change of president, and thus does not work well on other datasets. 4 4.1 Diachronic bias mitigation Masking methods We examine multiple masking methods, starting from word deletion to word replacement for input text data to mitigate the diachronic bias and to build a robust detection model for out-of-domain data. We utilize Named Entity Recognition (NER) (Akbik et al., 2019) in Flair (Akbik et al., 2018) to search for words to be used as masks. Examples of each masking method are listed in Appendix D. Named Entity (NE) Del: Words tagged with NEs are removed. This masking method aims to build a detection model independent of NE, same as (Suntwal et al., 2019). Basic NER: Words tagged with NEs are replaced with the corresponding labels, such as PER (person label), LOC (location label), etc. WikiD: Words tagged with PER labels are replaced with Wikidata (Wikidata) label, specifically, position held (P39), or alternatively occupation (P106) depending on availability. For example, the use of Wikid"
2021.wnut-1.21,D19-1475,0,0.0178667,"ows: LM I(w, l) = p(w, l) · log p(l|w) , We examine diachronic bias by analyzing four fake p(l) news detection datasets with different domains and where p(w, l) is calculated as count(w,l) , p(l|w) as |P | creation periods. Each article and post in these count(w,l) count(l) datasets has a binary label (real/fake). The details count(w) , p(l) as |P |, and |P |is the number of occurrences of all phrases in D. of the datasets are as follows and further informaTable 1 presents bi-grams that are highly corretion is provided in Appendix A: lated with each label in MultiFC and Constraint1 . MultiFC (Augenstein et al., 2019): This is a multi-domain dataset containing over 36,000 head- In MultiFC, the headlines prior to 2015 containing lines from 38 fact-checking organizations. We ex- words referring to the U.S. president at the time, 1 tracted 7,861 headlines from 2007 to 2015 and Appendix B lists bi-grams with high LMI in Horne17 and regarded headlines labeled as “truth!,” “true,” or Celebrity. 183 such as “barack obama,” were highly correlated with fake labels. In Constraint, words such as “bill gates” and “donald trump,” were highly correlated with the fake labels. These results revealed a bias in the relation"
2021.wnut-1.21,N19-1423,0,0.0776819,"Missing"
2021.wnut-1.21,P18-1128,0,0.0248163,"ConWikiD *0.556 0.607 0.570 WikiD+Del 0.544 0.627 0.590 straint achieved low accuracies, ranging from 0.48 WikiD+NER 0.549 0.607 0.570 to 0.58 on the same test set. These results imply the difficulty of generalizing the fake news detection Table 3: Accuracy of out-of-domain data. The leftmodel. NE Del achieved the same of higher accu- most column lists the training set, and each column with accuracy corresponds to each test set. We applied racy than No Mask in 9 out of 12 settings, although it is the simplest masking method. In particular, the statistical significance test by McNemar’s test (Dror et al., 2018) with Bonferroni correction to each method NE Del trained on Horne17 achieved the highest compared to No Mask. * indicates the significant difaccuracy for MultiFC and Celebrity among models ference over No Mask (p < 0.05). trained on Horne17. Although Basic NER trained on Horne17 also achieved the highest accuracy for Constraint, the improvement was smaller compared paratively lower accuracies than WikiD+Del. This to that of NE Del. indicates that we can build a more robust model by Wikidata-based masking methods WikiD and removing entities other than person names. We beWikiD+Del achieved high"
2021.wnut-1.21,C18-1287,0,0.0282228,"t al. (2019) proposed mitigation methods by replacing some words with specific labels to build a robust inference model for out-of-domain data. However, to the best of our knowledge, there has been no study or analysis of bias in fake news detection datasets. 3 3.1 Resources Datasets “mostly true” as real; those labeled “mostly false” or “false” as fake. Horne17 (Horne and Adali, 2017): This dataset contains news articles on the 2016 US presidential election, which are labeled real/fake/satire, based on the investigation of BuzzFeed News. We used articles with fake and real labels. Celebrity (Pérez-Rosas et al., 2018): This dataset is composed of news articles verified by Gossipcop, which targets news related to celebrities. Most of the articles, whose topics are mainly sensational, such as fights between celebrities, were published between 2016 and 2017. Constraint (Patwa et al., 2020): This dataset was used in the CONSTRAINT 2021 shared task and consists of social media posts related to COVID-19. These posts were verified by factchecking sites such as Politifact and Snopes. 3.2 Correlation between phrases and labels We investigated the correlation between phrases and labels to examine bias in each datase"
2021.wnut-1.21,P19-1163,0,0.0284328,"anked bi-grams in MultiFC and Constraint for real and fake labels with their p(l|w). LMI are written as value multiplied by 106 . Person names are written in bold. There is a tendency for real labels to be highly correlated with common phrases, while fake labels are highly correlated with person names. 2 Related Work Analysis and examination of mitigation methods for various types of bias have been conducted for detecting offensive language and hate speech: author bias (Wiegand et al., 2019), annotator bias (Ross et al., 2017), gender bias (Binns et al., 2017; Park et al., 2018), racial bias (Sap et al., 2019), political bias (Wich et al., 2020), etc. Dayanik and Padó (2020) focuses on frequency bias of person name in their dataset, while we handle person name considering the passage of time. In various studies, bias analysis and mitigation are addressed for the fact verification task, where given texts as judged as factual or otherwise from several pieces of evidence, and is one of the recognizing textual entailment tasks. For example, Schuster et al. (2019) and Suntwal et al. (2019) proposed mitigation methods by replacing some words with specific labels to build a robust inference model for out-"
C00-2131,C92-2101,0,0.896063,"Missing"
C00-2131,J94-4001,1,0.829914,", we get nally four phrasal correspondences P1 , P2 , P3 , and P4 . 3 Experiments 3.1 Corpus and Dictionary We used documents from White Papers on Science and Technology (1994 to 1996) published by the Science and Technology Agency (STA) of the Japanese government. STA published these White Papers in both Japanese and English. The Communications Research Laboratory of the Ministry of Posts and Telecommunication of the Japanese government supplied us with the bilingual corpus which is already roughly aligned. We made a bilingual corpus consisting of parsed dependency structures by using the KNP[2] Japanese parser (developed by Kyoto University) for Japanese sentences and the ESG[5] English parser (developed by IBM Watson Research Center) for English sentences. We made about 500 sentence pairs, each of which has a one-to-one sentence correspondence, from the raw data of the White Papers, and selected randomly about 130 sentence pairs for experiments. However, since a parser does not always produce correct parse trees, we excluded some sentence pairs which have severe parse errors, and nally got 115 sentence pairs as a test set. As a translation word dictionary between Japanese and Engli"
C00-2131,P93-1004,0,0.242877,"Missing"
C00-2131,J80-1003,0,0.104143,"Corpus and Dictionary We used documents from White Papers on Science and Technology (1994 to 1996) published by the Science and Technology Agency (STA) of the Japanese government. STA published these White Papers in both Japanese and English. The Communications Research Laboratory of the Ministry of Posts and Telecommunication of the Japanese government supplied us with the bilingual corpus which is already roughly aligned. We made a bilingual corpus consisting of parsed dependency structures by using the KNP[2] Japanese parser (developed by Kyoto University) for Japanese sentences and the ESG[5] English parser (developed by IBM Watson Research Center) for English sentences. We made about 500 sentence pairs, each of which has a one-to-one sentence correspondence, from the raw data of the White Papers, and selected randomly about 130 sentence pairs for experiments. However, since a parser does not always produce correct parse trees, we excluded some sentence pairs which have severe parse errors, and nally got 115 sentence pairs as a test set. As a translation word dictionary between Japanese and English, we rst used J-to-E translation dictionary which has more than 100,000 entries, but"
C00-2131,C96-1078,0,0.54206,"Missing"
C00-2131,C90-3044,0,0.0781638,"These structural correspondences are used as bases of translation patterns in corpus-based approaches. Figure 2 shows an example of extracting structural correspondences. In this gure, the left tree is a Japanese dependency tree, the right tree is a dependency tree of its English translation, dotted arrows represent word correspondence, and a pair of boxes connected by a solid line represent phrasal correspondence. We would like to extract these 1 Introduction So far, a number of methodologies and systems for machine translation using large corpora exist. They include example-based approaches [7, 8, 9, 12], pattern-based approaches [10, 11, 14], and statistical approaches. For instance, example-based approaches use a large set of translation patterns each of which is a pair of parsed structures of a source-language fragment and its target-language translation fragment. Figure 1 shows an example of translation by an example-based method, in which translation patterns (p1) and (p2) are selected as similar to a (left hand) Japanese dependency structure, and an (right hand) English dependency structure is constructed by merging the target parts of these translation patterns1. In this kind of system"
C00-2131,P96-1020,0,0.0185665,"d as bases of translation patterns in corpus-based approaches. Figure 2 shows an example of extracting structural correspondences. In this gure, the left tree is a Japanese dependency tree, the right tree is a dependency tree of its English translation, dotted arrows represent word correspondence, and a pair of boxes connected by a solid line represent phrasal correspondence. We would like to extract these 1 Introduction So far, a number of methodologies and systems for machine translation using large corpora exist. They include example-based approaches [7, 8, 9, 12], pattern-based approaches [10, 11, 14], and statistical approaches. For instance, example-based approaches use a large set of translation patterns each of which is a pair of parsed structures of a source-language fragment and its target-language translation fragment. Figure 1 shows an example of translation by an example-based method, in which translation patterns (p1) and (p2) are selected as similar to a (left hand) Japanese dependency structure, and an (right hand) English dependency structure is constructed by merging the target parts of these translation patterns1. In this kind of system, it is very important to collect a lar"
C00-2131,C96-2211,0,0.0374233,"d as bases of translation patterns in corpus-based approaches. Figure 2 shows an example of extracting structural correspondences. In this gure, the left tree is a Japanese dependency tree, the right tree is a dependency tree of its English translation, dotted arrows represent word correspondence, and a pair of boxes connected by a solid line represent phrasal correspondence. We would like to extract these 1 Introduction So far, a number of methodologies and systems for machine translation using large corpora exist. They include example-based approaches [7, 8, 9, 12], pattern-based approaches [10, 11, 14], and statistical approaches. For instance, example-based approaches use a large set of translation patterns each of which is a pair of parsed structures of a source-language fragment and its target-language translation fragment. Figure 1 shows an example of translation by an example-based method, in which translation patterns (p1) and (p2) are selected as similar to a (left hand) Japanese dependency structure, and an (right hand) English dependency structure is constructed by merging the target parts of these translation patterns1. In this kind of system, it is very important to collect a lar"
C00-2131,C92-2115,1,0.832893,"These structural correspondences are used as bases of translation patterns in corpus-based approaches. Figure 2 shows an example of extracting structural correspondences. In this gure, the left tree is a Japanese dependency tree, the right tree is a dependency tree of its English translation, dotted arrows represent word correspondence, and a pair of boxes connected by a solid line represent phrasal correspondence. We would like to extract these 1 Introduction So far, a number of methodologies and systems for machine translation using large corpora exist. They include example-based approaches [7, 8, 9, 12], pattern-based approaches [10, 11, 14], and statistical approaches. For instance, example-based approaches use a large set of translation patterns each of which is a pair of parsed structures of a source-language fragment and its target-language translation fragment. Figure 1 shows an example of translation by an example-based method, in which translation patterns (p1) and (p2) are selected as similar to a (left hand) Japanese dependency structure, and an (right hand) English dependency structure is constructed by merging the target parts of these translation patterns1. In this kind of system"
C00-2131,1993.tmi-1.25,1,0.870946,"Missing"
C00-2131,P98-2223,1,0.862324,"d as bases of translation patterns in corpus-based approaches. Figure 2 shows an example of extracting structural correspondences. In this gure, the left tree is a Japanese dependency tree, the right tree is a dependency tree of its English translation, dotted arrows represent word correspondence, and a pair of boxes connected by a solid line represent phrasal correspondence. We would like to extract these 1 Introduction So far, a number of methodologies and systems for machine translation using large corpora exist. They include example-based approaches [7, 8, 9, 12], pattern-based approaches [10, 11, 14], and statistical approaches. For instance, example-based approaches use a large set of translation patterns each of which is a pair of parsed structures of a source-language fragment and its target-language translation fragment. Figure 1 shows an example of translation by an example-based method, in which translation patterns (p1) and (p2) are selected as similar to a (left hand) Japanese dependency structure, and an (right hand) English dependency structure is constructed by merging the target parts of these translation patterns1. In this kind of system, it is very important to collect a lar"
C00-2131,C98-2218,1,\N,Missing
C16-1008,D11-1145,1,0.858088,"ions mentioned above, this study particularly examines detection of seasonal influenza epidemics because the influenza detection is a popular application of Twitter. To date, more than 30 Twitter-based influenza detection and prediction systems have been developed worldwide (Charles-Smith et al., 2015). Although the detailed functions of these systems differ, they share the underlying assumption that the flu spreading in the real world is immediately reflected to the tweets. Therefore, most systems have simply aggregated counts of daily flu-related tweets to obtain the current patient status (Aramaki et al., 2011; Collier et al., 2011; Chew and Eysenbach, 2010; Lampos and Cristianini, 2010; Culotta, 2013; Paul et al., 2014). Their typical materials are presented as shown below. • I got a flu I can not go to school for the rest of the week • I was diagnosed with a high fever. Maybe flu :( Although the former tweet is described by an actual influenza patient, the latter one merely expresses a suspicion of flu. From a practical (clinical) perspective, these differences have great importance because This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/l"
C16-1008,P15-1160,1,0.704845,"nd 0.94, respectively). Aramaki et al. (2011) achieved the best score for Japan (correlation ratio = 0.89). This study also examined Twitter data in Japan, and achieved competitive results for nowcasting. Another aspect of reviews of related studies is the manner of tweet counting. In earlier studies, a simple word counting, the direct number of tweets, is considered an index of the degree of disease epidemics. However, such a simple method is adversely affected by the huge numbers of noisy tweets. Currently, counting approaches of two types have been developed: (1) a classification approach (Kanouchi et al., 2015; SUN et al., 2014; Aramaki et al., 2011) aimed at extracting only tweets including patient information, and (2) a regression approach (Lamb et al., 2013; Culotta, 2010; Lampos and Cristianini, 2010; Paul and Dredze, 2011) that handles multiple words to build a precise regression model. The proposed study fundamentally belongs among regression approaches, which explore optimal weight perimeters for each word. An important difference is that this study handles one more parameter for each word: time shift (days). To handle many parameters, we first ascertain the best time shift widths. Then we e"
C16-1008,P15-3005,1,0.862737,"Missing"
C16-1008,N13-1097,0,0.066714,"nd achieved competitive results for nowcasting. Another aspect of reviews of related studies is the manner of tweet counting. In earlier studies, a simple word counting, the direct number of tweets, is considered an index of the degree of disease epidemics. However, such a simple method is adversely affected by the huge numbers of noisy tweets. Currently, counting approaches of two types have been developed: (1) a classification approach (Kanouchi et al., 2015; SUN et al., 2014; Aramaki et al., 2011) aimed at extracting only tweets including patient information, and (2) a regression approach (Lamb et al., 2013; Culotta, 2010; Lampos and Cristianini, 2010; Paul and Dredze, 2011) that handles multiple words to build a precise regression model. The proposed study fundamentally belongs among regression approaches, which explore optimal weight perimeters for each word. An important difference is that this study handles one more parameter for each word: time shift (days). To handle many parameters, we first ascertain the best time shift widths. Then we explore weight parameters using L1 or elastic net. It is noteworthy that this study does not employ any classification method, engaging a room to improve"
C16-1008,W14-5511,0,0.0314219,"Aramaki et al. (2011) achieved the best score for Japan (correlation ratio = 0.89). This study also examined Twitter data in Japan, and achieved competitive results for nowcasting. Another aspect of reviews of related studies is the manner of tweet counting. In earlier studies, a simple word counting, the direct number of tweets, is considered an index of the degree of disease epidemics. However, such a simple method is adversely affected by the huge numbers of noisy tweets. Currently, counting approaches of two types have been developed: (1) a classification approach (Kanouchi et al., 2015; SUN et al., 2014; Aramaki et al., 2011) aimed at extracting only tweets including patient information, and (2) a regression approach (Lamb et al., 2013; Culotta, 2010; Lampos and Cristianini, 2010; Paul and Dredze, 2011) that handles multiple words to build a precise regression model. The proposed study fundamentally belongs among regression approaches, which explore optimal weight perimeters for each word. An important difference is that this study handles one more parameter for each word: time shift (days). To handle many parameters, we first ascertain the best time shift widths. Then we explore weight para"
D11-1145,C10-2005,0,0.0965718,"lly examine the syntactic negation, this study detects the negative influenza, which is a specified semantic negation. Table 5 presents the difference between both negations. In general, the semantic operation is difficult in general. However, this paper revealed that the domain (influenza domain) specific semantic operation provides reasonable results. Another aspect of this study is the target material, Twitter data, which have drawn much attention. Twitter can provide suitable material for many applications such as named entity recognition (NER) (Finin et al., 2010) and sentiment analysis (Barbosa and Feng, 2010). Although these studies specifically examine the fundamental NLP techniques, this study directly targets an NLP application that can contribute to our daily life. 8 Figure 7: An influenza severance system “INFLU kun” using the proposed method is available at http://mednlp.jp/influ/. Conclusion This paper proposed a new Twitter-based influenza epidemics detection method, which relies on the Natural Language Processing (NLP). Our proposed method could successfully filter out the negative influenza tweets (f-measure=0.76), which are posted by the ones who did not actually catch the influenza. Th"
D11-1145,W07-1011,0,0.01118,"corresponds to the real influenza before the epidemic peaks, and vice versa. More acute detection is possible if we incorporate a model considering this aspect of human nature. 1574 7 Related Works The core technology of the proposed method is to classify whether the event is positive or negative. This task is similar to negation identification, which is a traditional topic, especially in medical fields. Therefore, we can find many previous studies of the topic in the relevant literature. An algorithm based approach, NegEx (Chapman et al., 2001), Negfinder (Mutalik et al., 2001), and ConText (Chapman et al., 2007), a machine learning based approach (Elkin et al., 2005; Huang and H.J. Lowe, 2007). Previous Negation (Syntactic) I caught a flu. Positive sentence I don’t have the flu! Negative sentence I have enough flu drugs. Positive sentence I have not recovered from Negative the flu. sentence This study: Negative Influenza (Semantic) Positive Influenza Negative Influenza Negative Influenza Positive Influenza Table 5: Our target influenza negation (semantic) and previous negation (syntactic) Although these approaches specifically examine the syntactic negation, this study detects the negative influenza,"
D11-1145,W10-0713,0,0.0386043,"ntactic) Although these approaches specifically examine the syntactic negation, this study detects the negative influenza, which is a specified semantic negation. Table 5 presents the difference between both negations. In general, the semantic operation is difficult in general. However, this paper revealed that the domain (influenza domain) specific semantic operation provides reasonable results. Another aspect of this study is the target material, Twitter data, which have drawn much attention. Twitter can provide suitable material for many applications such as named entity recognition (NER) (Finin et al., 2010) and sentiment analysis (Barbosa and Feng, 2010). Although these studies specifically examine the fundamental NLP techniques, this study directly targets an NLP application that can contribute to our daily life. 8 Figure 7: An influenza severance system “INFLU kun” using the proposed method is available at http://mednlp.jp/influ/. Conclusion This paper proposed a new Twitter-based influenza epidemics detection method, which relies on the Natural Language Processing (NLP). Our proposed method could successfully filter out the negative influenza tweets (f-measure=0.76), which are posted by the o"
I05-7008,J94-4001,0,\N,Missing
I05-7008,P98-2180,0,\N,Missing
I05-7008,C98-2175,0,\N,Missing
I05-7008,P99-1014,0,\N,Missing
I05-7008,C02-1120,0,\N,Missing
I08-1007,P07-1083,0,0.0453037,"ns of both these methods( hybrid-model(Bilac and Tanaka, 2004) and correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005)). Our proposed method employed a grapheme-based approach. We selected this kind of approach because it allows us to handle not only transliteration but also character omissions/substitutions, which we would not be able to address using a phoneme-based approach (and a combination approach). Yoon et al. (2007) also proposed a discriminative transliteration method, but their system was based on determining whether a target term was transliterated from a source term. Bergsma and Kondrak (2007) and Aramaki et al. (2007) proposed on a discriminative method for similar spelling terms. However, they did not deal with a transliterated probability. Masuyama et al. (2004) collected 178,569 Japanese transliteration variants (positive examples) from a large corpus. In contrast, we collected both positive and negative examples in order to train the classiﬁer. guages While our experimental set consisted of medical terms, including a few transliterations from Latin or German, transliteration-probability was trained using transliterations from the English language (using a general dictionary)."
I08-1007,C04-1119,0,0.468877,"Missing"
I08-1007,P07-1082,0,0.0669213,"the most relevant ﬁeld to our work, because it results in many orthographic variations. Most previous transliteration studies have focused on ﬁnding the most suitable back-transliteration of a term. For example, Knight (1998) proposed a probabilistic model for transliteration. Goto et al.(2004) proposed a similar method, utilizing surrounding characters. Their method is not only applicable to Japanese; it has already been used for Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). Our method uses a different kind of task-setting, compared to previous methods. It is based on determining whether two terms within the same language are equivalent. It provides high levels of accuracy, which should be practical for many applications. Another issue is that of how to represent transliteration phenomena. Methods can be classiﬁed into three main types: grapheme-based (Li et al., 2004); phoneme-based (Knight and Graehl, 6 Conclusion We developed an SVM-based orthographic disambiguation classiﬁer, incorporating transliteration probability. We also developed a method for collectin"
I08-1007,P07-1016,0,0.0301052,"Section 1, transliteration is the most relevant ﬁeld to our work, because it results in many orthographic variations. Most previous transliteration studies have focused on ﬁnding the most suitable back-transliteration of a term. For example, Knight (1998) proposed a probabilistic model for transliteration. Goto et al.(2004) proposed a similar method, utilizing surrounding characters. Their method is not only applicable to Japanese; it has already been used for Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). Our method uses a different kind of task-setting, compared to previous methods. It is based on determining whether two terms within the same language are equivalent. It provides high levels of accuracy, which should be practical for many applications. Another issue is that of how to represent transliteration phenomena. Methods can be classiﬁed into three main types: grapheme-based (Li et al., 2004); phoneme-based (Knight and Graehl, 6 Conclusion We developed an SVM-based orthographic disambiguation classiﬁer, incorporating transliteration probability. We als"
I08-1007,C04-1176,0,0.483612,"oth terms (t1 and t2 ) were transliterated from the same source term (s), to the machine learning features. Experimental results yielded high levels of accuracy, demonstrating the feasibility of the proposed approach. spaghetti Thompson operation * hi indicates a pronunciation. () indicates a translation. this paper, these variations can be termed orthographic variants. The Japanese language, in particular, contains many orthographic variants, for two main reasons: 1. It imports many words from other languages using transliteration, resulting in many possible spelling variations. For example, Masuyama et al. (2004) found at least six different spellings for“ spaghetti ”in newspaper articles (Table 1 Left). 2. Many characters in Japanese nouns can be omitted or substituted, leading to tons of insertion variations (Daille et al., 1996) (Table 1 Right). 1 Introduction Spelling variations, such as “center” and “centre”, which have different spellings but identical meanings, are problematic for many NLP applications including information extraction (IE), question answering (QA), and machine transliteration (MT). In To address these problems, this study developed a support vector machine (SVM) based classiﬁer"
I08-1007,2007.tmi-papers.3,1,0.711447,"d-model(Bilac and Tanaka, 2004) and correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005)). Our proposed method employed a grapheme-based approach. We selected this kind of approach because it allows us to handle not only transliteration but also character omissions/substitutions, which we would not be able to address using a phoneme-based approach (and a combination approach). Yoon et al. (2007) also proposed a discriminative transliteration method, but their system was based on determining whether a target term was transliterated from a source term. Bergsma and Kondrak (2007) and Aramaki et al. (2007) proposed on a discriminative method for similar spelling terms. However, they did not deal with a transliterated probability. Masuyama et al. (2004) collected 178,569 Japanese transliteration variants (positive examples) from a large corpus. In contrast, we collected both positive and negative examples in order to train the classiﬁer. guages While our experimental set consisted of medical terms, including a few transliterations from Latin or German, transliteration-probability was trained using transliterations from the English language (using a general dictionary). Therefore, PROPOSED+TR res"
I08-1007,C02-1099,0,0.0561473,"nglish languages. In a general domain, SIM-TR and PROPOSED+TR would probably yield higher accuracy. 5 Related Works As noted in Section 1, transliteration is the most relevant ﬁeld to our work, because it results in many orthographic variations. Most previous transliteration studies have focused on ﬁnding the most suitable back-transliteration of a term. For example, Knight (1998) proposed a probabilistic model for transliteration. Goto et al.(2004) proposed a similar method, utilizing surrounding characters. Their method is not only applicable to Japanese; it has already been used for Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). Our method uses a different kind of task-setting, compared to previous methods. It is based on determining whether two terms within the same language are equivalent. It provides high levels of accuracy, which should be practical for many applications. Another issue is that of how to represent transliteration phenomena. Methods can be classiﬁed into three main types: grapheme-based (Li et al., 2004); phoneme-based (Knight and Graehl,"
I08-1007,I05-1040,0,0.0616516,"n a general domain, SIM-TR and PROPOSED+TR would probably yield higher accuracy. 5 Related Works As noted in Section 1, transliteration is the most relevant ﬁeld to our work, because it results in many orthographic variations. Most previous transliteration studies have focused on ﬁnding the most suitable back-transliteration of a term. For example, Knight (1998) proposed a probabilistic model for transliteration. Goto et al.(2004) proposed a similar method, utilizing surrounding characters. Their method is not only applicable to Japanese; it has already been used for Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). Our method uses a different kind of task-setting, compared to previous methods. It is based on determining whether two terms within the same language are equivalent. It provides high levels of accuracy, which should be practical for many applications. Another issue is that of how to represent transliteration phenomena. Methods can be classiﬁed into three main types: grapheme-based (Li et al., 2004); phoneme-based (Knight and Graehl, 6 Conclusion We dev"
I08-1007,2007.mtsummit-papers.47,0,0.294223,"SIM-TR and PROPOSED+TR would probably yield higher accuracy. 5 Related Works As noted in Section 1, transliteration is the most relevant ﬁeld to our work, because it results in many orthographic variations. Most previous transliteration studies have focused on ﬁnding the most suitable back-transliteration of a term. For example, Knight (1998) proposed a probabilistic model for transliteration. Goto et al.(2004) proposed a similar method, utilizing surrounding characters. Their method is not only applicable to Japanese; it has already been used for Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). Our method uses a different kind of task-setting, compared to previous methods. It is based on determining whether two terms within the same language are equivalent. It provides high levels of accuracy, which should be practical for many applications. Another issue is that of how to represent transliteration phenomena. Methods can be classiﬁed into three main types: grapheme-based (Li et al., 2004); phoneme-based (Knight and Graehl, 6 Conclusion We developed an SVM-based ort"
I08-1007,P07-1119,0,0.0297772,"uracy. 5 Related Works As noted in Section 1, transliteration is the most relevant ﬁeld to our work, because it results in many orthographic variations. Most previous transliteration studies have focused on ﬁnding the most suitable back-transliteration of a term. For example, Knight (1998) proposed a probabilistic model for transliteration. Goto et al.(2004) proposed a similar method, utilizing surrounding characters. Their method is not only applicable to Japanese; it has already been used for Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). Our method uses a different kind of task-setting, compared to previous methods. It is based on determining whether two terms within the same language are equivalent. It provides high levels of accuracy, which should be practical for many applications. Another issue is that of how to represent transliteration phenomena. Methods can be classiﬁed into three main types: grapheme-based (Li et al., 2004); phoneme-based (Knight and Graehl, 6 Conclusion We developed an SVM-based orthographic disambiguation classiﬁer, incorporating translite"
I08-1007,W98-1005,0,0.121986,"probably yield higher accuracy. 5 Related Works As noted in Section 1, transliteration is the most relevant ﬁeld to our work, because it results in many orthographic variations. Most previous transliteration studies have focused on ﬁnding the most suitable back-transliteration of a term. For example, Knight (1998) proposed a probabilistic model for transliteration. Goto et al.(2004) proposed a similar method, utilizing surrounding characters. Their method is not only applicable to Japanese; it has already been used for Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). Our method uses a different kind of task-setting, compared to previous methods. It is based on determining whether two terms within the same language are equivalent. It provides high levels of accuracy, which should be practical for many applications. Another issue is that of how to represent transliteration phenomena. Methods can be classiﬁed into three main types: grapheme-based (Li et al., 2004); phoneme-based (Knight and Graehl, 6 Conclusion We developed an SVM-based orthographic disambiguation classiﬁ"
I08-1007,P07-1015,0,0.0438501,"Missing"
I08-1007,P04-1021,0,\N,Missing
I08-1007,C04-1086,0,\N,Missing
I08-1007,J98-4003,0,\N,Missing
P15-1160,D11-1145,1,0.808815,"suggest that our task is independent of the type of disease/symptom. 1 Introduction Social media services, including Twitter and Facebook, provide opportunities for individuals to share their experiences, thoughts, and opinions. The wide use of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is flu surveillance, i.e., predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples incl"
P15-1160,N13-1121,0,0.066141,"Missing"
P15-1160,J96-2004,0,0.0431357,"Missing"
P15-1160,P14-2111,0,0.0261289,"symptoms of another person in such cases. In other words, the episode classifier can predict a positive label for these symptoms without knowing the subjects of these symptoms. 4 Related Work 4.1 Twitter and NLP NLP researchers have addressed two major directions for Twitter: adapting existing NLP technologies to noisy texts and extracting useful knowledge from Twitter. The former includes improving the accuracy of part-of-speech tagging (Gimpel et al., 2011) and named entity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7 For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and"
P15-1160,P10-1160,0,0.0932612,"the task of identifying subjects is independent of the type of diseases/symptom. We verify the possibility of transferring supervision data to different targets of diseases and symptoms. In other words, we verify that it is possible to utilize the supervision data for a particular disease/symptom to improve the accuracy of predicting subjects of another disease/symptom. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example, the subject of the verb “beat” is the first person “I” in the sent"
P15-1160,P11-2008,0,0.131837,"Missing"
P15-1160,P11-1038,0,0.037841,"we seldom mention the symptoms of another person in such cases. In other words, the episode classifier can predict a positive label for these symptoms without knowing the subjects of these symptoms. 4 Related Work 4.1 Twitter and NLP NLP researchers have addressed two major directions for Twitter: adapting existing NLP technologies to noisy texts and extracting useful knowledge from Twitter. The former includes improving the accuracy of part-of-speech tagging (Gimpel et al., 2011) and named entity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7 For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), even"
P15-1160,P13-4002,0,0.0262922,"cted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of"
P15-1160,N13-1097,0,0.258011,"Missing"
P15-1160,D14-1214,0,0.0348142,"nner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to Pro"
P15-1160,P14-1016,0,0.0322227,"nner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to Pro"
P15-1160,E12-1062,0,0.0297451,", first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun"
P15-1160,W04-2705,0,0.134012,"challenges in this study. 2. The experimental results show that the task of identifying subjects is independent of the type of diseases/symptom. We verify the possibility of transferring supervision data to different targets of diseases and symptoms. In other words, we verify that it is possible to utilize the supervision data for a particular disease/symptom to improve the accuracy of predicting subjects of another disease/symptom. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example,"
P15-1160,J05-1004,0,0.0123815,"r profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG 0 (typically, subject or agent) is “customer” and ARG 1 (typically, objects, patients, themes) is “issue” for the nominal predicate “complaints” in the sentence “There have been no customer complaints about that issue.” Gerber and Chai (2010) improved the coverage of NomBank by handling implicit arguments. Some studies have addressed the task of identifying implicit and omitted arguments for nominal predicates in Japanese (Komachi et al., 2007; Sasano et al., 2008). Our wo"
P15-1160,C14-1168,0,0.0255805,"ably. This is because the subjects for these symptoms are mostly F IRST P ERSON, as we seldom mention the symptoms of another person in such cases. In other words, the episode classifier can predict a positive label for these symptoms without knowing the subjects of these symptoms. 4 Related Work 4.1 Twitter and NLP NLP researchers have addressed two major directions for Twitter: adapting existing NLP technologies to noisy texts and extracting useful knowledge from Twitter. The former includes improving the accuracy of part-of-speech tagging (Gimpel et al., 2011) and named entity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7 For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock ma"
P15-1160,C04-1174,0,0.0455393,"tudy. 2. The experimental results show that the task of identifying subjects is independent of the type of diseases/symptom. We verify the possibility of transferring supervision data to different targets of diseases and symptoms. In other words, we verify that it is possible to utilize the supervision data for a particular disease/symptom to improve the accuracy of predicting subjects of another disease/symptom. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example, the subject of the v"
P15-1160,C08-1097,0,0.0123551,"opBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG 0 (typically, subject or agent) is “customer” and ARG 1 (typically, objects, patients, themes) is “issue” for the nominal predicate “complaints” in the sentence “There have been no customer complaints about that issue.” Gerber and Chai (2010) improved the coverage of NomBank by handling implicit arguments. Some studies have addressed the task of identifying implicit and omitted arguments for nominal predicates in Japanese (Komachi et al., 2007; Sasano et al., 2008). Our work shares a similar goal with the abovementioned studies, i.e., identifying an implicit ARG 0 for a disease and symptom. However, these studies do not regard a disease/symptom as a nominal predicate because they consider verb nominalizations as nominal predicates. In addition, 1667 they use a corpus that consists of newswire text, the writing style and word usage of which differ considerably from those of tweets. For these reasons, we proposed a novel task setting for identifying subjects of diseases and symptoms, and we built an annotated corpus for developing the subject classifier a"
P15-1160,N13-1135,0,0.0604966,"Missing"
P15-1160,P13-2005,0,0.0142294,"normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7 For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be"
P15-1160,P12-2044,0,0.0303353,". The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG 0 (typically, subject or agent) is “customer” and ARG 1"
P15-1160,P14-2114,0,0.0245499,"ode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates"
P15-1160,P15-3005,1,\N,Missing
P15-1160,P13-1159,0,\N,Missing
P15-3005,D11-1145,1,0.801339,"nalysis: shallow modality analysis based on a surface string match and deep modality analysis based on predicate-argument structure analysis. The main contribution of this paper is two-fold: Consequently, this study proposes the use of modality features to improve disease event detection from Twitter messages, or “tweets”. Experimental results demonstrate that the combination of a modality dictionary and a modality analyzer improves the F1-score by 3.5 points. 1 • We annotate a new dataset extracted from Twitter for flu detection and prediction task, and extend the naïve bag-of-words model of Aramaki et al. (2011) and propose several Twitter-specific features for disease event detection tasks. Introduction The rapidly increasing popularity of Social Networking Services (SNSs) such as Twitter and Facebook has greatly eased the dissemination of information. Such data can serve as a valuable information resource for various applications. For instance, Huberman et al. (2009) investigated actual linked structures of human networks, Boyd et al. (2010) mapped out retweeting as a conversational practice, and Sakaki et al. (2010) detected earthquakes using SNSs. An important and widespread application of SNS mi"
P15-3005,P15-1160,1,0.513046,"Missing"
P15-3005,D14-1214,0,0.0613251,"a from Twitter texts, but the main challenge is to filter noise from this data. For example, Aramaki et al. (2011) reported that half of the tweets containing the word “cold (disease)” simply mention some information about a disease, but do not refer to the actual eventuality of having the disease. To address that problem, a classifier was produced to ascertain the factuality of the disease event. This paper follows that approach, using modality analysis, which provides a strong clue for factuality analysis (Saurí and Pustejovsky, 2012). Modality has been used and discussed in various places. Li et al. (2014) employ such modality features, although they do not describe the effect of using modality features in web application tasks. Furthermore, several workshops have been organized around the use of specific modalities, such as Negation and Speculation (e.g. NeSPNLP1 ). In this study, we use generic modality features to improve factuality analysis. 1 3.2 Shallow modality feature In Japanese, multiple words can serve as a function word as a whole (Matsuyoshi et al., 2007). We designate them as “functional expressions.” Even though functional expressions often carry modality information, previous wo"
P15-3005,matsuyoshi-etal-2010-annotating,0,0.0757664,"ies (Aramaki et al., 2011) 2 . If a tweet writer (or anybody near the writer) is infected with influenza, then the label is positive. Otherwise, the label is negative. Additionally, we save the time stamp when the tweet was posted online. Table 1 presents some examples. For this study, we use 10,443 Japanese tweet messages including the word “flu.” In this dataset, the number of positive examples is 1,319; the number of negative examples is 9,124. Because language heavily relies on modality to judge the factuality of sentences, modality analysis is a necessary process for factuality analysis (Matsuyoshi et al., 2010b). In line with this observation, we propose two ways to incorporate modality analysis for factuality analysis. Twitter is the SNS that is most frequently used for influenza detection (Achrekar et al., 2012; Aramaki et al., 2011; Ji et al., 2012; Sadilek et al., 2012; Lamb, 2013). Previous research on the subject has revealed a high correlation ratio between the number of influenza patients and actual tweets related to influenza. It is possible to obtain large amounts of data from Twitter texts, but the main challenge is to filter noise from this data. For example, Aramaki et al. (2011) repor"
P15-3005,J12-2002,0,0.0273849,"and actual tweets related to influenza. It is possible to obtain large amounts of data from Twitter texts, but the main challenge is to filter noise from this data. For example, Aramaki et al. (2011) reported that half of the tweets containing the word “cold (disease)” simply mention some information about a disease, but do not refer to the actual eventuality of having the disease. To address that problem, a classifier was produced to ascertain the factuality of the disease event. This paper follows that approach, using modality analysis, which provides a strong clue for factuality analysis (Saurí and Pustejovsky, 2012). Modality has been used and discussed in various places. Li et al. (2014) employ such modality features, although they do not describe the effect of using modality features in web application tasks. Furthermore, several workshops have been organized around the use of specific modalities, such as Negation and Speculation (e.g. NeSPNLP1 ). In this study, we use generic modality features to improve factuality analysis. 1 3.2 Shallow modality feature In Japanese, multiple words can serve as a function word as a whole (Matsuyoshi et al., 2007). We designate them as “functional expressions.” Even t"
P19-1202,H05-1042,0,0.183494,"Missing"
P19-1202,P16-1154,0,0.0600758,"ed data including sports commentary (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecast (Liang et al., 2009; Mei et al., 2016), biographical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metrics for measuring the informativeness"
P19-1202,P16-1014,0,0.0614873,"sports commentary (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecast (Liang et al., 2009; Mei et al., 2016), biographical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metrics for measuring the informativeness of generated summaries."
P19-1202,D18-1130,0,0.0233321,"that the both consider a sequence of data records as content planning. However, our proposal differs from theirs in that ours uses a recurrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropr"
P19-1202,D17-1195,0,0.0211576,"rrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropriate attribute in each timestep, updates their states, and generates coherent summaries from the selected data record. 3 Data Through c"
P19-1202,D16-1032,0,0.014764,"that ours uses a recurrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropriate attribute in each timestep, updates their states, and generates coherent summaries from the selected data record."
P19-1202,N16-1099,1,0.856363,"ea is similar to ours in that the both consider a sequence of data records as content planning. However, our proposal differs from theirs in that ours uses a recurrent neural network for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salien"
P19-1202,D16-1128,0,0.141635,"Missing"
P19-1202,N18-1204,0,0.0431553,"Missing"
P19-1202,P09-1011,0,0.277927,"Missing"
P19-1202,D15-1166,0,0.0235551,"riptions from structured or non-structured data including sports commentary (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecast (Liang et al., 2009; Mei et al., 2016), biographical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metric"
P19-1202,N16-1086,0,0.161392,"Missing"
P19-1202,P02-1040,0,0.103895,"rmation to each of the two stages, we can clearly see which part of the model to which the writer information contributes to. For Puduppully et al. (2019) model, we attach the writer information in the following three ways: 1. concatenating writer embedding w with the input vector for LSTM in the content planning decoder (stage 1); 2. concatenating writer embedding w with the input vector for LSTM in the text generator (stage 2); 3. using both 1 and 2 above. For more details about each decoding stage, readers can refer to Puduppully et al. (2019). 5.3 As evaluation metrics, we use BLEU score (Papineni et al., 2002) and the extractive metrics proposed by Wiseman et al. (2017), i.e., relation generation (RG), content selection (CS), and content ordering (CO) as evaluation metrics. The extractive metrics measure how well the relations extracted from the generated summary match the correct relations6 : 6 5 Our code is available from https://github.com/ aistairc/sports-reporter Evaluation metrics The model for extracting relation tuples was trained on tuples made from the entity (e.g., team name, city name, player name) and attribute value (e.g., “Lakers”, “92”) ex2107 - RG: the ratio of the correct relation"
P19-1202,Q17-1007,0,0.0173455,"phical text from infobox in Wikipedia (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018) and market comments from stock prices (Murakami et al., 2017; Aoki et al., 2018). Neural generation methods have become the mainstream approach for data-to-text generation. The encoder-decoder framework (Cho et al., 2014; Sutskever et al., 2014) with the attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanism (Gu et al., 2016; Gulcehre et al., 2016) has successfully applied to data-to-text tasks. However, neural generation methods sometimes yield fluent but inadequate descriptions (Tu et al., 2017). In data-to-text generation, descriptions inconsistent to the input data are problematic. Recently, Wiseman et al. (2017) introduced the ROTOW IRE dataset, which contains multisentence summaries of basketball games with boxscore (Table 1). This dataset requires the selection of a salient subset of data records for generating descriptions. They also proposed automatic evaluation metrics for measuring the informativeness of generated summaries. Puduppully et al. (2019) proposed a two-stage method that first predicts the sequence of data records to be mentioned and then generates a summary condi"
P19-1202,D17-1197,0,0.0124483,"ork for saliency tracking, and that our decoder dynamically chooses a data record to be mentioned without fixing a sequence of data records. 2.2 Memory modules The memory network can be used to maintain and update representations of the salient information (Weston et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). This module is often used in natural language understanding to keep track of the entity state (Kobayashi et al., 2016; Hoang et al., 2018; Bosselut et al., 2018). Recently, entity tracking has been popular for generating coherent text (Kiddon et al., 2016; Ji et al., 2017; Yang et al., 2017; Clark et al., 2018). Kiddon et al. (2016) proposed a neural checklist model that updates predefined item states. Ji et al. (2017) proposed an entity representation for the language model. Updating entity tracking states when the entity is introduced, their method selects the salient entity state. Our model extends this entity tracking module for data-to-text generation tasks. The entity tracking module selects the salient entity and appropriate attribute in each timestep, updates their states, and generates coherent summaries from the selected data record. 3 Data Through careful examination,"
P19-1202,P98-2209,0,0.875231,"Missing"
S07-1103,C92-2082,0,0.0534204,"in other methods. 1. Most entities (except abstract entities) have a physical size. 2. Several semantic relations are sensitive to physical size. For example, a content-container relation (e1 content-container e2) naturally means that e1 has a smaller size than e2. A book is also smaller than its container, library. A partwhole relation has a similar constraint. 1 Introduction Classification of semantic relations is important to NLP as it would benefit many NLP applications, such as machine translation and information retrieval. Researchers have already proposed various schemes. For example, Hearst (1992) manually designed lexico-syntactic patterns for extracting is-a relations. Berland and Charniak (1999) proposed a similar method for part-whole relations. Brin (1998) employed a bootstrapping algorithm for more specific relations (author-book relations). Kim and Baldwin (2006) and Moldovan et al.(2004) focused on nominal relations in compound nouns. Turney (2005) measured relation similarity between two words. While these methods differ, they all utilize lexical patterns between two entities. Within this context, our goal was to utilize information specific to an entity. Although entities con"
S07-1103,P06-2064,0,0.0219184,"book is also smaller than its container, library. A partwhole relation has a similar constraint. 1 Introduction Classification of semantic relations is important to NLP as it would benefit many NLP applications, such as machine translation and information retrieval. Researchers have already proposed various schemes. For example, Hearst (1992) manually designed lexico-syntactic patterns for extracting is-a relations. Berland and Charniak (1999) proposed a similar method for part-whole relations. Brin (1998) employed a bootstrapping algorithm for more specific relations (author-book relations). Kim and Baldwin (2006) and Moldovan et al.(2004) focused on nominal relations in compound nouns. Turney (2005) measured relation similarity between two words. While these methods differ, they all utilize lexical patterns between two entities. Within this context, our goal was to utilize information specific to an entity. Although entities contain many types of information, we focused on the physical size of an entity. Here, physical size refers Our next problem was how to determine physical sizes. First, we used Google to conduct Web searches using queries such as “book (*cm x*cm)” and “library (*m x*m)”. Next, we"
S07-1103,W04-2609,0,0.0968388,"Missing"
S07-1103,P99-1008,0,0.0297007,"everal semantic relations are sensitive to physical size. For example, a content-container relation (e1 content-container e2) naturally means that e1 has a smaller size than e2. A book is also smaller than its container, library. A partwhole relation has a similar constraint. 1 Introduction Classification of semantic relations is important to NLP as it would benefit many NLP applications, such as machine translation and information retrieval. Researchers have already proposed various schemes. For example, Hearst (1992) manually designed lexico-syntactic patterns for extracting is-a relations. Berland and Charniak (1999) proposed a similar method for part-whole relations. Brin (1998) employed a bootstrapping algorithm for more specific relations (author-book relations). Kim and Baldwin (2006) and Moldovan et al.(2004) focused on nominal relations in compound nouns. Turney (2005) measured relation similarity between two words. While these methods differ, they all utilize lexical patterns between two entities. Within this context, our goal was to utilize information specific to an entity. Although entities contain many types of information, we focused on the physical size of an entity. Here, physical size refer"
W03-0312,A00-2018,0,\N,Missing
W03-0312,S01-1009,1,\N,Missing
W03-0312,2002.tmi-papers.9,0,\N,Missing
W03-0312,J94-4001,1,\N,Missing
W03-0312,W01-1402,0,\N,Missing
W03-0312,C94-1015,0,\N,Missing
W03-0312,C90-3044,0,\N,Missing
W03-0312,W01-1406,0,\N,Missing
W03-0312,2001.mtsummit-ebmt.4,0,\N,Missing
W03-0312,C90-3101,0,\N,Missing
W03-0312,2001.mtsummit-papers.5,1,\N,Missing
W09-1324,I08-1007,1,0.800498,"Missing"
W09-1324,J94-4001,0,0.0456816,"Missing"
W09-1324,W06-0305,0,0.0589679,"Missing"
W09-1324,N06-1009,0,0.0200422,"Missing"
W09-1324,W08-0606,0,0.105235,"Missing"
W09-1324,W07-1011,0,\N,Missing
W09-3513,P07-1016,0,0.0176822,"raining (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl, 1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classified into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grapheme-based approach. Employing phonemes is a challeng"
W09-3513,C02-1099,0,0.034715,"llows: (1) EN–KO requires long training time (11 minutes) which gave poor performance (0.17 ACC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl, 1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classified into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re"
W09-3513,I05-1040,0,0.018048,"quires long training time (11 minutes) which gave poor performance (0.17 ACC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl, 1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classified into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh"
W09-3513,2007.mtsummit-papers.47,0,0.0294339,"g time (11 minutes) which gave poor performance (0.17 ACC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl, 1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classified into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Ou"
W09-3513,I08-1007,1,0.821883,"07). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classified into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grapheme-based approach. Employing phonemes is a challenge reserved for future studies. Aramaki et al. (2008) proposed a discrimina67 tive transliteration approach using Support Vector Machines (SVMs). However, their goal, which is to judge whether two terms come from the same English words or not, differs from this paper goal. A. Kumaran and Tobias Kellner. 2007. A generic framework for machine transliteration. In SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 721–722. 5 Conclusions Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine transliteration. In Proceedings of the Meeting"
W09-3513,P07-1119,0,0.040022,"CC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl, 1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classified into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grapheme-based approach. Employ"
W09-3513,C04-1086,0,0.0189502,"ous languages: for Japanese (Goto et al., 2004; Knight and Graehl, 1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classified into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grapheme-based approach. Employing phonemes is a challenge reserved for future studies. Aramaki et al. (2008) proposed a discrimina67 tive transliteration approach using Support Vector Machines (SVMs). However, their goal, which is to judge whether two terms come from the same English words or not, differs from this paper goal. A. Kumaran and Tobias Kellner. 2007. A generic framework for machine transliteration. In SIGIR ’07: Proceedings of the 30th a"
W09-3513,W98-1005,0,0.0543667,"poor performance (0.17 ACC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl, 1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classified into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grap"
W09-3513,J93-2003,0,0.0158151,"s numerous labels (536 labels), EN–RU needs only 131 labels. This divergence roughly corresponds to both training-time and accuracy as follows: (1) EN–KO requires long training time (11 minutes) which gave poor performance (0.17 ACC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl, 1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classified into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combina"
W09-3513,C04-1119,0,0.016879,"corresponds to both training-time and accuracy as follows: (1) EN–KO requires long training time (11 minutes) which gave poor performance (0.17 ACC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl, 1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classified into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-"
W09-3513,P07-1082,0,0.0161212,"ich gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl, 1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classified into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grapheme-based approach. Employing phonemes is a challenge reserved for future studies. Ara"
W09-3513,P04-1021,0,\N,Missing
W09-3513,J98-4003,0,\N,Missing
W10-3911,S07-1085,0,0.0254014,"described in previous reports in the bio-medical domain. Friedman et al. (1994) describes the certainty in findings of clinical radiology. Certainty is also known in scientific papers of biomedical domains as speculation (Light et al., 2004). Vincze et al. (2008) are producing a freely available corpus including annotations of uncertainty along with its scope. Dependency structure feature which we utilized to extract adverse–effect relations are widely used in relation extraction tasks. We present previous works which used syntactic/dependency information as a feature of a statistical method. Beamer et al. (2007), Giuliano et al. (2007), and Hendrickx et al. (2007) all used syntactic information with machine learning techniques in SemEval-2007 Task:04 and achieved good performance. Riedel et al. (2009) used dependency path features with a statistical relational learning method in BioNLP’09 Shared Task on Event Extraction and achieved the best performance in the event enrichment subtask. Miyao et al. (2008) compared syntactic information of various statistical parsers on PPI. 3 3.1 Texts Comprising the Corpus We used a discharge summary among various documents in a hospital as the source data of the ta"
W10-3911,W04-3103,0,0.029573,"elation. The protein–protein interaction (PPI) annotation extraction task of BioCreative II (Krallinger et al., 2008) is a task to extract PPI from PubMed abstracts. BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009) is a task to extract bio-molecular events (bioevents) from the GENIA event corpus. Similar characteristics to those of the adverse–effect relation are described in previous reports in the bio-medical domain. Friedman et al. (1994) describes the certainty in findings of clinical radiology. Certainty is also known in scientific papers of biomedical domains as speculation (Light et al., 2004). Vincze et al. (2008) are producing a freely available corpus including annotations of uncertainty along with its scope. Dependency structure feature which we utilized to extract adverse–effect relations are widely used in relation extraction tasks. We present previous works which used syntactic/dependency information as a feature of a statistical method. Beamer et al. (2007), Giuliano et al. (2007), and Hendrickx et al. (2007) all used syntactic information with machine learning techniques in SemEval-2007 Task:04 and achieved good performance. Riedel et al. (2009) used dependency path featur"
W10-3911,P08-1006,0,0.149294,"tilized to extract adverse–effect relations are widely used in relation extraction tasks. We present previous works which used syntactic/dependency information as a feature of a statistical method. Beamer et al. (2007), Giuliano et al. (2007), and Hendrickx et al. (2007) all used syntactic information with machine learning techniques in SemEval-2007 Task:04 and achieved good performance. Riedel et al. (2009) used dependency path features with a statistical relational learning method in BioNLP’09 Shared Task on Event Extraction and achieved the best performance in the event enrichment subtask. Miyao et al. (2008) compared syntactic information of various statistical parsers on PPI. 3 3.1 Texts Comprising the Corpus We used a discharge summary among various documents in a hospital as the source data of the task. The discharge summary is a document created by a doctor or another medical expert at the conclusion of a hospital stay. Medications performed during a stay are written in discharge summaries. If adverse–effect relations were observed during the stay, they are likely to be expressed in free text. Texts written in discharge summaries tend to be written more roughly than texts in newspaper article"
W10-3911,W01-0521,0,0.0479031,"ations with various certainties. To establish this goal, we used a dependency structure for the adverse–effect relation extraction method. Adverse–effect statements are assumed to share a dependency structure to a certain degree. For example, if we obtain the dependency structures as shown in Figure 3, then we can easily determine that the structures are similar. Of course, obtaining such perfect parsing results is not always possible. A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001). A statistical parser will likely output incomplete results in these texts and will likely have a negative effect on relation extraction methods which depend on it. The specified research topic of this study is to investigate whether incomplete dependency structures are effective and how they behave in the extraction of uncertain relations. 76 2 effect relation extraction method. This section presents a description of details of the corpus. Related Works Various studies have been done to extract semantic information from texts. SemEval-2007 Task:04 (Girju et al., 2007) is a task to extract se"
W10-3911,P06-1015,0,0.0454687,"ance of a &lt;symptom relation=“1”&gt;headache &lt;/symptom&gt;. for no-PP Lasix wo-PP headache no-PP appear niyori-PP minimal path label drug symptom negative Lasix hyperpiesia positive Lasix headache Lasix, wo-PP, headache, no-PP, appear, niyori-PP, suspend, ta-AUX Figure 5. Pair extraction example. Figure 6. Dependency chain example. several examples, wherein “relation=1” denotes the ID of a adverse–effect relation. In the corpus, 236 relations were annotated. 4 suspend ta-AUX verse–effect relations or not. A pattern-based semi-supervised approach like Saeger et al. (2008), or more generally Espresso (Pantel and Pennacchiotti, 2006), can also be taken, but we chose a pair classification approach to avoid the effect of seed patterns. To capture a view of an adverseness of a drug, a statistic of adverse–effect relations is important. We do not want to favor certain patterns and chose a pair classification approach to equally treat every relation. Extraction steps of our method are as presented below. STEP 1: Pair Extraction All combinations of drug–symptom pairs that appear in a same sentence are extracted. Pairs Extraction Method We present a simple adverse–effect relation extraction method. We extract drug–symptom pairs"
W10-3911,S07-1003,0,0.012739,"which the parser is trained (Gildea, 2001). A statistical parser will likely output incomplete results in these texts and will likely have a negative effect on relation extraction methods which depend on it. The specified research topic of this study is to investigate whether incomplete dependency structures are effective and how they behave in the extraction of uncertain relations. 76 2 effect relation extraction method. This section presents a description of details of the corpus. Related Works Various studies have been done to extract semantic information from texts. SemEval-2007 Task:04 (Girju et al., 2007) is a task to extract semantic relations between nominals. The task includes “Cause–Effect” relation extraction, which shares some similarity with a task that will be presented herein. Saeger et al. (2008) presented a method to extract potential troubles or obstacles related to the use of a given object. This relation can be interpreted as a more general relation of the adverse–effect relation. The protein–protein interaction (PPI) annotation extraction task of BioCreative II (Krallinger et al., 2008) is a task to extract PPI from PubMed abstracts. BioNLP’09 Shared Task on Event Extraction (Ki"
W10-3911,W09-1406,0,0.0136495,"ical domains as speculation (Light et al., 2004). Vincze et al. (2008) are producing a freely available corpus including annotations of uncertainty along with its scope. Dependency structure feature which we utilized to extract adverse–effect relations are widely used in relation extraction tasks. We present previous works which used syntactic/dependency information as a feature of a statistical method. Beamer et al. (2007), Giuliano et al. (2007), and Hendrickx et al. (2007) all used syntactic information with machine learning techniques in SemEval-2007 Task:04 and achieved good performance. Riedel et al. (2009) used dependency path features with a statistical relational learning method in BioNLP’09 Shared Task on Event Extraction and achieved the best performance in the event enrichment subtask. Miyao et al. (2008) compared syntactic information of various statistical parsers on PPI. 3 3.1 Texts Comprising the Corpus We used a discharge summary among various documents in a hospital as the source data of the task. The discharge summary is a document created by a doctor or another medical expert at the conclusion of a hospital stay. Medications performed during a stay are written in discharge summarie"
W10-3911,S07-1028,0,0.0288912,"reports in the bio-medical domain. Friedman et al. (1994) describes the certainty in findings of clinical radiology. Certainty is also known in scientific papers of biomedical domains as speculation (Light et al., 2004). Vincze et al. (2008) are producing a freely available corpus including annotations of uncertainty along with its scope. Dependency structure feature which we utilized to extract adverse–effect relations are widely used in relation extraction tasks. We present previous works which used syntactic/dependency information as a feature of a statistical method. Beamer et al. (2007), Giuliano et al. (2007), and Hendrickx et al. (2007) all used syntactic information with machine learning techniques in SemEval-2007 Task:04 and achieved good performance. Riedel et al. (2009) used dependency path features with a statistical relational learning method in BioNLP’09 Shared Task on Event Extraction and achieved the best performance in the event enrichment subtask. Miyao et al. (2008) compared syntactic information of various statistical parsers on PPI. 3 3.1 Texts Comprising the Corpus We used a discharge summary among various documents in a hospital as the source data of the task. The discharge summar"
W10-3911,S07-1039,0,0.0591267,"Missing"
W10-3911,C08-1024,0,0.119924,"it. The specified research topic of this study is to investigate whether incomplete dependency structures are effective and how they behave in the extraction of uncertain relations. 76 2 effect relation extraction method. This section presents a description of details of the corpus. Related Works Various studies have been done to extract semantic information from texts. SemEval-2007 Task:04 (Girju et al., 2007) is a task to extract semantic relations between nominals. The task includes “Cause–Effect” relation extraction, which shares some similarity with a task that will be presented herein. Saeger et al. (2008) presented a method to extract potential troubles or obstacles related to the use of a given object. This relation can be interpreted as a more general relation of the adverse–effect relation. The protein–protein interaction (PPI) annotation extraction task of BioCreative II (Krallinger et al., 2008) is a task to extract PPI from PubMed abstracts. BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009) is a task to extract bio-molecular events (bioevents) from the GENIA event corpus. Similar characteristics to those of the adverse–effect relation are described in previous reports in the b"
W10-3911,W08-0606,0,0.0331798,"protein interaction (PPI) annotation extraction task of BioCreative II (Krallinger et al., 2008) is a task to extract PPI from PubMed abstracts. BioNLP’09 Shared Task on Event Extraction (Kim et al., 2009) is a task to extract bio-molecular events (bioevents) from the GENIA event corpus. Similar characteristics to those of the adverse–effect relation are described in previous reports in the bio-medical domain. Friedman et al. (1994) describes the certainty in findings of clinical radiology. Certainty is also known in scientific papers of biomedical domains as speculation (Light et al., 2004). Vincze et al. (2008) are producing a freely available corpus including annotations of uncertainty along with its scope. Dependency structure feature which we utilized to extract adverse–effect relations are widely used in relation extraction tasks. We present previous works which used syntactic/dependency information as a feature of a statistical method. Beamer et al. (2007), Giuliano et al. (2007), and Hendrickx et al. (2007) all used syntactic information with machine learning techniques in SemEval-2007 Task:04 and achieved good performance. Riedel et al. (2009) used dependency path features with a statistical"
W10-3911,W09-1401,0,0.0630387,"Missing"
W13-4601,W09-1119,0,0.0234817,"d named entity recognizer. The method assumes that information to be extracted can be expressed as named entities. NER can be interpreted as a sequential labeling problem. We utilized linear-chain CRF (Lafferty et al., 2001), one of widely used methods to handle the problem, with character-level node. Character-level processing is chosen since Japanese text is unsegmented text and a character-level NER is known to achieve the state-of-the-art accuracy (Asahara and Matsumoto, 2003). NER is known as a knowledge-intensive task and the use of external knowledge often boosts the performance of it (Ratinov and Roth, 2009). Various knowledge resources (e.g. dictionary, terminology, ontology) are available in medical fields. We decided to exploit three publicly available medical terminologies, MedDRA/J4 (Brown et al., 1999), MEDIS Byomei Master5 (Medical Information System Development Center, 2012), and MEDIS Shojo Shoken Master hShintai Shoken Heni6 . Additionally to these terminologies, we also utilized information obtained from an external corpus in a medical domain. We introduced named entities that are defined on the updated version of the discharge summary corpus (DS Corpus) mentioned in Aramaki et al. (20"
W13-4601,W06-2809,0,0.0397189,"Missing"
W13-4601,N03-1002,0,0.0808575,"Missing"
W13-4601,W03-0425,0,0.0555076,"d entities and disease name entities in DS Corpus can be a clue to recognize complaints and diagnoses (high recall) but differences between them degraded the certainty of recognition (low precision). Table 4: The number of terms that are present in NTCIR-10 MedNLP data for each terminology. MEDIS BM & SSM is the union of the two MEDIS terminologies that we used. NER can be realized with character level processing. Klein et al. (2003) demonstrated the effectiveness of using character substrings in an English NER. The effectiveness of using dictionaries or gazetteers is shown in previous works. Florian et al. (2003) used location, person, and organization gazetteers in their NER framework and reported an error reduction in an extraction performance. Cohen and Sarawagi (2004) exploited a state, a city, a person, and a company dictionaries to improve NER. Jonnalagadda et al. (2013) used various medical resources in their NER system and showed an increase in an extraction performance of medical concepts. Automatic constructions of a dictionary/gazetteer are also examined. Kazama and Torisawa (2007) and Toral and Mu˜noz (2006) exploited Wikipedia to construct a dictionary/gazetteer that is useful for NER. 5."
W13-4601,W13-0404,0,0.0287627,"data for each terminology. MEDIS BM & SSM is the union of the two MEDIS terminologies that we used. NER can be realized with character level processing. Klein et al. (2003) demonstrated the effectiveness of using character substrings in an English NER. The effectiveness of using dictionaries or gazetteers is shown in previous works. Florian et al. (2003) used location, person, and organization gazetteers in their NER framework and reported an error reduction in an extraction performance. Cohen and Sarawagi (2004) exploited a state, a city, a person, and a company dictionaries to improve NER. Jonnalagadda et al. (2013) used various medical resources in their NER system and showed an increase in an extraction performance of medical concepts. Automatic constructions of a dictionary/gazetteer are also examined. Kazama and Torisawa (2007) and Toral and Mu˜noz (2006) exploited Wikipedia to construct a dictionary/gazetteer that is useful for NER. 5.2 Generality of Knowledge Resource Incorporation The approach we took for the incorporation of terminology has a high generality. The approach requires only entries of a terminology. More rich contents like glosses or synonyms are not required. This characteristic make"
W13-4601,D07-1073,0,0.184534,"of our method (detail will be described in Section 3.1) was trained on DS Corpus to realize a DS Corpus named entity recognizer. Table 1 lists all features that are used in our method. For all features, sliding window features illustrated in figure 2 are considered. All features derive information from character, morpheme, or external knowledge. Therefore several preprocesses are done prior to the feature extraction. A morphological analysis and assignments of the resulting morphemes to character nodes are done to extract “M-*” features. A BIOstyle match of the three terminologies similar to Kazama and Torisawa (2007) is applied to extract “K-MEDDRA”, “K-MEDIS-BM”, and “KMEDIS-SSM” features. The DS Corpus named entities are recognized and the BIO-style matches of them are performed to extract “K-NE-SD” feature. 2.1 Implementation This section briefly describes the method in an implementation perspective. Figure 3 portraits the architecture of the method. Text Normalization Module Three simple text normalization processes are applied to an input text as a first step. Firstly, a Unicode normalization in form NFKC7 is applied. Secondly, all upper case characters are converted to lower case ones based on the d"
W13-4601,W03-0428,0,0.116769,"Missing"
W13-4602,W09-1324,1,0.839396,"nd Support Services Organization, 2007) have been used for the representation of clinical concepts. There are other studies in this domain (Sager et al., 1994; Cimino, 1991; Kong et al., 2008; Peleg and Tu, 2006). Lastly, there have been several lines of work that explored the tools for information extraction on clinical reports. For example, (Friedman et al., 1994) developed Medical Language Extraction and Encoding (MedLEE) to encode clinical documents in a structured form. The Mayo Clinic also developed a similar NLP system (cTakes) (Savova et al., 2010) for clinical reports and TEXT2TABLE (Aramaki et al., 2009) targeted Japanese discharge summaries. Processing (NLP). Physicians may describe a finding in a sentence, which is common for pathological and radiological findings. Such a finding might have a corresponding term, and an elaborated system might cleverly map the sentence into a standardized vocabulary. However, this process involves various tasks, such as processing of negation, dependency, ambiguity, and abstraction, most of which are still unreliable for clinical use at this point. Even mapping of phrases is a challenge. For example, physicians may describe the concept “mental retardation” i"
W13-4602,W07-1011,0,0.0833719,"Missing"
W13-4606,C10-2128,0,\N,Missing
W15-1701,D11-1145,1,0.689984,"tion. They judged whether the tweet was posted just after an earthquake using a support vector machine (SVM), and determined the seismic center from the formatted tweets. In addition, they developed a system that raises the alarm about an earthquake from the predicted results. Bollen et al. (2011) extracted the social mood, and predicted the stock price fluctuation N days from the day of observation by using evaluated data of the ’mood-related’ dictionary. As a result, they concluded that they could show the 3 days from the ’calm-mood’ day might be able to predict the stock price fluctuation. Aramaki et al. (2011) predicted an influenza epidemic from tweets. They showed the possibility of information extraction from the tweets that reflects the actual world’s situation by using language processing technologies. Boyd et al. (2010) examined a practice of retweeting as a way by which participants can be “in a conversation.” Paul and Dredze (2011) considered a broader range of public health applications for Twitter and showed quantitative correlations with public health data and qualitative evaluations of model output. Baldwin et al. (2013) explored how linguistically noisy or otherwise it is over a range"
W15-1701,I13-1041,0,0.0132289,"lm-mood’ day might be able to predict the stock price fluctuation. Aramaki et al. (2011) predicted an influenza epidemic from tweets. They showed the possibility of information extraction from the tweets that reflects the actual world’s situation by using language processing technologies. Boyd et al. (2010) examined a practice of retweeting as a way by which participants can be “in a conversation.” Paul and Dredze (2011) considered a broader range of public health applications for Twitter and showed quantitative correlations with public health data and qualitative evaluations of model output. Baldwin et al. (2013) explored how linguistically noisy or otherwise it is over a range of social media sources empirically over popular social media text types, in the form of YouTube comments, Twitter posts, web user forum posts, blog posts and Wikipedia. Yin et al. (2012) constructed a system architecture for leveraging social media to enhance emergency situation awareness with high-speed text streams retrieved from Twitter during natural disasters and crises. In these researches, the location of an SNS document plays an important role in extracting informa3 tion, and in most cases, rely on GPS function connect"
W15-1701,C12-1064,0,0.0329781,"he IP address of past content cannot be accessed, and this approach is becoming increasingly ineffective with the increased use of portable terminals. As a result, location name disambiguation should now focus on procedures that consider the original text. As information references, Web pages and change logs in Wikipedia have been used as the basis of location name disambiguation. These resources are homogeneous and manageable. In contrast, the numerous data on SNS often contain noise, which makes disambiguation unmanageable. A number of studies have investigated location name disambiguation. Han et al. (2012) extracted location-indicative words from tweet data by calculating the information gain ratios. Their paper states that the words improved the estimation performance of the users’ location. They concluded that the procedure requires relatively little memory, is fast, and could potentially be used by lexicographers to extract location-indicative words. Backstrom et al. (2008) developed a probabilistic framework to quantify the spatial variation manifested in search queries. This allowed them to obtain a measure of spatial dispersion that indicates regional information. Adams and Janowicz (2012"
W15-1701,P13-4002,0,0.0143475,"GPS information. 1 “I arrived at Prefectural Office Ave. from Shuri Station!” Introduction As the volume of documents on the Web increases, technologies to extract useful information from them become increasingly essential. For instance, information extracted from social network services (SNS) such as Twitter and Facebook is useful because it contains a lot of location-specific information. To extract such information, it is necessary to identify the location of each location-relevant expression within a document. However, many previous studies on SNS rely only on geo-tagged documents (e.g., (Han et al., 2013; Han et al., 2014)), which include GPS information, In this paper, we propose a method that identifies the locations of location expressions in Twitter tweets on the basis of the following two clues: (1) spatial proximity, and (2) temporal consistency. Spatial proximity assumes that all locations mentioned in a tweet are close to one another. In the above document, for example, we would assume that “Prefectural Office Ave.” is “Prefectural Office Ave. (Okinawa)” using the proximity between “Shuri Station” and “Prefectural Office Ave. (Okinawa)” The other clue is temporal consistency, 1 Semioc"
W16-4211,W14-3210,0,0.0401643,"dies. Well-known studies were those conducted by Roark et al. (Roark et al. 2007; Roark et al. 2011), which analyzed the lexical features and syntactic feature from transcripts of spoken narrative such as neuropsychological approaches (Moriyama et al. 2015) and automatic speech analysis approaches (König et al. 2015). Some of them used automatic speech recognition (Tóth et al. 2015). Aramaki et al. specifically examined vocabulary size in speech transcription (Aramaki et al. 2016). Tanaka et al. proposed a novel approach using computer avatars (Tanaka et al. 2016). In addition Orimaye et al. (Orimaye et al. 2014) used machine learning algorithms to build diagnostic models using syntactic and lexical features and Jarrold et al. used LIWC for aided diagnosis of Dementia (Jarrold et al. 2014). Author Method Disease Aramaki et al. (Aramaki et al. 2016) Analysis of vocabulary size in speech MCI, AD 22 2016 Tanaka et al. (Tanaka et al. 2016) Spoken dialog with computer avatars MCI 18 2016 König et al. (König et al. 2015) Automatic speech analyse MCI, AD 64 2015 Tóth et al. (Tóth et al. 2015) Acoustic indicator MCI 51 2015 Moriyama et al. (Moriyama et al. 2015) Neuropsychological battery AD 299 2015 Orimaye"
W16-4211,W07-1001,0,0.224518,"oot et al. 2000). However, these are often dependent on the type of picture confrontation naming task, the severity or stage of the disease, or other unique patient-level circumstances (Geda 2012). MCI, part of which constitutes a pre-stage of dementia, might indicate a boundary between aging-related non-dementia reduction in cognition and dementia on the spectrum of cognitive function. Using the above characteristics, various dementia screening methods have been proposed to date. Table 2 shows the summary of previous screening studies. Well-known studies were those conducted by Roark et al. (Roark et al. 2007; Roark et al. 2011), which analyzed the lexical features and syntactic feature from transcripts of spoken narrative such as neuropsychological approaches (Moriyama et al. 2015) and automatic speech analysis approaches (König et al. 2015). Some of them used automatic speech recognition (Tóth et al. 2015). Aramaki et al. specifically examined vocabulary size in speech transcription (Aramaki et al. 2016). Tanaka et al. proposed a novel approach using computer avatars (Tanaka et al. 2016). In addition Orimaye et al. (Orimaye et al. 2014) used machine learning algorithms to build diagnostic models"
W16-4211,W14-3204,0,0.0249562,"spoken narrative such as neuropsychological approaches (Moriyama et al. 2015) and automatic speech analysis approaches (König et al. 2015). Some of them used automatic speech recognition (Tóth et al. 2015). Aramaki et al. specifically examined vocabulary size in speech transcription (Aramaki et al. 2016). Tanaka et al. proposed a novel approach using computer avatars (Tanaka et al. 2016). In addition Orimaye et al. (Orimaye et al. 2014) used machine learning algorithms to build diagnostic models using syntactic and lexical features and Jarrold et al. used LIWC for aided diagnosis of Dementia (Jarrold et al. 2014). Author Method Disease Aramaki et al. (Aramaki et al. 2016) Analysis of vocabulary size in speech MCI, AD 22 2016 Tanaka et al. (Tanaka et al. 2016) Spoken dialog with computer avatars MCI 18 2016 König et al. (König et al. 2015) Automatic speech analyse MCI, AD 64 2015 Tóth et al. (Tóth et al. 2015) Acoustic indicator MCI 51 2015 Moriyama et al. (Moriyama et al. 2015) Neuropsychological battery AD 299 2015 Orimaye et al. (Orimaye et al. 2014) Machine learning algorithms AD 556 2014 Jarrold et al. (Jarrold et al. 2014) Analysis of spontaneous speech AD 48 2014 Roark et al. (Roark et al. 2011)"
W17-5803,C16-1008,1,0.815613,"y influenza, cause many deaths and spread rapidly. Next, from the perspective of informatics, epidemics of these diseases are suitable targets because some epidemics have the following characteristics that make them easy to ascertain from Introduction Nowadays, the concept of social sensors (Sakaki et al., 2010) has been shown to have great potential feasibility for various practical applications. Particularly, disease detection is a core target of social sensor based studies. To date, detection has been demonstrated for influenza (Aramaki et al., 2011; Paul et al., 2014; Lampos et al., 2015; Iso et al., 2016; Wakamiya et al., 2016; Zhang et al., 18 Proceedings of the International Workshop on Digital Disease Detection using Social Media 2017 (DDDSM-2017), pages 18–25, c Taipei, Taiwan, November 27, 2017. 2017 AFNLP This result contributes to social sensor reliability. This paper is the first reporting the overall relation between social sensor performance and its factors. Although detection of small and unseasonal events is difficult, the sensor can be applied in specified situations. social media: 1. Seasonal Event: some epidemics are seasonal diseases that have basically one big peak during one"
W17-5803,W04-3230,0,0.0940474,"”, and “diarrhea”. This keyword sets are selected in preliminary experiments that use 11 major complaints (Chester et al., 2011). Using the tweet corpus collected in the previous step, we built a classifier that judges whether a given tweet is sent by a patient (positive) or not (negative). This task is a sentence binary classification. We used a SVM-based classifier under the bag-of-words (BOW) representation (Cortes and Vapnik, 1995; Joachims, 1998). Then we split a Japanese sentence into a sequence of words using a Japanese morphological analyzer, MeCab2 (ver.0.98) with IPADic (ver.2.7.0) (Kudo et al., 2004). The polynomial kernel (d=2) is used as the kernel function. To build the training set, a human annotator assigned either a positive or negative label. For the labeling process, we followed conditions used in our previous study (Aramaki et al., 2011). Table 1 presents samples of tweets with labels. Finally, we classified tweets into areas for areabased disease surveillance. The area is resolved based on metadata attached to a tweet as follows: 3.2 Linear Regression Analysis of Patient Numbers Next we investigate the relation between the number of infected people and the number estimated using"
Y10-1075,I08-1007,1,0.872999,"Missing"
Y10-1075,P94-1038,0,0.256331,"Missing"
Y10-1075,P08-1047,1,0.887762,"Missing"
Y10-1075,D08-1047,0,0.0534269,"Missing"
Y10-1075,I08-1025,0,0.0329099,"Missing"
Y10-1075,C02-1119,0,0.053663,"Missing"
Y10-1075,J01-2002,0,0.052682,"Missing"
Y10-1075,P97-1017,0,\N,Missing
